Estimating Topical Context by Diverging from External Resources

Romain Deveaud
University of Avignon - LIA Avignon, France
romain.deveaud@univavignon.fr

Eric SanJuan
University of Avignon - LIA Avignon, France
eric.sanjuan@univavignon.fr

Patrice Bellot
Aix-Marseille University - LSIS Marseille, France
patrice.bellot@lsis.org

ABSTRACT
Improving query understanding is crucial for providing the user with information that suits her needs. To this end, the retrieval system must be able to deal with several sources of knowledge from which it could infer a topical context. The use of external sources of information for improving document retrieval has been extensively studied. Improvements with either structured or large sets of data have been reported. However, in these studies resources are often used separately and rarely combined together. We experiment in this paper a method that discounts documents based on their weighted divergence from a set of external resources. We present an evaluation of the combination of four resources on two standard TREC test collections. Our proposed method significantly outperforms a state-of-the-art Mixture of Relevance Models on one test collection, while no significant differences are detected on the other one.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Relevance feedback
Keywords
External resources, language models, topical context
1. INTRODUCTION
When searching for specific information in a document collection, users submit a query to the retrieval system. The query is a representation or an interpretation of an underlying information need, and may not be accurate depending on the background knowledge of the user. Automatically retrieving documents that are relevant to this initial information need may thus be challenging without additional information about the topical context of the query. One common approach to tackle this problem is to extract evidences from query-related documents [8, 16]. The basic idea is to expand the query with words or multi-word terms extracted
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright © 2013 ACM 978-1-4503-2034-4/13/07...$15.00.

from feedback documents. This feedback set is composed of documents that are relevant or pseudo-relevant to the initial query, and that are likely to carry important pieces of information. Words that convey the most information or that are the most relevant to the initial query are then used to reformulate the query. They can come from the target collection or from external sources and several sources can be combined [1, 3]. These words usually are synonyms or related concepts, and allow to infer the topical context of the user search. Documents are then ranked based, among others, on their similarity to the estimated topical context.
We explore the opposite direction and choose to carry experiments with a method that discounts documents scores based on their divergences from pseudo-relevant subsets of external resources. We allow the method to take several resources into account and to weight the divergences in order to provide a comprehensive interpretation of the topical context. More, our method equally considers sequences of 1, 2 or 3 words and chooses which terms best describe the topical context without any supervision.
The use of external data sets had been extensively studied in the pseudo-relevance feedback setting, and proved to be effective at improving search performance when choosing proper data. However studies mainly concentrated on demonstrating how the use of a single resource could improve performance. Data sources like Wikipedia [10, 15], WordNet [11, 15], news corpora or even the web itself [1, 3] were used separately for enhancing search performances. Combining several source of information was nonetheless studied in [1]. However the authors used web anchor and heading texts, which are very small units that are less likely to carry a complete context. They also used the entire Wikipedia but they did not report results of its contribution in the information sources combination. Diaz and Metzler [3] investigated the use of larger and more general external resources than those used in [1]. They present a Mixture of Relevance Models (MoRM) that estimates the query model using a news corpus and two web corpora as external sources, and achieves state-of-the-art retrieval performance. To our knowledge, this last approach is the closest one from the method we experiment in this paper.
2. DIVERGENCE FROM RESOURCES
In this work, we use a language modeling approach to information retrieval. Our goal is to accurately model the topical context of a query by using external resources. We use the Kullback-Leibler divergence to measure the information gain (or drift) between a given resource R and a

1001

document D. Formally, the KL divergence between two language models R and D is written as:

K L(R ||D )

=

tV

P (t|R) log

P (t|R) P (t|D)

=

P (t|R) log P (t|R) - P (t|R) log P (t|D)

tV

tV

 - P (t|R) log P (t|D)

(1)

tV

where t is a term belonging to vocabulary V . The first part is the resource entropy and does not affect ranking of documents, which allows us to simplify the KL divergence and to obtain equation (1). In order to capture the topical context from the resource, we estimate the R model through pseudo-relevance feedback. Given a ranked list RQ obtained by retrieving the top N documents of R using query likelihood, the feedback query model is estimated by:

P (t|^R) 

P (Q|DF ) - P (w|DF ) log P (w|DF )

DF RQ

wt

The right-hand expression of this estimation is actually equivalent to computing the entropy of the term t in the pseudorelevant subset RQ. One advantage of doing so it that t may not be necessarily a single term, like in traditional relevance models approaches [3, 9], or a fixed-length term [12]. When forming the V set, we slide a window over the entire textual content of RQ and consider all sequences of 1, 2 or 3 words.
Following equation (1), we compute the information divergence between a resource R and a document D as:

D(^R||D) = - P (t|^R) log P (t|D)
tV

The final score of a document D with respect to a given user query Q is determined by the linear combination of query word matches (standard retrieval) and the weighted divergence from general resources. It is formally written as:

s(Q, D) =  log P (Q|D) - (1 - )

R · D(^R||D) (2)

RS

where S is a set of resources, P (Q|D) is standard query likelihood with Dirichlet smoothing and R represents the weight given to resource R. We use here the information divergence to reduce the score of a document: the greater the divergence, the lower the score of the document will be. Hence the combination of several resources intuitively acts as a generalization of the topical context, and increasing the number of resources will eventually improve the topical representation of the user information need. While we chose to use traditional query likelihood for practical and reproducibility reasons, it could entirely be substituted with other state-of-the-art retrieval models (e.g. MRF-IR [12], BM25 [13]...).

3. EXPERIMENTS
3.1 Experimental setup
We performed our evaluation using two main TREC1 collections which represent two different search contexts. The first one is the WT10g web collection and consists of 1,692,096
1http://trec.nist.gov

web pages, as well as the associated TREC topics (451-550) and judgments. The second data set is the Robust04 collection, which is composed of news articles coming from various newspapers. It was used in the TREC 2004 Robust track and is composed of standard corpora: FT (Financial Times), FR (Federal Register 94), LA (Los Angeles Times) and FBIS (i.e. TREC disks 4 and 5, minus the Congressional Record). The test set contains 250 topics (301-450, 601-700) and relevance judgements of the Robust 2004 track. Along with the test collections, we used a set of external resources from which divergences are computed. This set is composed of four general resources: Wikipedia as an encyclopedic source, the New York Times and GigaWord corpora as sources of news data and the category B of the ClueWeb092 collection as a web source. The English GigaWord LDC corpus consists of 4,111,240 news-wire articles collected from four distinct international sources including the New York Times [4]. The New York Times LDC corpus contains 1,855,658 news articles published between 1987 and 2007 [14]. The Wikipedia collection is a recent dump from May 2012 of the online encyclopedia that contains 3,691,092 documents3. We removed the spammed documents from the category B of the ClueWeb09 according to a standard list of spams for this collection4. We followed authors recommendations [2] and set the "spamminess" threshold parameter to 70. The resulting corpus contains 29,038,220 web pages.
Indexing and retrieval were performed using Indri5. The two test collections and the four external resources were indexed with the exact same parameters. We use the standard INQUERY english stoplist along with the Krovetz stemmer. We employ a Dirichlet smoothing and set the  parameter to 1, 500. Documents are ranked using equation (2). We compare the performance of the approach presented in Section 2 (DfRes) with that of three baselines: Query Likelihood (QL), Relevance Models (RM3) [9] and Mixture of Relevance Models (MoRM) [3]. In the results reported in Table 1, the MoRM and DfRes approaches both perform feedback using all external resources as well as the target collection, while RM3 only performs feedback using the target collection. QL uses no additional information.
RM3, MoRM and DfRes depend on three free-parameters:  which controls the weight given to the original query, k which is the number of terms and N which is the number of feedback documents from which terms are extracted. We performed leave-one-query-out cross-validation to find the best parameter setting for  and averaged the performance for all queries. Previous research by He and Ounis [5] showed that doing PRF with the top 10 pseudo-relevant feedback documents was as effective as doing PRF with only relevant documents present in the top 10, and that there are no statistical differences. Following these findings, we set N = 10 and also k = 20, which was found to be a good PRF setting. DfRes depends on an additional parameter R which controls the weight given to each resource. We also perform leave-one-query-out cross-validation to learn the best setting for each resource. Although the results in Table 1 correspond to this parameter setting, we explore in the following section the influence of the N and k parameters. In
2http://boston.lti.cs.cmu.edu/clueweb09/ 3http://dumps.wikimedia.org/enwiki/20110722/ 4http://plg.uwaterloo.ca/~gvcormac/clueweb09spam/ 5http://www.lemurproject.org/

1002

wt10g robust

QL MAP P@20

0.2026 0.2461

0.2429 0.3528

RM3 MAP P@20
0.2035 0.2449 0.2727 0.3677

MoRM

MAP

P@20

0.2339, 0.2869,

0.2833, 0.3799,

DfRes

MAP

P@20

0.2463, 0.3147,,

0.2954, 0.4024,,

Table 1: Document retrieval results reported in terms of Mean Average Precision and Precision at 20 documents. We use a two sided paired wise t-test to determine significant differences over baselines. ,  and  indicate statistical improvements over QL, RM3 and MoRM respectively, with p < 0.05.

the following section, when discussing results obtained using single sources of expansion with DfRes, we use the notation DfRes-r where r  (Web,Wiki,NYT,Gigaword).
3.2 Results
The main observation we can draw from the ad hoc retrieval results presented in Table 1 is that using a combination of external information sources performs always better than only using the target collection. The numbers we report vary from those presented in [3], however we could not replicate the exact same experiments since the authors do not detail indexing parameters. DfRes significantly outperforms RM3 on both collections, which confirms that state that combining external resources improves retrieval.
We see from Figure 1 that DfRes-Gigaword is ineffective on the WT10g collection, which is not in line with the results reported in [3] where the Gigaword was found to be an interesting source of expansion. Another remarkable result is the ineffectiveness of the WT10g collection as a single source of expansion. However we see from Table 2 that the learned weight R of this resource is very low (= 0.101), which significantly reduces its influence compared to other best performing resources (such as NYT or Web).

wt10g robust

nyt
0.303 0.309

wiki
0.162 0.076

gigaword
0.121 0.281

web
0.313 0.149

robust
0.185

wt10g
0.101 -

Table 2: R weights learned for resources on the two collections. We averaged weights over all queries.

Results are more coherent on the Robust collection. DfResNYT and DfRes-Gigaword achieve very good results, while the combination of all resources consistently achieves the best results. The very high weights learned for these resources hence reflect these good performances. As previously noted, the Robust collection is composed of news articles coming from several newspapers (not including the NYT). In this specific setting, it seems that the nature of the good-performing resources is correlated with the nature of the target collection. We observed that NYT and Gigaword articles, which are focused contributions produced by professional writers, are smaller on average (in unique words) than Wikipedia or Web documents.
We explored the influence of the number of feedback documents used for the approximation of each resource. We omit the plots of retrieval performances for the sake of space, and also because they are not noteworthy. Performances indeed remain almost constant for all resources as N varies. Changes in MAP are about ± 2% from N = 1 to N = 20 depending on the resource. However we also explored the influence of the number of terms used to estimate each resource's model. While we could expect that increasing the number of terms would improve the granularity of the model

and maybe capture more contextual evidences, we see from Figure 2 that using 100 terms is not really different than using 20 terms. We even see that using only 5 terms achieves the best results for DfRes on the WT10g collection.
Overall, these results show support for the principles of polyrepresentation [6] and intentional redundancy [7] which state that combining cognitively and structurally different representations of information needs and documents will increase the likelihood of finding relevant documents. Since we use several resources of very different natures ranging from news articles to web pages, DfRes takes advantage of this variety to improve the estimation of the topical context. Moreover, the most effective values of  tend to be low, which means that DfRes is more effective than the initial query. We even see on Figure 1 that only relying on the divergence from resources (i.e. setting  = 0) achieves better results than only relying on the user query (i.e. setting  = 1). More, setting  = 0 for DfRes also outperforms MoRM (significantly on the Robust collection). This suggests that DfRes is actually better as estimating the topical context of the information need than the user keyword query.
We also observe from Figure 1 and 2 that the NYT is the resource that provides the best estimation of the topical context for the two collections, despite being the smallest one. This may be due to the fact that articles are well-written by professionals and contain lots of synonyms to avoid repetition. Likewise, the failure of Wikipedia may be due to the encyclopedic segmentation of articles. Since each Wikipedia article covers a specific concept, it is likely that only conceptrelated articles compose the pseudo-relevant set, which may limit a larger estimation of the topical context. One of the originality of the DfRes is that it can automatically take into account n-grams without any supervision (such as setting the size of the grams prior to retrieval). In practice, there is on average 1.19 words per term, but most of the time articles like "the" are added to words that already were selected (i.e. "the nativity scene", where "nativity" and "scene" were used before as single words).
4. CONCLUSION & FUTURE WORK
Accurately estimating the topical context of a query is a challenging issue. We experimented a method that discounts documents based on their average divergence from a set of external resources. Results showed that, while reinforcing previous research, this method performs at least as good as a state-of-the-art resource combination approach, and sometimes achieves significantly higher results. Performances achieved by the NYT as a single resource are very promising and need further exploration, as well as the counter-performance of Wikipedia. More specifically, using nominal groups or sub-sentences that rely on the good quality of NYT articles could be interesting and in line with ongoing research in the Natural Language Processing field.

1003

wt10g

robust

MAP 0.20 0.22 0.24 0.26 0.28 0.30

MAP 0.12 0.14 0.16 0.18 0.20 0.22 0.24

wt10g web wiki nyt gigaword all

robust web wiki nyt gigaword all

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

lambda

lambda

Figure 1: Retrieval performance (in MAP) as a function of the  parameter. The DfRes results reported in Table 1 are depicted by curve "all", while all other curves correspond to DfRes with a single resource. Baselines are shown for reference: dashed lines represent RM3 and dotted lines represent MoRM.

wt10g

robust

0.30

0.25

MAP

MAP 0.14 0.16 0.18 0.20 0.22 0.24

0.20

wt10g web wiki nyt gigaword all

robust web wiki nyt gigaword all

0.15

0

20

40

60

80

100

0

20

40

60

80

100

k

k

Figure 2: Retrieval performance (in MAP) as a function of the number of terms k used for estimating the

resource language model. Legend is the same as in Figure 1.

5. ACKNOWLEDGMENTS
This work was supported by the French Agency for Scientific Research (Agence Nationale de la Recherche) under CAAS project (ANR 2010 CORD 001 02).
6. REFERENCES
[1] M. Bendersky, D. Metzler, and W. B. Croft. Effective query formulation with multiple information sources. In Proceedings of WSDM, 2012.
[2] G. Cormack, M. Smucker, and C. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information Retrieval, 2011.
[3] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In Proceedings of SIGIR, 2006.
[4] D. Graff and C. Cieri. English Gigaword. Philadelphia: Linguistic Data Consortium, LDC2003T05, 2003.
[5] B. He and I. Ounis. Finding good feedback documents. In Proceedings of CIKM, 2009.
[6] P. Ingwersen. Polyrepresentation of information needs and semantic entities: elements of a cognitive theory for information retrieval interaction. In Proc. of SIGIR, 1994.
[7] K. Jones. Retrieving Information Or Answering Questions? British Library annual research lecture. British Library Research and Development Department, 1990.

[8] R. Kaptein and J. Kamps. Explicit extraction of topical context. J. Am. Soc. Inf. Sci. Technol., 62(8):1548­1563, Aug. 2011.
[9] V. Lavrenko and W. B. Croft. Relevance based language models. In Proceedings of SIGIR, 2001.
[10] Y. Li, W. P. R. Luk, K. S. E. Ho, and F. L. K. Chung. Improving weak ad-hoc queries using Wikipedia as external corpus. In Proceedings of SIGIR, 2007.
[11] S. Liu, F. Liu, C. Yu, and W. Meng. An effective approach to document retrieval via utilizing WordNet and recognizing phrases. In Proceedings of SIGIR, 2004.
[12] D. Metzler and W. B. Croft. Latent Concept Expansion Using Markov Random Fields. In Proc. of SIGIR, 2007.
[13] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. In Proceedings of SIGIR, 1994.
[14] E. Sandhaus. The New York Times Annotated Corpus. Philadelphia: Linguistic Data Consortium, LDC2008T19, 2008.
[15] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of semantic knowledge. In Proceedings of WWW, 2007.
[16] R. W. White, P. Bailey, and L. Chen. Predicting user interests from contextual information. In Proceedings of SIGIR, 2009.

1004

Finding Knowledgeable Groups in Enterprise Corpora

Shangsong Liang s.liang@uva.nl

Maarten de Rijke derijke@uva.nl

ISLA, University of Amsterdam, Amsterdam, The Netherlands

ABSTRACT
The task of finding groups is a natural extension of search tasks aimed at retrieving individual entities. We introduce a group finding task: given a query topic, find knowledgeable groups that have expertise on that topic. We present four general strategies to this task. The models are formalized using generative language models. Two of the models aggregate expertise scores of the experts in the same group for the task, one locates documents associated with experts in the group and then determines how closely the documents are associated with the topic, whilst the remaining model directly estimates the degree to which a group is a knowledgeable group for a given topic. We construct a test collections based on the TREC 2005 and 2006 Enterprise collections. We find significant differences between different ways of estimating the association between a topic and a group. Experiments show that our knowledgeable group finding models achieve high absolute scores.
Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing
Keywords
Group finding, expertise retrieval, language modeling
1. INTRODUCTION
A major challenge within any organization is managing the expertise within the organization such that groups with expertise in a particular area can be identified [2]. Rather than finding knowledgeable individuals, sometimes locating a group with appropriate skills and knowledge in an organization is of great importance to the success of a project being undertaken [6].
Traditional approaches to finding knowledge, whether in individuals or in groups within an organization, often include two main steps. For a given task the expertise of the experts in each group is recorded and then the expertise of a group is computed by aggregating the expertise values of all group members. Both steps are traditionally done manually and require considerable effort. In
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright is held by the owner/author(s). Publication rights licensed to ACM. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

addition, this approach is usually restricted to a fixed set of expertise areas [7]. To reduce the effort of recording and evaluating the expertise of people from their representations, many automatic approaches have been proposed. There has been an increasing move to automatically extract such representations for evaluating expertise from heterogeneous document collections [2]. To compute the expertise values of a group, in principle, many aggregation operators are available, e.g., sum or average. These can be employed to combine individual experts' expertise values. There are at least 90 families of aggregation operators [11], which have been put to use in a range of applications. But the problem of how to aggregate expertise values of experts within a group so that the expertise scores of different groups can be easily compared and ranked is unknown.
We treat the problem of finding a knowledgeable group differently. Four distinct models are proposed. Our models are based on probabilistic language modeling techniques. Each model ranks groups according to the probability of the group being a knowledgeable group for a query topic, but the models differ in how this is performed. Three types of variable play a key role in our estimations: groups (G), queries (Q) and documents (D). The order in which we estimate these is reflected in our naming conventions. E.g., the model named GDQ proceeds by first collecting evidence of whether a group is knowledgeable on the topic via the experts in the group (G), and then determining whether each expert in the group has expertise on the topic via documents (D), and finally whether a document is talking about the given query (Q) topic.
2. RELATED WORK
Significant research effort has been invested in locating a group of individuals in an organization. Yang et al. [10] try to find a group of attendees familiar with a given activity initiator, and ensure each attendee in the group to have tight social relations with most of the members in the group. Sozio and Gionis [9] study a querydependent variant of the community-detection problem: given a graph, and a set of nodes in the graph as their input query, find a subgraph that contains the input query nodes and is densely connected. Lappas et al. [6] study the problem of given a task, a pool of individuals  with different skills and a social network that captures the compatibility among them, finding a subset of , who together have the skills to complete a task with minimal communication costs. Kargar and An [5] design communication cost functions for two types of communication structures.
The problem we deal with is different. We introduce a new group finding task: given a topic query, determine a list of knowledgeable groups within which the experts have expertise on the topic. Our group finding problem includes two sub-problems. The first is to answer questions such as "Which groups are knowledgeable groups on topic T ?" whilst the second is to answer the question "What does group G know?" We focus on the first sub-problem.

1005

3. MODELING GROUP FINDING
In our modeling of the knowledgeable group finding task, groups, documents and queries are considered in different orders. Groups are ranked according to how likely they have expertise on the given query according to the estimated language model.
Problem definition and context. We address the following problem: what is the probability of a group g being a knowledgeable group given query topic q? We have to estimate the probability of a group g given a query q and then rank groups according to this probability. The top k groups will be considered to be the most knowledgeable groups for the given query topic. Instead of computing this probability directly, we apply Bayes' Theorem, and obtain p(g|q) = p(q|g)p(g)p(q)-1, where p(q) is the probability of a query and p(g) the probability of a group, both of which can be assumed to be uniform for a query and a group, considering that q is the same during retrieval and there is no group that is more likely to be relevant. Hence, ranking groups according p(g|q) boils down to ranking a query topic given a group: p(q|g). To determine p(g|q) or p(q|g) we consider experts, groups, documents and queries in different orders, so as to arrive at four distinct models.
Four group finding models. The first of four models for group finding is presented in some detail; because of lack of space, the others are presented much more concisely. We start with two types of aggregation model: the Group-Query-Document (GQD) model and the Group-Document-Query (GDQ) model. The order of the key terms in these names signifies the following: GQD means that the evidence of whether a group is knowledgeable on the topic is collected via the experts in the group (G), then how likely each expert in the group has expertise on each subtopics in the query (Q) topic is computed via the documents (D). GDQ denotes that the evidence of whether a group is knowledgeable is collected via the experts in the group (G), then via each document (D) the expertise of each expert in the group on the query (Q) topic is computed directly via the documents. We assume that experts in the same group g are conditionally independent given the group, such that:
p(g|q) = exg p(ex|q)as(ex,g),
where ex is an expert belonging to group g, p(ex|g) is the probability of how likely an expert ex belonging to a group g, and as(ex, g) is the association between an expert ex and the group g. Instead of computing p(ex|g) directly, we apply Bayes' Theorem, and obtain p(ex|q) = p(q|ex)p(ex)p(q)-1, where p(q|ex) is the probability of a query given an expert, p(ex) is the probability of an expert, and p(q) is the probability of the query. As we assume that each expert is equally important, p(ex) is assumed to be constant. Additionally, for each query topic, p(q) is the same, hence, p(ex|q) is proportional to p(q|ex). So, p(g|q) becomes
p(g|q) ra=nk exg p(q|ex)as(ex,g).

The GQD Model. To obtain p(q|ex), we assume that each term t in query q is conditionally independent given expert ex, such that:
p(q|ex) = tq p(t|ex)n(t,q),
where p(t|ex) is the probability of a term given an expert and n(t, q) is the number of occurrences of term t in query q. Combined, we can rewrite p(g|q) as follows.

p(g|q) ra=nk exg

as(ex,g)

tq p(t|ex)n(t,q)

.

To obtain p(t|ex), we take the sum over documents d in the collection. This can be expressed as p(t|ex) = d p(t|d)p(d|ex),

where p(t|d) is the probability of term t given document d, and p(d|ex) is the probability of d given expert ex. Now we can obtain the probability of a group given a query, i.e., our GQD model:

p(g|q) ra=nk exg

tq

d p(t|d)p(d|ex) n(t,q) as(ex,g)

The GDQ Model. We can compute the probability of a query q given an expert ex in a different way. By taking the sum over all documents d, p(q|ex) can be obtained. Formally, this can be expressed as: p(q|ex) = d p(q|d)p(d|ex), where p(q|d) and p(d|ex) are the probability of query q given document d and of document d given query q, respectively. Based on this, we obtain
our second aggregation model, i.e., our GDQ model:

p(g|q) ra=nk exg

d

as(ex,g)

tq p(t|d)n(t,q) p(d|ex)

.

The DGQ model. Next we consider a document model. Instead of aggregating expertise scores of all the experts within a group as in our aggregation models, as the key terms DGQ in this model's name suggests, the probability g(g|q) can be computed directly via the documents (D). For each we compute how likely the group (G) is associated with it, and how likely it is talking about the given query (Q) topic, such that: p(g|q) = d p(g|d)p(d|q), where p(g|d) and p(d|q) are the probability of group g given document d and the probability of d given query q, respectively. This, then, is how p(g|q) can be represented, i.e., our DGQ model:

p(g|q) ra=nk d

exg p(d|ex)as(ex,g)

tq p(t|d)n(t,q) .

The QDG model. Finally, we present a query model for group finding. We use "query model" not in the sense of building rich representations of a query but to indicate that our estimations of a group finding model start with the query. As the name QDG indicates, it first considers how likely a group knows about a query (Q) topic. QDG computes this via documents (D) and then determines how likely each expert in the group (G) is associated with each document. We collect evidence of how knowledgeable group g is via all documents in the collection and obtain p(t|g) = d p(t|d)p(d|g), where p(d|g) is the probability of document d given group g. The final version of p(g|q) can then be represented as:

p(g|q) ra=nk tq

n(t,q)

d p(t|d) exg p(d|ex)as(ex,g)

.

And this is our QDG model. It first computes the probability of how likely a group is talking about a query topic; it collects evidence of how knowledgeable the group is for a given query via all documents in the collection. For each expert within a group, we determine how likely the expert is associated with the documents.

4. ASSOCIATIONS AND SMOOTHING

Expert-document associations. For all models described in the previous section, we need to be able to estimate the probability of an expert ex in group g being associated with document d. In recent years, this problem has attracted considerable attention [2]. Following Balog et al. [1], to define this probability, we assume that associations a(d, ex) between experts ex and documents d have been calculated and define

a(d, ex)

p(d|ex) =

,

(1)

d D a(d , ex)

where D is the set of documents in the collection, and a(d, ex) is simply defined as to be 1 if if the full name or email address of expert ex (exactly) appears in document d, otherwise a(d, ex) = 0.

1006

Group-expert associations. For all of the group finding models described in the previous section, we also need to be able to estimate the strength of the association between expert ex and group g to which the expert belongs. We define the following group expert association as(ex, g) = |g|-1, where |g| is the total number of experts within the group to which they belong.
Smoothing strategies. In our four models, the term p(g|q) may contain zero probabilities due to data sparsity. E.g., in our aggregation models, GQD and GDQ, p(g|q) will contain zero probabilities if there exist experts who have no expertise on the given query. Hence, we have to infer a group model g, such that the probability of a group given a query model is p(g|q). We employ JelinekMercer smoothing [4] to estimate p(g|q); we consider two types.
To facilitate comparisons and for the sake of uniformity, instead of estimating p(g|q) directly, we can easily infer a document model d such that the probability of term t given a document d model is p(t|d), and infer an expert model ex such that the probability of a document d given an expert ex is p(d|ex). The document model, then, is a linear interpolation of the background model p(t) and the smoothed estimate: p(t|d) = (1 - )p(t|d) + p(t), where  is a smoothing parameter (0 <  < 1). The expert model is a linear interpolation of the background model p(d) and the smoothed estimate: p(d|ex) = (1-)p(d|ex)+p(d), where  is a smoothing parameter (0 <  < 1). Let (, t, d) be short for p(t|d) = (1 - )p(t|d) + p(t), and (, d, ex) be short for p(d|ex) = (1 - )p(d|ex) + p(d). Then, the group finding model GQD can be smoothed and estimated as

p(g|q) ra=nk exg

tq

d(, t, d) · (, d, ex)

n(t,q)

as
,

where as abbreviates as(ex, g). The other group finding models, GDQ, DGQ, and QDG, can be smoothed and estimated in an analogous manner. As to the GDQ model:

p(g|q) ra=nk exg

d

as
tq(, t, d)n(t,q) (, d, ex) ,

with as as before. For the DGQ model we have

p(g|q) ra=nk d exg(, d, ex)as(ex,g) tq(, t, d)n(t,q) ,

and the QDG model can be smoothed and estimated as

p(g|q) ra=nk tq

n(t,q)

d(, t, d) exg(, d, ex)as(ex,g)

.

For the GQD model, we also consider a second type of smoothing strategy with one parameter: p(g|q) ra=nk exg{ tq{(1-) d p(t|d)p(d|ex) + p(t)}n(t,q)}as .

5. EXPERIMENTAL SETUP
Next, we describe the experimental setup for testing our knowledgeable group finding methods. We specify our research questions, describe our data set, and detail our ground truth.
Research questions. We consider the following questions. How do different group finding models perform compared against each other, under different ground truths or different evaluation metrics? Are some queries harder than others for the same model? Finally: Are the models different from each other?
Experimental collection. For evaluation purposes we use data made available for the expert finding task at the TREC 2005 and 2006 Enterprise tracks [3, 8]. The document collection used is a crawl of the World Wide Web Consortium (W3C; 330K documents, 5.7GB). The expert finding qrels for the two years differ: in 2005,

50 working group titles were the test topics for the expert finding task, resulting in 1509 expert-group pairs, with 2 to 391 experts in the same group and approximately 30 experts per group on average; names and e-mail addresses of 1092 expert candidates (W3C members) are given as part of the collection. For the TREC 2006 expert finding task, 55 queries queries were created, but only 49 are provided with expert finding ground truth.

Three types of ground truth. We use the qrels of the TREC 2006

expert finding task and propose three types of ground truth: binary,

graded and number.

Binary A working group g is considered relevant for topic q if

there is at least one expert ex who is a member of g (according to

the TREC 2005 expert finding qrels) and has expertise on the topic

q (according to the TREC 2006 expert finding qrels).

Grade A slightly more sophisticated definition of group rele-

vance uses grades: the level of relevance of group g for query q

is defined based on the fraction of the experts in the group. We

distinguish between |L| different levels of relevance, i.e., {0, 1, 2,

. . . , |L - 1|}. The relevance grade of group g for topic q is defined

as follows:

let f (g, q)

=

|{exg:rel(ex,q)=1}| |g|

,

where

{ex



g

:

rel(ex, q) = 1} is the set of experts in g with expertise on topic q

according to the TREC 2006 expert finding qrels and |g| is the total

number

of

experts

in

group

g.

If

1 |L|

·l



f (g, q)

<

1 |L|

· (l + 1),

the grade level for this group is l. In this paper, we set |L| = 10.

Number Here, the level of relevance of group g for query q is

defined based on the number of experts in the group. For instance,

if there are 15 experts who have expertise on the given query topic,

then the level of the relevance for this group is 15. The level of

relevance ranges from 0 to 30 with a majority smaller than 4.

Runs. We run our experiments with all documents in the collection for our four group finding models. We perform a grid search to find optimal settings of the smoothing parameters (with 0.1 increments). We generate runs on the full collection and on subsets defined by taking the top n documents returned by a standard document retrieval run when using the topic as query. Evaluation measures used are MAP, precision@5, 10, nDCG and nDCG@5, 10 against our three types of ground truth. Evaluation was done using the trec_eval program (available from http://trec.nist.gov).

6. RESULTS
We start by comparing the results of the optimized models with two smoothing parameters. Next, we present the results of query differences. Finally, we test whether the models smoothed by two or one parameters are statistically significantly different.
Model comparison. How do our knowledgeable group finding models perform compared to each other? For each specific performance evaluation metric, we compare the models using optimal smoothing parameters. We use two parameters  and  to smooth the proposed four knowledgeable group finding models, i.e., GQD, GDQ, DGQ and QDG.
Table 1 lists the scores for the various metrics. Clearly, DGQ outperforms the other models on all metrics using the binary and graded ground truth, but GDQ outperforms DGQ on all metrics using the number ground truth. QDG model is the worst performing model for all metrics and against all types of ground truth. The table also shows that GQD, GDQ and DGQ have a similar performance for all metrics against all types of ground truth. (The MAP and p@N scores against the number ground truth are the same as those against the binary ground truth, and are therefore omitted.)
Query differences. Our aim here is to find out whether some queries are harder than others for the same model against the metrics. We turn to a topic-level analysis of the MAP performance for

1007

Table 1: Evaluation results for all optimal models with two smoothing parameters, using the binary, graded and number ground truths. For each metric, we report the evaluation results, followed by the optimal smoothing parameters  and .

Ground

NDCG@

p@

truth Model NDCG   5   10   MAP   5   10  

binary

GQD .8861 .1 .2 .8165 .1 .2 .7850 .1 .3 .7127 .1 .7 .8041 .1 .9 .7306 .1 .3 GDQ .9009 .1 .9 .8291 .3 .5 .7936 .2 .5 .7552 .1 .9 .8122 .1 .6 .7408 .2 .8 DGQ .9133 .1 .9 .8680 .1 .9 .8420 .1 .9 .7772 .1 .9 .8571 .1 .9 .7918 .1 .9 QDG .7623 .1 .2 .5604 .1 .1 .5568 .1 .2 .4882 .1 .4 .5592 .1 .1 .5265 .1 .2

graded GQD .8245 .2 .3 .7237 .2 .3 .7595 .1 .2 .7403 .1 .3 .6367 .1 .1 .5224 .1 .2 GDQ .8457 .1 .4 .7675 .2 .3 .7945 .1 .4 .7673 .1 .4 .6776 .1 .3 .5510 .1 .4 DGQ .8631 .2 .9 .7991 .5 .9 .8160 .7 .8 .8092 .6 .8 .7102 .5 .6 .5531 .7 .9 QDG .3916 .1 .8 .1012 .1 .1 .1282 .1 .2 .2182 .1 .4 .1714 .1 .1 .1673 .1 .1

number GQD .7964 .1 .9 .6210 .1 .8 .6730 .6 .8 GDQ .8160 .1 .9 .6496 .1 .9 .6905 .1 .9 DGQ .7907 .1 .9 .6062 .1 .9 .6758 .1 .9 QDG .6222 .1 .2 .3328 .1 .1 .4258 .1 .1

1

1

0.5

0.5

AP difference AP difference

0

0

-0.5

-0.5

-1 topics
(a) GQD, binary

-1 topics
(b) GQD, graded

Figure 1: Topic-level differences from the mean scores for GQD using the binary/number and graded ground truths.

each model against binary/number, graded ground truth. We plot the differences in performances (per topic) between the average AP score and the AP score per topic, sorted by performance difference. Fig. 1(a) shows the plots for GQD when using the binary/number ground truth, whilst Fig. 1(b), shows the plots for GQD when using the graded ground truth. From Fig. 1(a), it is clear that the performance in MAP does not differ dramatically from the mean when using the binary or number ground truths. In comparison, for some topics the performance is dramatically worse than the mean for GQD when using the graded ground truth. (We observed similar phenomena for the other three models.)
Statistical significance. Finally, we determine whether the observed differences between our group finding approaches with two smoothing parameters strategy are statistically significant. We use a two-tailed paired t-test between two models on NDCG and MAP data and test for significance differences at the 0.95 confidence level. Table 2 indicates that when using NDCG as a metric the differences between GQD and GDQ against graded ground truth are not significant. This is also true for the differences between GQD and DGQ, and between GDQ and DGQ against the graded and number ground truth. When using MAP, the differences between all models are statistically significant except for those between GQD and GDQ. We also test differences between the optimal GQD model with smoothing with two parameters and with a single smoothing parameter based on NDCG and MAP against three types of ground truth; there are no statistically significant differences at the 0.95 confidence level except based on NDCG against binary ground truth where the p value is 0.0270. Hence, we cannot really distingush between GQD with smoothing with two parameters and GQD with smoothing with one parameter in our experiments.

Table 2: Two-tailed paired t-test between different models on NDCG and MAP metrics.

metric

ground truth

GQD vs.
GDQ

GQD vs.
DGQ

GQD vs.
QDG

GDQ vs.
DGQ

GDQ vs.
QDG

DGQ vs.
QDG

NDCG binary graded number

.0107 .2349 .0086

.0005 .0616 .4107

.0000 .0000 .0000

.0496 .2282 .0570

.0000 .0000 .0000

.0000 .0000 .0000

MAP bin./numb. .0013 graded .2647

.0000 .0140

.0000 .0000

.0198 .0221

.0000 .0000

.0000 .0000

7. CONCLUSIONS
We have introduced a group finding task. We proposed four models, GQD, GDQ, DGQ and QDG. We also constructed an experimental collection by using the TREC 2005 and 2006 Enterprise collections. We introduced three kinds of ground truth and evaluated our models along many dimensions. Directly collecting expertise evidence from documents is the most effective way to find knowledgeable groups when using the binary or graded ground truths, and aggregating the expertise of each experts in the same group can also be a good way to find the groups. Our models are not very sensitive to changes of the parameters when using a two parameter smoothing strategy. We found statistically significant differences between the models when using MAP scores based on multiple types of ground truth.
Acknowledgements. We thank our reviewers for their helpful comments. This research was supported by the European Community's Seventh Framework Programme (FP7/2007-2013) under agreements 258191 (PROMISE) and 288024 (LiMoSINe), the Netherlands Organisation for Scientific Research (NWO) under nrs 640.004.802, 727.011.005, 612.001.116, HOR-11-10, the Center for Creation, Content and Technology (CCCT), the BILAND project funded by the CLARIN-nl program, the Dutch national program COMMIT, the ESF Research Network Program ELIAS, the Elite Network Shifts project funded by the Royal Dutch Academy of Sciences (KNAW), and the Netherlands eScience Center under number 027.012.105.
8. REFERENCES
[1] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert finding in enterprise corpora. In SIGIR'06, 2006.
[2] K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and L. Si. Expertise retrieval. Foundations and Trends in Information Retrieval, 6(2-3): 127­256, 2012.
[3] N. Craswell, A. P. de Vries, and I. Soboroff. Overview of the TREC 2005 enterprise track. In TREC'05, 2005.
[4] F. Jelinek and R. Mercer. Interpolated estimation of markov sourceparameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, 1980.
[5] M. Kargar and A. An. Discovering top-k teams of experts with/without a leader in social networks. In CIKM'11, 2011.
[6] T. Lappas, K. Liu, and E. Terzi. Finding a team of experts in social networks. In KDD'09, pages 467­475, 2009.
[7] G. A. Pryor, J. W. Myles, D. R. R. Williams, and J. K. Anand. Team management of the elderly patient with hip fracture. The Lancet, pages 401­403, 1988.
[8] I. Soboroff, A. P. de Vries, and N. Craswell. Overview of the TREC 2006 enterprise track. In TREC'06, 2006.
[9] M. Sozio and A. Gionis. The community-search problem and how to plan a successful cocktail party. In KDD'10, pages 939­948, 2010.
[10] D.-N. Yang, Y.-L. Chen, W.-C. Lee, and M.-S. Chen. On social-temporal group query with acquaintance constraint. In VLDB'11, 2011.
[11] S.-M. Zhou, F. Chiclana, R. I. John, and J. M. Garibaldi. Alpha-level aggregation. IEEE Trans. Knowl. and Data Eng., 23:1455­1468, 2011.

1008

Shame to be Sham: Addressing Content-Based Grey Hat Search Engine Optimization

Fiana Raiber
Technion -- Israel Institute of Technology
fiana@tx.technion.ac.il

Kevyn Collins-Thompson
Microsoft Research
1 Microsoft Way
Redmond, WA, USA 98052
kevynct@microsoft.com

Oren Kurland
Technion -- Israel Institute of Technology
kurland@ie.technion.ac.il

ABSTRACT
We present an initial study identifying a form of contentbased grey hat search engine optimization, in which a Web page contains both potentially relevant content and manipulated content: we call such pages sham documents, because they lie in the grey area between `ham' (clearly normal) and `spam' (clearly fake). Sham documents are often ranked artificially high in response to certain queries, but also may contain some useful information and cannot be considered as absolute spam. We report a novel annotation effort performed with the ClueWeb09 benchmark where pages were labeled as being spam, sham, or legitimate content. Significant inter-annotator agreement rates support the claim that there are sham documents that are highly ranked by a very effective retrieval approach, yet are not spam. We also present an initial study of predictors that may indicate whether a query is the target of shamming. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
Keywords: sham, spam, search engine optimization
1. INTRODUCTION
The Web constitutes an adversarial retrieval setting: in Web search, content creators try to have their pages ranked high in response to queries, to increase visibility and traffic to their sites. To that end, they often employ various search engine optimization (SEO) techniques [11]. Search engines, on the other hand, aim to rank pages in response to queries by the overall relevance of their content and strive to minimize the effect of potential adversarial methods.
A distinction is made between white hat SEO, which typically does not involve deceptive techniques and obeys search engine anti-spam guidelines, and black hat SEO, which uses deceptive content, links and other manipulation that ultimately may result in the site using them being penalized or banned by search engines. The black hat SEO area has attracted much research attention; e.g., there is a large body of work on spam classification [11, 5, 8, 19], where a page is
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

deemed spam if it bears no useful content. Grey hat SEO, on the other hand, is an intermediate form of search ranking manipulation that yields neither a spam page, nor a (very) high-quality representative of relevant content for queries seeking information on that topic. Grey hat SEO has attracted very little research attention in the literature [13]. There is no rigorous definition of what makes for a grey hat SEO attempt: because grey hat documents can often contain relevant information as well as SEO content, identifying such documents is often a subjective decision.
In this paper we address the problem of identifying a specific, highly important type of grey hat SEO: pages whose content has been manipulated so as to have the page ranked higher than it should with respect to some queries. However, these pages are not spam in the absolute sense, in that they can still satisfy information needs. We use the term sham to refer to such pages1. For example, excessive use of query variants can turn an otherwise useful page to sham but not to spam, along with more obvious manipulations like keyword stuffing [11]. An example of a sham passage is shown in Figure 1.
By definition, sham pages can have considerable negative impact on the effectiveness of relevance ranking. For example, the frequency of query terms in them can be a misleading signal as to the actual information needs these pages can satisfy and the extent to which they can satisfy them. Our motivation for focusing on content-based shamming is threefold. First, relevance is typically determined by the ability to satisfy an information need using the page content. Second, content-based features are highly important when applying learning to rank methods for Web search [16]. Third, it is relatively easier to manipulate the content of pages compared to other derived relevance signals such as clickthrough rate, hyperlink structure, etc.
This paper provides two main contributions toward an initial understanding of the nature of sham. First, we describe the results of a sham annotation pilot effort applied to the publicly available ClueWeb09 benchmar2 [7]. Documents that were highly ranked in response to queries by an effective learning-to-rank (LTR) method were classified into one of three categories: spam, sham, or legitimate. We found significant inter-annotator agreement for the task of labeling sham pages. Furthermore, many top-ranked pages were agreed to be sham: some were relevant to the queries, while most were not. Second, we present an initial study of predic-
1Appropriately, in English the word `sham' can denote something fake or not genuine. 2www.lemurproject.org

1013

Your good accident lawyer will deal with whiplash injury claims, spinal injury claims, personal injury claims, and back injury claims. It is the job of the car accident lawyer to negotiate with the insurance company to get you a high auto accident insurance settlement as well as perform other duties such as...
Figure 1: Example sham passage for the query [auto injury attorney]. In sham documents, content is manipulated, e.g., with excessive related queries (underlined), while retaining some degree of relevance.

tors for when a query is likely to be the target of shamming efforts. Our results show that query-specific features outperform an approach that uses spam classification for this task as well as content-based query-independent document quality measures [2].
2. RELATED WORK
There is much work on spam classification (e.g., [11, 14, 20, 8, 5, 19]) and on nullifying its effects on retrieval effectiveness [14, 9]. As already noted, the task of sham classification that we pursue here is different from spam classification by definition. Specifically, sham documents are manipulated for specific queries, and do bear useful content, while spam documents are absolutely useless. There has been some work on using query-dependent features to classify spam in its absolute query-independent sense [20]. We use query-dependent features for sham classification.
Using query-independent document quality measures is known to improve retrieval effectiveness [3, 2]. We note that sham documents are not necessarily of low quality. Furthermore, we show that the methods we study for the task of predicting whether a query is the target of shamming outperform methods that use content-based document quality measures which are very effective for search [2].
The realization that "not all content that complicates ranking is spam" was echoed in previous work [13]. However, we are not aware of any previous studies that study the difficulty of labeling Web pages as grey hat SEO, or using preand post-retrieval features to predict how likely a query is to be the target of such SEO attempts.
3. SHAM ANNOTATION PROCESS
As a first step in examining the shamming phenomenon, we conducted an annotation effort, wherein a total of 300 documents, 30 queries × 10 documents per query, were evaluated. (Further details regarding the set of documents and the queries are provided in Section 5.) The documents were evenly divided between five Ph.D. and M.Sc. students in the Information Retrieval field. Each document was labeled by two annotators as being either spam, sham, or legitimate page. In case of a disagreement between the first two annotators, a third annotator was asked to label the document. Thus, for each document we have either two or three labels.
Annotators were instructed to label a document as sham if any of the following conditions was met. (i) The document contained text that did not seem to satisfy any information need, including information needs satisfied by other parts of the same document (ii) The page contained text that appeared to be related to information needs satisfied by this page, but the text contained many artificial, repeated, or otherwise unnecessary extra words, phrases or sentences added to the page solely for the purpose of promoting the page in the result lists; and (iii) The page contained sections of content that were copied from other pages (e.g.,

from a Wikipedia page) for presumably the sole purpose of promoting the page. A document was labeled as spam if it contained no useful information at all for satisfying any information need.
The annotators were asked to base their decision only upon the content of the document, and to ignore non-content elements such as incoming and outgoing links, page URL, site domain, or ads and sponsored links that might appear on the page. The documents were presented to the annotators in a random query-independent order and the query itself was not presented.

4. PREDICTORS FOR QUERY SHAMMING
We next examine the problem of predicting which queries are more susceptible to sham, as measured, e.g., by the fraction of sham documents in the top 10 results. To this end, we study the use of various predictors which were developed to estimate retrieval effectiveness [4]. As we show below, some of these are negatively correlate with the percentage of sham, typically because most sham documents are nonrelevant, while others have a positive correlation.
Pre-retrieval predictors are based on properties of the query and the corpus. Post-retrieval predictors utilize, in addition, information about the result list of documents, Dres, which was returned in response to the query [4]. Details regarding the retrieval model used in our experiments are provided in Section 5. In what follows we present the different classes of predictors analyzed for our prediction task.

Pre-retrieval predictors. The SumSCQ predictor mea-
sures the similarity between the query and the collection using the sum of the TF.IDF values of the query terms [21]. SumVar [21] is the sum over query terms of the variance of the TF.IDF values of the term in documents in the corpus in which it appears. The SumIDF predictor is the sum of the IDF values of the query terms [10]. Another pre-retrieval predictor that we use is QueryLength. Since sham documents are assumed to negatively affect relevance ranking, a lower pre-retrieval predicted value is assumed to be correlated with higher percentage of sham.

Post-retrieval predictors. The Clarity [10] prediction value

is the KL divergence between a relevance language model

[15], R, induced from the documents in the result list Dres

and the (unsmoothed) language model of the collection. For-

mally, let p(w|d) and s(d) be the probability assigned to

term w by an unsmoothed language model induced from

document d, and the score assigned to d by the search al-

gorithm that was used to create Dres, respectively. Then,
def
the probability assigned to w by R is defined as p(w|R) =

dDres p(w|d)

s(d) di Dres

s(di) .

The

assumption

is

that

sham

documents for a given query are similar to one another; for

example, due to the repeated terms that they contain. Thus,

1014

these documents presumably form a cluster whose language

model is focused with respect to that of the corpus.

We also consider a variant of WIG [22] which is sim-

ply the average retrieval score in Dres:

1 |Dres |

dDres s(d).

The premise is that sham documents are likely to be as-

signed high retrieval scores, as by definition they have been

manipulated to be promoted in result lists.

Query-independent document-quality measures. The en-
tropy of the term distribution in a document, Entropy, the stopwords to non-stopwords ratio in a document, SW1, and the percentage of stopwords that appear in the document, SW2, were shown to be highly effective content-based query-independent quality measures for Web search [2], and for predicting query performance over the Web [17]. We used INQUERY's stopword list [1]. The document PageRank score [3] is another measure widely used for improving retrieval effectiveness. We use these measures for our sham prediction task. The prediction value for the result list Dres is the sum of the per-document values assigned by a measure. While Entropy, SW1 and SW2 are based on the content of a document, PageRank is based on the hyperlink structure. The content-based measures quantify the presumed textual content breadth in the document. We hypothesize that the content breadth in sham documents is low, as there is focus on a small subset of terms that are the target of shamming. Conversely, since by definition sham documents can still satisfy information needs, we assume that the PageRank score of such documents is likely to be high.

Spam-based predictor. To study the connection between
spam and sham documents, we use the recently proposed NS predictor [17]. Documents in Dres that are classified as spam by Waterloo's spam classifier are treated as "nonrelevant"; the rest are treated as "relevant". Then, the mean average precision (MAP) at cutoff |Dres| is computed based on these artificially created "relevant" and "non-relevant" labels. This score is then multiplied by the total number of "relevant" (non-spam) documents.

Query-log-based predictors. For each query, we derived
the following features from the query logs of a commercial search engine. RawImpressions: the total frequency the query (with stopwords) was used to search; ClicksOnResult: percentage of clicks on the search engine results page (SERP) that were on a search result vs. other parts of the page, such as query suggestions; ClicksForPaging: the percentage of clicks on the SERP used to get to the next or previous page of results. For all features we used one month of traffic from January 2013 originating from the U.S. locale.

5. EVALUATION
Our experiments were conducted using the ClueWeb09 Category B collection, which contains about 50 million English Web pages. We used queries 1-150 from TREC 20092011. The Indri toolkit (www.lemurproject.org/indri) was used for experiments. We applied Krovetz stemming upon queries and documents, and removed stopwords on the INQUERY list [1] only from queries.
To create a highly effective result list, Dres, we did the following. First, for each query q, the documents in the collection were ranked using the negative cross entropy between

the unsmoothed unigram language model induced from q, and the Dirichlet-smoothed unigram language model induced from a document (with the smoothing parameter  set to 1000). Then, following previous work [9], documents assigned by Waterloo's spam classifier with a score below 50 were removed top to bottom from that ranking until 100 (presumably non-spam) documents were accumulated. These documents were then re-ranked using SVMrank [12] applied with 130 features. Ten-fold cross validation was performed.
We used a subset of the features used by Microsoft's learning to rank datasets3. Our features do not include the Boolean Model, Vector Space Model, LMIR.ABS, and the number of outgoing links. The SiteRank score, the two quality measures, QualityScore and QualityScore2, and the clickbased features were also not included as these information types are not available for the ClueWeb09 collection. Three additional features that we used, and which were found to be highly effective for Web retrieval [2], are Entropy, SW1, and SW2 which were described in Section 3. As is the case with the features mentioned thus far, these three features were also computed separately for all the text in the document, its anchor text, URL, body, and title. Waterloo's spam classifier score [9] is another feature that was used. The BM25 score was computed with k1=1 and b=0.5, following experiments with b  {0.5, 0.75} and k1  {1, 1.2, 1.5, 1.7}. LMIR.DIR and LMIR.JM, the language-model-based features, were computed with =1000 and =0.9, respectively. Lastly, 30 queries were randomly sampled, and the top 10 non-Wikipedia documents were used to form Dres. We assume that Wikipedia pages are not sham.
We used two approaches to aggregate the labels assigned by the annotators to the documents. According to the first approach, a document is considered sham if it was labeled as such by at least one of the annotators. This `weak' label definition is henceforth referred to as AtLeastOne. The second approach, denoted AtLeastTwo, is more strict: a document is considered sham if it was marked as sham by at least two annotators.
To evaluate the quality of the various predictors, we report Pearson's correlation [4] between the values assigned by a predictor to each of the tested queries, and the percentage of sham documents computed for a query, according to the AtLeastOne and AtLeastTwo approaches described above.
The number of terms used to construct the relevance language model, which is used by the Clarity predictor, is set to 50. Given that documents assigned with a score below 50 by Waterloo's spam classifier are removed in the process of creating Dres, to determine the non-spam documents that are used by the NS predictor we use a threshold. The threshold is selected from {60, 70, 80, 90} so as to optimize the prediction quality of NS. Documents assigned with a score above that threshold are considered as non-spam ("relevant").
Annotation Results. The inter-annotator agreement on the label pairs from the two main judges for each document was 0.69, computed using the free-marginal multi-rater kappa measure [18]. Overall, 79% of the 300 label pairs were in agreement. The average percentage of sham documents per query was 30% based on AtLeastOne labels, and 21% based on AtLeastTwo labels.
Query shamming predictor analysis. The correlations of the predictors described in Section 4 with the per-
3www.research.microsoft.com/en-us/projects/mslr

1015

Predictor Type
Pre-retrieval
Post-retrieval QueryIndependent Spam Query Log

Predictor Name
SumSCQ SumVar SumIDF QueryLength
Clarity WIG
Entropy SW1 SW2 PageRank
NS
RawImpressions ClicksOnResult ClicksForPaging

Sham Label Defn. AtLeastOne AtLeastTwo

-0.500 -0.398 -0.424 -0.553

-0.401 -0.337 -0.363 -0.432

+0.276 +0.361

+0.235 +0.351

-0.324 -0.001 -0.024 +0.226

-0.242 -0.022 +0.046 +0.129

+0.099

+0.183

+0.178 -0.216 +0.277

+0.231 -0.320 +0.387

Table 1: Pearson's correlation (r) of various predictor variables (Section 4) with % sham documents in the top 10 results, for weak (AtLeastOne) and strict (AtLeastTwo) sham label definitions.

centage of sham pages retrieved for a query are presented in Table 1. We can see that the pre-retrieval predictors are negatively correlated with shamming. As these predictors were designed to predict the retrieval effectiveness of using the query for search, we can conclude that sham documents are prevalent in queries whose performance is predicted to be low. Indeed, we found that about 75% of sham documents are non-relevant, for both AtLeastOne and AtLeastTwo.
QueryLength is negatively correlated with the percentage of sham documents, for both weak (AtLeastOne) and strict (AtLeastTwo) sham label definitions. That is, a shorter query has a higher chance of being the target of shamming. This might be attributed to the fact that short queries are more ambiguous, and as a result documents returned in response to these queries might be promoting the page with respect to different information needs, not necessarily the information need that the current query expresses.
The post-retrieval predictors, Clarity and WIG, are positively correlated with the percentage of sham documents, as hypothesized in Section 4. However, the prediction quality is lower than that for the pre-retrieval predictors.
We can also see that the correlation between the queryindependent document-quality measures and shamming is relatively low. Furthermore, the NS predictor is not correlated with sham. These findings suggest that sham documents might be of either high or low quality, and that there is no evident connection between sham and spam documents.
For query log features, the positive correlation of RawImpressions with the level of sham is consistent with the fact that frequent queries are more susceptible to shamming. ClicksForPaging is positively correlated with the percentage of sham, while the ClicksOnResult feature is negatively correlated. These findings resonate with the fact that shamming degrades retrieval performance.
Finally, for the task of predicting sham percentage for a given query at low (< 0.3), medium ( [0.3, 0.6)), or high ( 0.6) levels, we performed ordinal regression [6] using all the predictors with default regression settings. The mean absolute error (MAE), the absolute difference between the predicted class ordinal and the true ordinal, averaged over 100 randomized trials with a 2:1 train/test split, was 0.532. For comparison, an oracle run using one rater's sham labels to predict sham levels derived from other raters' labels, had

MAE 0.37; while always guessing category `medium' had MAE 0.67. These initial results show that different predictors can be effectively integrated for predicting per-query sham levels.
6. CONCLUSION
We have provided an initial study on the identification of sham documents: a form of grey-hat SEO using contentbased feature manipulation, along with an analysis of features associated with low- or high-sham queries. Even search engines that use highly effective ranking methods still suffer from sham documents in the top-most rankings that adversely affect retrieval quality. These findings call for a principled treatment of query-specific grey hat SEO.
Acknowledgments. We thank the reviewers for their comments. This work was supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center.
7. REFERENCES
[1] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. INQUERY and TREC-9. In Proc. of TREC, pages 551­562, 2000.
[2] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased ranking of web documents. In Proc. of WSDM, pages 95­104, 2011.
[3] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In Proc. of WWW, pages 107­117, 1998.
[4] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis lectures on information concepts, retrieval, and services. Morgan & Claypool, 2010.
[5] C. Castillo, D. Donato, L. Becchetti, P. Boldi, S. Leonardi, M. Santini, and S. Vigna. A reference collection for web spam. SIGIR Forum, 40(2):11­24, 2006.
[6] W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. J. Machine Learning Res., 6:1019­1041, 2005.
[7] C. L. A. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 Web track. In Proc. of TREC, 2009.
[8] G. V. Cormack. TREC 2007 spam track overview. In Proc. of TREC, 2007.
[9] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information Retrieval, 14(5):441­465, 2011.
[10] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proc. of SIGIR, pages 299­306, 2002.
[11] Z. Gy¨ongyi and H. Garcia-Molina. Web spam taxonomy. In Proc. of AIRWeb, pages 39­47, 2005.
[12] T. Joachims. Training linear SVMs in linear time. In Proc. of KDD, pages 217­226, 2006.
[13] T. Jones, R. S. Sankaranarayana, D. Hawking, and N. Craswell. Nullification test collections for web spam and SEO. In Proc. of AIRWeb, pages 53­60, 2009.
[14] V. Krishnan and R. Raj. Web spam detection with Anti-Trust rank. In Proc. of AIRWeb, pages 37­40, 2006.
[15] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proc. of SIGIR, pages 120­127, 2001.
[16] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3), 2009.
[17] F. Raiber and O. Kurland. Using document-quality measures to predict web-search effectiveness. In Proc. of ECIR, pages 134­145, 2013.
[18] J. J. Randolph. Online kappa calculator (2008). Retrieved February 16, 2013, http://justus.randolph.name/kappa.
[19] D. Sculley and G. Wachman. Relaxed online SVMs for spam filtering. In Proc. of SIGIR, pages 415­422, 2007.
[20] K. M. Svore, Q. Wu, C. Burges, and A. Raman. Improving web spam classification using rank-time features. In Proc. of AIRWeb, 2007.
[21] Y. Zhao, F. Scholer, and Y. Tsegay. Effective pre-retrieval query performance prediction using similarity and variability evidence. In Proc. of ECIR, pages 52­64, 2008.
[22] Y. Zhou and B. Croft. Query performance prediction in web search environments. In Proc. of SIGIR, pages 543­550, 2007.

1016

Bias-Variance Decomposition of IR Evaluation
Peng Zhang1, Dawei Song1,2, Jun Wang3, Yuexian Hou1
1Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, China 2The Computing Department, The Open University, United Kingdom
3Department of Computer Science, University College London, United Kingdom
{darcyzzj, dawei.song2010}@gmail.com, jun_wang@acm.org, yxhou@tju.edu.cn

ABSTRACT
It has been recognized that, when an information retrieval (IR) system achieves improvement in mean retrieval effectiveness (e.g. mean average precision (MAP)) over all the queries, the performance (e.g., average precision (AP)) of some individual queries could be hurt, resulting in retrieval instability. Some stability/robustness metrics have been proposed. However, they are often defined separately from the mean effectiveness metric. Consequently, there is a lack of a unified formulation of effectiveness, stability and overall retrieval quality (considering both). In this paper, we present a unified formulation based on the bias-variance decomposition. Correspondingly, a novel evaluation methodology is developed to evaluate the effectiveness and stability in an integrated manner. A case study applying the proposed methodology to evaluation of query language modeling illustrates the usefulness and analytical power of our approach.
Category and Subject Descriptors: H.3.3 [Information Search and Retrieval]
General Terms: Theory, Measurement, Performance
Keywords: Bias-Variance, Decomposition, Effectiveness, Stability, Robustness, Evaluation
1. INTRODUCTION
Recently, it has been noticed that when we are trying to improve the mean retrieval effectiveness (e.g., measured by MAP [3]) over all queries, the stability of performance across different individual queries could be hurt. For example, compared with a baseline (using the original query in the first round retrieval), query expansion based on pseudo relevance feedback can generally achieve better MAP, but it can hurt the performance for some individual queries [2].
In the literature, various stability (or robustness) measures, e.g., R-Loss [2], Robustness Index [2], and < Init [9] have been proposed. R - Loss computes the averaged net loss of relevant documents (due to query expansion failure) in the retrieved documents. <Init calculates the percent-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

age of queries for which the retrieval performance of a query model is worse than that of the original query model. The robustness index is defined as RI(Q) = (n+ - n-)/|Q| [2], where n+ is the number of queries for which the performance is improved, n- is the number of queries hurt, over the original model, and |Q| is the number of all queries.
These existing measures have some limitations, as shown by the example in Table 1. Let us consider model A as the original query model, as well as regard B and C as two query expansion models. The example shows that A is less effective (with a lower MAP), but more stable (with a lower < Init) than B. In this case, we can not judge which model (A or B) has the better overall performance (considering both effectiveness and stability). For comparison of B and C, both <Init and robustness index can not distinguish the stability between them. The MAPs of B and C are also the same. Intuitively, B would seem more stable due to the less variance of its AP values for different queries. However, the above stability metrics do not take into account such variance (denoted as V ar in Table 1). For example, one way of computing the variance of AP for model A can be [(0.3 - 0.2)2 + (0.1 - 0.2)2]/2 = 0.01, which calculates the derivation of AP from its MAP (i.e., 0.2). In Table 1, V ar distinguishes the models A, B and C: the smaller V ar can indicate the better retrieval stability.
Now let us change the AP values of C to 0.32 and 0.11 (for q1 and q2 respectively), then its MAP, < Init and Robustness Index become 0.215, 0 and 1 respectively, suggesting C is more stable but less effective than B. We are not sure which one (B or C) is overall better when considering both effectiveness and stability. This is mainly because the existing stability metrics are often defined separately from the effectiveness metric (MAP). There is a lack of a unified formulation of the effectiveness and stability to allow them to be looked at in an integrated manner.
In this paper, we present a formulation based on a fundamental concept in Statistics, namely the bias-variance decomposition, to tackle the problem. In a nutshell, we view the unsatisfactory overall performance as one total error, which can be decomposed into bias and variance of the AP values of different queries. The bias captures the expected difference of the APs from their upper bounds (the best AP obtained by model T in Table 1). The variance has been illustrated earlier. The detailed formulation will be given in the next section. Briefly speaking, the smaller bias or variance reflects the better retrieval effectiveness or stability, respectively. The total error (denoted as Bias2 +V ar) can reflect the overall quality of a model. As an illustration,

1021

Table 1: Examples of Bias-Variance (AP)

Model AP

A q1 q2 0.3 0.1

B q1 q2 0.6 0.08

C

q1

q2

0.65 0.03

T q1 q2 0.7 0.2

MAP

0.2

0.34

0.34

0.45

Robust Index

0

0

0

1

< Init

0

0.5

0.5

0

Bias
V ar Bias2 + V ar

0.25 0.01 0.0725

0.11 0.0646 0.0797

0.11 0.0961 0.182

0 0.0625 0.0625

in Table 1, Bias2 +V ar indicates that (1) the target model T has the best overall retrieval quality; (2) the model B is more desirable than C; (3) in comparison with the baseline A, both query expansion models B and C reduce the bias but enlarge the variance as well as the total error.
Recently, Wang and Zhu [7] studied the mean-variance analysis regarding the document relevance scores. The pertopic variance of AP (of each query) was investigated in [6]. In this paper, we explore the variance of AP (across queries) and the bias-variance decomposition.

2. BIAS-VARIANCE DECOMPOSITION OF
RETRIEVAL PERFORMANCE
In Statistics [1], bias and variance, which are measurements of estimation quality in different aspects, are decomposed from the expected squared error of the estimated values with respect to the target value. In this paper, we formulate the bias-variance decomposition in the IR evaluation scenario. Let us first assume there exists a target model T that can have an upper-bound performance for each query 1.

2.1 Bias-Variance of AP
We can let AP be a random variable over queries. Bias(AP) essentially calculates the average difference between the target AP (i.e. the AP of the target model T, denoted APT ) and the actual AP of a retrieval model over all different queries. Specifically,

Bias(AP) = E(APT - AP) = E(APT ) - E(AP) (1)

where the expectation E(·) is over a set of queries that are

assumed to be uniformly distributed. E(APT ) corresponds to the target MAP (i.e., the MAP of the target model T,

denoted MAPT ), and E(AP) corresponds to the MAP of

the retrieval model under evaluation.

It turns out that the smaller bias indicates the better mean

retrieval effectiveness over queries. In Table 1, the bias for

model

A

is

1 2

[(0.7

-

0.3)

+

(0.2

-

0.1)]

= 0.25.

According

to

Eq. 1, it can be equivalently calculated as 0.45-0.2 = 0.25,

where 0.45 is the target MAP and 0.2 is the MAP of A.

Similarly, the variance of the APs for different queries is

defined as:

V ar(AP) = E(AP - E(AP))2

(2)

The smaller V ar(AP) indicates the better retrieval stability of AP across queries.

1We will relax this constraint in Sections 2.4.

Table 2: Examples of Additional Bias-Variance ()

Model 

A q1 q2 0.4 0.1

B q1 q2 0.1 0.12

C

q1

q2

0.05 0.17 0

T q1 q2 00

Bias

0.25

0.11

0.11

0

V ar

0.0225

0.0001

0.0036

0

Bias2 + V ar 0.0850

0.0122

0.0157

0

We now add up the bias and variance, yielding:
Bias2(AP) + V ar(AP) = [E(APT ) - E(AP)]2 + E(AP - E(AP))2 (3) = E(AP - MAPT )2
It turns out E(AP - MAPT )2 is not exactly the expected squared error E(AP - APT )2. However, we will show later that E(AP - MAPT )2 is a simplified version of an expected squared error E(AP-1)2 in Section 2.3. This error formulation will help us understand the difference between different bias-variance decompositions (see Section 2.3).

2.2 Additional Bias-Variance
To clarify the above observation, let us first look at the decomposition of the expected squared error E(AP-APT )2. We need to define another random variable

 = APT - AP

(4)

As an illustration, for the model A in Table 2,  is 0.4 (0.70.3) and 0.1 (0.2-0.1), for q1 and q2 respectively.
Let T be the target value of , which is indeed 0, since for model T,  = APT - APT =0. We need T in the following analysis, since the target value is an important component
in the standard definition of bias. We define

Bias() = E() - T

(5)

where E() is the averaged  over all queries. Since T is 0, Bias() equals to E() = E(APT -AP), which is Bias(AP).
The variance based on , denoted V ar(), computes the variance of the difference between the AP (of the test model)
and the AP target across all queries. Formally,

V ar() = E( - E())2

(6)

As shown in Table 2, V ar() of B and C are smaller than V ar() of A, indicating that B and C are more stable than A. This observation is different from < Init and V ar(AP) which shows that B and C are less stable than A in Table 1.
Now, we derive the decomposition of E(AP - APT )2:
E(AP - APT )2 = E( - T )2
= E( - E())2 + (E() - T )2 (7)
= V ar() + Bias2()

The above equations show the expected squared error E(AP - APT )2 can be exactly decomposed into bias and variance on . The expected squared error E(AP - APT )2 always computes the error of the target model as zero, no
matter whether or not the target model still has room for
improvement. It is likely that the current best performance
for some queries can be further advanced in the near future.

1022

2.3 Further Investigation on Decomposition of Expected Squared Error
In order to further investigate two aforementioned biasvariance decompositions, we set 1 (the maximum AP) as the upper-bound AP for each query. We can have an expected squared error E(AP - 1)2 and its decomposition as:
E(AP - 1)2
= E(AP - APT + APT - 1)2
= E(AP - APT )2 + E(APT - 1)2 + 2E(AP - APT )(APT - 1) (8)

which shows that E(AP-APT )2 is only one part of E(AP- 1)2, and the target model T still has an error E(APT - 1)2,
suggesting there is still a room for improvement. We can also decompose E(AP - 1)2 as:

E(AP - 1)2

= E[AP - E(AP) + E(AP) - 1]2

= E(AP - E(AP))2 + [E(AP) - 1]2

(9)

= V ar(AP) + (MAP - 1)2

= (1 - MAP)2 + V ar(AP)

It turns out the variance parts in Eq. 3 and Eq. 9 are the same (i.e., V ar(AP)). The term (1 - MAP)2 in Eq. 9 has the same trend as Bias2(AP) (i.e., (MAPT -MAP)2), where MAPT is the upper bound of the MAP. The above observations show that the decomposition in Eq. 3 can be considered
as a simplified version of the decomposition in Eq. 9.

2.4 Comparison between Two Bias-Variance
First, the bias-variance of  requires a target AP for every query. On the other hand, the bias-variance of AP (in Eq. 3) can be used when only a target MAP (i.e., MAPT ) is given. Specifically, two biases (i.e., Bias(AP) and Bias()) are equivalent and both can be computed by MAPT -MAP. Regarding variance, the variance of  requires APT for each query (see Eq. 6 and Eq. 4), while the variance of AP can be calculated without knowing APT (see Eq. 2). Thus, the bias-variance based on AP is more flexible for practical use.
Second, the bias-variance of  is an exact decomposition of E(AP - APT )2 and it always regards the target model as a zero-error model. However, the decomposition of E(AP-1)2 in Eq. 8 shows that the target model can still has error. On the other hand, under the bias-variance of AP, the variance for the target model still exists, indicating that although we can assume a target model as an upper-bound at a certain stage, it can be further improved.

2.5 Bias-Variance Evaluation
To carry out IR evaluation based on the proposed the bias-variance decomposition methods, one needs to choose the upper-bound settings (i.e., the target model T). There can be a number of ways. First, the upper-bound AP for each query can be simply set as 1, which corresponds to a perfect target model. Second, the upper-bound AP for each query can be obtained based on the best AP among evaluated retrieval models in the historic TREC results. Third, one can simply set an upper-bound MAP (i.e., MAPT ).
The first and second upper-bound settings correspond to a virtual target model. For example, in the second setting, the target model collects the best AP among many retrieval

models. The third setting can correspond to a real target model (see the model settings in the experiments.) With the first and second upper-bound settings, either of biasvariance metrics (based on AP or ) can be adopted. The third setting is suitable for bias-variance of AP (see Section 2.4). Once an upper-bound setting and a variable (AP or ) are chosen, we can calculate bias-variance figures for a series of models and then see which model can have the minimum bias, or minimum variance, or the sum of them (i.e., the total error). We can also get an overview on the trend of bias and variance over parameters.

3. EMPIRICAL EVALUATION

We use the query modeling as an example of bias-variance evaluation. Due to the page limit, we only report the evaluation results on two standard TREC collections, i.e., WSJ (87-92) over queries 151-200 and ROBUST 2004 over queries 601-700. The title field of the queries is used. Lemur 4.7 [5] is used for indexing and retrieval. The top n = 30 ranked documents in the initial ranking by the original query model are selected as the pseudo-relevance feedback (PRF) documents. The number of expanded terms is fixed as 100. 1000 documents are retrieved by the KL-divergence model [5].
Model Settings We customize a number of query models according to three factors (i.e., model complexity, model combination, ground-truth data) that can generally influence the bias-variance tradeoff [1]. The first model is the original query model which is a maximum likelihood estimation of original query. We also evaluate an expanded query model RM (i.e., RM1 in [4]) which is generated from feedback documents D. Compared with the original query model, the expanded query model is more complex due to the fact that it has more terms and additional assumptions (e.g., assuming that all feedback documents are relevant). The above two models can be combined, leading to a combined query model (also called as RM3). Let  be the combination coefficient which is in the range (0,1). When  gets close to 0, the combined model is towards the RM, and otherwise towards the original model. In RM, we can gradually remove a percentage (denoted by rn) of nonrelevant documents DN along the initial ranking of feedback documents, based on the available relevance judgements (as ground-truth) [8]. We finally derive a target model which are generated by keeping only relevant documents DR in D:

p(w|q(t)) 

p(w|d)

dDR

(10)

which gives the best MAP over the aforementioned query models. In Eq. 10, the document weights are set as uniform (different from the weights in RM), since the relevant judgements of relevant documents are the same (i.e., 1).
Bias-Variance Metrics Setting We report the biasvariance on AP in our evaluation, since the target query model used in our evaluation only guarantees the upperbound MAP. According to the discussion in Sections 2.4 and 2.5, the bias-variance of AP is more suitable.
Evaluation Results We first look at the bias-variance results in Figure 1. Recall that the smaller bias or variance (based on AP) corresponds to the better retrieval effectiveness or stability, respectively. Figure 1(a) shows that on WSJ8792, the original query model has the smallest variance (the highest stability), but the largest bias (the lowest MAP). This is a tradeoff between bias and variance. One

1023

Var(AP)

0.066 0.064 0.062
0.06

Bias-Variance (AP)
Original Model Expanded Model RM Combinaed Model Removing DN Target Model

0.058

0.056

0.054 0

0.005 0.01 0.015 0.02 0.025 0.03 0.035 Bias2(AP)
(a) WSJ8792

Var(AP)

0.06 0.058 0.056 0.054 0.052
0.05 0.048 0.046 0.044 0.042
0.04 0

Bias-Variance (AP)
Original Model Expanded Model RM Combinaed Model Removing DN Target Model 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045
Bias2(AP)
(b) ROBUST2004

Figure 1: Results of Bias and Variance of AP of all the concerned query models. The x-axis shows the squared bias and the y-axis shows the variance.

reason in Statistics language is that the expanded models are all complex than the original one. The bias of the expanded model RM is much smaller, while its variance is much larger, than the original model. The different variants of the combined model (with  in [0.1, 0.9] with increment 0.1) are lying between the expanded and original models. We can also see that 2 (out of 9) combined models (when =0.1 and 0.2) has smaller bias and smaller variance than the expanded query model RM. This suggests that the combined query model with good parameters can achieve better effectiveness and stability over RM. Among the combined query models, bias and variance are negatively correlated, indicating a bias-variance tradeoff occurs. On the other hand, if we remove the non-relevant documents DN (with rn in [0.1, 1] with increment 0.1) in RM, the bias and variance are positively correlated, and 9 (out of 10) models (when rn = 0.2 to 1) can reduce both the bias and variance over RM. The target model has zero bias, although there is a variance of its AP across different queries. On ROBUST2004, the results are similar. The differences are that: 1) by removing non-relevant documents, only 6 (out of 10) models (when rn = 0.5 to 1) reduce both bias and variance over RM; 2) there are 3 (out of 9) (when  = 0.1 to 0.3) variants of the combined model for which both bias and variance can be smaller than those of RM.
Figure 2 shows Bias2 + V ar results for all the concerned models. On both collections, the target method achieves the smallest Bias2 + V ar and the original query model achieves the largest Bias2 +V ar. Recall that Bias2 +V ar represents the total error and the smaller error can reflect the better overall quality (or performance), by considering both the effectiveness and stability in one single criterion (Bias2 + V ar). The results also suggest that the model combination and the relevance judgements are helpful to reduce Bias2 + V ar over the original model and the expanded model RM.
4. CONCLUSIONS AND FUTURE WORK
In this paper, a novel evaluation strategy based on the bias-variance decomposition has been presented. We formulate two forms of the bias-variance decomposition and theoretically compare them. We also use query expansion as an example to demonstrate the use of the bias-variance for IR evaluation and analysis, in terms of bias, variance or sum of them. In the future, we will further explore the other upper-bound settings and variables discussed in Section 2.5,

Bias2(AP)+Var(AP) Bias2(AP)+Var(AP)

Bias-Variance (AP) 0.09

0.085

Bias-Variance (AP)

0.085

0.08

0.08

0.075

0.075 0.07
0.065 0.06 0

Original Model Expanded Model RM Combinaed Model Removing DN Target Model
0.005 0.01 0.015 0.02 0.025 0.03 0.035 Bias2(AP)
(a) WSJ8792

0.07

0.065 0.06
0.055 0

Original Model Expanded Model RM Combinaed Model Removing DN Target Model
0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 Bias2(AP)

(b) ROBUST2004

Figure 2: Results of Bias2 and Bias2+V ar of AP of all the concerned query models. The x-axis shows the squared bias and the y-axis shows the Bias2 + V ar.

test with more retrieval models and explore the deep insights behind the bias-variance trends.
5. ACKNOWLEDGMENTS
This work is supported in part by the Chinese National Program on Key Basic Research Project (973 Program, grant No. 2013CB329304), the Natural Science Foundation of China (grant No. 61272265, 61070044), and EU's FP7 QONTEXT project (grant No. 247590).
6. REFERENCES
[1] C. M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., 2006.
[2] K. Collins-Thompson. Reducing the risk of query expansion via robust constrained optimization. In CIKM '09, pages 837­846, 2009. ACM.
[3] G. V. Cormack and T. R. Lynam. Statistical Precision of Information Retrieval Evaluation. In SIGIR '06, pages 533­540, 2006. ACM.
[4] V. Lavrenko and W. B. Croft. Relevance-based language models. In SIGIR '01, pages 120­127, 2001.
[5] P. Ogilvie and J. Callan. Experiments using the lemur toolkit. In TREC '02, pages 103­108, 2002.
[6] S. E. Robertson and E. Kanoulas. On per-topic variance in ir evaluation. In SIGIR '12, pages 891­900, 2012.
[7] J. Wang and J. Zhu. Portfolio theory of information retrieval. In SIGIR '09, pages 115­122, 2009.
[8] P. Zhang, Y. Hou and D. Song. Approximating true relevance distribution from a mixture model based on irrelevance data. In SIGIR '09, pages 107­114, 2009.
[9] L. Zighelnic and O. Kurland. Query-drift prevention for robust query expansion. In SIGIR '08, pages 825­826, 2008.

1024

An Adaptive Evidence Weighting Method for Medical Record Search

Dongqing Zhu and Ben Carterette
Department of Computer & Information Sciences University of Delaware
Newark, DE, USA 19716
[zhu | carteret]@cis.udel.edu

ABSTRACT
In this paper, we present a medical record search system which is useful for identifying cohorts required in clinical studies. In particular, we propose a query-adaptive weighting method that can dynamically aggregate and score evidence in multiple medical reports (from different hospital departments or from different tests within the same department) of a patient. Furthermore, we explore several informative features for learning our retrieval model.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models
Keywords
medical record search; EMR; information retrieval; cohort identification; language models
1. INTRODUCTION
The rich health information contained in electronic medical records (EMR) is useful for improving quality of care. One important application is to search EMR to identify cohorts for clinical studies, which requires retrieval systems specifically designed with medical domain knowledge.
To promote research on medical information retrieval, particularly for EMR retrieval, the Text REtrieval Conference (TREC) organized a Medical Records track in 2011 and 2012 [11, 10]. The task is an ad hoc search task for patient visits based on unstructured text in EMR. One particular problem in EMR search is how to aggregate and score evidence that distributes across multiple documents. This is because a patient can have multiple medical reports generated from several hospital departments or even from different tests within a single department.
In this paper, we propose a novel weighting method that can adaptively weight evidence with respect to different queries. We evaluate our algorithm on TREC test collections. The

cross-validation results show that our weighting method is better than a fixed-weighting method across several evaluation metrics. Though the improvement is not statistically significant, we believe that our method has the potential to be further improved when more test collections are available.
Our work makes the following contributions: 1) we propose a novel adaptive weighting method for aggregating and scoring evidence in medical records, 2) we propose and explore several features that are based on semantic similarity between medical concepts for predicting the weights of our adaptive weighting method.
2. RETRIEVAL TASK AND DATA
We use the official test collection of the TREC 2011 & 2012 Medical Records Track [11, 10] for our experiments. The test collection contains 100,866 de-identified medical reports, mainly containing clinical narratives, from the University of Pittsburgh NLP Repository.
The retrieval task1 is an ad hoc search task for patient visits. A patient visit to the hospital usually results in multiple medical reports, meaning there is a 1-to-n relationship between visits and reports.
ID Topic 107 Patients with ductal carcinoma in situ (DCIS) 118 Adults who received a coronary stent during an admission 109 Women with osteopenia 112 Female patients with breast cancer with mastectomies during admission
Table 1: Example topics of medical records track.
NIST released 81 information needs (or "topics" in TREC terminology) which were designed to require information mainly from the free-text fields (i.e., topics are not answerable solely by the diagnostic codes). Topics are meant to reflect the types of queries that might be used to identify cohorts for comparative effectiveness research [11]. Table 1 lists several TREC topics as examples. The topic specifies the patient's condition, disease, treatment, etc. Relevance judgments for the topics were also developed by TREC assessors based on the pooled results from TREC participants.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

3. ADAPTIVE EVIDENCE AGGREGATION
Evidence in a visit can have different forms of distribution. Generally, there are two extreme cases: 1) Strong evidence exists in only one report of the visit; 2) Evidence spreads almost evenly across the majority of reports associated with the visit.
1 http://www- nlpir.nist.gov/projects/trecmed/2011/tm2011.html

1025

Report-based Retrieval For the first case, we can estimate the relevance of a visit based on its most relevant report. Thus, we use reports as the initial retrieval units (i.e., building an index for reports and applying the retrieval model to each report), and then transform a report ranking into a visit ranking based on the strongest report-level evidence, which is equivalent to using the following report score merging method for ranking visits:
scoreR(V, Q) = MAX(score(R1V , Q), score(R2V , Q), ...), (1)
where RjV is a report associated with visit V based on the report-to-visit mapping, score(RjV , Q) is the relevance score of the report with respect to query Q.

Visit-based Retrieval Eq.1 cannot handle case 2 well. For example, if visit V1 has strong evidence in multiple reports and visit V2 has strong evidence in only one report, V1 and V2 will have the same relevance score by using Eq.1. To deal with this problem,
we aggregate evidence by merging reports from a single visit field by field into a single visit document V , and then performing retrieval from an index of visits.
Like report-based retrieval, this visit-based retrieval has
its own disadvantages since it cannot handle case 1 well. For example, if visits V1 and V2 both have strong evidence in only one of their reports but V1 has three times more reports than V2, the strong evidence in V1 will be weakened after merging, resulting in V1 receiving a lower relevance score than V2.

3.1 A Novel Scoring Function
The comparison of the report-based and visit-based retrievals shows that these two strategies complement each other. Thus, we propose a new query-adaptive scoring function as shown below:

score(V, Q) = Q ·scoreR(V, Q)+(1-Q)·scoreV(V, Q), (2)

where scoreR(V, Q) and scoreV(V, Q) are the relevance scores of document V from report-based and visit-based retrievals respectively, and Q is the query-adaptive coefficient for scoring merging. If we can adjust Q appropriately, Eq.2 should be able to deal with all the evidence distribution
cases mentioned above.

3.2 Learning Algorithm
In this paper, we propose to adaptively set Q with respect to different queries by learning the weight Q based on a set of features.
In particular, we can view Q as a mixing probability: the probability that the evidence clusters in only one report rather than spreads across multiple reports. Then, assuming the log-odds of that probability can be expressed as a linear combination of feature values, we may write:

log

1

Q - Q

m
= 0 + ixi +
i=1

Q

where 0 is a model intercept (or bias term), xi is the value of feature number i, i is the weight coefficient of that feature, and Q is a slack variable.
This is essentially a logistic regression model2. Logistic
regression is fit using iteratively reweighted least squares to

2While logistic regression is often used for 0/1 classification problems,

find the values of the  coefficients that are the best fit to training data. Given feature values and their  coefficients, we can then predict the mixing probability Q for new queries.

3.3 Features
We propose 14 features that are possibly related to the evidence distribution in visits, and can be used to predict the weight Q in Eq.2. All these features are based on characteristics of the medical concepts contained in the query. We detect these medical concepts using MetaMap [1], a medical NLP tool developed at the National Library of Medicine (NLM) to map biomedical text to concepts in the Unified Medical Language System (UMLS) Metathesaurus. The concepts are represented by the Concept Unique Identifier (CUI) in UMLS Metathesaurus. Thus, we use QC to represent a concept query that is converted from the original text query Q and contains only CUIs. Next, we describe these 14 features in detail:

1. Length of the query Intuitively, evidence is more likely to resides across reports for long queries. Thus, we use the length of query |Q| as the feature to estimate the evidence distribution. It is defined formally as |Q| = wQ cnt(w, Q), where c(w, Q) is the count of term w in Q.

2. Number of concepts in the query

Similarly, if a query contains more medical concepts, it is

more likely to find that the evidence distributes across mul-

tiple reports. We define this feature formally as |QC | =

w

winQQCCc.nQt(Cw,aQbCe)t,tewrhfeeraetucrnet(twh,aQn CQ)

is the count of term because if the query

contains a medical concept whose name is very long then Q

might not be a good indicator of the evidence distribution.

3. Broad/narrow query concepts

A text query can contain several medical concepts, for each

of which the MetaMap program will return 1 to 10 candi-

dates. We hypothesize that a concept with more candidates

is less specific, and thus more likely to be a broad concept

and appears in multiple reports. Thus, the average number

of returned MetaMap candidates for concepts in a query

may be a good indicator of evidence distribution. We define

this feature as RC =

wQC |Meta(w)|
|QC |

,

where

|QC |

the

orig-

inal concept query length (i.e., the length before expansion),

|Meta(w)| is the number of concept candidates returned by

MetaMap for term w in concept query QC .

4. Semantic similarity among query concepts Intuitively, if QC contains concepts that are semantically close, the associated evidence in a visit may also co-occur in a single report. However, if the concepts are semantically distant, the corresponding evidence may tend to distribute across reports. Thus, we use the semantic distance among query concepts to estimate how the evidence distributes.
We use YTEX3 to measure semantic similarity. Given a pair of UMLS concepts, YTEX can produce knowledge based and distributional based similarity measures. The former uses knowledge sources such as dictionaries, taxonomies,

it can also be used when the target variable is a real number between
0 and 1. In this case it is sometimes called a "quasibinomial" model. 3 http://code.google.com/p/ytex/wiki/SemanticSim_V06

1026

Type Knowledge-based Distributional-based

Method Path-Finding
Intrinsic IC based Corpus IC based

Notation WUPALMER
LCH PATH RADA IC LIN IC LCH IC PATH IC RADA JACCARD SOKAL CIC LIN

Name Wu & Palmer Leacock & Chodorow Path Rada Lin Leacock & Chodorow Jiang & Conrath Rada Jaccard Sokal & Sneath Lin

Table 2: Semantic similarity measures.

and semantic networks, while the latter mainly uses the distribution of concepts within some domain-specific corpus [3].
We use the 11 measures listed in Table 2 as our features. Due to the limited space, we will not describe these features; Garla and Brandt provide a detailed overview [3].
For each query and each specific measure, we take the mean of the semantic similarity scores for all UMLS concept pairs in the query. This averaged semantic similarity score will be the feature score.
4. EXPERIMENTAL SETUP
We use the Indri4 retrieval system for indexing and retrieving. In particular, we use the Porter stemmer to stem words in both text documents and queries, and use a standard medical stoplist [4] for stopping words in queries only.
Our retrieval model is a linear combination of the Markov random field model (MRF) [8] and a mixture of external collection-based relevance models (MRM) [2] for query expansion. Our collections for expansion are the ClueWeb09 Category B (excluding the Wikipedia pages) corpus, the 2009 Genomics Track corpus, 2012 Medical Subject Headings (MeSH), and the medical records corpus itself. Both report and visit-based retrievals use this system.
Because the focus of this work is to evaluate the adaptive scoring function as shown in 2, we will set the parameters of the MRF and MRM models to some default values. We use the same set of parameter values for both the report and visit-based retrievals. We set the Dirichlet smoothing parameter  to 2500. For MRF model, we follow Metzler and Croft [8] and set the feature weights (T , O, U ) to (0.8, 0.1, 0.1). For MRM model, we take take the top-weighted 10 terms from the top-ranked 50 documents for each expansion collection. More detail about our model is presented in recent work [13].
To evaluate our learning algorithm as described in Section 3, we first obtain the optimal coefficient Q-opt for each topic Q by sweeping [0, 1] at a step size of 0.1. Then we conduct leave-one-out cross-validation (LOOCV), in each iteration of which the system predicts Q for one new topic based on Q-opt's for the other 80 topics. With limited topics available for learning a relatively complex prediction model, using LOOCV can maximize the size of training data we can use in each iteration of the cross-validation, and lead to a better estimate for each feature weight.
We train our systems on MAP. This is because: 1) training on MAP is most commonly used in IR to improve retrieval performance; 2) we find that training on MAP improves the retrieval performance on other evaluation metrics as well while training on other evaluation measures does not
4 http://www.lemurproject.org/indri/

Feature IC RADA WUPALMER
RADA JACCARD

Significance 0.0112 0.0299 0.0368 0.0647

Feature RC
SOKAL IC LIN IC PATH

Significance 0.0654 0.0671 0.0824 0.0876

Table 3: Features in the pruned set using LOOCV, sorted by their statistical significance scores.

improve the overall performance. Thus, MAP will be the primary evaluation measure in this work. In fact, MAP correlates well with other evaluation measures as we will show in the Section 5.
To access the statistical significance of differences in the performance of two systems, we perform one-tailed paired t-test for MAP (since we train systems on MAP). We report scores for MAP, R-precision (Rprec), bpref, and precision at rank 10 (P10).
5. RESULTS AND ANALYSIS
5.1 Feature Selection
To choose a good feature combination, we use a greedy feature elimination approach in which we start with a full set of features and iteratively eliminate exactly one feature at a time that has the greatest negative impact on the retrieval performance until when further removing any feature will degrade the performance.
After the above feature set pruning step, there are 8 features left as shown in Table 3. We further study the importance of each feature by analyzing the prediction model trained in a randomly selected iteration of LOOCV using these 8 features. Based on the statistical significance of each feature as shown in Table 3, we can infer that:
1) All the intrinsic IC based features except IC LCH are in the pruned feature set, indicating that these types of similarity measures are generally more effective for predicting Q than other measures. In fact, the intrinsic IC similarity measure incorporates taxonomical evidence explicitly modeled in ontologies (such as the number of leaves/hyponyms and subsumers), which are not captured by the path-finding based measure. Furthermore, the intrinsic IC similarity measure avoids dependence on the availability of domain corpora, thus is considered more scalable and easily applicable than the distributional-based measure [9].
2) RC is a good feature though it only uses similarity information about each query concept and its neighbors (rather than other query concepts) in the semantic network.
3) Neither |Q| nor QC is in the pruned set, indicating that non-semantic-similarity-related features are generally not useful for estimating the evidence distribution.
4) RADA is a feature that might worth further exploration because both the Path-finding based and the intrinsic IC based RADA features are in the pruned set.
5.2 Adaptive Weighting
Fixed Weighting We first evaluate the performance of Eq. 2 when  is fixed (i.e., not adaptive). In each iteration of the LOOCV, we obtain the best value setting for  on the 80 training topics by sweeping [0, 1] at a step size of 0.1, and then apply the trained  value to the single testing topic. We show the results in the `Fixed-weighting' row of Table 4. Note that

1027

System Visit-based Report-based Fixed-weighting Adaptive-weighting Optimal-weighting

MAP
0.4122 0.4354V 0.4472V,R 0.4485V,R 0.4639V,R,F,A

R-prec 0.422 0.435 0.443 0.447 0.457

bpref 0.499 0.511 0.520 0.523 0.539

P10 0.619 0.607 0.631 0.642 0.656

Pred. MSE ­ ­
0.128 0.125 0.000

Table 4: Performance comparison. A superscript on the MAP score of system X corresponds to the initial of system Y, and indicates statistical significance (p < 0.05) in the MAP difference between X and Y. The last column is the mean square error of the predicted weights. `Fixed-weighting' corresponds to one of the top-ranked TREC systems as mentioned in Sections 4 and 5.2.

this system is a better version of system udelSUM [13] which is one of the top-ranked 2012 Medical Records track systems.
Optimal Weighting We also obtain the optimal Q-opt for each topic separately by sweeping  from 0 to 1 with a step size of 0.1. Then, we use the Q-opt's to compute the best retrieval performance (i.e., an upper-bound) Eq. 2 can possibly achieve, as shown in the `Optimal-weighting' row of Table 4.
Performance Comparison Table 4 shows performance comparison of our adaptive merging method with fixed-weighting, optimal-weighting, and two other baselines (report-based retrieval and visit-based retrieval). Our adaptive merging method is better than the fixed weighting method on all the evaluation metrics. The improvement is not statistically significant (p = 0.191), possibly because 81 topics may not be enough to train a good prediction model for our adaptive weighting method. In addition, the data are slightly skewed as Figure 1 showing that Q-opt = 1 or 0.9 on about one third of the topics.

Figure 1: Distribution of topics against Q-opt.
6. RELATED WORK
Due to the sensitivity of patient data, methods emerging from research on information retrieval for EMR retrieval have not been well explored by academic researchers. Fortunately, the Text REtrieval Conference (TREC) organized the Medical Records track in 2011 & 2012 making a set of real medical records and human judgments of relevance to search queries available to the research community.
Some interesting work have been done using the TREC collection. Limsopatham et al. [5] proposed an effective term representation to handle negated phrases in clinical text. They also incorporated dependence information of the negated terms into the term representation and achieved significant improvement over a baseline system that had no negation handling mechanism.
More recently, Limsopatham et al. [7] proposed an effective representation for EMR retrieval, in which medical

records and queries are represented by medical concepts that directly relate to symptom, diagnostic, test, diagnosis, and treatment. We have built on their work, combining a concept representation with text-based retrieval to improve on both and provide a base in which additional medical knowledge can be incorporated easily.
Among more relevant works, Limsopatham et al. [6] explored using the type of medical records for enhancing retrieval performance. They demonstrated that incorporating department level evidence of the medical reports in their extended voting model and federated search model could improve the retrieval effectiveness. Their work opens another interesting direction for exploring evidence distribution and score merging. Zhu and Carterette's system [12] aggregated report-level evidence and visit-level evidence, and achieved significant improvement over a strong baseline.
7. CONCLUSION AND FUTURE WORK
In this paper, we present a medical record search system which is useful for identifying cohorts required in clinical studies. In particular, we propose a query-adaptive weighting method that can dynamically aggregate and score evidence within multiple medical reports. We show by crossvalidation that our weighting method is better than a fixedweighting method across several evaluation metrics. Though the improvement is not statistically significant, we believe that our method has the potential to be further improved by incorporating other useful features or by using advanced prediction models. Furthermore, we explore several informative features for weight prediction. We believe these features might be useful for improving medical IR systems.
8. REFERENCES [1] A. R. Aronson. Effective mapping of biomedical text to the UMLS metathesaurus: The MetaMap program. Proceedings of AMIA Symposium, pages 17­21, 2001. [2] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In Proceedings of SIGIR, pages 154­161, 2006. [3] V. Garla and C. Brandt. Semantic similarity in the biomedical domain: an evaluation across knowledge sources. BMC Bioinformatics, 13:261, 2012. [4] W. Hersh. Information Retrieval: A Health and Biomedical Perspective. Health Informatics. Springer, 3rd edition, 2009. [5] N. Limsopatham, C. Macdonald, R. McCreadie, and I. Ounis. Exploiting term dependence while handling negation in medical search. In Proceedings of SIGIR, pages 1065­1066, 2012. [6] N. Limsopatham, C. Macdonald, and I. Ounis. Aggregating evidence from hospital departments to improve medical records search. In Proceedings of ECIR, 2013. [7] N. Limsopatham, C. Macdonald, and I. Ounis. A task-specific query and document representation for medical records search. In Proceedings of ECIR, 2013. [8] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, page 472, 2005. [9] D. Sa´nchez and M. Batet. Semantic similarity estimation in the biomedical domain: An ontology-based information-theoretic perspective. Journal of Biomedical Informatics, 44(5):749 ­ 759, 2011. [10] E. M. Voorhees. DRAFT: Overview of the TREC 2012 medical
records track. In TREC, 2012. [11] E. M. Voorhees and R. M. Tong. DRAFT: Overview of the
TREC 2011 medical records track. In TREC, 2011. [12] D. Zhu and B. Carterette. Combining multi-level evidence for
medical record retrieval. In Proceedings of SHB, 2012. [13] D. Zhu and B. Carterette. Exploring evidence aggregation
methods and external expansion sources for medical record search. In Proceedings of TREC, 2012.

1028

TweetMogaz: A News Portal of Tweets
Walid Magdy
Qatar Computing Research Institute Qatar Foundation Doha, Qatar
wmagdy@qf.org.qa

ABSTRACT
Twitter is currently one of the largest social hubs for users to spread and discuss news. For most of the top news stories happening, there are corresponding discussions on social media. In this demonstration TweetMogaz is presented, which is a platform for microblog search and filtering. It creates a real-time comprehensive report about what people discuss and share around news happening in certain regions. TweetMogaz reports the most popular tweets, jokes, videos, images, and news articles that people share about top news stories. Moreover, it allows users to search for specific topics. A scalable automatic technique for microblog filtering is used to obtain relevant tweets to a certain news category in a region. TweetMogaz.com demonstrates the effectiveness of our filtering technique for reporting public response toward news in different Arabic regions including Egypt and Syria in real-time.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information filtering.
Keywords
Twitter, Microblog Filtering, TweetMogaz, Arabic text.
1. INTRODUCTION
Microblogging sites, such as Twitter, are currently one of the main platforms for exchanging real-time information and discussions. In fact, Twitter and Facebook were instrumental in facilitating the launch of the so-called "Arab Spring". This large amount of information led to the need for an effective platform for information filtering, to allow only relevant information reaches users. The currently implemented and straightforward microblog filtering technique on Twitter is the "follow" feature. This allows users to follow other accounts of entities, persons, or events to be fed with their tweets. This method is personalized according to user's interest. Another method for following specific micrblogs on Twitter is searching for given hashtags (#tags), which is a common way for users to get updates about some topics based on the mention of the hashtag within tweets text. This method is less strict in filtering information, where more tweets are generally presented to user. However, many unneeded tweets would be retrieved because of the misusage of hashtags by some users. Additionally, many relevant tweets to a hashtag topic may not include the hashtag itself, which leads to their absence in the retrieved results.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. ACM 978-1-4503-2034-4/13/07.

In this demonstration we present a news portal website, TweetMogaz, which is generated from tweets. TweetMogaz presents the most popular content people share on Twitter regarding the ongoing news in different regions. Visitors of the website can see a comprehensive report of the most popular tweets, jokes, videos, images, and news articles that people share on Twitter related to the top news stories of the day. Standard news sites give their visitors an idea about what is happening in given regions, while TweetMogaz gives visitors an idea about what are the topics in news that people are interested in, and how they react towards them. In addition, it captures additional aspects of news stories shared on social media that may not exist in the news sites.
TweetMogaz applies microblog filtering technique for retrieving tweets. A set of key players in news for a certain region are manually listed, and one or multiple news sites for the same region are set to our system. Potential relevant tweets to news are retrieved; then a relevance classification technique is used to identify the relevant tweets and discard the rest. Classified relevant tweets are then used to generate comprehensive report with most popular content people share as in [3]. Information on website is updated every 15 minutes to keep following fresh news and their response on social media.
Our approach for retrieving and filtering relevant tweets shows high performance. The approach is applied to follow political news in counties in the Arabic region such as the Egypt and Syria. However, it is potentially applicable to different regions and different news categories. The only requirement is preparing a list of key players of a news category in a given region and setting a regional news site.
2. RELATED WORK
Previous work in Microblog retrieval focused on analyzing the search process [4, 8], improving ad-hoc microblog search [6], and developing platforms for improved user experience with search through analyzing results [3, 9]. Additional work studied the role of micrblogs in news reporting and discovery, and how users' profiles can be used for news recommendation [5, 7]. Other work investigated detecting comments about news from Twitter to be presented to readers along with news articles [2].
Recently, microblog filtering grabbed some attention as an important task for allowing users to follow certain topics. Microblog filtering was introduced as a new task in TREC Microblog track 2012, where the aim was to filter a feed of tweets by getting relevant ones to some topics. The best achieved result in the track got a precision of 0.6, which is considerably low for usage in a practical environment [6].
Our system presented in this demonstration applies microblog filtering [6] for getting tweets relevant to regional news [5, 7] in a practical environment with high precision. Search results are presented to user in the form of a comprehensive report [3]. Our system enables user to get a summary about the public response towards ongoing news [2].

1095

3. MICROBLOG FILTERING APPROACH

Our filtering approach for retrieving relevant tweets to regional

news is shown in Figure 1. The approach is decomposed into the

following steps:

1. Retrieving initial set of relevant tweets

Any region has a set of key players who are expected to be a

usual actor in news headlines. For example, "Obama" is a key

player in US politics. These set of key players are nearly static,

where they do not change frequently by time. Therefore, we

prepare a list of accurate predefined queries representing key

player in a certain region to retrieve an initial set of relevant

tweets. Queries include politicians, parties, institutes ... etc, and

their corresponding Twitter accounts. The list of queries requires

updating each few months or years according to changes in the

region. These queries are set carefully to achieve high precision to

avoid the retrieval of irrelevant tweets. For example, setting a

query "Obama" referring to the US president is acceptable, since

most of the tweets talking about "Obama" refer to the president

himself. While searching for "Clinton" as a query for "Bill

Clinton" can lead to the retrieval of a large number of irrelevant

tweets for those tweets referring to "Hillary Clinton". Therefore,

in the latter case, it is better to have the query as "Bill Clinton" to

emphasis on high precision results.

Stream of tweets that match any of the predefined queries are

considered relevant. Matching tweets are referred to as Key

Players Tweets set (TweetsKP).
2. Retrieving set of Potential Relevant Tweets

Tweets about accidental regional news may not be captured

with the set of predefined queries. To overcome this problem,

news is explored on one or more news sites, and keywords are

extracted. Keywords usually exist in news articles as metadata.

Collected keywords are then used to retrieve additional tweets.

Tweets matching keywords are then assigned to a relevance

classifier, since keywords may include general or incorrect terms

that can lead to the retrieval of large number of irrelevant tweets.

This set of tweets is referred to as Keywords Tweets (TweetsKW).

3. Classifying Tweets

An SVM classifier is trained with TweetsKP, acting as the positive examples and a set of randomly selected tweets as

negative examples (TweetsN). TweetsN should not match neither the predefined queries nor the extracted keywords from news.

This guarantees:

.

The number of negative examples is selected to be 10 times the

positive examples since the spectrum of irrelevant tweets is

expected to be much wider. Positive and negative examples are

selected from the past 24 hours to be representing recent data. Of

course training examples are not 100% accurate, since they are

selected automatically. However, this technique of selection is

more scalable and examples are generally correct.

The set of features used to train the SVM classifier are the terms

appearing in TweetsKP more than 10 times. In addition, a feature represents the percentage of terms in a tweet that do not match

any of these terms is used. Generated model is then used to

classify TweetsKW. Classified relevant tweets are then added to TweetsKP to form the full set of relevant tweets. Finally a comprehensive report is generated out of these tweets [3].

The process of training the classifier is applied each 15 minutes

to keep the user updated with tweets relevant to news in real-time.

Typically, the tweets classified as relevant enrich the total number

of relevant tweets significantly; especially when accidental news

occurs with new entities. Subjectively, the increase of relevant

tweets ranges between 50% and 200% according to the type of

news at that time, with noticeable high precision.

Predefined Queries

News RSS Feed

Retrieve Matching Microblogs
List of Relevant Microblogs
Train Classifier

Stream of Microblogs
Extract Random
List of Random Microblogs

Extract Keywords
Retrieve Matching Microblogs
List of Matching Microblogs

Classification Model

Classify

Considered Relevant Considered Irrelevant (TweetsN) Mix of Rel. & Irrel.(TweetsKW)

Additional List of Relevant
Microblogs

Figure 1 Filtering Approach for Relevant Tweets

4. TWEETMOGAZ DEMONSTRATION
TweetMogaz is running over a collection of Arabic tweets. It collects an average of 3-4 million Arabic tweets per day since May 2012. Tweets text is pre-processed using state-of-the-art normalization techniques for social Arabic text [1] and indexed using Solr. Search is enabled by specifying a query and time span, and results are presented in a comprehensive report similar to work in [3]. The homepage of TweetMogaz displays a real-time report about political news in the past 24 hours for countries in the Arabic region including Egypt and Syria. Tens of thousands of tweets are identified daily as relevant to each region using the presented filtering approach. The number of relevant tweets can reach up to 300k tweets on days with hot news, such as the Egyptian presidential elections day, and the days when there are severe battles between the Syrian free army and the regime's army. User can see examples of these days by browsing archived daily reports on the website.
5. REFERENCES
1. K. Darwish, W. Magdy, A. Mourad (2012). Language Processing for Arabic Microblog Retrieval. CIKM 2012
2. A. Kuthari, W. Magdy, K. Darwish, A. Mourad, A. Taei. (2013). Detecting Comments on News Articles in Microblogs. ICWSM 2013
3. W. Magdy, A. Ali, K. Darwish (2012). A Summarization Tool for Time-Sensitive Social Media. CIKM 2012
4. N. Naveed, T. Gottron, J. Kunegis, A. Alhadi. (2011). Searching microblogs: coping with sparsity and document quality. CIKM 2011.
5. O. Phelan, K. McCarthy, M. Bennett, and B. Smyth. (2011). Terms of a feather: content-based news recommendation and discovery using twitter. ECIR 2011.
6. I. Soboroff, I. Ounis, J. Lin, I. Soboroff. (2012). Overview of the TREC-2012 Microblog Track. TREC 2012
7. I. Subasic, B. Berendt. (2011). Peddling or Creating? Investigating the Role of Twitter in News Reporting. ECIR-2011
8. J. Teevan, D. Ramage, M. Morris. (2011). #Twittersearch: A comparison of microblog search and web search. WSDM 2011.
9. S. R. Yerva, Z. Miklós, F. Grosan, A. Tandrau, K. Aberer. (2012). TweetSpector: Entity-based retrieval of Tweets. SIGIR 2012

1096

InfoLand: Information Lay-of-Land for Session Search
Jiyun Luo, Dongyi Guan, Hui Yang
Department of Computer Science Georgetown University
{jl1749,dg372}@georgetown.edu, huiyang@cs.georgetown.edu

ABSTRACT
Search result clustering (SRC) is a post-retrieval process that hierarchically organizes search results. The hierarchical structure offers overview for the search results and displays an "information lay-of-land" that intents to guide the users throughout a search session. However, SRC hierarchies are sensitive to query changes, which are common among queries in the same session. This instability may leave users seemly random overviews throughout the session. We present a new tool called InfoLand that integrates external knowledge from Wikipedia when building SRC hierarchies and increase their stability. Evaluation on TREC 2010-2011 Session Tracks shows that InfoLand produces more stable results organization than a commercial search engine.
Categories and Subject Descriptors
H.3.3 [Information Systems ]: Information Storage and Retrieval--Information Search and Retrieval
Keywords
Search Results Clustering; Session Search
1. INTRODUCTION
Search result clustering (SRC) [1, 4] is a post-retrieval process that hierarchically organizes search results. It is used in Meta search engines such as Yippy.com (previously known as Clusty). SRC hierarchies display an information "lay of land" for search and help users to quickly locate relevant documents from piles of search results.
Session search has recently attracted more attentions in Information Retrieval (IR) research. A session usually contains multiple queries. These queries are usually highly related to a main topic and to each other. Ideal SRC hierarchies generated for queries in the same session should be highly related too. However, the state-of-the-art SRC hierarchies are usually sensitive to query changes and hence
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. ACM 978-1-4503-2034-4/13/07.

Figure 1: SRC hierarchies generated by Yippy.
Figure 2: SRC hierarchies generated by InfoLand.
demonstrate unstable hierarchies throughout the session. Figure 1 shows hierarchies generated by Yippy for queries `diet' and `low carb diet' (TREC 2010 session 123). Although many sessions only show slightly changes among queries1, the hierarchies built for these queries' search results can be dramatically different from each other.
The reason that causes unstable hierarchies lies in the fact that that many hierarchy construction approaches are datadriven [1, 4]. A common approach, which is also used in Yippy, is to first group similar documents into clusters and then label the clusters. We observe that clustering-based ap1For instance, about 38.6% adjacent queries in TREC 2010 and 2011 Session tracks [2] only show one word difference and 26.4% show two word difference.

1097

proaches often produce mixed-initiative clusters and reduce hierarchy stability.
We propose a novel hierarchy construction tool, InfoLand, which injects world knowledge to an existing hierarchy to increase its stability. Evaluation over TREC 2010 and 2011 Session tracks shows that InfoLand produces significantly more stable stable SRC hierarchies than Yippy.

Table 1: Stability of SRC Hierarchies for TREC queries.

 indicates a significant improvement at p < 0.005.

2010

FBS Node overlap Parent-child precision

Yippy

0.463 0.415

InfoLand 0.603 0.529

0.144 0.450

2011

FBS Node overlap Parent-child precision

Yippy

0.440 0.327

InfoLand 0.504 0.420

0.115 0.247

2. BUILD STABLE CONCEPT HIERARCHIES
We propose three major steps in building stable concept hierarchies: concept extraction, mapping to Wiki entries, and hierarchy construction. First, a single query q and its search results D are processed and a set of concepts C that best represents D are extracted by algorithms described in [5]. Concepts from the hierarchy built by Yippy are also included in C.
Next, for a concept c  C, InfoLand maps it to its most relevant Wiki entry e, which is called a reference Wiki entry. We built a Lemur2 index over the entire Wikipedia collection in ClueWeb09.3 A concept c is sent as a query to the index and the top 10 returned Wiki pages are examined. The titles of these pages are considered as candidate Wiki entries for c and are denoted as {ei}, i = 1 · · · 10. Due to ambiguity in natural language, the top returned results may not be related to the current search session. We hence disambiguate Wiki entries by measuring the similarity between the entires and the topics mentioned in the search queries. The similarity is measured by by mutual information between an entry candidate ei and all concepts C for query q:

MI(ei, C) = PMI(ei, c|E) × log(1 + ctf (c)) · idf (c) (1)
cC

where log(1+ctf (c))·idf (c) measures the importance of con-

cept c in representing the main topic in D. Point-wise Mu-

tual Information (PMI) measures the similarity between ei

and

c w.r.t.

a corpus

E:

PMI(ei, c|E)

=

log

df(ei ,c;E)×|E| df(ei ;E)×df(c;E)

,

where df(x; E) is the document frequency of term x in corpus

E and |E| is the collection size.

The most relevant Wiki entry to the query is selected as

the reference Wiki entry. We obtain reference Wiki entries

ex and ey for concepts x and y and decide whether x sub-

sumes y based on the following cases:

(a) ex is a Wiki category: From ey's Wiki page, we extract

the Wiki categories that ey belongs to. We call the list of

Wiki categories for ey super categories and denote them as

Sy. x subsumes y if ex  Sy.

(b) Only ey is a Wiki category: x does not subsumes y.

(c) Neither ex nor ey is a Wiki category: We form super

category sets for both Sy and Sx. For each syi  Sy, we

extract its super categories and form a super-supercategory

set SSy for ey. We then measure the normalized overlap be-

tween

SSy

and

Sx:

Scoresub(x, y) =

, count(s;sSx and sSSy )
min(|Sx|,|SSy |)

where count(s; s  Sx and s  SSy) denotes the number of

categories that appear in both Sx and SSy. If Scoresub(x, y)

for a potential parent-child pair (x, y) is above 0.6, we con-

sider x subsumes y.

Lastly, based on the subsumption relationship identified,

we form SRC hierarchies as in [3].

2http://www.lemurproject.org. 3http://www.lemurproject.org/clueweb09.php/.

3. EVALUATION

Data from TREC 2010 and 2011 Session tracks is used

in the evaluation. For every query q, we retrieve the top

1000 documents from an index built over the ClueWeb09

CatB as its search results D. All TREC official ground truth

documents are also merged into the results set. In total, our

dataset contains 299,000 documents, 124 sessions, and 299

queries (on average 2.41 queries per sessions).

Given a session S with queries q1, q2, ... qn, we measure

the stability of SRC by averaging the hierarchy similarity

among query pairs in S. It is defined as: Stability(S) =

2 n(n-1)

n-1 i=1

n j=i+1

Simhie(Hi,

Hj

),

where

n

is

the

number

of queries in S, Hi and Hj are hierarchies built for qi and

qj, and Simhie(Hi, Hj) is the hierarchy similarity. Methods

to calculate Simhie include fragment-based similarity (FBS),

node overlap, and parent-child precision [5].

Table 1 compares the stability evaluation for hierarchies

generated by InfoLand and by Yippy over the TREC 2010

and 2011 datasets. InfoLand significantly outperforms Yippy

in stability in all metrics for both datasets.

Figure 2 shows the SRC hierarchies build by InfoLand

for TREC 2010 session 123. Comparing to Figure 1, we

observe a local expansion of concepts from the left hierarchy

to the right. It coincides well with the fact that this session

contains a specification from `diet' to `low carb diet'. Other

parts of the two hierarchies remain almost the same; which

demonstrates high hierarchy stability.

4. CONCLUSIONS
Search results hierarchies built for queries in the same session are usually sensitive to query changes. This partly diminishes the benefits that search result organization intents to offer. We present a new tool called infoLand that incorporates external knowledge to improve the stability of SRC hierarchies and enable them to better serve as information lay-of-land to guide session search. Evaluation over TREC 2010 and 2011 Session tracks demonstrates that InfoLand produces more stable hierarchies than Yippy.

5. ACKNOWLEDGMENTS
This research was supported by NSF grant CNS-1223825.

6. REFERENCES
[1] D. C. Anastasiu, B. J. Gao, and D. Buttler. A framework for personalized and collaborative clustering of search results. In CIKM '11.
[2] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2011 session track.
[3] M. Sanderson and B. Croft. Deriving concept hierarchies from text. In SIGIR '99.
[4] U. Scaiella, P. Ferragina, A. Marino, and M. Ciaramita. Topical clustering of search results. In WSDM '12.
[5] H. Yang. Personalized Concept Hierarchy Construction. PhD thesis, 2011.

1098

How Do Users Respond to Voice Input Errors?

Lexical and Phonetic Query Reformulation in Voice Search

Jiepu Jiang
School of Information Sciences, University of Pittsburgh
jiepu.jiang@gmail.com

Wei Jeng
School of Information Sciences, University of Pittsburgh
wej9@pitt.edu

Daqing He
School of Information Sciences, University of Pittsburgh
dah44@pitt.edu

ABSTRACT
Voice search offers users with a new search experience: instead of typing, users can vocalize their search queries. However, due to voice input errors (such as speech recognition errors and improper system interruptions), users need to frequently reformulate queries to handle the incorrectly recognized queries. We conducted user experiments with native English speakers on their query reformulation behaviors in voice search and found that users often reformulate queries with both lexical and phonetic changes to previous queries. In this paper, we first characterize and analyze typical voice input errors in voice search and users' corresponding reformulation strategies. Then, we evaluate the impacts of typical voice input errors on users' search progress and the effectiveness of different reformulation strategies on handling these errors. This study provides a clearer picture on how to further improve current voice search systems.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ query formulation, relevance feedback.
General Terms
Measurement, Experimentation, Human Factors.
Keywords
Query reformulation; voice search; voice input errors.
1. INTRODUCTION
Supporting query reformulation has long been recognized as an important strategy to help users further their search progress [3]. Users may need to reformulate queries several times until their information needs are fully satisfied. The need for reformulation may be attached to the users themselves. As users may have limited understanding of their information needs, the retrieval system and the collection, it is difficult for them to develop one single query to complete the search. At the same time, the need for reformulation may come from search problems being explorative where relevant documents may be scattered among different subtopics, so that it is impossible to retrieve all relevant documents with a single query. Therefore, many studies [7, 22] concentrated on supporting reformulation of textual queries.
Along with the rapidly increasing usage of mobile devices and the improvement of speech processing, voice search becomes an
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright © 2013 ACM 978-1-4503-2034-4/13/07...$15.00.

alternative search mode. During voice search, users can vocalize their queries and the retrieval system utilizes the voice recognition results for retrieval [6, 19]. Though previous studies found that query reformulation plays an important role in conventional textual search systems, to the best of our knowledge, there are very limited studies on voice search, especially concerning users' query reformulation in voice search.
In this paper, we therefore focus on explaining query reformulation behaviors in the context of voice search. The term voice query1 refers to the query in voice search. It contains not only the lexical contents, but also the phonetic characteristics such as the speaker's stress, speed, and intonation. In comparison, we refer to those searches in which users need to type queries on a keyboard as conventional searches.
We mainly concentrate on three research objectives in this study. First, voice search relies on users' vocalization of queries and systems' automatic speech recognition to transcribe voice queries, which may result in various voice input errors. Voice input errors include not only the errors from automatic speech recognition but also the system's interruptions during users' vocalization of queries. Therefore, our first objective is to characterize the types of voice input errors in voice search and evaluate their impacts on voice search.
Second, upon recognition of voice input errors, users will take actions in their subsequent query reformulation to overcome the errors. As voice queries involve both lexical and phonetic characteristics, users' reformulation choices and preferences would also be different from those in conventional searches. Therefore, our second objective is to identify and characterize users' query reformulation patterns in voice search.
Third, as the ultimate goal of this study is to shed light on how to support query reformulation in voice search, it is important to analyze users' preferences of using different reformulation patterns and examine the effectiveness of the reformulation patterns in handling voice input errors. In this study, we evaluate the effectiveness of the reformulation patterns by how they overcome the voice input errors and improve the retrieval performance.
To meet our research objectives, we conducted a series of voice search experiments involving native English speakers working on TREC search topics using the Google voice search app on the iPad. The participants were only permitted to speak voice queries to initiate searches and reformulate queries. Within a certain time limit, the participants could freely issue multiple voice queries, read or click on returned search results, and use Google's query suggestions. Users' voice queries, the system's transcription
1 In this paper, we use voice queries to refer to spoken queries and speech queries, which were used in previous studies [5]. Our rationale is to keep a consistency with Google Voice Search, the platform used in our experiment.

143

results to the voice queries, and the clicked documents were all recorded for analysis.
The rest of the paper is organized as follows: Section 2 reviews related studies in query reformulation and voice-based search; Section 3 introduces our methods for experimentation and analysis; in Section 4, we characterize voice query input errors and voice query reformulation patterns; Section 5 evaluates the impacts of voice input errors on voice search; Section 6 evaluates the effectiveness of each type of voice query reformulation; finally, we discuss suggestions for future development of voicebased search systems and outline our conclusions.
2. RELATED WORKS
2.1 Voice Search
Voice search [8, 23] or voice-enabled search [2, 20] refer to the search systems that allow users to input search queries in a spoken language and then retrieve the relevant entries based on systemgenerated transcriptions of the voice queries. Currently, voice search is commonly applied via mobile devices. Researchers examined the scenario of using voice search compared with traditional desktop search. For example, Schalkwyk et al. [19] analyzed Google's search logs and found that users utilized Google Voice Search more frequently when they tried to find information such as food and local geographical information (e.g. city names and local restaurants). However, it remains unclear whether the location-related information needs are intrinsically related to voice search, or are due to the fact that the current devices supporting voice search are mostly mobile devices.
Existing studies on voice search are very limited, especially those related to users' voice queries and query reformulations. Schalkwyk et al. [19] reported statistics of queries from Google Voice's search logs which found that voice queries are statistically shorter than desktop search queries. Crestani et al. [6] conducted a user experiment based on collections of users' voice queries. However, the experiment environment did not involve a real search system. Participants were asked to formulate voice queries without knowing whether their voice queries could be recognized, or if they would retrieve meaningful results. In comparison, in our experiments, participants freely interacted with the voice search systems, so that the participants' interactions, particularly their responses to voice input errors, could be collected and studied.
2.2 Query Reformulation
The lexical query reformulation patterns we adopted in this paper come from the summarization of many previous studies, including [9­11, 18, 21]. As we did not aim to create a systematic taxonomy of the reformulation patterns in voice search, we simplified the patterns to only four types: addition, substitution, removal, and re-ordering. However, our substitution pattern involved many other patterns defined in previous works, such as stemming [9, 21] and acronyms [9]. Also, many textual reformulation patterns that do not exist in voice search were removed, including: punctuation [21], URL stripping [9], substring [9], spelling correction [1], and capitalization [21].
3. METHODS
3.1 Settings and Experiment System
As stated, we are interested in users' query reformulation behaviors in voice search, especially how they utilize query reformulations to cope with voice input errors. Admittedly, as currently voice search is mostly used on mobile devices, an ideal experiment setting for our study should simulate mobile search environment, including many issues previously found to have an

impact on automatic speech recognition (ASR) and voice search, such as the background noise [19]. However, after consideration, we decided to conduct our study in a controlled laboratory experiment setting for the following reason: our focus is on how users change their queries when voice input errors happen in voice search. Therefore, automatic speech recognition (ASR) errors and the often concerned noise and vocabulary issues in ASR [19], though important in voice search, are just part of the problem and have secondary importance in our study.
Among the state-of-the-art web search engines that support voice search, we adopted the Google search app on the iPad for our experiment because of the popularity of Google in conventional web search. We believed that users with Google search experience could more easily understand its voice search function. In addition, using the iPad for experiment also replicated some form of mobile search environment.
As our study focus on query reformulation behaviors in voice search, we simply adopted Google voice search as an out-of-box system, despite it is unclear how the voice search system and its ASR were implemented. Although the voice search system and the effectiveness of ASR can influence experiment results, we believe that Google voice search system is probably the best choice for this experiment and the experiment results would be still representative of users in other voice search systems.
Figure 1 contains screenshots of the system2. As shown in Fig.1 (a), a user can touch the voice search icon to issue voice queries. If the user stops speaking or pauses for a while, the system concludes that the user has completed the voice query. Then it starts the recognition of the voice query and uses the transcribed query to search (see Fig.1 (b)).
Google voice search system provides different audio cues to indicate its various statuses, which includes: starting or stopping "listening" a voice query; displaying the transcribed query; and failing to generate the transcribed query. These audio cues are very useful in our transcriptions of the experiment recordings.

(a)

(b)

(c)

Figure 1. Screenshots of the Google search app on iPad.

3.2 Search Tasks and Topics
Our experiment setting is similar to the one adopted by the TREC session track [17], in which users can issue multiple queries to work on one search topic.
Ideally, search topics should be representative of users' information needs in the mobile search environment. However, as discussed in Section 3.1, our experiment setting was not a real mobile environment, therefore we selected conventional TREC ad hoc search topics in our experiments. On the one hand, we could not find many mobile search topics due to limited resources. On the other hand, we also wanted to study the connections between query reformulation in voice search and those in conventional

2 The screenshots were made in January 2013, 6 months after the experiments. However, the main system features did not change.

144

textual queries as part of our future study. Therefore, we selected 50 TREC topics for our study, of which 30 are from the TREC robust track in 2004, and 20 are from the TREC web track in 2009 and 2010. The selected topics were representative of informational search problems [4]. Table 1 shows the selected TREC topic numbers.
Although the literature shows that many searches on mobile devices involve location-related information needs [19], we did not want to restrict our findings by not including other types of information needs. The first reason is that there is no absolute demarcation line between mobile devices and portable computers. The second is that many voice search systems such as Google can be used on laptop and desktop computers.

Table 1. Selected topics for experiments.

Datasets
Robust Track 2004

Selected Topics
301, 302, 303, 307, 309, 311, 313, 314, 316, 318, 321, 322, 338, 348, 351, 356, 365, 380, 404, 406, 608, 628, 630, 637, 647, 651, 654, 672, 683, 698

Web Track 51, 52, 54, 56, 68, 70, 72, 73, 74, 91, 94, 100, 2009, 2010 104, 107, 108, 110, 112, 113, 122, 141

3.3 Experiment Procedure
We recruited 20 participants (14 females and 6 males). The majority of them were college students (13) and graduate students (3). All 20 participants were native English speakers, and their average age is 23.7 with standard deviation being 4.72.
Each participant was compensated financially for their involvement in the experiment, which lasted for about 1.5 hours. At the beginning of the experiment, each participant was trained to work on one TREC topic (other than the 50 topics in table 1) to make sure that they knew how to use the voice search system, and were clear about what operations they were allowed to do during the experiments.
They then each worked on 25 of the 50 topics listed in Table 1. We alternated the topic assignments to reduce learning and fatigue bias. For each topic, the participant first read the topic description on a computer screen and then worked on each topic using the Google voice search app on iPad for 2 minutes. The participant could only vocalize queries, browse and click the search results, and use Google's query suggestions. The participant was not allowed to type queries on the iPad touch screen. After each topic, the participants were asked to answer a short questionnaire regarding their perceptions of topic difficulty and familiarity.
The experiment paused for a 5-minute break after the participant finished 15 topics. When all 25 topics were completed, each participant was interviewed for about 10 minutes on his/her perceptions of the voice search and query reformulation. The whole experimental process was recorded for later transcription and analysis of users' voice queries and interviews.
3.4 Data
Two coders manually transcribed the voice queries and agreed on 100% of the transcribed texts except for the use of plurals and prepositions (which are difficult to identify and usually do not affect search results after stemming and stopword removal).
Google's search history automatically records the system's transcribed queries and the users' click-through pages. For each participant, we created a Google user account and recorded the user's search history during the experiments.
Each participant went through a semi-structured interview at the end of the experiments on their opinions of using voice search systems and especially on how they constructed and reformulated voice queries. Some of the interview questions were based on our own experience of using voice search and a pilot study. As the

study was highly exploratory, we also developed new interview questions for the experiments. We hired a professional transcription company to transcribe the interview texts.
The experiment was conducted in July 2012. In total, we collected 1,650 voice queries and 32 cases of using query suggestions. On average, each subject issued 3.30 voice queries per topic (SD=2.50). Among the 1,650 voice queries, 742 were correctly recognized. Voice input error happened in 908 voice queries, of which 810 were caused by speech recognition error and 98 by system interruption. We also found 42 voice queries for which the system could not provide any transcription results. For these queries, we simply counted their transcribed queries as empty strings and their search results as empty lists. These voice queries and query suggestions provided us with 1,182 query reformulation pairs. The average number of results clicked by a user throughout the session of a search topic were 1.41 (SD=1.14). On average, for each topic, 9.76 unique clicked results were aggregated as qrels (SD=3.11).
3.5 Search Effectiveness Evaluation
For each topic, we assume that a set of topic-level relevant results can be collected to evaluate each query in the search sessions dealing with that topic. Such evaluation method was widely adopted in multi-query search session evaluation, e.g. [12­ 14, 16]. Due to the time limitation of the experiments, we did not ask the participants to make relevance judgments, but relied on the clicked results as relevant documents for evaluation. Similar methods were widely adopted in web search [15].
Due to the voice input errors, sometimes a participant will not be able to find any meaningful results within the 2-minute session. Thus, for each topic, we aggregated the results clicked by any of the participants when they were working on that topic. Each clicked result was assigned a relevance score of 1 for that topic. Other results were considered non-relevant (relevance score is 0). On such a basis, we can calculate standard evaluation metrics such as nDCG of the queries.
Note that this method will be biased toward the transcribed queries in evaluation, because only those results retrieved by the transcribed queries have the chance to being clicked upon (i.e. some of the voice queries' results were not clicked upon because they were never shown to the participants). Thus, the evaluated effectiveness of the voice queries may be underestimated. However, this problem does not affect the validity of our study. As will be shown in Section 6, even if they are underestimated, voice queries still outperform their corresponding transcribed queries in nearly all the cases.
Google search history only records clicked results of queries. Thus, we crawled the first page of Google results for each of the voice queries and system transcribed queries. These results were accessed 5 months after our experiments. Although these results may be somewhat different from those at the time we conducted the experiments, we assume they do not influence the comparison between queries.
4. VOICE INPUT ERRORS AND REFORMULATION PATTERNS
4.1 Voice Input Error Identification
In voice search, a user speaks a voice query (qv), and the search system generates the transcription of qv for search, which is referred to as the transcribed query (qtr). We say a voice input error occurs when the actual content of a voice query qv differs from its transcribed query qtr. Let {qv(1), ... , qv(n)} be n voice queries, and {qtr(1), ... , qtr(n)} be the corresponding n transcribed

145

queries. The transition from qv(i) to qv(i+1) is referred to as a voice query reformulation.
Through comparison of manually transcribed voice queries with system transcribed voice queries, we can obtain recognition errors, which include:
Missing words: words in qv that do not appear in qtr. Incorrect words: words in qtr that do not appear in qv. When identifying recognition errors in this experiment, we did not consider the word differences caused by letter case (e.g. "United States" and "united states" are considered as equivalent) and plurals (e.g. "neil young tickets" is considered as equivalent to "neil young ticket"). The reason for this is that these types of errors do not have a significant impact on search results. In addition to the system's speech recognition errors, voice input errors can also be caused by the system's interruption of the participants' voice inputs. While vocalizing a query, if the participant pauses for a certain amount of time, the system will "think" that the participant has completed the query. So the system will stop listening to the participant's voice input and directly transcribe the unfinished voice query for search. This type of error can be reliably identified by listening to the recording. The participant would pause and then start to talk again but the system had already issued the audio cue for stopping listening. Therefore, we manually annotated each voice query with one of the following four categories, two of which indicate the voice input error type: Speech Recognition Error: the participant completed a query without any interruption, but the voice query was not recognized correctly. This error can be characterized by missing words or/and incorrect words as mentioned earlier. System Interruption: the participant was improperly interrupted by the system and failed to speak all of the query words. No Error: no voice input error. Query Suggestion: the participant used a Google's query suggestion. If the search history recorded that the participant searched for a query while we did not hear it in the recording, we consider that to be a case of using Google's query suggestion. During the annotation of voice input errors, the two coders agreed on 100% of the voice queries' category types. Because the participants usually stopped speaking when system interruption happened, we cannot determine the unspoken contents of the queries (i.e. for queries with system interruption, we can only have information on qtr but not qv). Thus, in much of the later analysis that requires the information on qv, we mainly focus on queries without voice input errors and those with speech recognition errors.
4.2 Voice Query Reformulation Patterns
As voice queries have both lexical and phonetic characteristics, voice query reformulation can incorporate not only textual changes to the query but also phonetic changes. Thus, voice query reformulation can have lexical query reformulation, phonetic query reformulation or both. In the remainder of this section, we will discuss the patterns of voice query reformulation, which were summarized from previous works [9] and our observations on the experiment's results.
4.2.1 Lexical Query Reformulation
Expanded from previous studies [9], we characterized lexical query reformulation into addition, substitution, removal, and reordering of words, or the combination of these patterns. Although these patterns also exist in conventional search, users may utilize them for different reasons in voice search.
Addition (ADD): adding new words to the query. We refer to the newly-added words as ADD words. For example:

Voice Query

Transcribed Query ADD words

q1 the sun

the son

q2 the sun solar system the sun solar system solar system

Substitution (SUB): replacing words with semantically-related words. In voice search, we noticed that users may substitute the words that were incorrectly recognized with other words of similar meanings. We refer to the words being replaced and the new words as SUB words. For example:

Voice Query Transcribed Query SUB words

q1 art theft

test

q2 art embezzlement are in Dublin

q3 stolen artwork stolen artwork

theftembezzlement embezzlementstolen artartwork

Different from the substitution pattern in [9], we also count "acronym", "abbreviation", and "word stemming" in [9] as word substitution patterns, for example:

avp  association of volleyball professionals united states  us ireland peace talk interruption  ireland peace talk interrupted

Removal (RMV): removing words from the query. In voice search, we noticed that the participant may remove a part of a voice query, if the part was not correctly recognized and was not essential to the search topic. The words being removed are referred to as RMV words. For example:

Voice Query

Transcribed Query RMV words

q1 advantages of same sex schools andy just open it goes

q2 same sex schools

same sex schools

advantages of

Re-ordering (ORD): changing the order of the words in a query. The words being re-ordered are referred to as ORD words. In voice search, we noticed that the words being re-ordered are usually those wrongly recognized. For example:

Voice Query

Transcribed Query

q1 interruptions to ireland peace talk is directions to ireland peace talks q2 ireland peace talk interruptions ireland peace talks interruptions

4.2.2 Phonetic Query Reformulation

Phonetic query reformulation is unique in voice search. During

our transcription of experiment recordings, we found the

following human recognizable phonetic query reformulation

patterns:

Partial Emphasis (PE). Partial emphasis refers to the behavior

of phonetically emphasizing a part of the current query that also

appeared in the previous query. Typically, the users can put stress

(STR) on certain words, or slow down (SLW) at these words, or

use both. Sometimes the users may only emphasize a vowel or

consonant in the word. We also noticed other ways of

emphasizing words when speaking voice queries. For example,

some users spell out each letter in the word (SPL), or try different

pronunciations (DIF) for some non-English words (e.g. Puerto

Rico). Overall, STR and SLW are the two primary patterns of

partial emphasis, whereas SPL and DIF occurred rarely in our

experiments. The recurring words being emphasized during

speaking are referred to as PE words. We use the following

methods to represent the PE methods:

PE STR SLW SPL
DIF

Example
rap and crime rap and c-r-i-m-e P·u·e·r·t·o Rico PuertoRico

Explanation put stress on "rap" slow down at "crime" spell out each letter in "Puerto"
pronounce "Puerto" differently

In voice search, we notice that the part of the query being emphasized is usually that part being incorrectly recognized in previous searches. For example:

146

Voice Query

Transcribed Query PE words

q1 rap and crime

rap and crying

q2 rap and c-r-i-m-e

rob and crime

q3 rap music influence rap music influence

crime rap

Whole Emphasis (WE). Whole emphasis is to place emphasis on every part of the query, usually by putting stress or slow down on each of the words. It usually happens when the majority of the previous query was wrongly recognized. For example:

Voice Query

Transcribed Query

q1 art embezzlement

are in dublin

q2 a-r-t e-m-b-e-z-z-l-e-m-e-n-t art embezzlement

We did not find other meaningful phonetic reformulation patterns other than PE and WE in our transcription.

4.2.3 Recognition of Query Reformulation Types
We recognize lexical query reformulation types by automatic and manual methods. Let q1q2 be a lexical query reformulation, then the procedures of recognizing the patterns are:
Step 1: automatically check whether all words in q1 also appear in q2. If yes, any extra words in q2 are recognized as ADD words, and q2 is an ADD of q1. Similarly, if all q2's words are in q1, any extra words in q1 are recognized as RMV words, and q2 is an RMV of q1.
Step 2: For the rest of the query pairs, check manually whether q2 contains SUB words of q1. The two coders agreed on 93.9% of the cases at the beginning, and finally came to agreements on the remaining 6.1% after further discussion.
Step 3: Compared with q1, if some newly appeared words in q2 are not recognized as SUB words, we mark them as ADD words and q2 as an ADD of q1. Similarly, if q2 removed some words in q1 and the removed words are not substituted by other words, we mark them as RMV words and q2 as an RMV of q1.
Step 4: Finally, if two words appeared in both q1 and q2, and their sequence was changed, we mark q2 as an ORD of q1.
Note that ADD, RMV, SUB, and ORD are not exclusive of each other. For example:

Reformulation
Reformulation Type & Words

q1: information retrieval system q2: search system development
ADD: development SUB: retrieval  search RMV: information

The phonetic query reformulation types and the PE words were manually recognized. In transcribing the recordings, we found that STR and SLW almost always happened together. Thus, we mark STR and SLW as one type "STR/SLW". Finally, we come to four exclusive phonetic reformulation patterns: STR/SLW, SPL, DIF, and WE. The two coders agreed on 87.6% of the cases at the beginning, and finally came to agreement on the remaining 12.4% after further discussion.
5. INFLUENCE OF VOICE INPUT ERROR

5.1 Voice Input Errors in Individual Queries

RQ1: How do speech recognition errors affect voice queries?

Speech recognition error is the major type of voice input error. It occurred in 810 voice queries (89.2% of all 908 queries with voice input errors in our study). We found that speech recognition error can greatly change the content and results of voice query, most likely hurting the performance of voice search.
At the word level, we calculated the average percentage of missing words in voice queries and the average percentage of incorrect words in transcribed queries. As shown in Table 2, when speech recognition error occurred, about half of the words (49.7%) in voice queries were missing in the transcribed queries. Similarly,

about half of the words (49.3%) in transcribed queries were incorrect transcriptions. On average there were 1.77 missing words and 1.84 incorrect words per query.
Such high proportions of missing words and incorrect words greatly affected the results of voice search. For each of the 810 voice queries with speech recognition errors, we calculated the Jaccard similarity of Google's first pages of results between voice query and transcribed query (i.e. Jaccard(qv, qtr) in Table 2). As shown in Table 2, the average Jaccard similarity was only 0.118, indicating very low overlap between those retrieved by the transcribed queries and those that should have been retrieved by the voice queries' true content. Figure 2(a) further illustrated the low overlap by showing the distribution of Jaccard similarity, which indicated that, for 69% (556 out of 810) of voice queries with speech recognition errors, the search results will be totally different from users' expectations (i.e. Jaccard similarity is 0).

Table 2. Comparison of voice queries that contained no errors, speech recognition errors, or system interruptions.

No Errors 742 Queries

Speech Recognition System

Errors

Interruptions

810 Queries

98 Queries

mean SD mean

SD

nDCG@10 of qv 0.275 0.20 0.264

0.22

nDCG@10 of qtr 0.275 0.20 0.083* 0.16

Length of qv 3.82 1.68 4.14*

1.99

Length of qtr 3.82 1.68 4.21*

2.31

# missing words -

- 1.77

1.09

# incorrect words -

- 1.84

1.44

% missing words -

- 49.7% 29%

% incorrect words -

- 49.3% 31%

Jaccard(qv, qtr) -

- 0.118

0.27

nDCG@10 -

- -0.182 0.23

mean SD

-

-

0.061 0.14

-

-

2.34 1.41

-

-

-

-

-

-

-

-

-

-

-

-

*: the difference between queries with no errors and recognition errors is significant at 0.01 level according to Welch t-test; : the difference
between queries with no errors and system interruptions is significant at 0.01 level according to Welch t-test; : the difference between qv and qtr under the same error conditions is significant at 0.01 level according to
paired t-test.

1.0 Jaccard 0.8

0.6 nDCG @10
0.4

0.2 0.6

0.0

0.4

1 101 201 301 401 501 601 701 801

-0.2

# of queries

0.2

-0.4

0.0

-0.6

1 101 201 301 401 501 601 701 801

# of queries -0.8

(a)

(b)

Figure 2. Jaccard similarity and nDCG@10 of the top 10 results of qv and qtr for 810 queries with recognition errors.
In addition, speech recognition errors hurt the performance of voice search significantly. As shown in Table 2, the average nDCG@10 of the 810 voice queries with speech recognition errors was 0.084. However, if all the speech recognition errors were corrected, the average nDCG@10 could be significantly improved to as high as 0.264, comparable to the average nDCG@10 of voice queries with no voice input errors (0.275).
Figure 2(b) further shows the distribution of nDCG@10 for the 810 queries (i.e. the difference of nDCG@10 between the transcribed query and the voice query). For 500 queries (62% of the 810), nDCG@10 declined. The remaining 310 queries, whose search performance was not hurt, were intrinsically inefficient queries. Even inputted correctly, these queries could only have an average nDCG@10 value of 0.117, which is significantly less than other queries. Therefore, these queries' performance was not

147

hurt probably because there was not much room to degrade their search performance.
RQ2: How do system interruptions affect voice queries?
System interruptions occurred in 98 queries (10.8% of all 908 queries with voice input errors), which also greatly altered the content of queries and hurt the performance of voice search. When system interruption occurred, it was impossible to determine the real content of the voice queries. Therefore, we calculated statistics only for the transcribed queries.
Compared with the 742 correctly recognized voice queries, the 98 queries with system interruptions performed significantly worse (0.061 vs. 0.275 in average nDCG@10). When system interruption occurred, the transcribed queries were also significantly shorter than those of the correctly recognized queries (2.34 vs. 3.82 words), probably because the users were interrupted improperly and were not able to vocalize the entire query words.
RQ3: When do speech recognition errors happen?
We found that query length may be one factor related to speech recognition errors. As shown in Table 2, queries with speech recognition errors were significantly longer than those correctly recognized queries (4.14 vs. 3.82 words). On the one hand, this is not surprising: as recognition error may happen in any word of a voice query, the more words spoken, the more likely an error happens. On the other hand, the longer the query, the richer the context it provides, which may help the speech recognition. Therefore, further study is needed on whether or not query length can affect speech recognition errors.
We also explored the relationship between speech recognition errors and certain types of words. We calculated recognition error rates for the words used by the participants, which is defined as the number of times a word was not recognized correctly divided by the total number of times the word was used in voice queries. We only calculated error rates for words being used at least 10 times. Table 3 shows the categorization of the 20 words with the highest recognition error rate.
The first recognizable category of words with high recognition error rates are acronyms, such as "ER" (emergency room, a TV show), "AVP" (the Association of Volleyball Professionals), US and USA. One can hardly expect the system to recognize certain obscure acronyms, such as "ER" and "AVP".
Our interviews showed that more than half of the participants (N=14) reported their concerns about the use of acronyms. When the acronyms were not recognized, they tended to reformulate queries using the full words. For example, participant S14 said that "I was a little concerned ... Like how I said AVP, and it pops up APP, which would be a totally different topic. I was a little worried about that ... Once I realized what AVP was, I tried to use association, the full name. [sic]". Participant S20 said that "When I did the NRA, instead of giving me a single letter, N-R-A, it spelled out `in' like that. Then I just switched over to actually saying the National Rifle Association because that was quicker."
Acronyms, named entities and non-English words comprise half of the top 20 words with the highest error rates. Examples of the uncategorized words are also listed in Table 3 as "other words".
5.2 Voice Input Errors in Search Sessions
RQ4: How do voice input errors influence search sessions?
We collected 500 search sessions (20 participants with each working on 25 topics). We divided the 500 sessions into two groups by whether or not voice input errors occurred in the session. As shown in table 4, voice input errors occurred in 187 sessions.

Table 3. Categorization of 20 words with the highest recognition error rates.

Type Acronym Named Entity
Non-English Other words

Examples (# NOT recognized correctly / # used)
ER(29/29), AVP(11/11), US(57/61), USA(6/11)
Owen(25/26), Culpeper(18/27), Ralph(22/36), Gulf(13/24), Falkland(14/27)
Nino(31/46)
theft(14/14), achievement(10/10), taxing(18/21), fraud(12/14), violence(19/27), talk(9/15), sun(24/41), aspirin(23/43), embezzlement(9/18), maglev(8/16)

Table 4. Comparison of session-level statistics between sessions with and without voice input errors.

187 Sessions 313 Sessions

w/o Voice

w/ Voice

Input Errors Input Errors

mean SD mean SD

# voice queries

1.44 0.82 4.41* 2.51

# unique voice queries

1.44 0.82 3.30* 1.87

# queries w/o voice input errors # queries w/ recognition errors

1.44 0.82 1.51 1.36

0

0

2.59* 2.14

# queries w/ system interruptions # unique results by qv # unique results by qtr
# unique relevant results by qv # unique relevant results by qtr # clicked results in the session

0 13.38 13.38 2.90 2.90 1.39

0

0.31* 0.65

6.66 26.69* 13.90

6.66 37.95* 21.00

1.56 3.04 1.59

1.56 2.78 1.71

1.01 1.34 1.23

% sessions user clicked results 84.49% - 69.97% -

% sessions qtr found relevant results 95.72% - 92.01% -

*: the difference between sessions w/ and w/o voice input errors is

significant at 0.01 level according to Welch t-test;  and : the difference

between qv and qtr is significant at 0.01 level according to paired t-test.

We found that, within the same period of time (a 2-minute search session), the participants issued significantly more voice queries when voice input errors occurred in the search session. As shown in Table 4, the average number of voice queries in sessions with errors was 4.41 and 1.44 without errors (the difference is significant). When voice input errors occurred in the search session, on average 1.11 queries in the session were repeating previously used queries, whereas when no errors occurred, users seldom repeated used queries. After removing the repeated queries, the participants still issued significantly more unique voice queries when voice input error occurred (3.30 vs. 1.44).
One consequence of the increased number of voice queries in sessions with voice input errors was that the participants had to spend more efforts to browse and examine the extra returned results. As showed in Table 4, the unique number of results returned by the transcribed queries in sessions with voice input errors was significantly higher than that of those without voice input errors. Although some of the participants could immediately reformulate the voice query without looking at any results, the increased number of returned results at least would not reduce the participants' search efforts.
We further looked into retrieval effectiveness of search sessions. In sessions with voice input errors, although more results were returned within a session, on average less unique relevant results were actually found. In the 313 sessions with voice input errors, on average the transcribed queries returned only 2.78 unique relevant results within a session. Whereas, if no voice input errors occurred, those sessions' voice queries should result in on average 3.04 relevant results (the difference is significant). Compared with the 313 sessions with voice input errors, the transcribed queries also returned more relevant results in the 187 sessions without any voice input error (2.90 vs. 2.78) and triggered more clicks (1.39 vs. 1.34), but the differences are not statistically significant.

148

Voice input error also has a higher likelihood of causing a failed search session, in which no relevant result were found. On average, 95.72% of the sessions without voice input errors returned at least one relevant result and in 84.49% of the sessions the participants clicked at least one result. In comparison, when voice input error occurred, only 92.01% of the sessions returned at least one relevant result and in 69.97% of the sessions the participants clicked at least one result.
In addition, voice input errors can also affect the participants' affective feelings. In our interviews, 90% of our participants reported frustration with their search experience when voice input

error occurred. For example, participant S15 reported: "It's frustrating! I know I'm saying the word right and I know what I'm looking for, but it's just not connecting, and that disconnection is like arrgh! ... (hope I can) just type it. [sic]".
To summarize, our results demonstrated that voice input errors significantly affected the performance of voice queries, and consequently made the whole search process more difficult and less effective. In response, users utilized both lexical and phonetic reformulations to handle the errors, which will be analyzed in the next section.

Table 5. Change in nDCG@10 after query reformulation.

qv(2)

No Error

Recognition Error

System Interruption

Query Suggestion

All

nDCG@10

nDCG@10

nDCG@10

nDCG@10

nDCG@10

No Error

qv

0.266  0.218  0.255  0.204

-

-

-

qtr

0.266  0.218  0.255  0.095  0.256  0.059  0.290  0.244

0.262  0.164 

# cases

209

143

27

15

394

qv(1)

Recognition Error
System

qv qtr Frequency
qtr

0.248  0.248 0.053  0.248 
231
0.071  0.237 

Interruption Frequency

30

0.261  0.267 0.058  0.074
392
0.038  0.085 56

0.096  0.062
44
0.134  0.012 7

0.099  0.226
14
0

0.059  0.135 
681
0.056  0.128  93

Query

qtr

0.299  0.100

Suggestion Frequency

4

0.189  0.020 6

0.235  0.000 1

0.233  0.110 3

0.233  0.061  14

All

qtr

0.150  0.233 

Frequency

474

0.104  0.079  597

0.156  0.056 79

0.201  0.223  32

0.129  0.143  1,182

 and : the difference between qv(1) and qv(2), or between qtr(1) and qtr(2), is significant at 0.01 level according to paired t-tests.

6. VOICE QUERY REFORMULATION
In this section, we focus on users' query reformulations. In the following discussion, we use qv(1) and qtr(1), qv(2) and qtr(2) for the voice query and transcribed query both before and after query reformulation, respectively.
6.1 Effectiveness
RQ5: Can users' query reformulations improve search performance of voice queries?
We found that query reformulation in voice search led to overall improvements in performance, but the magnitude depends on whether voice input errors occurred before or after reformulation.
Table 5 shows the comparison of search performance before and after query reformulation when different types of voice input errors occurred in qv(1) and qv(2). If counting all 1,182 cases of reformulation, search performance (as measured by nDCG@10) improved significantly from 0.129 to 0.143 (+10.85%) because of query reformulation. However, the improvements mainly occurred in the cases where voice input error occurred in qv(1) and qv(2) was correctly recognized, e.g. "Recognition Error"  "No Error" and "System interruption"  "No Error". If no voice input error occurred in qv(1) or voice input error occurred in qv(2), query reformulation resulted in limited improvements and it sometimes even hindered search performance.
Since results in Section 5 demonstrated the great influence of voice input errors on search performance, it is not surprising that the effectiveness of query reformulations also largely relied on whether or not voice input errors occurred in qv(2).
RQ6: Can users' query reformulation correct the speech recognition errors in previous queries?
We found that when recognition error occurred in qv(1), users' query reformulation corrected some of the missing words in qv(1). However, at the same time, new voice input errors could also

happen in qv(2), which may counteract the corrected errors and
finally lead to degradation in search performance.
Table 6 shows the missing and incorrect words before and after
query reformulation for 681 query reformulation cases in which speech recognition errors occurred in qv(1). We separately
calculated the statistics by the different types of queries and voice input errors in qv(2). As showed in Table 6, when no voice input error occurred in qv(2) (231 out of 681 cases), it is not surprising
that the number of missing and incorrect words both dropped to 0
after query reformulation. When speech recognition errors occurred in qv(2) (392 out of 681), the number of missing words
only dropped slightly from 1.89 to 1.74 (the difference is
significant at 0.05 level of significance) and the number of
incorrect words slightly increased (the difference is not
significant).
Does this mean users' query reformulations can only correct
voice input errors when the reformulated queries are correctly
recognized? On the contrary, in further analysis, we found that even when speech recognition errors occurred again in qv(2), users' query reformulation did correct parts of the errors in qv(1). However, at the same time, new errors also appeared in qv(2).
To better explain the case, we calculated: the number of missing words in qv(1) that were correctly recognized in qtr(2); the number of missing words in qv(1) that were removed in qv(2); and the number of new missing words in qv(2) (those are missing words in qv(2) but not in qv(1)). As shown in Table 6, when speech recognition error occurred in qv(2), 27.5% (0.52 out of 1.89) of the missing words in qv(1) were corrected after query reformulation
and 18.0% (0.34 out of 1.89) were simply removed. However, on average, 0.72 new missing words were produced in qv(2), which
still impeded the performance. When system interruption occurred in qv(2), on average, only
0.23 missing words in qv(1) were corrected, which is significantly

149

less than the 0.52 missing words corrected in the cases in which speech recognition error occurred in qv(2).

Table 6. Comparison of the missing and incorrect words
before and after query reformulation for the 681 query pairs in which speech recognition error happened in qv(1).

qv(2)

# missing words
qv(1)  qv(2)

# incorrect words
qtr(1)  qtr(2)

# missing
words in qv(1)
corrected in qtr(2)

# missing
words in qv(1)
removed in qv(2)

# new
missing
words in qv(2)

No Errors 1.75  0.00** 1.81  0.00** 1.13

0.61 0.00

Rec Errors 1.89  1.74* 1.72  1.78

0.52

0.34 0.72

Sys Interrupt 1.71

-

0.23

-

-

Suggestion 1.14

-

0.86

-

-

* and **: the difference of qv(1) and qv(2) is significant at 0.05 and 0.01 level.

Table 7. The frequencies of using reformulation patterns.

qv(1)

ADD SUB RMV ORD Lexical

No Errors 90.50 % 15.04 % 66.75 % 33.51 % 99.74 % Rec Errors 32.98 % 16.34 % 37.93 % 43.03 % 77.36 %
Overall 53.82 % 14.87 % 48.37 % 39.58 % 85.47 %

qv(1)

STR/ SLW

SPL

DIF

WE Phonetic

No Errors

0 %

0 %

0 % 0.26 % 0.26 %

Rec Errors 14.84 % 0.60 % 0.90 % 9.30 % 25.64 % Overall 9.46 % 0.39 % 0.57 % 6.02 % 16.44 %

6.2 Use of Reformulation Patterns

Lexical & Phonetic
0.26 % 11.99 % 7.74 % Repeat w/o PE or WE
0 %
20.54 % 13.58 %

RQ7: How do users utilize different query reformulate patterns in voice search? Do voice input errors influence the use of query reformulation patterns?

Table 7 shows the frequency of using different reformulation patterns in voice search. Despite how the query input mechanism changes dramatically in voice search, lexical reformulations were still the primary forms of query reformulation. No matter if speech recognition errors occurred, lexical reformulations were consistently used much more frequently than phonetic reformulations.
However, speech recognition errors did significantly affect the use of specific lexical query reformulation patterns. When speech recognition errors occurred, the participants tended to reformulate queries using more substitution (SUB) and re-ordering (ORD) patterns but dramatically less addition (ADD) and removal (RMV) patterns. As further examined in RQ8, this is probably because substitution and re-ordering can effectively correct the missing words in previous queries, whereas addition and removal cannot.
The use of phonetic reformulation patterns is almost always associated with speech recognition errors. As shown in Table 7, when no voice input error occurred in qv(1), only 0.26% of the query reformulations adopted phonetic reformulation patterns. In comparison, 25.64% of the query reformulations adopted phonetic reformulation patterns when speech recognition errors happened in qv(1). In addition to the phonetic reformulation patterns, repeating is also closely connected with speech recognition errors. When speech recognition errors occurred in qv(1), we found that 20.54% of the reformulations were simply repeating qv(1) without any recognizable phonetic changes.
Among all of the phonetic reformulation patterns, partial emphasis (PE) was used more frequently than whole emphasis (WE). As we mentioned in Section 4, stressing (STR) and slowing down (SLW) were the most frequent patterns for partial emphasis, while spelling (SPL) and using different pronunciations (DIF)

rarely happened. Repeating was used as frequently as phonetic reformulation patterns when recognition errors happened in qv(1).
To conclude, our results indicate that in voice search, a user's adoption of both lexical and phonetic query reformulation patterns were greatly impacted by voice input errors. As further illustrated in RQ8, many of the reformulation patterns were used specifically to correct the missing words occurred in previous queries.
RQ8: How do users utilize different reformulation patterns to handle speech recognition errors? Are these patterns effective in correcting speech recognition errors?
When speech recognition errors happen, it is very common for some of the words spoken by the users to be incorrectly recognized or missing from the system's transcribed queries. Solutions to speech recognition errors should be able to effectively correct these errors. Among the lexical and phonetic query reformulation patterns summarized in our paper, four patterns can be used specifically related to the missing words: substitution (SUB), removal (RMV), re-ordering (ORD), and partial emphasis (PE). Users can substitute other words for the missing words, or remove the missing words, or re-order the missing words and other words, or phonetically emphasize the missing words. In comparison, the other patterns affect equally the missing words and other words in the query.
We evaluate the reformulation patterns by their effectiveness of correcting the missing words in voice queries. Similarly, we can evaluate by their effectiveness of reducing the incorrect words in transcribed queries. However, due to space limitation, we only reported the following measures regarding the missing words:
(1) For each of the four patterns that can be used specifically for handling the missing words (i.e. SUB, RMV, ORD, and PE), we calculated the percentage that the pattern was used specifically related to the missing words (i.e. the missing words were substituted, removed, re-ordered, or emphasized) out of all the cases that the reformulation pattern was used.
(2) The success rate of each pattern in correcting the missing words. For re-ordering (ORD) and partial emphasis (PE) patterns, the success rate was calculated as the percentage of missing words being corrected out of all the cases that the missing words were re-ordered or specifically emphasized. For addition (ADD), whole emphasis (WE), and repeating patterns, the success rate was calculated as the percentage of missing words being corrected out of all the cases that ADD, WE, or repeating was used (since it is difficult to identify whether these patterns were used specifically on the missing words). For substitution, the success rate was calculated as the percentage of the replaced words being correctly recognized out of all the cases that the missing words were replaced.
(3) The improvement in nDCG@10 between qtr(1) and qtr(2) when each pattern was used.
As shown in Table 8, the percentage of the patterns used specifically related to the missing words indicates users' adoption of the pattern to solve speech recognition errors. Among all of the patterns, partial emphasis (PE) has most usage. When PE was used, it was nearly always (93.69%) the case that the words emphasized were the missing words from qv(1). In comparison, substitution (SUB), removal (RMV), and re-ordering (ORD) patterns have fewer but still considerably high usage (84.30%, 62.82% and 75.23%). Results indicate that, when recognition errors happened, these lexical patterns were primarily used to correct speech recognition errors, which is different from the intention to use these patterns in conventional searches.
Table 8 also reveals the effectiveness of different reformulation patterns in correcting speech recognition errors. As indicated in

150

the results, different reformulation patterns vary widely in their success rates in correcting missing words in previous queries. Among these patterns, substitution (SUB) and re-ordering (ORD) had the two highest success rates (73.5% and 69.1%). In comparison, partial emphasis (PE) was less effective (62.5%). It is indicated that when recognition errors happened, it was usually more effective to modify the missing words into others (SUB) or to change the contexts around the missing words (ORD), rather than emphasizing with phonetic changes (PE).

Table 8. Effectiveness of reformulation patterns in correcting speech recognition errors that occurred in previous queries.

% used Success

specifically rate of related to correcting the missing missing

nDCG@10 qtr(1)  qtr(2)

words in qv(1) words

ADD

-

40.73 % 0.085  0.119

SUB 84.30 % 73.53 % 0.052  0.156 

RMV 62.82 %

-

0.077  0.111

ORD 75.23 % 69.14 % 0.062  0.147 

PE 93.69 % 62.50 % 0.022  0.150 

WE

-

60.94 % 0.028  0.110 

Repeat w/o PE and WE

-

59.73 % 0.051  0.142 

Overall

-

47.45 % 0.058  0.132 

: the difference of nDCG@10 is significant at 0.01 level according to

paired t-tests.

We suspect that users' adoption of partial emphasis (PE) is

directly related to their everyday life experience: when others

miss your words, it is natural to repeat and emphasize the missing

part. However, it seems that this method cannot work well for

automatic speech recognition systems. The speech recognition

algorithms are usually trained with samples of the normal way of

speaking, but the phonetic query reformulations may make the

queries quite different from the normal way of speaking.

According to the success rates, partial emphasis (PE), whole

emphasis (WE), and repeating effectively helped to correct the

missing words (compared to the overall success rate of only

47.45%). However, we suspect that the effectiveness of the

phonetic reformulation patterns is over-estimated. Compared with

repeating, the phonetic patterns emphasized either certain parts of

the queries or the entire queries. Therefore, we can use repeating

as a baseline to evaluate the effectiveness of phonetic emphasis.

However, as partial emphasis (PE) and whole emphasis (WE) had

only slightly higher success rates compared to repeating, it is

arguable whether or not phonetic emphasis was truly useful.

Finally, we looked into the improvement of the transcribed

queries' search performance (by nDCG@10) after each pattern

had been used in reformulated queries. Except for addition (ADD)

and removal (RMV), we observed significant improvements with

other patterns. In addition, the magnitude of nDCG@10

improvements for other patterns was also greater than those of

ADD and RMV patterns. This indicates that ADD and RMV are

less effective solutions to speech recognition errors.

To conclude, we found that substitution, re-ordering, partial

emphasis, whole emphasis, and repeating were five effective

reformulation strategies in voice search to handle recognition

errors. Among these patterns, substitution and re-ordering are

lexical patterns, but they outperformed the other three phonetic

patterns in solving speech recognition errors.

7. DISCUSSION AND FUTURE WORK

(1) Should we use and support long and natural language queries or short and keyword queries in voice search?
Our results show that query length is an important factor associated with speech recognition errors (see Table 2 and

discussion in RQ3). Long queries are prone to speech recognition errors. This reminds us of the different findings in previous studies: Schalkwyk et al. found that voice search queries were tend to be shorter than in conventional searches [19], whereas Crestani et al. found that voice queries tend to longer and more similar to natural language [6].
Since we did not conduct conventional search experiments for comparison, we cannot come to an answer to this disputable issue. We suggest that further studies are needed to identify the characteristics of queries in voice search. We believe that users' adoption of short or long queries depends on various factors. On the one hand, as voice search may be closer to people's normal ways of speaking, voice queries are probably also closer to natural language queries. On the other hand, as long queries may have more speech recognition errors, users may also prefer shorter and simpler keyword queries in voice search.
(2) Query suggestion in voice search. Although the participants were told explicitly that they could use Google's query suggestions in our experiment, we did not observe many cases of them doing so (see Table 5). We tried some cases in Google and found that currently, Google's query suggestion in voice search is simply suggesting queries based on the transcribed queries' texts. Therefore, it is not surprising that the suggestions are ineffective when the transcribed texts are likely to be incorrect (due to voice input errors). For example, we submitted an incorrect transcription "rap and crying" (the correct one is "rap and crime") to Google and obtained two suggestions that are irrelevant to "rap and crime" but probably relevant to "rap and crying": "rapper crying at bet awards" and "soulja boy crying". This shows that query suggestion is more challenging in voice search. In addition, we believe that query suggestion is more important for users in voice search than in conventional search. As shown in our results, despite various query reformulation methods have been developed, users' voice query reformulations might not totally resolve the old recognition errors, and at the same time could introduce new errors. In comparison, it may be a better solution for users to accept a good query suggestion for query reformulation. This calls for studies on query suggestion algorithms specifically designed for voice search. Probably a promising solution is to develop effective query suggestion algorithms considering not only the transcribed texts, but also speech recognition results.
(3) Interface for supporting voice query inputs and voice query reformulation.
Considering the effort and risk of issuing a voice query, voice search systems should employ proper methods to reduce the efforts and risks of constructing and reformulating voice queries. Based on our observation, one suggestion is to design a voice query reformulation interface that frees users from having to speak the whole voice query again if they only intend to correct one or two error words. For example, the users should be given the ability to specify and repeat the part of the query that they want to modify and let the search system recompose a new voice query based on the updated information.
In addition, our experiments also shown that system interruptions greatly harmed the performance of voice search, even though they occurred less frequently (see Table 2 & 5). The participants could not finish their voice queries, and sometimes became really frustrated after several consecutive interruptions. Voice query generation may impose higher cognitive load on the users than typing textual queries. Therefore, voice search systems should better manage their interruptions. For example, systems

151

can allow users to control whether or not they will be interrupted while speaking voice queries.
8. CONCLUSION
In this paper, we studied two significant and closely related issues in voice search. First, what is the influence of voice input errors on search effectiveness in voice search? Second, how do users utilize different query reformulation patterns, including both lexical and phonetic query reformulation patterns, to handle these voice input errors? We conducted a controlled laboratory experiment for voice search, which helped answer these questions.
Our study systematically evaluated the influence of voice input errors on voice search from the aspects of individual queries and overall search sessions. We found that voice input errors greatly changed the content and results of queries, resulting significant decline of search performance for individual queries. This in turn led to increased efforts and negative feelings of users, hindering overall performance of the search session. In addition, current query suggestion algorithms may fail to generate effective suggestions due to voice input errors in transcribed queries.
Then, we characterized users' query reformulation patterns in voice search and evaluated the effectiveness of those patterns in handling voice input errors and improving search effectiveness. We found that users utilized both lexical query reformulation patterns that exist in conventional search and phonetic query reformulation patterns newly found in voice search. Despite some of the patterns effectively corrected voice input errors, users' query reformulation resulted in limited overall improvements in search performance, because voice input errors occurred frequently in reformulated queries.
Our study suggested voice input errors as the essential issue to be resolved in voice search. A possible solution is to better support users' query reformulation, which includes designing better interface supporting voice query reformulation and developing query suggestion algorithms using both lexical and phonetic information. To a broader extent, our study explored the influence of query input devices on user behaviors and search systems. Our methods and results may shed light on user behaviors and search systems in similar situations, such as when handwriting is used for input.
Admittedly, our study has one limitation in that the experiment setting did not fully replicate mobile search environment and tasks. This may influence the occurrences of the different types of voice input errors and users' adoption of the voice query reformulation patterns. However, it is very likely that the impacts of voice input errors on voice search systems and the effectiveness of different voice query reformulation patterns are representative of the cases in other voice search systems.
9. REFERENCES
[1] Anick, P. 2003. Using terminological feedback for web search refinement: a log-based study. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval (SIGIR '03): 88-95.
[2] Ballinger, B. et al. 2010. On-Demand Language Model Interpolation for Mobile Speech Input. Interspeech (2010): 1812­1815.
[3] Bates, M.J. 1979. Information search tactics. Journal of the American Society for Information Science, 30(4): 205­214.
[4] Broder, A. 2002. A taxonomy of web search. SIGIR Forum 36(2): 3-10.
[5] Crestani, F. 2002. Spoken query processing for interactive information retrieval. Data Knowl. Eng. 41, 1 (April 2002): 105-124.

[6] Crestani, F. et al. 2006. Written versus spoken queries: A qualitative and quantitative comparative analysis. J. Am. Soc. Inf. Sci., 57: 881­890.
[7] Dang, V. and Croft, W.B. 2010. Query reformulation using anchor text. In Proceedings of the third ACM international conference on Web search and data mining (WSDM '10): 4150.
[8] Feng, J. and Bangalore, S. 2009. Effects of word confusion networks on voice search. (Mar. 2009): 238­245.
[9] Huang, J. and Efthimiadis, E. N. 2009. Analyzing and evaluating query reformulation strategies in web search logs. In Proceedings of the 18th ACM conference on Information and knowledge management (CIKM '09): 77-86.
[10] Jansen, B.J. et al. 2005. A temporal comparison of AltaVista Web searching. J. Am. Soc. Inf. Sci., 56(6): 559­570.
[11] Jansen, B.J. et al. 2009. Patterns of query reformulation during Web searching. J. Am. Soc. Inf. Sci., 60(7): 1358­ 1371.
[12] Järvelin, K. et al. 2008. Discounted Cumulated Gain Based Evaluation of Multiple-Query IR Sessions. LNCS 4956: Proceedings of the 30th European Conference on Information Retrieval (ECIR '08): 4­15.
[13] Jiang, J. et al. 2012. Contextual evaluation of query reformulations in a search session by user simulation. In Proceedings of the 21st ACM international conference on Information and knowledge management (CIKM '12): 26352638.
[14] Jiang, J. et al. 2012. On Duplicate Results in a Search Session. Proceedings of the 21st Text REtrieval Conference, (TREC 2012).
[15] Joachims, T. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '02): 133-142
[16] Kanoulas, E. et al. 2011. Evaluating multi-query sessions. Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval (SIGIR '11): 1053­1062.
[17] Kanoulas, E. et al. 2011. Session Track 2011 Overview. The 20th Text REtrieval Conference Notebook Proceedings (TREC 2011).
[18] Rieh, S.Y. et al. 2006. Analysis of multiple query reformulations on the web: The interactive information retrieval context. Information Processing & Management. 42(3): 751­768.
[19] Schalkwyk, J. et al. 2010. "Your Word is my Command": Google Search by Voice: A Case Study. Advances in Speech Recognition SE - 4. A. Neustein, ed. Springer US. 61­90.
[20] Song, Y.-I. et al. 2009. Voice search of structured media data. 2009 IEEE International Conference on Acoustics, Speech and Signal Processing (Apr. 2009): 3941­3944.
[21] Teevan, J. et al. 2007. Information re-retrieval: repeat queries in Yahoo's logs. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR '07): 151-158.
[22] Wang, X. et al. 2008. Mining term association patterns from search logs for effective query reformulation. In Proceedings of the 17th ACM conference on Information and knowledge management (CIKM '08): 479-488.
[23] Wang, Y.-Y. et al. 2008. An introduction to voice search. Signal Processing Magazine, IEEE.

152

An Information-Theoretic Account of Static Index Pruning

Ruey-Cheng Chen
National Taiwan University 1 Roosevelt Rd. Sec. 4 Taipei 106, Taiwan
rueycheng@turing.csie.ntu.edu.tw

Chia-Jung Lee
University of Massachusetts 140 Governors Drive
Amherst, MA 01003-9264
cjlee@cs.umass.edu

ABSTRACT
In this paper, we recast static index pruning as a model induction problem under the framework of Kullback's principle of minimum cross-entropy. We show that static index pruning has an approximate analytical solution in the form of convex integer program. Further analysis on computation feasibility suggests that one of its surrogate model can be solved efficiently. This result has led to the rediscovery of uniform pruning, a simple yet powerful pruning method proposed in 2001 and later easily ignored by many of us. To empirically verify this result, we conducted experiments under a new design in which prune ratio is strictly controlled. Our result on standard ad-hoc retrieval benchmarks has confirmed that uniform pruning is robust to high prune ratio and its performance is currently state of the art.
Categories and Subject Descriptors
H.1.1 [Systems and Information Theory]: Information theory; H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.4 [Systems and Software]: Performance evaluation (efficiency and effectiveness)
Keywords
Static index pruning; principle of minimum cross-entropy; model induction; uniform pruning
1. INTRODUCTION
Kullback discussed one famous problem in his seminal work [14] about inducing a probability measure based on some previous measurement. When one has some initial hypothesis about a system and seeks to update this measurement incrementally, she needs to choose a new hypothesis from a set of feasible measures that best approximates her current belief. Here, the difficulty lies in defining the notion of closeness in the probability space. While at the time this was an important issue in everyday probabilistic modeling, a genuine solution had yet to come.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

To answer the question he had raised, Kullback introduced a method called "minimum discrimination information," or in the recent literature known as the principle of minimum cross-entropy. This approach has later become one of the most influential inductive principles in statistics, and also has benefited numerous fields, including some subareas in information retrieval [15,20]. Kullback's solution was simple and elegant: One shall choose a measure that most closely resembles the previous measurement in terms of KullbackLeibler divergence. Specifically, this is equivalent to solving the following optimization problem, given some prior measure p and a set of feasible measures F:

minimize subject to

D(q||p) q  F.

(1)

In this paper, we apply this induction framework to a classic problem in information retrieval, called static index pruning. Static index pruning is a task that reduces the index size for improving disk usage and query throughput [1]. Size reduction is done by removing index entries. Generally, the aim in static index pruning is to find a subset of index entries that best approximates the full index in terms of retrieval performance. This aspect, as we will show later, is closely related to model induction.
One key assumption in this paper is that an inverted index is a nonparametric, conditional distribution of document D given term T , i.e., p(D|T ). This follows directly from Chen et al.'s definition [10], which allows us to measure the resemblance between two versions of inverted indexes the way we do probability distributions. Here, the following definitions put static index pruning into the framework of Equation (1):

· The prior distribution p is defined as the full (unpruned) inverted index.

· The set of feasible hypotheses F contains all the possible pruned indexes of p that have reached some given prune ratio . In other words, each element q  F is a pruned version of the original inverted index p.

This conception marks the very beginning of our quest for developing an efficient solution of static index pruning. Through analysis, we first show that static index pruning is essentially a combinatorial optimization problem. Nevertheless, in Section 3, we manage to obtain a weaker analytical solution that is practically operable by trading off some mathematical rigor. We found that, under appropriate assumptions, static index pruning reduces to a convex integer program. But this is not a good solution in general, since the number of variables in the convex program is linear to the

163

number of postings in the inverted index, which may easily exceed a few millions on any medium-sized text collection. That means this solution does not scale at all, even with the latest super-efficient convex solver.
We further attacked this problem using an alternative approach, called surrogate modeling. We created a surrogate problem that is easier to solve. As we will show in later sections, this analytical solution has pointed us to a general version of a simple pruning method called uniform pruning. Sharp-eyed readers might notice that uniform pruning is by no means a new invention. Uniform pruning was originally introduced to static index pruning in Carmel et al.'s paper as a baseline approach [9]. In a preliminary experiment, Carmel et al. compared this method with their termbased pruning method. Using TF-IDF as the score function, they found that, even though term-based method performed slightly better, in general the performance for both approaches was roughly comparable. While this was indeed a very interesting finding, the exploration was discontinued as they went ahead to study other important issues.
To the best of our knowledge, since then uniform pruning has not been studied in any follow-up work. It is easy to see why this has been the case. The lack of control on one experiment variable, prune ratio, has made the performance result difficult to interpret. When we make comparisons between methods, this variable needs to be strictly controlled so that the comparisons make sense. Nevertheless, very few in the previous work adopted this design. As a result, there was no obvious way to conduct any form of significance testing to static index pruning. Without serious scrutiny--by which we mean significance assessment--it is only reasonable to dismiss uniform pruning, for that it seemed like an ad-hoc and maybe inferior approach.
In our study, the rediscovery of uniform pruning has gained us a second chance to rethink this issue. Our answer was a redesigned empirical study, in which prune ratio for each experimental method is strictly controlled to minimize the experimental error, and the performance is analyzed using multi-way repeated-measure analysis of variance. As we will shortly cover, the experiment result suggests that uniform pruning with Dirichlet smoothing significantly outperformed the other term-based methods under diverse settings.
The rest of the paper is structured as follows. Section 2 covers an overview to static index pruning and the relevant research. In Section 3, we motivate static index pruning in the minimum cross-entropy framework and show that the analytical solution leads to the uniform pruning method. An empirical study is given in Section 4. We put the theoretical and empirical evidence together and discuss the implication in Section 5. Section 6 delivers the concluding remarks.
2. RELATED WORK
In the coming subsections, we briefly review the literature and discuss the recent development of static index pruning. Following an overview, some notable pruning methods will be treated in slightly more details. Note that this is only aimed at providing enough background knowledge for the reader. A complete coverage is not attempted here.
2.1 Overview
The idea of static index pruning first appeared in the groundbreaking work of Carmel et al. [9] and has since garnered much attention for its implication to Web-scale re-

trieval [8, 11]. Static index pruning is all about reducing index size--by removing index entries from the inverted index. This technique was proposed to mitigate the efficiency issue caused by operating a large index, for that a smaller index loads faster, occupies less disk space, and has better query throughput. But since only partial term-document mapping is preserved, a loss in retrieval performance is inevitable.
Much effort has been driven towards developing importance measures of individual index entries, so that one can easily prioritize index entries on their way out of the index. Many such measures have been proposed and tested in various retrieval settings. One simple example is impact, the contribution of a term-document pair to the final retrieval score [8, 9]. Other approaches in this line include probability ranking principle (PRP) [7], two-sample two proportion (2P2N) [19], and information preservation (IP) [10]. Some measures assess only term importance [6], so the corresponding pruning algorithms can only choose between keeping the entire term posting list or not at all. Some others assess only documents importance [21].
2.2 Methodologies
Term-based pruning (or term-centric pruning) is proposed by Carmel et al. [9]. It was so named because it attempts to reduce the posting list for each term in the index. The basic idea is to compute a cutting threshold for each term, and throw away those entries with smaller impact values. Since the cutting threshold depends on some order statistics (i.e., the k-th largest impact value) about the posting list, term-based pruning is less efficient than the other methods.
In contrast to the aforementioned term-centric approach, document-centric pruning seeks to reduce the posting list for each document. Bu¨ttcher and Clarke [8] considered the contribution for term t to the Kullback-Leibler divergence D(d||C) between document d and the collection model C. This quantity is used to measure the importance of a posting. Analogously, for each document, a cutting threshold has to be determined based on some order statistics.
There are also other pruning strategies that focus on removing an entire term posting list (whole-term) or an entire document (whole-document) all at once. Blanco and Barreiro [6] presented four term-importance measures, including inverse document frequency (idf), residual inverse document frequency (ridf), and two others based on term discriminative value (TDM). They adopted a whole-term pruning strategy. Analogously, Zheng and Cox [21] proposed an entropy-based measure in a whole-document pruning setting. Both parties have reported comparable performance to term-based pruning on some standard benchmark.
Blanco and Barreiro [7] developed a decision criterion based on the probability ranking principle [18]. The idea is to take every term in the index as a single-word query and calculate the odd-ratio of relevance p(r|t, d)/p(r|t, d). This quantity is used in prioritizing all the term-document pair. Since there is only one cutting threshold determined globally, the implementation is relatively easy and efficient.
Thota and Carterette [19] used a statistical procedure, called two-sample two-proportion (2P2N), to determine if the occurrence of term t within document d is significantly different from its occurrence within the whole collection. Chen et al. [10] developed a method called information preservation. They suggest using the innermost summand of the conditional entropy H(D|T ) to measure predictive power

164

contributed by individual term-document pairs to the index model. This quantity is claimed easier to compute than the probability ranking principle.
Altingovde et al. [2] proposed an interesting query-view technique that works orthogonally with the aforementioned measure-based approaches. The general idea is to count the number of time a document falls within the top-k window of any given query collected from the query log. The count collected from a query is then evenly distributed to individual query terms. Thus the larger this number, the greater importance the posting is. The query view algorithm would later use this information to prune the entries.
Our work in this paper departs from the previous effort in three major ways. First, our approach is model-based, meaning that we infer a pruned model as a whole rather than partially. This is a novel approach in contrast to all the previous methods. Second, other information-theoretic approaches, such as Zheng and Cox [21] and Chen et al. [10], focused on minimizing the loss of information, while ours focused on minimizing the divergence from the full index. These are entirely different concepts in information theory. Three, our result on the uniform pruning method is more general than Carmel et al.'s description because we considered the query model p(t). Our take of uniform pruning is a weighted version, which may be useful when such a query model (e.g., session logs) is available.

3. MINIMUM CROSS-ENTROPY AND STATIC INDEX PRUNING

3.1 Problem Definition

Let us first develop some notation for describing an inverted index. Let T denote the set of terms and D denote the set of documents. We define an index entry (posting) as a 3-tuple of the form (t, d, n), where t  T , d  D, and n  N+ (i.e., n is a positive integer.) This means that "term t appears n times in document d." We further consider an inverted index as a probabilistic model p(D|T ; ) that takes a set of index entries  as parameters. This model is therefore nonparametric because the number of its parameters is not fixed. For brevity, in this paper we sometimes abuse the notation and use one symbol, e.g., , to represent both a distribution and its parameters.
In static index pruning, one seeks to induce a pruned model  from a full model 0 such that the following two constraints are satisfied: (i)  is a subset of the full model 0, and (ii) the size of  is 1 -  times the size of 0. Here, 0 <  < 1 denotes the prune ratio. Note that these constraints only specify what we need as the output from static index pruning, not how pruning shall be done. As there are exponentially many ways to prune an index down to a given ratio, it is natural to ask how does one engineer this decision to avoid excessive performance loss.
Now, to illustrate this point, let us assume the existence of a function g() that measures the retrieval performance of model . With this hypothetical construct, we formally define static index pruning as follows.

maximize g()

subject to   0

(2)

||/|0| reaches 1 - .

It is not difficult to envision static index pruning being formulated this way, as a constrained optimization problem.

For now, we shall focus on estimation of this hypothetical function. The conventional approach, as discussed in Section 2.2, is to devise an importance measure to take the role of g(·), which is expected to capture certain properties of an index relevant to retrieval performance. Yet one caveat is that sometimes we risk being arbitrary: The importance measure may only be empirically tested and does not necessarily come with any theoretical guarantee.
One simple idea that we had failed to see casted away all these doubts. We noticed the similarity between this formulation and Kullback's famous induction framework. As we replace g(·) in Equation (2) with the negative KullbackLeibler divergence (KL divergence), the static index pruning problem reduces to a model induction problem, written in a minimization form:

minimize D(||0)

subject to   0

(3)

||/|0| reaches 1 - .

In the following subsections, we shall develop a procedure to practically solve this optimization problem. For brevity, we write p(·) and p0(·), respectively, to denote the models parametrized by inverted indexes  and 0. The probability measures that we consider here are conditional distributions of D given T . To make this explicit, we define:

D(||0)  D(p(D|T )||p0(D|T )).

(4)

3.2 Assumptions
Before diving into the full analysis, we need to make explicit two important assumptions.

Assumption 1 (Query and Index Models). We can separate a joint distribution of D and T into a product of two models: (1) a distribution of T , called the query model, and (2) a conditional distribution of D given T , called the index model. We assume there is only one query model q(t) and it is independent of the index model in use. In other words, we have:
p(d, t) = p(d|t)q(t), p0(d, t) = p0(d|t)q(t).
Sometimes, we simply write p(t) or p0(t) to denote the query model when the meaning is clear in the context.

Assumption 1 simply states that the query model q(t) (or p(t)) has to be estimated from somewhere else. It makes little sense to infer a query model from the index.

Assumption 2 (Normalization Factor). Let p(t|d) and p0(t|d) be the conditional distributions of T given D for the induced and the original models, respectively. Let It,d be a binary variable that indicates whether an index entry (t, d, n) (for some n  N+) in the original model is retained in the induced model. We have p(t|d)  It,dp0(t|d)/Zd, where Zd is the normalization factor for document d.
In Assumption 2, we introduce a normalization factor Zd for each document d. As we shall address later, setting an appropriate value for Zd is the key step in the subsequent analysis. To correctly normalize p(t|d), we need to set:
Zd = It,dp0(t|d).
t

165

But this would make the resulting formula intractable, since the value of It,d depends on other variables in the same document, i.e., I·,d. To deal with this issue, we suggest setting Zd = k for all d  D, where k > 0 is some constant. Using this normalization trick results in weak inference and inevitably sacrifices mathematical rigors. We want to emphasize that this is a necessary compromise, without which the following analysis would not have been possible.

3.3 Analysis
Now, we shall go ahead and analyze the objective function. First of all, let us write out the objective in full:

D(p(D|T )||p0(D|T )) 

t,d

p(d,

t)

log

p(d|t) p0(d|t)

.

(5)

We use Assumption 1 to dissect the joint distribution p(d, t) into the product of the query model p(t) and the index model p(d|t). Applying Bayes Theorem to p(d|t) and p0(d|t) and assuming uniform p(d) and p0(d), we have the objective organized as follows:

p(t)

t

d

p(t|d) d p(t|d)

log

p(t|d) p0(t|d)

d d

p0(t|d) p(t|d)

.

(6)

Observe that, since in this optimization framework we look for a subset of 0, we are essentially dealing with a combinatorial problem ("assignment problem"). Each index entry (t, d, n)  0 either stays within the induced model  or gets removed. This combinatorial nature is best characterized via the indicator variables I·,· in Assumption 2.
Let us now replace all the occurrences of p(t|d). Note that, under the setting Zd = k (suggested), all the normalization factors cancel out. We have:

p(t)

t

d

It,dp0(t|d) d It,d p0(t|d)

log

It,d

d p0(t|d) . (7) d It,d p0(t|d)

As we separate the support of the inner summation over d into two subsets according to whether It,d is switched on, i.e., one over {d|It,d = 1} and the other over {d|It,d = 0}, the latter sub-summation disappears since 0 log 0 = 0. The resulting equation becomes:

p(t)

t

d:It,d =1

d

p0(t|d) It,d p0(t|d)

log

d

d p0(t|d) It,d p0(t|d)

.

(8)

Notice that the innermost logarithm does not depend on d anymore. We can therefore move that entire term out of the inner summation. From there, we have the inner summation over d canceled out. The equation is now written as:

p(t) log
t

d

d p0(t|d) It,d p0(t|d

)

.

(9)

We can get rid of the numerator, i.e., d p0(t|d), in the logarithm when minimizing this equation, because the numerator does not depend any combinatorial choice we make. Once again, we rewrite it as a maximization problem by taking the negation. The final form of static index pruning is expressed as the following:

maximize

t p0(t) log d It,dp0(t|d)

subject to It,d is binary, for all (t, d, ·)  0, (10)

t,d It,d = (1 - )|0|.

Input: a global threshold  begin
for t  T do for d  postings(t) do compute A(t, d) = p(t)p(t|d) if A(t, d) <  then remove d from postings(t) end end
end end
Algorithm 1: The weighted uniform pruning algorithm.

Equation (10) is in general ill-posed even though it can be solved with a convex integer program solver. This is because the number of index entries can easily exceed a few millions in any production retrieval system. Solving this exactly is only possible for very small test collections. To tackle this issue, we resort to a technique called surrogate modeling (or optimization transfer), which approximates the original objective using a majorization/minorization function that is analytically or numerically efficient to compute. See Lange et al. [16] for a comprehensive treatment.
To the best of our knowledge, there are two major approaches for inducing such surrogate models: taking the first-order Taylor approximation, or using the Jensen's inequality. In this paper, we stick with the second approach1. Recall that Jensen's inequality states that the following properties hold for any convex (or concave) function f :
Ef (X)  f (EX) (f is convex)
Ef (X)  f (EX) (f is concave).
Let f be the logarithmic function. The original objective in our problem (Equation 10) now corresponds to the lefthand side Ef (X). Since the logarithmic function is concave, we have the surrogate model f (EX) an upper bound of the original objective:

maximize log It,dp0(t)p0(t|d),

(11)

t,d

or equivalently:

maximize

It,dp0(t)p0(t|d).

(12)

t,d

This surrogate model has a simple analytical solution: Sort the index entries according to weighted query likelihood, i.e., p(t)p(t|d), and keep only the top (1 - )N entries. It can be shown that a simple maneuver such as Algorithm 1, called weighted uniform pruning, guarantees to maximize the objective. This corresponds to a weighted version of Carmel et al.'s uniform pruning method. This algorithm would fall back to the unweighted form when we supply a uniform p(t) and a plug-in estimate of the query likelihood. Note that the plug-in approach is valid only when the score function is proportional to the true likelihood.
For simplicity, we take a very loose definition of query likelihood in this paper so as to cover the well-known BM25 function. As we shall present shortly, the empirical result

1In our case, the first-order Taylor expansion leads to an even more sophisticated objective.

166

shows that the performance of BM25 is no worse than that of a rigorously defined language model (with Jelinek-Mercer smoothing). What is left unsettled is how to estimate  given a target prune ratio . This issue is treated in Section 4.2.

4. EXPERIMENT
Thus far, we have established the theoretical ground for uniform pruning. Our next quest is to find empirical evidence that supports this result. In the coming subsections, we shall briefly describe the experiment settings and present the experimental result in greater detail.
4.1 Setup
We used three test collections in this experiment: TREC disks 4 & 5, WT2G, and WT10G. The first two collections are tested against topics 401-450 and the latter against topics 451-550. For each topic, we tested both short (title) and long (title + description) queries. Details about the benchmark are summarized in Table 1. All three collections were indexed using the Indri toolkit2. To preprocess the documents, we applied the porter stemmer and used the standard 411 InQuery stoplist. No additional text processing is done to the test collections.
According to how index traversal is preferred, a pruning method can be either term-centric or document-centric. Since different traversal strategies rely on different index creation procedures, it is difficult to have both sets implemented in one place. For simplicity, in this experiment we focused only on term-centric methods. Specifically, we tested the following methods:

1. Uniform pruning (UP) [9]: This method is the subject of this experiment. In this experiment, we tested three variations of uniform pruning, each using a different score function. These functions are BM25 (UP-bm25), language model using Dirichlet smoothing (UP-dir), and language model using Jelinek-Mercer smoothing (UP-jm). For BM25, we used the standard setting provided by Indri. For language models, we set µ = 2500 in Dirichlet smoothing and  = 0.6 for the JelinekMercer smoother.

2. Top-k term-centric pruning (TCP) [9]: We set k = 10 as suggested to maximize the top-10 precision and used BM25 as the score function. Note that other score functions such as language models may also apply to this pruning method. Here, we simply comply with the previous work.

3. Probability ranking principle (PRP) [7]:

p(r|t, d) p(r|t, d)



p(t|D)p(r|D) p(t|r)(1 - p(r|D))

.

As suggested, we use the following equations to estimate these probabilities:

p(t|D) = (1 - )pML(t|D) + p(t|C), (13)

p(r|D)

=

1 2

+

1 10

tanh

dl

- Xd Sd

,

(14)

p(t|r) = p(t|C).

(15)

2http://www.lemurproject.org/indri.php

Collection Disks 4 & 5 WT2G WT10G

# Documents 528k 247k 1692k

Query Topics 401-450 401-450 451-550

Table 1: Test collections and the corresponding query topics.

Note that dl is the document length. Xd and Sd respectively are the sample mean and sample standard deviation of document length. For query likelihood, we set  = 0.6.

4. Information preservation, with uniform document prior (IP-u) [10]:

-

p(t|d)p(d) d p(t|d)p(d)

log

p(t|d)p(d) d p(t|d)p(d

)

.

In this formula, the query likelihood p(t|d) is estimated using Jelinek-Mercer smoothing. Here, We set  = 0.6 and assumed uniform document prior p(d).

We are aware that document-length update may improve the TCP and PRP retrieval performance [5,7]. Nevertheless, in this study we did not implement this feature. This shall be addressed in the future work.
4.2 Prune Ratio
In this experiment, we settled on 9 fixed prune levels at  = 0.1, 0.2, . . . , 0.9. To control the prune ratio, comparison is only allowed between experimental runs at the same prune level. In each reference method, the true prune ratio depends on some parameter (e.g.,  in TCP and PRP), which we called the threshold parameter. To reduce the index down to the right size, we employed two different approaches to determine this cutting threshold:

1. Sample percentile: Collect the prune scores on top of a sample of index entries and use the percentile estimates to determine the right cutting threshold. This is mostly useful when the prune score is globally determined. Here, we used Definition 8 from Hyndman and Fan [13] to estimate percentiles.

2. Bisection: Take an interval of feasible parameter values [a, b], and test-prune using the median value (a + b)/2. Return the current median if the test-prune reached the expected ratio; otherwise shrink the interval in half and repeat. This method is useful when the prune score for each index entry depends on the others in the same posting list, as in TCP.

Bisection requires several test-prune runs into the entire index and is therefore more time-consuming. Sample percentile needs only one pass through the index, but the resulting prune ratio can be less precise than that with the values learned using bisection. In this paper, we applied bisection to TCP to learn the parameter , and applied sample percentile to the rest of methods. Specifically, we used a sample size of 10% of the entire index. For either case, the prune ratio error is controlled to within ±0.2%.

167

MAP (t) 0.10

MAP (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 160 157 152 145 139 134 127 118 095 159 158 154 148 144 140 135 127 102 160 157 153 149 145 143 142 137 120 160 158 153 146 140 133 126 110 085 156 152 148 142 133 123 110 088 045 156 153 148 140 135 123 107 092 046

MAP (td) 0.00 0.10 0.20 0.00

MAP (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 203 197 187 177 163 147 142 130 091 204 189 177 170 160 158 141 141 116 204 199 191 183 177 175 174 164 136 204 199 185 179 164 150 137 103 074 186 174 160 150 136 123 112 090 050 189 171 165 149 143 124 116 095 051

P10 (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 260 256 252 246 245 236 209 201 174 259 258 254 239 226 225 204 192 168 261 257 254 248 249 242 241 236 222 261 259 253 249 243 237 225 190 168 256 250 239 224 201 182 157 125 107 257 248 243 214 202 193 164 133 106

P@10 (t)

0.20

Unpruned TCP UP-bm25 UP-dir

0.10

P10 (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 347 347 339 332 315 290 279 267 231 351 339 336 309 295 287 264 248 209 347 350 341 338 327 317 316 311 291 347 350 341 335 319 299 267 229 191 344 324 300 281 249 222 181 169 127 340 329 310 280 253 233 186 175 122

P@10 (td) 0.10 0.20 0.30

UP-jm PRP IP-u
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

Figure 1: The overall performance result on WT10g. All the measurements are rounded to the 3rd decimal place, with preceding zero and decimal point removed. The best raw performance at each prune level is underlined. The unpruned baseline has achieved 0.160/0.204 (t/td) in MAP and 0.261/0.347 (t/td) in P@10.

4.3 Retrieval Performance
We followed Blanco and Barreiro [7] for using BM25 as the post-pruning retrieval method. Retrieval performance is measured in mean average precision (MAP) and precisionat-10 (P@10). The result on the largest set WT10G is summarized in Figure 1 (see Figures 2 and 3 at the very end of this paper for results on the smaller sets). Each figure has four sets of measure-to-query-type combinations, and the result for each combination is given both as a table on the left and a plot on the right. These combinations from top to bottom, respectively, are MAP-t, MAP-td, P@10-t, and P@10-td. Table columns and x-axes in the plots indicate prune levels, from 0.1 to 0.9 (10% to 90%). Rows and curves indicate pruning methods.
Our result shows that, at small prune levels ( 0.5), all these methods differ little in performance, and the difference at larger prune levels seems more evident. Both PRP and IP-u, whose performance was nearly identical, have consistently achieved the bottom performance in all settings. In general, the performance for the UP family and TCP is comparable, though UP-dir performed slightly better than the other. We noticed that the performance of UP-dir is also robust to high prune ratio. On WT10G, when tested under an extreme setting with 90% prune ratio, UP-dir still managed to retain 75% of the baseline MAP for short queries,

and 66.7% for long queries. Of the baseline P@10, UP-dir retained 85.1% for short queries and 83.9% for long queries. Under a less aggressive setting such as 50% prune ratio, UPdir have done even better by retaining 90.6% and 86.8% of baseline MAP, and 95.4% and 94.2% of baseline P@10, respectively for short and long queries.
4.4 Significance Tests
We further conducted an analysis of variance (ANOVA) to check if the performance difference is significant. Due to the unbalanced size of measurement, we tested each corpus independently. Here, we assume a fixed-effect, 4-way no interaction, repeated measure design, expressed as:
Yi,j,k,l = ai + bj + ck + dl + i,j,k,l,
where Yi,j,k,l is the measured performance, ai is the querytype effect, bj the prune-level effect, ck the method effect, and dl the topic effect, and i,j,k,l denotes the error.
The result is covered in Table 2. Each row indicates a measure-effect combination and each column a test corpus. Test statistics, such as degrees of freedom (DF) and F-values (F), are given for every test case. We used partial eta-square (p2) to measure the effect size [17]. We first ran an omnibus test to see if any main effect is significant. Of all three collections, all the main effects were tested significant for

168

Disks 4 & 5

WT2G

WT10G

Response Main Effect DF

MAP

Query Type F(1, 5336)

F p2 DF 74.10 .01 F(1, 5336)

F p2 DF

F p2

42.57 .01 F(1, 10686) 192.25 .02

Prune Ratio F(8, 5336) 240.30 .26 F(8, 5336) 306.17 .31 F(8, 10686) 193.26 .13

Method

F(5, 5336) 11.00 .01 F(5, 5336) 40.20 .04 F(5, 10686) 61.47 .03

Query Topic F(49, 5336) 885.35 .89 F(49, 5336) 335.89 .76 F(49, 10686) 422.46 .80

P@10

Query Type F(1, 5336) 66.16 .01 F(1, 5336) 10.89 .00 F(1, 10686) 622.34 .06

Prune Ratio F(8, 5336) 105.00 .14 F(8, 5336) 133.98 .17 F(8, 10686) 122.43 .08

Method

F(5, 5336) 20.34 .02 F(5, 5336) 44.06 .04 F(5, 10686) 71.01 .03

Query Topic F(49, 5336) 484.06 .82 F(49, 5336) 296.88 .73 F(49, 10686) 226.31 .68

Table 2: The 4-way no-interaction ANOVA result. Each cell indicates a combination of performance measure
(row) and test collection (column). Degrees of freedom and F-values are given for testing all the main effects. Effect size is given in p2. In our experiment, all the main effects are significant for p < 0.001.

MAP P@10

Disks 4 & 5

Method Mean Group

UP-bm25 .204 a..

UP-dir

.200 a..

TCP

.196 ab.

UP-jm

.191 .bc

PRP

.187 ..c

IP-u

.187 ..c

UP-dir

.433 a..

TCP

.433 a..

UP-jm

.424 a..

UP-bm25 .417 a..

PRP

.392 .b.

IP-u

.389 .b.

WT2G

Method Mean

UP-dir

.223

UP-bm25 .211

TCP

.204

UP-jm

.192

IP-u

.181

PRP

.179

UP-dir

.404

TCP

.385

UP-jm

.367

UP-bm25 .359

IP-u

.322

PRP

.319

Group a... .b.. .b.. ..c. ...d ...d a... ab.. .bc. ..c. ...d ...d

WT10G

Method Mean

UP-dir

.162

UP-bm25 .151

TCP

.148

UP-jm

.145

IP-u

.129

PRP

.127

UP-dir

.286

TCP

.268

UP-jm

.265

UP-bm25 .259

IP-u

.222

PRP

.219

Group a.. .b. .b. .b. ..c ..c a.. .b. .b. .b. ..c ..c

Table 3: The overall result for Tukey's HSD test. For each combination of performance measure (row) and test collection (column), pruning methods are ordered in descending mean and tested for group difference. Methods that differ significantly do not share the same group label.

p < 0.001. Further analysis shows that query type and prune method have relatively small effect sizes, suggesting that query topic and prune ratio have much greater influence on the retrieval performance than the others do.
Post-hoc tests are then called for to examine the difference caused by different factor values. Since our experimental setting involves multiple comparison, we employed Tukey's honest significance difference (HSD) to control the overall Type I error [12]. Note that since Tukey's HSD is a one-way test, only one effect is tested in each run. In the following paragraphs, we summarize the HSD results for all the main effects. Here, since our focus is on the method effect, we shall briefly cover the other three for the sake of completeness.
Method Effect. Table 3 summarizes this HSD result for the
method effect. For each measure-corpus combination, we assigned group labels, e.g., "a" to "d", to individual methods based on the pairwise differences in their means. The difference between two methods is significant if and only if they share no common group label.
The result is briefly summarized as follows. First, the UP family and TCP consistently achieved top performance in both MAP and P@10 across different test settings. In the leading group, UP-dir delivers slightly better performance than the others. This is even more pronounced under the Web settings, in which UP-dir significantly outperformed the rest of methods in MAP (on both corpora) and in P@10 (on WT10G only). Second, the performance for the rest

of UP family and TCP is in general comparable. Take UPbm25 and TCP. The performance difference between the two is subtle: UP-bm25 was shown superior in MAP on Disks 4 & 5 but inferior in P@10 on WT2G. Third, PRP and IP-u are inferior to all the other methods. This result is consistent with our analysis on the raw performance measurements.
Query Type Effect. Long queries (td) achieve better per-
formance than short queries (t), which is expected because short queries are less precise than longer ones. This difference is confirmed on all three test collections, and appears more evident in the largest set WT10G.
Prune Ratio Effect. Small prune levels do better than large
ones in both metrics, which is also expected since more aggressive pruning results in less information in the index. According to the pairwise comparison made within the HSD test, this result is generally true except for a few small pairs such as 0.1-against-0.2. Specifically, WT10G has many such insignificant small pairs, suggesting that retrieval on larger Web collections is less sensitive to information loss.
Topic Effect. The result is difficult to interpret due to the
size of topic pairs, e.g., topics 451-551 on WT10G has produced 4950 such pairwise comparisons. In general, only a small number of queries have significantly deviated from the average performance, meaning that most queries are designed to be about equally difficult.

169

5. DISCUSSION
The experiment result for uniform pruning is generally in line with our understanding to impact, much of this was contributed by the previous work in index compression and dynamic pruning. Since many ideas come from the same outlet in the indexing pruning community, it is no surprise that uniform pruning is related to many existing impact-based methods. For example, Anh et al. [3] concluded that impactsorted indexes combined with early termination heuristics can best optimize retrieval system performance. This technique is conceptually equivalent to uniform pruning. Further work in this line investigated impact-based pruning, an application of impact-sorting to dynamic query pruning [4]. And again, this is a dynamic version of uniform pruning. Adding to these results, our analysis shows that impactbased methods are good approximate solutions to the proposed model induction problem.
One further question that invites curious eyes is why Dirichlet smoothing worked so well with uniform pruning that it significantly outperformed all the other variations on our Web benchmark WT2G and WT10G. So far the answer is still unclear to us. Here, let us discuss a few possibilities:
· BM25 might be a poor approximation to the probability p(t|d) since the framework presented in this paper was tailored specifically for language models. While this may explain why BM25 was inferior to Dirichlet smoothing in our experiments, it does not tell us why the performance for Jelinek-Mercer smoother and for BM25 were comparable.
· Another possibility is that, since parameter optimization is lacking in our experiment, we might have failed in producing the most competitive result for BM25 and Jelinek-Mercer smoother. If this theory is true, score functions will need task-specific fine-tuning in their further use. But for that to make sense, one needs to point out in what major way the role of a score function in index pruning departs from that in ordinary ad-hoc retrieval. This may point to an interesting direction for future work, but based on the evidence collected so far this claim is difficult to verify.
With the argument given in Section 3 about the convex integer program, one may argue that it is important to prevent depleting any term posting since doing so would take the objective in Equation 10 to minus infinity. In other words, an additional constraint, called "no depletion", shall be added into the index pruning guideline. This is because, even though we do not attempt to solve the convex program, the constraint still needs to be enforced to guarantee that information loss is bounded. In this respect, it is necessary to adopt a top-k preserving strategy (i.e., skip any term posting that has less than k entries), such as the one in TCP, to avoid depleting term postings.
6. CONCLUSION
In this paper, we review the problem of static index pruning from a brand new perspective. Given the appropriate assumptions, we show that this problem can essentially be tackled within a model induction framework, using the principle of minimum cross-entropy. The theory guarantees that the induced model best approximates the full model in terms

of probability divergence. We show that static index pruning can be written as a convex integer program. Yet exact inference, though possible as it might be, is generally computationally infeasible for large collections. So we further propose a surrogate model to address the computation issue, and show that uniform pruning is indeed an optimal solution to the formalism. To verify the correctness of our result, we conducted an extensive empirical study. The experiment was redesigned to take two factors, variable control and significance testing, into consideration. This setup has helped us reduce possible experimental bias or error.
Our result confirms that, when paired with the Dirichlet smoother, the performance of uniform pruning is state of the art. Significant improvement over the other methods were observed across diverse retrieval settings. Uniform pruning also exhibits an advantage in robustness with respect to large prune ratio. Specifically, our result on WT10G for short queries suggests that uniform pruning with the Dirichlet smoother retains at least 90% of the baseline performance at 50% prune ratio and 85% at 80% prune ratio. To the best of our knowledge, this is by far the best performance ever reported for static index pruning on the standard benchmark.
This research work has given rise to many technical issues, some have been addressed in Section 5 and some remain unsettled. It shall be interesting to see how uniform pruning responds to other test environments, such as different retrieval engines, corpora, or tasks. Document-length update and pseudo relevance feedback have been two landmark issues that we are ready to explore. Since we did not fine-tune the baseline performance, testing pruning methods against optimized, strong baseline shall provide more insight about this art. Besides all these possibilities, one promising direction is to extend the model induction idea to other type of structured data, such as lexicons or language models. Further investigation into the theory may shed us some light in the role that impact plays in different IR tasks.
7. ACKNOWLEDGMENT
We would like to thank Wei-Yen Day, Ting-Chu Lin, and the anonymous reviewers for their useful comments.
8. REFERENCES
[1] I. S. Altingovde, R. Ozcan, and O. Ulusoy. A practitioner's guide for static index pruning. In M. Boughanem, C. Berrut, J. Mothe, and C. Soule-Dupuy, editors, Advances in Information Retrieval, volume 5478 of Lecture Notes in Computer Science, chapter 65, pages 675­679. Springer Berlin / Heidelberg, Berlin, Heidelberg, 2009.
[2] I. S. Altingovde, R. Ozcan, and O. Ulusoy. Static index pruning in web search engines: Combining term and document popularities with query views. ACM Transactions on Information Systems, 30(1), Mar. 2012.
[3] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space ranking with effective early termination. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '01, pages 35­42, New York, NY, USA, 2001. ACM.
[4] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In Proceedings of the

170

MAP (t) 0.00 0.10 0.20

MAP (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 225 221 213 206 198 186 172 149 109 227 226 223 218 208 194 168 166 143 225 222 213 206 197 190 178 157 117 224 221 211 202 192 180 163 140 103 227 224 219 209 194 168 148 149 108 227 225 220 211 195 173 147 147 108

MAP (td) 0.00 0.10 0.20

MAP (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 251 246 239 232 219 201 184 162 117 250 246 235 229 222 202 179 176 158 251 247 239 232 220 210 196 173 133 250 246 238 228 216 194 173 148 110 253 246 228 212 195 168 148 146 129 252 240 230 215 195 173 145 138 125

P@10 (t) 0.10 0.25 0.40

P10 (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 438 434 434 430 428 426 424 384 326 438 440 436 438 438 412 370 358 294 438 434 432 428 426 434 416 398 344 438 434 432 428 424 416 410 370 318 436 442 444 452 428 400 340 284 204 438 440 440 450 430 406 338 278 190

Unpruned TCP UP-bm25 UP-dir

P10 (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 476 468 478 474 468 460 462 432 358 470 486 482 470 444 420 400 390 324 474 468 472 462 458 450 440 442 386 476 470 470 468 466 442 430 412 336 486 486 468 470 428 388 350 316 236 474 478 474 464 436 400 340 292 226

P@10 (td) 0.10 0.25 0.40

UP-jm PRP IP-u
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

Figure 2: The performance result on Disk 4 & 5, with all measurements rounded to the 3rd decimal place, preceding zero and decimal point removed. The best raw performance at each prune level is underlined. The unpruned baseline has has achieved 0.228/0.256 (t/td) in MAP and 0.436/0.478 (t/td) in P@10.

29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '06, pages 372­379, New York, NY, USA, 2006. ACM.
[5] R. Blanco and A. Barreiro. Boosting static pruning of inverted files. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 777­778, New York, NY, USA, 2007. ACM.
[6] R. Blanco and A. Barreiro. Static pruning of terms in inverted files. In G. Amati, C. Carpineto, and G. Romano, editors, Advances in Information Retrieval, volume 4425 of Lecture Notes in Computer Science, chapter 9, pages 64­75. Springer Berlin Heidelberg, Berlin, Heidelberg, 2007.
[7] R. Blanco and A. Barreiro. Probabilistic static pruning of inverted files. ACM Transactions on Information Systems, 28(1), Jan. 2010.
[8] S. Bu¨ttcher and C. L. A. Clarke. A document-centric approach to static index pruning in text retrieval systems. In Proceedings of the 15th ACM international conference on Information and knowledge management, CIKM '06, pages 182­189, New York, NY, USA, 2006. ACM.

[9] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer. Static index pruning for information retrieval systems. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '01, pages 43­50, New York, NY, USA, 2001. ACM.
[10] R.-C. Chen, C.-J. Lee, C.-M. Tsai, and J. Hsiang. Information preservation in static index pruning. In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM '12, pages 2487­2490, New York, NY, USA, 2012. ACM.
[11] E. S. de Moura, C. F. dos Santos, D. R. Fernandes, A. S. Silva, P. Calado, and M. A. Nascimento. Improving web search efficiency via a locality based static pruning method. In Proceedings of the 14th international conference on World Wide Web, WWW '05, pages 235­244, New York, NY, USA, 2005. ACM.
[12] D. Hull. Using statistical testing in the evaluation of retrieval experiments. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '93, pages 329­338, New York, NY, USA, 1993. ACM.

171

0.00 0.10 0.20 0.30 0.00 0.10 0.20

MAP (t)

MAP (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 248 242 233 222 208 195 176 143 090 249 250 249 236 225 213 188 133 108 247 240 234 226 207 195 198 181 139 247 239 229 220 199 173 143 107 080 253 249 237 226 196 166 114 092 063 250 247 240 225 205 164 126 081 074

MAP (td)

MAP (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 288 278 258 241 222 206 183 147 096 288 276 260 237 226 209 193 151 122 290 279 268 258 238 225 224 209 161 289 281 262 245 217 187 149 113 078 284 262 233 214 189 166 119 093 067 284 263 236 216 190 161 124 087 079

P@10 (t) 0.10 0.25 0.40

P10 (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 416 410 408 398 400 380 374 338 268 414 420 418 386 380 384 340 238 210 414 402 402 404 380 386 392 374 332 414 404 402 402 378 348 336 306 248 418 418 408 394 362 342 250 126 144 410 408 400 390 380 338 268 138 126

Unpruned TCP UP-bm25 UP-dir

P10 (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u

.1 .2 .3 .4 .5 .6 .7 .8 .9 464 446 428 410 410 388 356 328 302 464 446 412 370 378 384 326 268 218 460 446 432 436 422 428 422 408 346 460 446 430 420 388 354 340 276 260 442 434 392 382 340 334 260 142 146 450 430 402 392 348 324 258 158 170

P@10 (td) 0.10 0.25 0.40

UP-jm PRP IP-u
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

Figure 3: The performance result on WT2G. All measurements were rounded to the 3rd decimal place, and preceding zero and decimal point removed. The best raw performance at each prune level is underlined. The unpruned baseline has has achieved 0.249/0.293 (t/td) in MAP and 0.414/0.460 (t/td) in P@10.

[13] R. J. Hyndman and Y. Fan. Sample quantiles in statistical packages. The American Statistician, 50(4):361­365, Nov. 1996.
[14] S. Kullback. Information Theory and Statistics. Wiley, New York, 1959.
[15] J. D. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In W. B. Croft, D. J. Harper, D. H. Kraft, J. Zobel, W. B. Croft, D. J. Harper, D. H. Kraft, and J. Zobel, editors, SIGIR, SIGIR '01, pages 111­119, New York, NY, USA, 2001. ACM.
[16] K. Lange, D. R. Hunter, and I. Yang. Optimization transfer using surrogate objective functions. Journal of Computational and Graphical Statistics, 9(1), 2000.
[17] D. C. Montgomery. Design and analysis of experiments (6th ed.). Wiley, 2004.
[18] S. Robertson. The probability ranking principle in IR. In K. S. Jones and P. Willett, editors, Reading in Information Retrieval, chapter The probability ranking principle in IR, pages 281­286. Morgan

Kaufmann Publishers Inc., San Francisco, CA, USA, 1997.
[19] S. Thota and B. Carterette. Within-Document Term-Based index pruning with statistical hypothesis
testing. In P. Clough, C. Foley, C. Gurrin, G. Jones, W. Kraaij, H. Lee, and V. Mudoch, editors, Advances in Information Retrieval, volume 6611 of Lecture Notes in Computer Science, chapter 54, pages 543­554. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.
[20] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In Proceedings of the tenth international conference on Information and knowledge management, CIKM '01, pages 403­410, New York, NY, USA, 2001. ACM.
[21] L. Zheng and I. J. Cox. Entropy-Based static index pruning. In M. Boughanem, C. Berrut, J. Mothe, and C. Soule-Dupuy, editors, Advances in Information Retrieval, volume 5478 of Lecture Notes in Computer Science, chapter 72, pages 713­718. Springer Berlin / Heidelberg, Berlin, Heidelberg, 2009.

172

Document Identifier Reassignment and Run-Length-Compressed Inverted Indexes for Improved
Search Performance

Diego Arroyuelo
Dept. of Informatics, Univ. Técnica F. Santa María. Yahoo! Labs Santiago, Chile
darroyue@inf.utfsm.cl

Senén González
Department of Computer Science, University of Chile. Yahoo! Labs Santiago, Chile
sgonzale@dcc.uchile.cl
Victor Sepulveda
Yahoo! Labs Santiago, Chile
vsepulve@dcc.uchile.cl

Mauricio Oyarzún
University of Santiago. Yahoo! Labs Santiago, Chile
mauricio.silvaoy@usach.cl

ABSTRACT
Text search engines are a fundamental tool nowadays. Their efficiency relies on a popular and simple data structure: the inverted indexes. Currently, inverted indexes can be represented very efficiently using index compression schemes. Recent investigations also study how an optimized document ordering can be used to assign document identifiers (docIDs) to the document database. This yields important improvements in index compression and query processing time. In this paper we follow this line of research, yet from a different perspective. We propose a docID reassignment method that allows one to focus on a given subset of inverted lists to improve their performance. We then use run-length encoding to compress these lists (as many consecutive 1s are generated). We show that by using this approach, not only the performance of the particular subset of inverted lists is improved, but also that of the whole inverted index. Our experimental results indicate a reduction of about 10% in the space usage of the whole index (just regarding docIDs), and up to 30% if we regard only the particular subset of list on which the docID reassignment was focused. Also, decompression speed is up to 1.22 times faster if the runs must be explicitly decompressed and up to 4.58 times faster if implicit decompression of runs is allowed. Finally, we also improve the Document-at-a-Time query processing time of AND queries (by up to 12%), WAND queries (by up to 23%) and full (non-ranked) OR queries (by up to 86%).
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.2.4 [Systems]: Textual databases
Partially funded by Fondecyt Grant 11121556.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

General Terms
Algorithms, Experimentation, Performance
Keywords
Inverted index compression, document reordering, query processing.
1. INTRODUCTION
Inverted indexes are the de facto data structure to support the high-efficiency requirements of a text search engine [29, 4, 10, 18, 31]. This includes, for instance, providing fast response to thousands of queries per second and using as less space as possible, among others. Given a document collection D with vocabulary  = {w1, . . . , wV } of V different words (or terms), an inverted index for D stores a set of inverted lists Iw1 [1..n1], . . . , IwV [1..nV ]. Every list Iwi [1..ni] stores a posting for each of the ni documents that contain the term wi  . Typically, a posting stores the document identifier (docID) of the document that contains the term, the number of occurrences of the term in this document (the term frequency) and, in some cases, the positions of the occurrences of the term within the document. The inverted index also stores a vocabulary table, which allows us to access the respective inverted lists.
The space required by the inverted lists is dominant in the space of the index. Therefore, lists are kept compressed, not only to reduce their space usage but also to reduce the transference time from disk--which can be up to 4­8 times slower if the disk-resident lists are not compressed [10, see Table 6.9­page 213]. To answer a query, the involved lists must be decompressed--fully or partially. Hence, fast decompression is a key issue to support quick answers.
Inverted index compression has been studied in depth in the literature [29, 18, 4, 10]. Usually, docIDs, frequencies and positions are stored separately, hence they can be compressed independently. Even though it is important to compress each of these components, this paper is devoted to compressing just the docIDs. Frequencies and term positions can be usually compressed using similar techniques to that used for docIDs. However, the improvements obtained in this paper will be tailored to docIDs. According to [30, 3],

173

docIDs correspond to about 65% of a docIDs+frequencies index. If we also consider positional information, docIDs correspond to about 20% of the overall space [3].
The docIDs of list Iwi are compressed sorting them by increasing docID, to then represent the list using gap encoding: the first docID of the list is represented as it is, whereas the remaining docIDs are represented as the difference with the previous docID. We call DGap this difference. For instance, given the inverted list 10, 30, 65, 66, 67, 70, 98 , its DGap encoding is 10, 20, 35, 1, 1, 3, 28 . This generates a distribution with smaller numbers, in particular for long lists. As we shall see through this paper, many of these DGaps are actually 1s, which correspond to terms that appear in documents whose docIDs are consecutive. To support searching, inverted lists are logically divided into blocks of, say, 128 DGaps each. This allows us skipping blocks at search time, decompressing just the blocks that are relevant for a query, not necessarily the whole list. Among the existing compression schemes for inverted lists, we have classical encodings like Elias [13] and Golomb/Rice [14], as well as the more recent ones VByte [28], Simple 9 [1], Interpolative [19] and PForDelta [32] encodings. All these methods benefit from sequences of small integers.
The assignment of docIDs to a given document database is not trivial. This task--usually known as document reordering--consists in ordering the documents in D, to then assign the docIDs following this order. For instance, a simple and effective method consists in ordering the documents according to their urls [23]. These are called ordered document collections and will be the focus of this paper. Document reordering is not always feasible, as discussed in [30]. However, there are many applications where this can be used. The advantage of assigning docIDs in an optimized way is that it yields smaller DGaps in the inverted lists, hence better compression can be achieved [5, 6, 21, 23, 24] and in general a much better inverted index performance [30, 26]. Reductions of up to 50% both in space usage and Document at a Time (DAAT) query processing have been reported [30, 26]. The main reason of this improvement in the query processing time is that the docIDs that are relevant to a query tend to be clustered within the same inverted lists blocks, hence less blocks need to be decompressed.
A remarkable feature of ordered document collections is that the number of 1 DGaps is increased. For instance, after reordering the TREC GOV2 document collection, the distribution of DGaps has almost 60% of 1s, whereas a random ordering yields just 11% of 1s. Actually, these 1s tend to form long runs in the inverted lists. Having runs of equal symbols in a sequence allows us to use run-length encoding [14]: we simply encode a run writing its length. There are, nowadays, five main ways to take advantage of these runs in the lists, namely:
1. To use compression approaches that have been particularly tuned to compress small DGaps [30].
2. To use compression approaches like Interpolative Encoding [19], which is suitable to compress small DGaps.
3. To use compression approaches like Interval Encoding [7], able to compress the runs in the lists.
4. To use compression approaches like the one proposed in [2] to compress long runs of 1s.

5. To use compression approaches like VSEncoding [25], which computes the optimal block partitioning to improve both compression ratio and decompression speed.
Approach (1) above compresses every 1 in a run explicitly. If one uses the OptPFD approach [30] and a given run of 1s is large enough, we can encode each 1 using one bit. Approach (2), on the other hand, is able to represent the runs implicitly, which is a desirable feature: when the integer encoding process gets into a run, it will encode the extreme values of the run and then each 1 within the run is not represented. That is, interpolative encoding deals with the runs in a natural way. The only detail is that, depending on where a run lies within the list, it could be split into several consecutive runs by the encoding process. This may be not optimal, yet efficient enough in most cases. The main drawback of approach (2) is that both the decoding and query-processing performance are not competitive [10, 30]. Approach (3) also encodes the intervals of consecutive docIDs (i.e., the runs of 1s in the DGap-encoded lists), storing the initial docID in the run, followed by the length of the run. All runs in a list are stored in a separate storage. The remaining docIDs (i.e., those that are not in any run) are called residuals. These are encoded using DGaps, as usual. Even though approach (3) is effective for web graph compression, it has two potential drawbacks in our information retrieval setting. First, the separate storage of the docIDs (i.e., intervals and residuals) is not adequate for DAAT query processing. Second, after removing the intervals from the list, bigger DGaps are generated, which is similar to the effect seen in [17]. Unfortunately, this worsens the compression ratio achieved. Approach (4) suggests that their method can compress runs of 1s in inverted lists. Yet, it is not studied in depth [2]. Finally, approach (5) could be used along with the techniques proposed in this paper to implicitly compress runs of 1s.
In this paper we study in depth how the inverted index compression and DAAT query processing are affected by the runs of 1s that are generated in ordered document collections. We conclude that properly managing these runs can lead to improvements in the space usage and query processing time. See Section 3 for a summary of our contributions.
2. PREVIOUS CONCEPTS
2.1 Inverted List Compression Schemes
We study here the basic compression schemes for inverted lists, which shall be used throughout this paper.
2.1.1 Variable-Byte Encoding
VByte encodes an integer using an integral number of bytes [28]. As the encoding is byte-aligned, it can be decoded faster. To obtain this encoding, the binary representation of the integer to be encoded is split into 7-bit chunks, adding an extra continuation bit (or flag) to every chunk. This flag indicates whether the current chunk is the last one in the representation of the integer or not. The compression ratio of VByte is not efficient when the inverted lists have small DGaps like 1, 2 or 3 (which are relatively frequent in large inverted lists, particularly for ordered collections).
2.1.2 Simple 9 Encoding
The Simple 9 encoding (S9 for short) [1] aims at a fast decompression, yet achieving a competitive compression ratio. Assuming a machine word of 32 bits, we divide it into as

174

many chunks of equal size as we can, storing a DGap in each chunk. We use a 4-bit header to indicate the decoder how many DGaps have been stored in that word. Hence, we have 28 bits left to store the DGaps. There are 9 ways--hence its name--of dividing the remaining 28 bits into equal-size chunks: 1 chunk of 28 bits; 2 chunks of 14 bits; 3 chunks of 9 bits, 1 unused bit; 4 chunks of 7 bits; 5 chunks of 5 bits, 3 unused bits; 7 chunks of 4 bits; 9 chunks of 3 bits, 1 unused bit; 14 chunks of 2 bits; and 28 chunks of 1 bit.
To decompress an S9 word, its header is used to determine the case using a switch statement in C, where the 9 cases are hard-coded to improve decompression speed. In practice, the decompression speed is slightly better than VByte, and it has a better compression ratio in general.
2.1.3 PForDelta Encoding
The PForDelta encoding [32] divides an inverted list into blocks of, usually, 128 DGaps each. To encode the DGaps within a given block, it gets rid of a given percentage-- usually 10%--of the largest DGaps within the block, and stores them in a separate memory space. These are the exceptions of the block. Next, the method finds the largest remaining DGap in the block, let us say x, and represents each DGap in the block in binary using b = log x bits. Though the exceptions are stored in a separate space, we still maintain the slots for them in their corresponding positions. This facilitates the decoding process. For each block we maintain a header that indicates information about the compression used in the block, e.g., the value of b.
To retrieve the positions of the exceptions, we store the position of the first exception in the header. In the slot of each exception, we store the offset to the next exception in the block. This forms a linked list with these slots. In case that b is too small and cannot accommodate the offset to the next exception, the algorithm forces to add extra exceptions between two original exceptions. This increases the space usage when the lists contain many small numbers.
To decompress a block, we first take b from the header, and invoke a specialized function that obtains the b-bit DGaps. Each b has its own extracting function, so they can be hardcoded for high efficiency. Once we decode the DGaps, we traverse the list of exceptions of the block, storing the original DGaps in their corresponding positions. This step can be slower, yet it is carried out just for 10% of the block. In typical implementations of PForDelta, the header is implemented in 32 bits, since we only need to store the values of b (in 6 bits, since 1  b  32) and the position of the first exception (in 7 bits, since the block has 128 positions). PForDelta has shown to be among the most efficient compression schemes [30], achieving a high decompression speed.
2.2 Document Reordering
The document reordering problem (or, equivalently, the document identifier assignment problem) consists in assigning similar docIDs--ideally, consecutive docIDs--to documents that contain similar terms [5, 6, 21, 23, 24]. This generates smaller DGaps in the inverted lists. As discussed in [30], document reordering is not always feasible. In particular, when global measures of page ranking (e.g., PageRank [8] and Hits [16]) are used for early termination. However, some applications benefit from document reordering.
Previous work [30] studies the performance of different compression schemes for ordered document collections. One

of the main conclusions from [30] is that some compression schemes are not suitable for these cases, e.g., PForDelta, since most DGaps are small numbers, like 1 and 2. Hence, small values of b should be used in each block (e.g., b = 1 or b = 2). However, recall that PForDelta forces to add extra exceptions when b is not enough to represent an offset, which makes it unsuitable in these cases.
Two new alternative are introduced in [30]. The first one is called New PForDelta (NewPFD from now on), which supports using any value of b  1 without adding any extra exception. The trick is to use the b-bit slot to store just b bits of the offset, while the remaining bits of the offset are stored in a separate space of memory. At decompression time, the original offsets are reconstructed using bit masks. The second alternative, called Optimized PForDelta (OptPFD from now on), allows us to use a variable number of exceptions in each block. Hence, one can choose either to minimize the space usage or to maximize the decompression speed, yielding also different trade-offs. One of the main results from [30] is that, given a target decompression speed, NewPFD and OptPFD have a better space usage than PForDelta.
3. OUR CONTRIBUTIONS
In this paper we show that by properly representing the runs of 1s that appear in inverted lists when document reordering is used, relevant improvements in space usage and query time can be achieved. In particular, we introduce a method for focusing the generation of runs of 1s to a particular set of lists and then show how to compress these runs using methods that support efficient query processing. Overall, our results are:
1. We reduce the space usage of the most efficient alternatives in [30] by about 10%.
2. The decompression speed is up to 1.22 faster if the runs must be explicitly decompressed and up to 4.58 times faster if implicit decompression of runs is allowed.
3. We improve the DAAT query processing time of AND (by up to 12%), WAND (by up to 23%), and full OR queries (by up to 86%).
4. We show that by using our approaches, the number of docIDs and the list blocks that are decompressed at query time can be reduced by up to 55%.
5. We show that our approaches are more suitable to compress static caches of inverted lists.
4. DOCUMENT REORDERING METHODS TO GENERATE RUNS OF 1S
Inverted list compressors benefit from distributions with small DGaps. Document reordering (Section 2.2) yields small DGaps in the resulting inverted lists. E.g., in the inverted index of the TREC GOV2 collection under random document order, just 10.75% of DGaps have value 1. If, on the other hand, we sort the documents by urls [23], we obtain 60.30% of DGaps with value 1. Thus, the ordered case generates many 1 DGaps [12]. Actually, it can be observed that many of these 1s are grouped into long runs. Instead of regarding these 1s separately (as in previous work [30]) we deal with the runs. We aim at improving the space usage and

175

query processing time of current alternatives. Runs can be encoded more efficiently, e.g., using run-length compression [14]: a run of repeated symbols can be encoded indicating the symbol and then writing the length of the run. This approach has been successfully used in many areas [14, 20].
The existing document reordering methods are mainly based on the document collection to carry out the ordering. This yields smaller indexes with enhanced query time [30, 26] since, after reordering, the documents that are relevant to a query tend to group into a few inverted-list blocks. Thus, less blocks need to be decompressed at query time. However, typically in practice some inverted lists are more important than others. Hence, one would want to focus on improving their particular performance, as opposed to improving the whole index. A particular such application is that of static inverted-list caches: the aim is to maintain in main memory the most-frequently-queried inverted lists, as well as other big inverted lists that are eventually queried and whose big size makes prohibitively expensive to transfer them from secondary storage. This effect cannot be achieved if we use methods that first assign docIDs and then construct the index following this assignment.
We develop next a heuristic to assign docIDs such that it improves the compression of a particular set of inverted lists. Our method uses the inter-list dependencies (i.e., the list intersections) to assign docIDs. As a result, these intersections become runs when encoded as DGaps. This will improve the compression ratio of these lists, avoiding the space-usage problems of [17] in cases where document reordering is allowed. Also, in Section 6 we will show that the resulting run (i.e., the intersection) will be stored inside a single block, reducing the number of blocks that are decompressed at query time. In general, finding the docID assignment that optimizes the amount of runs in the lists is known to be an NP-hard problem [15]. Thus, our technique is a heuristic that attempts just to improve the distribution and quality of runs, though it may not be the optimal one.
4.1 A Flexible docID Assignment Method for Focusing the Creation of Runs
Let I be the inverted index for a given document collection D, where docIDs have been assigned arbitrarily. Let L = Ii1 , Ii2 , . . . , Iim be an ordered subset of the inverted lists of I, such that Iij is the jth list in L. The document ordering process will be based on this ordered set of lists. Let F : Z  Z be a function such that (i, j)  F iff docID i has been already reenumerated as docID j.
Initially, we set F  , and start the process from Ii1 , which stores docIDs d1, d2, . . . , dl. A simple approach could be to rename d1  1, d2  2, . . ., dl  l. For each such di, we set F  F {(di, i)}. Notice how this transforms Ii1 into a single run when representing the list as DGaps. Now, we go on to reenumerate Ii2 , with the restriction that every docID i such that (i, j)  F (for a given j) cannot be reenumerated anymore during the process. These are called fixed docIDs. Hence, we assign consecutive docIDs (starting from l + 1) to any docID i in Ii2 such that (i, j)  F , for any j, and add the corresponding pair to F . For instance, let us consider the following example: Ii1 = 10, 30, 65, 66, 67, 70, 98 and Ii2 = 20, 30, 66, 70, 99, 101 . The first step reenumerates Ii1 assigning 10  1, 30  2, 65  3, 66  4, 67  5, 70  6, 98  7. Now we reenumerate Ii2 , having into account that docIDs 30, 66 and 70 are now fixed. Hence, we

assign 20  8, 99  9, 101  10. After the process, the result is Ii1 = 1, 2, 3, 4, 5, 6, 7 and Ii2 = 2, 4, 6, 8, 9, 10 .
Notice from the previous example that after the first step, all docIDs in the intersection Ii1 Ii2 are fixed docIDs, hence they cannot be changed when processing Ii2 . Notice also how the inter-list dependencies complicate the run generation in Ii2 , as the documents in Ii1  Ii2 could have been reenumerated in any order when processing Ii1 .
We can fix this problem as follows. We instead start by reenumerating the docIDs in Ii1  Ii2 , renaming them from 1 to |Ii1  Ii2 | and add them as fixed docIDs in F . Next, and since the remaining of both lists do not intersect each other, we can reenumerate them to generate a run in each list. In this way, one of the lists will become a single run when DGap encoded, whereas the other will contain two runs. For the previous example, we start reenumerating the intersection Ii1  Ii2 as 30  1, 66  2, 70  3. Then, we reenumerate the remaining elements in Ii1 as 10  4, 65  5, 67  6, 98  7. Finally, we reenumerate the remaining elements in Ii2 as 20  8, 99  9, 101  10. The resulting lists are Ii1 = 1, 2, 3, 4, 5, 6, 7 and Ii2 = 1, 2, 3, 8, 9, 10 . This generates more runs than the previous approach.
In general, we start from Ii1 and compute Ii1 Ii2 . If |Ii1  Ii2 |  M , for a given threshold M , we compute Ii1 Ii2 Ii3 , and repeat the process until |Ii1  · · ·  Iij+1 | < M . At this point, we take the d documents in Ii1  · · ·  Iij and assign them consecutive docIDs (e.g., from 1 to d). Next, we add them as fixed docIDs in F . This will generate a run of length d when computing the DGap encoding of Iij . Then, we go back and assign consecutive docIDs to the d documents in Ii1  · · ·  Iij-1 . Notice that d of these docIDs are already fixed from the previous step, hence we only assign docIDs from d + 1 to d to the remaining ones. This generates a run of length d in Iij-1 . Eventually, this process gets back to Ii1 , where all documents in Ii1  Ii2 are fixed and will form a run. Next, the inverted lists Ii1 , Ii2 , . . . , Iij are removed from L, and their non-yet-fixed docIDs are re-inserted into L as independent lists, following some well-defined order. This process is repeated until L = .
We call intersection-based docID assignment (IBDA) this process. It is important to note that the intersections:
· Ii1  Ii2 in either Ii1 and Ii2 ;
· Ii1  Ii2  Ii3 in Ii3 ;
· ...;
· Ii1  · · ·  Iij in Iij
are transformed into respective runs in each of these lists after the first step of IBDA. As we shall learn through this paper, this gives us a way to precompute intersections as runs. We just need to be careful in setting a suitable initial order for the lists in L: if, for some reason, we want to transform the intersection between two given inverted lists into a run, these lists must be consecutive in L. This can have many advantages at query time (see Section 6).
To summarize, notice the flexibility of our method, because of the following:
· It allows any initial order in the lists to be processed.
· It is based on just an initial inverted index; no information about the underlying documents is needed.

176

· It allows any initial docID assignment.
· It allows any re-insertion order for the tails of the lists.
This allows us to adapt the method to different situations.
5. RUN-LENGTH COMPRESSION OF INVERTED LISTS
Instead of using Interpolative Encoding [19] or Interval Encoding [7] to deal with the runs, we adapt some of the most effective compression schemes to run-length encode runs of 1s in inverted lists. We aim at efficient query processing time and improved compression ratios.
5.1 Run-Length Encoded VByte
To run-length encode the runs of 1s with VByte, we need to set a special mark that indicates, at decompression time, that we are in the presence of a run. Since we are encoding DGaps > 0, we use the byte 00000000 as the special mark The idea is to replace x  3 consecutive 1s in an inverted list by the 00000000 mark, followed by the VByte encoding of x. We call RLE VByte this scheme. RLE VByte is able to detect and handle very small runs (of length  3). We restrict the minimum length to be 3, because this yields the same space as for runs of minimum length 2 (in both cases the minimum run uses 2 bytes), yet the former has better decompression times. Indeed, this will be the only method in this paper able to deal with small runs of length 3    27.
We introduce now an alternative implementation of the original VByte scheme, on which will be based the RLE VByte implementation. The idea is to take advantage, at decompression time, of several consecutive small DGaps. Assuming that 0 is used as terminator flag in VByte, we take 4 consecutive bytes from the encoding (i.e., a 32-bit word) and carry out a bitwise AND with 0X80808080. This allows one to determine at once the 4 terminator flags of these bytes. In particular, if the bitwise AND results in a 0, it means that the terminator flags are all 0, hence we have 4 DGaps encoded in 4 bytes. We can then hard-code all cases, which can make a difference if most DGaps use 1 byte.
A potential drawback of RLE VByte is that, at decompression time, we need to make an extra comparison per element in the list, to know whether it is the mark of a run or not. However, to decode a run, the original VByte has to make one comparison for every 1. RLE VByte, on the other hand, is able to decode the run more efficiently, as we shall see in our experiments.
5.2 Run-Length Encoded S9
S9 can be easily adapted to run-length encode the runs of 1s, as it is suggested for a variant of this scheme [2]. We only need to add a new S9 case, which will be used to encode runs--recall that we use just 9 out of 16 cases available for the 4-bit headers. The remaining bits of the S9 word are used to represent the length of the run. It is rather common in practice to find S9 words that encode 28 ones, followed by a different S9 case (indeed, this is one of the most frequent cases for GOV2 inverted lists).
We define RLE S9 as follows, in order to compress runs of 1s: (C1) 1 chunk of 28 bits; (C2) 2 chunks of 14 bits each; (C3) 3 chunks of 9 bits each, 1 unused bit; (C4) 4 chunks of 7 bits each; (C5) 7 chunks of 4 bits each; (C6) 9 chunks of 3 bits each, 1 unused bit; (C7) 14 chunks of 2 bits

each; (C8) 28 chunks of 1 bit each followed by C1; (C9) 28 chunks of 1 bit each followed by C2; (C10) 28 chunks of 1 bit each followed by C3; (C11) 28 chunks of 1 bit each followed by C4; (C12) 28 chunks of 1 bit each followed by C5; (C13) 28 chunks of 1 bit each followed by C6; (C14) 28 chunks of 1 bit each followed by C7; (C15) 28 chunks of 1 bit each followed by 5 chunks of 5 bits each; (C16) there are two cases still missing. The first one consists of a word storing 5 chunks of 5 bits each. The second case corresponds to runs of 1s longer than 28. Since the case of 5 chunks of 5 bits each has 3 bits unused, we use one of them to make the difference between these cases. If the bit is a 0, then we have 5 chunks of 5 bits each. If the bit is 1, then we use the remaining 27 bits to represent the length of the run.
Cases 1 to 15 are identified with 4-bit headers from 0000 to 1110. The remaining two cases use 5-bit headers 11110 and 11111. These variable-length headers can be uniquely decoded very efficiently using a switch statement in C, checking just 4-bit headers as usual. If the 4-bit header is 1111, we check the following bit to determine the 5-bit header.
The compression process is carried out in two steps: (1 ) We first compress the list using the original S9 algorithm; (2 ) we traverse the S9 words generated in the previous step, and replace the S9 words that store 28 ones by one of the new cases defined above: if there are multiple consecutive such S9 words, we replace them by case 11111; if instead we have one such word followed by a different case, we replace it by the corresponding case 8 to 15.
5.3 Run-Length Encoded PForDelta
We define now RLE PFD, which adds run-length-encoding capabilities to PForDelta, with minor changes to the original scheme. We define two kind of blocks: (1 ) normal blocks, containing 128 DGaps (as usual), and prefixed by a 32-bit block header; (2 ) run blocks, which encode a run storing its length within the 32-bit header. We need an extra bit in the header to indicate whether it corresponds to a normal block or encodes a run length. Fortunately, the original PForDelta leaves a unused bit in the header. At decompression time, the flag is checked to see whether one must decompress a normal block or a run.
This scheme is unable to encode relatively short runs. First, we use 32 bits (the header) to represent the run length. Hence, we define run blocks as encoding runs of length  32 (hence, we use at most 1 bit to encode each 1 in the run). Second, it is very likely that some of the 1s lying at the beginning of a run will be used to fill the preceding normal block. Just the remaining 1s could be used to form a run--provided they are 32 or more. Hence, runs of moderate length ( 100­200)--which are rather frequent in practice-- are easy to detect. We can have, for instance, that the first docID in an inverted list is i = 1, and next it has a run of 158 1-DGaps. RLE PFD will not be able to encode the run as such: the first block will be a normal block storing i and then 127 1-DGaps. The next will be also a normal block storing the 31 remaining 1-DGaps (which cannot be regarded as a run, as we already said).
5.4 Experimental Evaluation
For our experiments we use an HP ProLiant DL380 G7 (589152-001) server, with a Quadcore Intel(R) Xeon(R) CPU E5620 @ 2.40GHz processor, with 128KB of L1 cache, 1MB

177

of L2 cache, 2MB of L2 cache, and a 96GB RAM, running version 2.6.34.8-68.fc13.i686.PAE of Linux kernel.
We index the 25.2 million documents from the TREC GOV2 collection. We implement our RLE schemes using C++. We compile our codes with g++ 4.4.5, with optimization flag -O5. For PForDelta, NewPFD, and OptPFD we use the highly-efficient implementations from [30]. We base our implementation of RLE PFD on OptPFD. VByte and S9 implementations are from ourselves. For VByte, S9, PForDelta, NewPFD and OptPFD, we subtract 1 to each DGap in the index, so they start from zero.
5.4.1 Document Reordering Methods
In our experiments, we use the following document orders to assign the docIDs to the TREC GOV2 collection: (1) the original order given to the documents in the collection ("Unsorted" in our tables); (2) the URL sorting ("URL" in our tables); (3) the IBDA ("IBDA" in our tables) method introduced in Section 4.1.
In our experiments, we found out that IBDA works better on inverted indexes for already-sorted document collections. Thus, we apply IBDA to the inverted index constructed for the URL-sorted document collection. In our particular implementation of IBDA, we sort the inverted lists of the index according to the following procedure (recall from Section 4.1 that we need an ordered set of list L). We take the 10,000 first queries from the TREC 2006 query log and compute the pairs of terms that are most frequently queried. We then sort the inverted lists such that if the most-frequentlyqueried pair of terms is wi and wj , hence the inverted lists Iwi and Iwj are the first lists in L. We then take the second most-frequent pair of terms and add their inverted lists to L (just in case they have not being added before). This procedure is repeated until all frequent pairs have been added. Notice how in this way we force the inverted lists of the most-frequently-queried pairs of terms to be in consecutive positions of L. The result is that the intersection of these pairs is transformed into a run by IBDA. This precomputes these intersections to speed-up query processing. All lists that do not appear among the most frequent pairs are sorted by length, from longest to shortest. In each step of the algorithm, the non-processed list tails are re-inserted according to their lengths.
5.4.2 Space Usage
Table 1 shows the experimental space usage of the docID data of the whole inverted index, for different compression schemes and the various docID assignment schemes we tested.
The main conclusions are the following:
· The space usage of either interpolative encoding, VByte, S9 and OptPFD is slightly improved when using our IBDA enumeration method, rather than just sorting by URL.
· When we use the RLE-encoded schemes on docIDs assigned by URL and IBDA, the space usage improves considerably compared with the corresponding normal schemes. The best improvement is achieved by RLE VByte: a reduction in space usage of 44.58%. This is because relatively-short runs are rather frequent in inverted indexes and, unlike other schemes, RLE VByte is able to encode runs of length  3.

Table 1: Overall space usage (in MB) of the TREC GOV2 inverted index, for different compression schemes and docID assignment methods. The space includes just the docIDs.

Compression Scheme

Reorder Method Unsorted URL IBDA

Interpolative Encoding VByte S9 OptPFD

5,507 7,526 6,687 6,348

2,712 6,726 3,777 4,600

2,641 6,754 3,735 4,504

RLE VByte RLE S9 RLE PFD

7,418 6,687 6,384

3,861 3,743 3,455 3,392 4,264 4,137

· Disregarding interpolative encoding, the smallest space usage is achieved by RLE S9 on docIDs assigned by IBDA. The improvement over S9 on docIDs assigned by URL (the previously-known best performer [30]) is of about 10.19%.
· Finally, RLE S9 uses between 25.82% and 28.44% extra space on interpolative encoding, depending on the docID assignment scheme used.
5.4.3 Decompression Speed
To test decompression speed, we use the TREC 2006 query log and decompress the inverted lists corresponding to the query terms. Table 2 shows the average decompression speed (in millions of DGaps per second) for the different compression schemes and docID assignment methods we have studied. All RLE methods implicitly decompress the runs.

Table 2: Average decompression speed (in mil-

lions of DGaps per sec.) for different compres-

sion schemes and docID assignment methods, on the

TREC GOV2 inverted index.

Compression

Reorder Method

Scheme

Unsorted URL IBDA

Interpolative Encoding VByte S9 OptPFD

43.94 818.81 749.28 927.02

43.05 827.98 983.84 857.48

64.44 917.47 1199.61 852.65

RLE VByte RLE S9 RLE PFD

446.58 779.52 989.74

1,313.03 1,988.86 1,812.22 2,691.09 2,026.54 3,931.73

The main conclusions are the following:
· Except for OptPFD, using IBDA (instead of just URL) increases the decompression speed of either interpolative (49.67%), S9 (21.93%) and VByte (10.81%).
· The decompression performance of our VByte variant (recall Section 5.1) is faster than the original implementation of VByte (which in our tests decompresses 571 million docIDs per second for Unsorted, 576 millions for URL, and 594 millions for IBDA). This is because 1-byte DGaps are the most frequent in inverted indexes and our implementation takes advantage of

178

this fact. Notice that the decompression speed of our VByte implementation is competitive with S9. Dean [11] shows similar approaches to group the flag bits to decode them fast. We think that our implementation can be also useful to speed-up the decompression of text snippets [27].
· RLE S9 on URL yields an enhancement of 84.19% over S9 on URL. For RLE PFD on URL the improvement is of 136.34%. RLE S9 on IBDA, on the other hand, increases the decompression speed by 173.53% over S9 on URL. Finally, for RLE PFD on IBDA the decompression speed is improved by 358.52%.
· RLE PFD shows an enhancement of decompression speed of 34.22% compared to PForDelta on URL. If, on the other hand, we consider RLE PFD on IBDA rather than PForDelta on IBDA, the improvement is of 45.32%.
The results in Table 1 and Table 2 indicate that RLE S9 on IBDA offers the best space vs. decompression-time tradeoff. We can conclude that IBDA by itself is able to improve decompression speed by up to 22% on URL. However, using the RLE encodings on IBDA data is the key to obtain the remarkable speedups that we mention above.
6. RUNS OF 1S AND DAAT PROCESSING
Document-at-a-Time (DAAT) and Term-at-a-Time (TAAT) are the usual ways to support efficient query processing in inverted indexes [10]. DAAT is generally faster and uses less memory than TAAT. Next we adapt DAAT query processing to efficiently handle the runs of 1s in the lists.
6.1 DAAT Query Processing
DAAT processes simultaneously--and from left to right-- the inverted lists Iwi1 , . . . , Iwiq of the query terms. This process is carried out with function nextGEQ(Iwj , d), which yields the smallest docID d in the inverted list Iwj such that d  d. Since nextGEQ will be invoked with increasing values of d, every inverted list maintains a cursor curr with the current position in that list. Thus, function nextGEQ moves the cursor forward and each invocation to nextGEQ starts from the position where the previous invocation stopped.
To support DAAT, inverted lists are compressed using a block layout: each list is divided into blocks of a given amount of DGaps--typically, 128 DGaps per block. For each block, a block header is maintained. Each such header stores, among other things, the largest absolute docID of the block. This is used for skipping at query time. Assume that we must search for docID d in inverted list Iwj . Hence, we invoke nextGEQ(Iwj , d). This function starts from position curr and moves through the block headers comparing d with the last docID in each block, skipping all non-relevant blocks. Once we arrive at the block that potentially stores d, we fully decompress it into an auxiliary Buffer. (This is not completely true if the block has been compressed using VByte, where a DGap per DGap decompression and checking is more efficient.) Next, we move curr through Buffer from left to right, looking for d.
6.2 DAAT and Implicit Run Decompression
As in Section 5.4.3, we do implicit decompression of runs, this time to support DAAT query processing. That is, if when decompressing a block we detect that a run has been

compressed, we do not write every such 1 in Buffer. Instead, we just write the special mark 0 in the corresponding position of Buffer, followed by the length of the run. In other words, the time needed to decompress the whole run will be that needed to decompress just its length (i.e., a single integer) from our RLE schemes.
Assume that, at query time, we invoke nextGEQ(Iwj , d) and that the block that potentially stores d has been decompressed into Buffer. Suppose that, looking for d, we reach position j of Buffer and it holds that Buffer[j] = 0. That is, we have reached a run of 1s of length  = Buffer[j + 1]. Let d denote the docID stored at position j - 1 in Buffer. Notice that the docIDs represented in the run lie in the interval [d + 1, d + ]. If it holds that d +  < d, we can safely skip the run and go on from position k + 2 in Buffer. Otherwise, if d +   d holds, the sought docID lies within the run, hence we cannot skip it. Notice that p = d - d is the position within the run corresponding to docID d. In this case we set d = d (indicating to the subsequent nextGEQ invocation that we have processed up to docID d) and return to the caller. We maintain the values of p and d for the next search on the list. Assume now that we invoke nextGEQ(Iwj , d), for d > d. Assuming that d lies within the same block as d, we check whether d +  - p < d holds. If that is the case, we can skip the run and go on form position k + 2 in Buffer. Otherwise, d also lies within the run, hence we act as before, updating d and p.
This approach can be seen as an abstract optimization of inverted lists, since we only need to change the way in which function nextGEQ is implemented. Hence, this approach can in principle be used by any DAAT-like algorithm [9, 22], obtaining the gains in space usage of Table 1.
6.3 Redefining the Block Layout of the Lists
We need to redefine the block structure of the lists, since they store long segments that have been implicitly represented--the runs. If, as usual, we force the blocks to store a fixed number of DGaps, then we would need to break some long runs in order to accommodate them within a block. This would dilute the benefits of having long runs. Lists are divided into blocks of fixed size because whole blocks must be decompressed to search inside then (as we saw before). Hence, a block cannot store too many DGaps. However, we do not need to decompress runs explicitly, thus the cost of decompressing a run is actually the same as decompressing a single integer. Also, handling a run in Buffer does not incur in considerable extra costs, just checking whether we can skip the run or not. Thus, we define blocks of fixed size, say 128 DGaps, and regard every run as if it were a single DGap. For RLE S9, if it happens that the 128th DGap lies in the middle of an S9 word, then we include the whole S9 word within the block. Hence, S9 blocks can have slightly more than 128 DGaps. In the case of RLE PFD, as we already saw in Section 5.3, it handles runs of length  32.
Notice that in this way the number of blocks in the lists could be reduced, except for RLE PFD. As a result, we would need to store less block headers, thus reducing the space usage. Also, we will see in the experiments below that the number of blocks decompressed at query time is reduced when compared to the results in [30], improving the decompression effectiveness.

179

6.4 Experimental Evaluation
We test now the efficiency of our approaches at query time. We use the original TREC 2006 query log for the GOV2 collection.
6.4.1 Space Usage
Table 3 shows the space usage of the TREC GOV2 inverted index. This time we include the space required by the block headers. Disregarding interpolative encoding, the smallest space is achieved by RLE S9 on docIDs assigned by IBDA. The space is about 11.08% smaller than S9 on URL and uses 21.70% extra space on that of interpolative encoding.

Table 3: Space usage (in MB) of the TREC GOV2

inverted index, for different compression schemes

and docID assignment methods. The space includes

the docIDs and the block headers.

Compression

Reorder Method

Scheme

Unsorted URL IBDA

Interpolative Encoding VByte S9 OptPFD

6,551 8,264 7,524 6,835

3,756 7,464 4,601 5,087

3,685 7,497 4,572 5,105

RLE VByte RLE S9 RLE PFD

8,448 7,524 7,267

4,626 4,485 4,147 4,091 5,003 4,850

6.4.2 AND Queries
Table 4 shows the experimental query time (in milliseconds per query) for different query processing algorithms (in particular, AND, WAND and OR). We compare different compression schemes and reordering methods.
For AND queries, S9 on URL sorting (the best existing trade-off from [30]) achieves 5.53 msec/q (according to our experiments). Using our IBDA technique to order the document collection and then our RLE PFD approach to compress the resulting docID inverted lists, we obtain 4.84 msec/q (an improvement of 12.11%). If, alternatively, we use RLE S9 on IBDA, we obtain 5.14 msec/q (an improvement of 7.05% over S9 on URL). Notice also the following: if we consider S9 and RLE S9, both on URL assignment, the query times changes just slightly: from 5.53 to 5.52, respectively. If, on the other hand, we consider using IBDA instead of URL to assign docIDs, the reduction of query time is bigger. This shows again the effectiveness of IBDA, as it is able to improve the already highly-efficient query times of AND queries [30] (recall, from Section 5.4.1, that we have used a query log to carry out IBDA).
6.4.3 WAND Queries
We also test with the WAND query-processing algorithm [9], which is rather popular in current search engines. We use tf-idf ranking and look for top-10 results. As it can be seen in Table 4, we are able to speed-up the query processing by using IBDA and the RLE compression methods. For instance, by using RLE S9 on IBDA docIDs we can enhance the query time from about 60 msec/q to about 47 msec/q (an improvement of about 20.84%). For RLE PFD on IBDA, the improvement is of about 23.44%. Table 5 shows the average

Table 4: Experimental query time for the different query processing algorithms we tested.

Query

Compression

Reorder Method

Algorithm Scheme

Unsorted URL IBDA

AND

VByte S9 OptPFD

42.85 11.97 11.90

22.88 5.53 5.57

28.88 5.43 5.56

RLE VByte RLE S9 RLE PFD

19.73

7.49

6.91

12.67

5.52

5.14

12.46

5.34

4.84

WAND (top-10)

VByte S9 OptPFD
RLE VByte RLE S9 RLE PFD

175.71 145.99 152.03
184.74 150.87 153.14

76.93 59.50 60.73
66.95 59.14 58.77

75.27 50.34 52.87
54.57 47.10 46.49

OR

VByte

330.39 292.07 288.38

S9

380.90 331.88 327.30

OptPFD

357.28 323.90 319.15

RLE VByte RLE S9 RLE PFD

340.29 475.70 465.35

83.99 141.58 177.10

41.62 59.97 66.03

number of docIDs (in millions) that are decoded per query (runs are regarded as single integers) and average number of blocks (in thousands) that are decompressed per query. Our

Table 5: Millions of docIDs decoded on average

per query ("Ints.") and thousands of blocks decom-

pressed on average per query ("Blks.") for WAND.

The processing is carried out for top-10 results.

Comp.

Reorder Method

Scheme

Unsorted

URL

IBDA

Ints. Blks. Ints. Blks. Ints. Blks.

VByte

10.07 63.29 5.34 24.04 6.23 18.24

S9

10.98 82.42 4.70 34.53 4.50 33.00

OptPFD 10.91 85.30 4.62 36.11 4.42 34.51

RLE VByte 6.93 60.53 1.71 15.51 1.30 11.58

RLE S9

10.89 81.85 3.21 24.21 2.09 15.87

RLE PFD 10.91 85.44 3.60 31.90 2.22 18.24

results indicate that the number of docIDs decoded (as well as the number of decompressed blocks) is reduced in two ways: first, by using an RLE compression scheme; second, by using IBDA rather than URL sorting. For instance, if we use RLE S9 on URL sorting instead of S9 on URL, we obtain a reduction of about 30% in decoded docIDs (the same figure can be observed for decompressed blocks). If we now use RLE S9 on IBDA sorting, we obtain a further reduction of about 34% over RLE S9 on URL. This shows that we are able to improve the decompression effectiveness, as we can process the same queries decompressing less blocks and docIDs. This is not only effective to reduce the query time (as we can see in Table 4) but also in cases where accessing blocks is expensive (for instance, when blocks need to be transferred from secondary storage).

180

Finally, we think that this query-time reduction can be also achieved by the Block-Max WAND algorithm from [22].
6.4.4 Full-OR Queries
We use now the run-length encoding of inverted lists to improve the performance of OR queries. In particular, full OR queries, where the full result (rather than a ranking) must be obtained. The main application is the offline merging of inverted lists.
Let I1 and I2 be the involved inverted lists. The search algorithm proceeds as usual for DAAT OR queries. However, if while processing I1 we arrive at a run whose docIDs define the interval [d, d + ], we switch to I2 and look for d +  + 1 in it. If we find that d +  + 1 also lies within a run [d, d +  + 1] in I2, then we switch to I1 again and look for d +  + 1 in it. We keep repeating this process until the current docID d we are looking for is not within a run. In this case, we can report the run-length encoding of the interval [d, d - 1]. This allows us to compute the union of these intervals without decompressing them. Notice also that runs are written in the output already encoded. We call RLE OR this process.
Table 4 shows the experimental results. For VByte, S9 and PForDelta we use the traditional DAAT OR processing. As it can be seen, RLE OR introduces relevant improvements. In particular, for RLE VByte on IBDA, where we obtain a reduction of 85.75% compared to VByte on URL. The rationale behind this exceptional performance of RLE VByte is that it is the only RLE scheme able to catch short runs. These are rather frequent in practice, which explains the good performance.
6.5 Inverted List Caching
We study now how the RLE compression methods and the IBDA docID assignment behave when compressing static inverted-list caches [17]. We show that the RLE compression schemes on docIDs assigned with IBDA are more suitable for compressing a small set of inverted lists, rather than using the classic compression schemes on a global docID assignment method, such as URL sorting.
In our experiments, we use as a building block the TREC GOV2 inverted index with docIDs assigned by URL sorting. We use the TREC 2006 query log to compute the frequency of the query terms. We then sort the inverted lists according to this frequency (from most frequent to least frequent). Next, we consider the top-Q lists in this sorting, for Q = 1 thousand, 2 thousands, . . . , 10 thousands. For each such Q, we apply IBDA on the resulting set of lists, using the order we already mentioned.
Figure 1 shows the space usage as a function of the number of lists considered in each case. We also show a table with the cache hit ratio achieved for every Q. We include just S9 compression, since similar results are obtained for the remaining methods (yet, S9 yields the smallest space usage among all other schemes). As it can be seen, RLE S9 compression yields a considerable reduction of space usage, either on URL and IBDA sorting. For instance, we can conclude that 10 thousand lists compressed with RLE S9 on IBDA sorting use about the same amount of space than 2 thousand lists compressed with S9 on URL. In other words, within the same space used by S9 on URL to achieve a 24% hit ratio, RLE S9 on IBDA is able to achieve a 51% hit ratio.
An important feature that must be noted is that the re-

Cache size (MB)

1800

Cache compression

1600

1400

1200

1000 800

S9 on URL RLE S9 on URL
S9 on IBDA RLE S9 on IBDA

600 1

2 3 4 5 6 7 8 9 10 Number of cached lists (times 1,000)

Lists: 1 2 3 4 5 6 7 8 9 10 % Hit ratio: 13 24 31 37 40 42 44 46 49 51

Figure 1: (Above) Cache space usage as a function of the number of inverted lists compressed, using variants of S9. (Below) Cache hit ratio achieved for the different number of lists.

duction of space usage degrades as we compress more lists. In our experiments, for 1,000 lists and using RLE S9 on IBDA the reduction is of about 31%. For 10,000 lists, on the other hand, the reduction is of about 23% (recall that for the whole index we obtain a reduction of about 10%, recall Tables 1 and 3).
7. CONCLUSIONS AND FUTURE WORK
We have shown that using our docID assignment scheme IBDA (for focusing the generation of run on a given set of inverted list) and then run-length encoding these lists yields better general performance. This reinforces and improves the results obtained in previous related work [30]. We obtain an improvement of about 10% in space usage compared with the (already very efficient) results from [30]. If we compare our decompression speed with that of [30], we are up to 1.22 times faster if the runs must be explicitly decompressed and up to 4.58 times faster if implicit decompression of runs is allowed. DAAT query processing can be also improved using our approaches: AND queries can be improved by up to 12%, WAND queries by up to 23% and full OR queries by up to 86%. Finally, we have shown that our approaches are useful for the efficient caching of inverted lists, achieving a reduction in space usage of up to 31%.
As future work, we plan to test our approaches to improve the performance of the Block-Max WAND algorithm of [22]. It would be also interesting to use the approach of [25] along with our approaches. Also, it would be interesting to adapt our schemes to graph compression [7]. That is, adapting our IBDA scheme to generate runs in the adjacency lists to then run-length encode these lists.
8. ACKNOWLEDGEMENTS
The authors want to thank the thorough comments from the anonymous referees, which definitely helped to improve the quality of this paper.

181

9. REFERENCES
[1] V. N. Anh and A. Moffat. Inverted index compression using word-aligned binary codes. Information Retrieval, 8(1):151­166, 2005.
[2] V. N. Anh and A. Moffat. Improved word-aligned binary compression for text indexing. IEEE Trans. Knowl. Data Eng., 18(6):857­861, 2006.
[3] D. Arroyuelo, S. Gonz´alez, M. Marin, M. Oyarzu´n, and T. Suel. To index or not to index: time-space trade-offs in search engines with positional ranking functions. In Proc. of 35th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 255­264. ACM, 2012.
[4] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval - the Concepts and Technology Behind Search, Second Edition. Pearson Education Ltd., Harlow, England, 2011.
[5] R. Blanco and A. Barreiro. Document identifier reassignment through dimensionality reduction. In Proc. of 27th European Conference on IR Research (ECIR), LNCS 3408, pages 375­387. Springer, 2005.
[6] D. Blandford and G. Blelloch. Index compression through document reordering. In Proc. of 2002 Data Compression Conference (DCC), pages 342­351. IEEE Computer Society, 2002.
[7] P. Boldi and S. Vigna. The webgraph framework I: compression techniques. In Proc. of 13th Int. Conference on World Wide Web (WWW), pages 595­602. ACM, 2004.
[8] S. Brin and L. Page. Reprint of: The anatomy of a large-scale hypertextual web search engine. Computer Networks, 56(18):3825­3833, 2012.
[9] A. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In Proc. of 2003 ACM CIKM Int. Conf. on Information and Knowledge Management, pages 426­434. ACM, 2003.
[10] S. Bu¨ttcher, C. Clarke, and G. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, 2010.
[11] J. Dean. Challenges in building large-scale information retrieval systems: invited talk. In Proc. of 2nd Int. Conf. on Web Search and Web Data Mining (WSDM), page 1. ACM, 2009.
[12] S. Ding, J. Attenberg, and T. Suel. Scalable techniques for document identifier assignment in inverted indexes. In Proc. of 19th Int. Conference on World Wide Web (WWW), pages 311­320. ACM, 2010.
[13] P. Elias. Universal codeword sets and representations of the integers. IEEE Transactions on Information Theory, 21(2):194­203, 1975.
[14] S. Golomb. Run-length encoding. IEEE Transactions on Information Theory, 12(3):399­401, 1966.
[15] D. Johnson, S. Krishnan, J. Chhugani, S. Kumar, and S. Venkatasubramanian. Compressing large boolean matrices using reordering techniques. In Proc. of 30th Int. Conf. on Very Large Data Bases (VLDB), pages 13­23. Morgan Kaufmann, 2004.
[16] J. Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604­632, 1999.
[17] H. T. Lam, R. Perego, N. T. M. Quan, and

F. Silvestri. Entry pairing in inverted file. In Proc. of 10th Int. Conf. on Web Information Systems Engineering (WISE), LNCS 5802, pages 511­522. Springer, 2009.
[18] C. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, 2008.
[19] A. Moffat and L. Stuiver. Binary interpolative coding for effective index compression. Information Retrieval, 3(1):25­47, 2000.
[20] D. Salomon. Data compression - The Complete Reference, 4th Edition. Springer, 2007.
[21] W.-Y. Shieh, T.-F. Chen, J. J.-J. Shann, and C.-P. Chung. Inverted file compression through document identifier reassignment. Information Processing and Management, 39(1):117­131, 2003.
[22] D. Shuai and T. Suel. Faster top-k document retrieval using block-max indexes. In Proc. of 34th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 993­1002. ACM, 2011.
[23] F. Silvestri. Sorting out the document identifier assignment problem. In Proc. of 29th European Conference on IR Research (ECIR), LNCS 4425, pages 101­112. Springer, 2007.
[24] F. Silvestri, S. Orlando, and R. Perego. Assigning identifiers to documents to enhance the clustering property of fulltext indexes. In Proc. of 27th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 305­312. ACM, 2004.
[25] F. Silvestri and R. Venturini. Vsencoding: efficient coding and fast decoding of integer lists via dynamic programming. In Proc. of 19th ACM Conf. on Information and Knowledge Management (CIKM), pages 1219­1228, 2010.
[26] N. Tonellotto, C. Macdonald, and I. Ounis. Effect of different docid orderings on dynamic pruning retrieval strategies. In Proc. of 34th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 1179­1180. ACM, 2011.
[27] A. Turpin, Y. Tsegay, D. Hawking, and H. Williams. Fast generation of result snippets in web search. In Proc. of 30th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 127­134, 2007.
[28] H. Williams and J. Zobel. Compressing integers for fast file access. The Computer Journal, 42(3):193­201, 1999.
[29] I. Witten, A. Moffat, and T. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images, 2nd Edition. Morgan Kaufmann, 1999.
[30] H. Yan, S. Ding, and T. Suel. Inverted index compression and query processing with optimized document ordering. In Proc. of 18th Int. Conference on World Wide Web (WWW), pages 401­410. ACM, 2009.
[31] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Comput. Surveys, 38(2), 2006.
[32] M. Zukowski, S. H´eman, N. Nes, and P. Boncz. Super-scalar RAM-CPU cache compression. In Proc. of 22nd Int. Conf. on Data Engineering (ICDE), page 59. IEEE Computer Society, 2006.

182

Fast Document-at-a-time Query Processing using Two-tier Indexes

Cristian Rossi
Univ. Federal do Amazonas
cristiManan.ianufos,rA@Mg, Bmraazilil.com

Edleno Silva de Moura
Univ. Federal do Amazonas
edlenMoa@naicuso,mAMp,.uBfraamzil.edu.br

Andre Luiz Carvalho Altigran Soares da Silva

Univ. Federal do Amazonas

Univ. Federal do Amazonas

andreM@anicauosm,ApM.u, fBarmaz.iel du.br alti@Miacnoamusp,.AuMfa, mBr.aezdilu.br

ABSTRACT
In this paper we present two new algorithms designed to reduce the overall time required to process top-k queries. These algorithms are based on the document-at-a-time approach and modify the best baseline we found in the literature, Blockmax WAND (BMW), to take advantage of a two-tiered index, in which the first tier is a small index containing only the higher impact entries of each inverted list. This small index is used to pre-process the query before accessing a larger index in the second tier, resulting in considerable speeding up the whole process. The first algorithm we propose, named BMW-CS, achieves higher performance, but may result in small changes in the top results provided in the final ranking. The second algorithm, named BMW-t, preserves the top results and, while slower than BMW-CS, it is faster than BMW. In our experiments, BMW-CS was more than 40 times faster than BMW when computing top 10 results, and, while it does not guarantee preserving the top results, it preserved all ranking results evaluated at this level.
Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous; D.2.8 [Software Engineering]: Metrics--complexity measures, performance measures
Keywords
Top-k Query Processing, Efficiency, Two-tier indexes
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1. INTRODUCTION
Computing ranking of results by using information retrieval (IR) models is one of the core tasks of search systems. While search systems often index a massive number of documents, usually their users are not interested in an in-depth list of results related to a query, but rather to a small list of highly relevant documents that will satisfy their informational needs. Thus, part of the recent research related to search systems is aimed at improving the quality of the top results presented to users, instead of the overall quality of the presented list. This focus on a narrower set of high quality results has led to the development of a number of technologies to improve the efficiency of methods to compute the top results in search systems.
When determining the best results for a given query, a search system usually deploys a number of different sources of relevance evidence. For instance, web search engines use information such as titles of the pages, URL tokenization, and link analysis, among others. These sources of relevance evidence may be combined using a myriad of approaches, such as the adoption of learning to rank techniques. Even in these cases, the initial process of computing the ranking consists of applying a basic IR model, such as BM25 [11] or the Vector Space Model [12], to compute an initial rank of top results, typically limited to the size of just about one thousand documents [5].
In this paper we propose two new algorithms which reduce the overall time required to compute the final query ranking. The algorithms modify the best baseline we found in the literature, the Blockmax WAND (BMW) [6], to take advantage of a two-tiered index. The first algorithm we proposed, named BMW-CS, from BMW using the first tier as a candidate selector, uses the first tier to select candidate documents that are taken into account to compute the final ranking when processing the second tier. It achieves higher performance, but may result in small changes in the top results provided in the final ranking. In BMW-CS, the entries of the first tier are not present in the second tier. The second algorithm, named BMW-t, from BMW using the first tier as a threshold selector, preserves the top results and, while slower compared to BMW-CS, it is faster than BMW. The first tier is used only to compute a safe initial threshold to be adopted when processing the second tier, thus allowing

183

a slightly faster query processing when compared to BMW. The price paid is that the second tier in BMW-t should contain the full index and the first tier is an extra index.
While there were previous approaches based on using a smaller index tier to improve efficiency, our proposed algorithms are designed to take advantage of dynamic pruning techniques, not to minimize the effort of processing answers from the first tier, but to reduce the time required to read and process entries from the second tier.
The remainder of this article is structured as follows. Section 2 presents the background and related research necessary to better understand the proposed methods. Section 3 presents our methods, BMW-CS and BMW-t. Section 4 presents the experimental results. Finally, Section 5 presents the conclusion and prospective future work.
2. BACKGROUND AND RELATED WORK
Usually, most of the data needed by a search system to process user queries is stored in data structures known as inverted files [3]. They contain, for each term t, the list of documents where it occurs and an impact factor, or weight, associated with each document. This list of pairs of document and term impacts is called the inverted list of t and it is used to measure the relative importance of terms in the stored documents. Each document is represented in these lists by a value named document id, referred to as docId in this article. The inverted files may become huge in some systems, thus they are usually stored in a compressed format.
2.1 TAAT approach
In a query processing approach named Term-At-A-Time (TAAT), the inverted lists are sorted by term impact in nonincreasing order. The query results are obtained by sequentially traversing one inverted list at a time. As an advantage, we can mention the fact that a sequential access behavior to inverted lists may speed up the process.
As the main disadvantage, we can mention that this strategy requires the usage of large amounts of memory to store partial scores achieved by each document when traversing each inverted list. These partial scores should be stored to accumulate the score results obtained by each document when traversing each inverted list. The final ranking can be computed only when all the inverted lists are processed. Several authors proposed methods to discard partial scores, thus reducing the amount of memory required to process queries in the TAAT mode [15, 2, 1]. Despite the significant reduction of required memory for query processing, the space required to store the partial scores remains one of the drawbacks of the TAAT query processing approach.
Anh and Moffat [1] studied the application of dynamic pruning over inverted indexes where the entries are sorted by impact, thus adopting a TAAT approach. In their work, a first phase processes blocks containing entries with higher impact in a disjunctive mode. Once all documents that might be present in the top-k results, k being a parameter, are found, the method starts a second phase applying a conjunctive mode query processing where only documents included as results by the first phase are considered. They also present a modified version of their algorithm, named as method B, where the top results may be changed, thus giving approximate results. In method B, only a percentage of the results obtained in the first phase are taken into account

in the second phase. This modified version results in even faster query processing, but does not guarantee top k results of the ranking will not be modified.
Strohman and Croft [15] proposed a new method for efficient query processing when documents are stored in main memory that modifies the method presented by Anh and Moffat [1]. In their proposal, a dynamic pruning is applied in each phase of query processing over the candidate documents with the goal of obtaining the final query results without requiring a full evaluation of all candidates. The query processing is performed over impact sorted inverted lists and the impact values are discretized in a small range of integer values.
2.2 DAAT approach
Another approach to process queries is to adopt a DocumentAt-A-Time (DAAT) query processing. In this alternative, the inverted lists are sorted by docIds, which allows the algorithms to traverse all the inverted lists related to a query in parallel. As a consequence, the final scores of documents can be computed while traversing the lists and the system may store only the top results required by the user, so the memory requirements are fairly smaller. On the other hand the fragmented access may slow down the query processing and, since the inverted lists are sorted by docIds, important entries are spread along the inverted lists, making the pruning of entries more complex in this query processing approach.
As in the TAAT approach, several authors have presented algorithms and data structures to accelerate the query processing in the DAAT approach. For instance, data structures to allow fast jumps in the inverted lists, named as skiplists [7], are adopted to accelerate the query processing. Skiplists divide the inverted lists into blocks of entries and provide pointers for fast access to such blocks, so that a scan in the skiplist determines in which block a document entry may occur, if it does, in the inverted list.
The problem of efficiently computing the ranking of results for a given user query has been largely addressed in the literature in several research articles. We here detail the ones closer to our research, focusing on DAAT, which is the approach adopted by our algorithms.
2.2.1 WAND
Broder et al [4] proposed a successful strategy for query processing, known as WAND, which allows fast query processing for both conjunctive and disjunctive queries, with the possibility of configuring the method for preserving the top k documents in the query answer or not. In the case where the top answers are not guaranteed to be preserved, the method leads to even faster query processing.
WAND processes the queries using DAAT approach, so the inverted lists of the query terms are traversed in parallel. A heap of scores is created to keep the top k documents with larger scores at each step of the query processing, k being the number of documents requested to the search system. The smaller score in the heap at each moment is taken as a discarding threshold to accelerate the query processing. A new document is evaluated and inserted in the heap only if it has a score higher than this discarding threshold.
The WAND has a two level evaluation approach. In the first level, a document pivot has its maximum possible score evaluated using the information of maximum score of each

184

list where the document may occur. If the maximum possible score is greater than the actual discarding threshold, the document has its actual score evaluated. Otherwise, the document is discarded and a new pivot is selected.

(matex rsmcore)

t1

...

(7)

60 70 ...

t2
(8)

... 40 90 ...

t3
(5)

...

50 95 ...

Figure 1: Inverted lists when processing query with terms t1, t2, and t3. Marked entries are the ones currently processed.

At each moment in the DAAT query processing, there is a pointer to the next document to be processed in each inverted list associated with the query. For instance, if we have a query with terms t1, t2 and t3, there will be a pointer to the next document processed in each of their lists, as illustrated in Figure 1. In the Figure, documents 60, 40 and 50 are currently pointed in lists t1, t2 and t3, respectively. The WAND algorithm assures that previous occurrences in each list were already examined and that each of the pointed documents represents the smaller docId in the list that was not yet processed [4]. Thus, since the lists are organized by docIds, at this point we know the smaller docId (40) occurs only in the list of term t2, while document 50 occurs in t3 and might occur in t2. Finally, document 60 occurs in the list of t1 and might occur in the other two lists.
At this point, knowing the maximum score a document may reach in each list, we can estimate the maximum scores each of the currently pointed documents can obtain when processing the query: 40 can reach maximum possible score equal to the maximum score a document may achieve in the list of t2; 50 can reach the sums of maximum scores of t2 and t3, and 60 can reach the sum of maximum scores of the three terms as its maximum possible score. Using this information, we may discard the documents which are not able to reach the discarding threshold, i.e., cannot achieve a score higher than the minimum score among the top k results already processed at this moment.
So, to discard documents, we check the current documents pointed in each inverted list associated with the query and estimate their maximum possible score. The entry with smaller docId among the ones that reach a maximum possible score higher than the current discarding threshold is then chosen as a candidate to be included in the answer. This entry is known as the pivot. We then move all posting lists so that they point to docIds of at least the same as the pivot. Notice only pointers of lists where the current docIds are smaller than the pivot require a movement. If one of the posting lists with docId smaller than the pivot does not have the pivot document, the document is discarded, a new pivot is selected and the process repeated. Otherwise, if all these lists contain the document, its actual score is then computed. If the actual score of the pivot is higher than the discarding threshold, it is included in the answer set and the discarding threshold is updated. After processing the pivot,

we move all the lists to the next document with docId bigger than the pivot and start the process again.
An interesting feature of the WAND method is that it can be configured as a more or less aggressive pruning method by either applying a reduction in the max score values of each list or by increasing the pruning threshold so that fewer documents will be accepted in the top results. When the pruning threshold is increased, there is no guarantee of preserving the exact top result set. Further details about the WAND method can be found in the article where it was first proposed [4].
2.2.2 Blockmax WAND
Ding and Suel [6] have recently revisited the ideas presented in the WAND method, and proposed an even faster solution named the Blockmax WAND (BMW). In BMW, the entries of the inverted lists are grouped into compressed blocks that may be jumped without any decompression of their content. Each block contains information about the maximum impact among the entries found in it. Whenever such maximum impact is not relevant to change the results for a given query, the whole block is discarded, which avoids important query processing costs.
Authors present experiments which indicate BMW results in significant reduction of query processing times when compared to previous work, being currently the fastest query processing method found by us.
The BMW is based on the WAND algorithm and uses the same approach for selecting a document pivot during the query processing. In BMW, the pruning of entries is performed using two main pieces of information: (i) the maximum impact found inside an inverted list, which is also adopted in WAND; and (ii) The maximum impact found in each block pointed by the skiplist entries, known as the block max score, so that before accessing a block it is possible to predict the maximum impact of an entry among those found in such a block.
The basic idea of BMW is to take advantage of information (ii) to speedup query processing. Once a candidate document is selected to have its score computed, the algorithm uses the block max score information to accelerate the query processing by discarding documents with no chance of being in the top answers. The algorithm also allows skipping entire blocks of inverted list entries based on the block max score present on the skiplists, a procedure that they call shallow movement. Contrary to the regular movement present on the inverted lists, the shallow movement accesses only the skiplist entries, which avoids costs to decompress blocks of the inverted lists when processing queries.
The BMW algorithm starts with a pivoting phase, where a document is selected as a candidate to be inserted in the answer. Its pivoting phase is similar to the one performed in the WAND algorithm. Using the global max score of each list and the lists ordered by the value of the current docId pointed in each of them.
Before decompressing and evaluating the pivot document, BMW makes a shallow movement to align the inverted lists over the blocks that possibly have the document. After the alignment, the algorithm uses the information of block max score, stored in the skiplists, to estimate a local upper bound score of the candidate document. If this upper bound score is lower than a given pruning threshold, the document is discarded and one of the lists is advanced.

185

As in WAND, the pruning threshold is dynamically updated according to the score of the evaluated candidate. As the processing is DAAT, each evaluated candidate has its complete score calculated, since all inverted lists are processed in parallel. Further details about the BMW algorithm can be found in the article where it was proposed [6].
Shan et al [13] show that the performance of BMW is degraded when static scores, such as Pagerank, are added in the ranking function. They study efficient techniques for Top-k query processing in the case where a page's static score is given and propose a set of new algorithms based on BMW which outperform the existing ones when static scores are taken into account when computing the final ranking. Their study can also be applied to our proposal as a future work, being orthogonal to the study presented here.
2.3 Multi-tier indexes
Some authors proposed the splitting of the inverted index in more than one tier in an attempt to accelerate query processing. In these architectures, the query processing starts in a small tier and only if necessary proceeds to larger tiers. Risvik and Aasheim [10] divided the index into three tiers with the goal of achieving better scaling when distributing the index. According to static and dynamic sources of evidence, documents considered as more relevant are selected to compose the smaller tier. The query processing starts in the higher tier and only if the result set is not satisfactory, according to an evaluating algorithm, the query processing proceeds to the next tier. Performance gains are achieved when the processing does not visit the larger tiers. There is no guarantee that the result set is the exact set if compared to the exhaustive query processing.
In our proposed algorithms, we also kept the documents considered more important, in our case those with higher impact, in the smaller tier. However, in this paper we use each tier as part of the query processing. Our algorithms could, for instance, be applied to each tier proposed by Ravisk and Aesheim [10], being then ortoghonal to their proposal.
Ntoulas and Cho [8] presented a two-tiered query processing method that avoids any degradation of the quality of results, always guaranteeing the exact top-k results set. The first tier contains the documents considered the most important to the collection, selecting the entries by using static and dynamic pruning strategies that remove non-relevant documents and terms. The second tier contains the complete index. Their proposal uses the first tier as a cache level to accelerate query processing. Whenever the method detects that the results of the first tier assures that the top results will not be changed, it does not access the second tier, basing its results only on the first tier. Otherwise, they process the query using the second tier.
To guarantee that their method preserves the top answer results when compared to a system without pruning, the method evaluates the ranking function when processing the first tier, assigning the maximum possible score that could be achieved when processing the full index for entries that are not present in the first tier. The authors show how to determine the optimal size of a pruned index and experimentally evaluate their algorithms in a collection of 130 million Web pages. In their experiments, the presented method achieved good results for first tier index sizes varying from 10% to 20% of the full index. The two-tier strategy is also adopted in our article, but instead of using the first tier as

a cache, we use it as a candidate selection layer as part of our algorithms to compute the top results of a given query. Another important difference is that we always process the queries using both tiers, and we do not guarantee the exact top-k results.
Skobeltsyn et al [14] evaluate the impact of including a cache in the system when using the two-tier method presented by Ntoulas et al [8] and shows the query distribution workload is affected by the cache system. When using the cache, queries with a few terms, which would be the ones that would benefit more from the two-tier strategies, are usually solved by the cache system. As a consequence, the queries with more terms, which are difficult to process with pruning strategies, become more important in the workload when considering a system that adopts a cache of results.
3. BMW-CS AND BMW-T
In this section, we present the details of the algorithms we proposed to accelerate the query processing when computing the ranking of results, which are named BMW-CS, from BWW with candidate selection, and BMW-t, from BMW with a threshold selection. These algorithms are based on a two-tiered index organization in which the first tier is a small index created using the entries with the highest impact from each term list, while the second tier is a larger index. The idea of relying on a high-quality tier is similar to the one adopted by Ntoulas et. al. [8].
The main objective of our algorithms is to use the first tier to accelerate the query processing in the second tier (i.e., the larger index) when computing the final ranking. In BMW-CS, the two tiers are disjointed, that is, the highimpact entries in the small index in first tier are not present in the second one. In BMW-t, the second tier contains the full index. Thus, even the high-impact entries in the first tier are also present in it.
In both cases, to select the entries for the first tier, we compute a global threshold to select entries so that the size of the first tier is about % of the full index. The parameter  provides an estimation of the final size of the first tier index. We adopted a minimum size of 1000 entries in each inverted list to prevent any individual list from becoming too small.
In our index organization, we used skiplists in both tiers in order to accelerate the query processing. For each block of 128 document entries, a skiplist entry is created that keeps the information of the current docId and the highest impact among the documents of the block. Also, for each term in the collection, the highest impact (max score) in the whole inverted list and the lowest impact found in this list in the first tier are computed and stored along with the term information. The lowest impact in the list at the first tier can be seen as an upper bound for the impact of the document entries that were not included in this list. Using this information, we can set the upper bound of contribution for the entries that are not present in the first tier, but appear in the inverted list in the second tier.
3.1 BMW-CS
Listing 1 presents our first algorithm called BWM with candidate selection, or BMW-CS. In its first phase, BMWCS uses the first tier to select documents that are candidates to be present in the top results. Initially, the set of candidate documents, denoted by A, is generated by the function

186

Listing 1: Algorithm BMW-CS

1 BMW-CS(queryTerms [ 1 . . q] , k)

2 A  SelectCandidates(queryTerms, k)

3
4 sort A by score 5 min score  Ak . score

6

7 //Remove the candidates with low upper score

8 for (i = 0 to |A|)

9

i f (Ai . upper score < min score) remove Ai

10 end for

11

12 R  CalculateCompleteScore(A , queryTerms, k)

13
14 return R

SelectCandidates, which computes the candidates for the top k results of a query composed of a set of terms (Listing 2). Then, the algorithm trims this set of documents, removing all candidates that cannot be present in the topk results. This trimming decreases the cost of the second phase. In the second phase, the second tier is processed to compute the final ranking of the top k results. This phase is performed by the function CalculateCompleteScore (Listing 6).
The main idea behind BMW-CS is to take advantage of the fact that the first tier has entries with higher impact in order to significantly reduce the amount of documents analyzed in the second tier. We show in the experiments that this approach yields a quite competitive query processing algorithm.
3.1.1 Candidates Selection
BMW-CS selects candidate documents from the first tier. However, the inverted lists in the first tier are not complete, which means, for instance, that a document which appears only in the list of one of the terms of a query in the first tier, may appear in lists of other terms of this query when considering the entries present in the second tier. Thus, during the candidate selection phase, the algorithm may discard a document that could have a high enough score when the full inverted lists are evaluated.
To reduce the possible negative impacts of this incompleteness of the lists in the first tier, we modified the BMW algorithm so that it considers the possibility of a missing pair (term, docId) in the first tier to occur in the second tier. During the pivoting phase and the upper boundary score checking, a lower boundary score is added for each missing term of the document that might be absent in the first tier, thus avoiding the possibility of discarding high score candidates due to incompleteness in the first tier.
This lower boundary score represents the max score that a document can achieve after processing the inverted list in the second index. With these two values, we can adjust the discarding threshold used to prune documents in BMW. A minimum heap is used to store the top-k documents with a higher score. The smallest score of the heap is used as the discarding threshold for BMW to dynamically prune entries with no chance to be part of the final top-k results. All evaluated documents that have a score higher than the discarding threshold when processing the first tier are added to the set of candidate documents.
We can see the detailed algorithm for the candidate selection phase in Listing 2. The algorithm starts by selecting

Listing 2: Algorithm SelectCandidates

1 SelectCandidates (queryTerms [ 1 . . q] , k) 2 Let H be the minimum heap to keep the top k results 3 Let A be the l i s t of candidates 4 Let Icand be the f i r s t t i e r index

5

6 l i s t s  Icand (queryTerms) ;// Gets inverted l i s t s 7   0;

8 //Point to the f i r s t docId in each l i s t

9 for each {0  i < | l i s t s |} do Next( l i s t s [ i ] , 0) ;

10
11 repeat

12

sortByCurrentPointedDocId( l i s t s ) ;

13

p  Pivoting( lists , ) ;

14

i f (p == -1) break ; //No more candidates

15

d  l i s t s [p ] . curDoc;

16

i f (d == M AXDOC ) break ; //End of the l i s t

17

18

//Move only the skip pointers

19

for each {0  i  p} do NextShallow( l i s t s [ i ] , d) ;

20

21

i f ( CheckBlockMax( , p) == T RU E )

22

i f ( l i s t s [ 0 ] . curDoc == d)

23

doc . docId  d;

24

doc . score 

p i=0

BM25( l i s t s [ i ] ) ;

25

doc . upper score  doc . score +

26

|lists| i=p+1

l i s t s [i ] . min score ;

27

28

i f (|H| < k) H  H  doc ;

29

else i f (H0 . score < doc . score )

30

remove H0 ; // the one with smallest score

31

H  H  doc ;

32

  H0 . score ; //Update the threshold

33

end i f

34

35

//Insert only documents with possible score > 

36

i f ( <= doc . upper score)

37

doc . terms  queryTerms [ 0 . . p ] ;

38

A  A  doc ;

39

i f ({dLow  A| dLow. upper score < })

40

A  A - dLow ;

41

endif

42

endif

43

//Advance a l l evaluated l i s t s

44

for each {0  i  p} do Next( l i s t s [ i ] , d+1);

45

else

46

j  {x| l i s t s [x ] . curDoc < d ^

47

| l i s t s [x] | < | l i s t s [ y ] |, 0  y < p} ;

48

Next( l i s t s [ j ] , d) ;

49

end i f

50

else

51

d next  GetNewCandidate( l i s t s [ j ] , p) ;

52

j  {x| | l i s t s [x] | < | l i s t s [ y ] | , 0  y  p} ;

53

Next( l i s t s [ j ] , d next) ;

54

end i f

55 end repeat

56
57 return A

the inverted lists to be processed (Line 6), which are the lists that represent each query term. The discarding threshold, , is initially set to 0(Line 7) and is updated to the minimum score stored in the heap H if it is full (Line 32). Line 9 makes each of the inverted lists point to their first document. The function N ext(l, d) searches in the skiplist associated with list l for the block where there is the first occurrence of a docId equal or bigger than d, setting l.current block to the found position. Then, it moves the pointer to the current document of the list, l.curDoc, to the smallest entry with value greater than d.
The lists shown in Listing 2 are represented by vector lists and each list has an internal pointer to the docId being processed at each moment, the current docId. In Line 12 we sort this vector into increasing order according to the current docId pointed by each of these lists. We then compute in Line 13 the next document that has a chance to be present in the top results, performing the pivoting, which is

187

Listing 3: Algorithm Pivoting

1 Pivoting ( lists , )

2 accum  0;

3 for each 0  i < |lists| do

4

accum  accum + l i s t s [ i ] . max score ;

5

accum min 

|lists| j=i+1

l i s t s [j ] . min score

6

i f (accum + accum min >= )

7

while(i+1<|l i s t s | AND

8

l i s t s [ i+1].curDoc == l i s t s [ i ] . curDoc) do

9

i  i + 1;

10

end while

11

return i

12

end i f

13 end for

14 return -1;

Listing 4: Algorithm CheckBlockMax

1 CheckBlockMax ( l i s t s , p, )

2

3 //Sum the max score of each block , that d can appear

4

max 

p i=0

l i s t s [ i ] . getBlockMaxScore() ;

5

6 //Add the min score of the l i s t s that d may appear in the f u l l index

7

max  max +

|lists| i=p+1

l i s t s [i ] . min score ;

8 i f (max > ) return true

9 return f alse

the main step of the BMW heuristic. Our pivoting, however, is computed taking into consideration the possibility of some of the entries of a document being not included in the first tier, which let us add this information to compute the upper score, which is the maximum score when selecting the pivot. This procedure is described in Listing 3. Lines 15 to 17 test break conditions and set the current document to be analyzed by the algorithm.
Line 19 adopts the function NextShallow to move the current documents pointer in each list. The function NextShallow is the same presented in the original BMW proposal, and differs from function Next because it does not need to access the documents, accessing only the skiplists of the inverted lists to move their pointers and set a new current block in each inverted list. Using this function, we can skip entries without needing to access or decompress them. Line 21 calls function CheckBlockMax, detailed in Listing 4, which is also modified when compared to the original one proposed in BMW, since it also needs to deal with the incompleteness of the first tier.
The remaining algorithm checks whether a document has enough score to be included in the answer. Line 25 takes the incompleteness of the first tier into consideration when computing the upper score. The score of each document is used to include it in heap H, Lines 28 to 33. H is maintained to control the discarding threshold . The upper score of each document is used in Line 36 to check whether a document should be included in the candidate documents set A.
The threshold  changes as more documents are processed, so, whenever we add a document to A, we also check if there is at least one document in A with an upper score value lower than the current value of . In such cases, we remove the document (Lines 39 to 40). This procedure avoids wasting memory by keeping elements in A which will be discarded at the end of the process. By the end of the candidate selection algorithm, the list of candidate documents A is returned, so that the final result can be obtained by processing the remainder of the index in the second tier.
3.1.2 Computing the Final Ranking
In function CalculateCompleteScore (Listing 6), the scores of candidates with missing terms are evaluated using the larger index in the second tier, which contains the index entries not present in the first tier. To avoid unnecessary costs with decompression, the shallow movement described in [6] is used to align all the term lists. Then, a second BlockMaxScore check is made to verify whether the document

Listing 5: Algorithm GetNewCandidate

1 GetNewCandidate ( l i s t s , p)

2 mindoc  M AXDOC

3

4 //Selects the lower docId between the blocks boundaries

5 // of the l i s t s already checked

6 for each {0  i  p} do

7

i f (mindoc > l i s t s [ i ] . getDocBlockBoundary() )

8

mindoc  l i s t s [ i ] . getDocBlockBoundary() ;

9

end i f

10 end for

11

12 //Select the lower docId between the l i s t s not checked

13 for each {p + 1  i < |lists|} do

14

i f (mindoc > l i s t s [ i ] . curDoc)

15

mindoc  l i s t s [ i ] . curDoc;

16

end i f

17 end for

18

19 return mindoc; //Return the smallest docId found

can be part of the top k results or not. Each document is evaluated only if it has a high enough score, otherwise it is discarded. As in the first phase, we keep a minimum heap with the documents with the greatest scores evaluated, and the minimum score of this set is used as a discarding threshold to prune candidates.
As the candidate set is small, and only documents with incomplete scores are evaluated, this phase is expected to be performed extremely fast, even considering that it processes a the larger tier.
In BWM-CS, the first tier may not contain enough information to assure all top documents are considered as candidate documents. Since only candidate documents can be included in the final results, it does not guarantee exact results in the final ranking. For instance, a document which contains entries for all three terms of a query, but whose entries are present only in the second tier, will not be included in the candidate selection. However, this document may achieve scores higher than the ones in the top documents found by the candidate selection, in cases where there are top results that do not contain all query terms. Notice however that such a situation tends to occur for documents that would be included in the final positions of the top results and, as we show in our experiments, is it does not affect the final results very much.
3.2 BMW-t
In our second algorithm, BMW with threshold selection, or BMW-t, we use just the first tier to set the initial discarding threshold adopted by methods WAND and BMW. In these methods, this initial discarding threshold is set to 0 at the beginning, and grows as the documents with higher scores

188

Listing 6: Algorithm CalculateCompleteScore

1 CalculateCompleteScore( A , queryTerms [ 1 . . q] , k) 2 Let H be the minimum heap to hold the k most relevant
candidates

3 Let Isecond tier be the second index 4 l i s t s  Isecond tier (queryTerms) //Select the terms l i s t s 5 0
6 H 7 sort A by docId ;

8

9 for each {0  i < |A|} do

10

i f (Ai . score < Ai . upper score)

11

local upper score  Ai . score ;

12

13

for each {0  j < |lists|} do

14

i f (queryT erm[j]  Ai . terms)

15

NextShallow( l i s t s [ j ] , Ai . docId) ;

16

local upper score  local upper score +

17

18

end i f

19

end for

l i s t s [ j ] . getBlockMaxScore() ;

20

21

if (local upper score > )

22

for each {0  j < |lists|} do

23

i f (queryT erm[j]  Ai . terms)

24

Next( l i s t s [ j ] , Ai . docId) ;

25

end i f

26

end for

27

//Complete the score with the missing l i s t s

28

for each {0  x < |lists||lists[x].curDoc = Ai.docId} do

29

Ai . score  Ai . score + BM25( l i s t s [ x ] ) ;

30

end for

31

end i f

32

end i f

33

34

i f ( < Ai . score )

35

i f (|H| < k)

36

H  H Ai ;

37

e l s e i f (H0 . score < Ai . score )

38

remove H0 ;

39

H  H Ai ;

40

  H0 . score ;

41

end i f

42

end i f

43 end for

44
45 sort H by score ; 46 return H ; 47 end

are found and included in the answer. As a consequence, the query processing discards fewer documents at the beginning of the process, since the discarding threshold starts with a small value. We thus propose the usage of the first tier of the index to support a pre-processing stage just to compute an initial discarding threshold that is higher than 0. This simple strategy naturally may speed up the process if the gains when processing the full index are worth the cost of computing the initial discarding threshold when processing the first tier.
We then experiment with a variation of BMW, we named BMW-t, and a variation of WAND, we named WAND-t. These variations use the first tier to select an initial discarding threshold when processing the queries. This new usage of the two tier index presents the advantage of preserving the top k results, which does not happen in BMW-CS. The WAND-t performed worse than the BMW-t, thus we report only BMW-t in the experiments.
4. EXPERIMENTAL EVALUATION
We used the TREC GOV2 collection for the experiments in this paper. The collection has 25 million web pages crawled from the .gov domain in early 2004. It has 426 gigabytes of text, composed of HTML pages and the extracted

content of pdf and postscripts documents. The full index has about 7 gigabytes of inverted lists and a vocabulary of about 4 million distinct terms. We applied the Porter Stemmer [9] when indexing the collection. Our indexes use the frequency of the terms as the impact information. To evaluate the quality of query results, we randomly selected a set of 1000 queries from the TREC 2006 efficiency queries set and removed the stop-words from these queries. During the query processing, the entire index is loaded to memory, to avoid any possible bias in the query processing time. | All these setup options were chosen for being similar to those adopted in the baseline [6] and previous studies [15]. We ran the experiments in a 24-cores Intel(R) Xeon(R), with X5680 Processor, 3.33GHz and 64 GB of memory. All the queries were processed in a single core.
We used Okapi BM25 as the rank function, but our method can be adopted to compute other ranking functions. The generated skiplists have one entry for each block of 128 documents. Each skiplist entry keeps the docId, to help the random decompression, and the maximum impact registered in the block. We also experimented with blocks of 64 entries, and the results and conclusions were about the same, thus we decided to report only results with 128. We varied the first tier % from 1 to 20 percent of the full index in the experiments.
Another parameter evaluated in the experiments was the size of the top results required by the algorithms. We evaluated the algorithms requesting 10 and 1000 results. Retrieving top 1000 results was included to simulate an environment where the top results are computed to feed a more sophisticated ranking method that performs a re-rank of results. The top 10 results was included to simulate a more common scenario where the user is interested in getting only a small list of results.
We evaluated the methods in terms of query response time, the amount of accumulators required to process the queries, amount of decoded entries from the inverted lists, and finally the mean reciprocal rank Distance (MRRD), presented in Equation 1, which was the measure adopted by Broder et al [4] to evaluate the distance between the results when preserving top results to rankings that do not preserve the top results. Using the MRRD distance, we are able to know how much an approximated rank differs from the one that preserves the top results. This measure returns a value between 0 and 1, where identical results provide MRRD=0, and completely distinct results provide MRRD=1.

M RRD(B, P ) =

k i=1,di B -P

1/i

k i=1

1/i

(1)

4.1 Baselines
One of the implemented baseline algorithms was BMW [6]. As one of our methods may not preserve the top ranking results, we also have considered including as a baseline the version of WAND that does not preserve the top results, proposed by Broder et al [4]. However, the results achieved by the approximate WAND were even slower than the BMW. We thus removed it from the baselines, and modified the method BMW to implement the same approximation strategy proposed to WAND. As a result, we transformed BMW to an approximated top-k query processing algorithm, which does not guarantee preserving top results, but may be faster

189

0.5

BMW-SP-10%

BMW-SP-30%

BMW-SP-50%

0.4

BMW-F1.5

BMW-F2

BMW-CS-1%

BMW-CS-5%

0.3

BMW-CS-10%

MRR

0.2

0.1

0 0 100 200 300 400 500 600 700 800 900 1000 k
Figure 2: MRRD values compared to a exact rank for baselines BMW-SP and BMW-f

than original BMW. Basically, we artificially increase the pruning threshold during the query processing by multiplying this threshold for a pruning factor f . If f is 1, the exact top-k is guaranteed. When f is greater than 1, there is no guarantee of preserving the exact rank, but less entries will be evaluated, resulting in performance gains.
Finally, a reader could wonder if the results would also be good if we had adopted just the first tier for processing queries, considering the first tier as a statically pruned index. Thus, in order to avoid these doubts, we included a naive method using only the first tier for processing queries. We named it BMW-SP (applying BMW with static pruning).
4.2 Results
We begin the report of our experimental results by answering the possible doubts about the advantage of using the BMW-CS algorithm, which does not guarantee that all the top results are preserved, when compared to a simple static pruning strategy that adopts only the first tier to compute the ranking. Figure 2 presents the MRRD results when computing the top k results by using our method with the first tier of 1%, 5% and 10% compared to the usage of the static pruning approach, named BMW-SP. As can be seen in Figure 2, even when using BMW-SP with the first tier with 50% of the full index, the error level (MRRD) obtained by BMW-CS with 1% is still smaller than it. Even considering this observation, for the sake of completeness, we still report the time efficiency results obtained when using the BMW-SP with the first tier being 50%.
Figure 2 also presents the BMW-f MRRD results when varying parameter f . As can be seen in Figure 2, BMW-f achieves error levels worse than BMW-CS even when setting the factor f to the low value of 1.5. Lower factor values would slow down the performance of the method, thus we adopt this factor in our efficiency experiments, even considering that its error level is higher than those achieved by our method. We stress that both BMW-f and BMW-SP are included in the experiments to avoid doubts about these possible variations in the usage of BMW. These methods were not explicitly proposed in the literature.
Next we present the percentage of entries we included in the first tier of BMW-CS. This choice affects three main factors: the time for processing queries, the MRRD results of

our methods and the amount of memory required to process queries. Variation of these parameters are illustrated in Figure 3. As can be seen, when computing the top 1000 results, the MRRD results decrease as the size of the first tier increases, being almost zero for first tier sizes higher than 8%. When computing the top 10 results, MRRD was always zero for all sizes of first tier experimented. Time tends to increase as the first tier increases in both cases. On the other hand, the number of accumulators presents more complex behavior. In the top 1000, it first increases as the size of the first tier increases. Then, at some point, it starts to decrease, since the candidate selection procedure starts to perform better pruning, thus reducing the set of candidate documents.
When looking to the general results presented in Figure 3, we conclude that a good size for the first tier in BMW-CS is 2% when tuning the method to compute the top 10 results and 10% when tuning the method to top 1000 results. These parameters provide a good combination of a low number of candidate documents, low query processing times and low MRRD. Notice however that even if choosing other first tier sizes among the ones presented in the experiments, still our method would be quite fast and competitive.
Figure 3 (c) and (d) also indicates how much memory our algorithm needs to store the candidate documents. We can see the requirement is not so high in both the top 10 and top 1000 scenarios, being limited to a few times greater than the size of top results required to be computed.
Finally, regarding the MRRD error level achieved by BMWCS it is noteworthy that a user would almost not perceive the differences in top k results when using BMW-CS even when considering higher values of k. For instance, as k = 1000, the error of BMW-CS is still smaller than 0.0001 in terms of MRRD for the 10% first tier size. To better illustrate what this error level means, analyzing the results in detail, we perceived that BMW-CS with a 10% first tier resulted in no changes in the top 1000 results for 90% of the evaluated queries. Further, in all experimented first tier sizes, the top 10 results were preserved for all queries. Changes in the ranking, when they occur, are more common in the bottom results. For instance, we preserved the top 100 results for all queries, and preserved the top 200 results for 99.9% of the queries.
We also studied the MRR variation of method BMW-t, but do not present the variation due to space restrictions. Its performance is close to the best when using 1% for the first tier, becoming slower as the partition increases and not improving so much when it decreases. We report the results on the remaining experiments with our methods using 2% and 10% first tier sizes in case of BMW-CS, and 1% in case of BMW-t.
In Tables 1 and 2, we can observe the performance of the algorithms in terms of MRRD, decoded integers and query time when processing the top-10 and top-1000. The results of time are presented with confidence level of 95%. BMWCS provides extremely low MRRD results, which means these answers are almost the same as the correct top k results. On the other hand, its performance is considerably better than BMW and BMW-f , which do not have a preprocessing phase during query evaluation, and thus are performed directly over the full index.
We can see the performance of BMW-t, which preserves the top results, presents an improvement of around 10% in

190

Query Time

MRRD value

a) top10:MRRD and Time

1

20

MRRD

0.8

Time

15 0.6

0.4

10

0.2

5

0 0
0 2 4 6 8 10 12 14 16 18 20 Size of first tier

c) top10:Candidates and Time

900 20

800

#candidates

Time

700

15 600

500

400

10

300

200

5

100

0

0

0 2 4 6 8 10 12 14 16 18 20

Size of first tier

Query Time # of candidates

Query Time MRRD value

b) top1000: MRRD and Time

0.018

80

0.016

MRRD Time

70

0.014

60

0.012

50

0.01 40
0.008

0.006

30

0.004

20

0.002

10

0

0

0 2 4 6 8 10 12 14 16 18 20

Size of first tier

d) top1000: Candidates and Time

30000

80

#candidates

70

25000

Time

60

20000

50

15000

40

10000

30

20

5000

10

0

0

0 2 4 6 8 10 12 14 16 18 20

Size of first tier

Query Time

# of candidates

Figure 3: Variation in time and MRRD(a and b); and between time and number of candidate documents stored in (c and d) when processing first tier for method BMW-CS when using distinct sizes of first tier.

query processing times when compared to BMW. The approximated version BMW-f , using the factor f =1.5, processes query 20% faster than the exact BMW. However, BMW-CS is not only faster than BMW-f , but also presents lower MRRD values.
Still regarding Tables 1 and 2, we can see that BMWCS preserves the result set more than the other approximated methods implemented and is about 40 times faster than BMW when computing the top 10 results (using a 2% tier), and about 4.75 times faster when computing the top 1000 results(using a 10% tier). These gains can also be seen when analyzing the number of decoded integers, which is one of the main costs when elements are stored in memory.

top 10

Algorithm MRRD Decoded

time

BMW

0 2353066 100.1 ± 9.9

BMW-f1.5 0.3386 1797451 78.5 ± 7.8

BMW-SP-50 0.0032 1700488 79.8 ± 8.0

BMW-t

0 2082782 89.5 ± 8.8

BMW-CS-2

0 48989 2.4 ± 0.4

BMW-CS-10

0 217627 10.4 ± 0.5

Table 1: MRRD, number of decoded entries, and time(ms) efficiency achieved by the experimented methods when computing top 10 results.

Table 3 presents the performance of the algorithms when processing distinct query sizes for both the top 10 and top 1000 results computation. BMW-CS was the fastest option for all query sizes. Although the gain is smaller for queries with more than 5 terms, our method still results in

top 1000

Algorithm MRRD Decoded

time

BMW

0 5032834 226.0 ± 18.0

BMW-f1.5 0.1149 4463099 193.7 ± 15.4

BMW-SP-50 0.0584 3495885 174.8 ± 13.0

BMW-t

0 4799477 205.7 ± 16.9

BMW-CS-2 0.0043 1258859 39.8 ± 4.6

BMW-CS-10 0.0001 921687 47.6 ± 4.2

Table 2: MRRD, number of decoded entries, and time(ms) efficiency achieved by the experimented methods when computing top 1000 results.

impressive gains when compared to all baselines even for long queries.
One explanation for the smaller gain in long queries is that in the first phase of the process, as we have many terms in the query, the upper score of a candidate will be higher because it sums the minimum score of the missing lists. Thus, as we have an index with only a small fraction of the full index, the number of documents in the first phase with a complete score will be lower according to the number of terms in the query, making the threshold values lower when compared to the estimated upper scores. This configuration will lead to a less effective pruning during the candidate selection, increasing the costs of the whole process.
We observed differences in time results when comparing our experiments to the ones presented by Ding et al [6]. While we adopt exactly the same dataset, the query processing times we obtained are lower than the ones presented in their article. However we see that the number of decoded

191

Algorithm 2

BMW

12.0

BMW-f1.5 7.8

BMW-SP-50 10.3

Query time (ms) 3 4 5 >5 top 10
46.6 100.5 197.9 442.7 34.0 77.3 155.5 367.7 38.7 82.4 160.1 331.6

BMW-t

10.4 41.1 87.7 179.3 401.7

BMW-CS-2 0.58 1.42 2.42 3.74 9.96

BMW-CS-10 2.3 5.7 11.6 19.7 36.8

top 1000

BMW

37.9 122.3 243.6 437.9 848.6

BMW-f1.5 29.9 102.5 206.3 390.1 725.0

BMW-SP-50 32.6 102.1 192.0 333.5 610.3

BMW-t

29.8 104.6 220.9 406.5 803.5

BMW-CS-2 8.49 20.62 44.58 79.07 139.1

BMW-CS-10 14.6 29.3 51.8 82.2 160.8

Table 3: Time achieved by the experimented methods when processing queries with distinct sizes and computing top 10 and top 1000 results.

integers still similar. The final difference in times can be due to the choice of the queries, since we do not know the exact set of queries adopted by them, the architecture of the implemented system, and the machines used for the experiments. These differences do not affect the conclusions presented in our study because they affect all the experimented methods.
Acknowledgements
This work is partially supported by INWeb (MCT/CNPq grant 57.3871/2008-6), DOMAR (MCT/CNPq 476798/20116), TTDSW (FAPEAM), by CNPq fellowship grants to Edleno Moura (307861/2010-4) and Altigran Silva (308380/2010-0), and FAPEAM scholarship to Cristian Rossi.
5. CONCLUSION
The algorithms proposed and studied by us outperform the existent state-of-the-art algorithms for DAAT query processing. BMW-CS presents the advantage of being about 40 times faster than BMW when computing top 10 results and 4.75 times faster when computing top 1000 results. While it does not guarantee to preserve the top results, we show through experiments that the application of the algorithm does not change the results very much. The MRRD error level is quite small and the algorithm provides an impressive gain in performance. Thus, in situations where preserving the top results is not mandatory, the BMW-CS algorithm is an interesting alternative.
The price paid for this fast query processing is the necessity of more memory for processing queries. As we show, the number of candidate documents stored by the algorithm, which is the extra memory required by it, is not prohibitive, being, for instance, around 10 times the size of the final results in our experiments. Further, in practice a search system usually processes queries in multiple threads per machines, and the reduction in the query processing times cooperates to increase the throughput, thus compensating for the extra memory required by each thread. We intend to better study this question in future studies, since this was not the focus of this current study.

The second algorithm proposed by us, BMW-t, presents the advantage of preserving the top results. It delivers smaller, but significant gains when compared to the application of plain BMW, being about 10% faster.
Finally, in the future, we also want to study the combination of our algorithm BMW-CS to ranking strategies that take more information into account. In this regard, we plan to study how our algorithms can be adapted to strategies as those presented by Shan et al [13], where the authors study the impact of including external sources of relevance evidence into the performance of query processing algorithms, and such as [5], in which the authors show how to encode several features into a single impact value.
6. REFERENCES
[1] V. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In ACM SIGIR, pages 372­379, 2006.
[2] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space ranking with effective early termination. In ACM SIGIR, pages 35­42, 2001.
[3] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley Publishing Company, USA, 2nd edition, 2011.
[4] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In ACM CIKM, pages 426­434, 2003.
[5] A. Carvalho, C. Rossi, E. S. de Moura, D. Fernandes, and A. S. da Silva. LePrEF: Learn to Pre-compute Evidence Fusion for Efficient Query Evaluation. JASIST, 55(92):1­28, 2012.
[6] S. Ding and T. Suel. Faster top-k document retrieval using block-max indexes. In ACM SIGIR, pages 993­1002, 2011.
[7] A. Moffat and J. Zobel. Self-indexing inverted files for fast text retrieval. ACM TOIS, 14(4):349­379, 1996.
[8] A. Ntoulas and J. Cho. Pruning policies for two-tiered inverted index with correctness guarantee. In ACM SIGIR, pages 191­198, 2007.
[9] M. Porter. An algorithm for suffix stripping. Program: electronic library and information systems, 40(3):211­218, 2006.
[10] K. Risvik, Y. Aasheim, and M. Lidal. Multi-tier architecture for web search engines. In First Latin American Web Congress, pages 132­143, 2003.
[11] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In ACM SIGIR, pages 232­241, 1994.
[12] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Technical report, Ithaca, NY, USA, 1974.
[13] D. Shan, S. Ding, J. He, H. Yan, and X. Li. Optimized top-k processing with global page scores on block-max indexes. In WSDM, pages 423­432, 2012.
[14] G. Skobeltsyn, F. Junqueira, V. Plachouras, and R. Baeza-Yates. ResIn: a combination of results caching and index pruning for high-performance web search engines. In ACM SIGIR, pages 131­138, 2008.
[15] T. Strohman and W. B. Croft. Efficient document retrieval in main memory. In ACM SIGIR, pages 175­182, 2007.

192

Faster and Smaller Inverted Indices with Treaps 

Roberto Konow
Dept. of Computer Science Univ. of Chile, Chile
EIT, Univ. Diego Portales

Gonzalo Navarro
Dept. of Computer Science Univ. of Chile, Chile

Charles L. A. Clarke Alejandro López-Ortíz
School of Computer Science Univ. of Waterloo, Canada

ABSTRACT

16]. In the first stage, a fast and simple filtration proce-

We introduce a new representation of the inverted index that performs faster ranked unions and intersections while using less space. Our index is based on the treap data structure, which allows us to intersect/merge the document identifiers while simultaneously thresholding by frequency, instead of the costlier two-step classical processing methods. To achieve compression we represent the treap topology using compact data structures. Further, the treap invariants allow us to elegantly encode differentially both document identifiers and frequencies. Results show that our index uses about 20% less space, and performs queries up to three times faster, than state-of-the-art compact representations.

dure extracts a subset of a few hundreds or thousands of candidates from the possibly billions of documents forming the collection. In the second stage, more complex learned ranking algorithms are applied to the reduced candidate set in order to obtain a handful of high-quality results. In this paper, we focus on improving the efficiency of the first stage, freeing more resources for the second stage and increasing the overall performance. In contexts where traditional ranking methods are sufficient, the goal of the first stage is to directly convey a few top-quality results to the final user.
The first stage aims to return either a set of the highest ranked documents containing all the query terms (a ranked intersection) or some of the most important query terms (a

Categories and Subject Descriptors

ranked union). In most cases, ranked intersections are solved via a Boolean intersection, followed by the computation of

H.3.3 [INFORMATION STORAGE AND RETRIEVAL]: scores for the resulting documents. Ranked unions are gen-

Information Search and Retrieval

erally solved only in approximate form, avoiding a costly

Boolean union. However, Ding and Suel [21] showed that

General Terms

ranked intersections can be processed faster than Boolean intersections. They also obtained the best known performance

Algorithms, Performance

for ranked unions, giving exact, rather than approximate re-

sults, and demonstrating the feasibility of their approach.

Keywords

In this paper, we introduce a new compressed representation for posting lists that performs ranked intersections and

Treap, Inverted Index, Top-k, Query Processing

(exact) unions directly. This representation is based on the

treap data structure [34], a binary tree that simultaneously

1. INTRODUCTION
Modern Web search engines, and other information retrieval systems, face two competing challenges. On the one hand, they have to manage huge amounts of data. On the other hand, they have to provide very precise results in response to user queries, often identifying a few relevant documents among increasingly larger collections. These requirements can be addressed via a two-stage ranking process [37,

represents a left-to-right and a top-to-bottom ordering. We use the left-to-right ordering for document identifiers (which supports fast Boolean operations) and the top-to-bottom ordering for term weights (which supports the thresholding of results simultaneously with the intersection process). Using this data structure, we can obtain the top-k results for a ranked intersection/union without having to first produce the full Boolean result.
Additionally, the treap representation allows us to dif-

Partially funded by Fondecyt grant 1-110066 , by the Conicyt PhD Scholarship Program, Chile and by the Emerging Leaders in the Americas Program, Government of Canada.

ferentially encode both document identifiers and weights, which is crucial for the space-efficient representation of inverted indexes. Posting lists have been compressed for de-

cades [38] to handle very large collections within minimal

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation

space. Using other representations we must choose either identifiers or weights for differential encoding, but not both.
Our experiments show that the space usage of our treap-

on the first page. Copyrights for components of this work owned by others than the

based inverted index representation is less than the state-of-

author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or

the-art compressed representations: around 22% less space

republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright is held by the owner/author(s). Publication rights licensed to ACM.

than Block-Max [21] and 18% less space than Dual-Sorted [25]. As for the time, treaps outperform previous techniques for k up to 30 on intersections, and up to 130 on unions,

Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

193

being up to three times faster than the alternatives in some cases. Those ranges of k values make this result of particular interest both in applications where a limited result set is of interest, and in large-scale distributed systems in which each node contributes a limited set to the global result.

2. BASIC CONCEPTS

The inverted index plays a central role in the efficient pro-

cessing of ranked and Boolean queries [38, 41, 18, 16, 5].

It can be seen as an array of lists or postings, where each

entry of the array corresponds to a different term or word in

the collection, and the lists contain one element per distinct

document where the term appears. For each document, the

index stores the document identifier (docid) and the weight

of the term in the document. The set of terms is called the

vocabulary of the collection, which is comparatively small in

most cases [24].

In the first stage of query processing, a simple metric is

used to assign a score to a document with respect to a query.

In the classical bag-of-words model, the query Q is seen as

a set of q terms t  Q, and the score of a document d is

computed as score(Q, d) = tQ w(t, d), where w(t, d) is the weight of term t in document d. For example, in the

well-known tf-idf scoring scheme, this weight is computed

as w(t, d) = tf t,d · idft. Here, tf t,d is the term frequency of t

in d, that is, the number of times t occurs in d. The second

term

is

idft

=

log

D dft

,

where

dft

is

the

document

frequency,

that is, the number of documents where the term t appears,

and D is the total number of documents. Since idft (or

dft) depends only on t, an efficient way to store w(t, d) in

an inverted index is to store idft or dft together with each

distinct vocabulary term, and store the values tf t,d in the posting list of term t, together with each docid d. In this

paper we will assume that term frequencies are stored in the

posting lists, but any other integer measure, such as impacts

[2] could be used.

In the bag-of-words model we are given Q and k, and asked

to retrieve k documents d with the highest score(Q, d) val-

ues. In the two-stage model, typical values of k are hundreds

to thousands, as discussed earlier. In simpler one-stage sys-

tems, typical values of k are below 20. Note that it is not

necessary for all the terms of Q to appear in a returned doc-

ument d; a missing term t simply implies that w(t, d) = 0.

This problem is frequently called ranked union. A variant

of the problem, popularized by Web search engines to favor

precision over recall, is the ranked intersection, where only

documents containing all the terms are returned. Nowadays,

ranked intersections are more common than unions.

The Boolean intersection problem, without ranking, aims

at retrieving all the documents d where all the terms of

Q appear. A typical way to solve a ranked intersection is

to first compute a Boolean intersection, then compute the

scores of all the resulting documents, and finally keep the

documents with the k highest scores. This approach has

triggered much research on the Boolean intersection problem

[20, 6, 32, 8, 25]. This approach is, of course, suboptimal,

since in principle one could use weight information to filter

out documents that belong to the intersection but one can

ensure will not make it to the top-k list. Only recently some

schemes specifically aimed at solving ranked intersections

have appeared [21]. All these schemes store the posting lists

in increasing docid order, which is convenient for skipping

documents during intersections.

Ranked unions, instead, cannot be efficiently solved through a Boolean union, as this returns too many results. In this case, most research has aimed at returning an approximate answer within good time bounds [30, 2]. Most of these techniques order the posting lists by decreasing weight values, not by docids. Recently, it has been shown that ranked unions can be solved in exact form within reasonable time [15, 35, 21] by using increasing docid order for the posting lists in the best solution [21].
Traditionally, the posting lists were stored on disk. With the availability of large amounts of main memory, this trend has changed to use the main memory of a cluster of machines, and many intersection algorithms have been designed for random access [20, 6, 32, 19, 33, 35, 8, 25]. In distributed main-memory systems, usually documents are distributed across independent inverted indexes, and each index contributes with a few results to the final top-k list. Therefore, it is most interesting that an individual inverted index solves top-k queries efficiently for k values in the range 10­100 [16].
Both when stored on disk and in main memory, reducing the size of the inverted index representation is crucial. On disk, it reduces transfer time. In main memory, it increases the size of the collections that can be managed within a given RAM budget, or alternatively reduces the amount of servers that must be allocated in a cluster to hold the index, the energy they consume, and the amount of communication. Compression of inverted indexes is possibly the oldest and most successful application of compressed data structures (e.g., see [38]). The main idea to achieve compression is to differentially encode either the document identifers or the weights (depending on how the lists are sorted), whereas the other value (weight or docid, respectively) becomes harder to compress. The problem of this duality in the sorting, and how it affects compression and query algorithms, has been discussed in past work [38, 4, 25].
In this context, our contribution is a new in-memory posting list representation that, on the one hand, achieves improved compression because it allows differential encoding of both docids and frequencies, and on the other hand, performs exact ranked intersections and unions directly and natively without having to first intersect/merge and then rank.
3. RELATED WORK
3.1 Query Processing Strategies
Two kinds of approaches are used for unions and intersections (ranked or Boolean): Term-at-a-time (TAAT) and Document-at-a-time (DAAT) [16].
TAAT processes one posting list after the other. The lists are considered from shortest to longest, starting with the first one as a candidate answer set, and refining it as we consider the next lists. TAAT is especially popular for processing ranked unions [30, 2, 35], as the successive lists have decreasing idft value and thus a decreasing impact on the result, not only for the tf-idf model, but also for BM25 and other models. The documents in each list are sorted by decreasing weight. Thus heuristic thresholds can be used to obtain an approximate ranked union efficiently, by pruning the processing of lists earlier, or avoiding lists completely, as we reach less relevant documents and our candidate set becomes stronger [30, 2]. A more sophisticated approach based on similar ideas can be used to guarantee that the answer is exact [35].

194

DAAT processing is more popular for Boolean intersections and unions. Here the q lists are processed in parallel, looking for the same document in all of them. Posting lists must be sorted by increasing docid, and we keep a pointer to the current position in each of the q lists. Once a document is processed, the pointers move forward. Much research has been carried out on Boolean intersections [20, 6, 32, 19, 8]. While a DAAT processing is always used to intersect two lists, experimental results suggest that the most efficient way to handle more lists is to intersect the two shortest ones, then the result with the third, and so on. This can be seen as a TAAT strategy.
Many ranked intersection strategies employ a full Boolean intersection followed by a postprocessing step for ranking. However, recent work has shown that it is possible to do better [21]. The advantage of DAAT processing is that, once we have processed a document, we have complete information about its score, and thus we can maintain a current set of top-k candidates whose final scores are known. This set can be used to establish a threshold on the scores other documents need to surpass to become relevant for the current query. Thus the emphasis on ranked DAAT is not on terminating early but on skipping documents. This same idea has been successfully used to solve exact (not approximate) ranked unions [15, 21].
The strategies we use to solve ranked union and intersection queries in this paper are best classified as DAAT. We use sophisticated mechanisms to skip documents using the current threshold given by the current top-k candidate set.
3.2 Compressed Posting List Representations
A list p1, p2, p3, . . . p is usually represented as a sequence of d-gaps p1, p2 - p1, p3 - p2, . . . , p - p -1 , and uses a variable-length encoding for these differences, for example -codes, -codes or Rice/Golomb codes [38], the latter usually giving the best compression. Recent proposals make use of byte-aligned [33, 19] or word-aligned [39, 1] codes, which are faster at decoding at a small loss in compression. Extracting a single list or merging lists is done optimally by traversing the lists from the beginning, but intersections can be done much faster if random access to the sequences is possible. A typical solution to provide random access is to perform a sampling of the sequences, cutting them into blocks that are differentially encoded, while storing in a separate sequence the absolute values of the block headers and pointers to the encoded blocks. Different sampling strategies have been used [19, 32] and the intersection algorithms have been tailored to them.
When lists are sorted by decreasing weight (for approximate ranked unions), the differential compression of docids is not possible, in principle. Instead, term weights can be stored differentially. When storing tf values, one can take advantage of the fact that long runs of equal tf values (typically low ones) are frequent, and thus not only run-length encode them, but also sort the corresponding docids increasingly, so as to encode them differentially [4, 41].
3.3 State of the Art for Exact Ranked Queries
The following two approaches have recently displayed the best performance for exact ranked intersections and unions.

3.3.1 Block-Max
Block-Max [21] is a special-purpose structure for ranked intersections and unions. It sorts the lists by increasing docid, cuts the lists into blocks, and stores the maximum weight for each block. This enables them to skip whole blocks whose maximum possible contribution is very low, by comparing its maximum weight with a threshold given by the current candidate set. Block-Max obtains considerable performance gains over the previous techniques for exact ranked unions [15, 35], and also over the techniques that perform ranked intersections via a Boolean preprocessing.
The basic concept is as follows: Suppose the next document of interest, d, belongs to blocks b1, . . . , bq in the q lists. Compute an upper bound to score(Q, d) using the block maxima instead of the weights w(t, d). If even this upper bound does not surpass the kth best score known up to now, no document inside the current blocks can make it to the top-k list. So we can safely skip some blocks.
Our technique can be seen as a generalization of the BlockMax idea, in which we use the treap concept to naturally define a hierarchical blocking scheme. The generalization is algorithmically nontrivial, but it is practical and beats the flat Block-Max. In addition, the treap structure allows us to differentially encode both docids and weights, which translates into space savings with respect to Block-Max.
3.3.2 Dual-sorted inverted lists
Dual-Sorted inverted lists [29, 25] represent the posting lists sorted by decreasing frequency, using a wavelet tree data structure [23, 28]. The wavelet tree efficiently simulates ordering by increasing docids. TAAT processing is used for approximate ranked unions and DAAT-like processing for (exact) ranked intersections. The latter, although building on Boolean intersections, is implemented in native form on wavelet trees, which makes it particularly fast, even faster than Block-Max. Basically, the wavelet tree can recursively subdivide the universe of docids and efficiently determine that some list has no documents in the current interval.
Our technique shares with Dual-Sorted the ability to maintain the lists sorted by both docids and weights simultaneously, and is able to perform a similar kind of native intersection, that is, determine that in an interval of documents there is a list with no elements. In contrast, Dual-Sorted does not know the frequencies until reaching the individual documents, whereas our treaps give an upper bound to the frequencies in the current interval. This allows us to perform ranked intersections faster than the Boolean intersections of Dual-Sorted. In addition, the treap uses less space, since Dual-Sorted cannot use differential encoding on docids.
4. DIFFERENTIALLY ENCODED TREAPS
We describe our data structure in this section. First, we survey the treap data structure and show it can be used to represent a posting list. Then we describe how we represent the resulting data structure using little space. In addition, we describe some practical improvements on the basic idea. At the end, we describe how query processing is carried out on the final representation.
4.1 The Treap Data Structure
A treap [34] is a binary tree where nodes have two attributes: a key and a priority. The treap satisfies the invariants of a binary search tree with respect to the keys: the root key is larger than those of its left subtree and smaller

195

docids 4 9 13 14 15 22 27 30 35 37 39 44 freqs 6 2 14 1 1 2 1 24 6 1 2 3

24 30

6 4

14 13

21

9

14

2 22
1 27

1 15

6 35
3 44
2 39
1 37

Figure 1: An example posting list (with docids and frequencies) and the corresponding treap representation in our scheme. Note that docids (inside the nodes) are sorted inorder and frequencies (outside the nodes) are sorted top to bottom.

than those of its right subtree. Furthermore, the treap satisfies the invariants of a binary heap with respect to the priorities: the priority of the parent is larger than those of its descendants.
Given its invariants, a treap can be searched for a key just as a binary search tree, and it can be simultaneously used as a binary heap. While in the literature it has mostly been used with randomly assigned priorities [34, 26, 13] to ensure logarithmic expected height independently of the order of insertions, a treap can also be seen as the Cartesian tree [36] of the sequence of priorities once the values are sorted by keys. Such Cartesian tree can be built in O(n) time from a sequence of n elements already sorted by key, even in compressed form [10, 9, 22].
Treaps are a particular case of priority search trees [27], which can guarantee balancedness but are unlikely to be as compressible as Cartesian trees. There has been some work on using priority search trees for returning top-k elements from suffix trees and geometric range searches [12, 11] but, as far as we know, our use of treaps for ranked queries on inverted indexes, plus their differential compression, is novel.
4.2 Inverted Index Representation
We consider the posting list of each term as a sequence sorted by docids (which act as keys), each with its own term frequency (which act as priorities). Term impacts, or any other term weights, may also be used as priorities. We then use a treap to represent this sequence. Therefore the treap will be binary searchable by docid, whereas it will satisfy a heap ordering on the frequencies. This means, in particular, that if a given treap node has a frequency below a desired threshold, then all the docids below it in the treap can be discarded as well.
Figure 1 illustrates a treap representation of a posting list. This treap will be used as a running example.
4.3 Compressing the Treap
In order to compete with existing compressed representations of posting lists, we represent the treap data (topology,

docids, and frequencies) in compact form. The key issue is that we choose a representation where all the treap operations can be carried out efficiently, so as to exploit the treap properties at query time.
4.3.1 Compact topology representation
Given a posting list of n documents, the treap will be a binary tree of n nodes. We represent it as a general tree using a well-known isomorphism: First, a fake root node vr is created. The children of vr become the nodes in the rightmost path of the treap, from the root to the leaf. Then each of those nodes are converted recursively.
With this transformation, the treap root is the first child of vr. The left child of a treap node v is its first child in the general tree. The right child of v is its next sibling in the general tree. An inorder traversal of the treap corresponds to a postorder traversal of the general tree.
There are (4n/n3/2) general trees of n nodes, and thus one needs log2(4n/n3/2) = 2n - (log n) bits to represent any such tree. There exist various compact tree representations using 2n+o(n) bits that can in addition carry out many tree operations efficiently, including taking the first child, next sibling, computing postorder of a node, and so on. We will use a recent representation that has proven to be efficient in practice [31, 3]. It is based on a balanced parentheses representation of the tree, obtained by a preorder traversal where we append an opening parenthesis when reaching a node and a closing parenthesis when leaving it.
4.3.2 Differentially encoded trees
In addition to the tree topology, we must represent docids and term frequencies. Our plan is not to access the posting lists in sequential form as in classical schemes, thus a differential encoding each docid with respect to the previous one is not directly applicable. Instead, we make use of the invariants of the treap data structure.
Let id(v) be the docid of a treap node v, and f (v) its frequency. We represent id(v) and f (v) for the root in plain form, and then represent those of its left and right children recursively. For each node v that is the left child of its parent u, we represent id(u) - id(v) instead of id(v). If, on the other hand, v is the right child of its parent u, we represent id(v) - id(u) [17]. In both cases, we represent f (u) - f (v) instead of f (v). Those numbers get smaller as we move downwards in the treap.
The sequence of differentially encoded id(v) and f (v) values is represented according to an inorder traversal of the treap. As we move down the treap, we can easily maintain the correct id(v) and f (v) values for any node arrived at, and use it to compute the values of the children as we descend.
For this sake we need to randomly access a differential value in the sequence, given a node. We store those values in an array indexed by node preorders (in the general tree), which we can easily compute in our topology representation. Furthermore, we need a storage mechanism for the differences that: i) can access any value in the sequence directly, while ii) uses fewer bits to represent smaller numbers. We use Direct Addressable Codes (DACs) [14], which are designed precisely with this aim.
DACs encode a sequence of numbers x1, . . . , xn as follows. The log2(max{xi} + 1) bits needed to represent any xi are divided into chunks of varying size. Then the first chunk of lowest bits of all the numbers are represented in

196

24 30

6 4

14 13

21

9

14

2 22
1 27

6 35
3 44
2 39

1 15

1 37

Topology ( ( ( ( ) ( ) ) ( ( ) ( ) ) ( ) ) ( ) ( ( ( ) ) ) )
diff ids 9 5 17 8 1 9 5 30 5 2 5 9
diff freqs 8 4 10 1 0 12 1 24 18 1 1 3
Figure 2: The compressed representation of the example treap. The original binary tree edges (dashed) are replaced by a general tree, whose topology is represented with parentheses. Docids and frequencies are sorted inorder and represented in differential form with respect to their parent.

a first sequence, the second chunks in a second sequence, and so on. Some numbers xi will only participate in the first sequences because they are smaller than others. Compact bitmap representations are used to drive the extraction process for any xi through the different sequences where its chunks are represented. DACs can tune the block sizes so as to use minimum space, given the sequence of xi values.
Figure 2 illustrates our compressed treap representation.
4.4 Practical Improvements
The scheme detailed above would not be so successful without two important improvements. First, because many posting lists are very short, it turns out to be more efficient to store two single DAC sequences, with all the differential docids and all the differential frequencies for all the lists together, even if using individual DACs would have allowed us to optimize their space for each sequence separately. The overhead of storing the chunk lengths and other administrative data overweights the benefits for short sequences.
A second improvement is to break ties in frequencies so as to make the treap as balanced as possible, by choosing the maximum that is closest to the center of each interval. This improves the binary searches for docids.
The third, and more important, improvement is to omit from the treap representation all the elements of the lists where the frequency is below some threshold f0. According to Zipf's law [40, 18, 16, 5], a large number of elements will have low frequencies, and thus using a separate posting list for each frequency below f0 will save us from storing those frequencies wherever those elements would have appeared in the treap. Further, the docids of each list can be differentially encoded in classical sequential form, which is more efficient than in treap order.
It turns out that many terms do not have to store a treap at all, as they never occur more than f0 times in any document. We represent the gap-encoded lists using Rice codes and take an absolute sample every 128 values (which form

24 30

14 13

6 4

21

9

14

2 22
1 27

6 35
3 44
2 39

1 15

1 37

ids1 14 15 27 37 ids2 9 22 39

diff ids1 14 1 12 10 diff ids2 9 13 17

Figure 3: Separating frequencies below f0 = 2 in our example treap. The part that is removed from the treap is dashed. For the documents with frequencies 1 and 2, we show the absolute docids on the left and their differential version on the right.

a block). Samples are stored separately and explicitly in an array, with pointers to the block [19]. Searches in these lists will ask for consecutively larger values, so we remember the last element found and exponentially search for the next query starting from there. Figure 3 illustrates the separation of low-frequency elements from our example treap.
A neat feature of these lists is that often we will not need to access them at all during queries, since ranked queries aim at the highest frequencies.
4.5 Query Processing
4.5.1 General procedure
Let Q be a query composed of q terms t  Q. To obtain the top-k documents from the intersection or union of q posting lists we proceed in DAAT fashion: We traverse the q posting lists in synchronization, identifying the documents that appear in all or some of them, and accumulating their weights w(t, d) into a final score(Q, d) = t w(t, d) =
t tf t,f · idft. Those documents are inserted in a minpriority queue limited to k elements, where the priority is the score. Each time we insert a new element and the queue size reaches k + 1, we remove the minimum. At the end of the process, the priority queue contains the top-k results. Furthermore, at any stage of the process, if the queue has reached size k, then its minimum score L is a lower bound to the scores we are interested in for the rest of the documents.
4.5.2 Intersections
Let d be the smallest docid not yet considered (initially d = 1). All the treaps t maintain a stack of nodes (initially holding just a sentinel value element ut with id(ut) = + and f (ut) = +), and a cursor vt (initially the treap root). The stack will contain the nodes in the path from the root to vt where we descend by the left child. We will always call ut the top of the stack, thus ut is an ancestor of vt and it holds id(ut) > id(vt).
We advance in all the treaps simultaneously towards a node v with docid id(v) = d, while skipping nodes using the current lower bound L. In all the treaps t we maintain the

197

invariant that, if v is in the treap, it must appear in the subtree rooted at vt. In particular, this implies d < id(ut).
Because of the decreasing frequency property of treaps, if d is in a node v within the subtree rooted at vt, then f (v)  f (vt). Therefore, we can compute an upper bound U to the score of document d by using values f (vt) instead of f (v), for example U = tQ f (vt) · idft for a tf-idf scoring1. If this upper bound is U  L, then there is a valid topk answer where d does not participate, so we can discard d. Further, no node that is below all the current vt nodes can qualify. Therefore, we can safely compute a new target d  mint(id(ut)). Each time the value of d changes (it always increases), we must update the stack of all the treaps t to restore the invariants: While id(ut)  d, we assign vt  ut and remove ut from the stack. We then resume the global intersection process with this new target d. The upper bound U is recomputed incrementally each time any vt value changes (U may increase or decrease).
When U > L, it is still feasible to find d with sufficiently high score. In this case we have to advance towards the node containing d in some treap. We obtained the best results by choosing the treap t of the shortest list. We must choose a treap where we have not yet reached d; if we have reached d in all the treaps then we can output d as an element of the intersection, with a known score (the current U value is the actual score of d), insert it in the priority queue of top-k results as explained (which may increase the lower bound L), and resume the global intersection process with d  d + 1 (we must update stacks, as d has changed).
In order to move towards d = id(vt) in a treap t, we proceed as follows. If d < id(vt), we move to the left child of vt, lt, push vt in the stack, and make vt  lt. Instead, if d > id(vt), we move to the right child of vt, rt, and make vt  rt. We then recompute U with the new vt value.
If we have to move to the left and there is no left child of vt, then d does not belong to the intersection. We stay at node vt and redefine a new target d  id(vt). If we have to move to the right and there is no right child of vt, then again d is not in the intersection. We make vt  ut, remove ut from the stack, and redefine d  id(ut). In both cases we adjust the stacks of the other treaps to the new value of d, as before, and resume the intersection process.
Algorithm 1 gives pseudocode for the intersection.
4.5.3 Handling low-frequency lists
We have not yet considered the lists of documents with frequencies up to f0, which are stored separately, one per frequency, outside the treap. While a general solution is feasible (but complicated), we describe a simple strategy for the case f0 = 1, which is the case we implemented.
Recall that we store the posting lists in gap-encoded blocks. Together with the treap cursor, we will maintain a list cursor, which points inside some block that has been previously decompressed. Each time there is no left or right child in the treap, we must search the list for potential elements omitted in the treap. More precisely, we look for elements in the range [d, id(vt) - 1] if we cannot go left, or in the range [d, id(ut) - 1] if we cannot go right. Those elements must be processed as if they belonged to the treap before proceeding
1Replacing f (v) by f (vt) will yield an upper bound whenever the scoring function is monotonic with the frequencies. This is a reasonable assumption and holds for most weighting formulas, including tf-idf and BM25.

Algorithm 1 Top-k of intersection using treaps.
Intersect(Q, k)
results   // priority queue of pairs (key, priority) for t  Q do
stackt   // stack of treap t, id() = f () = + vt  root of treap t end for compute score U using f (vt) values, e.g. tQ f (vt) · idft d1 L  - while d < + do while U  L do
changed(mintQ id(top(stackt))) end while if t  Q, d = id(vt) then
report(d, U ) changed(d + 1) else t  treap of shortest list such that d = id(vt) if d < id(vt) then
lt  left child of vt if lt is not null then
push(stackt,vt) changev(t, lt) else changed(id(vt)) end if else rt  right child of vt if rt is not null then changev(t, rt) else changev(t, pop(stackt)) changed(id(vt)) end if end if end if end while return results
changed(newd)
d  newd for t  Q do
v  vt while d  id(top(stackt)) do
v  top(stackt) pop(stackt) end while changev(t, v) end for
report(d, s)
results  results  (d, s) if |results| > k then
remove minimum from results L  minimum priority in results end if
changev(t, v)
remove contribution of f (vt) from U , e.g. U - f (vt) · idft vt  v add contribution of f (vt) to U , e.g. U + f (vt) · idft

198

in the actual treap. Finding this new range [l, r] in the list may imply seeking and decompressing a new block.
The cleanest way to process range [l, r] is to search as if it formed a subtree fully skewed to the right, descending from vt. If we descended to the left of vt towards the range, we push vt into the stack. Since all the elements in the list have the same frequency, when we are required to advance towards (a new) d we simply scan the interval until reaching or exceeding d, and the docid found acts as our new id(vt) value. When the interval [l, r] is exhausted, we return to the treap. Note that the interval [l, r] may span several physical list blocks, which may be subsequently decompressed.
4.5.4 Unions
The algorithm for ranked unions requires a few changes on the algorithm for intersections. First, in the two lines that call changed(id(vt)), we do not change the d for all the treaps when the current treap does not find it. Rather, we keep values nextdt where each treap stores the minimum d  d it contains, thus those lines are changed by nextdt  id(vt). Second, we will choose the treap t to advance only among those where id(vt) = d and nextdt = d, as if nextdt > d we cannot find d in treap t. Third, when all the treaps t where id(vt) = d satisfy nextdt > d, we have found exactly the treaps where d appears. We add up score(Q, d) over those treaps where id(vt) = d, report d, and advance to d+1. If, however, this happens but no treap t satisfies id(vt) = d, we know that d is not in the union and we can advance d with changed(mintQ nextdt). Finally, changed(newd) should not only update d but also update, for all the treaps t, nextdt to max(nextdt, newd).
5. EXPERIMENTS AND RESULTS
5.1 Experimental Setup
We use the TREC GOV2 collection, parsed using Porter's stemming algorithm. The collection contains about 25.2 million documents and about 32.8 million terms in the vocabulary. The inverted lists contain about 4.9 billion postings in total. We used the TREC2006 Efficiency Queries dataset using distinct amounts of terms, from q = 2 to 5.
We compare our results with two baselines: (1) BlockMax [21], using their implementation and modifying it to use tf-idf scoring, and (2) Dual-Sorted [25], using their implementation. As additional baselines, we implemented (3) our own version of a traditional docid-sorted inverted index using Rice encoding of the gaps, sampling values every 128 values to support random access via exponential search, and Rice encoding of absolute frequencies, and (4) our own version of a traditional frequency-sorted inverted index, using gap- and run-length encoding for the frequencies, and Rice encoding of absolute docids (except within equal frequencies, where docids are sorted and differentialy encoded).
Our experiments were performed on an Intel(r) Xeon(r) model E5620 running at 2.40 GHz with 96GB of RAM and 12,288KB of cache, running version 2.6.31-41 64 bits of the Linux kernel. All solutions were implemented in C++, compiled with g++ version 4.4.3 and -O3 optimization.
5.2 Space Usage
Figure 4 shows the space usage of the main structures compared, for increasing subsets of GOV2. The line "Treap w/o f0" shows the space of our basic treap mechanism with-

Size (MB)

12000 10000 8000

Treap Treap w/o f0
Block Max DualSorted

6000

4000

2000

0 40 80 120 160 200 240 Documents Inserted (x 100000)
Figure 4: Space usage per document inserted.

docid freq

Differential

8.9 1.6

Treap

8.9 2.1

Treap w/o f0 11.0 5.6

Absolute

14.8 5.7

Figure 5: Pie chart with fraction of space used by the structures in our index. Bottom right: bpp for different representations of docids and frequencies.

out representing the low-frequency lists separately. As it can be seen, the treap mechanism to encode both docids and frequencies in partially differential form is not sufficient by itself to beat a representation like Block-Max (which does not encode frequencies differentially, but encodes docids differentially much better, in left-to-right order) or Dual-Sorted (which does not encode docids differentially, but encodes frequencies differentially and with run-length encoding).
When we separate the low-frequency lists ("Treap" in the figure), the situation is much better, as this is roughly equivalent to run-length compressing consecutive postings with frequency 1, and in addition the overhead of the tree topology and of non-left-to-right differential encoding of documents disappears within those lists. With this improvement, our compressed treap structures offer a space gain of 22% over Block-Max and of 18% over Dual-Sorted. While the idea of low-frequency lists could be applied to Block-Max and Dual-Sorted, it would probably make them slower (unlike our treaps, which become faster when processing lowfrequency lists) and the impact in their space would not be as high as on the treaps (e.g., they have no tree topology and their differential encoding of docids would not improve).
Our tree representation (with low-frequency lists) requires about 12.3 bits per posting (bpp). Figure 5 shows how that space distributes across our structures. Almost half of the space is used for the treap DACs, the docids using about twice the space of the frequencies. The docids for frequency

199

Docid sorted Freq sorted Treap
Treap w/o f0 Block-Max Dual-Sorted

Docid 8.9
14.8 8.9
11.0

Freq 5.7 1.6 2.1 5.6

Topology
1.3 2.4

Total
14.6 16.4 12.3 19.0 15.8 15.0

Table 1: Bpps of main components of the indexes.

f0 = 1 (F0), differentially encoded, use more than 40% of the space, which shows again how large is their impact on the space. Finally, the compressed treap topologies require about 10% of the space. Adding DACs and F0, we use 8.9 bpp for the docids, plus 2.1 bpp for the frequencies and 1.3 bpp for the topology. Our version without separating lowfrequency lists, instead, uses 11 bpp for the docids, 5.6 bpp for the frequencies, and 2.4 bpp for the topology, adding up to 19 bpp. This is compared in the figure (bottom right) with differential and absolute encodings of docids and frequencies (as detailed in Section 5.1), showing how treaps achieve intermediate results. Table 1 shows how the space of the various indexes adds up from those components.
5.3 Ranked Intersection
Figure 6 gives ranked intersection times for varying k, averaging over all the queries, and for k = 10 and k = 20, separating the queries by number of words (q). As noted in previous work [25], Dual-Sorted is unique in that it improves for longer queries, taking over for queries of 4­5 words or more. Averaged over all the queries, it performs similarly to Block-Max, and both are superior to a Boolean intersection followed by a ranking (labeled "Intersection") implemented over our docid-sorted inverted index. None of these methods is much affected by k (which is expected for Dual-Sorted and Intersection since they always produce the full intersection and then rank the resulting documents).
Our treaps are more affected by the value of k, achieving larger speedups over a plain intersection for smaller k. Indeed, they are much faster than all the alternatives for small k values, and become similar to them for k = 30. For k = 10, treaps outperform the others by a wide margin (up to 3 times faster than Block-Max, its closest competitor) for queries up to 4 words. For more than 4 words, as explained, Dual-Sorted takes over. The scenario is similar, yet less sharp, for k = 20.
To explain the improved times of the treap representation compared to a plain intersection, Figure 7 shows the number of documents accessed in both cases. It can be seen that the treap data structure is very effective to prune the number of documents that must be considered, starting examining about 2.6% of the documents considered by a Boolean intersection for k = 10, and becoming ineffective only at k = 1000. On the other hand, the compact data structures of the treap (namely the compressed representation of the topology and the DAC representation of the docids and frequencies) are significantly slower than a plain representation: both require about 300 nanoseconds per basic operation [3, 14], whereas a plain memory access costs 15­30 nanoseconds. This 10­20-fold slowdown makes the final time competitive only up to k = 30. The figure also shows the number of docids accessed at the low-frequency lists (F0), which increases with k but stays around 20% of the total.

Intersection, varying k

25 Treap

Block-Max

20

Dual-Sorted

Intersection

15

Milliseconds per query

10

5

0

10

20

30

40

50

60

k

Intersection, k=10

25 Treap

Block-Max

20

Dual-Sorted

Intersection

15

Milliseconds per query

10

5

0

2

3

4

5

Query length

Intersection, k=20

25 Treap

Block-Max

20

Dual-Sorted

Intersection

15

Milliseconds per query

10

5

0

2

3

4

5

Query length

Figure 6: Time performance for ranked intersections. On top, for all the queries and increasing k. The other two discriminate by number of words in the query and use fixed k = 10 and k = 20.

5.4 Ranked Union
Figure 8 shows the results for ranked union queries. Using a Boolean union as a filter for these queries is ineffective, so we use our frequency-sorted inverted index to implement an approximate ranked union, Persin et al.'s [30] (labeled "Persin" in the plots). Dual-Sorted also implements Persin et al.'s algorithm, so both report only approximate results. Only Block-Max and our treaps give exact results.
It can be seen that all the times worsen as k and q increase, more than linearly on q and sublinearly on k. Our treaps outperform Block-Max and Dual-Sorted for all k values up to 130. They are 2.5 times faster, for example for 3-word

200

30000 25000 20000

Intersection Treap f0

Amount of documents

15000

10000

5000

0 10 100 200 300 400 500 600 700 800 900 1000
k
Figure 7: Documents evaluated during a ranked intersection query.

queries. Treaps are only outperformed by the native Persin implementation, which however is not exact.
6. EXTENSIONS
It is not hard to adapt our algorithm for unions to the ranked version of the more general thresholded queries [7], which in addition to Q give a value q < q, so that at least q of the q query terms must appear in the reported documents. In this more general view, ranked unions correspond to q = 1 and ranked intersections to q = q. The more general Weak-AND operator [15] can also be easily supported. When we find which treaps t reach value id(vt) = d, we can evaluate document d and determine whether it qualifies.
On the other hand, approximate answers for ranked union queries have been the norm for decades, and our treap data structures can efficiently implement those as well. For example, we could easily implement Persin et al.'s [30] TAAT processing, even better than classical frequency-sorted lists. We could maintain the candidate set as a list sorted by docid. Each new treap that is processed is traversed in docid order, stopping at nodes where the threshold for considering documents is reached. As we produce the qualifying documents in docid order, we can simply merge them with the candidate set, without the need of more sophisticated structures. Furthermore, for subtrees of treap nodes whose frequency is below the threshold for inserting new documents in the candidate set, we can switch to a mode where the subtree is intersected with the candidates, using the next candidate docid to skip treap nodes.
7. CONCLUSIONS AND FUTURE WORK
We have introduced a new inverted index representation based on the treap data structure. Treaps turn out to be an elegant and flexible tool to represent simultaneously the docid and the weight ordering of a posting list. We use them to design efficient ranked intersection and union algorithms that simultaneously filter out document by docid and frequency. The treap also allows us to represent both docids and frequencies in differential form, thus enabling better compression of the posting lists. Our experiments show significant gains in space and time compared to the state of the art: not only our structure uses about 20% less space than previous ones, but also it is faster (sometimes as much as

Union, varying k

60 Treap

50

Block-Max Dual-Sorted

Persin 40

Milliseconds per query

30

20

10

Milliseconds per query

0 10 20 30 40 50 60 70 80 90 100 110 120 130

k

Union, k=10

60 Treap

50

Block-Max

Dual-Sorted

Persin 40

30

20

10

0

2

3

4

5

Query length

Union, k=20

60 Treap

50

Block-Max

Dual-Sorted

Persin 40

Milliseconds per query

30

20

10

0

2

3

4

5

Query length

Figure 8: Time performance for ranked unions. On top, for all the queries and increasing k. The other two discriminate by number of words in the query and use fixed k = 10 and k = 20.

three times faster) for up to k = 30 on ranked intersections and k = 130 on ranked unions.
A first future work question is how the scheme performs with other scoring formulas. We have used simple tf-idf, but we could use BM25, impacts, etc. Some require to adapt the way we compute the upper bound U , such as considering document sizes in BM25 (but this has been solved [15, 21]).
A second question is what would be the impact of reassigning docids. There is much recent research on this topic (see, e.g., [21]) that shows that reassignment can significantly improve both space and processing time. How much would treaps improve with such schemes? Can we optimize the reassignment for a treap layout?

201

An important part of our gain owed to separating lists with frequency f0 = 1. How to efficiently separate lists with higher frequencies is a challenge, and it can lead to substantial further gains. It is also interesting to test how this idea impacts on schemes like Block-Max and Dual-Sorted.
Finally, our performance degrades sharply with q, an effect already noted before in Block-Max [21]. We believe the time would become almost nonincreasing with q if we used treaps under a TAAT scheme where the longer lists were processed after determining good lower bounds with the shorter lists.
8. REFERENCES
[1] V. Anh and A. Moffat. Inverted index compression using word-aligned binary codes. Inf. Retr., 8(1):151­166, 2005.
[2] V. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In Proc. 29th SIGIR, pages 372­379, 2006.
[3] D. Arroyuelo, R. Ca´novas, G. Navarro, and K. Sadakane. Succinct trees in practice. In Proc. 11th ALENEX, pages 84­97, 2010.
[4] R. Baeza-Yates, A. Moffat, and G. Navarro. Searching large text collections. In Handbook of Massive Data Sets, pages 195­244. Kluwer, 2002.
[5] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley, 2nd edition, 2011.
[6] R. Baeza-Yates and A. Salinger. Experimental analysis of a fast intersection algorithm for sorted sequences. In Proc. 12th SPIRE, pages 13­24, 2005.
[7] J. Barbay and C. Kenyon. Adaptive intersection and t-threshold problems. In Proc. 13th SODA, pages 390­399, 2002.
[8] J. Barbay, A. Lo´pez-Ortiz, T. Lu, and A. Salinger. An experimental investigation of set intersection algorithms for text searching. ACM J. Exp. Alg., 14:art. 7, 2009.
[9] M. Bender and M. Farach-Colton. The LCA problem revisited. In Proc. 9th LATIN, pages 88­94, 2000.
[10] O. Berkman and U. Vishkin. Recursive star-tree parallel data structure. SIAM J. Comp., 22(2):221­242, 1993.
[11] I. Bialynicka-Birula. Ranked Queries in Index Data Structures. PhD thesis, University of Pisa, 2008.
[12] I. Bialynicka-Birula and R. Grossi. Rank-sensitive data structures. In Proc. 12th SPIRE, pages 79­90, 2005.
[13] G. Blelloch and M. Reid-Miller. Fast set operations using treaps. In Proc. 10th SPAA, pages 16­26, 1998.
[14] N. Brisaboa, S. Ladra, and G. Navarro. DACs: Bringing direct access to variable-length codes. Inf. Proc. Manag., 49(1):392­404, 2013.
[15] A. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In Proc. 12th CIKM, pages 426­434, 2003.
[16] S. Bu¨ttcher, C. Clarke, and G. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, 2010.
[17] F. Claude, P. Nicholson, and D. Seco. Differentially encoded search trees. In Proc. 22nd DCC, pages 357­366, 2012.

[18] B. Croft, D. Metzler, and T. Strohman. Search Engines: Information Retrieval in Practice. Pearson Education, 2009.
[19] J. Culpepper and A. Moffat. Compact set representation for information retrieval. In Proc. 14th SPIRE, pages 137­148, 2007.
[20] E. Demaine, A. Lo´pez-Ortiz, and J. Munro. Adaptive set intersections, unions, and differences. In Proc. 11th SODA, pages 743­752, 2000.
[21] S. Ding and T.Suel. Faster top-k document retrieval using block-max indexes. In Proc. 34th SIGIR, pages 993­1002, 2011.
[22] J. Fischer and V. Heun. Space-efficient preprocessing schemes for range minimum queries on static arrays. SIAM J. Comp., 40(2):465­492, 2011.
[23] R. Grossi, A. Gupta, and J. Vitter. High-order entropy-compressed text indexes. In Proc. 14th SODA, pages 841­850, 2003.
[24] H. Heaps. Information Retrieval - Computational and Theoretical Aspects. Academic Press, NY, 1978.
[25] R. Konow and G. Navarro. Dual-sorted inverted lists in practice. In Proc. 19th SPIRE, pages 295­306, 2012.
[26] C. Mart´inez and S. Roura. Randomized binary search trees. J. ACM, 45(2):288­323, 1997.
[27] E. McCreight. Priority search trees. SIAM J. Comp., 14(2):257­276, 1985.
[28] G. Navarro. Wavelet trees for all. In Proc. 23rd CPM, pages 2­26, 2012.
[29] G. Navarro and S. Puglisi. Dual-sorted inverted lists. In Proc. 17th SPIRE, pages 309­321, 2010.
[30] M. Persin, J. Zobel, and R. Sacks-Davis. Filtered document retrieval with frequency-sorted indexes. J. Am. Soc. Inf. Sci., 47(10):749­764, 1996.
[31] K. Sadakane and G. Navarro. Fully-functional succinct trees. In Proc. 21st SODA, pages 134­149, 2010.
[32] P. Sanders and F. Transier. Intersection in integer inverted indices. In Proc. 9th ALENEX, 2007.
[33] F. Scholer, H. Williams, J. Yiannis, and J. Zobel. Compression of inverted indexes for fast query evaluation. In Proc. 25th SIGIR, pages 222­229, 2002.
[34] R. Seidel and C. Aragon. Randomized search trees. Algorithmica, 16(4/5):464­497, 1996.
[35] T. Strohman and B. Croft. Efficient document retrieval in main memory. In Proc. 30th SIGIR, pages 175­182, 2007.
[36] J. Vuillemin. A unifying look at data structures. Comm. ACM, 23(4):229­239, 1980.
[37] L. Wang, J. Lin, and D. Metzler. A cascade ranking model for efficient ranked retrieval. In Proc. 34th SIGIR, pages 105­114, 2011.
[38] I. Witten, A. Moffat, and T. Bell. Managing Gigabytes. Morgan Kaufmann, 2nd edition, 1999.
[39] H. Yan, S. Ding, and T. Suel. Inverted index compression and query processing with optimized document ordering. In Proc. 18th WWW, pages 401­410, 2009.
[40] G. Zipf. Human Behaviour and the Principle of Least Effort. Addison-Wesley, 1949.
[41] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Comp. Surv., 38(2):art. 6, 2006.

202

How Query Cost Affects Search Behavior

Leif Azzopardi
School of Computing Science, University of Glasgow
Glasgow, United Kingdom
leif@dcs.gla.ac.uk

Diane Kelly
School of Information and Library Science, University of
North Carolina Chapel Hill, NC, USA
dianek@email.unc.edu

Kathy Brennan
School of Information and Library Science, University of
North Carolina Chapel Hill, NC, USA
knb11@live.unc.edu

ABSTRACT
We investigate how the cost associated with querying in the context of information retrieval affects how users interact with a search system. Microeconomic theory is used to generate the cost-interaction hypothesis that states as the cost of querying increases, users will pose fewer queries and examine more documents per query. A between-subjects laboratory study with 36 undergraduate subjects was conducted, where subjects were randomly assigned to use one of three search interfaces that varied according to the amount of physical cost required to query: Structured (high cost), Standard (medium cost) and Query Suggestion (low cost). Results show that subjects who used the Structured interface submitted significantly fewer queries, spent more time on search results pages, examined significantly more documents per query, and went to greater depths in the search results list. Results also showed that these subjects spent longer generating their initial queries, saved more relevant documents and rated their queries as more successful. These findings have implications for the usefulness of microeconomic theory as a way to model and explain search interaction, as well as for the design of query facilities.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval:Search Process; H.3.4 [Information Storage and Retrieval]: Systems and Software:Performance Evaluation
General Terms
Theory, Experimentation, Economics, Human Factors
Keywords
Search Behavior, Economic Models, Production Theory, Interactive Information Retrieval, Query Interfaces, Query Cost
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1. INTRODUCTION
During interactive information retrieval (IIR), users perform various interactions, such as posing queries, examining snippets and evaluating documents. Each action requires a certain amount of effort and comes at a cost, whether mental, physical, temporal, fiscal, or a combination thereof. In the early days of search system research, cost and utility figured prominently in both systems-centered and usercentered evaluation frameworks [6, 36]; however, less attention has been devoted to costs in contemporary information search research. Fiscal costs are presumed to be less important because search tools and information are freely available online. Mental and physical costs have been incorporated into evaluation measures as fixed parameters (for example, by introducing discounting based on rank [24]) and used to characterize interactions [25], but they have rarely been studied as independent variables because they are difficult to manipulate and measure. While many interactive search studies incorporate effort-based measures, such as the number of documents examined, number of queries issued and amount of time spent performing different actions [35, 25], few studies have attempted to model how interaction costs shape search behavior [27]. There have been some exceptions, though, where information seeking and retrieval behavior has been formally modeled using a cost-benefit framework [2, 31, 34].
In this work, we explore how the costs associated with querying affect search behaviors in the context of IIR. This is an important question to address because understanding the relationship between cost, behavior and performance might help explain how and why users interact with search systems in particular ways, and subsequently, enable designers of such systems to influence user interaction and search behavior. We ground our experiment using the recently proposed Search Economic Theory [2], which uses microeconomics to model the search process and provides a means to reason about interaction, cost and performance. Using this theory, we formulated the cost-interaction hypothesis that states: as the cost of querying increases, users will pose fewer queries and examine more documents per query. We conducted a laboratory experiment with 36 subjects and three interfaces that varied according to query cost, where cost was operationalized as time required to submit a query. Our findings show that subjects who used the more costly query interface submitted significantly fewer queries and examined significantly more documents per query than subjects who used the interfaces with lower querying cost.

23

2. BACKGROUND
Cost has had a long history in IR research and has figured prominently in both systems-centered and user-centered research. Cost have been defined in a number of ways, including as mental, physical, temporal and fiscal cost. In the preWeb era, research often focused on fiscal costs since search tools and online information were not free; many measures of fiscal costs focused on the amount users were willing to pay for search results [6, 9, 41]. For example, Cooper [10] proposed that users associate dollar amounts with search results as a way to understand subjective utility and Salton [36] proposed a number of cost-based measures related to the operational environment and response times. Today, only a few IR measures attempt to incorporate some type of cost into their computation, including nDCG [24], RBP [28], and time-biased gain [40].
While fiscal costs are no longer investigated as much, temporal cost, and more specifically response time, continues to be an important variable in research [12]. Since the late 1990s, a number of studies have demonstrated that network latency and download speeds impact how people interact with Web pages and their evaluations of information and website quality (e.g., [14, 23, 29]). In a recent study of time delays and search behavior, Taylor et al. [42] investigated the relationship between the amount of time Web pages take to load, the number of pages people viewed and the amount of information examined per page. Taylor et al. hypothesized that as response time increased, the number of pages searched would decrease and the amount of information examined per page would increase. The researchers hypothesized a step relationship between response time and search behavior rather than a linear or curvilinear relationship (that is, changes in behavior would only occur after a critical time delay point). Results showed partial support for the hypothesis related to response time and number of pages examined, and full support for the hypothesis related to response time and the amount of information examined per page. While these results provide evidence that cost, as measured by response time, impacts search behavior, the task explored in this study presented participants with a set of static Web pages, and query cost and participants' interactions with result pages were not examined.
There is some evidence that introducing time delays and query constraints can impact search behavior. Brutlag [7] reports in a blog about research conducted at Google that the time taken to return search results impacts the number of searches conducted by users. An increase in 400 milliseconds was shown to reduce the number of searches by 0.59% across a six week period on the Google search engine. Fujikawa et al. [19] constrained the number of queries a user could issue, ostensively making queries more valuable, and found that participants whose querying was constrained posed fewer queries and examined more documents per query, while participants who were not constrained submitted more queries and examined fewer documents per query. While this study was not focused on query cost, the results suggest that limitations on people's abilities to input queries can impact search behavior. Several studies have examined how query input facilities impact the types and properties of queries entered by users, and how this subsequently impacts search outcomes [1, 5, 17], but these relationships were not investigated in a cost-benefit framework. Furthermore, these studies have mainly focused on encouraging different querying

behaviors, rather than understanding how the query facility and the cost of querying shapes the entire search interaction.
Recently, Baskaya et al. [3] used a simulation to study the cost of querying on two different devices. Query cost was measured as time and set to different speeds related to how long it takes the average person to enter a query using a desktop computer and smart phone. Baskaya et al. found that increasing the time to enter a query resulted in a reduction in the number of queries submitted across a variety of querying strategies and across sessions of different lengths. Essentially, this simulation suggests that as query cost increases, the number of queries issued will decrease. However, the findings have yet to be empirically validated.
Researchers have also used physical costs, or the amount of effort a person exerts during search, as a way to evaluate IR systems and user interaction. These effort-based measures include number of queries issued, number of result pages evaluated and documents viewed [27]. However, few laboratory studies have attempted to model behavior or understand if and how different costs shape behavior, or how different interfaces are associated with different costs. Studies using large scale search log data have made some progress on modeling effort and search behavior (e.g., [16, 46]), but since physical cost cannot always be manipulated in operational environments, these studies present only a partial view of search behavior under particular circumstances. In addition, while these studies indicate how people act with a given technology, they do not show how people might act given different interface costs.
Researchers have also tried to model search costs by examining mental effort. This represents a much smaller body of work (but no less important, since searching is, by nature, a mental activity) presumably because of the difficulty of measuring mental effort. Both Dennis et al. [15] and Gwizdka [20] have examined the cognitive costs of different interfaces using cognitive load theory. In these works, the authors conducted experiments to estimate the mental effort required to undertake various search interactions during the search process. However, the relationship among cost, performance and interaction was not explored.
In terms of theoretical research there has been a number of proposals that model costs and search behavior within a cost-benefit framework [2, 18, 4, 31, 32, 33, 34]. Bates [4], for example, suggests one of the search tactics users adopt while interacting with a system is to make decisions about whether to pursue the current strategy or to change strategy, depending on a cost-benefit analysis. While Bates did not formally pursue this idea, Russell et al. explored this in their work on the cost of sense-making [34]. Here they analyzed the possible actions a user could take during the information seeking process in terms of the gain that would be accumulated over time. Then actions could be compared under the assumption that users would try to maximize gain while minimizing total cost. In this process, gain can be seen as the amount of relevant information found (or the value of the relevant information found). Information Foraging Theory (IFT) [31] provides a theoretical grounding for these ideas. IFT models how users would act and behave within heterogeneous information environments in ecologically valid ways. Specifically, it proposes that information seekers aim to minimize effort and maximize gain as they move between information patches, follow scents and assume a particular information diet. In experiments using a clustering inter-

24

face based on the Scatter-Gather principle, it was shown that users tend to act in an ecologically valid manner (that is, they conserve effort while seeking the most gain) [33].

Another way to formally model the information seeking process is through microeconomics. Azzopardi [2] proposes Search Economic Theory (SET) as a way to predict and explain search behavior. The model consists of a gain function and a cost function, which are parameterized by the type and number of interactions performed during the search process. Like the other formal models, the model assumes people will seek to minimize costs and maximize gain. Unlike the previous work, the theory was specifically developed to model the interaction between a user and an information retrieval system. As a result, the theory may provide useful insights into search behavior and can guide empirical investigation. In this paper, we explore how query costs affect search behavior under SET and then describe an empirical study that investigates this theory.

3. SEARCH ECONOMIC THEORY
In Azzopardi [2], the search process was modeled using an analogy to Production Theory [44]. In Production Theory, a firm takes inputs (i.e., capital and labor) and converts them to output (i.e., widgets). When applied to search, a user with a search engine is considered the firm, the user's interactions are considered inputs and the relevant information found is considered as the output (and measured by Cumulative Gain (CG) [24]). Azzopardi defined the inputs as the number of queries posed (Q) and the number of documents assessed per query (A). A functional relationship was proposed such that performance was related to interaction as follows: CG = g(Q, A). This function (referred to as the search production function) denoted the upper bound on output given a specific input combination (i.e., the maximum that can be produced with the given inputs). It was shown that for several standard information retrieval models (such as Boolean, VSM with TFIDF and BM25 [11]) the function g(Q, A) could be modeled closely with a CobbsDouglas production function [44]:

g(Q, A) = k.Qb.A(1-b)

(1)

where k denoted how well the hypothetical user could convert actions into relevant documents using the search system, and b was a mixing parameter which regulated the interplay between querying and assessing. The following cost model c(Q, A) was then used to ascribe a total cost to the interactions undertaken [2]:

c(Q, A) = a.Q + Q.A

(2)

where a denoted the relative cost of querying to assessing. The cost of assessing documents was assumed to be 1 (where the total number of documents assessed was Q multiplied by A).
Given this model of interaction defined by the gain and cost functions, it is possible to explore how the search behavior of a user would change when different variables are manipulated. We conducted a simulation to illustrate what changes in search behavior we could expect to see when query cost increases under the proposed model. We explored a range of relative querying costs (a = 0.5, 1, 2, 4) across various search production functions where b was varied from 0.5 to 0.6 and k was set to 3. Then we set CG = g(Q, A) = 30

Total Cost

40 b=0.55
30

No. of Queries

20

10

0

0

5

10

15

20

No. of Assessments per Query

200 a=1/2

180

a=1

160

a=2 a=4

140

120

100

80

0

5

10

15

20

No. of Assessments per Query

Figure 1: An example production function when b=0.55 (top), and the corresponding cost curves for different levels of a (bottom). The red line indicates the minimum cost point on each cost curve.
which models the situation where users are looking for 30 relevant documents (assuming binary relevance for computing cumulative gain)1.
Figure 1 shows the search production function when b = 0.55 (top plot) and the corresponding cost curves (bottom plot). Each point on the production curve represents the number of queries (Q) and the number of assessments per query (A) required to find 30 relevant documents. When b = 0.55, there exists a number of possible combinations of inputs that would yield the desired output: a user could, for example, issue approximately 10 queries and assess 10 documents per query, or issue approximately 4 queries and assess 20 documents per query to obtain the same gain.
The bottom plot shows the corresponding cost curves associated with the gain function when b = 0.55. The red line indicates the minimum cost point on each cost curve (i.e., the combination of inputs which minimizes the cost given the relative cost of querying). It is clear from this plot that as the relative query cost increases, the number of assessments per query also increases (and this results in a corresponding decrease in the number of queries issued). This trend
1Note that when we varied k this only affected the number of queries required not the number of assessments per query. So as k increased, Q decreased.

25

Query Cost a 0.5 1.0 2.0 4.0 A 12.5 24.5 49.0 98.0 b=0.51 Q 8.1 4.2 2.2 1.1 mc 104.9 107.8 110.8 113.8 A 4.0 8.0 15.5 31.5 b=0.53 Q 22.5 12.2 6.8 3.6 mc 101.4 109.7 118.6 128.3 A 2.5 4.5 9.0 18.0 b=0.55 Q 31.1 19.2 10.9 6.2 mc 93.3 105.7 119.9 136.0 A 1.5 3.0 6.0 12.5 b=0.57 Q 41.8 24.8 14.7 8.5 mc 83.7 99.2 117.6 139.4 A 1.0 2.5 4.5 9.0 b=0.59 Q 49.5 26.2 17.4 10.8 mc 74.3 91.7 113.2 139.9
Table 1: For different production curves characterized by b, the A and Q that result in the minimum cost (mc) for the relative query cost a is shown. As a increases, Q decreases and A increases.
is similar for different values of b. Table 1 shows a number of outcomes where b is varied between 0.51 and 0.59. The combination of Q and A that minimizes the total cost for each a is shown for each gain function with value b. These results show more generally that the model predicts the following: as relative query cost increases (i.e. a = 0.5  4), to minimize overall cost, a user should decrease the number of queries issued and increase the number of documents assessed per query.
It should be noted that as b decreases, assessing documents becomes more productive (i.e. there are more relevant documents in the ranked list) and so assessing will begin to dominate the production process. That is, at some point the best strategy is to keep looking at more documents, rather than querying. This is because as b decreases the gain derived from assessing documents increases. Put more formally, as b tends to zero, then A1-b tends to A. For the above cost function, this point is when b  0.5, where from then on, the combination of inputs that minimizes the total cost is to issue one query, and then continue assessing until the desired level of gain is obtained. This can be seen as a boundary case. On the other hand, if b increases, then the combination of inputs that minimizes costs tends towards issuing many queries and assessing only one document per query (again this depends on the cost model). That is, if high precision queries are very cheap then it makes sense to keep issuing them until the desired level of gain is achieved. However, as the cost of a query increases there will be a point where it is better to substitute querying for more assessments per query. The b values shown in Table 1 show this trade-off. Note the b values shown are representative of standard IR algorithms [2].
This model of search behavior follows the basic economic principle that if cost goes up, consumption goes down [44]. It also reflects some of the empirical and simulated observations made in prior work [3, 7]. While this is promising, the model and its predictions are based on a number of assumptions which need to be considered.
Modeling Caveats and Assumptions Firstly, the model assumes that users are rational and will behave such that

they would minimize interaction costs, and maximize performance. This is a common modeling assumption often employed (c.f. [4, 31, 32, 33, 34]). In the context of searching, an operation that users repeat and practice often, it has been shown that users adapt their behavior to systems [38], and that users do try to minimize effort and maximize performance [33] (i.e. they subscribe to Zipf's Principle of Least Effort). While the model may overestimate how well users could use an IR system, the assumption that they will try to optimize their behavior is, at least, reasonable. On an operational level, the model assumes that users will assess a fixed number of documents per query. However a user is likely to examine a different number of documents per query depending on the performance of the query. Given this assumption, it means that the model is rather coarse grained, considering the average number of documents assessed per query. A second operational point is that the cost model seems to ignore other costs, like viewing snippets, and how the costs of certain interactions may increase or decrease during the information seeking process. For example, a user may begin to run out of ideas for queries, and thus, increase the amount of time to generate subsequent queries. Ultimately, these simplifying assumptions reduce the problem to only the most salient factors, enabling us to reason about the relationship between the two main interactions within the search process: querying and assessing. The model generates a testable hypothesis, which we shall refer to as the cost-interaction hypothesis:
As the relative cost of querying increases the average number of queries issued will decrease and the average number of documents assessed per query will increase.
4. METHOD
To test this hypothesis, we conducted an experiment, where we operationalized cost in terms of query time as done in [40]. Three interfaces were created that required different amounts of time to enter queries. Subjects were randomly assigned to use one of these interfaces (Figure 2):
1. Structured Query Interface (high cost);
2. Standard Query Interface (medium cost);
3. Query Suggestion Interface (low cost).
Aside from the different query facilities, these interfaces were similar and displayed 10 search results per page. Each query facility occupied the same amount of vertical space. To approximate the query cost in terms of time, we employed the GOMS Keystroke Level Model [8] using the timings from a search experiment by [39] (shown in Table 2). For this approximation, we assumed that the average length of a query was three terms, and each term was, on average, seven characters in length. A summary of the GOMS analysis for these query interfaces is provided in Table 3. This analysis is discussed in more detail in each sub-section below.
4.1 Standard Query Interface
The Standard interface functioned as the baseline and is similar to what is provided by modern search engines. We estimated that this interface would require a medium amount of query effort relative to the other interfaces. To issue a query using the Standard interface subjects need to: (1) Go

26

Figure 2: The Structured query interface (behind), the Standard query interface (middle) and the Query Suggestion interface (front).
to the search box, (2) Enter 3 query terms of seven characters in length (plus two spaces), (3) Submit the query by pressing the return key and then wait for a response. With respect to the GOMS analysis, the corresponding lowlevel actions for each of these steps are (1) MHPCH, (2) 3*7K+2K, (3) KR. Using the estimates shown in Table 2, the total amount of time is 10.9 seconds per query.
4.2 Structured Query Interface
The Structured interface consisted of 15 query boxes (3 rows x 5 columns), a search button and a plus button. Subjects could enter one term per box, but could not press the tab or enter keys to move among boxes. Each row of search boxes provided different Boolean functionality: AND, OR and NOT. While this interface might seem overly cumbersome, such search interfaces are common in commercial services; for example, EBSCO Host and ERIC provide a structured query interface similar to the one used in this study (see Hearst [22] for other examples). The plus button allowed query term boxes to be added. For this interface the GOMS analysis is as follows: (1) Go to a search box (MHPCH), (2) Type in the first query term (7K), (3) Go to the next search box (MHPCH), (4) type in the second query term (7K), (5) type in third query term (MHPCH + 7K),

Action K P H
M R C

Time 0.28 1.1 0.4
1.35 0.8 0.2

Description Press a key or button Point with a mouse (excluding click) Move hands to keyboard from mouse
or vice versa Mentally Prepare System Response
Click

Table 2: GOMS Keystroke Level Model (time in seconds for each low level interaction)[8].

Interface Structured Standard Suggestion

Number of Queries Issued 12345 17.6 35.2 52.8 70.4 88.0 10.9 21.8 32.7 43.6 54.5 10.9 14.7 18.5 22.3 26.1

Table 3: Estimated GOMS total time spent querying in seconds for each interface.

and then (6) Submit the query - which requires the user to click the submit button (HC) and wait for response (R). The estimated total time required to enter each query is 17.6 seconds. To ensure that the physical costs associated with the Structured interface were as close to the GOMS model as possible, subjects had to retype query terms if they wanted to modify their previous query, i.e., query terms were removed from the boxes once the query was submitted, but the query was displayed on the screen. We assumed that this interface would also increase mental effort because subjects would have to understand how to enter terms.
4.3 Query Suggestion Interface
The Suggestion interface was identical to the Standard interface except query suggestions were presented after subjects entered their initial queries. For each topic, eight query suggestions were provided. These queries were collected in a previous study [26] and led to good results, where the queries retrieved between two and five TREC relevant documents in the top ten results. To issue a query using the Suggestion interface subjects needed to perform the same actions as for the Standard interface. However, for subsequent queries, subjects could select query suggestions, rather than type in queries. This resulted in the action sequence MHPCR taking approximately 3.8 seconds per suggestion. The Suggestion interface, while decreasing the amount of time to issue a query, should also reduce the mental effort associated with querying as it provides useful predefined suggestions. The query suggestions can be considered as a type of externalization, which is generally believed to reduce cognitive load in human-computer interactions [37]. Gwizdka's study of cognitive load in search found that an interface that displayed category terms reduced participants' cognitive load during query modification [20]. Thus, we anticipated that the query suggestion interface would also require less mental effort than the other two interfaces.
4.4 Corpus, Search Topics and System
A 3GB Text Retrieval Conference (TREC) test collection of over one million newspaper articles was used [45]. We selected three search topics from this collection: 344 (Abuses of E-mail); 347 (Wildlife Extinction) and 435 (Curbing Population Growth). We selected topics that had some contemporary relevance, that we thought would be of interest to

27

our target subjects and had a similar number of relevant documents available (123, 165 and 152, respectively). Our selection was also based on evidence from previous user studies with a similar system setup [26] where it was shown that the difficulty of these topics did not significantly differ. Subjects searched all three topics, where the topics were rotated with a Latin-square. The Whoosh IR Toolkit was used as the core of the retrieval system, with BM25 as the retrieval algorithm, using standard parameters, but with an implicit ANDing of query terms to restrict the set of retrieved documents to only those that contain all the query terms (similar to BM25A used in [2]). Subjects were not provided with a tutorial of the system.
4.5 Search Behaviors
To measure the impact of the cost variations on search behavior, the following signals were logged as subjects searched: start time of search tasks, end time of search tasks, queries issued, query suggestions used, results pages viewed, documents viewed, documents marked relevant, and task descriptions viewed. From the log it was possible to calculate the amount of time subjects spent issuing queries; examining search results pages and reformulating queries at this page; and viewing documents. It was also possible to examine features of the search interaction, such as, the number of terms per query, the depth of the last document viewed in the rank list, the number of queries issued and the number of documents saved.
4.6 Overall Workload: NASA TLX
The NASA Task Load Index (TLX) questionnaire was used to elicit subjects' perceptions of: Mental Demand, Physical Demand, Temporal Demand, Performance, Effort and Frustration. After finishing the three search tasks, subjects completed a NASA TLX questionnaire to rate their overall experience with the system, and then completed another NASA TLX questionnaire focused specifically on querying (Table 7). We reduced the number of scale points from 21 to 7 (the number of points on the computer version of the scale were difficult to distinguish and psychometric research shows that there is little gain in reliability beyond 11 points [30]) and we modified the factor statements so they matched the target task (in Hart's 2006 review of the usage of the NASA TLX it was noted that this modification was often performed by researchers [21]).
4.7 Subjects
Subjects were recruited from the University of Glasgow, UK2. Thirty-six undergraduate student subjects participated (12 subjects per interface). Twenty-one subjects were female and fifteen were male. Their average age was 21.8 (SD=4.4). Forty-seven percent were science majors and 53% were humanities majors. Subjects' search experience was measured using a modified version of the Search Self-Efficacy scale [13]. This instrument contains 14-items describing different search-related activities. Subjects respond to each item by indicating their confidence in completing each activity on a 10-point scale (1=totally unconfident; 10=totally confident). Subjects reported fairly high search self-efficacy [M=7.26 (SD=1.69)].
2Ethical Approval to conduct this study was obtained from the College of Science and Engineering, Reference No. CSE00913.

4.8 Instructions and Incentives
Subjects were instructed to imagine they were newspaper reporters and needed to gather documents to write stories about the provided topics. Subjects were told that there were over 100 relevant documents in the collection for each topic and they should try to find as many of these as possible during the allotted time (10 minutes per topic). To incentivize subjects, monetary bonuses were given to the top three performers for each topic per condition. Each subject was compensated with £10 and could earn an extra £2.50 per topic as a bonus.

5. RESULTS
Results are presented in three sections. The first two sections show how the search interface affected subjects' search behaviors, including the amount of time they spent engaged in different search processes. The third section presents results of the NASA TLX. We used ANOVA and post-hoc tests with Bonferroni's correction for analysis.
5.1 Search Behavior
The mean number of queries submitted by subjects, documents assessed per query, relevant documents found and assessment depth per query is shown in Table 5. Depth is the average position at which the last document was viewed in the search results list when at least one document was viewed.

Interactions Queries (Q) Query Len. Assess. / Q
Depth / Q Rels. Saved TREC Rels.

Structured 19.4 ± 10.6 3.7 ± 1.49
4.7 ± 4.2 14.9 ± 12.2 47.7 ±24.1 17.3 ±8.1

Interface Standard 35.0 ± 8.9 3.5 ± 0.7 1.6 ± 0.5 5.0 ± 3.7 34.7 ± 18.5 11.5 ± 7.3

Suggestion 31.2 ± 10.4
3.2 ± 0.5 2.5 ± 1.5 9.3 ± 10.1 43.2 ± 18.7 15.3 ± 6.7

Table 5: Search behavior and performance recorded per interface.
Subjects who used the Structured interface issued significantly fewer queries than those using the Standard and Suggestion interfaces (F (2, 33) = 7.59, p < 0.01). There was no difference in query length across different interfaces. Subjects who used the Structured interface viewed significantly more documents per query and went to greater depths to view these documents. Both the number of documents examined per query (F (2, 33) = 3.26, p = 0.05) and the depth per query were significantly different (F (2, 33) = 4.45, p < 0.01), but only between the Structured and Standard interfaces. Subjects who used the Structured interface saved the most documents, followed by those who used the Suggestion interface and those who used the Standard interface, but these differences were not significant. Of these saved documents, the number relevant was determined by using the relevance judgments included in the TREC test collection. Subjects who used the Structured interface found more relevant documents than those who used the Suggestion and Standard interfaces. However, these differences were not significant.
5.2 Time Spent Engaged in Search Processes
Table 6 displays the average amount of time subjects spent issuing their first query, on search results pages (QSERPs)

28

Factors Mental Demand Physical Demand Temporal Demand
Effort Performance Frustration Level

Questions for: (a) System (b) Query How mentally demanding was it to (a) use this system to complete the search tasks?, (b) query? How physically demanding was it to: (a) use this system to complete the search tasks?, (b) query? How hurried or rushed did you feel when (a) using this system to complete the search tasks?, (b) querying? How hard did you have to work to (a) accomplish your level of performance with this system?, (b) query? How successful were (a) you using this system to complete the search tasks?, (b) your queries? How insecure, discouraged, etc. were you while (a) using this system?, (b) submitting queries?

Table 4: Modified NASA TLX factor definitions for overall system load and query load.

and on document pages. The average amount of time spent on QSERPs includes the time spent viewing snippets and formulating queries. Subjects who used the Structured interface spent approximately 20 seconds longer formulating their first query than subjects who used the Standard and Suggestion interfaces (F (2, 33) = 4.05, p = 0.027). Follow-up tests confirm that subjects who used the Structured interface took significantly more time to construct their initial queries than subjects who used the other two interfaces. These subjects also spent significantly more time on QSERPs, viewing snippets and reformulating queries (F (2, 33) = 8.149, p < 0.01). There were no statistically significant differences in the amount of time subjects spent viewing documents among interface conditions, although subjects who used the Standard interface spent slightly more time per document.

Interactions First Query
QSERPs. Documents

Structured 44.1 ± 35.7 62.1 ± 32.9 15.1 ± 7.9

Interface Standard 22.9 ± 11.3 28.6 ± 7.7 17.4 ± 5.1

Suggestion 19.9 ± 12.3 34.7 ± 16.1 15.3 ± 6.5

Table 6: Mean (SD) time (in seconds) spent formulating first queries, on search results pages viewing snippets and reformulating queries (QSERP) and viewing documents.

5.3 NASA TLX
Table 7 (top) displays the overall NASA TLX scores for each factor for each interface. These results capture the overall workload experienced by subjects as they engaged in all search behaviors (querying, viewing snippets and assessing documents). Subjects using the Standard interface reported experiencing the highest mental demand, followed by those using the Structured and Suggestion interfaces. This ordering was consistent for physical demand and temporal demands. Subjects indicated similar levels of success (performance), effort and frustration. None of the differences identified above were statistically significant.
Individual factor scores were summed to arrive at a total workload score. The Standard interface received the highest overall workload score, followed by the Structured and the Suggestion interfaces. These differences were also not significant. Table 7 also provides information about the relative contributions of each factor to overall workload for each interface. For example, effort was rated as the highest contributor to load by those who used the Structured and Suggestion interfaces, while mental demand was rated highest by those using the Standard interface. More interestingly, physical demand received the lowest scores regardless of interface. It was also the case that temporal demand was evaluated as the second highest contributor to load by subjects who used the Structured and Standard interfaces, but as the second lowest for those using the Suggestion interface.
Table 7 (bottom) displays the NASA TLX ratings for

System Load Factor Mental Physical
Temporal Performance
Effort Frustration
Total

Structured 4.5 ± 1.4 2.3 ± 1.3 4.6 ± 1.8 4.0 ± 1.4 4.8 ± 1.5 3.5 ± 1.4 23.7 ± 5.2

Interface Standard 5.1 ± 0.9 2.7 ± 1.1 5.0 ± 1.4 4.1 ± 1.4 4.9 ± 1.4 3.7 ± 1.7 25.5 ± 5.9

Suggestion 3.9 ± 1.6 1.9 ± 1.4 3.6 ± 1.9 4.2 ± 1.4 4.7 ± 1.6 3.8 ± 1.8 22.1 ± 5.1

Query Load Factor Mental Physical
Temporal Performance
Effort Frustration
Total

Structured 4.2 ± 1.4 2.2 ± 1.3 4.2 ± 1.9 4.2 ± 2.0 4.4 ± 1.3 4.0 ± 1.7 23.2 ± 5.1

Interface Standard 4.8 ± 1.3 2.1 ± 1.2 4.1 ± 1.8 3.1 ± 1.6 4.9 ± 1.6 4.3 ± 1.5 23.3 ± 7.0

Suggestion 3.7 ± 1.9 2.2 ± 1.6 3.0 ± 2.0 3.9 ± 2.0 4.5 ± 1.6 3.1 ± 2.0 20.4 ± 8.6

Table 7: Mean (SD) of NASA TLX Factors

query load. These results help us understand the load experienced by subjects as they queried. Subjects using the Standard interface reported experiencing the highest mental demand when querying, followed by those using the Structured and Suggestion interfaces. There were no differences among interfaces according to physical demand. With respect to temporal demand, subjects using the Structured and Standard interfaces reported higher demands than those using the Suggestion interface. Subjects who used the Structured interface described their queries as more successful (performance) than those using the Standard and Suggestion interfaces. Subjects using the Standard interface reported greater levels of frustration and effort. None of the differences identified above were statistically significant. With respect to overall query load, the Structured and Standard interfaces received similar scores, which were higher than overall query load for the Suggestion interface. These differences were not significant.
6. DISCUSSION
We found that subjects who used the Structured (high cost) interface submitted significantly fewer queries, examined more documents per query and went to greater depths in the search results list than subjects who used the lower cost Standard and Suggestion interfaces. This finding supports the cost-interaction hypothesis we generated from the microeconomic theory: as the cost of querying increases, the number of queries issued will decrease and the number of documents evaluated will increase. However, counter to what we expected, we found that subjects who used the Standard interface, which was constructed to represent medium cost, issued the most queries, viewed the fewest documents per query and were the shallowest in their evaluation of the

29

search results list. Our expectation was that these behaviors would be associated with the Suggestion interface since the cost of querying was considered lowest.
One possible explanation for this finding is that subjects who used the Suggestion interface may not have experienced any meaningful differences with respect to cost since they did not always click on the available suggestions. On average, subjects selected 7.31 query suggestions out of 24 query suggestions, which were available to them during their search sessions. The initial GOMS analysis assumed a greater uptake of suggestions, which would have resulted in much lower querying costs. If we revise our GOMS estimate, based on the actual usage statistics, then the time spent per query on the Suggestion interface would have been approximately, 9.24 seconds, on average3. This indicates that in terms of querying, the costs between these interfaces was very similar (and not as great as we originally anticipated). The empirical findings also confirmed that there was no significant difference in querying time between the Suggestion and Standard interface. However, the Suggestion interface may have introduced some added costs since subjects needed to examine and make decisions about the query suggestions. This is partially supported by the differences in the amount of time subjects spent on QSERPs evaluating snippets and formulating queries. Subjects who used the Suggestion interface spent slightly longer on QSERPs than subjects who used the Standard interface (34.7 and 28.6 seconds, respectively) and this increased time might have reflected time spent evaluating query suggestions. Though, here, the differences were not significant, which implies that the querying cost between these conditions was not different.
This raises a number of issues when applying the microeconomic theory in practice: (1) what costs are at play and how do we estimate them within the cost function, and (2) if the query costs were not significantly different, why do we observe different behavior between the Standard and Suggestion interfaces. Regarding costs, our research has shown that cost and how it is operationalized within the economic model needs to be reconsidered. Is the querying cost inclusive of the time spent viewing snippets or not, and if not, is it a document cost? If we use the QSERP estimates to denote the query cost (i.e. time spent querying and viewing snippets per query) then the Standard interface would be the least expensive: and our findings would be consistent with the cost-interaction hypothesis. However, the theory and the cost models employed would need to be refined, perhaps, redefined, to determine whether this is appropriate. We leave this to further work, and consider alternative explanations regarding the differences in search behavior between these two interfaces below.
We first considered whether the results with regard to the Standard and Suggestion interfaces were caused by differences in query quality and search result quality. Since the Suggestion interface provided good quality queries that had reasonably high precision, differences in query quality might explain why subjects who used the Suggestion interface tended to assess more documents per query and issue fewer queries than subjects using the Standard interface (i.e., they might have seen better search results). It is important to note that query quality differences would not
3Revised GOMS estimate: (23.89 queries entered manually * 10.9 seconds per query + 7.31 suggestions clicked * 3.8 seconds per suggestion) / (31.2 total queries) = 9.24 seconds per query issued.

#Q P@5 P@10 P@15

Structured 232 0.159 0.139 0.121

Standard 420 0.191 0.171
0.162*

Suggestion 374
0.239** 0.217** 0.196**

Table 8: Mean precision of queries issued across each interface. * (**) indicates significantly better than Structured (Structured and Standard).
have been caused by individual ability since subjects were randomly assigned to interface condition.
To determine whether there was a difference in query quality, subjects' queries were submitted to the retrieval system and evaluated using TREC relevance judgements to better understand what types of performance subjects encountered. Table 8 reports the precision values at 5, 10 and 15 documents along with the number of queries issued in each group. The results show that the quality of the queries on the Suggestion interface was higher than the other two interfaces. Statistical testing revealed that system performance was significantly better across these three precision measures (p < 0.05). This finding suggests that subjects who used the Suggestion interface issued better quality queries and potentially encountered more TREC relevant documents in the ranked lists, but we note that there were no significant differences in the number of documents subjects saved or the number of TREC-relevant saved, so we cannot conclude that subjects using the Suggestion interface experienced better performance, especially considering the work of Turpin and Hersh [43] who have shown that system performance evaluated in this way does not always map to user performance.
The results from the NASA TLX also provide additional insights about how subjects experienced these interfaces, and the extent to which cost, as we have manipulated it in this study, impacted their experiences. Although many interesting differences are discussed below, it is important to keep in mind that none of these were statistically significant, so this discussion is primarily presented in the service of future research.
The NASA TLX was administered twice: once to focus on system load and once to focus on query load. The system load allowed us to understand the entire experience (querying, evaluating snippets and results, search result quality), while query load isolated the load introduced by the query facilities. One of the most interesting and consistent findings was that physical demand contributed the least to subjects' overall loads. For query load, the Structured, Standard and Suggestion interfaces were rated nearly the same for this factor (2.2, 2.2, and 2.1, respectively). There were greater differences in subjects' ratings of physical demand for system load, with the Standard interface receiving a rating of 2.7, followed by Structured (2.3) and Suggestion (1.9) interfaces. These ratings were, in general, low, when compared to the other factors, which suggests that our manipulation of physical demand may not have been as extreme as expected.
Results also showed that subjects associated the most mental demand with the Standard interface (4.8), followed by the Structured (4.2) and Suggestion (3.7). This ordering was consistent for the mental demand associated with system load, although the values were slightly larger (5.1, 4.5 and 3.9, respectively). Compared to the ratings for physical demand, it is clear that mental demand contributed more

30

to subjects' overall workloads. The Suggestion interface received the lowest ratings for mental demand as expected, but the Standard received higher ratings than the Structured, which was unexpected. We believe that the higher mental demand associated with the Standard interface was a result of subjects creating more queries while searching.
While one might argue that the Structured interface is clunky and less usable than the other interfaces, subjects did not report high levels of frustration with the query facility. Rather, the highest levels of frustration were reported by subjects who used the Standard interface. This suggests that subjects did not find the added time costs associated with the Structured interface annoying and that subjects were more frustrated by having to enter more queries with the Standard interface. The Suggestion interface, as expected, received the lowest frustration rating for query load. These results make us question how subjects would have rated the usability or aesthetics of the interfaces. We hypothesize that subjects would rate the Structured interface the lowest, which is interesting since they actually performed better with it. We leave this for future work.
A final difference we observed in the NASA TLX was with respect to the temporal factor, which gauged how hurried or rushed subjects felt when doing the search tasks. Subjects who used the Standard interface reported experiencing the highest levels of temporal demand (5.0), followed by those who used the Structured interface (4.6) and those who used the Suggestion interface (3.6). This difference suggests that a system that encourages the type of search behavior that subjects who used the Standard interface engaged in - issuing more queries, evaluating fewer documents per query and shallowly evaluating search results lists - might have negative psychological consequences, assuming that feeling hurried and rushed generates stress. There were no differences between the Standard and Structured interfaces for temporal demand for query load, which seems to further suggest that it was the total search strategy engaged in by those who used the Standard interface that contributed to the differences in overall temporal demand.
7. CONCLUSIONS AND FUTURE WORK
In this paper, we investigated how query cost affects search behavior. We used microeconomic theory to motivate our study and conducted a theoretical analysis, which generated the cost-interaction hypothesis. A laboratory study with 36 subjects was conducted to evaluate this hypothesis using three interfaces: Structured, Standard and Suggestion. We found partial support for this hypothesis: subjects who used the high cost Structured interface submitted fewer queries, spent more time on search results pages, examined more documents per query, and went to greater depths in the search results list than subjects who used the lower cost Standard and Suggestion interfaces.
Our results have both theoretical and practical implications. Attempts to formally model search interaction are promising, but the results of this study suggest that refinements to the microeconomic theory are required to improve the realism of the model. Our results also imply that at least one additional factor should be included in the gain and cost functions to account for viewing snippets, and that more sophisticated cost functions may be more appropriate. We found that in testing the theory, care needs to be taken to control each of the factors involved. Changing the costs

may change other aspects of the interaction and these need to be accounted for when testing hypotheses generated by the model.
Future work will focus on further refining the search economic theory/models and exploring how alternative designs for query facilities and other search interface features might encourage users to engage in and adopt more positive and successful search behaviors.
Acknowledgments We would like to thank Kelly Marshall for her help in conducting the user experiments.
8. REFERENCES
[1] E. Agapie, G. Golovchinsky, and P. Qvardordt. Encouraging behavior: A foray into persuasive computing. In Proceedings on the Symposium on Human-Computer Information Retrieval, 2012.
[2] L. Azzopardi. The economics in interactive information retrieval. In Proceedings of the 34th ACM conference on research and development in information retrieval (SIGIR), pages 15­24, 2011.
[3] F. Baskaya, H. Keskustalo, and K. J¨arvelin. Time drives interaction: simulating sessions in diverse searching environments. In Proceedings of the 35th ACM conference on research and development in information retrieval (SIGIR), pages 105­114, 2012.
[4] M. J. Bates. Training and education for online. chapter Information search tactics, pages 96­105. Taylor Graham Publishing, London, UK, UK, 1989.
[5] N. J. Belkin, D. Kelly, G. Kim, J.-Y. Kim, H.-J. Lee, G. Muresan, M.-C. Tang, X.-J. Yuan, and C. Cool. Query length in interactive information retrieval. In Proceedings of the 26th ACM conference on research and development in information retrieval (SIGIR), pages 205­212, 2003.
[6] N. J. Belkin and A. Vickery. Interaction in Information Systems. University Press, 1985.
[7] J. Brutlag. Speed matters for google web search. In Technical Report, Retrieved online at http://googleresearch.blogspot.com/2009/06/speedmatters.html, on May 11, 2013, 2009.
[8] S. K. Card, T. P. Moran, and A. Newell. The keystroke-level model for user performance time with interactive systems. Communications of the ACM, 23(7):396­410, 1980.
[9] M. D. Cooper. A cost model for evaluating information retrieval systems. Journal of the American Society for Information Science, pages 306­312, 1972.
[10] W. S. Cooper. On selecting a measure of retrieval effectiveness, part 1: The subjective philosophy of evaluation. Journal of the American Society for Information Science, 24:87­100, 1973.
[11] B. Croft, D. Metzler, and T. Strohman. Search Engines: Information Retrieval in Practice. 2009.
[12] J. Dabrowski and E. V. Munson. 40 years of searching for the best computer system response time. Interacting with Computers, 23:555­564, 2011.
[13] S. Debowski, R. Wood, and A. Bandura. The impact of guided exploration and enactive exploration on self-regulatory mechanisms and information

31

acquisition through electronic enquiry. Journal of Applied Psychology, 86:1129­1141, 2001.
[14] A. R. Dennis and N. J. Taylor. Information foraging on the web: The effects of acceptable internet delays on multi-page information search behavior. Decision Support Systems, 42:810­824, 2006.
[15] S. Dennis, P. Bruza, and R. McArthur. Web searching: a process-oriented experimental study of three interactive search paradigms. Journal of the American Society for Information Science and Technology, 53(2):120­133, Jan. 2002.
[16] G. Dupret and B. Piwowarski. A user behavior model for average precision and its generalization to graded judgments. In Proceedings of the 33rd ACM conference on research and development in information retrieval (SIGIR), pages 531­538, 2010.
[17] K. Franzen and J. Kalgren. Verbosity and interface design. In SICS Technical Report: T2000:04, Retrieved online at http://soda.swedish-ict.se/2623/2/irinterface.pdf on May 11, 2013, 1997.
[18] N. Fuhr. A probability ranking principle for interactive information retrieval. Information Retrieval, 11(3):251­265, 2008.
[19] K. Fujikawa, H. Joho, and S. Nakayama. Constraint can affect human perception, behaviour, and performance of search. In Proceedings of the 14th International Conference on Asia-Pacific Digital Libraries (ICADL), pages 39­48, 2012.
[20] J. Gwizdka. Distribution of cognitive load in web search. Journal of the American Society for Information Science and Technology, 61(11):2167­2187, Nov. 2010.
[21] S. G. Hart. Nasa-task load index (nasa-tlx); 20 years later. In Proceedings of the Human Factors and Ergonomics Society 50th Annual Meeting, pages 904­908, 2006.
[22] M. A. Hearst. Search User Interfaces. New York, NY: Cambridge University Press, 2009.
[23] J. A. Jacko, A. Sears, and M. S. Borella. The effect of network delay and media on user perceptions of web resources. Behaviour and Information Technology, 19(6):427­439, 2000.
[24] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of information retrieval technology. ACM Trans. on Information Systems, 20(4):422­446, 2002.
[25] D. Kelly. Methods for evaluating interactive information retrieval systems with users. Foundations and Trends in Information Retrieval, 3:1­224, 2009.
[26] D. Kelly, K. Gyllstrom, and E. W. Bailey. A comparison of query and term suggestion features for interactive searching. In Proceedings of the 32nd ACM conference on research and development in information retrieval (SIGIR), pages 371­378, 2009.
[27] D. Kelly and C. Sugimoto. A systematic review of interactive information retrieval evaluation studies, 1967-2006. Journal of the American Society for Information Science and Tech., 64(4):745­770, 2013.
[28] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. on Information Systems, 27(1):2:1­2:27, 2008.

[29] F. F. H. Nah. A study on tolerable waiting time: How long are web users willing to wait? Behaviour and Information Technology, 23(3):153­163, 2004.
[30] J. C. Nunnally. Psychometic Theory. McGraw, 1978. [31] P. Pirolli and S. Card. Information foraging.
Psychological Review, 106:643­675, 1999. [32] P. Pirolli and W. T. Fu. Snif-act: a model of
information foraging on the www. In Proceedings of the 9th International Conference on User Modeling, pages 45­54, 2003. [33] P. Pirolli, P. Schank, M. Hearst, and C. Diehl. Scatter/gather browsing communicates the topic structure of a very large text collection. In Proceedings of the ACM SIGCHI Conference, pages 213­220, 1996. [34] D. M. Russell, M. J. Stefik, P. Pirolli, and S. K. Card. The cost structure of sensemaking. In Proceedings of the INTERACT and SIGCHI Conference, pages 269­276, 1993. [35] I. Ruthven. Interactive information retrieval. Annual Review of Information Science and Technology (ARIST), 42:43­92, 2008. [36] G. Salton. Evaluation problems in interactive information retrieval. Information Storage and Retrieval, 6:29­44, 1970. [37] M. Scaife and Y. Rogers. External cognition: How do graphical representations work? International Journal of Human-Computer Studies, 45:185­213, 1996. [38] C. L. Smith and P. B. Kantor. User adaptation: good results from poor systems. In Proceedings of the 31st ACM conference on research and development in information retrieval (SIGIR), pages 147­154, 2008. [39] M. D. Smucker. Towards timed predictions of human performance for interactive information retrieval evaluation. In Proceedings of the Symposium on Human-Computer Information Retrieval, 2009. [40] M. D. Smucker and C. L. Clarke. Time-based calibration of effectiveness measures. In Proceedings of the 35th ACM conference on research and development in information retrieval (SIGIR), pages 95­104, 2012. [41] L. T. Su. Evaluation measures for interactive information retrieval. Information Processing and Management, 28:503­316, 1992. [42] N. J. Taylor, A. R. Dennis, and J. W. Cummings. Situation normality and the shape of search: The effects of time delays and information presentation on search behavior. Journal of the American Society for Information Science and Tech., 64(5):909­928, 2013. [43] A. Turpin and W. Hersh. Why batch and user evaluations do not give the same results. In Proceedings of the 24th ACM conference on research and development in information retrieval (SIGIR), pages 225­231, 2001. [44] H. R. Varian. Intermediate microeconomics: A modern approach. W.W. Norton, New York:, 1987. [45] E. M. Voorhees. Overview of the trec 2005 robust retrieval track. In Proceedings of TREC-14, 2006. [46] E. Yilmaz, M. Shokouhi, N. Craswell, and S. Robertson. Expected browsing utility for web search evaluation. In Proceedings of the 19th ACM conference on Information and knowledge management (CIKM), pages 1561­1564, 2010.

32

Ranking Document Clusters Using Markov Random Fields

Fiana Raiber fiana@tx.technion.ac.il

Oren Kurland kurland@ie.technion.ac.il

Faculty of Industrial Engineering and Management, Technion Haifa 32000, Israel

ABSTRACT
An important challenge in cluster-based document retrieval is ranking document clusters by their relevance to the query. We present a novel cluster ranking approach that utilizes Markov Random Fields (MRFs). MRFs enable the integration of various types of cluster-relevance evidence; e.g., the query-similarity values of the cluster's documents and queryindependent measures of the cluster. We use our method to re-rank an initially retrieved document list by ranking clusters that are created from the documents most highly ranked in the list. The resultant retrieval effectiveness is substantially better than that of the initial list for several lists that are produced by effective retrieval methods. Furthermore, our cluster ranking approach significantly outperforms stateof-the-art cluster ranking methods. We also show that our method can be used to improve the performance of (stateof-the-art) results-diversification methods.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation
Keywords: ad hoc retrieval, cluster ranking, query-specific clusters, markov random fields
1. INTRODUCTION
The cluster hypothesis [33] gave rise to a large body of work on using query-specific document clusters [35] for improving retrieval effectiveness. These clusters are created from documents that are the most highly ranked by an initial search performed in response to the query.
For many queries there are query-specific clusters that contain a very high percentage of relevant documents [8, 32, 25, 14]. Furthermore, positioning the constituent documents of these clusters at the top of the result list yields highly effective retrieval performance; specifically, much better than that of state-of-the art retrieval methods that rank documents directly [8, 32, 25, 14, 10].
As a result of these findings, there has been much work on ranking query-specific clusters by their presumed relevance
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

to the query (e.g., [35, 22, 24, 25, 26, 14, 15]). Most previous approaches to cluster ranking compare a representation of the cluster with that of the query. A few methods integrate additional types of information such as inter-cluster and cluster-document similarities [18, 14, 15]. However, there are no reports of fundamental cluster ranking frameworks that enable to effectively integrate various information types that might attest to the relevance of a cluster to a query.
We present a novel cluster ranking approach that uses Markov Random Fields. The approach is based on integrating various types of cluster-relevance evidence in a principled manner. These include the query-similarity values of the cluster's documents, inter-document similarities within the cluster, and measures of query-independent properties of the cluster, or more precisely, of its documents.
A large array of experiments conducted with a variety of TREC datasets demonstrates the high effectiveness of using our cluster ranking method to re-rank an initially retrieved document list. The resultant retrieval performance is substantially better than that of the initial ranking for several effective rankings. Furthermore, our method significantly outperforms state-of-the-art cluster ranking methods. Although the method ranks clusters of similar documents, we show that using it to induce document ranking can help to substantially improve the effectiveness of (state-of-the-art) retrieval methods that diversify search results.
2. RETRIEVAL FRAMEWORK
Suppose that some search algorithm was employed over a corpus of documents in response to a query. Let Dinit be the list of the initially highest ranked documents. Our goal is to re-rank Dinit so as to improve retrieval effectiveness.
To that end, we employ a standard cluster-based retrieval paradigm [34, 24, 18, 26, 15]. We first apply some clustering method upon the documents in Dinit; C l(Dinit) is the set of resultant clusters. Then, the clusters in C l(Dinit) are ranked by their presumed relevance to the query. Finally, the clusters' ranking is transformed to a ranking of the documents in Dinit by replacing each cluster with its constituent documents and omitting repeats in case the clusters overlap. Documents in a cluster are ordered by their query similarity.
The motivation for employing the cluster-based approach just described follows the cluster hypothesis [33]. That is, letting similar documents provide relevance status support to each other by the virtue of being members of the same clusters. The challenge that we address here is devising a (novel) cluster ranking method -- i.e., we tackle the second step of the cluster-based retrieval paradigm.

333

Figure 1: The three types of cliques considered for graph G. G is composed of a query node (Q) and three (for the sake of the example) nodes (d1, d2, and d3) that correspond to the documents in cluster C. (i) lQD contains the query and a single document from C; (ii) lQC contains all nodes in G; and, (iii) lC contains only the documents in C.

Formally, let C and Q denote random variables that take as values document clusters and queries respectively. The cluster ranking task amounts to estimating the probability that a cluster is relevant to a query, p(C|Q):

p(C |Q)

=

p(C, Q) p(Q)

ra=nk

p(C, Q).

(1)

The rank equivalence holds as clusters are ranked with respect to a fixed query.
To estimate p(C, Q), we use Markov Random Fields (MRFs). As we discuss below, MRFs are a convenient framework for integrating various types of cluster-relevance evidence.

2.1 Using MRFs to rank document clusters
An MRF is defined over a graph G. Nodes represent random variables and edges represent dependencies between these variables. Two nodes that are not connected with an edge correspond to random variables that are independent of each other given all other random variables. The set of nodes in the graph we construct is composed of a node representing the query and nodes representing the cluster's constituent documents. The joint probability over G's nodes, p(C, Q), can be expressed as follows:

p(C, Q) = lL(G) l(l) ;

(2)

Z

L(G) is the set of cliques in G and l is a clique; l(l) is a potential (i.e., positive function) defined over l; Z =
C,Q lL(G) l(l) is the normalization factor that serves to ensure that p(C, Q) is a probability distribution. The normalizer need not be computed here as we rank clusters with respect to a fixed query.
A common instantiation of potential functions is [28]:

l(l) d=ef exp(lfl(l)),
where fl(l) is a feature function defined over the clique l and l is the weight associated with this function. Accordingly, omitting the normalizer from Equation 2, applying the rank-preserving log transformation, and substituting the potentials with the corresponding feature functions results in our ClustMRF cluster ranking method:

p(C|Q) ra=nk

l fl (l).

(3)

lL(G)

This is a generic linear (in feature functions) cluster ranking function that depends on the graph G. To instantiate a specific ranking method, we need to (i) determine G's structure,

specifically, its clique set L(G); and, (ii) associate feature functions with the cliques. We next address these two tasks.

2.1.1 Cliques and feature functions
We consider three types of cliques in the graph G. These are depicted in Figure 1. In what follows we write d  C to indicate that document d is a member of cluster C.
The first clique (type), lQD, contains the query and a single document in the cluster. This clique serves for making inferences based on the query similarities of the cluster's constituent documents when considered independently. The second clique, lQC , contains all nodes of the graph; that is, the query Q and all C's constituent documents. This clique is used for inducing information from the relations between the query-similarity values of the cluster's constituent documents. The third clique, lC, contains only the cluster's constituent documents. It is used to induce information based on query-independent properties of the cluster's documents.
In what follows we describe the feature functions defined over the cliques. In some cases a few feature functions are defined for the same clique, and these are used in the summation in Equation 3. Note that the sum of feature functions is also a feature function. The weights associated with the feature functions are set using a train set of queries. (Details are provided in Section 4.1.)

The lQD clique. High query similarity exhibited by C's

constituent documents can potentially imply to C's rele-

vance [26]. Accordingly, let d ( C) be the document in

lQD .

We

define fgeo-qsim;lQD (lQD)

d=ef

1
log sim(Q, d) |C| ,

where |C| is the number of documents in C, and sim(·, ·) is

some inter-text similarity measure, details of which are pro-

vided in Section 4.1. Using this feature function in Equation

3 for all the lQD cliques of G amounts to using the geometric

mean of the query-similarity values of C's constituent docu-

ments. All feature functions that we consider use logs so as

to have a conjunction semantics for the integration of their

assigned values when using Equation 3.1

The lQC clique. Using the lQD clique from above results
in considering the query-similarity values of the cluster's documents independently of each other. In contrast, the lQC clique provides grounds for utilizing the relations between these similarity values. Specifically, we use the log
1Before applying the log function we employ add- (= 10-10) smoothing.

334

of the minimal, maximal, and standard deviation2 of the {sim(Q, d)}dC values as feature functions for lQC, denoted min-qsim, max-qsim, and stdv-qsim, respectively.

The lC clique. Heretofore, the lQD and lQC cliques served

for inducing information from the query similarity values of

C's documents. We now consider query-independent proper-

ties of C that can potentially attest to its relevance. Doing so

amounts to defining feature functions over the lC clique that

contains C's documents but not the query. All the feature

functions that we define for lC are constructed as follows.

We first define a query-independent document measure, P,

and apply it to document d ( C) yielding the value P(d).

Then, we use log A({P(d)}dC) where A is an aggregator function: minimum, maximum, and geometric mean. The

resultant feature functions are referred to as min-P, max-

P, and geo-P, respectively. We next describe the document

measures that serve as the basis for the feature functions.

The cluster hypothesis [33] implies that relevant docu-

ments should be similar to each other. Accordingly, we mea-

sure for document d in C its similarity with all documents

in C:

Pdsim(d) d=ef

1 |C|

diC sim(d, di).

The next few query-independent document measures are

based on the following premise. The higher the breadth of

content in a document, the higher the probability it is rel-

evant to some query. Thus, a cluster containing documents

with broad content should be assigned with relatively high

probability of being relevant to some query.

High entropy of the term distribution in a document is a

potential indicator for content breadth [17, 3]. This is be-

cause the distribution is "spread" over many terms rather

than focused over a few ones. Accordingly, we define

Pentropy(d) d=ef - wd p(w|d) log p(w|d), where w is a term and p(w|d) is the probability assigned to w by an unsmoothed

unigram language model (i.e., maximum likelihood estimate)

induced from d.

Inspired by work on Web spam classification [9], we use

the inverse compression ratio of document d, Picompress(d),

as an additional measure. (Gzip is used for compression.)

High compression ratio presumably attests to reduced con-

tent breadth [9].

Two additional content-breadth measures that were pro-

posed in work on Web retrieval [3] are the ratio between the

number of stopwords and non-stopwords in the document,

Psw1(d); and, the fraction of stopwords in a stopword list

that appear in the document, Psw2(d). We use INQUERY's

stopword list [2]. A document containing many stopwords

is presumably of richer language (and hence content) than

a document that does not contain many of these; e.g., a

document containing a table composed only of keywords [3].

For some of the Web collections used for evaluation in

Section 4, we also use the PageRank score [4] of the docu-

ment, Ppr(d), and the confidence level that the document is

not spam, Pspam(d). The details of the spam classifier are

provided in Section 4.1.

We note that using the feature functions that result from

applying the geometric mean aggregator upon the query-

independent document measures just described, except for

2It was recently argued that high variance of the querysimilarity values of the cluster's documents might be an indicator for the cluster's relevance, as it presumably attests to a low level of "query drift" [19].

dsim, could have been described in an alternative way. That
1
is, using log P(d) |C| as a feature function over a clique con-
taining a single document. Then, using these feature functions in Equation 3 amounts to using the geometric mean.3

3. RELATED WORK
The work most related to ours is that on devising cluster ranking methods. The standard approach is based on measuring the similarity between a cluster representation and that of the query [7, 34, 35, 16, 24, 25, 26]. Specifically, a geometric-mean-based cluster representation was shown to be highly effective [26, 30, 15]. Indeed, ranking clusters by the geometric mean of the query-similarity values of their constituent documents is a state-of-the-art cluster ranking approach [15]. This approach rose as an integration of feature functions used in ClustMRF, and is shown in Section 4 to substantially underperform ClustMRF.
Clusters were also ranked by the highest query similarity exhibited by their constituent documents [22, 31] and by the variance of these similarities [25, 19]. ClustMRF incorporates these methods as feature functions and is shown to outperform each.
Some cluster ranking methods use inter-cluster and clusterdocument similarities [14, 15]. While ClustMRF does not utilize such similarities, it is shown to substantially outperform one such state-of-the-art method [15].
A different use of clusters in past work on cluster-based retrieval is for "smoothing" (enriching) the representation of documents [20, 16, 24, 13]. ClustMRF is shown to substantially outperform one such state-of-the-art method [13].
To the best of our knowledge, our work is first to use MRFs for cluster ranking. In the context of retrieval tasks, MRFs were first introduced for ranking documents directly [28]. We show that using ClustMRF to produce document ranking substantially outperforms this retrieval approach; and, that which augments the standard MRF retrieval model with query-independent document measures [3]. MRFs were also used, for example, for query expansion, passage-based document retrieval, and weighted concept expansion [27].

4. EVALUATION 4.1 Experimental setup

corpus AP
ROBUST
WT10G GOV2 ClueA ClueAF ClueB ClueBF

# of docs 242,918
528,155
1,692,096 25,205,179
503,903,810

data Disks 1-3
Disks 4-5 (-CR)
WT10g GOV2
ClueWeb09 (Category A)

queries
51-150 301-450, 600-700 451-550 701-850
1-150

50,220,423 ClueWeb09 (Category B) 1-150

Table 1: Datasets used for experiments.

The TREC datasets specified in Table 1 were used for experiments. AP and ROBUST are small collections, composed mostly of news articles. WT10G and GOV2 are Web
3Similarly, we could have used the geometric mean of the query-similarity values of the cluster constituent documents as a feature function defined over the lQC clique rather than constructing it using the lQD cliques as we did above.

335

collections; the latter is a crawl of the .gov domain. For the ClueWeb Web collection both the English part of Category A (ClueA) and the Category B subset (ClueB) were used. ClueAF and ClueBF are two additional experimental settings created from ClueWeb following previous work [6]. Specifically, documents assigned by Waterloo's spam classifier [6] with a score below 70 and 50 for ClueA and ClueB, respectively, were filtered out from the initial corpus ranking described below. The score indicates the percentage of all documents in ClueWeb Category A that are presumably "spammier" than the document at hand. The ranking of the residual corpus was used to create the document list upon which the various methods operate. Waterloo's spam score is also used for the Pspam(·) measure that was described in Section 2.1. The Pspam(·) and Ppr(·) (PageRank score) measures are used only for the ClueWeb-based settings as these information types are not available for the other settings.
The titles of TREC topics served for queries. All data was stemmed using the Krovetz stemmer. Stopwords on the INQUERY list were removed from queries but not from documents. The Indri toolkit (www.lemurproject.org/indri) was used for experiments.
Initial retrieval and clustering. As described in Section
2, we use the ClustMRF cluster ranking method to re-rank an initially retrieved document list Dinit. Recall that after ClustMRF ranks the clusters created from Dinit, these are "replaced" by their constituent documents while omitting repeats. Documents within a cluster are ranked by their query similarity, the measure of which is detailed below. This cluster-based re-ranking approach is employed by all the reference comparison methods that we use and that rely on cluster ranking. Furthermore, ClustMRF and all reference comparison approaches re-rank a list Dinit that is composed of the 50 documents that are the most highly ranked by some retrieval method specified below. Dinit is relatively short following recommendations in previous work on cluster-based re-ranking [18, 25, 26, 13]. In Section 4.2.7 we study the effect of varying the list size on the performance of ClustMRF and the reference comparisons.
We let all methods re-rank three different initial lists Dinit. The first, denoted MRF, is used unless otherwise specified. This list contains the documents in the corpus that are the most highly ranked in response to the query when using the state-of-the-art Markov Random Field approach with the sequential dependence model (SDM) [28]. The free parameters that control the use of term proximity information in SDM, T , O, and U , are set to 0.85, 0.1, and 0.05, respectively, following previous recommendations [28]. We also use MRF's SDM with its free parameters set using cross validation as one of the re-ranking reference comparisons. (Details provided below.) All methods operating on the MRF initial list use the exponent of the document score assigned by SDM -- which is a rank-equivalent estimate to that of log p(Q, d) -- as simMRF (Q, d), the document-query similarity measure. This measure was used to induce the initial ranking using which Dinit was created. More generally, for a fair performance comparison we maintain in all the experiments the invariant that the scoring function used to create an initially retrieved list is rank equivalent to the documentquery similarity measure used in methods operating on the list. Furthermore, the document-query similarity measure is

used in all methods that are based on cluster ranking (including ClustMRF) to order documents within the clusters.
The second initial list used for re-ranking, DocMRF (discussed in Section 4.2.4), is created by enriching MRF's SDM with query-independent document measures [3].
The third initial list, LM, is addressed in Section 4.2.5. The list is created using unigram language models. In contrast, the MRF and DocMRF lists were created using retrieval methods that use term proximity information. Let pDz ir[µ](·) be the Dirichlet-smoothed unigram language model induced from text z; µ is the smoothing parameter. The LM similarity between texts x and y is simLM (x, y) d=ef
exp -CE pDx ir[0](·) pDy ir[µ](·) [37, 17], where CE is
the cross entropy measure; µ is set to 1000.4 Accordingly, the LM initial list is created by using simLM (Q, d) to rank the entire corpus.5 This measure serves as the documentquery similarity measure for all methods operating over the LM list, and for the inter-document similarity measure used by the dsim feature function.
Unless otherwise stated, to cluster any of the three initial lists Dinit, we use a simple nearest-neighbor clustering approach [18, 25, 14, 26, 13, 15]. For each document d ( Dinit), a cluster is created from d and the k - 1 documents di in Dinit (di = d) with the highest simLM (d, di); k is set to a value in {5, 10, 20} using cross validation as described below. Using such small overlapping clusters (all of which contain k documents) was shown to be highly effective for cluster-based document retrieval [18, 25, 14, 26, 13, 15]. In Section 4.2.6 we also study the performance of ClustMRF when using hierarchical agglomerative clustering.
Evaluation metrics and free parameters. We use MAP
(computed at cutoff 50, the size of the list Dinit that is reranked) and the precision of the top 5 documents (p@5) and their NDCG (NDCG@5) for evaluation measures.6 The free parameters of our ClustMRF method, as well as those of all reference comparison methods, are set using 10-fold cross validation performed over the queries in an experimental setting. Query IDs are the basis for creating the folds. The two-tailed paired t-test with p  0.05 was used for testing statistical significance of performance differences.
For our ClustMRF method, the free-parameter values are set in two steps. First, SVMrank [12] is used to learn the values of the l weights associated with the feature functions. The NDCG@k of the k constituent documents of a cluster serves as the cluster score used for ranking clusters in the learning phase7. (Recall from above that documents in a
4The MRF SDM used above also uses Dirichlet-smoothed unigram language models with µ = 1000. 5Queries for which there was not a single relevant document in the MRF or LM initial lists were removed from the evaluation. For the ClueWeb settings, the same query set was used for ClueX and ClueXF. 6We note that statAP, rather than AP, was the official TREC evaluation metric in 2009 for ClueWeb with queries 1­50. For consistency with the other queries for ClueWeb, and following previous work [3], we use AP for all ClueWeb queries by treating prel files as qrel files. We hasten to point out that evaluation using statAP for the ClueWeb collections with queries 1­50 yielded relative performance patterns that are highly similar to those attained when using AP. 7Using MAP@k as the cluster score resulted in a slightly less effective performance. We also note that learning-to-

336

AP ROBUST WT10G GOV2 ClueA ClueAF ClueB ClueBF

MAP p@5 NDCG@5
MAP p@5 NDCG@5
MAP p@5 NDCG@5
MAP p@5 NDCG@5
MAP p@5 NDCG@5 MAP p@5 NDCG@5
MAP p@5 NDCG@5
MAP p@5 NDCG@5

Init
10.1 50.7 50.6
19.9 51.0 52.5
15.8 37.5 37.2
12.7 59.3 48.6
4.5 19.1 12.6 8.6 46.3 32.4
12.5 33.1 24.4
15.8 44.8 33.2

TunedMRF
9.9 48.7 49.4
20.0 51.0 52.7
15.4 36.9 35.3i
12.7 60.8
49.5 4.9i
21.1 15.6i
8.7 47.8 33.1 13.5i
35.5
27.0 16.3i 46.8 34.3

ClustMRF
10.8
53.0
54.4t 21.0it 52.4
54.7 18.0it 44.9it 42.8it 14.2it 70.1it 56.2it 6.3it 44.6it 29.4it
8.9
50.2
33.9 16.1it 48.7it 37.4it 17.0
48.5
36.9

Table 2: The performance of ClustMRF and a tuned MRF (TunedMRF) when re-ranking the MRF initial list (Init). Boldface: the best result in a row. 'i' and 't' mark statistically significant differences with Init and TunedMRF, respectively.

cluster are ordered based on their query similarity.) A ranking of documents in Dinit is created from the cluster ranking, which is performed for each cluster size k ( {5, 10, 20}), using the approach described above; k is then also set using cross validation by optimizing the MAP performance of the resulting document ranking. The train/test split for the first and second steps are the same -- i.e., the same train set used for learning the l's is the one used for setting the cluster size. As is the case for ClustMRF, the final document ranking induced by any reference comparison method is based on using cross validation to set free-parameter values; and, MAP serves as the optimization criterion in the training (learning) phase.
Finally, we note that the main computational overhead, on top of the initial ranking, incurred by using ClustMRF is the clustering. That is, the feature functions used are either query-independent, and therefore can be computed offline; or, use mainly document-query similarity values that have already been computed to create the initial ranking. Clustering of a few dozen documents can be computed efficiently; e.g., based on document snippets.
4.2 Experimental results
4.2.1 Main result
Table 2 presents our main result. Namely, the performance of ClustMRF when used to re-rank the MRF initial list. Recall that the initial ranking was induced using MRF's SDM with free-parameter values set following previous recommendations [28]. Thus, we also present for reference the re-ranking performance of using MRF's SDM with its three free parameters set using cross validation as is the case for
rank methods [23] other than SVMrank, which proved to result in highly effective performance as shown below, can also be used for setting the values of the l weights.

ClustMRF

AP ROBUST WT10G GOV2

MAP p@5 NDCG@5
MAP p@5 NDCG@5
MAP p@5 NDCG@5
MAP p@5 NDCG@5

10.8 53.0 54.4 21.0 52.4 54.7 18.0 44.9 42.8 14.2 70.1 56.2

ClustMRF

ClueA ClueAF ClueB ClueBF

MAP p@5 NDCG@5
MAP p@5 NDCG@5
MAP p@5 NDCG@5
MAP p@5 NDCG@5

6.3 44.6 29.4 8.9 50.2 33.9 16.1 48.7 37.4 17.0 48.5 36.9

stdvqsim
9.4 43.7c 45.0c 19.0c 50.7 52.4 15.4c 38.4c 37.8c 12.7c 59.3c 48.2c
maxsw2
5.4c 28.7c 20.3c 8.6 47.2 32.5 14.2c 41.9c 30.1c 16.3 45.0 35.5

maxsw2
9.7 44.6c 45.8c 17.7c 46.9c 49.1c 12.2c 31.7c 28.6c 12.9c 62.3c 48.8c
maxsw1
5.3c 29.3c 20.5c 7.8c 40.4c 28.9c
15.4 42.9c 32.5c 15.7c 42.3c 32.8

geoqsim
10.6 50.9 52.0 20.6 50.4 52.4 16.3c 39.3c 39.0c 13.2c 58.0c 46.6c
maxqsim
4.5c 18.7c 12.4c 8.3 49.3 34.3
12.8c 33.9c 25.5c 14.8c 42.9c 32.8

minsw2
9.6 49.1 50.4 16.8c 44.7c 45.9c 14.2c 33.9c 32.4c
14.2
66.3 52.3
geoqsim
4.8c 20.9c 14.0c 8.6 48.7 33.9 12.9c 34.2c 25.6c 15.9 43.2 33.6

Table 3: Using each of ClustMRF's top-4 feature functions by itself for ranking the clusters so as to re-rank the MRF initial list. Boldface: the best performance per row. 'c' marks a statistically significant difference with ClustMRF.

the free parameters of ClustMRF; TunedMRF denotes this method. We found that using exhaustive search for finding SDM's optimal parameter values in the training phase yields better performance (on the test set) than using SVMrank [12] and SVMmap [36]. Specifically, T , O, and U were set to values in {0, 0.05, . . . , 1} with T + O + U = 1.
We first see in Table 2 that while TunedMRF outperforms the initial MRF ranking in most relevant comparisons (experimental setting × evaluation measure), there are cases (e.g., for AP and WT10G) for which the reverse holds. The latter finding implies that optimal free-parameter values of MRF's SDM do not necessarily generalize across queries.
More importantly, we see in Table 2 that ClustMRF outperforms both the initial ranking and TunedMRF in all relevant comparisons. Many of the improvements are substantial and statistically significant. These findings attest to the high effectiveness of using ClustMRF for re-ranking.
4.2.2 Analysis of feature functions
We now turn to analyze the relative importance attributed to the different feature functions used in ClustMRF; i.e., the l weights assigned to these functions in the training phase by SVMrank. We first average, per experimental setting and cluster size, the weights assigned to a feature function using the different training folds. Then, the feature function is assigned with a score that is the reciprocal rank of its corresponding (average) weight. Finally, the feature functions are ordered by averaging their scores across experimental settings and cluster sizes. Two feature functions, pr and spam, are only used for the ClueWeb-based settings. Hence, we perform the analysis separately for the ClueWeb and nonClueWeb (AP, ROBUST, WT10G, and GOV2) settings.

337

MAP

AP

p@5

NDCG@5

MAP

ROBUST p@5

NDCG@5

MAP

WT10G p@5

NDCG@5

GOV2

MAP p@5

NDCG@5

MAP

ClueA p@5

NDCG@5

MAP

ClueAF p@5

NDCG@5

MAP

ClueB

p@5

NDCG@5

MAP

ClueBF p@5

NDCG@5

Init
10.1 50.7 50.6
19.9c 51.0 52.5
15.8c 37.5c 37.2c 12.7c 59.3c 48.6c 4.5c 19.1c 12.6c 8.6 46.3 32.4
12.5c 33.1c 24.4c 15.8 44.8 33.2

Inter
10.4 55.9i
56.0i
20.8i 52.2 53.9
15.1c 38.0c 36.8c 12.9c 62.9c 50.2c 5.3c 24.3c 17.8c 8.9
44.8 32.6 14.9i 44.5i 34.3i
16.7 48.2 36.4

AMean
10.6 51.1
52.2
20.3c 49.1c 51.2c 16.6ic 39.6ic 38.5c 13.1ic 58.8c 47.8c
4.6c 19.3c 13.2c 8.8 49.8i 35.0i
13.0ic 34.7c 26.1ic 15.9 45.6 34.4

GMean
10.6 50.9 52.0 20.6i 50.4 52.4
16.3c 39.3c 39.0c 13.2ic 58.0c 46.6c
4.8c 20.9c 14.0c 8.6 48.7 33.9
12.9c 34.2c 25.6c
15.9 43.2 33.6

CRank
10.0 50.0
50.5
19.7c 46.6ic 49.1ic 14.5c 34.2c 32.7ic 12.7c 62.3c 48.4c
5.2c 24.3c 18.5ic 8.3
41.5c 30.0 16.0i 46.6i 35.3i 17.7i
50.3
38.0i

CMRF
10.8
53.0 54.4 21.0i
52.4
54.7 18.0i 44.9i 42.8i 14.2i 70.1i 56.2i 6.3i 44.6i 29.4i
8.9
50.2
33.9 16.1i 48.7i 37.4i
17.0 48.5 36.9

Table 4: Comparison with cluster-based retrieval methods used for re-ranking the MRF initial list. (CMRF is a shorthand for ClustMRF.) Boldface marks the best result in a row. 'i' and 'c' mark statistically significant differences with the initial ranking and ClustMRF, respectively.

For the non-ClueWeb settings, the feature functions, in descending order of attributed importance, are: stdv-qsim, max-sw2, geo-qsim, min-sw2, max-sw1, max-qsim, min-dsim, geo-sw2, min-icompress, min-qsim, min-sw1, geo-icompress, max-dsim, geo-dsim, max-icompress, geo-entropy, min-entropy, geo-sw1, max-entropy. For the ClueWeb settings the feature functions are ordered as follows: max-sw2, max-sw1, maxqsim, geo-qsim, max-spam, geo-sw2, min-icompress, minsw2, geo-sw1, min-sw1, min-qsim, stdv-qsim, max-pr, mindsim, min-entropy, max-entropy, min-spam, geo-icompress, geo-entropy, max-icompress, geo-spam, geo-pr, geo-dsim, minpr, max-dsim.
Two main observations rise. First, each of the three types of cliques used in Section 2.1 for defining the MRF has at least one associated feature function that is assigned with a relatively high weight. For example, the geo-qsim function defined over lQD, the max-qsim function defined over lQC , and the max-sw2 function defined over lC , are among the 4, 6 and 2 most important functions in both cases (nonClueWeb and ClueWeb settings). Second, for the ClueWeb settings, the feature functions defined over the lC clique and which are based on query-independent document measures (e.g., max-sw1, max-sw2, max-spam) are attributed with high importance. In fact, among the top-10 feature functions for the ClueWeb settings only two (max-qsim and geoqsim) are not based on a query-independent measure. This is not the case for the non-ClueWeb settings where different statistics of the query-similarity values are among the top10 feature functions. We note that using some of the queryindependent document measures utilized here was shown in work on Web retrieval to be effective for ranking documents directly [3]. We demonstrated the merits of using such measures for ranking document clusters.

In Table 3 we present the performance of using each of the top-4 feature functions (for the non-ClueWeb and ClueWeb settings) by itself as a cluster ranking method. As in Section 4.2.1, we use the cluster ranking to re-rank the MRF initial list. We see in Table 3 that in almost all relevant comparisons ClustMRF is more effective -- often to a substantial and statistically significant degree -- than using one of its top-4 feature functions alone. Thus, we conclude that ClustMRF's effective performance cannot be attributed to a single feature function that it utilizes.
We also performed ablation tests as follows. ClustMRF was trained each time without one of its top-10 feature functions. This resulted in a statistically significant performance decrease with respect to at least one of the three evaluation metrics of concern (MAP, p@5 and NDCG@5) for all top-10 feature functions for the ClueWeb settings. (Actual numbers are omitted as they convey no additional insight.) Yet, there was no statistically significant performance decrease for any of the top-10 feature functions for the non-ClueWeb settings. These findings attest to the redundancy of feature functions when employing ClustMRF for the non-ClueWeb settings and to the lack thereof in the ClueWeb settings.
Finally, we computed the Pearson correlation of the learned l's values (averaged over the train folds and cluster sizes) between experimental settings. We found that for pairs of non-ClueWeb settings, excluding AP, the correlation was at least 0.5; however, the correlation with AP was much smaller. For the ClueWeb settings, the correlation between ClueB and ClueBF was high (0.83) while that for other pairs of settings was lower than 0.5. Thus, we conclude that the learned l values can be collection, and setting, dependent.

4.2.3 Comparison with cluster-based methods

We next compare the performance of ClustMRF with that

of highly effective cluster-based retrieval methods. All meth-

ods re-rank the MRF initial list.

The InterpolationF method (Inter in short) [13] ranks

documents directly using the score function:

Score(d; Q) d=ef (1 - )

+ sim(Q,d)
dDinit sim(Q,d )



. CC l(Dinit) sim(Q,C)sim(C,d)
dDinit CC l(Dinit) sim(Q,C)sim(C,d )

This state-of-the-

art re-ranking method represents the class of approaches

that use clusters to "smooth" document representations [13].

In contrast to Inter, ClustMRF belongs to a class of meth-

ods that rely on cluster ranking. Accordingly, the next ref-

erence comparison methods represent this class. Section 4.1

provided a description of how the cluster ranking is trans-

formed to a ranking of the documents in Dinit. The AMean

method [26, 15], for example, scores cluster C by the arith-

metic mean of the query similarity values of its constituent

documents.

Formally,

Score(C; Q) d=ef

1 |C|

dC sim(Q, d).

Scoring C by the geometric mean of the query-similarity

values of its constituent documents, Score(C; Q) d=ef

|C| dC sim(Q, d), was shown to yield state-of-the-art clus-

ter ranking performance [15]. This approach, henceforth

referred to as GMean, results from aggregating several fea-

ture functions (geo-qsim) that are used in our ClustMRF

method. (See Section 2.1 for details.)

An additional state-of-the-art cluster ranking method is

ClustRanker (CRank in short) [15]. Cluster C is scored by

Score(C; Q) d=ef (1 - )

+ sim(Q,C)p(C)
C C l(Dinit) sim(Q,C )p(C )

338

AP ROBUST WT10G GOV2 ClueA ClueAF ClueB ClueBF

MAP p@5 NDCG@5
MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP
p@5
NDCG@5 MAP p@5 NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

DocMRF
9.9 50.7 51.0 20.3 52.1 54.0 17.1 42.0 40.4
15.0 66.3 54.0 9.8 42.4 28.4 9.5
52.6
35.7
16.6 45.6 33.6 17.6 50.3 37.5

ClustMRF
11.0 53.5 53.5 21.2d 53.2 55.3
17.7 42.5 40.3 15.3 68.7 55.8
10.0 49.3d 33.4d
9.5 49.6 35.7 18.9d 52.9d 39.9d 19.4d 55.3d 41.9d

Table 5: Using ClustMRF to re-rank the DocMRF [3] list. Boldface: best result in a row. 'd' marks a statistically significant difference with DocMRF.



; dC sim(Q,d)sim(C,d)p(d)
C C l(Dinit) dC sim(Q,d)sim(C ,d)p(d)

p(C)

and

p(d)

are

estimated based on inter-cluster and inter-document (across

clusters) similarities, respectively. These similarities, com-

puted using the language-model-based measure simLM (·, ·),

are not utilized by ClustMRF that uses inter-document sim-

ilarities only within a cluster.

Following the original reports of Inter [13] and CRank [15],

we estimate sim(Q, C) and sim(C, d) in these methods using

simLM (·, ·); C is represented by the concatenation of its con-

stituent documents. For a fair comparison with ClustMRF,

sim(Q, d) is set in all reference comparisons considered here

to simMRF (·, ·), which was used to create the initial MRF

list that is re-ranked.

All free parameters of the methods are set using cross val-

idation. Specifically,  which is used by Inter and CRank

is set to values in {0, 0.1, . . . , 1}. The graph out degree

and the dumping factor used by CRank are set to values

in {4, 9, 19, 29, 39, 49} and {0.05, 0.1, . . . , 0.9, 0.95}, respec-

tively. The cluster size used by each method is selected from

{5, 10, 20} as is the case for ClustMRF. Table 4 presents the

performance numbers.

We can see in Table 4 that in a vast majority of the rele-

vant comparisons ClustMRF outperforms the reference com-

parison methods. Many of the improvements are substantial

and statistically significant. In the few cases that ClustMRF

is outperformed by one of the other methods, the perfor-

mance differences are not statistically significant.

4.2.4 Using ClustMRF to re-rank the DocMRF list
Heretofore, we studied the performance of ClustMRF when used to re-rank the MRF initial list. The analysis presented in Section 4.2.2 demonstrated the effectiveness -- especially for the ClueWeb settings -- of using feature functions that utilize query-independent document measures. Thus, we now turn to explore ClustMRF's performance when employed over a document ranking that is already based on using query-independent document measures.

To that end, we follow some recent work [3]. We re-rank the 1000 documents that are the most highly ranked by MRF's SDM that was used above to create the MRF initial list. Re-ranking is performed using an MRF model that is enriched with query-independent document measures [3]. We use the same document measures utilized by ClustMRF, except for dsim which is based on inter-document similarities and which was not considered in this past work that ranked documents independently of each other [3]. The resultant ranking, induced using SVMrank for learning parameter values, is denoted DocMRF. (SVMrank yielded better performance than SVMmap.) We then let ClustMRF rerank the top-50 documents. In doing so, we use the exponent of the score assigned by DocMRF to document d, which is a rank equivalent estimate to that of log p(Q, d), as the sim(Q, d) value used by ClustMRF. Thus, we maintain the invariant mentioned above that the scoring function used to induce the ranking upon which ClustMRF operates is rank equivalent to the document-query similarity measure used in ClustMRF. We note that ClustMRF is different from DocMRF in two important respects. First, by the virtue of ranking clusters first and transforming the ranking to that of documents rather than ranking documents directly as is the case in DocMRF. Second, by the completely different ways that document-query similarities are used.
Comparing the performance of DocMRF in Table 5 with that of the MRF initial ranking in Table 2 attests to the merits of using DocMRF for re-ranking. We can also see in Table 5 that applying ClustMRF over the DocMRF list results in performance improvements in almost all relevant comparisons. Many of the improvements for the ClueWeb settings are substantial and statistically significant.
4.2.5 Using ClustMRF to re-rank the LM list
The third list we re-rank using ClustMRF is LM, which was created using unigram language models. For reference comparison we use the cluster-based Inter method which was used in Section 4.2.3. Experiments show -- actual numbers are omitted due to space considerations -- that for reranking the LM list, the GMean cluster ranking method is more effective in most relevant comparisons than the other two cluster ranking methods used in Section 4.2.3 for reference comparison (AMean and CRank). Hence, GMean is used here as an additional reference comparison.
ClustMRF, Inter and GMean use the simLM (·, ·) similarity measure, which was used for inducing the initial ranking, for sim(Q, d). All other implementation details are the same as those described above. As a result, ClustMRF, as well as Inter and GMean, use only unigram language models in the LM setting considered here. This is in contrast to the MRF-list setting considered above where term-proximities information was used.
An additional reference comparison that uses unigram language models is relevance model number 3 [1], RM3, which is a state-of-the-art query expansion approach. RM3 is also used to re-rank the LM list. All (50) documents in the list are used for constructing RM3. Its free-parameter values are set using cross validation. Specifically, the number of expansion terms and the interpolation parameter that controls the reliance on the original query are set to values in {5, 10, 25, 50} and {0.1, 0.3, . . . , 0.9}, respectively. Dirichletsmoothed language models are used with µ = 1000.

339

MAP

AP

p@5

NDCG@5

MAP

ROBUST p@5

NDCG@5

MAP WT10G p@5

NDCG@5

MAP

GOV2 p@5

NDCG@5

MAP

ClueA p@5

NDCG@5

MAP ClueAF p@5

NDCG@5

MAP

ClueB

p@5

NDCG@5

MAP

ClueBF p@5

NDCG@5

Init
9.9 49.6 49.9 19.3c 49.5c 51.6c 15.0
36.4c 35.8 11.8c 56.6c 46.5c 3.3c 16.1c 10.7c 8.0c 47.4 32.3 11.4c 29.0c 21.2c 14.7c 42.9c 32.1c

Inter
10.6i 56.1ic 55.6i
20.1i 50.9
53.1 14.9 37.5 37.1 12.6ic 62.4ic 50.4i 5.0i 24.6ic 17.9ic 8.5i 46.7 32.6 13.8ic 40.5i 29.6i
15.6
46.3
34.6

GMean RM3

10.8i 9.9

50.7 49.1

51.8 20.6i
52.1 53.8 14.9

49.3 19.7ic 49.7c
52.1c 14.5

37.5 35.5 12.4ic 60.8ic 48.8c 3.7ic 17.2c
11.5c
8.2 45.7 32.3 12.0ic 31.6ic 23.4ic 15.5
43.4
33.4c

36.6c 35.9 12.7ic 60.4ic 49.1c 3.8ic 17.4c
11.0c 8.7i
47.6 34.3 13.9ic 40.2i 30.0i 16.4i 48.9i 36.6i

ClustMRF
10.5 51.3 51.7 20.5i 52.9i 55.6i
14.6 42.2i
39.3 13.5i 68.4i 54.3i 5.5i 43.3i 27.7i 8.7i
51.5
35.6 16.0i 46.0i 34.8i 16.8i 49.2i 38.7i

Table 6: Re-ranking the LM initial list. Boldface: the best result in a row. 'i' and 'c' mark statistically significant differences with the initial ranking and ClustMRF, respectively.

We see in Table 6 that ClustMRF outperforms the reference comparisons in a vast majority of the relevant comparisons. Many of the improvements are substantial and statistically significant. These results, along with those presented in Sections 4.2.1 and 4.2.4, attest to the effectiveness of using ClustMRF to re-rank different initial lists.

4.2.6 Varying the clustering algorithm

Thus far, we used ClustMRF and the reference compar-

isons with nearest-neighbor (NN) clustering. In Table 7 we

present the retrieval performance of using hierarchical ag-

glomerative clustering (HAC) with the complete link mea-

sure. This clustering was shown to be among the most ef-

fective hard clustering methods for cluster-based retrieval

[24, 13].

We

use

1 simLM (d1 ,d2)

+

1 simLM (d2,d1)

for an inter-

document dissimilarity measure; and, cut the clustering den-

drogram so that the resultant average cluster size is the clos-

est to a value k ( {5, 10, 20}). Doing so somewhat equates

the comparison terms with using the NN clusters whose size

is in {5, 10, 20}. Cross validation is used in all cases for

setting the value of k.

The MRF initial list is clustered and serves as the ba-

sis for re-ranking. Experiments show (actual numbers are

omitted due to space considerations) that among the three

cluster ranking methods which were used above for refer-

ence comparison (AMean, GMean, and CRank) CRank is

the most effective when using HAC. Hence, CRank serves

as a reference comparison here.

We see in Table 7 that in the majority of relevant com-

parisons, ClustMRF improves over the initial ranking when

using HAC. In contrast, CRank is outperformed by the ini-

tial ranking in most relevant comparisons for HAC. Indeed,

ClustMRF outperforms CRank in most cases for both NN

and HAC. We also see that ClustMRF is (much) more effec-

tive when using the overlapping NN clusters than the hard

AP ROBUST WT10G GOV2 ClueA ClueAF ClueB ClueBF

MAP p@5 NDCG@5
MAP p@5 NDCG@5
MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP p@5 NDCG@5
MAP p@5 NDCG@5
MAP p@5 NDCG@5

Init
10.1 50.7 50.6 19.9 51.0 52.5 15.8 37.5 37.2 12.7 59.3 48.6 4.5 19.1 12.6 8.6 46.3 32.4 12.5 33.1 24.4 15.8 44.8 33.2

HAC

NN

CRank ClustMRF CRank ClustMRF

9.9
49.8
50.5
19.1 50.1 51.7 14.8 36.6 34.4 13.2i 61.5 49.7 5.6i 23.7 16.9i 8.4 43.9 32.0 14.4i 39.5i 30.6i
15.3
43.9
32.7

9.6i 46.5i 46.8i 19.6
50.4
51.9
15.8
38.2
37.0 13.6i
63.9
51.5 5.8i 31.7ic 21.0i
9.2 48.9 33.4 14.5i 39.7i 30.3i 15.2 43.1 32.5

10.0 50.0 50.5
19.7 46.6i 49.1i
14.5 34.2 32.7i
12.7 62.3 48.4
5.2 24.3 18.5i 8.3 41.5 30.0 16.0i 46.6i 35.3i 17.7i
50.3
38.0i

10.8
53.0
54.4 21.0ic 52.4c 54.7c 18.0ic 44.9ic 42.8ic 14.2ic 70.1ic 56.2ic 6.3ic 44.6ic 29.4ic 8.9
50.2c 33.9 16.1i 48.7i 37.4i
17.0 48.5 36.9

Table 7: Using nearest-neighbor clustering (NN) vs. (complete link) hierarchical agglomerative clustering (HAC). The MRF initial list is used. Boldface: the best result in a row per clustering algorithm; underline: the best result in a row. 'i' and 'c': statistically significant differences with the initial ranking and CRank, respectively.

clusters created by HAC. The improved effectiveness of using NN in comparison to HAC echoes findings in previous work on cluster-based re-ranking [13]. For CRank, the performance of using neither NN nor HAC dominates that of using the other.
4.2.7 The effect of the size of the initial list
Until now, ClustMRF and all reference comparison methods were used to re-rank an initial list of 50 documents. Using a short list follows common practice in work on clusterbased re-ranking [18, 25, 26, 13] as was mentioned in Section 4.1. We now turn to study ClustMRF's performance when re-ranking longer lists. To that end, we use for the initial list the n ( {50, 100, 250, 500}) documents that are the most highly ranked by MRF's SDM [28] which was used above for creating the MRF initial list. For reference comparisons we use TunedMRF (see Section 4.2.1); and, the AMean and GMean cluster ranking methods described in Section 4.2.3. Nearest-neighbor clustering is used.
We see in Figure 2 that in almost all cases -- i.e., experimental settings and values of n -- ClustMRF outperforms both the initial ranking and TunedMRF; often, the performance differences are quite substantial. Furthermore, in most cases (with the notable exception of AP) ClustMRF outperforms AMean and GMean.
4.2.8 Diversifying search results
We next explore how ClustMRF can be used to improve the performance of search-results diversification approaches. Specifically, we use the MMR [5] and the state-of-the-art xQuAD [29] diversification methods.

340

MAP

13.5 13.0 12.5 12.0 11.5 11.0 10.5 10.0 9.5
50 100
11.0 10.0 9.0 8.0 7.0 6.0 5.0 4.0
50 100

AP
Init TunedMRF
AMean GMean ClustMRF

250

500

n

ClueA

Init TunedMRF
AMean GMean ClustMRF

250

500

n

MAP

MAP

23.0 22.5 22.0 21.5 21.0 20.5 20.0 19.5
50 100
10.0
9.5
9.0
8.5
8.0
7.5 50 100

ROBUST

Init TunedMRF
AMean GMean ClustMRF

250

500

n

ClueAF

Init TunedMRF
AMean GMean ClustMRF

250

500

n

MAP

MAP

19.0 18.5 18.0 17.5 17.0 16.5 16.0 15.5 15.0
50 100
20.0 19.0 18.0 17.0 16.0 15.0 14.0 13.0 12.0 11.0
50 100

WT10G

Init TunedMRF
AMean GMean ClustMRF

250

500

n

ClueB

Init TunedMRF
AMean GMean ClustMRF

250

500

n

MAP

MAP

18.0 17.0 16.0 15.0 14.0 13.0 12.0
50 100
19.0 18.0 17.0 16.0 15.0 14.0
50 100

GOV2

Init TunedMRF
AMean GMean ClustMRF

250

500

n

ClueBF

Init TunedMRF
AMean GMean ClustMRF

250

500

n

MAP

Figure 2: The effect on MAP(@50) performance of the size n of the MRF initial list that is re-ranked.

ClueA ClueAF ClueB ClueBF

-NDCG ERR-IA P-IA -NDCG ERR-IA P-IA -NDCG ERR-IA P-IA -NDCG ERR-IA P-IA

Init
24.5 16.0 11.8 42.6 32.0 21.0 33.2 21.1 15.4 41.6 29.7 18.9

MRF
26.2c 17.3c 10.3c 42.9 32.3 20.2c 33.6c 21.3c 14.4ic 42.6ic 30.2ic 18.4

MMR QClust ClustMRF

25.4c
17.5c 9.6ic 39.0ic 29.8c 14.9ic
33.9c
21.5c 12.8ic 38.7ic 27.0ic 14.5ic

38.7i 30.5i 16.7i
43.8
34.2 17.6i 43.7i 32.0i 17.4i 45.4i 33.3i
17.8

MRF
27.4ic 17.9ic 13.3c 44.3i 33.4i
21.0
39.7ic 25.9ic 19.4ic 46.1ic 33.2i 21.4ic

xQuAD QClust ClustMRF

28.9ic 19.6ic 13.6ic 43.7
33.1

38.8i 30.6i 17.2i 45.5i 34.9i

20.0
39.3ic 25.3ic 19.2ic 44.2ic 31.2c 20.9ic

20.6 45.5i 32.9i 21.0i
48.1i 34.8i 22.0i

Table 8: Diversifying search results. Underline and boldface mark the best result in a row, and per diversification method in a row, respectively. 'i' and 'c' mark statistically significant differences with the initial ranking (Init) and ClustMRF, respectively. The MRF initial list is used.

MMR and xQuAD iteratively re-rank an initial list Dinit. In each iteration the document in Dinit \ S assigned with the highest score is added to the set S; S is empty at the beginning. The final ranking is determined by the order of insertion to S.
The score MMR assigns to document d ( Dinit \ S) is sim1(Q, d)-(1-) maxdiS sim2(d, di);  is a free parameter; sim1(·, ·) and sim2(·, ·) are discussed below. In contrast to MMR, xQuAD uses information about Q's subtopics, T (Q), and assigns d with the score p(d|Q)+
(1 - ) tT (Q) p(t|Q)p(d|t) diS (1 - p(di|t)) ; p(t|Q) is the relative importance of subtopic t with respect to Q; p(d|Q) and p(d|t) are the estimates of d's relevance to Q and t, respectively.
The parameter  controls in both methods the tradeoff between using relevance estimation and applying diversification. Our focus is on improving the former and evaluating the resulting (diversification based) performance. This was also the case in previous work that used cluster ranking

for results diversification [11]. Hence, this work serves for

reference comparison below.8

We study three different estimates for sim1(Q, d) (used in MMR) which we also use for p(d|Q) (used in xQuAD).9

The first, simMRF (Q, d), is that employed in the evalua-

tion above to create the MRF initial list that is also used

here for re-ranking. (Further details are provided below.)

The next two estimates are based on applying cluster rank-

ing and transforming it to document ranking using the ap-

proach

described

in Section

4.1.

In these

cases,

1 r(d)

serves

for sim1(Q, d), where r(d) is the rank of d in the document

result list produced by using the cluster ranking method.

The first cluster ranking method is ClustMRF. The second,

QClust, was used in the work mentioned above on utilizing

cluster ranking for results diversification [11]. Specifically,

cluster C is scored by simLM (Q, C) (see Section 4.1 for de-

8There is work on using information induced from clusters for the diverisification itself (e.g., [21]). Using ClustMRF for cluster ranking in these approaches is future work. 9For scale compatibility, the two resultant quantities that are interpolated (using ) in MMR and xQuAD are sum normalized with respect to all documents in Dinit before the interpolation is performed.

341

tails of simLM (·, ·)); C is represented by the concatenation

of its documents.

We use MMR and xQuAD to re-rank the MRF initial

list that contains 50 documents. simLM (·, ·) serves for the

sim2(·, ·) measure used in MMR and for p(d|t) that is used in

xQuAD. The official TREC subtopics, which are available

for the ClueWeb settings that we use here, were used for

experiments. Following the findings in [29], we set p(t|Q) d=ef

|T

1 (Q)|

.

The value of  is selected from {0.1, 0.2, . . . , 0.9}

using cross validation; -NDCG (@20) is the optimization

metric. In addition to -NDCG (@20), ERR-IA (@20) and

P-IA (@20) are used for evaluation.

Table 8 presents the results. We see that using the MRF

similarity measure in MMR and xQuAD outperforms the ini-

tial ranking, which was created using this measure, in most

relevant comparisons. This attests to the diversification ef-

fectiveness of MMR and xQuAD. Using QClust outperforms

the initial ranking in most cases, but is consistently out-

performed by using the MRF measure and our ClustMRF

method. More generally, the best performance for each di-

versification method (MMR and xQuAD) is almost always

attained by ClustMRF, which often outperforms the other

methods in a substantial and statistically significant man-

ner. Thus, although ClustMRF ranks clusters of similar

documents, using the resultant document ranking can help

to much improve results-diversification performance.

5. CONCLUSIONS
We presented a novel approach to ranking (query specific) document clusters by their presumed relevance to the query. Our approach uses Markov Random Fields that enable the integration of various types of cluster-relevance evidence. Empirical evaluation demonstrated the effectiveness of using our approach to re-rank different initially retrieved lists. The approach also substantially outperforms state-of-the-art cluster ranking methods and can be used to substantially improve the performance of results diversification methods.

6. ACKNOWLEDGMENTS
We thank the reviewers for their comments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center.

7. REFERENCES
[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey, X. Li, M. D. Smucker, and C. Wade. UMASS at TREC 2004 -- novelty and hard. In Proc. of TREC-13, 2004.
[2] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. INQUERY and TREC-9. In Proc. of TREC-9, 2000.
[3] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased ranking of web documents. In Proc. of WSDM, pages 95­104, 2011.
[4] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In Proc. of WWW, pages 107­117, 1998.
[5] J. G. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR, pages 335­336, 1998.
[6] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Informaltiom Retrieval Journal, 14(5):441­465, 2011.
[7] W. B. Croft. A model of cluster searching based on classification. Information Systems, 5:189­195, 1980.
[8] D. R. Cutting, D. R. Karger, J. O. Pedersen, and J. W. Tukey. Scatter/Gather: A cluster-based approach to browsing large document collections. In Proc. of SIGIR, pages 318­329, 1992.

[9] D. Fetterly, M. Manasse, and M. Najork. Spam, damn spam, and statistics: Using statistical analysis to locate spam web pages. In Proc. of WebDB, pages 1­6, 2004.
[10] N. Fuhr, M. Lechtenfeld, B. Stein, and T. Gollub. The optimum clustering framework: implementing the cluster hypothesis. Information Retrieval Journal, 15(2):93­115, 2012.
[11] J. He, E. Meij, and M. de Rijke. Result diversification based on query-specific cluster ranking. JASIST, 62(3):550­571, 2011.
[12] T. Joachims. Training linear svms in linear time. In Proc. of KDD, pages 217­226, 2006.
[13] O. Kurland. Re-ranking search results using language models of query-specific clusters. Journal of Information Retrieval, 12(4):437­460, August 2009.
[14] O. Kurland and C. Domshlak. A rank-aggregation approach to searching for optimal query-specific clusters. In Proc. of SIGIR, pages 547­554, 2008.
[15] O. Kurland and E. Krikon. The opposite of smoothing: A language model approach to ranking query-specific document clusters. Journal of Artificial Intelligence Research (JAIR), 41:367­395, 2011.
[16] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. In Proc. of SIGIR, pages 194­201, 2004.
[17] O. Kurland and L. Lee. PageRank without hyperlinks: Structural re-ranking using links induced by language models. In Proc. of SIGIR, pages 306­313, 2005.
[18] O. Kurland and L. Lee. Respect my authority! HITS without hyperlinks utilizing cluster-based language models. In Proc. of SIGIR, pages 83­90, 2006.
[19] O. Kurland, F. Raiber, and A. Shtok. Query-performance prediction and cluster ranking: Two sides of the same coin. In Proc. of CIKM, pages 2459­2462, 2012.
[20] K.-S. Lee, Y.-C. Park, and K.-S. Choi. Re-ranking model based on document clusters. Inf. Process. Manage., 37(1):1­14, 2001.
[21] T. Leelanupab, G. Zuccon, and J. M. Jose. When two is better than one: A study of ranking paradigms and their integrations for subtopic retrieval. In Proc. of AIRS, pages 162­172, 2010.
[22] A. Leuski. Evaluating document clustering for interactive information retrieval. In Proc. of CIKM, pages 33­40, 2001.
[23] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.
[24] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proc. of SIGIR, pages 186­193, 2004.
[25] X. Liu and W. B. Croft. Experiments on retrieval of optimal clusters. Technical Report IR-478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts, 2006.
[26] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In Proc. of ECIR, pages 454­462, 2008.
[27] D. Metzler. A feature-centric view of information retrieval. Springer, 2011.
[28] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proc. of SIGIR, pages 472­479, 2005.
[29] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proc. of WWW, pages 881­890, 2010.
[30] J. Seo and W. B. Croft. Geometric representations for multiple documents. In Proc. of SIGIR, pages 251­258, 2010.
[31] J. G. Shanahan, J. Bennett, D. A. Evans, D. A. Hull, and J. Montgomery. Clairvoyance Corporation experiments in the TREC 2003. High accuracy retrieval from documents (HARD) track. In Proc. of TREC-12, pages 152­160, 2003.
[32] A. Tombros, R. Villa, and C. van Rijsbergen. The effectiveness of query-specific hierarchic clustering in information retrieval. Inf. Process. Manage., 38(4):559­582, 2002.
[33] C. J. van Rijsbergen. Information Retrieval. Butterworths, second edition, 1979.
[34] E. M. Voorhees. The cluster hypothesis revisited. In Proc. of SIGIR, pages 188­196, 1985.
[35] P. Willett. Query specific automatic document classification. International Forum on Information and Documentation, 10(2):28­32, 1985.
[36] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support vector method for optimizing average precision. In Proc. of SIGIR, pages 271­278, 2007.
[37] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. of SIGIR, pages 334­342, 2001.

342

A Novel TF-IDF Weighting Scheme for Effective Ranking
Jiaul H. Paik
Indian Statistical Institute, Kolkata, India
jia.paik@gmail.com

ABSTRACT
Term weighting schemes are central to the study of information retrieval systems. This article proposes a novel TF-IDF term weighting scheme that employs two different within document term frequency normalizations to capture two different aspects of term saliency. One component of the term frequency is effective for short queries, while the other performs better on long queries. The final weight is then measured by taking a weighted combination of these components, which is determined on the basis of the length of the corresponding query.
Experiments conducted on a large number of TREC news and web collections demonstrate that the proposed scheme almost always outperforms five state of the art retrieval models with remarkable significance and consistency. The experimental results also show that the proposed model achieves significantly better precision than the existing models.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Retrieval : Retrieval Models
General Terms
Algorithm, Experimentation, Performance
Keywords
Document ranking, Retrieval model, Term weighting
1. INTRODUCTION
Term weighting schemes are the central part of an information retrieval system. Effectiveness of IR systems are thus crucially dependent on the underlying term weighting mechanism. Almost all retrieval models integrate three major variables to determine the degree of importance of a term for a document: (i) within document term frequency, (ii) document length and (iii) the specificity of the term in the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

collection. Term frequency and document length combination is used to infer the saliency of a term in a document, and when a query contains more than one term, term specificity is used to reward the documents that contain the terms rare in the collection.
Retrieval models can be broadly classified into two major families based on their term weight estimation principle. Vector space model casts queries and documents as finite dimensional vectors, where the weight of an individual component is computed using numerous variations of tf-idf scores. On the other hand, probabilistic models [16, 17] primarily focus on estimating the probabilities of the terms in the documents, and the estimation techniques differ from one approach to the other. But in essence all of them use the same basic principles that we have outlined before.
Most of the existing models (possibly all) employ a single term frequency normalization mechanism that does not take into account various aspects of a term's saliency in a document. For example, frequency of a term in a document relative to the frequency of the other terms in the same document gives us an important clue that can not be achieved by the commonly used document length based normalization scheme. On the contrary, length based normalization can restrict the likelihood of retrieval of extremely long documents which can not be taken care of by the relative frequency based term weighting.
Another major limitation of the present models is that they do not balance well in preferring short and long documents. Such limitation makes a system to retrieve low quality documents at the top of the ranked list when they face queries of varying length. For example, in pivoted document length normalization scheme, if the parameter is set to a smaller value, it performs better for shorter queries, and when the parameter value is larger, longer queries are benefited more than the shorter queries [10]. Similar observation can be made for other models such as BM25, language model or relatively recent divergence from randomness based models [13, 10].
The main reason is that when the parameter is set to a static value, most of the models prefer either short documents or long documents. If a weighting scheme prefers long documents, it pulls up extremely long documents when longer queries are encountered, since the longer documents have higher verbosity level it matches more query terms[28]. On the other direction, preference of short documents may degrade the overall retrieval performance, since it violates the likelihood of relevance versus retrieval pattern suggested by Singhal et al. [28].

343

This article proposes a term weighting scheme that can address these weaknesses in an effective way. In particular, we argue that the two aspects of term frequencies, when combined appropriately, leads to significant performance benefit. In this article we make the following contributions.
· It introduces a two aspect term frequency normalization scheme, that combines relative tf weighting and the tf normalization based on document length. One component of the term frequency tends to prefer long documents, while the other component prefers short documents and therefore, it maintains a good balance in preferring short and long documents.
· It uses the query length information to emphasize the appropriate component. In particular, when the system faces a long query, it down-weights the part that prefers long documents in order to compensate the effect and vice versa.
· It modifies the usual term discrimination measure (idf) by integrating mean term frequency of a term in the set of documents the term is contained in.
· Finally, we use asymptotically bounded function (similar to Robertson and Walker [22]) to transform the tf factors that better handles the term coverage issues in the documents and also helps to combine the two tf factors more easily. As a bi-product of such transformation, the ranking function easily produces the similarity values in the range of [0-1].
In order to assess the effectiveness of the proposed model we carry out a set of experiments on a large number of standard test collections containing news and web data. The experimental results show that the proposed weighting function consistently and significantly outperforms five state of the art retrieval models (from vector space as well as probabilities families) measured in terms of the standard metrics such as MAP and NDCG. The experimental outcomes also attest that the model achieves significantly better precision than all the other models when measured in terms of a recently popularized metric, namely, expected reciprocal rank (ERR) [6].
The remainder of the article is organized as follows. In Section 2 we review the state of the art. Section 3 describes the proposed weighing scheme. Description about the test collections, evaluation metrics and the details of the baselines are given in Section 4. Experimental results are presented in Section 5, where we compare the performance of the proposed model with the state of the art vector space models, followed by the comparison with the probabilistic models. Finally, we conclude in Section 6.
2. STATE OF THE ART
Information retrieval systems, when encounter a query, tries to rank documents by their likelihood of relevance. Most IR systems assign a numeric score to the documents and then they are ranked based on these scores. Three widely used models in IR are the vector space model [26, 25], the probabilistic models [21] and the inference network based model [30]. In this section we review some of the state of the art models.
In vector space model, queries and documents are represented as the vector of terms. To compute a score between a

document and a query the model measures the similarity be-

tween the query and document vector using cosine function.

The central part of the vector space model is to determine

the weight of the terms that are present in the query and

the documents. Three main factors that come into play to

compute the weight of a term are: (i) frequency of the term

in the document, (ii) document frequency of the term in the

collection (first proposed in [29]) and (iii) the length of the

document that contains the term. Fang et al. [10] give a

comprehensive analysis of four retrieval models by defining

a set of constraints that needs to be satisfied for effective

retrieval. Using these constraints the strengths and weak-

nesses of some well known models are analyzed and some

of the models are modified. There are also a number of re-

cent works that focus on the constraint based analysis of the

retrieval models [8, 9].

Salton and Buckley [24] summarize a number of term

weighting approaches which use various types of normalization. It is evident that document length is an important

component in effective term weighting. Singhal et al. [28]

identify a number of weaknesses of cosine and maximum tf

normalization and they observe that a weighting formula

that retrieves documents with chances similar to their probability of relevance performs better. Following this observation, they propose a pivoted normalization scheme that

acts as a correction factor of old normalization and is one

of the most effective term weighting schemes in the vector

space framework. The pivoted length normalization scheme

computes the term weight as follows [27]:

X
tQD

1

+

ln(1 + ln(T F (t, D)))

1

-

s

+

s

len(D) ADL(C)

·

T F (t, Q)

·

ln N + 1 df (t)

(1)

The parameter s controls the extent of normalization with respect to the document. Typically, the term weighting functions in vector space model are designed heuristically, which are based on the researchers experience. Several work tried to use the data to learn the patterns that satisfy the data. For example, Greiff [11] uses exploratory data analysis to uncover some important relationship between the document frequency and the relevance of a document.
The key part of the probabilistic models is to estimate the probability of relevance of the documents for a query. This is where most probabilistic model differs from one another. Binary independence model is perhaps the most widely accepted technique in the classical probabilistic model. A number of weighting formula have been developed and BM25 [20] has been the most effective among the formulae. The major differences between BM25 and the other commonly used TFIDF models are the slightly variant IDF formulation and the use of the query term frequency. The length normalization factor uses the average document length and a parameter has been introduced to control the relative length effect.
Probabilistic language modeling technique [19, 14] is another effective ranking model that is widely used today. Typically, language modeling approaches compute the probability of generating a query from a document, assuming that the query terms are chosen independently. Unlike TFIDF models, language modeling approaches do not explicitly use document length factor and the idf component. It seems that the length of the document is an integral part of this formula and that automatically takes care of the length normalization issue. However, smoothing is crucial and it has very similar effect as the parameter that controls the

344

length normalization components in pivoted normalization or BM25 model. Three major smoothing techniques (Dirichlet, Jelinek-Mercer and Two-stage) are commonly used in this model [31].
Relatively recent, another probabilistic model is proposed in [3] that computes the weight of a term by measuring the informative content of a term by computing the amount of divergence of the term frequency distribution from the distribution based on a random process. Like most of the well known models, they also use the same basic components. However, the integration of various component are derived theoretically. This family of formula also uses the average document length as an ideal length of the documents and the term frequencies are normalized with respect to the average document length.
In inference network, document retrieval is modeled as an inference process [30]. A document instantiates a term with a certain strength and given a query the credit from multiple terms is accumulated to compute a relevance that is very equivalent to the similarity score of vector space model. From an operational angle, the strength of instantiation of a term for a document can be considered as weight of the term in a document. The strength of instantiation of a term can be computed using any reasonable formula.
3. PROPOSED WORK
3.1 Preliminaries
Given a query Q and a document D, the main task of a ranking function is to assign a score to D with respect to the query Q. The main objective of a term weighting scheme is to quantify the saliency of the query terms in the document. This section describes a novel TF-IDF term weighting scheme that serves above purpose. En-route to the development, we are guided by a number of hypotheses that are commonplace in quantifying the importance of a term. Thus, before we give the main motivation behind our work, let us first revisit the three key hypotheses.
1. Term Frequency Hypothesis (TFH): The weight of term in a document should increase with the increase in term frequency (T F ). However, it seems unlikely that the importance of a term grows linearly with the increase in T F . Therefore, researchers have used dampened TF instead of the raw TF for ranking. The most widely used damping function has been log(T F ) and the basis of this damping can be best captured by the following advanced hypothesis.
Advanced TF Hypothesis (AD-TFH): The modified term frequency hypothesis captures the intuition that the rate of change of weight of a term should decrease with the larger TF. For example, the change in the weight caused by increasing TF from 2 to 3 should be higher than that caused by increasing TF from 25 to 26 [10]. Thus, the raw T F has to be transformed to fulfill the above goal. Formally, we hypothesize that, a function Ft(T F ), that maps the original T F to the resultant value (which will be used for final weighting), should possess the following two properties.
(a) Ft (T F ) > 0
(b) Ft (T F ) < 0

2. Document Length Hypothesis (DLH): This hypothesis captures the relationship between the term frequency and the document length. Long documents tend to use a term repeatedly, thus term frequency can be higher in a long document. Therefore, if T F is considered in isolation (disregarding the document length), long documents are given undue preference. Thus, it is necessary to regulate the T F value in accordance with the document length. The general principle is that if two documents have different lengths and the same T F values for a term t, then the contribution of T F (of t) should be higher for the shorter document.

3. Term Discrimination Hypothesis (TDH): If a query contains more than one term, then a good weighting scheme should prefer a document that contains the rare term (in the collection).

3.2 Two Aspects of TF
Most existing weighting schemes employ the above heuristics to quantify the term importance. However, they generally normalize the term frequency that captures a single aspect of the saliency of the terms and hence disregards another important aspect that we detail next. In particular, we consider the following two aspects:

1. Relative Intra-document TF (RITF) : In this factor, the importance of a term is measured by considering its frequency relative to the average T F of the document. Thus, a natural formulation for this could be

RIT F (t, D) = T F (t, D)

(2)

Avg.T F (D)

where T F (t, D) and Avg.T F (t, D) denote the frequency of the term t in D and average term frequency of D respectively. However, Equation 2 may too much prefer excessively long documents, since the denominator is close to 1 for a very long document [28]. Hence, a sublinear damping of T F seems to be a better choice over the raw T F and thus we use the following function.

RIT F (t, D) = log2(1 + T F (t, D))

(3)

log2(1 + Avg.T F (D))

Indeed, such a formula has been used by Singhal et al. [28] in the pivoted length normalization framework to normalize the tf values in accordance with the number of unique terms in the document.

2. Length Regularized TF (LRTF) : This factor nor-

malizes the term frequency by considering the number

of terms present in a document. Similar to Robert-

son's [22] notion, we assume that the appropriate length

of a document should be the average document length

of the collection and the frequency of the terms of an

average length document should remain unchanged.

Thus, a reasonable starting point could be T F (t, D) ×

ADL(C) len(D)

,

where

ADL(C)

is

the

average

document

length

of the collection and len(D) is the length of the doc-

ument D. But once again, it seems unlikely that the

increase in term frequency follows a linear relationship

with the document length, and thus the above formula

over-penalizes the long documents. To overcome this

345

bias, we employ the following function (used in [3]) to

achieve the length dependent normalization.

,,

«

LRT F (t, D) = T F (t, D) × log2

1 + ADL(C) len(D)

(4)

Equation 4 still punishes the long documents but with diminishing effect.

However, we believe that any document length normalization can be used to achieve this purpose. Some of the possible alternatives might be the length normalization component of BM25 or that of the pivoted normalization scheme.

3.2.1 Motivation
In order to motivate the use of two T F factors, let us consider the following two somewhat toy examples. We use these examples just to introduce the basic idea.
Example 1 Let D1 and D2 be two documents of equal lengths, with the following statistics.

1. len(D1) = 20, # distinct term of D1 = 5, T F (t, D1)=4
2. len(D2) = 20, # distinct term of D2 = 15, T F (t, D2)=4
In both of the cases, LRT F considers t equally important. A little thought will convince us that this is not appropriate, since the focus of the document D1 seems to be divided equally among 5 terms and therefore t should not be considered salient, while t seems to be important for D2. Thus, in the later case RIT F seems to be a better choice to LRT F .
Let us now turn to the other direction and consider the second example.

Example 2 Let D1 and D2 be two documents with the following statistics.

1. len(D1) = 20, # distinct term of D1 = 15, T F (t, D1)=4
2. len(D2) = 200, # distinct term of D2 = 150, T F (t, D2)=4
For this instance however, RIT F considers the term t equally important for both D1 and D2, which is not right, since D2 contains more distinct terms and thus seems to cover many other topics (also possibly uses t repeatedly). Therefore, in this case, the use of LRT F seems to be a potential choice over RIT F .
Motivated by the above examples, our main goal now is to integrate the above two factors into our weighting scheme. However, we do not use the T F factors as defined in the Equations 3 and 4. We transform these T F values for our final use that in some sense makes use of the hypothesis ADTFH. The next section details the transformation procedure and the underlying motivation.

3.2.2 Transforming TF Factors
Recall that the main motivation behind the advanced hypothesis on term frequency (AD-TFH) is that a good weighting function, while emphasizing on term frequencies and term discrimination factors, should also pay attention to the term coverage issue (i. e number of match). For example, if a document D1 contains a query term 10 times and another document D2 contains two query terms (of the same query)

5 times each, then the assigned score should be higher for D2

(assuming that both the query terms have equal term dis-

crimination values). That is probably the most important

reason why raw T F does not work well in practice. Second,

another common trait of many weighting schemes (for exam-

ple, pivoted normalization) is that they use the T F functions

that are not bounded above. We transform the T F factors

using a function f (x) that possesses the following proper-

ties: (i) vanishes at 0, (ii) satisfies the two conditions of

AD-TFH hypothesis (f (x) > 0 and f (x) < 0), and (iii)

asymptotically upper bounded to 1.

One of the simplest functions that satisfies the above prop-

erties is f (x) =

x 1+x

.

Indeed, similar functions have been

employed before in [22] and in [3]. Using this function, we

now transform the two T F factors as follows:

RIT F (t, D)

BRIT F (t, D) =

(5)

1 + RIT F (t, D)

BLRT F (t, D) = LRT F (t, D)

(6)

1 + LRT F (t, D)

3.2.3 Combining Two TF Factors
Now the key question that we face: how should we combine BRIT F (t, D) and BLRT F (t, D)? A natural way to do this is as follows:

T F F (t, D) = w × BRIT F (t, D) + (1 - w) × BLRT F (t, D) (7)

where 0 < w < 1. The next important issues that arise out of Equation 7 are the following:

· Should we prefer BRIT F (t, D) (w > 0.5)?

· Should we prefer BLRT F (t, D) (w < 0.5)?

In order to answer these questions, we now analyze the properties of the two TF components. From Equation 5, it is clear that BRIT F (t, D) has a tendency to prefer long documents, since for long documents the denominator part of RIT F (t, D) is close to 1, and T F is usually larger. On the other hand, BLRT F (t, D) tends to prefer short documents, since LRT F (t, D)  0 as len(D)  . Therefore, when a query is long, BRIT F (t, D) heavily prefers extremely long documents, since the number of matches is more or less proportional to the length of the document [28]. On the contrary, since BLRT F (t, D) prefers short documents it can penalize extremely long documents when it faces longer queries, and thus it is preferable when longer queries are encountered. Another interesting property of BRIT F (t, D) is that it emphasizes on the number of matches, since the main component of this formula RIT F (t, D) heavily punishes the term frequency, and thus important for the short queries. Hence, the foregoing discussion suggests that, for short queries BRIT F (t, D) should be preferred, while for longer queries, BLRT F (t, D) should be given more weight
Based on the discussion given in the previous section, we now turn to incorporate the query length information into our weighting formula. The value of w should decrease with the increase in query length, while it must lie between [0-1]. Specifically, we characterize the query length factor (QLF (Q)) by the following variables. (i) QLF (Q) = 1 for |Q| = 1, (ii) QLF (Q) < 0 and (iii) 0 < QLF (Q) < 1. Numerous different functions can be constructed that satisfy the above conditions. We used the following three different

346

functions.

1

QLF1(Q) = log2(1 + |Q|)

(8)

2

QLF2(Q) = 1 + log2(1 + |Q|)

(9)

3

QLF3(Q) = 2 + log2(1 + |Q|)

(10)

The first function descends more rapidly than the second function, while the second function descends more rapidly than the third function. Our experiments suggest that function 9 performs consistently better than the other two functions on all the collections. Hence, we set w = QLF2(Q). We leave this issue for further investigation.

3.3 Term Discrimination Factor

The goal of the term discrimination factor in weighting

is to assign higher score to the documents that contain the

terms which are rare in the collection. Inverse document

frequency (IDF ) is a well known measure that serves the

above purpose. A number of IDF formulation are prevalent

in the IR literature, all of which essentially quantify the

above intuition. We use the following standard idf measure.

,,

«

IDF (t, C) = log

CS(C) + 1 DF (t, C)

(11)

The above IDF measure considers only the presence or

absence of a term in a document and does not take into ac-

count the document specific term occurrence. We hypoth-

esize that the term discrimination is a combination of the

above two factors. In particular, we hypothesize that if two

terms have equal document frequencies, then the term dis-

crimination should increase with the increase in average elite

set term frequency. The average elite set term frequency

(AEF )

is

defined

as

CT F (t,C) DF (t,C)

,

where

CT F (t, C)

denotes

the

total occurrence of the term t in the entire collection. In

fact, Kwok [18] used AEF for term weighting, but the pur-

pose was different. However, the combination of raw AEF

with IDF may disturb the overall term discrimination value,

since the IDF values are obtained by dampening through

log function. Hence, we employ a slowly increasing function

to transform the AEF values for this combination. Once

again, we use the function f (x) = x/(1 + x) to transform

the AEF values for the final use. The final term discrimi-

nation value of term t is computed as

T

DF

(t,

C)

=

IDF

(t,

C)

×

1

AEF (t, C) + AEF (t, C)

(12)

Our experiments reveal that the use of the above term discrimination has not very significant effect on the overall performance. However, it is observed that the improvements, although are small, consistent across the collections.

3.4 Final Formula
Integrating the above factors we now obtain the following final scoring formula.

X |Q|

Sim(Q, D) = T F F (qi, D) × T DF (qi, C)

(13)

i=1

Again, since T F F (qi, D) < 1, we obtain the following rela-

tionship.

X |Q|

Sim(Q, D) < T DF (qi, C)

(14)

i=1

Therefore, we can easily modify Equation 13 to get the normalized similarity scores (0 < Sim(Q, D) < 1) as follows:

|PQ| T F F (qi, D) × T DF (qi, C)

Simnorm(Q, D) = i=1

|PQ| T DF (qi, C)

i=1

(15)

Equations 13 and 15 are equivalent in the sense that they produce the same ranked lists. However, an application that requires normalized scores, Equation 15 can be used as a suitable alternative.

4. EXPERIMENTAL SETUP
In this section we describe the details of our experimental setup. First, in Section 1 we give the details of the test collections used in our experiments. In Section 4.2 and Section 4.3 we describe the evaluation measures and the baseline retrieval models respectively.
4.1 Data
Table 1 summarizes the statistics on test collections used in our experiments. The experiments are conducted on a large number of standard test collections, that vary both by type, the size of the document collections and the number of queries.
TREC 6,7,8 and ROBUST are news collections containing 528,155 documents and supplemented by 150 (queries 301-450) and 100 (601-700) queries respectively. WT10G is a web collection of moderate size supplemented by 100 queries (451-550), while GOV2 is another web collection of larger size, which is crawled from .gov domain. There are 150 (queries 701-850) queries attached with GOV2 collection which were used in TREC terabyte [4] track for three years.

Table 1: Test Collection Statistics

Name

# of docs # of queries

TREC 6,7,8 528,155

150

ROBUST

528,155

100

WT10G

1,692,096

100

GOV2

25,205,179

150

MQ-07

25,205,179

1778

MQ-08

25,205,179

784

The MQ-07 and MQ-08 set of queries are based on the Million Query Track 2007 [2] and 2008 [1] respectively. This track was designed to serve two purposes. First, it was an exploration of ad-hoc retrieval on a large collection of documents. Second, it investigated questions of system evaluation, particularly whether it is better to evaluate using many shallow judgments or fewer thorough judgments. Both million query track use GOV2 as document collection. Topics for this task were drawn from a large collection of queries that were collected by a large Internet search engine. The queries also vary by their length, with short (2-3 words) to long (6-10 words). Specifically, MQ-07 and MQ-08 collections contain 505 and 433 queries of length higher than 5

347

respectively. Therefore, the test collections provide us a diverse experimental setup for assessing the effectiveness of the proposed weighting method.
Except TREC-6,7,8, all the test collections have three scale graded relevance assessment. The grades are 0, 1 and 2- meaning non-relevant, relevant and highly relevant respectively. TREC-6,7,8 collection uses binary relevance assessment.
4.2 Evaluation Measures and IR System
All our experiments are carried out using TERRIER1 retrieval system (version 3.5). Terrier is a flexible Information retrieval system which provides the implementation of many well known models. We use title field of the topics (note that two million query data contain more than 1000 queries that contain more than 5 terms). From all the collections we removed stopwords during indexing. Documents and queries are stemmed using Porter stemmer. Statistical significance tests are done using two sided paired t-test at 95% confidence level (i,e p < 0.05).
We use the following metrics to evaluate the systems.

· Mean Average Precision (MAP): This is a standard metric for binary relevance assessment.

· Normalized DCG at k (NDCG@k) [15]: Discounted cumulative gain (DCG) is an evaluation measure that can leverage the relevance judgment in terms of multiple grades, and has an explicit position-wise discount factor. NDCG is the normalized version of DCG.

· Expected Reciprocal Rank at k (ERR@k) [6]: To relax the additive nature and the underlying independent assumption in NDCG, another evaluation measure, namely, Expected Reciprocal Rank (ERR) is proposed in [6]. It discounts the documents which are shown below very relevant documents, and is defined as the expected reciprocal length of time that a user will take to find a relevant document. ERR@k is computed as follows:

ERR@k

=

Xk

R(gi) i

iY -1 (1

-

R(gi))

i=1

j=1

(16)

where

R(g)

=

, 2g -1
2hg

hg

is the highest grade and g1, g2 . . . gk

are the relevance grades associated with the top k doc-

uments. The value of mg is 2 for all the collections,

except TREC 6,7&8 (for this mg = 1).

The first two metrics are used to reflect the overall performance of the systems, while the last evaluation measure reflects better the precision of search results, thereby making more important for the precision oriented systems. ERR has been chosen as one of the official metrics for recent TREC web tracks [7].
Note that, two million query collections (MQ-07 and MQ08) have incomplete relevance assessment. Therefore, for the sake of more reliable conclusions, we evaluate the million query sets in two different ways. First, we skip the unjudged documents from the ranked lists to compute the values of well known metrics following the recommendation made in [23]. Additionally, we also present the statistical
1http://terrier.org/

average precision2 [5] which was one of the official metrics for the million query tracks [2].
4.3 Baselines
We have compared the performance of the proposed weighting scheme with five state of the art retrieval models. Since the proposed weighting function is a TF-IDF based formula, we have taken two well known state of the art TF-IDF models. We have also chosen BM25, language model with Dirichlet smoothing (LM), and relatively recent divergence from randomness based formula (PL2) as the other state of the art baselines. The choice of our baselines are primarily motivated by [10], which provides a thorough and detailed description of all the state of the art models along with the parameter sensitivity issues.
The performances of all the baseline models are dependent on the parameters they contain. Therefore, for the sake of more reliable comparisons with the baselines, we carry out two experiments by taking first 50 judged queries from MQ07 and MQ-08 collections. We search parameters by optimizing NDCG@20. The parameters values are given in the description of the corresponding baselines. Our experiments mostly agree with the findings reported by Fang et al. [10]. In particular, we find that the performances of Pivoted TFIDF and PL2 are very sensitive with the variation of the parameters. For example, the parameter value (s = 0.2) suggested for pivoted TF-IDF in the original paper [28] gives 12% poorer MAP than that we find here by training. Similarly, the default PL2 parameter (c = 1) is 14% poorer than the one we find. Therefore, for fair comparisons, we use these optimal parameter values for the baselines. The details of the baselines are given below.
1. Pivoted length normalized TF-IDF model: This model is one of the best performing TF-IDF formula in the vector space model framework. The value of the parameter s is set to 0.05.
2. Lemur TF-IDF model: This model is another TF-IDF model that uses Robertson's tf and the standard idf . The parameter of this model is set to its default value, 0.75.
3. Classical Probabilistic model (BM25): BM25 is chosen as a state of the art representative of the classical probabilistic model. The main differences with this model and the previous model are that BM25 uses query term frequency in a different way and the idf also differs with the standard one. The parameters of this model is set to k1 = 1.2, b = 0.6 and k3 = 1000. Note that we found (on training data) slightly better results for b = 0.6 than the default 0.75.
4. Dirichlet smooth language model (LM): Language model is another probabilistic model that performs very effectively. For this model we set the value of Dirichlet smoothing (µ) to 1700.
5. Divergence from Randomness model (PL2): Finally, PL2 [12] represents the recently proposed non-parametric probabilistic model from divergence from randomness
2the code available at TREC million query page is used to compute stat AP

348

(DFR) family. Similar to the previous models, its performance also depends on a parameter value (c in the formula). We conduct experiments for this model by setting c = 13.
5. RESULTS
In this section we present the experimental results of our proposed work and compare them with the state of the art retrieval models. In Section 5.1 we compare the performance of the proposed model (MATF for Multi Aspect TF) with the two TF-IDF models, followed by the comparison with three probabilistic models- BM25, language model (LM) and PL2. We use three evaluation measures to evaluate the performance of all the methods.
5.1 Comparison with TF-IDF Models
In this section we focus on to compare the performance of the proposed model (MATF) with the Lemur TF-IDF and and Pivoted TF-IDF models. Table 2 presents the experimental results for six test collections measured in terms MAP, NDCG@20 and ERR@20.
First, we describe the results in terms of MAP. Table 2 clearly shows that MATF gains significantly better MAP than both of the TF-IDF models on two news collections. MATF performs 12% and 15.7% better than Lemur TF-IDF model on TREC-678 and ROBUST respectively. MATF is also significantly surpasses the Pivoted TF-IDF model on these collections with a margin of 8.8% and 6.3% respectively.
The behavior of MATF is similar when we see the results for two web collections, namely, WT10G and GOV2. Once again, MATF outperforms Lemur TF-IDF model by a margin of more than 20% in both of the occasions, which is clearly highly significant as confirmed by the paired t test. Like the previous two news collections, MATF maintains its superior behavior over Pivoted TF-IDF in these web collections also. In particular MATF gains more than 8% and 19% average precision than Pivoted TF-IDF for both of the collections and paired t test once again attests the significance.
We now turn to describe the results on two million query data sets. These two collections are particularly interesting, since they contain real search queries collected from a commercial search engine and also because of their variations in length. Table 2 once again demonstrates that MATF unequivocally outperforms the two TF-IDF models with significantly large margin. The MAP achieved by MATF is nearly 11% and 7% better than that achieved by Lemur TF-IDF on MQ-07 and MQ-08 collections respectively. Similarly, MATF surpasses the Pivoted TF-IDF by more than 10% margin on both of the occasions. Significance tests show that the performance differences are always statistically significant.
Among the two TF-IDF models, Lemur TF-IDF often seems to perform poorer than Pivoted (except MQ-08 where Lemur TF-IDF is nearly 4% better than pivoted). One potentially interesting outcome that we can see from Table 2 is that, when the document collection is larger MATF outperforms Pivoted TF-IDF with larger margin. In particular, MATF gains a MAP on GOV2 collection which is almost 20% better than the pivoted TF-IDF, which is a clear sign of effectiveness of MATF over the state of the TF-IDF models.

So far our discussion of experimental outcomes primarily confined on the basis of the binary relevance assessment. Note that five out of six test collections used in our evaluation have graded assessment in three scales (0,1,2). Therefore, we now turn to describe the results measured in terms of NDCG, that leverages the graded assessment.
The middle segment of Table 2 presents the results in terms of NDCG@20. It is once again clear that the performances are more or less consistent with MAP. Specifically, MATF surpasses the Lemur TF-IDF models with consistently and significantly large margin on all six collections and often the differences are higher or close to 10%, which once again clearly demonstrates the effectiveness of MATF. Performance of pivoted TF-IDF is once again very similar under the graded assessment and it achieves larger NDCG than Lemur TF-IDF except in one occasion. MATF once again is significantly better than the pivoted TF-IDF on all the collections and the differences are larger for larger web collections.
Our final comparison between the proposed model and the TF-IDF models focus on precision enhancing capabilities measured in terms of a metric ERR, that consider three things simultaneously: rank of the document, quality conveyed by the assessor assigned grade (non-relevant, relevant and highly relevant) and the quality of the documents that have been seen before the document of our focus.
The last segment of Table 2 reports the ERR@20 values achieved by the competing models on six collections. We can easily infer that MATF once again unanimously beats the two TF-IDF models. Only on WT10G, pivoted TF-IDF performs slightly better than MATF. Consistent with the previous measures, ERR@20 results demonstrate that on larger web collections the performance differences between MATF and the two TF-IDF models are larger.
Table 3: Comparison with TF-IDF models (statAP). Lemur means Lemur TF-IDF. Superscripts have their usual meaning.
Lemur pivot MATF (% improv) MQ-07 29.0 29.7 34.4lp (18.2, 15.8) MQ-08 28.4 27.6 32.5lp (14.9, 17.8)
The performances of MATF and the two TF-IDF models on two million query data, measured by statistical average precision, are shown in Table 3. MATF transcends Lemur TF-IDF by a margin of 18% and 15% on MQ-07 and MQ08 respectively, while it is better than pivoted TF-IDF with more than 15% on both of the collections.
In summary, based on the results shown in Table 2 and Table 3 we can infer that MATF outperforms two state of the art TF-IDF models with remarkable significance and consistency, and the performance differences are often noticeably large. The performance measured by three evaluation metrics unequivocally demonstrate that MATF is highly effective in ranked retrieval. Moreover, the results also show that MATF is more effective for larger web collections.
5.2 Comparison with Probabilistic Models
In the last section we compare the performance of our model with two TF-IDF models. In this section we compare the performance of MATF with three well known state of the

349

Table 2: Comparison with the TF-IDF models measured in terms of MAP, NDCG@20 and ERR@20. MATF denotes the proposed model. The best results are boldfaced. Superscripts l and p denote that the performance difference is statistically significant (p < 0.05) compared to Lemur TF-IDF and Pivot TF-IDF respectively.

Metric

Method

Lemur.TF-IDF

MAP

Pivot.TF-IDF

MATF

% better than Lemur.TF-IDF

% better than Pivot.TF-IDF

TREC-678 20.9 21.5 23.4lp 12.0 8.8

ROBUST 26.1 28.4 30.2lp 15.7 6.3

WT10G 18.4 20.5 22.2lp 20.7 8.3

GOV2 24.8 26.5 31.7lp 27.8 19.6

MQ-07 39.6 40.0 44.2lp 11.6 10.5

MQ-08 42.8 41.2 45.7lp 6.8 10.9

Lemur.TF-IDF NDCG@20 Pivot.TF-IDF
MATF % better than Lemur.TF-IDF % better than Pivot.TF-IDF

40.0 41.5 44.6lp
11.5 7.5

37.5 40.2 41.5lp
10.7 3.2

31.6 33.4 34.6l
9.5 3.6

43.8 46.8 51.0lp
16.4 9.0

46.8 48.3 51.1lp
9.2 5.8

50.1 48.7 52.6lp
5.0 8.0

ERR@20

Lemur.TF-IDF Pivot.TF-IDF MATF

40.7
41.9 43.9lp

45.7
46.3 48.5lp

34.7
37.9 37.1l

48.3
49.4 53.4lp

40.6
42.9 44.9lp

44.5
44.6 47.3lp

art probabilistic retrieval models. Our evaluation strategy is once again similar to the previous section. We compare the performances of the models under MAP, NDCG@20 and ERR@20.
First we compare the performance of MATF with the BM25 model. Table 4 shows the summary of the retrieval results on six test collections. It is clear from the table that MATF is superior to BM25 model. This result holds for all the collections and for all three evaluation measure. When the performance differences between them are measured in terms of MAP, we notice that MATF is significantly effective for news as well as web corpora compared to BM25. In fact, MATF is nearly 10% better than BM25 on two news data, while on two web collections (WT10G and GOV2), MATF achieves 17% and 12% more MAP than BM25. The differences on MQ-07 and MQ-08 are similarly significant with substantial margins. The performance differences between MATF and BM25 revealed by NDCG metric are consistent with that revealed by MAP and once again, all the differences are statistically significant. ERR@20 depicts that for all the collections, MATF remains consistently superior to BM25, which clearly confirms that MATF is very effective for precision oriented systems.
We now compare the effectiveness of MATF and language model with Dirichlet prior language model. From Table 4 we clearly see that the performance differences between MATF and LM are larger in three out of six cases than that we had observed when comparing the performance of MATF and BM25. Specifically, MATF achieves close to or more than 10% MAP than LM on four out out six instances (except WT10G). The performance measured on graded relevance assessment also demonstrates that MATF unequivocally beats the Dirichlet prior language model based approach, and the differences are substantially large. On GOV2, MQ-07 and MQ-08 data, MATF surpasses LM with a margin of 14%, 8% and nearly 9% respectively. The comparison of precision enhancing abilities of MATF and LM also clearly indicates that MATF is always better than LM, which is very concordant with the experimental findings captured by MAP and NDCG.

We now compare the performance of the proposed model with another probabilistic model from the divergence from randomness family, namely, PL2. This model is relatively recent compared to the previous two probabilistic models and was also found to be better than BM25 in the experiments reported in [3]. Table 4 reflects two major facts. First, it appears from the table that PL2 is most effective among the probabilistic models and in particular only on MQ-08 data it performs worse than BM25 as reflected by both MAP and NDCG. The second major observation that can be made from Table 4 is that MATF beats this model also with harmonious consistency and performance differences are statistically significant on TREC-678, GOV2, MQ07 and MQ-08 data. Similar to the previous outcomes, on web collections the performance differences between MATF and PL2 are larger than that for the news collections. Lastly, ERR metric depicts that MATF is better than PL2 across all six collection.
Table 5: Comparison with probabilistic models (statAP).
BM25 LM PL2 MATF (% improvement) MQ-07 30.6 29.7 30.4 34.4blp (12.8, 15.8, 13.2) MQ-08 29.6 27.6 27.4 32.5blp (10.5, 17.8, 18.6)
Table 5 compares the performance of four models for million query collections measured in terms of statistical average precision. It is once again clearly evident that MATF is consistently better than all three models and all the differences are very large and it is very consistent with the performance measured in terms other metrics presented in Table 4.
Overall, the comparative analysis clearly shows that MATF is the most effective retrieval model, which unequivocally outperforms all three probabilistic models, when the performances are measured in terms of MAP, NDCG and a precision biased metric, namely, ERR. Also, the relative per-

350

Table 4: Comparison with probabilistic models measured in terms of MAP, NDCG@20 and ERR@20. MATF denotes the proposed model. The best results are boldfaced. Superscripts b, l and p denote that the performance differences are statistically significant compared to BM25, LM and PL2 respectively.

Metric

Method

BM25

MAP

LM

PL2

MATF

% better than BM25

% better than LM

% better than PL2

TREC-678 21.3 21.3 22.7 23.4blp 9.9 9.9 3.1

ROBUST 27.7 28.4 29.5 30.2bl 9.0 6.9 2.4

WT10G 18.9 21.3 21.3 22.2b 17.5 4.2 4.2

GOV2 28.3 29.1 29.7 31.7blp 12.1 8.9 6.7

MQ-07 41.2 40.1 40.9 44.2blp 7.3 10.2 8.1

MQ-08 43.6 41.0 41.5 45.7blp 4.8 11.5 10.1

BM25 NDCG@20 LM
PL2 MATF % better than BM25 % better than LM % better than PL2

41.2 40.2 42.9 44.6blp
8.3 10.9 4.0

39.3 39.3 41.1 41.5bl
5.6 5.6 1.0

32.4 32.5 33.1 34.6bl
6.8 6.5 4.5

45.2 44.6 46.1 51.0blp
12.8 14.3 10.6

48.1 47.2 48.0 51.1blp
6.2 8.3 6.5

50.9 48.3 49.0 52.6blp
3.3 8.9 7.3

ERR@20

BM25 LM PL2 MATF

41.1 41.2 43.0 43.9blp

45.6 46.5 47.0 48.5b

34.7 35.4 35.2 37.1b

48.2 47.6 47.7 53.4blp

41.3 39.9 40.7 44.9blp

44.8 42.7 42.7 47.3blp

Table 6: Performance of two TF factors on short

and long query. The values are MAP.

short

long

MQ-07 MQ-08 MQ-07 MQ-08

LRTF 43.5 43.3 39.9 44.3

RITF 45.4 45.0 37.7 41.8

formance differences are often substantially large and the differences are even larger for the web collections that contain large number of queries. Among the three probabilistic models, PL2 and Dirichlet prior language model perform almost equally, with PL2 having a marginal edge over LM.
5.3 Analysis
In this section we analyze the effect of the two TF factors on short and long queries. For this analysis we choose the two million query collections, primarily because the collections have large number of queries. We divide the queries in two sets. The queries having at least 5 terms are denoted as short, while the rest of the queries (longer than 5 words) are treated as long. The main goal of this section is to validate the hypothesis made in the proposed section that relative intra-document based TF (RITF) performs better on short queries, while length regularized TF (RLTF) performs better on long queries.
Table 6 presents the experimental results on two million query data. The results seem to confirm our aforesaid assumption. LRTF always performs better than RITF on both of the collections, while RITF does better for short queries. However, the performance differences between the methods on longer queries are noticeably better than that for shorter queries.

6. CONCLUSION
In this paper, we present a novel TF-IDF term weighting scheme. The proposed term weighting scheme employs two aspects of within document term frequency normalization to determine the importance of a term. One component of the term frequency tends to prefer short documents, while the other tends to prefer long documents. We then combine these two TF components using the query length information, that maintains a balanced trade-off in retrieving short and long documents, when the ranking function faces queries of varying lengths.
Experiments carried out on a set of news and web collections show that the proposed model outperforms two well known state of the art TF-IDF baselines with significantly large margin, when measured in terms of MAP and NDCG. The model also surpasses three state of the art probabilistic models with remarkable significance almost always. Moreover, the proposed model is also significantly better than all of the five baselines in improving precision.
Acknowledgments
I would like to thank Dipasree Pal, Mandar Mitra and Swapan Parui for their comments, suggestions and help.
7. REFERENCES
[1] J. Allan, B. Carterette, J. A. Aslam, V. Pavlu, and E. Kanoulas. Million query track 2008 overview. In E. M. Voorhees and L. P. Buckland, editors, The Sixteenth Text REtrieval Conference Proceedings (TREC 2008). National Institute of Standards and Technology, December 2009.
[2] J. Allan, B. Carterette, B. Dachev, J. A. Aslam, V. Pavlu, and E. Kanoulas. Million query track 2007 overview. In TREC, 2007.

351

[3] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, Oct. 2002.
[4] S. Bu¨ttcher, C. L. A. Clarke, and I. Soboroff. The trec 2006 terabyte track. In TREC, 2006.
[5] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation over thousands of queries. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '08, pages 651­658, New York, NY, USA, 2008. ACM.
[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM '09, pages 621­630, New York, NY, USA, 2009. ACM.
[7] C. L. A. Clarke, N. Craswell, I. Soboroff, and E. M. Voorhees. Overview of the trec 2011 web track. In TREC, 2011.
[8] S. Clinchant and E. Gaussier. Retrieval constraints and word frequency distributions a log-logistic model for ir. Inf. Retr., 14(1):5­25, Feb. 2011.
[9] R. Cummins and C. O'Riordan. A constraint to automatically regulate document-length normalisation. In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM '12, pages 2443­2446, New York, NY, USA, 2012. ACM.
[10] H. Fang, T. Tao, and C. Zhai. Diagnostic evaluation of information retrieval models. ACM Trans. Inf. Syst., 29(2):7:1­7:42, Apr. 2011.
[11] W. R. Greiff. A theory of term weighting based on exploratory data analysis. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '98, pages 11­19, New York, NY, USA, 1998. ACM.
[12] B. He and I. Ounis. A study of the dirichlet priors for term frequency normalisation. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '05, pages 465­471, New York, NY, USA, 2005. ACM.
[13] B. He and I. Ounis. On setting the hyper-parameters of term frequency normalization for information retrieval. ACM Trans. Inf. Syst., 25(3), July 2007.
[14] D. Hiemstra, S. Robertson, and H. Zaragoza. Parsimonious language models for information retrieval. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '04, pages 178­185, New York, NY, USA, 2004. ACM.
[15] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422­446, Oct. 2002.
[16] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval: development and comparative experiments - part 1. Inf. Process. Manage., 36(6):779­808, 2000.
[17] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval:

development and comparative experiments - part 2. Inf. Process. Manage., 36(6):809­840, 2000.
[18] K. L. Kwok. A new method of weighting query terms for ad-hoc retrieval. In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '96, pages 187­195, New York, NY, USA, 1996. ACM.
[19] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '98, pages 275­281, New York, NY, USA, 1998. ACM.
[20] S. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4):333­389, Apr. 2009.
[21] S. E. Robertson. Readings in information retrieval. chapter The probability ranking principle in IR, pages 281­286. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1997.
[22] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '94, pages 232­241, New York, NY, USA, 1994. Springer-Verlag New York, Inc.
[23] T. Sakai. Alternatives to bpref. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 71­78, New York, NY, USA, 2007. ACM.
[24] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Inf. Process. Manage., 24(5):513­523, Aug. 1988.
[25] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, Inc., New York, NY, USA, 1986.
[26] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613­620, Nov. 1975.
[27] A. Singhal. Modern information retrieval: A brief overview. IEEE Data Eng. Bull., 24(4):35­43, 2001.
[28] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '96, pages 21­29, New York, NY, USA, 1996. ACM.
[29] K. Sparck Jones. Document retrieval systems. chapter A statistical interpretation of term specificity and its application in retrieval, pages 132­142. Taylor Graham Publishing, London, UK, UK, 1988.
[30] H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Trans. Inf. Syst., 9(3):187­222, July 1991.
[31] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, Apr. 2004.

352

On the Measurement of Test Collection Reliability

Julián Urbano jurbano@inf.uc3m.es

Mónica Marrero mmarrero@inf.uc3m.es

Diego Martín dmartin@dit.upm.es

University Carlos III of Madrid Department of Computer Science
Leganés, Spain

Technical University of Madrid Department of Telematics Engineering
Madrid, Spain

ABSTRACT
The reliability of a test collection is proportional to the number of queries it contains. But building a collection with many queries is expensive, so researchers have to find a balance between reliability and cost. Previous work on the measurement of test collection reliability relied on databased approaches that contemplated random what if scenarios, and provided indicators such as swap rates and Kendall tau correlations. Generalizability Theory was proposed as an alternative founded on analysis of variance that provides reliability indicators based on statistical theory. However, these reliability indicators are hard to interpret in practice, because they do not correspond to well known indicators like Kendall tau correlation. We empirically established these relationships based on data from over 40 TREC collections, thus filling the gap in the practical interpretation of Generalizability Theory. We also review the computation of these indicators, and show that they are extremely dependent on the sample of systems and queries used, so much that the required number of queries to achieve a certain level of reliability can vary in orders of magnitude. We discuss the computation of confidence intervals for these statistics, providing a much more reliable tool to measure test collection reliability. Reflecting upon all these results, we review a wealth of TREC test collections, arguing that they are possibly not as reliable as generally accepted and that the common choice of 50 queries is insufficient even for stable rankings.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation.
General Terms
Experimentation, Measurement, Reliability.
Keywords
Test Collection, Evaluation, Reliability, Generalizability Theory, TREC.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specif c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1. INTRODUCTION

The purpose of evaluating the effectiveness of an Informa-

tion Retrieval (IR) system is to assess how well it would sat-

isfy real users. The main tool used in these evaluations are

test collections, which comprise a collection of documents

to search, a set of queries Q, and a set of relevance judg-

ments that contains information as to what documents are

relevant, and to which degree, to the queries [16]. Given

the results returned by a system A for one of the queries

q  Q, an effectiveness measure uses the information in the

relevance judgments to compute a score q,A that represents

the effectiveness of the system for that query. After run-

ning the system for all queries in the collection, the average

Q,A

=

1 |Q|

i,A is usually reported as the main measure

of system effectiveness, representing the expected behavior

of the system for an arbitrary new query. When comparing

two systems A and B, the main measure reported is the av-

erage effectiveness difference Q,AB = Q,A - Q,B. Based

on this difference, we conclude which system is better.

The immediate question to ask is: how reliable are those

conclusions about system effectiveness? Ideally, researchers

would evaluate the system with the set of all possible queries

that a user might request. In such a case, we could be sure

that the true average performance of the system corresponds

to the score we computed with the collection. The prob-

lem is that building such a collection is either impractical

for requiring an enormous amount of queries and relevance

judgments, or just impossible if the potential query set is

not defined, which use to be the case because we can not

account for future queries that do not yet exist. Therefore,

the query set Q in a test collection must be regarded as

a sample from the universe of all queries, and the sample

mean Q,A as an estimate of the true effectiveness mean A.

But because we are estimating this score with a sample of

queries, our estimates are erroneous to some degree. The

results may change drastically with a different query set Q,

so much that differences between systems could be reversed.

An evaluation result is reliable if it can be replicated with

another collection: if the set of queries Q suggests that sys-

tem A outperforms system B, we can be very sure that the conclusion would hold for a different set of queries Q, and in

the end, for the universe of all queries. A simple way to make

a collection reliable is to include many queries; the more we

employ the smaller the variance of the estimates and thus

the more reliable the conclusion. The problem is that more

queries also means more cost to create the collection, so re-

searchers have to find a balance between the reliability of

the results and the cost of the collection. To this end, it is

necessary to develop indicators of test collection reliability.

393

Several works in the last fifteen years have studied the problem of reliability in IR evaluation experiments. The basic methodology consisted in evaluating a series of systems with two different and random sets of queries, computing several reliability indicators that measured how similar those evaluations were. Using different query sample sizes and randomizing query selection, researchers were able to map query set size to reliability and extrapolate results to larger query sets. The data used consisted in runs submitted to several TREC tracks (mostly the Ad Hoc tracks), and the sets of queries employed in each edition. While these approaches are clearly faithful to the data, they are limited in that the full query set had to be partitioned in two disjoint sets to comply with the assumption that they were independent.
In 2007 Bodoff and Li [6] proposed Generalizability Theory (GT) as an alternative [7, 18]. GT is grounded on analysis of variance components, which allows to dissect the variability in effectiveness scores and figure out how much of it is due to system differences, query difficulty, assessors, etc. In an ideal evaluation setting, we would like all variance to be due to actual differences between systems and not due to query variability; if the queries in the collection are too varied, or differences between systems too small, then we need many queries to ensure that our estimates are reliable. From these variance components GT allows researchers to estimate the reliability of a test collection even before it has been created. Based on some previous data, GT can estimate the reliability of a collection with a larger number of queries, more than one assessor providing judgments for each query, etc. GT provides indicators for the stability of both the absolute scores and the relative differences by computing different variance ratios.
The main advantages of GT against the traditional databased approaches are that 1) it is based on statistical theory, 2) it is easy to employ because it does not require tedious and repetitive what if scenarios, and 3) it allows us to estimate the reliability of a collection or experimental design that does not exist yet. But it has disadvantages too: 1) it is unknown the extent to which reliability indicators are affected by the data used to estimate variance components, and 2) it is very hard to interpret them in practical terms.
We address these two problems of GT applied to the measurement of test collection reliability. In the next section we review past work following data-based approaches and the reliability indicators used. We then review the use of GT and discuss the motivation for this work. In Section 3 we show how the initial data used in GT studies has a very large effect on the results, discussing minimum sample sizes and interval estimators. Section 4 reports a study to provide an empirical mapping between GT-based indicators of reliability and the well known data-based ones. Next we discuss the reliability of several TREC collections based on the results from previous sections, presenting conclusions in Section 6.
2. INDICATORS OF RELIABILITY
Several indicators of test collection reliability have been proposed in the literature. This section reviews traditional indicators found in the early data-based studies and the GTbased indicators more recently proposed.
2.1 Data-based Indicators
Given a query set Q and a similar set Q of the same size, we can define the following data-based reliability indicators:

· Kendall correlation ( ), compares the order in which systems are ranked according to Q and Q, regardless of the magnitude of the differences AB. It ranges from 1 (same rankings) to -1 (reversed rankings), counting the number of system pairs that are swapped between the two rankings. For Q to be reliable,  must therefore tend to 1.
· AP correlation (AP ), adds a top-heaviness component to Kendall  , such that swaps between systems towards the top of the rankings are more penalized than swaps towards the bottom [23].
· Power ratio (), is the fraction of pairwise system differences that result statistically significant according to query set Q. If the difference Q,AB between two systems is deemed as statistically significant, it serves as further evidence that the true difference AB has the same sign. For Q to be reliable,  must therefore tend to 100%. In this paper we compute standard 2-tailed t-tests at the 0.05 level [19].
· Minor Conflict ratio (-), is the fraction of statistically significant differences with Q that have a sign swap with Q but are not statistically significant there. - is therefore the fraction of uncertain conclusions when measuring statistical significance, so for Q to be reliable - must therefore tend to 0%.
· Major Conflict ratio (+), is the fraction of statistically significant differences with Q that are also significant with Q but have a sign swap. + is therefore the fraction of incorrect conclusions when measuring statistical significance, so for Q to be reliable + must therefore tend to 0% as well.
· Absolute Sensitivity (a), is the minimum absolute difference Q,AB that need be observed between any two systems such that the differences with Q have the same sign at least 95% of the times. For Q to be reliable, a must therefore tend to 0, meaning that even small differences can be trusted.
· Relative Sensitivity (r), is the minimum relative difference Q,AB/ max Q,A, Q,B that need be observed with Q such that the differences with Q have the same sign at least 95% of the times. For Q to be reliable, r must therefore tend to 0% too.
· Root Mean Squared Error (), measures the difference between the absolute scores with Q and with Q. Thus, for Q to be reliable  must tend to 0 too.
One of the first reliability studies was conducted in 1998 by Voorhees [20], who analyzed the effect of having different assessors provide relevance judgments. Employing a methodology based on randomization, she concluded that the absolute scores could suffer wide variations between assessors, but that the ranking of systems was seldom altered, establishing  = 0.9 as the de facto minimum on ranking similarity. She also studied swap rates as a function of  and suggested a minimum of 25 queries to have a somewhat stable ranking. Also in 1998, Zobel [24] studied the effect of pool depth on absolute system scores, extrapolating trends to larger pool depths. He also compared different statistical procedures in terms of power and conflict ratios.
Buckley and Voorhees [8] compared in 2000 the reliability of various effectiveness measures by mapping effectiveness differences to error rates. Extrapolating to 50 queries, they concluded that   0.05 produced less than 1.5% system swaps when computing Average Precision (AP), while

394

other measures such as Precision at cutoff 10 (P@10) produced 3.6% of swaps. In 2002, Voorhees and Buckley [22] extended their work with other collections and methods, but again extrapolating trends. They concluded that with 50 queries the sensitivity of AP was a = 0.05, while increasing the query set size to 100 would yield a = 0.03. They also reported large differences across collections and effectiveness measures. Lin and Hauptmann [13] showed that the empirical model used by Voorhees and Buckley can be derived theoretically, and that the three factors affecting reliability are query set size, mean effectiveness scores, and variability of scores. Sanderson and Zobel [17] also revisited this work by computing relative sensitivity and incorporating statistical procedures to account for score variability. They concluded r = 10% with AP if coupled with statistical significance, and r = 25% if not. They observed very similar relative sensitivity between AP and P@10, arguing the use of more queries with fewer judgments as previous work suggested that much of the score variability is due to queries [4].
In 2007 Sakai [15] used similar methods to compare the reliability of several effectiveness measures, though he did not extrapolate to larger query sets. He computed  correlations, absolute sensitivity a and a variation of r, and observed that these indicators were not very correlated with statistical significance, arguing the importance of considering score variability rather than just means. Voorhees revisited in 2009 [21] the use of statistical procedures with the TREC Robust 2004 collection, computing reliability indicators with an unprecedented set of 100 queries, therefore avoiding the need to extrapolate to the usual size 50. When using AP, she observed power  = 47% and conflict ratios - = 2.7% and + = 0.04%. She showed again that P@10 is less reliable than AP also in these terms; and that nDCG showed higher reliability (agreeing with Sakai [15]). She also found that minor conflicts were usually coupled with large relative differences, thus suggesting that researchers employ several large collections to draw general conclusions.

2.2 GT-based Indicators
Bodoff and Li [6] proposed Generalizability Theory [7, 18] as an alternative to measure test collection reliability that directly addresses variability of scores rather than just the mean as was common before. GT has two stages: a Generalizability study (G-study) to estimate variance components based on previous data, and a Decision study (D-study) that subsequently computes reliability indicators for a different experimental design. We consider a fully crossed design and decompose variability of scores into three components: variance due to actual differences among systems (s2), variance due to differences in difficulty among queries (q2), and variance due to the system-query interaction effect whereby some systems are particularly good (or bad) for some queries (s2:q). The variance due to other effects, such as assessors, is in our case confounded with the interaction effect.
Using Analysis of Variance (ANOVA) procedures, these variance components can be estimated from previous data:

^s2:q = ^e2 = EMresidual

(1)

^s2

=

EMs - nq

^e2

(2)

^q2

=

EMq - ^e2 ns

(3)

where EM is the expected Mean Square of component , and ns and nq are the number of systems and queries [7, 18]. These estimates can be used to compute the proportion of total variance that is due to each of the effects, such as how much of it is due to actual differences between systems.
In the D-study, we can use the variance estimates from the G-study to compute the reliability of a larger query set. To this end, two reliability indicators are usually employed:

· Generalizability Coefficient (E2), is the ratio of system variance to itself plus relative error variance:

E2

nq

=

s2

s2

+

2 e
n

(4)

q

and it provides a measure of the stability of relative differences between systems . By extension, it measures the reliability of the ranking. For a collection to be reliable, E2 must therefore tend to 1.
· Index of Dependability (), is the ratio of system variance to itself plus absolute error variance:

 nq

=

s2

s2
+ 2+2 qe nq

(5)

and it provides a measure of the stability of absolute effectiveness scores . For a collection to be reliable,  must therefore tend to 1 as well.

The main advantage of these indicators is that they allow us to estimate the reliability of an arbitrary query set size nq, so there is no need to follow the traditional methodologies based on random what if scenarios and extrapolation. From equations (4) and (5) it can be seen that the reliability of the collection increases as nq increases, because the estimates of query difficulty (i.e. average system performance per query) are more precise. These indicators were used by Kanoulas and Aslam [12] to derive the gain and discount functions of nDCG that yield optimal reliability when nq is constant.
With simple algebraic manipulation, we can calculate the minimum number of queries needed to reach some level of relative or absolute stability :

nE2 () =

 · e2 s2 (1 - )

(6)

n () =

 q2 + e2 s2 (1 - )

(7)

which can be used to estimate how many more queries we need to add to our collection for it to be reliable. The main use of this approach can be found in the TREC Million Query Track [2, 1], which set out to study whether many queries with a few judgments yield more reliable results than a few queries with many judgments. The conclusion was that nq  80 queries are sufficient for a reliable ranking, while nq  130 are needed for reliable absolute scores.

2.3 Motivation
The two problems of GT can be clearly spotted at this point. First, equations (1) to (3) show that we do not compute the true 2 variance components, but just estimates ^2 based on some previous data. If we use a different, yet similar set of systems or queries to estimate these variance components, the resulting E^2 and ^ scores might be very

395

Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8 Web adhoc 8 Web adhoc 9 Web adhoc 2001 Web adhoc 2009 Web adhoc 2010 Web adhoc 2011 Web distillation 2002 Web distillation 2003 Web distillation 2004 Web diversity 2009 Web diversity 2010 Web diversity 2011 Novelty 2002 Novelty 2003 Novelty 2004 Genomics 2003 Genomics 2004 Genomics 2005 Robust 2003 Robust 2004 Robust 2005 Terabyte 2004 Terabyte 2005 Terabyte 2006 Terabyte all 2006 Enterprise 2005 Enterprise 2006 Enterprise 2007 Enterprise 2008 1MQ MTC 2007 1MQ MTC 2008 1MQ MTC 2009 1MQ statAP 2007 1MQ statAP 2008 1MQ statAP 2009 Medical 2011 Microblog 2011

Span covering 95% of ^ observations 0.0 0.2 0.4 0.6 0.8 1.0

Span covering 95% of E^2 observations 0.0 0.2 0.4 0.6 0.8 1.0

Variability due to queries

20

40

60

80

100

Initial number of queries in the G-study

20

40

60

80

100

Initial number of queries in the G-study

Span covering 95% of ^ observations 0.0 0.2 0.4 0.6 0.8 1.0

Span covering 95% of E^2 observations 0.0 0.2 0.4 0.6 0.8 1.0

Variability due to systems

20

40

60

80

100

Initial number of systems in the G-study

20

40

60

80

100

Initial number of systems in the G-study

Figure 1: Variability in E^2 (top) and ^ (bottom) scores as a function of the initial number of queries (left) and number of systems (right) used in the G-study to estimate variance components.

different. In a revised paper, Bodoff [5, §4.6] briefly discussed this issue and argued that differences are marginal. However, he reports the results when randomly selecting only one system per research group instead of all of them, and only one trial of such experiment. We argue that this situation is not representative because the full set of systems and the reduced set after removing runs by the same groups are actually very similar to begin with, so it is expected that reliability scores do not change much. Also, only one such randomly reduced set is compared, so there is really no evidence to support that claim. Likewise, he further suggests that as few as five queries or systems are often enough to provide stable estimates of the variance components in the G-study [5, §3.1]. We further analyze this issue in Section 3.
Second, equations (6) and (7) allow us to estimate the minimum number of queries nq to reach some stability level , but the greater question is: how much is stable enough? Bodoff [5] mentions that in most Social Science applications a stability coefficient of 0.8 is acceptable, but there is no similar standard for Engineering applications. Kanoulas and Aslam [12] set  = 0.95 as the target in their experiments, but this choice is arbitrary. In their analysis of the Million Query Track 2007 [2] and 2008 [1], Allan et al. [1] and Carterette et al. [9, 10] also set E2 = 0.95 as the target. They mention in a footnote that in their experiments E2 = 0.95 approximately corresponded to  = 0.9, but details are omitted. We study this issue in Section 4 by empirically mapping GT-based indicators onto data-based indicators that are easier to understand and use in practice.
3. VARIABILITY OF GT INDICATORS
To measure the effect of the number of queries and number of systems used in the G-study to estimate variance components, we use data from 43 TREC collections covering 12 tasks across 10 tracks, from TREC 3 to TREC 2011 (see Table 1). As in previous studies [22, 17, 6, 21], we remove

the bottom 25% of systems so that our results are not obscured by possibly buggy implementations. For each collection, we randomly selected nq = 5 queries and computed the variance components using the full set of systems. We then calculated E2 and  for the full query set size, and the required number of queries to reach 0.95 stability. This was repeated with increments in nq of 5 queries, up the maximum permitted by the collection or 100. For each query set size, we ran 200 random trials, each of which can be considered as the possible data available for a G-study when analyzing a test collection design. The same process was followed by varying the initial number of systems ns and using the full set of queries instead.
Figure 1 shows the variability in G-study results1. For each collection and initial number of queries used, the y-axis plots the length of the span covering 95% of the E^2 and ^ observations in the 200 random trials. The right hand side plots show the same span lengths, but for different number of systems used in the G-study. As expected, the queries have a larger effect. Most importantly, we see that the average span length with just 5 queries is about 0.5 across collections. That is, the stability estimates could be as low as 0.3 or as high 0.8, for example, just depending on the particular set of queries we use in the G-study. In fact, estimates of the minimum number of queries required can vary in orders of magnitude if not using enough data. For example, with as many as 30 initial queries and all 184 systems from the Microblog 2011 collection, GT may suggest from 63 to 133 queries to reach E2 = 0.95. Similarly, from 40 initial systems and all 34 queries from the Medical 2011 collection, GT may suggest from 109 to 566 queries. In general, at least 50 queries and 50 systems seem necessary for 95% of estimates to be within a 0.1 span. This means that GT may be trusted to measure the reliability of an existing collection, but that
1Given the amount of datapoints displayed in this paper, we recommend to access the full-color version available online.

396

L - nq

1,

U - nq

1

, where

(8)

L

=

Ms Me F:dfs ,dfe

U

=

Ms Me F1-:dfs ,dfe

nsL nsL +

nq

,

nsU nsU +

nq

, where

(9)

L

=

Ms2

- F:dfs,MsMe + (F:dfs, - F:dfs,dfe ) F,dfs,dfe Me2 (ns - 1)F:dfs,MsMe + F:dfs,dfq MsMq

U

=

Ms2

- F1-:dfs,MsMe + (F1-:dfs, - F1-:dfs,dfe ) F1-,dfs,dfe Me2 (ns - 1)F1-:dfs,MsMe + F1-:dfs,dfq MsMq

researchers should be cautious when planning a collection
based on the results of a handful of systems and queries.
These results clearly evidence the need for a measure of
confidence on GT indicators. Bodoff [5] suggests the use of
confidence intervals to account for this variability, but only
computes them for the variance components in the G-study.
Confidence intervals for the ultimately more useful D-study
can be worked out from various variance ratios (see equations (8) and (9)2). Feldt [11] derived exact 100(1 - 2)% confidence intervals for the ratio  = s2/e2 under the assumption of normally distributed scores. The confidence interval on E2(nq) is computed using the endpoints in (8):

E2

nq

=

nq  1 + nq

(10)

Arteaga et al. [3] derived approximate 100(1 - 2)% confidence intervals for the ratio  = s2/ s2 + q2 + e2 , again
assuming a normal distribution of scores. The confidence interval on  nq is computed using the endpoints in (9):



nq

= 1+

nq  nq - 1 

(11)

Brennan [7, §6] discusses different methods to compute confidence intervals in both G-studies and D-studies, showing that the above intervals work reasonably well even when the normality assumption is violated. The right hand side of Table 1 reports the point and 95% interval estimates of the stability of the 43 TREC collections we consider in this paper. These intervals provide a more suitable estimate of test collection reliability because they account for variability in the G-study. For example, researchers could use these intervals to infer the required number of queries to reach the lower endpoint of the interval instead of the point estimate:

nE2 () =

  (1 - )

(12)

n () =

 (1 - )  (1 - )

(13)

4. INTERPRETING GT INDICATORS
To empirically derive a mapping between GT-based and data-based reliability indicators, we again used the 43 TREC collections in Table 1. For each collection we proceeded as follows. Two random and disjoint query subsets of size nq = 10 were selected from the full set of queries; let these subsets be Q and Q. The full set of systems was evaluated with both query subsets, and all data-based reliability indicators in Section 2.1 were computed, along with the two GTbased indicators according to Q and Q. This was repeated
2F:df1,df2 is the quantile function of the F distribution with df1 and df2 degrees of freedom. In our fully crossed design, dfs = ns - 1, dfq = nq - 1, and dfe = (ns - 1)(nq - 1).

with increments in nq of 10 queries, up to the maximum permitted by the collection. For query subset size we ran
50 random trials, each trial providing us with 32 datapoints (E^2 and ^ according to Q and to Q, mapped to ^,^AP , ^, ^-, ^+, ^a, ^r and ^). Theoretically though, E2 is better related to  , AP , , -, + and a because it measures the stability of relative differences, while  is better related to
r and  because it measures the stability of absolute scores.
We thus mapped only these combinations.
Figure 2 shows the mappings. For each collection we fitted
a model with all available datapoints. However, we dropped points for which E^2 < 0.8 and ^ < 0.5 so that the trends
were not affected by mappings with such small stability to
be even practical. These thresholds were chosen based on
the observed stability of the 43 TREC collections; about
85% of them show larger stability scores (see Table 1). This
resulted in over 28,000 points for each plot. In the top three plots ( , AP and ) we fitted the model y = xa, where a is
the parameter to fit. This resulted in the desired theoretical
behavior that limx1 y = 1 and limx0 y = 0, that is, when all variability is due to system differences  should be 1
because the ranking cannot be altered, and if all variance is
due to queries then  should be 0 because the rankings are
completely random. Similarly, in the bottom four plots we fitted the model y = (1 - x)a, such that limx1 y = 0 and limx0 y = 1, that is,  should for example be 0 if there is no variability due to queries.
As the first plot shows, all 43 collections do actually need E2 > 0.95 to reach  = 0.9. In general, E2 = 0.95 corresponds to   0.85, and on average E2  0.97 is needed
across collections to reach  = 0.9. The two clear exceptions
are found in the Million Query Track. The 2008 collection
is the one that reaches the target  = 0.9 with the lowest stability (E2  0.93), while the 2007 collection needs the largest (E2  0.98). Note that these were the two collections for which the E2 = 0.95   = 0.9 correspondence
was established [1, 9, 10]. It should be noted here that these
fits have an exponential-like shape, meaning that it is hard to achieve a mid level of  , but once E2 is large enough
small improvements in stability translate into large improvements in  . However, the relation between nq and E2 has a logarithmic-like shape, meaning that it is increasingly more expensive to improve E2 to begin with. Thus, it should be
considered the required effort for slight improvements in  .
The second plot shows quite high AP scores at these levels of relative stability, but generally below  . This suggests
that the swaps in the rankings are still happening between
systems at the top of the rankings [23]. The third plot shows
that at these stability levels it is expected to observe statis-
tical significance in about 80% of system comparisons. In
the middle right plot we can see that the proportion of con-
flicting results is generally below the  = 0.05 significance level when E2  0.9.

397

Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8 Web adhoc 8 Web adhoc 9 Web adhoc 2001 Web adhoc 2009 Web adhoc 2010 Web adhoc 2011 Web distillation 2002 Web distillation 2003 Web distillation 2004 Web diversity 2009 Web diversity 2010 Web diversity 2011 Novelty 2002 Novelty 2003 Novelty 2004 Genomics 2003 Genomics 2004 Genomics 2005 Robust 2003 Robust 2004 Robust 2005 Terabyte 2004 Terabyte 2005 Terabyte 2006 Terabyte all 2006 Enterprise 2005 Enterprise 2006 Enterprise 2007 Enterprise 2008 1MQ MTC 2007 1MQ MTC 2008 1MQ MTC 2009 1MQ statAP 2007 1MQ statAP 2008 1MQ statAP 2009 Medical 2011 Microblog 2011



0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Kendall correlation

AP correlation

Power ratio

1.0

1.0

0.9

0.9

0.8

0.8

0.7

0.7



AP

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.80

0.85

0.90

0.95

1.00

E2

Absolute Sensitivity

0.80

0.85

0.90

0.95

1.00

E2

0.80

0.85

0.90

0.95

1.00

E2

Minor Conflict ratio

0.20

0.12

0.15

0.09

0.10

-

0.06

a

0.05

0.03

0.00

0.00

0.80

0.85

0.90

0.95

1.00

0.80

0.85

0.90

0.95

1.00

E2

E2

Relative Sensitivity

RMS Error

0.15

0.10



r 0.0 0.1 0.2 0.3 0.4 0.5 0.6

0.05

0.00

0.5

0.6

0.7

0.8

0.9

1.0

0.5

0.6

0.7

0.8

0.9

1.0





Figure 2: Mapping from GT-based to data-based reliability indicators on a per-collection basis.

Researchers interested in the particular mapping for one of these collections may use the estimates in Table 1 and the plots in Figure 2 to get a better understanding of the evaluation results and draw more informed conclusions. To assess the reliability of future collections and guide in their development process, we fitted a single model using all available data instead of one model per collection. Figure 3 shows these fits, along with 95% and 90% prediction intervals that theoretically cover 95% and 90% of all future observations. In terms of sensitivity, the middle left plots show that a  0.03 for E2  0.9, which is about 60% of what Voorhees and Buckley reported for the Ad Hoc tracks [22]; although the intervals cover their values well. In the bottom

left plot we see that r  20% for   0.75, generally agreeing with Sanderson and Zobel [17]. As to statistical significance, we replicated Voorhees's [21] study with random sets of 50 queries from the Ad Hoc 7-8 topics and Robust 2004 systems. The average relative stability is E^2  [0.81, 0.88], which corresponds to   [37%, 54%], -  [3.9%, 7.8%] and +  [0.38%, 1.3%]. These are again larger than she reported, but the intervals cover her values well.
Overall, these models produce a decent fit on the data, and they fill the gap between data-based methodologies and Generalizability Theory. They provide a valuable tool to rapidly assess and easily understand the reliability of a test collection design.

398

Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8 Web adhoc 8 Web adhoc 9 Web adhoc 2001 Web adhoc 2009 Web adhoc 2010 Web adhoc 2011 Web distillation 2002 Web distillation 2003 Web distillation 2004 Web diversity 2009 Web diversity 2010 Web diversity 2011 Novelty 2002 Novelty 2003 Novelty 2004 Genomics 2003 Genomics 2004 Genomics 2005 Robust 2003 Robust 2004 Robust 2005 Terabyte 2004 Terabyte 2005 Terabyte 2006 Terabyte all 2006 Enterprise 2005 Enterprise 2006 Enterprise 2007 Enterprise 2008 1MQ MTC 2007 1MQ MTC 2008 1MQ MTC 2009 1MQ statAP 2007 1MQ statAP 2008 1MQ statAP 2009 Medical 2011 Microblog 2011



0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Kendall correlation

AP correlation

Power ratio

1.0

1.0

0.9

0.9

0.8

0.8

0.7

0.7



AP

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.80

0.85

0.90

0.95

1.00

E2

Absolute Sensitivity

0.80

0.85

0.90

0.95

1.00

E2

0.80

0.85

0.90

0.95

1.00

E2

Minor Conflict ratio

0.20

0.12

0.15

0.09

0.10

-

0.06

a

0.05

0.03

0.00

0.00

0.80

0.85

0.90

0.95

1.00

0.80

0.85

0.90

0.95

1.00

E2

E2

Relative Sensitivity

RMS Error

0.15

0.10



r 0.0 0.1 0.2 0.3 0.4 0.5 0.6

0.05

0.00

0.5

0.6

0.7

0.8

0.9

1.0

0.5

0.6

0.7

0.8

0.9

1.0





Figure 3: General mapping from GT-based to data-based reliability indicators, with 95% (dark grey) and 90% (light grey) prediction intervals.

5. DISCUSSION
The last columns in Table 1 report point and 95% interval estimates of the stability of the 43 TREC collections we considered. Collections in the same group correspond to the same tasks, providing a historical perspective on the reliability of the collections used so far since 1994 and for a variety of tasks. For example, the average relative stability in the Ad Hoc collections was E2  [0.86, 0.93], which according to Figure 3 corresponds to   [0.65, 0.81]. For the Web Ad Hoc collections we find E2  [0.8, 0.93], which would correspond to   [0.53, 0.81]. There are large differences within some tasks, such as Web Distillation, Genomics, Terabyte

and Enterprise. This is further evidence of the variability in D-study results due to the data used in the G-study. Except for a few particular cases though, the computation of confidence intervals smooths the problem. Across collections the averages are E2 = 0.88 and  = 0.74, with some tasks having very low scores. According to Figure 3 the expected  correlation is 0.69 with variations from 0.49 to 0.95, that is, much lower than desired.
Figure 4 plots the historical trend of test collection reliability. The left plot shows that relative stability has varied in the (0.8,1) interval for the most part, but most importantly it suggests that the stability of collections has decreased very

399

Track Documents

Query Set

Measure ns nq

Ad Hoc 3 Disks 1 & 2

151-200

AP 40 50

Ad Hoc 4 Disks 2 & 3

201-250

AP 33 49

Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8

Disks 2 & 4 Disks 4 & 5 Disks 4 & 5 Disks 4 & 5

251-300 301-350  351-400  401-450 

AP 94 50 AP 74 50 AP 103 50 AP 129 50

WebAdHoc 8 WebAdHoc 9 WebAdHoc 2001 WebAdHoc 2009 WebAdHoc 2010 WebAdHoc 2011
WebDistillation 2002 WebDistillation 2003 WebDistillation 2004
WebDiversity 2009 WebDiversity 2010 WebDiversity 2011
Novelty 2002

WT2g WT10g WT10g ClueWeb09 ClueWeb09 ClueWeb09
.GOV .GOV .GOV
ClueWeb09 ClueWeb09 ClueWeb09
Disks 4 & 5

401-450 

AP 44 50

451-500

AP 104 50

501-550

AP 97 50

"W1-W50"  AP (MTC) 71 50

"W51-W100" 

AP 56 48

"W101-W150" 

AP 37 50

551-600

AP 71 49

TD1-TD50

AP 93 50

"WT04"

AP 74 75

"W1-W50"  -nDCG@20 48 50

"W51-W100"  -nDCG@20 32 50

"W101-W150"  -nDCG@20 25 50

50 from 300-450 

F 42 49

Novelty 2003 AQUAINT

N1-N50

F 55 50

Novelty 2004 AQUAINT

N51-N100

F 60 50

GenomicsAdHoc 2003 GenomicsAdHoc 2004 GenomicsAdHoc 2005
Robust 2003
Robust 2004
Robust 2005

MEDLINE MEDLINE MEDLINE
Disks 4 & 5 Disks 4 & 5 AQUAINT

"G1-G50"
"G51-G100"
"G101-150" 50 from 301-450 & 601-650 
301-450 & 601-700  50 from 301-700 

AP 49 50 AP 43 50 AP 62 49
AP 78 100 AP 110 249 AP 74 50

Terabyte 2004
Terabyte 2005
Terabyte 2006
TerabyteAll 2006 EnterpriseExpert 2005 EnterpriseExpert 2006 EnterpriseExpert 2007 EnterpriseExpert 2008

GOV2 GOV2 GOV2 GOV2
W3C W3C CERC CERC

701-750  751-800  801-850  701-850 
EX01-EX50 EX51-EX105 CE001-CE050 CE051-CE127

bpref 70 49 bpref 58 50 bpref 80 50 bpref 61 149
AP 37 50 AP 91 49 AP 55 50 AP 42 55

1MQ 2007

GOV2

"MQ1-MQ10000" AP (MTC) 29 1692

1MQ 2008

GOV2

"MQ10001-MQ20000" AP (MTC) 25 784

1MQ 2009 ClueWeb09

"MQ20001-MQ60000" AP (MTC) 35 542

1MQ 2007

GOV2

"MQ1-MQ10000"

statAP 29 1153

1MQ 2008

GOV2

"MQ10001-MQ20000"

statAP 25 564

1MQ 2009 ClueWeb09

"MQ20001-MQ60000"

statAP 35 475

Medical 2011

NLP

"M101-M135"

bpref 127 34

Microblog 2011 Tweets2011

MB1-MB50

P@30 184 49

E^2 (nq ) 0.933 0.893-0.963 0.907 0.847-0.952 0.856 0.804-0.9 0.898 0.855-0.933 0.919 0.891-0.943 0.908 0.88-0.932
0.929 0.89-0.96 0.876 0.833-0.912 0.862 0.813-0.904 0.81 0.729-0.876 0.829 0.746-0.895 0.804 0.685-0.895
0.901 0.858-0.935 0.45 0.249-0.619 0.89 0.844-0.927
0.903 0.852-0.943 0.882 0.803-0.94 0.844 0.725-0.929
0.919 0.873-0.955 0.966 0.949-0.979 0.801 0.708-0.876
0.94 0.909-0.965 0.903 0.848-0.945 0.77 0.664-0.855
0.846 0.784-0.897 0.95 0.934-0.964 0.864 0.807-0.911
0.953 0.933-0.97 0.875 0.815-0.923 0.762 0.668-0.841 0.94 0.913-0.962
0.916 0.864-0.955 0.965 0.952-0.976 0.884 0.827-0.929 0.565 0.315-0.757
0.999 0.999-1 0.998 0.996-0.999 0.96 0.936-0.979 0.992 0.986-0.996 0.978 0.962-0.99 0.96 0.935-0.979
0.774 0.704-0.835
0.92 0.899-0.938

^ (nq) 0.786 0.661-0.88 0.79 0.658-0.89 0.62 0.488-0.732 0.806 0.714-0.875 0.799 0.71-0.864 0.701 0.59-0.787
0.83 0.728-0.904 0.76 0.662-0.835 0.711 0.598-0.801 0.619 0.473-0.744 0.662 0.513-0.787 0.702 0.537-0.835
0.84 0.762-0.898 0.315 0.144-0.492 0.747 0.643-0.832
0.847 0.759-0.911 0.804 0.676-0.899 0.719 0.535-0.865
0.792 0.671-0.883 0.944 0.91-0.967 0.181 0.1-0.301
0.87 0.792-0.925 0.768 0.64-0.868 0.422 0.269-0.586
0.509 0.384-0.636 0.824 0.768-0.872 0.693 0.564-0.797
0.877 0.809-0.924 0.648 0.501-0.774 0.427 0.283-0.575 0.719 0.617-0.812
0.824 0.713-0.905 0.939 0.909-0.96 0.785 0.674-0.87 0.28 0.11-0.498
0.998 0.997-0.999 0.988 0.979-0.995 0.908 0.854-0.951 0.982 0.97-0.991 0.969 0.946-0.986 0.929 0.886-0.963
0.497 0.348-0.628
0.818 0.747-0.869

Table 1: Summary of all 43 TREC collections analyzed. Query sets with  are used in more than one collection. Query numbers in quotes are not official, but arbitrarily named for this paper. The last two columns report the point and 95% interval estimates of the GT-based reliability indicators.

slightly with the years. The clear exceptions are again the Million Query Track collections, which specifically aimed at increasing the number of queries. Within each task it appears that stability tended to decrease as the tasks got older despite that query set sizes were normally unaltered. The second plot shows that this decrease in stability could be due to system variance getting smaller with the years. That is, systems perform more similarly as the tasks get older, indicating that retrieval techniques are generally improved. The right plot shows that query difficulty also varied within tasks. Sudden peaks may be explained by changes in the document set or in the task definition. The general trend suggests that queries are getting more alike with the years, further contributing to the decrease in reliability.
Bodoff [5, §5] discusses the incorporation of the document set as another facet in Generalizability Theory, much like queries and systems, to measure variability due to documents [14]. He argues that it does not make sense in general, because we do no assign performance scores for indi-

vidual documents but for sets of documents (e.g. the first k retrieved when computing P @k). In our case we could compare different editions of the same task but with different document sets to get a (weak) clue of the variability due to documents. For example, the Ad Hoc task of the Web Track shows quite different stability scores in the first three editions (WT2g and WT10g collections) compared to the last three editions (ClueWeb09), given that they all used the standard query set size of 50. Similarly, the Expert Search task in the Enterprise Track shows very different stability levels when using the W3C collection or the CERC collection. We must bear in mind though that these differences might actually be due to the systems and queries used, which varied from year to year.
From the confidence intervals in Table 1, we used the models fitted in Section 4 to provide in Table 2 the estimated data-based reliability scores for all 43 collections. It is evident that expected  correlations are well below the desired 0.9 in most cases. In that line, some collections are clearly

400

Track
Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8
WebAdHoc 8 WebAdHoc 9 WebAdHoc 2001 WebAdHoc 2009 WebAdHoc 2010 WebAdHoc 2011 WebDistillation 2002 WebDistillation 2003 WebDistillation 2004 WebDiversity 2009 WebDiversity 2010 WebDiversity 2011 Novelty 2002 Novelty 2003 Novelty 2004
GenomicsAdHoc 2003 GenomicsAdHoc 2004 GenomicsAdHoc 2005
Robust 2003 Robust 2004 Robust 2005
Terabyte 2004 Terabyte 2005 Terabyte 2006 TerabyteAll 2006 EnterpriseExpert 2005 EnterpriseExpert 2006 EnterpriseExpert 2007 EnterpriseExpert 2008
1MQ 2007 1MQ 2008 1MQ 2009 1MQ 2007 1MQ 2008 1MQ 2009
Medical 2011
Microblog 2011

^
0.725-0.898 0.622-0.87
0.537-0.741 0.641-0.821
0.72-0.846 0.695-0.819
0.718-0.89 0.595-0.77 0.554-0.749 0.406-0.686 0.434-0.729 0.34-0.728
0.647-0.827 0.019-0.255 0.617-0.807
0.633-0.847 0.535-0.839 0.401-0.811
0.679-0.877 0.86-0.941
0.374-0.685
0.762-0.903 0.624-0.852 0.311-0.641
0.5-0.734 0.823-0.902 0.544-0.766
0.82-0.916 0.558-0.795
0.316-0.61 0.772-0.897
0.661-0.877 0.868-0.932 0.582-0.812 0.037-0.453
0.997-0.999 0.989-0.997 0.827-0.942 0.962-0.989 0.896-0.972 0.826-0.941
0.368-0.598
0.74-0.833

^AP
0.637-0.86 0.515-0.823 0.418-0.657 0.537-0.758 0.631-0.791
0.6-0.756
0.629-0.849 0.484-0.694 0.437-0.668
0.283-0.59 0.311-0.643 0.221-0.642
0.544-0.766 0.004-0.148 0.508-0.741
0.528-0.792 0.416-0.782 0.278-0.746
0.582-0.833 0.81-0.919
0.252-0.589
0.684-0.867 0.517-0.799 0.195-0.537
0.379-0.649 0.761-0.865 0.426-0.689
0.758-0.884 0.442-0.725
0.2-0.5 0.696-0.859
0.56-0.831 0.821-0.907 0.468-0.746
0.01-0.33
0.995-0.999 0.985-0.996
0.767-0.92 0.947-0.984 0.858-0.961 0.765-0.919
0.246-0.486
0.656-0.774

^ (%)
58-83 45-79 35-60 47-72 58-76 54-72
57-82 42-65 37-62 22-53 25-59 16-59
48-73 0-10
44-70
46-76 35-74 22-70
52-80 78-90 19-53
63-84 45-76 14-47
31-60 72-84 36-64
72-86 38-68 14-44 65-83
50-80 79-89 40-70
0-26
99-100 98-100
73-90 94-98 83-95 73-90
19-42
60-74

^- (%)
0.6-3.2 0.9-5.6 2.9-8.2 1.6-5.2 1.2-3.3 1.6-3.9
0.7-3.4 2.4-6.4 2.8-7.7 4.1-13.5 3.1-12.2 3.2-17
1.5-5 22.8-64.4
1.8-5.8
1.2-5.4 1.3-8.3 1.7-13.8
0.9-4.2 0.3-1.1 4.1-15.1
0.6-2.5 1.2-5.6 5.2-18.8
3.1-9.6 0.6-1.6
2.5-8
0.5-1.6 2-7.5
6-18.5 0.7-2.4
0.9-4.7 0.3-1
1.7-6.8 11.4-56
0-0 0-0 0.3-1.5 0-0.1 0.1-0.7 0.3-1.5
6.3-15.5
1.4-3

^+ (%)
0.02-0.28 0.03-0.72 0.23-1.38 0.08-0.62 0.05-0.29 0.08-0.38
0.02-0.3 0.17-0.9 0.21-1.22 0.41-3.24 0.27-2.73 0.27-4.81
0.07-0.59 7.89-47.06
0.1-0.76
0.05-0.66 0.06-1.4
0.09-3.35
0.03-0.44 0-0.04
0.42-3.93
0.02-0.18 0.05-0.71 0.62-5.69
0.25-1.78 0.02-0.08 0.17-1.32
0.01-0.08 0.12-1.19
0.8-5.52 0.02-0.16
0.03-0.52 0.01-0.03
0.09-1 2.41-37.02
0-0 0-0 0-0.07 0-0 0-0.02 0-0.08
0.88-4.08
0.07-0.24

^a
0.01-0.03 0.01-0.06 0.03-0.08 0.02-0.05 0.01-0.03 0.02-0.04
0.01-0.03 0.02-0.06 0.03-0.08 0.04-0.13 0.03-0.12 0.03-0.17
0.01-0.05 0.23-0.64 0.02-0.06
0.01-0.05 0.01-0.08 0.02-0.14
0.01-0.04 0-0.01
0.04-0.15
0.01-0.02 0.01-0.05 0.05-0.19
0.03-0.09 0.01-0.02 0.02-0.08
0-0.02 0.02-0.07 0.06-0.18 0.01-0.02
0.01-0.05 0-0.01
0.02-0.07 0.11-0.56
0-0 0-0 0-0.01 0-0 0-0.01 0-0.01
0.06-0.15
0.01-0.03

^r (%)
6-25 6-25 18-42 7-20 8-20 13-31
5-18 10-24 12-31 17-44 13-39 10-37
5-16 41-82 10-26
4-16 5-23 7-37
6-24 1-4
63-87
3-13 7-27 32-67
27-53 7-15
13-34
4-12 15-41 33-65 11-29
5-20 2-4
7-23 41-86
0-0 0-1 2-8 0-1 0-2 1-6
28-57
7-17

^
0.001-0.029 0.001-0.03
0.013-0.112 0.001-0.017 0.001-0.017 0.006-0.054
0-0.014 0.003-0.028 0.005-0.051 0.011-0.122 0.006-0.095
0.003-0.08
0.001-0.009 0.108-0.6
0.003-0.034
0-0.009 0.001-0.025 0.001-0.081
0.001-0.026 0-0
0.309-0.709
0-0.006 0.001-0.035 0.055-0.358
0.036-0.204 0.001-0.008 0.005-0.066
0-0.004 0.008-0.103
0.06-0.336 0.004-0.043
0-0.017 0-0
0.001-0.025 0.104-0.683
0-0 0-0 0-0.002 0-0 0-0 0-0.001
0.039-0.246
0.001-0.011

n^E2(.95) 37-114 47-169
106-233 69-161 58-117 69-130
40-118 92-190 102-220 135-354 107-311 112-438
65-154 585-2862
112-264
58-166 61-234 73-360
44-136 21-52
135-392
35-95 56-171 158-472
218-525 175-336
94-227
30-68 80-217 181-474 111-269
46-149 24-48
73-200 335-2277
11-38 16-59 219-710 88-304 107-421 194-628
129-273
62-105

n^(.95)
130-487 116-484 348-999 136-381 150-389 257-662
102-355 189-484 236-640 327-1058 247-868 188-819
106-292 980-5631
288-791
93-301 107-457 149-826
124-457 33-94
2203-8579
78-250 146-536 657-2528
1087-3043 693-1428 242-733
77-220 279-947 702-2406 657-1761
100-383 39-93
143-459 1053-8458
30-104 81-313 534-1756 196-685 156-616 352-1156
383-1208
141-315

Table 2: Predicted reliability of all 43 TREC collections analyzed. All confidence intervals are based on the fits from Figure 3 at the endpoints of the 95% confidence intervals computed with equations (10) and (11).

not reliable, such as the Web Distillation 2003, Genomics Ad Hoc 2005, Terabyte 2006, Enterprise Expert Search 2008, or the very recent Medical 2011 and Web Ad Hoc 2011. Regarding the expected RMS Error of absolute scores, we can see that collections are somewhat stable, but with clear exceptions such as Web Distillation 2003, Novelty 2004 and Enterprise Expert Search 2008.
The last two columns in Table 2 report intervals on the number of queries, as per equations (12) and (13), required to achieve 0.95 stability. In general the number of queries needs to be at least doubled, and in many cases a few hundred queries seem to be needed. This is particularly interesting for the most recent collections, such as Web Ad Hoc 2010 and 2011, Medical 2011 and Microblog 2011, which stick to the traditional size of 50 queries but need about 200. What becomes clear from these figures is that the ideal size of a collection depends greatly on the task it will be used for, and thus it is not appropriate to fix some acceptable size such as 50 or 100 throughout tasks. Each task has different characteristics and should be analyzed accordingly.

6. CONCLUSIONS
In this paper we discussed the measurement of test collection reliability from the perspective of traditional databased methodologies and of Generalizability Theory. GT is regarded as a more appropriate, easy to use, and powerful method to assess reliability, but it has two drawbacks. First, we showed that GT is very sensitive to the particular sample of systems and queries used to estimate reliability of a larger query set. We showed that about 50 systems and 50 queries are needed for robust estimates of collection reliability. Therefore, researchers should be cautious in using GT when building new collections from scratch. To account for all this variability we discussed a more robust approach based on interval estimates of the stability indicators, which helps in making more appropriate decisions regarding number of queries or different structure in the experimental design. Second, we empirically established a mapping between GT-based and traditional data-based indicators to help interpreting results from GT which, otherwise, do not have a

401

Ad Hoc Web adhoc Web distillation Web diversity Novelty Genomics Robust Terabyte Enterprise 1MQ MTC 1MQ statAP Medical Microblog
Linear trend

E^2 0.5 0.6 0.7 0.8 0.9 1.0

Relative Stability

1995

2000

Year

2005

2010

^s2 (% of total)

0

5 10 15 20 25

Variability due to Systems

1995

2000

Year

2005

2010

^2q (% of total) 30 40 50 60 70 80 90

Variability due to Queries

1995

2000

Year

2005

2010

Figure 4: Historical trend of relative stability (left), variability due to systems (middle) and to queries (right).

clear and easily understandable meaning. Based on these results, we reviewed the reliability of 43 TREC test collections, evidencing that some of them are very little reliable. We show that the traditional choice of 50 queries is clearly not enough even for stable rankings, and in most cases a couple hundred queries are needed. Our results also show that the ideal query set size varies significantly across tasks, suggesting that we avoid the use of some fixed size such as 50 or 100 and that we analyze tasks and collections separately.
There are two clear lines for future research. First, we completely ignored the assessor facet in our study. It is evident that different assessors provide different results, so it would be interesting to include them in the analysis. Second, although we fitted the theoretically correct models, it is clear that they can be improved (see for instance Power and RMS Error in Figure 3). IR evaluation experiments generally violate assumptions of GT, such as normality of distributions and random sampling, so different models and features to better fit the actual data should be investigated.
We created some scripts for the statistical software R that can help researchers perform all these computations to easily assess the reliability of custom test collection designs. They can be downloaded from http://julian-urbano.info.
7. REFERENCES
[1] J. Allan, J. A. Aslam, B. Carterette, V. Pavlu, and E. Kanoulas. Million Query Track 2008 Overview. In Text REtrieval Conference, 2008.
[2] J. Allan, B. Carterette, J. A. Aslam, V. Pavlu, B. Dachev, and E. Kanoulas. Million Query Track 2007 Overview. In Text REtrieval Conference, 2007.
[3] C. Arteaga, S. Jeyaratnam, and G. A. Franklin. Confidence Intervals for Proportions of Total Variance in the Two-Way Cross Component of Variance Model. Communications in Statistics: Theory and Methods, 11(15):1643­1658, 1982.
[4] D. Banks, P. Over, and N.-F. Zhang. Blind Men and Elephants: Six Approaches to TREC data. Information Retrieval, 1(1-2):7­34, 1999.
[5] D. Bodoff. Test Theory for Evaluating Reliability of IR Test Collections. Information Processing and Management, 44(3):1117­1145, 2008.
[6] D. Bodoff and P. Li. Test Theory for Assessing IR Test Collections. In ACM SIGIR, pages 367­374, 2007.
[7] R. L. Brennan. Generalizability Theory. Springer, 2001.
[8] C. Buckley and E. M. Voorhees. Evaluating Evaluation Measure Stability. In ACM SIGIR, pages 33­34, 2000.

[9] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation Over Thousands of Queries. In ACM SIGIR, pages 651­658, 2008.
[10] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. If I Had a Million Queries. In ECIR, pages 288­300, 2009.
[11] L. S. Feldt. The Approximate Sampling Distribution of Kuder-Richardson Reliability Coefficient Twenty. Psychometrika, 30(3):357­370, 1965.
[12] E. Kanoulas and J. A. Aslam. Empirical Justification of the Gain and Discount Function for nDCG. In ACM CIKM, pages 611­620, 2009.
[13] W.-H. Lin and A. Hauptmann. Revisiting the Effect of Topic Set Size on Retrieval Error. In ACM SIGIR, pages 637­638, 2005.
[14] S. Robertson and E. Kanoulas. On Per-Topic Variance in IR Evaluation. In ACM SIGIR, pages 891­900, 2012.
[15] T. Sakai. On the Reliability of Information Retrieval Metrics Based on Graded Relevance. Information Processing and Management, 43(2):531­548, 2007.
[16] M. Sanderson. Test Collection Based Evaluation of Information Retrieval Systems. Foundations and Trends in Information Retrieval, 4(4):247­375, 2010.
[17] M. Sanderson and J. Zobel. Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability. In ACM SIGIR, pages 162­169, 2005.
[18] R. J. Shavelson and N. M. Webb. Generalizability Theory: A Primer. Sage Publications, 1991.
[19] J. Urbano, M. Marrero, and D. Mart´in. A Comparison of the Optimality of Statistical Significance Tests for Information Retrieval Evaluation. In ACM SIGIR, 2013.
[20] E. M. Voorhees. Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness. In ACM SIGIR, pages 315­323, 1998.
[21] E. M. Voorhees. Topic Set Size Redux. In ACM SIGIR, pages 806­807, 2009.
[22] E. M. Voorhees and C. Buckley. The Effect of Topic Set Size on Retrieval Experiment Error. In ACM SIGIR, pages 316­323, 2002.
[23] E. Yilmaz, J. A. Aslam, and S. Robertson. A New Rank Correlation Coefficient for Information Retrieval. In ACM SIGIR, pages 587­594, 2008.
[24] J. Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In ACM SIGIR, pages 307­314, 1998.

402

Deciding on an Adjustment for Multiplicity in IR Experiments

Leonid Boytsov
Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA
leo@boytsov.info

Anna Belova
Abt Associates Inc. Bethesda, MD, USA
anna@belova.org

Peter Westfall
Texas Tech University Lubbock, TX, USA
peter.westfall@ttu.edu

ABSTRACT
We evaluate statistical inference procedures for small-scale IR experiments that involve multiple comparisons against the baseline. These procedures adjust for multiple comparisons by ensuring that the probability of observing at least one false positive in the experiment is below a given threshold. We use only publicly available test collections and make our software available for download. In particular, we employ the TREC runs and runs constructed from the Microsoft learning-to-rank (MSLR) data set. Our focus is on non-parametric statistical procedures that include the HolmBonferroni adjustment of the permutation test p-values, the MaxT permutation test, and the permutation-based closed testing. In TREC-based simulations, these procedures retain from 66% to 92% of individually significant results (i.e., those obtained without taking other comparisons into account). Similar retention rates are observed in the MSLR simulations. For the largest evaluated query set size (i.e., 6400), procedures that adjust for multiplicity find at most 5% fewer true differences compared to unadjusted tests. At the same time, unadjusted tests produce many more false positives.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance Evaluation
General Terms
Experimentation
Keywords
Statistical significance, multiple comparisons, t-test, MaxT, permutation test, randomization test, Holm-Bonferroni.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1. INTRODUCTION
1.1 Description of the Problem
Multiple comparisons/testing is a fundamental experimental issue that arises when a certain hypothesis is being repeatedly tested in different settings. For example, a researcher proposes a new retrieval algorithm and verifies its effectiveness against a baseline. In reality, this method is equivalent to the baseline, but, after exhaustive testing with different collections and parameter settings, he observes a statistically significant improvement, which has happened by chance. Most false positives arising from multiple testing can be eliminated by considering a family of tests as a whole and requiring stronger evidence, i.e., smaller p-values, in each test. This approach is commonly referred to as an adjustment for multiple comparisons (testing).
The multiple comparisons issue received a lot of attention in a bio-medical research. In clinical trials, the cost of making a wrong conclusion is high. Thus, the US Food and Drug Administration strongly recommends to employ adjustments for multiple comparisons and requires a justification if multiplicity adjustments are not performed [1]. In contrast, in IR experiments, multiplicity issues are rarely taken into account. Yet, there is a non-negligible cost related to (1) human effort in reproducing experimental results, (2) computational effort related to aggregating results from several retrieval methods. These efforts are wasted on methods whose improvement over the baseline was observed due to spurious, i.e., random effects. This is why we believe that the IR community should also adopt the practice of reporting corrected p-values.
How do we define a family of tests where p-values should be adjusted for multiple comparisons? It turns out that the choice of the family is very subjective [6, 33]. Westfall and Young state that
. . . there can be no universal agreement: statisticians have argued back and forth (sometimes vehemently) over this issue, often arriving at dramatically different conclusions [33].
They note, however, that there is more agreement on adjusting p-values in a single experiment. This is especially pertinent when results are summarized in a single conclusion [2]. For example, the researcher may compare 10 methods against a baseline, adjust p-values, and state that only 3 differences are jointly significant.
In our work we adopt this point of view and focus on adjustments that provide a strong control of a family-wise

403

error rate (FWER) at a significance level . In other words, the probability of observing a false positive among all tests is at most . We also limit our attention to the case when a small number of methods are compared against a single baseline. This is a common scenario in the TREC setting, where a group submits 2-3 official runs that are evaluated by TREC organizers. Additionally, the group may evaluate several unofficial runs on their own (using relevance judgements produced by TREC assessors). There are several other approaches to deal with multiple testing that provide a weaker control: e.g., limiting the probability to observe at most k > 1 false positives [15, 35] or controlling a false discovery rate (FDR) [3]. We believe that these methods are less useful for the purpose of discovering and publishing significant results, but they may be appealing to practitioners, e.g., those who seek to reduce dimensionality of machine learning models [37, 21].
1.2 Related Work
There are several papers covering a wide range of reliability issues in IR experiments [22, 5, 31, 25, 38]. We encourage the reader to follow these articles and references therein.
Wilbur [34] carried out the first comprehensive assessment of methods for testing statistical significance in IR. He used several pre-TREC collections and evaluated the Wilcoxon test, the sign test, the permutation test (also known as the randomization test), and several modifications of bootstrapping. According to Wilbur, the permutation test and the bootstrapping test had comparable statistical power, superior to that of the Wilcoxon and the sign test. These findings were confirmed by Smucker et al. [27] who conducted similar experiments using several much larger TREC collections. The experiments of Cormack and Lynam [10], though, showed that both the Wilcoxon and the sign test were sufficiently accurate and powerful, but somewhat inferior to the t-test. In addition, they discovered that there was a strong agreement among the t-test, the bootstrapping test, and the permutation test. Savoy [23] recommended to use bootstrapping to estimate the sample median instead of the sample mean.
There are also several papers focusing on multiple testing adjustments in IR experiments. Tague-Sutcliffe and Blustein carried out a statistical analysis of TREC-3 results [29] and adjusted them using the Scheff´e's method [24]. They found that only large differences in performance metrics could be considered significant. Blanco and Zaragoza [4] presented an experimental analysis of spurious effects in IR and advocated for adoption of multiple comparisons adjustments. Carterette [7] modeled randomness with a linear regression and adjusted p-values for multiplicity using a singlestep method that relied on multivariate Student distribution. He found that in TREC-8 relative pairwise differences in the mean average precision smaller than about 50% were insignificant, which is in line with earlier findings of TagueSutcliffe and Blustein [29].
The focus of our paper is on permutation tests. These procedures were independently proposed by Pitman [18] and Fisher [12] in the 1930s, long before advances in computer hardware made this approach practical. A straightforward generalization of the permutation test that accounts for multiple testing is based on the closure principle proposed by Marcus et al. [16]. It entails verification of up to 2m - 1 null hypotheses (m is the number of tests). Westfall and Young

proposed a computational shortcut, which allows one to consider only m hypotheses [33, 32]. One method of Westfall and Young, called the MaxT permutation test, was shown to have high statistical power among methods that provided a strong control of the FWER in microarray experiments [11].

2. HYPOTHESIS TESTING
We consider a standard experimental setting in IR. There is a set of queries, which represent user's information needs, ground truth relevance judgements for these queries, and several retrieval systems. Selection of queries can be thought of as a random sampling from an infinite (or very large) population. The relevance judgements are compared against ranked sets of documents (called runs) retrieved by these systems in response to q queries. Effectiveness of retrieval is characterized by scores computed separately for each query using a performance metric, such as the Expected Reciprocal Rank at depth 20 (ERR@20). The mean of query-specific scores is then used to evaluate the overall performance of retrieval systems.
Let scores of systems X and Y be represented by vectors x = (x1, x2, . . . , xq) and y = (y1, y2, . . . , yq) with mean values equal to x¯ and y¯, respectively. Even if x¯ is substantially larger than y¯, we cannot safely infer that Y is inferior to X in the long run. The scores are highly variable across queries [29] and it is not uncommon for an inferior system to outperform a superior system on some subpopulation of queries. Performance of X and Y in this subpopulation is not a good indication of the relative performance in the entire population of queries. There is always a chance that our test sample has a lot of queries for which the inferior system outstrips the superior one. Thus, the measured difference between X and Y could be attributed to random sampling effects.
Significance testing is a standard approach to deal with this problem. Testing involves the following steps:

1. An IR researcher formulates a null hypothesis H (or simply a null), e.g., by assuming that there is no difference in ERR@20 (or some other performance metric) between X and Y . That is, the population means are equal. In addition, he sets a significance level  that controls the rate of false rejections (i.e., false positives).

2. He chooses a test statistic T (x, y) (a function of measured systems' scores) that provides evidence against the null hypothesis H. One example is the sample mean difference: T (x, y) = x¯ - y¯. Small absolute values of this statistic present evidence in favor of equality of population means, while large ones may signify that H is not true. Another well-known statistic is the paired t-statistic:

(x¯ - y¯) q(q - 1)

q i=1

(xi

-

yi

-

x¯

+

y¯)2

.

(1)

3. The researcher quantifies the evidence provided by the test statistic. Formally, he computes a statistic value t = T (x, y) from the sample data. Then, he estimates the probability of obtaining a test statistic value at least as extreme as t under the null hypothesis (i.e., when H is true). This probability is known as a p-value. If the p-value is less than the chosen significance level

404

, the observed value of the statistic is unlikely to happen by chance (i.e., due to randomness in selection of queries). Thus, the researcher can reject the null hypothesis with confidence 1 - . We discuss this approach in Section 2.2 in more detail.
To compute the p-value, we need to know the distribution of the test statistic under the null. In a parametric approach, we assume that data follows a theoretical distribution, which allows us to derive the distribution of the test statistic analytically. A widely used parametric test is the Student's t-test. In a non-parametric approach, the distribution of the test statistic is estimated through resampling of observed data (see Sections 2.2-2.3).
When, we observe an unusually small p-value this may be due to the following:
1. The null hypothesis is not true;
2. The null hypothesis is true and extreme statistic value is observed by chance;
3. Some underlying assumptions are violated.
The null hypothesis can be true even when the statistic value is extreme. Yet, if we reject the null only when the corresponding p-value is less than , we ensure that in a series of repeated experiments the probability to incorrectly reject the true null is . Thus, in the frequentist approach, one should avoid a temptation to interpret the p-value as the probability of the null hypothesis being true or as another measure that quantifies the veracity of the null.
Note especially the third case. If the statistical procedure relies on the distributional assumptions (such as the normality assumption for the t-test) and these assumptions are violated, this may also lead to a rejection of the null. Unfortunately, there is no good way to control a rate of false rejections due to assumption violations. Thus, it is very desirable to use tests requiring minimal assumptions such as the non-parametric randomization procedures assessed in our work.
Effectiveness of a testing procedure can be characterized by a proportion of true positives (correctly rejected false null hypotheses) and by a proportion of false positives (incorrectly rejected true null hypotheses).
2.1 Multiple Testing
The significance level  controls the probability of a false positive under the true null hypothesis only in a single test. Consider an example, where the researcher slightly modifies a baseline method 100 times and measures changes in performance. The significance level in each test is  = 0.05. Suppose that these modifications of the baseline method did not result in any real improvements. Therefore, he may expect to obtain at least one false positive with the probability of 1 - (1 - )100  0.99, and five false positives on average. If the researcher is sufficiently na¨ive, he may decide that merely obtaining relevance judgements for a larger set of queries will help to overcome this problem. However he would still obtain about five false positives on average, irrespective of the number of queries used. One can easily verify this statement using the simulation approach presented in Section 3.3.
This problem can be addressed by using an adjustment for multiplicity in testing. The classic adjustment method is the

0.0

0.1

0.2

0.3

0.4

0.0

0.1

0.2

0.3

0.4

-4

-2

0

2

4

(a) Identical systems

-4

-2

0

2

4

(b) Different systems

Figure 1: Distribution of statistic values obtained through random 100,000 permutations. Thick vertical lines denote statistic values computed from non-permuted system scores.

Bonferroni procedure. Let p1, p2, . . . , pm be a set of unadjusted p-values. The Bonferroni method consists in multiplying each pi by the number of tests m (values larger than 1 are set to 1). Then, we reject hypotheses with p-values smaller than . This procedure is equivalent to enforcing a significance level of /m in each of the m tests. In other words, the probability to observe a false positive in a single experiment should be /m, assuming that the null is true. Using the union bound, we obtain that in a series of m experiments, the probability to encounter at least one false positive is controlled at the level .
The Bonferroni adjustment provides a strong control of a family-wise error rate (FWER) at the significance level , but it is conservative. The Holm-Bonferroni adjustment [13] is a slightly more powerful method. Let p1  p2  . . .  pm be an ordered set of unadjusted p-values. The HolmBonferroni adjustment entails multiplying pi by m - i + 1 and enforcing monotonicity of obtained values. Formally, the i-th adjusted p-value is equal to:

min(1, max pj · (m - j + 1)).

(2)

ji

In the subsequent sections we present several non-parametric adjustment methods based on randomization. The discussion starts with a description of the permutation test for two systems.

2.2 Permutation Test (Two Systems)
We compare two systems represented by performance scores x = (x1, x2, . . . , xq) and y = (y1, y2, . . . , yq). The hypothesis of interest is whether systems' mean population values of the performance metric (e.g., ERR@20) are equal. We additionally assume that under the null hypothesis H the values of x and y are outcomes of exchangeable multivariate random variables X and Y . This can be viewed as a combination of two random processes. The first random process generates a pair of scores in response to a query. The second process randomly labels one of the scores as belonging to X and another as belonging to Y (with equal probabilities).
From the practical perspective, this means that the distribution of the test statistic under the null hypothesis can be computed by the following randomization procedure. First, the vectors of performance scores x and y are stored in the

405

form of the matrix with each vector representing a row:

x1 y1

x2 y2

... ...

xq yq

.

Then we repeatedly obtain new pseudo-observation vectors x and y by randomly exchanging (i.e., permuting) values in the columns of this matrix. If the hypothesis H is true, all such observations are equally likely outcomes of exchangeable variables X and Y . If, in addition, we compute the value of the statistic T (x, y) for all possible 2q permutations, we obtain an exact distribution of the test statistic (under the null). Computing all 2q statistic values is intractable for all but very small q. Instead, the distribution could be approximated by carrying out sufficiently many random permutations B.
In Figure 1, there are two approximate distributions of the t-statistic for B = 100, 000. The thick vertical lines indicate the values of the statistic t = T (x, y) computed using nonpermuted vectors x and y. The rightmost distribution in Figure 1 was computed for different systems. The value is t  3.5 and only about one in 2,000 of computed statistic values exceeds t. The p-value is 0.0005, which means that we can reject the hypothesis that the two systems are identical at  = 0.05. The leftmost distribution in Figure 1, was computed using very similar systems. The statistic value t  0 and the p-value  0.5 Hence, H cannot be rejected.
The described procedure is a one-sided (one-tailed) test, because we reject the null, when the statistic value falls into the right tail of the statistic distribution. If a statistic distribution is symmetric (as distributions in Figure 1), we may choose to reject the null, when the statistic value falls into the left tail, i.e., to compute the p-value as the probability to observe a statistic value at least as low as -T (x, y). If we use the paired t-statistic, the one-tailed test allows us to make statistical inference about directionality of the difference (i.e., which system has significantly better scores). For instance, if we observe a high positive value of the T (x, y) we can reject the hypothesis that Y is better (has a higher average score) than X.
If we do not know a priori which method is better, we may choose only to test whether methods are different or not. To this end, one can employ a two-sided (two-tailed) test, where a p-value is computed as the probability of observing statistic values that are at least as high as T (x, y) or at least as low as -T (x, y). In this paper we focus on two-sided tests and leave evaluation of one-sided tests for future work. One approach to directional inference involves carrying out a two-sided test and comparing mean performance scores if the difference is significant. This approach is widespread, but not fully rigorous, because it offers no protection against choosing the wrong direction [26].
From a computational perspective, there is no need to evaluate the distribution of T (x, y) explicitly. One can emulate this process using a counter C, initially set to zero. In each permutation step, we compute T (x, y) and verify if |T (x, y)|  |T (x, y)|. When this condition is true, the counter C is incremented. For a one-sided test, one increments the counter when T (x, y)  T (x, y). Finally, the p-value is computed as C/B, where B is the number permutations.
How many permutation steps are sufficient? The coefficient of variation (the standard error divided by the mean)
of an estimated p-value is equal to (1 - p)/(pB), where

p is the actual p-value [12]. For B = 20, 000 (the minimum number of permutations used in our tests) the coefficient of variation for p = 0.05 is approximately equal to 0.03. Using the Chebyshev's inequality, we get that the estimate is accurate within 15% for 96% of computed p-values (within 5 standard deviations).
Various test statistics can be employed with the permutation test. We use the paired t-statistic given by Equation (1), because it is asymptotically standard normal when differences in query-specific scores are independent or weekly dependent [28]. The sample mean difference does not have this property, because the variance of the statistic converges to zero when q grows. Our preliminary experiments showed that tests based on the sample mean difference sometimes suffer from subsantial loss of power.
2.3 Permutation Test (Multiple Systems)

2.3.1 Generalizing Permutation Algorithm
There are m retrieval systems and a baseline. We formulate m null hypotheses Hi by assuming that there is no difference between the system i and the baseline. Our focus is on testing all Hi jointly and controlling the FWER.
One may assume that the permutation algorithm of Section 2.2 can be generalized to deal with joint testing in the following fashion. Let m + 1 vectors xi represent performance scores of m + 1 systems, where x0 is the baseline. These vectors are combined in the matrix X (each vector is a row):

x01 x02 . . . x0q

X=

x11

x12 ...

. . . x1q

.

xm1 xm2 . . . xmq

Hypothesis Hi states that there is no difference between system i and the baseline. Each Hi is associated with a test statistic Ti(X) = T (xi, x0), where T (x, y) is a paired t-statistic (Equation 1), and the counter Ci, initially set to zero. The method involves sufficiently many permutation
steps, each of which includes:

· Randomly permuting values inside columns of X and obtaining a permuted matrix X;

· Computing m statistic values Ti(X) = T (xi, x0) and comparing them with statistic values obtained for the original matrix X. Formally, we increment all counters Ci such that |Ti(X)|  |Ti(X)|.
After B iterations, we compute the i-th p-value as Ci/B. This simple procedure runs in time O(mB), but it fails to produce p-values adjusted for multiplicity.
One can modify this method to verify the complete null hypothesis, i.e., that all methods are not distinguishable from the baseline. It is used as a part of the permutationbased closed testing presented in Section 2.3.3. When the complete null does not hold, there is at least one system different from the baseline. When, we reject the complete null, we are confident that not all systems are identical, but we cannot infer which systems are actually different.
To implement this modification, we need an aggregate statistic that incorporates all m individual statistics Ti(X). We use the maximum of all statistics:

Taggr(X) = max |Ti(X)|. 1im

406

Similarly to the permutation algorithm for two systems (see Section 2.2), we have one (complete) null hypothesis, one (aggregate) statistic, and a single counter C initialized with zero. We carry out B permutations steps, each of which includes random reshuffling of values inside columns of X to obtain the permuted matrix X. If Taggr(X)  Taggr(X), we increment C. Finally, we reject the complete null if C/B < . To verify a partial hypothesis that methods i1, i2, . . . , ik are the same, one should apply this algorithm to a sub-matrix containing only rows i1, i2, . . . , and ik.

2.3.2 The MaxT Permutation Test
Westfall and Young proposed the MaxT permutation test, which is an extension of the generic procedure presented in Section 2.3.1. It uses the following m functions:
MaxTi(X) = max |Tj(X)|. ijm
Let ti = |Ti(X)| be the statistic values computed for the unmodified matrix of observations X. Without a loss of generality we assume that ti are sorted in the non-increasing order : t1  t2  . . .  tm. There are also m counters Ci (one for each hypothesis) initially set to zero.
We carry out B iterations. In each iteration, we obtain the permuted matrix X and compute m values MaxTi(X). If MaxTi(X)  MaxTi(X) we increment the counter Ci. In the end, we obtain p-values using the formula:

p-valuei

=

max
1ji

Cj /B .

The MaxT permutation test runs in O(mB) time and controls the FWER under the assumption of subset pivotality. Subset pivotality means that distributions of observed p-values under any partial null hypothesis should be the same as under the complete null hypothesis. Subset pivotality does not always hold in practice. Departure from this assumption may result in a low test power or inflated rate of false positives [35].

2.3.3 Joint Hypotheses and Closed Testing
Consider hypotheses H1, H2, H3 and assume that we decided to reject at least one of them. In doing so, we express our belief that the respective joint hypothesis H1H2H3 = H1  H2  H3 is not true. This observation suggests that, before applying individual tests to H1, H2, and H3, we could test the joint hypothesis (also known as an intersection hypothesis). If the intersection hypothesis is rejected, we may make additional tests to decide which individual hypotheses can be rejected. However, if H1H2H3 is not rejected, we retain all the implied hypotheses and no further testing is needed.
This observation is the foundation of the closed testing procedure proposed by Marcus et al. [16]. In closed testing, all joint hypotheses are tested directly. If we fail to reject some joint hypothesis, we do not reject all implied hypotheses either. To test joint hypotheses, we use the permutation method from Section 2.3 and call this approach the permutation-based closed testing.
Assume now that H1, H2, and H3 are true null hypotheses and, therefore, H1H2H3 is also a true null. It is tested using an -level test. Thus, the probability of not rejecting this true joint hypothesis is at least 1-. According to the closed testing principle, if we do not reject the joint hypothesis H1H2H3, we do not reject any of the implied hypotheses

Figure 2: An example of closed testing with three hypotheses and significance level  = 0.05. We give a p-value for each intersection hypothesis. Gray denotes hypotheses that cannot be rejected.
H1, H2, and H3 either. Consequently, the probability of not rejecting any of them is at least 1 - , and the probability of rejecting at least one of them is at most . In other words, this test controls the family-wise error rate (FWER) in the family of three hypotheses H1, H2, and H3.
An example of closed testing with three hypotheses is given in Figure 2. We test 7 intersection hypotheses (including elementary hypotheses Hi as a special case) at the significance level  = 0.05. In that, H3 is rejected, because H3 itself as well as all three hypotheses that include H3, i.e., H1H3, H2H3, and H1H2H3, have p-values smaller than . H1 and H2 cannot be rejected, because we could not reject H1H2.
Closed testing is computationally intensive. Given that we have m individual hypotheses, there are 2m - 1 intersection hypotheses each of which can be tested in O(mB) time. Thus, the overall runtime of the permutation-based closed testing is O(m2mB).
To reduce runtime of the complete closed testing, one could start with the narrowest intersection hypothesis (that includes all m individual hypotheses) and proceed to more generic ones. Once a hypothesis H cannot be rejected, all hypotheses implied by H need not be considered. In the example of Figure 2, we could have started with H1H2H3 and proceeded to testing H1H2. Because H1H2 could not be rejected, H1 and H2 could not be rejected as well. Therefore, there is no need to test them explicitly. According to our experiments, this modification of closed-testing procedure is 10-20% faster than complete closed testing, but the overall runtime is still exponential in m.
2.4 TEST COLLECTIONS AND SYSTEMS
2.4.1 TREC Data
Similar to previous studies [27, 10, 7], we analyze data from several TREC ad hoc tasks.1 These tasks can be divided into two groups: TRECs 3-5, 7,8, and the latest Web tasks in TRECs 19, 20. The relevance judgements in the first group are binary. They were collected through pooling to depth at least 100 [5]. The second group has 5-grade relevance judgments obtained through pooling at depth 20
1http://trec.nist.gov/pubs.html

407

[9]. The average number of judgements per query varies among tasks: It is roughly 2,000 in the early TRECs and is about 400 in TRECs 19, 20. The proportion of documents considered (somewhat) relevant is typically 10-20%.
The retrieval systems were represented by official runs produced by TREC participants. We downloaded raw run data and relevance judgements from the TREC website and computed the mean average precision (MAP), ERR@20 [8], and NDCG@20 using utilities trec_eval, and gdeval, which are provided by TREC organizers.
2.4.2 Learning-to-Rank Set MSLR-WEB30K.
This data set is provided by Microsoft2. It contains machine learning data with 5-grade relevance judgements, which were obtained from an obsolete training set used internally by Microsoft Bing.
MSLR-WEB30K has relevance judgments for 30,000 queries, which makes it possible to derive reliable conclusions about relative standings of retrieval systems. The judgements were collected in a way similar to a standard pooling. (Personal communication with Tao Qin, Microsoft Research Asia.)
The Microsoft data set is different from TREC collections in several important ways. First of all, it contains machine learning data, where a response of a retrieval system to a specific query is represented by a set of feature vectors such as BM25 scores [20] or document quality scores. Each document returned for a given query is represented by exactly one feature vector and a single label that expresses the degree of relevance between the document and the query.
It is not possible to access the original collection as well as to implement a system that uses data other than a set of precomputed features (stored in the MSLR-WEB30K file). In contrast to TREC runs (potentially representing thousands of documents per query), the average number of judged documents per query in MSLR-WEB30K is only 126.
Microsoft provides a rich set of 136 features, which are not scaled and have clear descriptions (i.e., they are not anonymized). This allows us to generate runs closely resembling runs obtained from a real retrieval system without actually implementing such a system. We use the following three methods or a combination thereof:
Method 1 computes a weighted sum of BM25 scores [20] as well as scores based on the language models [19] with two types of smoothing: Jelinek-Mercer and Dirichlet [36]. The value of this sum may be additionally multiplied by a linearly transformed quality score (feature 133).
Method 2 employs randomization to produce Oracle runs that improve over BM25 in a predictable way. We randomly select queries to be improved (with the probability p). A weight of each document returned in response to the selected queries is multiplied by 1 + r(2l - 1), where r is a magnitude of improvement and l is the document relevance label.
Method 3 employs randomization to produce different runs, which nevertheless have almost identical performance scores. To make a randomized version of a run, we modify scores by adding a number drawn from a uniform distribution (with support 0­0.2) as proposed by Blanco and Zaragoza [4].
We evaluate methods using only ERR@10, which ignores documents at depths higher than 10.
2http://research.microsoft.com/en-us/projects/ mslr/

Table 1: Fractions of Individually Significant Results Deemed Insignificant due to Adjustments for Multiplicity (smaller is better),  = 0.05, ERR@20.

TREC
3 4 5 7 8 19 20 All

Closed test
16.1% 12.7% 7.5% 15% 8.2% 31.1% 33.5%
16.4%

MaxT
16.4% 12.7% 8.7% 15.4% 8.2% 32.1% 33.5% 16.8%

Holm Bonf
19.1% 15.4% 10% 17.3% 9.5% 32.1% 38.1%
18.8%

3. EXPERIMENTS
3.1 Statistical Tests Employed
Our experiments involve several statistical tests including permutation-based closed testing, the MaxT permutation test, and the Holm-Bonferroni adjustment (Equation 2) of the unadjusted permutation test p-values (see Section 2.2).
The permutation tests were implemented in C++. We use the Mersenne Twister generator of random numbers [17], which has a period of 219937 - 1. In the analysis of the TREC data, the number of permutations B = 50, 000; in the simulation studies with MSLR-WEB30K data, B = 20, 000. Our code is available for download at https://github.com/ searchivarius/PermTest.
3.2 TREC data
Our main goal is to assess (1) an agreement among different tests and (2) a degree of conservativeness of multiple comparisons adjustment procedures. To this end, we used TREC data to randomly choose retrieval systems and compare them against a randomly chosen baseline. Because closed testing run time is exponential in the number of compared systems, the number of systems is limited to 10. We carried out 50 iterations for several TREC data sets (see Table 1).
The agreement among these four statistical tests for TREC data is shown in Figure 3a. One can see that all tests that adjust for multiplicity produce larger p-values than the unadjusted permutation test. When we compare only among tests that adjust p-values for multiple comparisons, we can see that there is very little difference in p-values smaller than 0.1. The application of the Holm-Bonferroni adjustment does result in much larger p-values, but only for p-values that are already large (> 0.1). These two observations are consistent with findings by Dudoit et al. [11]. Also note that the outcomes from the permutation-based closed testing and the permutation MaxT tests are almost identical.
There is no ground truth information about relative performance of systems in TREC. This is why we can compare the power of tests only approximately, by examining the number of significant results. According to Table 1, multiple comparisons adjustments "kill" from 8 to 38 percent of results that were deemed significant by the unadjusted permutation test. In that, there is very little difference among tests. Closed testing is being slightly better than MaxT, and MaxT is slightly better than the Holm-Bonferroni adjustment. These is observed in all TRECs, but the difference

408

1.0

0.6 0.8 1.0

0.8

0.6
Unadjusted 0.4

0.2

0.0 0.2 0.4

0.0 1.0

0.6 0.8 1.0

0.8

0.6
Closed Test 0.4

0.2

0.0 0.2 0.4

0.0 1.0

0.6 0.8 1.0

0.8

0.6
MaxT 0.4

0.2

0.0 0.2 0.4

0.0 1.0

0.6 0.8 1.0

0.8

0.6
Holm-Bonf 0.4

0.2

0.0 0.2 0.4

0.0

(a) TREC data, 10 runs in a comparison

1.0

0.6 0.8 1.0

0.8

0.6
Unadjusted 0.4

0.2

0.0 0.2 0.4

0.0 1.0

0.6 0.8 1.0

0.8

0.6
Closed Test 0.4

0.2

0.0 0.2 0.4

0.0 1.0

0.6 0.8 1.0

0.8

0.6
MaxT 0.4

0.2

0.0 0.2 0.4

0.0 1.0

0.6 0.8 1.0

0.8

0.6
Holm-Bonf 0.4

0.2

0.0 0.2 0.4

0.0

(b) MSLR "Language models" data, 8 runs in a comparison

Figure 3: Agreement of p-values among adjustment methods. A performance metric is ERR@10.

is too small to be of practical importance. Note that the fraction of results that became insignificant due to multiple comparisons adjustments vary greatly among TRECs. Only about 10% of all results became insignificant in TREC-5, but in TREC-20 we lose almost half of the results, if multiple comparisons adjustments are used.
3.3 MSLR-WEB30K data
For MSLR-WEB30K we carried out a simulation study, in which we generated runs with 30,000 queries (one run for each retrieval system). These runs are "populations" that represent long-term performance of retrieval systems. Systems' responses to a smaller set of q queries were generated through repeated sampling from 30,000 queries. As a result, we obtained simulated runs representing performance of each system for selected q queries (as measured by ERR@10). In addition, mean value for each metric was computed. We evaluated several scenarios where q varied from 50 to 6,400. The sample size of 50 is intended to represent a typical TREC experimental setting, while much larger sets of queries mimic experimental environment accessible by a commercial search engine developer/researcher.
Even though the sample mean of a performance metric may vary considerably in each simulation step, the average values of ERR@10 converge to the mean population values (of 30,000 queries) as the number of simulation steps increases. Thus, we can use population values of performance metrics to establish ground truth relative standings among systems. To this end, we select a cutoff value  = 0.5% and consider all pairs of systems with percent differences in ERR@10 (computed for the whole population) smaller than  as identical. By repeating our analysis for   {0.05, 0.1, 0.5, 1}, we confirmed that conclusions did not depend on the choice of this cutoff value.
The cutoff-based approach reflects a point of view that small differences in system performance may be due to sam-

pling uncertainty of our 30,000 queries from a much larger super population. In fact, some statisticians question
. . . whether it makes sense to even consider the possibility that the means under two different experimental conditions are equal. Some writers contend that a priori no difference is ever zero (for a recent defense of this position, see Tukey 1991, 1993). Others, including this author, believe that it is not necessary to assume that every variation in conditions must have an effect [26].
The latter point is supported by studies showing that small differences may not affect user experience [30].
We generated three sets of populations: "Language Models", "Oracle 0.25", and "Oracle 0.5" (see Section 2.4). Language models were generated using Method 1. Oracle runs were generated by Method 2 with probabilities of improvement 0.25 and 0.5. The best runs in the "Language Models", "Oracle 0.25", and "Oracle 0.5" improved over the BM25 by 17%, 14%, and 27%, respectively. In each of the three population sets, we took a system with performance close to the median of the set, and replicated it 4 times using randomization (Method 3). This system is considered to be a baseline. Overall, each population set had 8 runs, half of which were almost identical (in terms of the mean value of ERR@10).
Given a specific sample size and a set of runs, we carried out 500 iterations of the resampling process and tested if the differences between simulated runs were significant (using selected statistical tests). Because we knew the ground truth relative standings of retrieval systems, it was possible to determine the number of false positives and negatives.
The agreement among tests is plotted in Figure 3b. The plot includes the "Language Model" runs only (all query set sizes from 50 to 6,400), because results obtained for Oracle runs are almost identical. One can see that agreement graphs are also similar to those for the TREC data:

409

(1) multiple comparisons adjustment procedures produce larger p-values than the unadjusted permutation test, (2) permutation-based closed testing and the MaxT permutation test agree almost ideally for the whole range of p-values, (3) the Holm-Bonferroni adjustment applied to p-values of the unadjusted permutation test is more conservative than MaxT and closed testing. Similar to TREC experiments, there is virtually no difference among all multiple comparisons methods for small p-values (< 0.1). Observation (2) is important, because we can use the MaxT permutation test instead of considerably less efficient closed testing (whose run time is exponential in the number of systems m).

Table 2: The Percent of False Negatives/Positives for different query set sizes ( = 0.05)

Query Set Size

50

100

400

1600

6400

"Lang. Models": 4 out of 8 runs same as the baseline

Unadjusted 85.7/14.4 80.8/11.6 53.9/10.0 25.9/15.4 2.5/17.0

Closed Test 92.9/0.0 88.8/0.2 69.5/1.7 36.6/3.1 5.2/6.8

MaxT

93.9/0.0 91.8/0.2 68.0/1.2 35.7/3.0 6.3/6.6

Holm-Bonf. 94.9/2.0 92.5/1.8 69.6/2.6 37.0/3.2 6.5/6.2

"Oracle 0.25": 4 out of 8 runs same as the baseline

Unadjusted 91.6/12.9 86.0/14.1 56.9/13.9 22.9/14.5 0.3/9.3

Closed Test 98.9/1.8 97.8/1.1 73.8/2.1 35.3/2.8 1.1/3.2

MaxT

97.3/2.0 96.4/3.0 74.4/3.0 36.1/5.5 1.0/4.6

Holm-Bonf. 98.2/2.4 97.0/3.4 74.9/2.6 37.4/4.8 1.9/4.2

"Oracle 0.5": 3 out of 8 runs same as the baseline

Unadjusted 87.2/8.1 76.0/8.5 49.0/9.5 22.0/8.9 18.6/6.9

Closed Test 98.2/1.1 93.8/0.4 62.5/2.5 26.0/2.1 19.6/2.1

MaxT

96.9/1.2 93.3/1.6 61.4/2.6 26.5/3.2 19.4/2.8

Holm-Bonf. 97.7/1.0 91.5/2.2 62.9/2.0 27.3/2.8 19.5/2.0

Format: false negative rate (blue)/false positive rate (red).

Using ground truth relative standings for system performance, we computed the rates of false positives and false negatives for different query set sizes. In Table 2, we present results of this evaluation. Surprisingly, there is very little difference in the rate of false negatives (approximately within 10%) between the unadjusted permutation test and any test that takes multiplicity into account. However, when the number of queries is small (as in TREC ) and the number of false negatives is close to 100%, the number of detected differences in system performance may vary greatly. For instance, in the case of 50 queries and "Language Models" runs, the unadjusted permutation test detects 14.3% of all true differences (85.7% false negative rate), while the MaxT permutation test detects only 6.1% (93.9% false negative rate). Detection of these additional 8.2% true differences comes at a price of at least one false finding in 14.4% of all experimental series. In contrast, the number of false positives for the MaxT test is zero in this case.
If the researcher does not know the true number of different systems, he may conclude that the MaxT test performs much worse than the unadjusted permutation test from the perspective of detection of true differences. Yet, in our opinion, both tests perform rather poorly in this situation. When there is a sufficient number of queries, all the tests detect more than 80-90% of true differences. In that, only the tests that adjust for multiplicity have the false positive rate close

to the nominal level of  = 0.05, i.e., they perform better than the unadjusted test, without being overly conservative.
Consider a somwehat extreme example where out of 100 systems 90 are equivalent to the baseline. For  = 0.05, unadjusted tests may find 4-5 statistically significant differences, which represent false positives. It is possible that for small sets of queries no true difference will be detected, if false negatives rates are as high as those listed in the first column of Table 2.
3.4 Discussion
Our results indicate that multiple comparisons adjustments can be conservative when the number of queries is small. Yet, as the number of queries increases, the FWER approaches the nominal level . When the number of queries is large, both types of tests (with and without multiplicity adjustment) detect similar number of true differences, but only adjustments for multiple comparisons allow us to control the number of false positives.
This conclusion may be affected by a small scale of our experiments (a joint test involves at most 10 systems). Yet, a small-scale experiment is not unusual for studies with both an exploratory and a confirmatory step. In the exploratory step, the researcher may "play" with a large number of systems and choose various heuristics to assess systems' performance. Multiple comparisons adjustments are typically not used in this step. The outcome is a small number of systems to be tested rigorously. During the confirmatory step, the researcher formulates the null hypotheses and carries out a statistical test using previously unseen data. We argue that in this step multiple comparisons adjustments are essential.
We found that the Holm-Bonferroni adjustment was only slightly more conservative than the MaxT permutation test and/or the permutation-based closed testing, which was true for both the TREC and the MSLR experiments. This is surprising, because performance scores across systems are correlated. In the presence of correlations, the MaxT permutation test and the permutation-based closed testing are expected to be more powerful than the Holm-Bonferroni adjustment.
However, permuting the data, subtracting the baseline row, and computing the t-statistic is equivalent to first subtracting the baseline row, then permuting the differences, and computing the t-statistic. Thus, it is the correlations among the deviations from the baseline that matter. We found that these correlations are small. For instance, for the TREC-8 data and ERR@20, the correlation is almost zero on average. This explains similar relative performance of the Holm-Bonferroni adjustment and the other two procedures. Yet, this may not generally hold.
We carry out an artificial experiment in which we took two vectors of performance scores such that there was a significant statistical difference between them with a p-value equal to . Then, we replicated one of the vector several times, which is equivalent to having a number of identical systems evaluated against the baseline. The p-value computed using either the MaxT permutation test or the permutation-based closed testing procedure was approximately  in all experiments. The Holm-Bonferroni correction produced a p-value of m, where m is the number of times the system was replicated. Thus, using the MaxT permutation test or the permutation-based closed testing can be advantageous.

410

While the run-time of the permutation-based closed testing procedure is exponential in the number of systems being evaluated, the run-time of the MaxT permutation test is reasonably short. For example, it takes 6 minutes to carry out 100K iterations of the MaxT permutation test to assess the joint statistical significance of 8 system runs represented by performance scores for as many as 30K queries.3
One may find our use of machine learning data objectionable, because it requires assumptions regarding what can be considered a retrieval system. Note, however, that the learning-to-rank community already made these assumptions and models the behavior of retrieval systems in the same fashion as we constructed "Language Model" runs. The only difference is that we designed a (semi)-linear ranking function with coefficients tuned by hand. They, instead, replace this step with a machine learning algorithm. They also evaluate performance of constructed runs using ERR@10 and employ statistical tests. Thus, it is important to show that the statistical tests work well in the learning-to-rank setting. Also note that all our tests exhibit similar behavior for both the TREC and MSLR data, which supports the hypothesis that MSLR runs are similar to those produced by real retrieval systems.
Even though permutation tests do not make strong distributional assumptions such as the normality or i.i.d, they are not assumption free. Exchangeability means that we test the equality of distributions instead of sample means. This may appear problematic, because sometimes the test may reject the null due to, e.g., a difference in variances. In particular, the simulation studies of Huang et al. [14] showed that inequality of distributions sometimes results in inflated rates of false positives. Yet, as noted by Efron and Tibshirani [12], permutation tests typically perform well in practice, even if the equality of distributions is not a reasonable assumption. They also suggest that the permutation test should be applied in all circumstances when there is "something to permute", even if other methods such, as the bootstrap test, are applicable as well. In addition, the equality of distributions is an underlying assumption for a number of statistical tests, such as the Student's t-test, already used by the IR community.
4. CONCLUSIONS
We carried out a comparative assessment of non-parametric testing procedures appropriate in the presence of multiplicity. To the best of our knowledge, such comparisons have not been done previously in the IR setting. We use only publicly available test collections and make our software available for download.
The experiments employ the realistic TREC runs and runs constructed from the Microsoft learning-to-rank dataset. The latter is a novel approach, which allows us to (1) obtain ground truth relative standings among systems, (2) experiment with much larger sets of queries and relevance assessments compared to the TREC setting.
Our recommendation is to employ adjustments for multiple comparisons in confirmatory experiments. When the number of queries is small, these procedures may, indeed, detect many fewer significant results than standard procedures such as the Student's t-test. However, the advantage of the tests without adjustments may be illusory. In this
3The CPU is Intel Core i7 (3.4 GHz).

case, both the unadjusted tests and tests that adjust for multiplicity detect only a small fraction of all true differences. In that, results obtained using unadjusted tests may contain a lot of false positives, possibly, more than significant results. When there is a large query set, both types of tests may have enough power to detect true differences among systems. Yet, only the procedures adjusting for multiplicity control the rate of false positives.
The permutation-based closed testing relies on fewer assumptions than the MaxT permutation test, yet, it is impractical for all but very small sets of runs. Our recommendation is to use the MaxT permutation test, which seems to produce very similar results while being reasonably fast. In our experiments, the Holm-Bonferroni adjustments performed as well as the other adjustment methods. Yet, this may be due to specifics of our simulations, where there are small correlations among deviations from the baseline. As the example in Section 3.4 shows, permutation methods can be much more powerful when strong correlations are present.
5. ACKNOWLEDGMENTS
We thank Tao Qin (Microsoft Research Asia) for information about the MSLR collection. Leonid Boytsov was partially supported by a SIGIR Student Travel Grant. Dr. Westfall was partially supported by the following grants: NIH RO1 DK089167. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any of the funding agencies.
6. REFERENCES
[1] Anonymous. Guidance for Industry - E9 Statistical Principles for Clinical Trials. Technical report, U.S. Department of Health and Human Services - Food and Drug Administration, Center for Drug Evaluation and Research, Center for Biologics Evaluation and Research, ICH, 1998.
[2] R. Bender and S. Lange. Adjusting for multiple testing--when and how? Journal of Clinical Epidemiology, 54(4):343 ­ 349, 2001.
[3] Y. Benjamini and Y. Hochberg. Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing. Journal of the Royal Statistical Society. Series B (Methodological), 57(1):289­300, 1995.
[4] R. Blanco and H. Zaragoza. Beware of relatively large but meaningless improvements. Technical report YL-2011-001, Yahoo! Research, 2011.
[5] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Information Retrieval, 10:491­508, 2007.
[6] R. J. Cabin and R. J. Mitchell. To Bonferroni or not to Bonferroni: when and how are the questions. Bulletin of the Ecological Society of America, 81(3):246­248, 2000.
[7] B. A. Carterette. Multiple testing in statistical analysis of systems-based information retrieval experiments. ACM Trans. Inf. Syst., 30(1):4:1­4:34, Mar. 2012.
[8] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceeding of the 18th ACM conference on

411

Information and knowledge management, CIKM '09, pages 621­630, New York, NY, USA, 2009. ACM.
[9] C. L. A. Clarke, N. Craswel, I. Soboroff, and G. V. Cormack. Overview of the TREC 2010 Web track. In TREC-19: Proceedings of the Nineteenth Text REtrieval Conference, 2010.
[10] G. V. Cormack and T. R. Lynam. Validity and power of t-test for comparing map and gmap. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 753­754, New York, NY, USA, 2007. ACM.
[11] S. Dudoit, J. Schaffer, and J. Boldrick. Multiple hypothesis testing in microarray experiments. Statistical Science, 18(1):71­103, 2003.
[12] B. Efron and R. Tibshirani. An Introduction to the Bootstrap. Monographs on Statistics and Applied Probability. Chapman & Hall, 1993.
[13] S. Holm. A Simple Sequentially Rejective Multiple Test Procedure. Scandinavian Journal of Statistics, 6:65­70, 1979.
[14] Y. Huang, H. Xu, V. Calian, and J. C. Hsu. To permute or not to permute. Bioinformatics, 22(18):2244­2248, 2006.
[15] E. L. Lehmann and J. P. Romano. Generalizations of the familywise error rate. Annals of Statistics, 33(3):1138­1154, 2005.
[16] R. Marcus, P. Eric, and K. R. Gabriel. On closed testing procedures with special reference to ordered analysis of variance. Biometrika, 63(3):655­660, 1976.
[17] M. Matsumoto and T. Nishimura. Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator. ACM Trans. Model. Comput. Simul., 8(1):3­30, Jan. 1998.
[18] E. Pitman. Significance tests which may be applied to samples from any population. Royal Statistical Society, Supplement, 4:119­130, 1937.
[19] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '98, pages 275­281, New York, NY, USA, 1998. ACM.
[20] S. Robertson. Understanding inverse document frequency: On theoretical arguments for IDF. Journal of Documentation, 60:503­520, 2004.
[21] Y. Saeys, I. n. Inza, and P. Larran~aga. A review of feature selection techniques in bioinformatics. Bioinformatics, 23(19):2507­2517, Oct 2007.
[22] M. Sanderson and J. Zobel. Information retrieval system evaluation: effort, sensitivity, and reliability. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '05, pages 162­169, New York, NY, USA, 2005. ACM.
[23] J. Savoy. Statistical inference in retrieval effectiveness evaluation. Information Processing & Management, 33(4):495 ­ 512, 1997.
[24] H. Scheff´e. A method for judging all contrasts in the analysis of variance. Biometrika, 40(1-2):87­110, 1953.

[25] F. Scholer, A. Turpin, and M. Sanderson. Quantifying test collection quality based on the consistency of relevance judgements. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR '11, pages 1063­1072, New York, NY, USA, 2011. ACM.
[26] J. P. Shaffer. Multiple hypothesis testing. Annual Review of Psychology, 46(1):561­584, 1995.
[27] M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, CIKM '07, pages 623­632, New York, NY, USA, 2007. ACM.
[28] J. Sunklodas. Approximation of distributions of sums of weakly dependent random variables by the normal distribution. In Y. Prokhorov and V. Statulevicius, editors, Limit Theorems of Probability Theory, pages 113­165. Springer Berlin Heidelberg, 2000.
[29] J. Tague-Sutcliffe and J. Blustein. A statistical analysis of TREC-3 data. In Overview of the Third Text REtrieval Conference (TREC-3), pages 385­398, 1994.
[30] J. Urbano, J. S. Downie, B. Mcfee, and M. Schedl. How significant is statistically significant? the case of audio music similarity and retrieval. In Proceedings of the 13th International Society for Music Information Retrieval Conference, pages 181­186, Porto, Portugal, October 8-12 2012.
[31] W. Webber, A. Moffat, and J. Zobel. Statistical power in retrieval experimentation. In Proceedings of the 17th ACM conference on Information and knowledge management, CIKM '08, pages 571­580, New York, NY, USA, 2008. ACM.
[32] P. H. Westfall and J. F. Troendle. Multiple testing with minimal assumptions. Biometrical Journal, 50(5):745­755, 2008.
[33] P. H. Westfall and S. S. Young. Resampling-Based Multiple Testing: Examples and Methods for p-Value Adjustment. Wiley-Interscience, 1 edition, Jan. 1993.
[34] W. J. Wilbur. Non-parametric significance tests of retrieval performance comparisons. J. Inf. Sci., 20:270­284, April 1994.
[35] H. Xu and J. C. Hsu. Applying the generalized partitioning principle to control the generalized familywise error rate. Biometrical Journal, 49(1):52­67, 2007.
[36] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '01, pages 334­342, New York, NY, USA, 2001. ACM.
[37] J. Zhou, D. P. Foster, R. A. Stine, and L. H. Ungar. Streamwise feature selection. Journal of Machine Learning Research, 7:1861­1885, 2006.
[38] J. Zobel, W. Webber, M. Sanderson, and A. Moffat. Principles for robust evaluation infrastructure. In Proceedings of the 2011 workshop on Data infrastructures for supporting information retrieval evaluation, DESIRE '11, pages 3­6, New York, NY, USA, 2011. ACM.

412

Preference Based Evaluation Measures for Novelty and Diversity

Praveen Chandar and Ben Carterette {pcr,carteret}@udel.edu
Department of Computer and Information Sciences
University of Delaware
Newark, DE, USA 19716

ABSTRACT
Novel and diverse document ranking is an effective strategy that involves reducing redundancy in a ranked list to maximize the amount of novel and relevant information available to users. Evaluation for novelty and diversity typically involves an assessor judging each document for relevance against a set of pre-identified subtopics, which may be disambiguations of the query, facets of an information need, or nuggets of information. Alternately, when expressing a preference for document A or document B, users may implicitly take subtopics into account, but may also take into account other factors such as recency, readability, length, and so on, each of which may have more or less importance depending on user. A user profile contains information about the extent to which each factor, including subtopic relevance, plays a role in the user's preference for one document over another. A preference-based evaluation can then take this user profile information into account to better model utility to the space of users.
In this work, we propose an evaluation framework that not only can consider implicit factors but also handles differences in user preference due to varying underlying information need. Our proposed framework is based on the idea that a user scanning a ranked list from top to bottom and stopping at rank k gains some utility from every document that is relevant their information need. Thus, we model the expected utility of a ranked list by estimating the utility of a document at a given rank using preference judgments and define evaluation measures based on the same. We validate our framework by comparing it to existing measures such as -nDCG, ERR-IA, and subtopic recall that require explicit subtopic judgments We show that our proposed measures correlate well with existing measures while having the potential to capture various other factors when real data is used. We also show that the proposed measures can easily handle relevance assessments against multiple user profiles, and that they are robust to noisy and incomplete judgments.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2034-4/13/07 ...$15.00.

Categories and Subject Descriptors: H.3 [Information Storage and Retrieval]; H.3.4 [Systems and Software]: Performance Evaluation
Keywords: Novelty and Diversity, Evaluation
1. INTRODUCTION
The concept of relevance is the probably the most critical aspect of theoretical and practical information retrieval (IR) models. But which documents are relevant can differ from user to user depending on their exact information need, even if they start with the same keyword query. Queries can be ambiguous and/or underspecified, and the retrieval systems are required to handle these diverse information needs while providing novel information. Traditional IR evaluation also works under the assumption that documents are independently relevant separate from any user context The major drawback with this approach is that it does not penalize redundancy in rankings, potentially reducing the amount of novel information available to the user. Recently a subtopic based approach was introduced, to handle the redundancy problem and account for diverse information needs. The underlying information need for a query is decomposed into set of subtopics, and the number of novel subtopics that a document is relevant to (i.e. not seen earlier in the ranking) provides a measure of novelty. Various evaluation measures have been defined based on this approach [1, 9, 23, 27].
While subtopics are used to account for the diverse information needs of a query, the relation between them varies from user to user. For example, consider the query living in India. A person planning to visit India could be interested in information for visitors and immigrants & how people live in India whereas a student writing an essay would be more interested in the history about life and culture in India. Even though all of these subtopics seem relevant to the query, the importance of a subtopic is dependent on the user and the scenario in which the search was performed. It is well-known that user preferences are influenced not only by topical relevance but also by other factors such as readability, subtopic importance, completeness, etc. User profiles can be used to represent the combination of relevant subtopics and the above mentioned factors that precisely reflects the user's information need. Currently, there is no evaluation measure that (a) takes into account various factors affecting user preference, (b) handles multiple user profiles for a given query.
In this work, we propose an evaluation framework and metrics based on user preference for the novelty and diversity task. The framework revolves around the idea of assigning

413

utility scores that reflect each set of user`s preference towards each document. The document utilities are estimated using a series of preference judgments collected conditional on previously ranked documents. Document utility at a given rank implicitly accounts for the subtopic coverage, novelty, topical relevance and the other factors as well. As pointed out earlier, the utility of document could differ for each user, thus user preference are obtained across a pool of users to account for diverse information need of a query. Evaluation metrics defined based on this framework directly models a user traversing a ranking from top to bottom seeking relevant and novel information for the issued query. Therefore, our proposed measures estimate the total utility of a ranked list available to the user for a given query.
The rest of the paper is organized as follows: a detail explanation of the existing evaluation framework and the existing metrics for novelty and diversity is provided in Section 2. We point out issues with the current method and propose preference-based evaluation measures in Section 3. A description of the datasets along with the experimental design employed in our work can be found in Section 4. We analyze in detail the performance of our metrics and compare it to various existing ones in Section 5. Finally, Section 6 summarizes our findings and sketches our future directions.
2. NOVELTY/DIVERSITY EVALUATION
Search result diversification is an effective strategy to deal with the diverse information needs of the user while reducing redundancy in the ranked list [19, 28, 25]. Several methods have been proposed to produce a ranking that maximizes relevance with respect to multiple information needs for a given query, starting with the maximum marginal relevance model of Carbonell et al. [4]. In addition to new models, the task demands new evaluation metrics, as traditional IR measures are focused on relevance with respect to a single user and do not penalize redundancy in results. Zhai et al. studied the subtopic retrieval task in the context of the TREC Interactive track [17], and defined simple evaluation measures such as subtopic recall and subtopic precision based on the relevance of documents to pre-defined subtopics. Clarke et al. proposed an evaluation strategy that decomposes underlying information needs of a query into information nuggets; document utility is determined by the number of novel nuggets covered by the document. NRBP, also introduced by Clarke et al. combines ideas from -nDCG and Rank-Biased Precision [12]. Agarwal et al. focused on the diversity problem in the web domain by taking into account the importance of user intents via a probability distribution. Each of these measures will be described in more detail below.
Almost all of the existing measures are based on the idea of explicit subtopics: decompositions of a given query into several pieces of information (such as facets, intents, or nuggets) that account for various underlying information needs. In this framework, novelty is solely dependent on the document's relevance to a subtopic. System effectiveness is estimated by iterating over the ranked list, penalizing relevant documents relevant to subtopic(s) seen earlier in the ranking, and rewarding documents relevant to unseen subtopic(s).
2.1 Test Collection
Test collections such as those produced for the TREC Interactive tracks [17] and the TREC Question Answering tracks [26] consist of subtopic-level judgments in documents.

The TREC Web track diversity datasets created to study the problem of novelty and diversity are most suitable to our work. These datasets comprise a set of topics, and for each topic a set of subtopics that were identified semiautomatically with the help of a tool that clusters reformulations of the given query. The tool combined evidences from clicks and reformulations to obtain clusters of queries; the track organizers used these clusters to manually pick the set of subtopics for a given target query.
Binary judgments of relevance were made by NIST assessors for each subtopic to each document. Note that the use of this method means that only subtopics evidenced by a large number of users will be present in the data; interpretations that are equally "real" yet less popular will not be represented when this method is used.
2.2 Evaluation Measures
Evaluation measures for novelty and diversity must account for both relevance and novelty in rankings. It is important that redundancy caused by documents containing previously retrieved information are penalized while documents containing novel information are rewarded; as described above, this is achieved using subtopic relevance judgments. A brief description of the commonly used metrics that employ a subtopic based approach is given below:

Subtopic recall measures the proportion of unique subtopics retrieved at a given rank [27]. Given that a query q has m subtopics, the subtopic recall at rank k is given by the ratio of number of unique subtopics contained by the subset of document up to rank k to the total number of subtopics m.

Sk  i=1

 subtopics(di)

S-recall@k =

(1)

m

-nDCG scores a result set by rewarding newly found subtopics and penalizing redundant subtopics [13]. Computation of the gain vector and a rank discount are key to -nDCG. The gain vector is computed by summing over subtopics appearing in the document at rank i:

m

G[i] = X(1 - )cj,i-1

(2)

j=1

where cj,i is the number of times subtopic j has appeared
in documents up to (and including) rank i.
The most commonly used discount function is log2(1 + i), although other discount functions are possible. Summing
gains over discounts gives discounted cumulative gain:

k
X -DCG@k =

G[i]

(3)

i=1 log2(1 + i)

-DCG must be normalized to compare the scores against various topics. This is done by finding an "ideal" ranking that maximizes -DCG, which can be done using a greedy algorithm. The ideal ranking computation is an NPComplete problem [5]. The ratio of -DCG to that ideal gives -nDCG.

Intent-aware family Agrawal et al. [1] studied the problem of evaluating ambiguous web queries. They proposed evaluating a ranking against each subtopic (or "intent") by

414

any traditional IR measure, and then combining the results based on importance of subtopic. This gave rise to a family of measures that are known as intent-aware. Most traditional measures such as precision@k, average precision (AP), nDCG, etc. can be cast as intent-aware versions; for instance, intent-aware AP would be expressed as:

m

AP -IA = X P (i|q)APi

(4)

i=1

where m is the number of intents/subtopics, P (i|q) is the probability that the user is interested in intent i for query q, and APi is average precision computed only with the documents relevant to intent i.

ERR-IA Expected Reciprocal Rank (ERR) is a measure based on "diminishing returns" for relevant documents [10]. According to this measure, the contribution of each document is based on the relevance of documents ranked above it. The discount function is therefore not just dependent on the rank but also on relevance of previously ranked documents.

ERR

=


X

1 i

Ri

i-1
Y(1

-

Rj )

(5)

i=1

j=1

where Ri is a function of the relevance grade of the document at rank i (typically defined to be (2g - 1)/2gmax). ERR-IA is defined exactly as other intent-aware measures: a weighted average of ERR computed separately for each subtopic/intent [9]. We mention it separately because it has some appealing mathematical properties and it is one of the official measures of the TREC Web track [9].

D-Measure The D and the D# measures described by Sakai et al. [22] aim to combine two properties into a single evaluation measure. The first property is to retrieval documents covering as many intents as possible and second is to rank documents relevant to more popular intents higher than documents relevant to less popular intents.

3. PREFERENCE BASED FRAMEWORK
The subtopic-based evaluation framework focuses on estimating the effectiveness of a system based on topical and sub-topical relevance. In practice, there may be many other factors such as reading level, presentation, completeness, etc. that influence user preferences for one document over another in the context of novelty and diversity [8]. We could describe the information needs of a user that consists of various details, including specifics of pieces of information the user is interested in, reading level of the user, and so on in a user profile. Then we could view the goal of an evaluation measure as determining how well a ranking of documents satisfies a variety of user profiles.
In order to understand the concept of user profiles, let us consider an example query from the TREC Web track: air travel information. Table 1 shows the subtopics defined for the Web track's diversity task and provides the information needs of three different possible users for the given query (assuming we restrict ourselves to the TREC paradigm and represent the user's information need using only subtopics). We can think of user A as a first time air traveler looking for information on air travel tips and guidelines, user B as a journalist writing an article on the current quality of air travel and looking for statistics and reports to accomplish

the task, and user C as an infrequent traveler looking restrictions and rules for check-in and carry-on luggages. Therefore, user A's profile for the above example query consists of subtopics d and e, user B's of c, and user C's of a and b. (In practice, the profiles would typically take into account other factors such as presentation, readability, and other factors as well, but none of this need be made explicit.)
Even if we restrict ourselves to modeling only subtopics, there are some issues with existing measures based on subtopics:
(a) subtopic identification is challenging and tricky as it is not easy to enumerate all possible information needs for a given query,
(b) measures often require many parameters to be set before use,
(c) measures assume subtopics to be independent of each other but in reality this is not true.
Let us refer to Table 1 to consider these issues. First, given the granularity of these subtopics, it would not be difficult to come up with additional subtopics that are not in the data. Top-ranked results from a major search engine suggest subtopics such as "Are airports currently experiencing a high level of delays and cancellations?", "I am disabled and require special consideration for air travel; help me find tips.", and "My children are flying alone, I am looking for tips on how to help them feel comfortable and safe." Are users with these needs going to be satisfied by a system that optimizes for the limited set provided?
Second, measures like -nDCG and ERR-IA have a substantial number of parameters that must be decided on. Some are explicit, such as  (the penalization for redundancy) [15] or P (i|q) (the probability of an intent/subtopic given a query1). Others are implicit, hidden in plain sight because they have "standard" settings: the log discount of nDCG or the grade value Ri of ERR-IA, for instance. Each of these parameters requires some value; it is all too easy to fall back on defaults even when they are not appropriate.
Third, some subtopics are clearly more related to each other than others (in fact, we used this similarity to create the profiles). Documents that are relevant to subtopic c are highly unlikely to also be relevant to any of the other subtopics, but it is more likely that there are pages relevant to both subtopics a and b.
In this work, we sidestep these issues by proposing an evaluation framework that simply allows users to express preferences between documents. Their preferences may be based on topical or subtopic relevance, but they may also be based on any other factors that are important to them. Preferences can be obtained over many users to capture the varying importance of topics and factors, and when a sufficiently large set of preferences has been obtained, systems can be evaluated according to how well they satisfy those users. Preference judgments have only scantly been used in IR evaluation, having been introduced by Rorvig [20] but not subject to empirical study until recently [7, 2]. Comparison studies between absolute and preference judgments show that preference judgments can often be made faster than graded judgments, with better agreement between assessors (and more consistency with individual assessors) [7] while making much finer distinctions between documents.
1The original definition of -nDCG has parameters for subtopic weights as well.

415

subtopic a. What restrictions are there for checked baggage during air travel?

user A user B user C 

b. What are the rules for liquids in carry-on luggage?



c. Find sites that collect statistics and reports about airports



d. Find the AAA's website with air travel tips.



e. Find the website at the Transportation Security Administration (TSA)  that offers air travel tips.

Table 1: An example topic (air travel information) along with its subtopics from the TREC Diversity dataset and three possible user profiles indicating the interests of different users.

Chandar and Carterette [8] introduced a preference-based framework similar to ours, but there exists no evaluation measure that incorporates preference judgments directly for novelty and diversity. Moreover, that work focused only on ranking novel documents, without considering the more general question of diversity--that different users will have different preferences depending on their profile.
3.1 Test Collection
Chandar and Carterette's preference-based framework is based on so-called levels of preference judgments. We use a similar idea; in this work, a test collection of preferences for novelty and diversity consists of two different types of preference judgments:
1. simple pairwise preference judgments, in which a user is shown two documents and asked which they prefer.
2. conditional preference judgments, in which a user is shown three or more documents and asked to express a preference between two of them, supposing they had read the others.
Simple pairwise preferences produce a relevance ranking: given a pair of documents, assessors select the preferred document based on some criteria. We expect topical relevance to be the primary criteria, although many criteria (such as ease of reading, completeness of information, salience of article, etc.) could factor into an assessor's choice. Since different users may have different needs and different preferences for the same query, pairs can be shown to multiple assessors to get multiple preferences. Over a large space of assessors, we would expect that documents are preferred proportionally according to the relative importance of the subtopics they are relevant to, with various other factors influencing finer-grained orderings.
Simple pairwise preferences cannot capture novelty; in fact, two identical documents should be equally preferred in all pairs in which they appear and therefore end up tied in the final ordering. Conditional preference judgments attempt to resolve this by asking for a preference for a given pair of document conditional on the information in other documents shown to the assessor at the same time. The assessor is asked to read those documents, then select which of the remaining two they would like to see next.
Figure 1 illustrates conditional preferences with a triplet of documents: the assessor would read document X, then select which of A or B they would like to see next2 We
2Note that any document may be placed at the top of a triplet; it need not be the most preferred document among the simple pairwise preferences.

Figure 1: Left: a simple pairwise preference for which an assessor chooses A or B. Right: a triplet of documents for conditional preference judgments. An assessor would be asked to choose A or B conditional on having read X.
expect the assessor's choice to be based not only on topical relevance, but also on the amount of new information given what is provided in the top document. Again, they can use other factors in their preferences, but novelty should be a primary consideration: if X is identical to A, we expect them to choose B, and then a system that ranks X and A adjacent would be penalized for failing to rank B after X.
Similarly, we could obtain preferences with quadruplets of documents, quintuplets of documents, and so on. In practice it becomes increasingly difficult for assessors to make such fine distinctions, so we limit to only obtaining judgments on triplets. A triplet in our framework corresponds to Chandar and Carterette's "level 2" judgments; as they showed, these judgments capture most of the necessary information about novelty. Preferences conditional on greater numbers of other documents contribute less and less [8].
3.2 Preference-Based Evaluation Measure
We propose a model-based measure using preferences to assess the effectiveness of systems for the novelty and diversity task. Model based measures can be composed from three underlying models: browsing model, document utility, and utility accumulation [6]. The way users interact with the ranked list is defined by the browsing model; we rely on the most accepted model in which the user scans documents down a ranked list one-by-one and stops at some rank k. The document utility model defines the amount of utility provided by a single document, and utility accumulation models the total utility derived during browsing.
We define our utility based model for novel and diversity ranking task as follows: a user scanning documents down a ranked list derives some utility U (d) from each document and stops at some rank k. We hypothesize that the utility of a document at rank i is dependent on previously ranked document (i.e. d1 to di-1). Given a probability distribution for a user stoping at rank k, the utility accumulation model

416

can be defined as:

n

X

P rf = P (k)U (d1, ..., dk)

(6)

k=1

where P (k) is the probability that a user stops at rank k and U (d1, ..., dk) is the total utility of the documents from ranks 1 through k.
We simplify this by formulating U (d1, ..., dk) as a sum of individual document utilities conditional on documents ranked before:

n

k

P rf = X P (k) X U (di|S)

(7)

k=1

i=1

where P (k) is the probability that a user stops at rank k, U (di|S) gives the utility of the document at rank i conditional on a set of previously ranked document S, and the sum from i = 1 to k gives the total utility of all documents from ranks 1 through k. There are two main components in the above equation: the probability that a user stops at a given rank (P (k)) and the utility of a document conditioned of previously ranked documents (U (di|S)). Carterette demonstrated different ways to model the stopping rank from the various ad-hoc measure such as Rank Biased Precision [16], nDCG, and Reciprocal Rank [6].

1. PRBP (k) = (1 - )k-1

2.

PDCG(k) =

1 log(k+1)

-

1 log(k+2)

3.

PRR(k) =

1 k(k+1)

Finally, we define the document utility model in which the document utility at a given rank is conditioned on previously ranked documents. The utility of the document at rank i is given by U (di) for i = 1 since at rank 1 the user would not have seen any other documents and therefore would not be conditioning on any other documents. For subsequent ranks, utility is U (di|di-1, ...d1), indicating that the utility depends on documents already viewed.
Now our goal is to estimate these utilities using preference judgments. Since we have simple pairwise preferences and conditional preferences in triplets, we decompose the document utility model as follows:

8 >U (di), <
U (di|S) = U (di|di-1),

if i is 1

if i is 2

(8)

>:F ({U (di|dj)}ij-=11), if i > 2

where the function F () takes an array of conditional utilities (U (di|dj )).
The utility U (di) can be directly obtained using the pairwise judgments; we simply compute it as the ratio of number of times a document was preferred to the number of times it appeared in a pair. The utilities U (di|di-1) can similarly be obtained from the conditional preferences, computed as the ratio of the number of times di was preferred conditional on di-1 appearing as the "given" document to the number of times it appear with di-1 as the "given" document. Note that these utilities can be computed regardless of how many times a document has been seen, how many different assessors have seen it, how much disagreement there is between assessors, and so on. Although, a document must be shown

at least few time in order to determine its relevance estimate. An estimate of the document's utility is obtain using the ratio of number of times the document was preferred to the number of time it was shown.
We experiment with two functions for F (): average and minimum. The intuition behind these functions can be explained with the help of an example. Consider a ranking R = {d1, d2, d3}. According to equation 8 the utility of d3 depends on U (d3|d1) and U (d3|d2). The minimum function assumes that d3 cannot be any more useful conditional on both d1 and d2 than it is on either one separately, thus giving a sort of worst-case scenario. The average function assumes that the utility of d3 conditional on both d1 and d2 is somewhere in between its utility conditioned on each separately, giving d3 some benefit of the doubt that it may contribute something more when appearing after both d1 and d2 than it does when appearing after either one on its own.
Our measure as defined is computed over the entire ranked list. In practice, measures are often computed only to rank 5, 10, or 20 (partially because relevance judgments may not be available deeper than that). When we compute the measure to a shallower depth, we must normalize it so that it will average over a set of queries. As a final step in the computation of nP rf , we normalize equation 7 cut off at rank K by the ideal utility score.

nP rf [K]

=

P rf [K] I-P rf [K]

(9)

where I-P rf [K] is the ideal utility score that could be obtained at rank K. This can be obtained by selecting the document with the highest utility value conditioned on previously ranked documents. Document (d1) with the highest utility value takes rank 1 and the document with highest utility when conditioned on d1 takes rank 2 and so on.
Table 2 provides an example showing the distinction between our preference based measure and -nDCG based on the user profiles in Table 1. The document utilities are estimated by obtaining the preference judgements for all documents from all three users. We would expect the users' preferences to be consistent with their information need, for example user A would prefer d1 and d2 consistently to other documents that are not relevant to their needs (but relevant to other needs). Notice that -nDCG weighs all subtopics equally but the preference measure takes into account the dependency between the subtopics.

4. EXPERIMENT DESIGN
In Section 3.2, we proposed various evaluation measures based on a user model for novelty and diversity. Evaluation of the proposed metrics is challenging since there is no ground truth to compare to; there are only other measures. Approaches used in the past to validate newly introduced metrics include comparing the proposed measure to existing measures or click metrics [18, 11]; using user preferences to compare the metrics [24]; and evaluating the metric on various properties such as discriminative power [21]. While each of these approaches have their own advantages, we argue that comparison of existing measures to our measures using simulated data is suitable for this work.
Remember, our goal is to build evaluation measures for our preference based framework that assigns utility scores to a document based on user preferences. In reality, user preferences are based on various implicit factors that include

417

documents

a

subtopics bcd

e

user A

d1 d2

 

user B

d3 d4

 

user C

d5 d6

 

List1 d1 d2 d3 1.0 0.9

List2 d1 d3 d5 1.0 1.0

-nDCG Preference Measure

Table 2: Synthetic example with 6 documents and 5 subtopics. The first ranked list does not satisfy all users where as the second one does but both rankings are scored by equally by -nDCG, while the preference metrics are able to distinguish the difference.

subtopic relevance as well as many other properties. Since prior work [8] has suggested that presence of subtopics in a document plays a major role in user preferences, we believe it is important to validate our measures when user preferences are based solely on subtopic information. We therefore rely on the existing data with subtopic information to simulate user preferences.
4.1 Data
In our experiments, we used the ClueWeb09 dataset3 consisting of one billion web pages (5 TB compressed, 25 TB uncompressed), in ten languages, crawled in January and February 2009. A subset of this collection with only English documents was used for the diversity task at TREC in 2009/10/11 [14]. A total of 150 queries have been developed and judged for the TREC Web track; the number of subtopics for each ranges from 3 to 8. For the diversity task, subtopic level judgments are available for each subtopic indicating the relevance of a document to each subtopic along with the general topical relevance. We also acquired the experimental runs submitted to TREC each year by Web track participants. A total of 48 systems were submitted by 18 groups in 2009, 32 system by 12 groups in 2010, and 62 systems by 16 groups in 2011.
4.2 Simulation of Users and Preferences
In order to verify and compare our metrics against existing measures, we acquire preferences by simulating them from subtopic relevance information. These will be based on the preferences of simulated users that are modeled by groupings of subtopics (as in Table 1). In this way we use only data that is provided as part of the TREC collection, and therefore achieve the fairest and most reproducible possible comparison between evaluation measures. In reality, our measure is well-suited for crowd-sourced assessments in a way that other measures are not, but we save that experiment for future work.
We created our user profiles by generating search scenarios for each query and marking subtopics relevant to the scenario. In Section 3, we explained our reasoning behind the user profiles in Table 1 for the query air travel information; we use the same approach to obtain the user profiles for all TREC queries. The user profiles were created by the authors of this paper and have been made available for public download at http://ir.cis.udel.edu/~ravichan/ data/profiles.tar. In addition, there is a mega-user that we refer to as the "TREC profile"; this user is equally interested in all subtopics.
3http://lemurproject.org/clueweb09.php

These profiles are used to determine the outcome of preferences. For simple pairwise preferences, we always prefer the document with greater number of subtopics relevant to the user profile. In the case of a tie, we make a random choice between the left or right document. For conditional preferences, we have three documents (left, right, and top); between the left and the right, we prefer the document that contains the greater number of subtopics relevant to the user profile and not present in the top document. Preference judgments obtained this way are used to compute our preference measure. Finally, using the "TREC profile" to simulate preferences for our measure offers the most direct comparison to other measures.
5. ANALYSIS
We have presented a family of preference-based measures for evaluating systems based on novelty and diversity, and outlined the advantages of our metrics over existing subtopicbased measures. In this section, we demonstrate how our metrics take into account the presence of subtopics implicitly by comparing them with -nDCG, ERR-IA, and s-recall.
5.1 System Ranking Comparisons
5.1.1 System Performance
We evaluated all experimental runs submitted to TREC in 2009, 2010, and 2011 using our proposed measure with three different stopping probabilities P (k) and two different utility aggregation functions F (). Figure 2 shows the performance of systems with respect to both -nDCG and our preference measure computed with PRBP (k) and Favg() functions and preferences simulated using the "TREC profile". Each point represents a TREC participant system; they are ordered on the x-axis by -nDCG. Black circles give -nDCG values as computed by the ndeval utility used for the Web track; blue x's indicate the preference measure score for the same system. In these figures we can see that the preference measure is roughly on the same scale as -nDCG, though typically 0.1 - 0.2 lower in an absolute sense.
Each increase or drop in the position of x's indicates disagreement with -nDCG. The increasing trend of the curves in Figure 2 indicates that the correlation between the preference measure and -nDCG is high. A similar trend was observed while using different P (k) and F () functions as well (not shown). Both -nDCG and our preference measure agree on the top ranked system in 2009 and 2010.
We analyzed the reason behind disagreement by carefully looking at the actual ranked lists. We investigated how nDCG and our proposed measures reward diversified sys-

418

ERR-IA@20 s-recall@20

-nDCG@20

0.893

0.828

ERRIA@20

-

0.739

Table 3: Kendall's  correlation values between the existing evaluation measures. Values were computed using 48 submitted runs in TREC 2009 dataset.

tems on a per topic basis. Based on our analysis, the major reason for disagreement is that -nDCG penalizes systems that miss documents containing many unique subtopics more harshly than the preference measure does. Much of the variance in -nDCG scores is due to differences in rank position of the documents with the greatest number of unique subtopics. In practice, this explains the lower scores returned by the preference measure as well.
5.1.2 Rank Correlation Between Measures
We measure the stability of our metrics using Kendall's  by ranking the experimental runs under different effectiveness measures. Kendall's  ranges from -1 (lists are reversed) to 1 (lists are exactly the same), with 0 indicating essentially a random reordering. Prior work suggest that a  value of 0.9 or higher between a pair of rankings indicates high similarity between rankings while a value of 0.8 or lower indicates significant difference [3].
Figure 3 summarizes the rank correlations between existing subtopic-based metrics and our proposed preference metric using all three P (k) (plus using no P (k) at all-- equivalent to a uniform stopping probability) and both F () functions, simulating preferences with the "TREC profile". The correlations are fairly high across TREC datasets, P (k) functions, and F () functions. The PDCG(k) rank function fares worst, with correlations dipping quite a bit for the 2010 data in particular. Subtopic recall is a very simple non-rank based metric for diversity and thus the Kendall's  values are expected to be slightly lower.
For comparison, Table 3 shows the Kendall's  correlation values between -nDCG, ERR-IA and s-recall. These correlations are similar to those in Figure 3, suggesting that the ranking of systems given by our preference measure varies no more than the rankings of systems given by any two standard measures.
There is almost no difference between the correlations for Favg() and Fmin() functions for aggregating utility. In fact, the correlation between preference measures computed with those two is nearly 1. Thus we can conclude that the choice of F () (between those two options) does not matter. There is a great deal of difference depending on choice of P (k), however, and thus this is a decision that should be made carefully based on the observed behavior of users.
5.2 Evaluating Multiple User Profiles
The experiments above are based on the "TREC profile", a user profile that considers every subtopic to be equally relevant. In this experiment, we demonstrate the ability of our methods to handle multiple, more realistic user profiles and show the stability of our metrics. Measures based on absolute subtopic judgments cannot naturally incorporate multiply-judged documents. One must average judgments, or take a majority vote, or use some other scheme. In contrast, judgments from multiple users can be incorporated

easily into our preference framework in the estimation of document utilities, as the document utility is simply the ratio of number of times a document was preferred to the number of times it appeared in a pair, regardless of which user or assessor happened to see it.
We simulate preferences for each of our user profiles for each topic in the TREC set. We compute the preference measure using each profile's preferences separately (giving at least three separate values for each system: one for each user profile), and then use the full set of preferences obtained to compute a single value of the measure. Note that the latter case is not the same as computing the preference measure with the "TREC profile": the TREC profile user uses all subtopics to determine the outcome of a preference, while individual users would never use a subtopic that is not relevant to them to determine the outcome of a preference.
We can also compute subtopic-based measures such as -nDCG against our profiles. To do this, we simply assume that only the subtopics that are relevant to the profile "count" in the measure computation. We will compare values of measures computed this way to our preference measures.
Our hypothesis for this experiment is twofold: 1) that the preference measure computed for a single profile will correlate well to subtopic-based measures computed against the same profile; 2) that the preference measure computed with preferences from all profiles will not be the same as an average of the individual profile measures, and also not the same as subtopic-based measures computed as usual. In other words, that the preference measure based on preferences from many different users is measuring something different than the preference measure based on preferences from one user, and also different from the subtopic measures.
Figure 4 shows the results of evaluating systems using user profile 1, 2, and 3 for each topic and averaging over topics (note that the user profile number is arbitrary; there is nothing connecting user profile 1 for topic 100 to user profile 1 for topic 110). We can see that the system ranking changes for both -nDCG and the preference measure, as expected. The correlation between the two remains high: 0.83, 0.88, and 0.82 for user profile 1, 2, and 3 respectively. This is in the same range of correlation values that we saw in Figure 3, and supports the first part of our hypothesis.
Figure 5 shows the results of evaluating systems with all user profiles, comparing to the evaluation with the TREC profile and with -nDCG computed with all subtopics. Note here that all three rankings are different, as evidenced by the  correlations reported in the inset tables. This supports the second part of our hypothesis: that allowing many different users the opportunity to express their preferences can result in a different ranking of systems than treating all assessors as equivalent, as the TREC profile and -nDCG do.
5.3 Incomplete Judgments
The test collection procedure discussed in Section 3.1 requires two sets of judgments: pairwise and conditional preferences. The number of pairwise judgments increases quadratically with increase in number of documents in the pool; it is not feasible to collect a complete set of preferences. We envision that our measure would always be computed with incomplete judgments. For this experiment we test the stability of our measures by comparing the system rankings obtained by using all preference judgments against a set of incomplete judgments.

419

Figure 2: TREC 09/10/11 diversity runs evaluated with our preference based metric at rank 20 (nPrf@20) with PRBP and FAverage. Compare to -nDCG scores.

Figure 3: Kendall's  correlation values between our proposed measures and -nDCG, ERR-IA, s-recall. Values were computed using the submitted runs in the TREC 2009/10/11 dataset. The scores for various P (k) and F() are shown.

To do this, we randomly select N triplets of documents for each query. For each triplet, one document is randomly selected to be the "top" document that the other two would be judged conditional on. Though we do not explicitly obtain simple pairwise preferences, we expect that there will

be enough cases in which the top document is not relevant to the user profile that they must fall back on a simple pairwise comparison. We then sample 5 user profiles (with replacement) from those defined for the topic and simulate their preferences for the triplet. In this way we obtain 5N prefer-

420

Figure 5: Comparison between -nDCG, our preference measure computed using the TREC profile, and our preference measure computed using a mix of user profiles. Note that all three rankings, while similar, have substantial differences as well.

0.0 0.1 0.2 0.3 0.4 0.5

Performance Scores 0.0 0.1 0.2 0.3 0.4 0.5

TREC 2009

nPrf@20 G -nDCG@20

Kendall's Tau = 0.775

G

G

G

G

G

G

G

GG
G G

G

G

GGGG

G

G

G

G

GG

G

GGGGGGG

GG

GGG

G

G

G

G

G

GG

G

G

G G

G

0

10

20

30

40

nPrf@20 G -nDCG@20

Kendall's Tau = 0.828

G G GGGG

G

GGG

G

G

G

G

G

G G

GG

GGG

G

G

G

GGGGGGG

G

G

G

G

G

G

G

G

G

G

G

G

G

GG

G

0

10

20

30

40

nPrf@20 G -nDCG@20

Kendall's Tau = 0.816

G GG

G

G

GG

G

G

G

G

G

G

G

G

G G

G

G

G

G

G

G

G

G

GGGGGG

G

G

G

G

G

G

G

G

G

G

G

G

G

G

G

G

G

0

10

20

30

40

Systems ordered by -nDCG@20 using TREC QRELS

0.0 0.1 0.2 0.3 0.4 0.5

Figure 4: Comparison between -nDCG and our preference measure computed against user profiles 1 (top), 2 (middle), and 3 (bottom) for TREC 2009 systems.

ences for each topic in a similar way as would be done in a real crowd-sourced assessment. We use those preferences to compute our measure, then compute the correlation to the measure computed with all available preferences. We repeat this 10 times for each topic, measure the correlation each time, and average the correlations.
Figure 6 shows the correlation between the system rankings when evaluated using complete judgements and increasing numbers of preferences. Correlation tends to increase as the number of preferences increases, though it does not reach 0.9. This may be partly because user profiles are not evenly represented in the preferences (which is in fact more realistic than when they are, as in the full-preference case), and

Kendall's Tau

0.4

0.5

0.6

0.7

0.8

0.9

TREC 09/10/11
TREC 09 TREC 10 TREC 11 500 1000 1500 2000 3000 3000 4000 5000 6000 7000 Number of Sampled Triplets
Figure 6: TREC 09/10/11 diversity runs evaluated with our preference based metric at rank 20 (nPrf@20) with PRR and FMinimum using single assessor with complete judgments and multiple assessor with incomplete judgments.
partly because our max number of preferences is still a fairly small fraction of the total number possible: even selecting triplets from only 100 documents, there are over 161,000 possible triplets, of which we have only obtained less than 5%! Thus we expect that continuing to increase the number of triplets would continue to push the correlations higher, even though we see dips in the trend (due to variance).
6. CONCLUSION AND FUTURE WORK
In this work, we proposed a novel evaluation framework and a family of measures for IR evaluation. Our measure incorporates novelty and diversity, but can also incorporate any property that influences user preferences for one document over another. Our measure is motivated directly by

421

a user model and has several advantage over the existing measures based on explicit subtopic judgments: it captures subtopics implicitly and at finer-grained levels, it accounts for subtopic importance and dependence as expressed by user preferences, and it requires few parameters--only a stopping probability function, for which there are several well-accepted options that can be chosen from by comparing to user log data. It correlates well with existing measures, but also clearly measures something different (which is a positive for a new measure).
This framework and measure is most well-suited for assessments done by crowd-sourcing. In a crowd-sourced assessment, we would naturally have a large user base with a wide range of preferences. Over a large number of preferences, the most important subtopics and intents would naturally emerge; documents relevant to those would become the documents with the highest utility scores. Yet the conditional judgments would prevent too many documents with those subtopics from reaching the top of the ranking. The measure is designed to handle multiple judgments, disagreements in preferences, and novelty of information, and as such it is novel to the information retrieval literature.
The clearest direction for future work is to perform an actual crowd-sourced assessment and determine whether our preference measure correlates better with human judgments of system performance than other measures. We plan to start this immediately. Another direction for future work is using triplets in a learning-to-rank algorithm to learn a novelty ranker. Since many learning algorithms are based on pairwise preferences, it seems a natural extension to triplets.
Acknowledgments: This work was supported in part by the National Science Foundation (NSF) under grant number IIS-1017026. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
7. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. Proceedings of WSDM '09, page 5, 2009.
[2] J. Arguello, F. Diaz, and J. Callan. Learning to aggregate vertical results into web search results. In Proceedings of CIKM '11, page 201, New York, USA, 2011. ACM Press.
[3] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In Proceedings of SIGIR '04, page 25, New York, USA, 2004. ACM Press.
[4] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. Proceedings of SIGIR '98, pages 335­336, 1998.
[5] B. Carterette. An analysis of np-completeness in novelty and diversity ranking. Information Retrieval, 14(1):89­106, Dec. 2010.
[6] B. Carterette. System effectiveness, user models, and user utility. In Proceedings of SIGIR '11, page 903, New York, USA, 2011. ACM Press.
[7] B. Carterette, P. N. Bennett, D. M. Chickering, and T. Susan. Here or there preference judgments for relevance. In Proceedings of ECIR '08, pages 16­27, 2008.
[8] P. Chandar and B. Carterette. Using preference judgments for novel document retrieval. Proceedings of SIGIR '12, page 861, 2012.
[9] O. Chapelle, S. Ji, C. Liao, E. Velipasaoglu, L. Lai, and S.-L. Wu. Intent-based diversification of web search results: metrics and algorithms. Information Retrieval, 14(6):572­592, May 2011.

[10] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. Proceeding of CIKM '09, page 621, 2009.
[11] C. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A comparative analysis of cascade measures for novelty and diversity. In Proceedings of WSDM '11, pages 75­84. ACM, 2011.
[12] C. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. Advances in Information Retrieval Theory, pages 188­199, 2010.
[13] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. Proceedings of SIGIR '08, page 659, 2008.
[14] C. L. A. Clarke, N. Craswell, I. Soboroff, and E. M. Voorhees. Overview of the trec 2011 web track. In Proceedings of The Eighteenth Text REtrieval Conference TREC, pages 1­9, Gaithersburg, Maryland, 2011. NIST.
[15] T. Leelanupab, G. Zuccon, and J. M. Jose. A query-basis approach to parametrizing novelty-biased cumulative gain. In Proceedings of the Third international conference on Advances in information retrieval theory, ICTIR '11, pages 327­331. Springer-Verlag, 2011.
[16] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Transactions on Information Systems, 27(1):1­27, Dec. 2008.
[17] P. Over. Trec-6 interactive track report. In The Sixth Text Retrieval Conference (TREC-6), pages 57­64, 1998.
[18] F. Radlinski, M. Kurup, and T. Joachims. How does clickthrough data reflect retrieval quality? In Proceedings of CIKM '08, pages 43­52, New York, USA, 2008. ACM.
[19] D. Rafiei, K. Bharat, and A. Shukla. Diversifying web search results. In Proceedings of WWW O~10, pages 781­790, New York, USA, Apr. 2010. ACM.
[20] M. E. Rorvig. The simple scalability of documents. Journal of the American Society for Information Science, 41(8):590­598, Dec. 1990.
[21] T. Sakai. Evaluating evaluation metrics based on the bootstrap. In Proceedings of SIGIR '06, pages 525­532, New York, USA, 2006. ACM.
[22] T. Sakai, N. Craswell, R. Song, S. Robertson, Z. Dou, and C. Lin. Simple evaluation metrics for diversified search results. In Proceedings of the 3rd International Workshop on Evaluating Information Access (EVIA), 2010.
[23] T. Sakai and R. Song. Evaluating diversified search results using per-intent graded relevance. In Proceedings of SIGIR '11, pages 1043­1052. ACM, 2011.
[24] M. Sanderson, M. Paramita, P. Clough, and E. Kanoulas. Do user preferences and evaluation measures line up? In Proceeding of SIGIR '10, pages 555­562. ACM, 2010.
[25] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. Proceedings of WWW '10, page 881, 2010.
[26] E. M. Voorhees and H. T. Dang. Overview of the trec 2005 question answering track. In In TREC 2005, 1999.
[27] C. Zhai, W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proceedings of SIGIR '03, pages 10­17. ACM, 2003.
[28] W. Zheng, X. Wang, H. Fang, and H. Cheng. Coverage-based search result diversification. Information Retrieval, 2011.

422

Utilizing Query Change for Session Search

Dongyi Guan, Sicong Zhang, Hui Yang
Department of Computer Science Georgetown University
37th and O Street, NW, Washington, DC, 20057
{dg372, sz303}@georgetown.edu, huiyang@cs.georgetown.edu

ABSTRACT
Session search is the Information Retrieval (IR) task that performs document retrieval for a search session. During a session, a user constantly modifies queries in order to find relevant documents that fulfill the information need. This paper proposes a novel query change retrieval model (QCM), which utilizes syntactic editing changes between adjacent queries as well as the relationship between query change and previously retrieved documents to enhance session search. We propose to model session search as a Markov Decision Process (MDP). We consider two agents in this MDP: the user agent and the search engine agent. The user agent's actions are query changes that we observe and the search agent's actions are proposed in this paper. Experiments show that our approach is highly effective and outperforms top session search systems in TREC 2011 and 2012.
Categories and Subject Descriptors
H.3.3 [Information Systems ]: Information Storage and Retrieval--Information Search and Retrieval
Keywords
Session search; query change model; retrieval model
1. INTRODUCTION
Session search is the Information Retrieval (IR) task that retrieves documents for a search session [4, 8, 13, 14, 15, 25, 32]. During a search session, a user keeps modifying queries in order to find relevant documents that fulfill his/her information needs. In session search, many factors, such as relevance feedback, clicked data, changes in queries, and user intentions, are intertwined together and make it a quite challenging IR task. TREC (Text REtrieval Conference) 20102012 Session tracks [18, 19, 20] studied session search with a focus on the "current query" task, which retrieves relevant documents for the current/last query in a session based on previous queries and interactions. Table 1 shows examples from the TREC 2012 Session track.1
1All examples mentioned in this paper are from TREC 2012. For simplicity, we use `sx' to refer to a TREC 2012 session where x is the session identification number.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM or the author must be honored. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Table 1: Examples of TREC 2012 Session queries.

session 6 1.pocono mountains pennsylvania 2.pocono mountains pennsylvania hotels 3.pocono mountains pennsylvania things to do 4.pocono mountains pennsylvania hotels 5.pocono mountains camelbeach 6.pocono mountains camelbeach hotel 7.pocono mountains chateau resort 8.pocono mountains chateau resort attractions 9.pocono mountains chateau resort getting to 10.chateau resort getting to 11.pocono mountains chateau resort directions session 85 1.glass blowing 2.glass blowing science 3.scientific glass blowing

session 28 1.france world cup 98 reaction stock market 2.france world cup 98 reaction 3.france world cup 98 session 32 1.bollywood legislation 2.bollywood law session 37 1.Merck lobbists 2.Merck lobbying US policy

From Table 1, we notice that queries change constantly in a session. The patterns of query changes include general to specific (pocono mountains  pocono mountains park), specific to general (france world cup 98 reaction  france world cup 98), drifting from one to another (pocono mountains park  pocono mountains shopping), or slightly different expressions for the same information need (glass blowing science  scientific glass blowing). These changes vary and sometimes even look random (gun homicides australia  martin bryant port arthur massacre), which increases the difficulty of understanding user intention. However, since query changes are made after the user examines search results, we believe that query change is an important form of feedback. We hence propose to study and utilize query changes to facilitate better session search.
One approach to handle query change is to classify them based on various types of explorations [20], such as specification, generalization, drifting, or slight change, then perform retrieval. Another approach is mapping queries into semantic graphical representations, such as ontologies [7] or query flow graphs developed from query logs [2], then studying how queries move in the graphs. However, ontology mapping is challenging [17], which may introduce inaccurate intermediate results and hurt the search accuracy. Moreover, relying on large scale query logs may not be applicable due to lack of such data. Therefore, although these approaches have been applied to IR tasks such as query reformulation [3] and query suggestion [2, 30], they have yet to be directly applied to session search. It is therefore necessary to explore new solutions to utilize query change for session search.
We propose to model session search as a Markov Decision Process (MDP) [16, 28], which is applicable to many human decision processes. MDP models a state space and an action space for all agents participating in the process. Actions from the agents influence the environment/states and the environment/states influence the agents' subsequent actions

453

Figure 1: Session search MDP. (example is from s32)
based on certain policies. A transition model between states indicates the dynamics of the entire system. In our MDP, queries are modeled as states. Previous queries that the user wrote influence the search results; the search results again influence the user's decision of the next query. This interaction continues until the search stops.
As illustrated in Figure 1, we consider two agents in this entire process: the user agent and the search engine agent. The user agent's actions are mainly human actions that are able to change search results, such as adding and deleting query terms, i.e. query change. Clicking is a human action; however, it does not explicitly impact the retrieval. Therefore, it is not considered as a user action here. Query change is the only form of user action in this paper. Based on the user actions, we design corresponding policies for the search engine agent; which is the main focus of this paper.
It is difficult to interpret the user intent [5, 31] behind query change. For instance, for a query change from Kurosawa to Kurosawa wife (s38), there is no indication about `wife' in the search results returned for the first query. However, Kurosawa's wife is actually among the information needs provided to the user by NIST's topic descriptions. Our experience with TREC Session tracks suggests that information needs and previous search results are two main factors that influence query change. However, knowing information needs before search could not easily be achieved. This paper focuses on utilizing evidence found in previous search results and the relationship between previous search results and query change to improve session search.
In this paper, we summarize various types of query changes based on potential user intents into user agent policies. We further propose corresponding policies for the search engine agent and model them in the query change retrieval model (QCM), a novel reinforcement learning [16] inspired framework. The relevance of a document to the current query is recursively calculated as the reward beginning from the starting query and continuing until the current query. This research is perhaps the first to employ reinforcement learning to tackle session search. Our experiments demonstrate that the proposed approach is highly effective and outperforms the best performing TREC 2011 and 2012 session search systems.
The remainder of this paper is organized as follows. Section 2 analyzes query change and summarizes policies for the user agent. Section 3 proposes policies for the search engine agent. Section 4 elaborates the query change retrieval model. Section 5 discusses how to handle duplicated queries. Section 6 evaluates our approach, followed by a discussion in Section 7. Section 8 presents the related work and Section 9 concludes the paper.

Table 2: Evidence that query change q appears in previous

search results Di-1.

qtheme +q

# in TREC'11

 Di-1

/ Di-1

184

20

80

124

# in TREC'12

 Di-1

/ Di-1

178

21

97

102

-q

141

63

112

87

Total

204 adjacent query pairs, 76 sessions

199 adjacent query pairs, 98 sessions

2. USER AGENT: QUERY CHANGE AS A
FORM OF FEEDBACK
We define a search session S = {Q, D, C} as a combination of a series of queries Q = {q1, ..., qi, ..., qn}, retrieved document sets D = {D1, ..., Di, ..., Dn}, and clicked information C = {C1, .., Ci, ..., Cn}, where n is the number of queries in the session (i.e., the session length) and i indexes the queries. In TREC 2010-2012 Session tracks, each retrieved document set Di contains the top 10 retrieval results di1, ..., di10 ranked in decreasing relevance for qi. Each clicked data Ci contains the user-clicked documents, clicking order, and dwell time. For instance, for s6 q6, pocono mountains camelbeach hotel (Table 1), C6 tells us that the user clicked the 4th ranked search result, followed by the 2nd, with dwell time 15 seconds and 17 seconds, respectively.
TREC 2010-2012 Session Tracks aim to retrieve a list of documents for the current query, i.e. the last query qn in a session, ordered in decreasing relevance. Without loss of specificity, we assume that any query between q1 to qn could be the last query. We therefore study the problem of retrieving relevant documents for qi, given all previous queries q1 to qi-1, previous retrieval results D1 to Di-1, and previous clicked data C1 to Ci-1.
We define query change qi as the syntactic editing changes between two adjacent queries qi-1 and qi:

qi = qi - qi-1

qi can be written as a combination of the shared portion between qi and qi-1 and query change: qi = (qi qi-1)+qi.
The query change qi comes from two sources. First, the added terms, which we call positive q, are new terms that the user adds to the previous query. Second, the removed terms, which we call negative q, are terms that the user deletes from the previous query. For example, in Table 1 s37, `US' and `policy' are the added terms; while in s28, `stock' and `market' are the removed terms.
We call the common terms shared by two adjacent queries theme terms since they often represent the main topic of a session. For example, in Table 1 s37 the theme terms are "Merck lobby".2
We thus decompose a query into three parts as theme terms, added terms, and removed terms and write it as:

qi = (qiqi-1)+(+qi)-(-qi) = qtheme+(+qi)-(-qi)

where qtheme are the theme terms, +qi and -qi represent added terms and removed terms, respectively.
Our observations suggest that documents that have been examined by the user factor in deciding the next query change. We therefore propose the following important assumption between qi, the query change between adjacent
2We perform K-stemming to all query terms. For instance, `lobbists' and `lobbying' are both stemmed to `lobby'.

454

Table 3: User agent's policies and actions about a query term t  qi-1. (Refer to sessions shown in Table 1)

t  Di-1

user intention
1. find more information about t 2. satisfied & move to the next information need 3. satisfied

user likes Di-1 user action example

add new terms s85

t about t

q1  q2

remove t & add new terms t as new focus keep t

s6 q8  q9
theme term

type specification
drift
no change

user intention

user dislikes Di-1 user action example

5. remove the wrong remove t

s28

terms

q1  q2

6. not satisfied & move to the next information need 7. try different expression for t

remove t & add new terms t slight change of t to t

s6 q6  q7
s85 q2  q3

type generalization
drift
slight change

4. inspired by add terms t t / Di-1 terms t in Di-1 about t

s37 q1  q2

specification 8. try different ex- slight

s32

pression for t to get change of t q1  q2

more documents for t to t

slight change

queries qi and qi-1, and Di-1, the search results for qi-1:
qi  Di-1.
The assumption basically says that previous search results decide query change. In fact, previous search results Di-1 could influence query change qi in quite complex ways. For instance, the added terms in s37 (Table 1) q1 to q2, are `US' and `policy'. D1 contains several mentions of `policy', such as "A lobbyist who until 2004 worked as senior policy advisor to Canadian Prime Minister Stephen Harper was hired last month by Merck". However, these `policy'-related mentions are about "Canada policy" whereas the user adds "US policy" in q2. This suggests that the user might have been inspired by `policy' in D1, however he preferred the policy in US, not in Canada. Therefore, instead of simply cutting and pasting identical terms from Di-1, the user creates related terms to add for the next search.
In another example, s28 (Table 1) q1, `stock' and `market' are frequent terms that are similar to stopwords. Documents in D1 are hence all about them and totally ignore the theme terms "france world cup 98." In q2, the user removes "stock market" to boost rankings for documents about the theme terms. In this case, removing terms is not only about generalization, but also about document re-ranking.
To provide a convincing foundation for our approach, we look for evidence to support our assumption. We investigate whether qi (at least) appears in Di-1. Table 2 shows how often theme terms, added terms, and removed terms are present in Di-1 for both TREC 2011 and 2012 datasets. Around 90% of the time theme terms occur in Di-1 and most removed terms (>60%) appear in Di-1.3 Added terms are new terms for the previous query qi-1; we thus expect to see few occurrences of added terms in Di-1. Surprisingly, however, more than a third of them appear in Di-1. It suggests that it is quite probable that previous search results motivate the subsequent query change.
Table 3 summarizes various types of query changes into possible policies for the user agent. This table mainly serves as a guide for us to design the policies for the search engine agent. We do not perform a thorough user study to validate this table. However, we believe that it is a good representative of various search scenarios and can help design a good session search agent.
Along two dimensions, Table 3 summarizes the user agent's actions and possible policies. The dimensions are whether a previous query term t  qi-1 appears in previous search
3A third of query terms that do not appear in Di-1 are removed by the user.

results Di-1 (the left most column) and whether the user likes Di-1 and the occurrence of t in Di-1 (the top most row). Combinations of the two dimensions yield 4 main cases (as in a contingency table) and 8 sub-cases. For each case, we identify four items: a rough guess of user intention, the user's actual action, an example, and the semantic exploration type for this action. For example, query change in s6 q8  q9, pocono mountains chateau resort attractions  pocono mountains chateau resort getting to can be interpreted as the following. Previous query term `attractions' appears in Di-1 and the user likes the returned documents Di-1. One possibility is that he is satisfied with what he reads and moves to the next information need. Therefore, the user removes `attraction' and adds new terms "getting to" as the new query focus. This is a drift in search focus. (case 2 in Table 3)
We further group the cases in Table 3 by types of user actions, i.e., query change, and summarize them into:
· Theme terms (qtheme), terms that appear in both qi-1 and qi. In fact, they often appear in many queries in a session. It implies a strong preference for those terms from the user. If they appear in Di-1, it shows that the user favors them since the user issues them again in qi. If they do not appear in Di-1, the user still favors towards them and insists to include them in the new query. This corresponds to t in cases 1 and 3 in Table 3.
· Added terms (+q), terms that appear only in qi, not in qi-1. They indicate specification, destination of drifting, or destination of slight change. If they appear in Di-1, for the sake of novelty [14], they will not be favored in Di. If they do not appear in Di-1, which means that they are novel and the user favors them now. This corresponds to t in cases 1, 2, 6, 7, and 8, and t in case 4 in Table 3.
· Removed terms (-q), terms that appear only in qi-1, not in qi. They indicate generalization, source of drifting, and source of slight change. If they appear in Di-1, removing them means that the user observes them and dislikes them. If they do not appear in Di-1, the user still dislikes the terms since they are not in qi anyway. This corresponds to t in cases 2, 5, 6, 7, and 8 in Table 3.
3. SEARCH ENGINE AGENT: STRATEGIES TO IMPROVE SEARCH
The search engine agent observes query change from the user agent and takes corresponding actions. For each type of query change, theme terms, added terms, and removed terms, we propose to adjust the term weights accordingly for better retrieval accuracy. The search engine agent's action include

455

Table 4: Search engine agent's policy. Actions are adjustments on the term weights.  means increasing,  means decreasing, and  means keeping the original term weight.

qtheme

 Di-1 Y
N

+q

Y N

-q

Y N

action Example

 "pocono mountain" in s6



"france world cup 98 reaction" in s28, q1  q2

 `policy' in s37, q1  q2  `US' in s37, q1  q2

 `reaction' in s28, q2  q3  `legislation' in s32, q2  q3

increasing, decreasing, and maintaining the term weights. Based on the observed query change as well as whether the query terms appeared in the previous search results Di-1, we can sense whether the user will favor the query terms in the current run of search. Table 4 illustrates the policies that we propose for the search engine agent.
As shown in Section 2, theme terms qtheme often appear in many queries in a session and there is a strong preference for them. Thus, we propose to increase the weights of theme terms no matter whether they appeared in Di-1 or not (rows 1 and 2 in Table 4). In the latter case, if a theme term was not found in Di-1 (top retrieval results), it is likely that the documents containing them were ranked low. Therefore, the weights of theme terms need to be raised to boost the rankings of those documents (row 2 in Table 4). However, since theme terms are topic words in a session, they could appear like stopwords within the session. To avoid biasing too much towards them, we lower their term weights proportionally to their numbers of occurrences in Di-1.
For added terms +q, if they occurred in previous search results Di-1, we propose to decrease their term weights for the sake of novelty [14]. For example, in s5 q1  q2, "pocono mountains""pocono mountains park", the added term `park' appeared in a document in D5. If we use the original weight of `park', this document might still be ranked high in D2 and the user may dislike it since he read it before. We hence decrease added terms' weights if they are in Di-1 (row 3 in Table 4). On the other hand, if the added terms did not occur in Di-1, they are the new search focus and we increase their term weights (row 4 in Table 4). In an interesting case (s37 q1  q2), part of +q, `policy', occurred in D1 whereas the other part, `US', did not. To respect the user's preference, we increase the weight of `US' while decreasing that of `policy' to penalize documents about other `polices' including "Canada policy".
For removed terms -q, if they appeared in Di-1, their term weights are decreased since the user dislikes them by deleting them (row 5 in Table 4). For example, in s28 q2  q3, `reaction' existed in D2 and is removed in q3. However, if the removed terms are not in Di-1, we do not change their weights since they are already removed from qi by the user (row 6 in Table 4).
In the sections below, we follow policies proposed for the search engine agent as shown in Table 4 and incorporate them into a novel query change retrieval model (QCM).

4. MODELING SESSION SEARCH
Markov Decision Process (MDP) [16, 28] models a state space S and an action space A. Its states S = {s1, s2, ...} change from one to another according to a transition model T = P (si+1|si, ai), which models the dynamics of the entire

system. A policy (s) = a indicates that at a state s, what

are the actions a can be taken by the agent. In session

search, we employ queries as states. Particularly, we denote

q as state, T as the transition model P (qi|qi-1, ai-1), D

as documents, and A as actions. Actions include keeping,

adding, and removing query terms for the user agent and

increasing, decreasing, and maintaining the term weights for

the search engine agent.

In a MDP, each state is associated with a reward function

R that indicates possible positive reward or negative loss

that a state and an action may result. In session search, we

consider the reward function to be the relevance function.

Reinforcement learning [16] offers general solutions to MDP

and seeks for the best policy for an agent. Each policy has

a value associated with the policy and denoted as V(s),

which is the expected long-term reward starting from state

s and continuing with policy  from then on. In a MDP, it

is believed that a future reward is not worth quite as much

as a current reward and thus a discount factor   (0, 1) is

applied to future rewards. By considering the discount fac-

tor, the value function starting from s0 for a policy  can be

written as: V(s0) = E[R(s0) + R(s1) + 2R(s2) + ...] =

E [

 t=0



tR(si

optimal value V

)]. The  for a

Bellman equation [16] describes the state s in the long run and is often

used to obtain the best value for a MDP:

V (s) = max R(s, a) +  P (s |s, a)V (s )
a s

where s is the next state after s, V (s) and V (s ) are the optimal values for s and s .
For session search, we observe that the influence of previous queries and previous search results to the current queries, becomes weaker and weaker. The user's desire for novel documents also supports this argument. We hence propose to employ the reinforcement learning model backwards. That is, instead of discounting the future rewards, we discount the past rewards, i.e. the relevant documents that appeared in the previous search results.
We propose the query change retrieval model (QCM) as the following. We consider the task of retrieving relevant documents for qi as ranking documents based on the reward, i.e., how relevant it is to qi. Inspired by the Bellman equation, we model the relevance of a document d to the current query qi as:

Score(qi, d) = P (qi|d)+ P (qi|qi-1, Di-1, a) max P (qi-1|Di-1)

a

Di-1

(1)

which recursively calculates the reward starting from q1 and

continues with the search engine agent's policy until qi.  

(0, 1) is the discount factor, maxDi-1 P (qi-1|Di-1) is the maximum of the past rewards, P (qi|d) is the current reward,

and P (qi|qi-1, Di-1, a) is the query transition model.

The first component in Eq.1, P (qi|d), measures the rel-

evance between qi and a document d that is under evalu-

ation. This component can be estimated by the Bayesian

belief network model [27]: P (qi|d) = 1 - tqi (1 - P (t|d)), where P (t|d) is calculated by the multinomial query genera-

tion language model with Dirichlet smoothing [33]: P (t|d) =

#(t,d)+µP |d|+µ

(t|C)

,

where

#(t, d)

denotes

the

number

of

occur-

rences of term t in document d, P (t|C) calculates the prob-

ability that t appears in corpus C based on Maximum Like-

456

lihood Estimation (MLE), |d| is the document length, and

ognize added terms +q and removed terms -q. Gener-

µ is the Dirichlet smoothing parameter (set to 5000).

ally, the terms that occur in the current query but not in

The remaining challenges of calculating Eq.1 include max-

the previous query constitute +q; while the terms occur in

imizing the reward function maxDi-1 P (qi-1|Di-1) and estimating the transition model P (qi|qi-1, Di-1, a). They are

the previous query but not in the current query constitute -q. In the above example, -q7 = "camelbeach hotel",

described in Section 4.1 and Section 4.2, respectively.

and +q7 = "chateau resort".

4.1 Maximizing the Reward Function

The search engine actions are decreasing, increasing, and maintaining term weights. According to Table 4 rows 3 and

When considering the past/future rewards, MDP uses only 5, we decrease a term's weight if the query change, either

the optimal (the maximum possible) values from those past

+q or -q, occurred in the effective previous search re-

/future rewards. This is reflected in maxDi-1 P (qi-1|Di-1) as part of Eq. 1.

sults Di-1. We propose to deduct term t's weight by P (t|d), i.e. t's default contribution to the relevance score between

Prior research [10, 22] suggests that Satisfying (SAT) clicks, i.e., clicked documents with dwell time longer than 30 seconds [10, 22], are probably the only ones that are effective

qi and the document under evaluation (denoted as d). Furthermore, since t already occurred in Di-1, for the sake of novelty, we deduct more weight that is proportional to t's

at predicting user behaviors and relevance judgments. Since
the user also skims snippets in search interactions, in this
work, we consider both the top 10 returned snippets and
SAT clicks as effective previous search results and denote them as Die-1.
To obtain an maximum reward from all possible reward

frequency in Di-1 such that the more frequently t occurred in Di-1, the more heavily t's weight is deducted from the current query qi and d. We formulate this weight deduction for a term t  +q or t  -q as:

log Pnew(t|d) = (1 - P (t|di-1)) log P (t|d)

(2)

functions P (qi-1|di-1), i.e., the text relevance of previous query qi-1 and all previous search results di-1  Di-1, we

where di-1 denotes the maximum rewarded document, d is the document under evaluation, and P (t|d) is calculated by

propose to generate a maximum rewarding document, denoted as di-1. We further propose that the candidates for the di-1 should only be selected from the effective previous search results Die-1. We define di-1 as the document(s) that is the most relevant to qi-1. To discover di-1, we first
rank all the documents (either a snippet or a document)

MLE. We apply the log function to avoid numeric underflow. We notice that Eq. 2 has an interesting connection with
the Kullback-Leibler divergence (KL divergence) [33]:

-

P

(t|di-1)

log

P

(t|d)

=

P

(t|di-1)

log

P

1 (t|d)

di-1  Die-1 by measuring the relevance between qi-1 and di-1 as: P (qi-1|di-1) = 1 - tqi-1 {1 - P (t|di-1)}, where

ra=nk

P

(t|di-1)

log

P (t|di-1) P (t|d)

(3)

P (t|di-1)

is

calculated by MLE: P (t|di-1)

=

, #(t,di-1 )
|di-1 |

#(t, di-1)

is the number of occurrences of term t in document di-1, and

ra=nk KLDt di-1 ||d

|di-1| is the document length. We do not apply smoothing here since P (t|di-1) can be zero, i.e., t / Die-1. In fact, we rely on this property in later calculation.
After ranking documents di-1 in Die-1, we generate di-1 by the following options: (1) using the document with the largest P (qi-1|di-1), (2) concatenating the top k documents in Die-1 with the largest P (qi-1|di-1), or (3) concatenating all documents in Die-1. Experiments show that option (1) works the best and we use this setting throughout the paper. For notation simplicity, we use Di-1 from now on to denote effective previous search results.
4.2 Estimating the Transition Model

where KLDt di-1 ||d denotes the contribution of term t to the KL divergence between two documents' language models di-1 and d. In Eq. 3, the larger the divergence between di-1 and d, the more novel document d is compared to Di-1, and the less deduction to the relevance score. In this sense, Eq. 2 models novelty for the added terms and the removed terms during a query transition.
According to Table 4 row 4, we increase a term's weight if it is an added term and did not occur in Di-1. We propose to raise the term weight proportional to its inverse document frequency (idf). This is to make sure that while increasing a preferred term's weight, we avoid increasing its weight

The transition model indicated in Eq. 1 is a P (qi|qi-1, Di-1, a). It includes the probabilities of query transitions
under various actions. We incorporate polices designed in

too much if it is a common term in many documents. We formulate this weight increase for a novel added term t (t  +q and t / Di-1) as:

Table 4 to calculate it. Search engine agent performs actions based on user agent's

log Pnew(t|d) = (1 + idf (t)) log P (t|d)

(4)

actions. We need to identify user's actions, i.e. query change

where idf (t) is the inverse document frequency of t in Corpus

q before search engine takes actions. Particularly, we rec-

C and P (t|d) is calculated by MLE. Note that this term

ognize q by the following procedure. First, we generate

weight adjustment is in a form of tf-idf.

qtheme based on the Longest Common Subsequence (LCS)

The increasing in term weights also applies to theme terms,

[11] in both qi-1 and qi. A subsequence is a sequence that

which corresponds to rows 1 and 2 in Table 4. Theme terms

appears in two strings in the same relative order but is not

repeatedly appear in a session, which implies the impor-

necessarily continuous. The LCS can be the common prefix

tance of them. Similar to the novel added terms, we should

or the common suffix of the two queries; it can also consist

avoid increasing their weights too much. We could discount

of several discontinuous common parts from the two queries.

the increment proportional to idf. However, theme terms

Take s6 q6  q7 as an example: q6="pocono mountains

are topical/common terms within a session, not necessarily

camelbeach hotel", q7="pocono mountains chateau resort",

common terms in the entire corpus. Therefore, idf may not

qtheme = LCS(q6, q7) = "pocono mountains". Next, we rec-

be applicable here. We hence employ the negation of the

457

number of occurrences of t in previous maximum rewarding document, 1 - P (t|di-1), to substitute idf. We formulate this weight increase for a theme term t  qtheme as:
log Pnew(t|d) = (1 + (1 - P (t|di-1))) log P (t|d) (5)
where di-1 denotes the maximum rewarded document and P (t|d) is calculated by MLE.

Table 5: Dataset statistics for TREC 2011 and 2012 Session.

#topics #sessions #queries #dups

2011 62 76 280 16

2012 48 98 297 5

#queries/session #sessions/topic #pages judged #sessions w/o rel. docs

2011 3.68 1.23 19,413
2

2012 3.03 2.04 17,861
4

Next we determine exact string matches between every query

For removed terms that did not appear in Di-1 (Table 4 row 6), the search agent does not change their term weights.

pair. The exactly matched query pairs are identified as duplicated queries.

By considering all possible cases for the transition model

Since the user may dislike the queries and their corre-

as defined in Eq. 1, the relevance score between the current

sponding search results between two duplicated queries, we

query qi and a document d is represented as below:

propose to eliminate from the MDP the undesired queries

Score(qi, d) = log P (qi|d) + 

and their interactions. We achieve this by setting the dis[1 - P (t|di-1)] log P (t|d) count factor to zero for any interaction between two dupli-

tqtheme

cated queries as well as that for the earlier query in the two.

-

P (t|di-1) log P (t|d) +

idf (t) log P (t|d)

The new discount factor  can be calculated as:

t+q tdi-1

-

P (t|di-1) log P (t|d)

t+q t/di-1

i =

0 i

{i|i  [j, k), qj = qk, j < k)} otherwise

(9)

t-q
(6) where , , , and  are parameters for each types of actions. Note that we apply different parameters  and  on +q and -q, since added terms and removed terms may affect the retrieval differently. We report the parameter selection in Section 6.

where i is the original discount factor for the ith query, i is the updated discount factor for the ith query after deduplication.
For the above example s6, the effects from q2 and q3 on the session are eliminated. The entire session is now equivalent to q1, q4, q5, ..., q11.

4.3 Scoring the Entire Session

It is worth noting that Eq. 6 is valid only when i > 1. When i = 1, there is no previous result for q1. We thus use

Score(q1, d) = log P (q1|d)

(7)

as a base case. P (q1|d) is calculated by Eq. 4. Using Eq. 7 as the base case for the recursive function
described in Eq. 1, we obtain the overall document relevance score Scoresession(qn, d) for a session that starts at q1 and ends at qn by considering all queries in the session:

Scoresession(qn, d) = Score(qn, d) + Scoresession(qn-1, d)
= Score(qn, d) +  [Score(qn-1, d) + Scoresession(qn-2, d)]
n
= n-iScore(qi, d)
i=1
(8)

where q1, q2, · · · , qn are in the same session, and   (0, 1) is the discount factor. Eq. 8 provides a form of aggregation over the relevance functions of all the queries in a session.

5. DUPLICATED QUERIES
Duplicated queries sometimes occur in a search session. Prior work shows that removing duplicated queries could effectively boost the search accuracy [8, 19]. Duplicated queries often occur when a user is frustrated by irrelevant documents in search results and comes back to one of the previous queries for a fresh start. For example, in s6 (Table 1), q2 and q4 are duplicates and both search for pocono mountains pennsylvania hotels. The query between them is q3: pocono mountains pennsylvania things to do. It suggests that the user might dislike the search results for q3 and he returns to q2 to search again (q2 = q4).
To detect query duplicates, we first remove punctuations and white spaces in queries, then apply stemming on them.

6. EVALUATION
The evaluation datasets are from TREC 2011 and 2012 Session tracks [18, 19]. Table 5 lists the statistics about these two datasets. Each search session includes several queries and the corresponding search results. The users (NIST assessors) were given a topic description about information needs before they searched. For example, s85 (Table 1) are related to topic 43 "When is scientific glass blowing used? What are the purposes? What organizations do scientific glass blowing?" Multiple sessions can relate to the same topic. The search engine used to create the sessions was Yahoo! BOSS. The top 10 returned documents were shown to the users and they clicked documents that were interesting to them and interacted with the system. We use TREC's official ground truth and official evaluation metrics nDCG@10 and MAP.
The corpus used in this evaluation is ClueWeb09 Category B collection (CatB).4 CatB contains the first 50 million English pages crawled from the Web during January to February 2009. We filter out the spam documents by removing documents whose WateQCMoo's "GroupX" spam ranking scores [6] are less than 70.
We compare the following systems in this evaluation:
· Lemur : Directly submitting the current query qn (with punctuations removed) to the Lemur search engine [21] (language modeling + Dirichlet smoothing) and obtain the returned documents.
· TREC best : The top TREC system as reported by NIST [13, 14]. It adopts a query generation model with relevance feedback and handles document novelty. CatB was used in their TREC submissions. This system is used as the baseline system in this evaluation.
· Nugget: Another top TREC 2012 session search system groups semantically coherent query terms as nuggets and
4http://lemurproject.org/clueweb09/.

458

Figure 2: nDCG@10 for TREC 2012 against the parameters. (a), (b), (c), and (d) are about , , , and  respectively.

Table 6: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2012 sessions. The runs are sorted by nDCG@10. A statistical significant improvement over the baseline is indicated with a  at p < 0.05 level.

Approach Lemur TREC median Nugget TREC best
QCM
QCM+Dup

nDCG@10 0.2474 0.2608 0.3021 0.3221
0.3353
0.3368

%chg -21.54% -17.29% -4.19% 0.00% 4.10% 4.56%

MAP 0.1274 0.1440 0.1490 0.1559
0.1529
0.1537

%chg -18.28% -7.63% -4.43% 0.00%
-1.92%
-1.41%

Table 7: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2011 sessions. The runs are sorted by nDCG@10. A statistical significant improvement over the baseline is indicated with a  at p < 0.05 level.

Approach Lemur TREC median TREC best
QCM
QCM+Dup Nugget

nDCG@10 0.3378 0.3544 0.4409
0.4728
0.4821 0.4836

%chg -23.38% -19.62% 0.00% 7.24% 9.34% 9.68%

MAP 0.1118 0.1143 0.1508
0.1713
0.1714 0.1724

%chg -25.86% -24.20% 0.00% 13.59% 13.66% 14.32%

creates structured Lemur queries [8]. We re-implement and apply it on both TREC 2011 and 2012. · TREC median: The median TREC system as reported by NIST [18, 19]. · QCM : The proposed query change retrieval model. · QCM + De-Duplicate (Dup): The proposed query change retrieval model with duplicated queries removed.

6.1 Search Accuracy
Table 6 and Table 7 demonstrate search accuracy for all systems under comparison for TREC 2012 and TREC 2011, respectively. The evaluation metrics are nDCG@10 and MAP, the same as in the official TREC evaluations. TREC best serves as the baseline.
Table 6 shows that the proposed QCM approach outperforms the best TREC 2012 system on nDCG@10 by 4.1%, which is statistically significant (one sided t-test, p = 0.05). The search accuracy is further improved by 0.46% through removing the duplicated queries. The experimental results strongly suggest that our approach is highly effective.
Table 7 shows that for TREC 2011, our approach again outperforms the baseline by a statistically significant 7.24% (one sided t-test, p = 0.05) and achieves a further improvement of 9.34% by the QCM+Dup approach. For TREC 2011, the performance gain by performing de-dup is 2.1%, which is bigger than that for TREC 2012 (0.46%). The reason is probably because that TREC 2012 only has 5 duplicated queries while TREC 2011 has 16 (shown in Table

5). However, the best approach for TREC 2011 is the nugget approach, which is slightly better than QCM+Dup.
Table 5 illustrates the dataset differences between TREC 2011 and 2012. These differences may affect search accuracy. The average number of sessions per topic is 2.04 in 2012, that is more than that in 2011 (1.23). Moreover, on average, TREC 2012 sessions contain less queries per session (3.03) than 2011 (3.68). As a result, the shorter sessions in 2012 may make the search task more difficult than 2011 since less information are provided by previous interactions. Another difference is that 2012 sessions have fewer (sometimes even none) relevant documents than 2011 sessions in CatB ground truth. It unavoidably hurts the performance for any retrieval system. Generally, we observe lower search accuracy in 2012 (Table 6) than in 2011 (Table 7).
6.2 Parameter Tuning
We investigate good values for parameters in Eq. 6. A supervised learning-to-rank method should be able to find the optimal values for those parameters. However, in this paper, we take a step-by-step parameter tuning procedure and leave the supervised learning method as future work.
We add each component, i.e., theme terms, added terms, and removed terms, one by one into Eq. 6. The tuning is performed for QCM only and the parameters are shared between QCM and QCM+Dup.
First, we plot nDCG@10 against  while setting other parameters to 0 (Figure 2(a)).  represents the parameter for theme terms.  ranges over [1.1, 2.5] by an interval of 0.1. We notice that nDCG@10 reaches its maximum at  = 2.2. We find 2 other local maximums at 1.6 and 1.2 for .
Next, we fix  to the above values and plot nDCG@10 against  (Figure 2(b)).  is the parameter for added terms that appeared in effective previous search results; we call them old added terms.  ranges over [1.0, 2.4] by an interval of 0.2. We choose the top 2 local values from each curve and pick 6 combinations for (, ) as indicated in Figure 2(c).
Then, we fix (, ) and plot nDCG@10 against (Figure 2(c)). is the parameter for added terms that did not appear in effective previous search results; we call them novel added terms. ranges over [0.05, 0.1] by an interval of 0.01. All the curves show similar trends and reach the highest nDCG@10 at around 0.07. We hence fix to 0.07.
Finally, we plot nDCG@10 against  (Figure 2(d)) with the parameter combinations that we discover eerlier. Eventually, nDCG@10 reaches its peak 0.3353 at  = 2.2,  = 1.8, = 0.07, and  = 0.4. We apply this set of parameters to both QCM and QCM+Dup.
As we can see, , , and  are much larger than . This is because that in Eq. 4, idf (t) = log(N/nd) falls in the range of [1, 10], while in Eq. 2 and Eq. 5, P (t|di-1) falls

459

by an interval of 0.02. Figure 3 illustrates the relationship between nDCG@10 and . nDCG@10 climbs to its peak 0.3368 when  = 0.92. The result suggests that a good discount factor  is very close to 1, implying that previous queries contribute to the overall search accuracy nearly the same as the last query. It suggests that in QCM, a discount between two adjacent queries should be mild.

7. DISCUSSION

Figure 3: Discount factor .

Figure 4: Error types.

7.1 Advantages of Our Approach

Table 8: Aggregation schemes.

A main contribution of our approach is that we treat a search session as a continuous process by studying changes

Approach

Query change model

Aggregation Scheme

Uniform PvC

Distance-based

qn 1 n = 1 n = 1 - p
n = 1 - p

qi(i  [1, n - 1])  n-i

i = 1

i = p

i

=

p n-i

among query transitions and modeling the dynamics in the entire session. Through the reinforcement learning style framework, our system provides the best aggregation scheme for all queries in a session (Table 9). This allows us to better handle sessions that demonstrate evolution and exploration

Table 9: nDCG@10 for various aggregation schemes. p is 0.4 in PvC.  is 0.92 in QCM and QCM+Dup. TREC 2012 best serves as the baseline. A significant improvement over the baseline is indicated with a  at p < 0.05 level.

Aggregation Scheme
Distance-based TREC best
Uniform

TREC 2011

nDCG@10 %chg

0.4431

-2.40%

0.4540

0.00%

0.4626

1.89%

TREC 2012

nDCG@10 %chg

0.3111 -3.42%

0.3221

0.00%

0.3316

2.95%

in nature than most existing systems do. On the contrary, for sessions that are clear in search goals and lack of a exploratory nature, the advantage of our system over other systems looks less significant.
This can be seen in Table 10, which illustrates the search accuracy for the TREC best, Nugget, and our system for various classes of sessions. The TREC best is used as the baseline and we also show the percentile improvement over it in Table 10. TREC 2012 sessions were created by consider-

PvC

0.4713

3.81%

0.3351

4.04%

ing and hence can be classified into two facets: search target

QCM

0.4728 4.14% 0.3353 4.10% (factual or intellectual) and goal quality (specific/good or

QCM+Dup

0.4821 6.19% 0.3368 4.56%

amorphous/ill) [19]. Table 10 shows that QCM works very well for all classes of sessions. Specifically, QCM works even

in the range of [0,0.1]. Therefore, the values of are two

better, i.e. outperforms the TREC best even more signifi-

magnitudes less than that for the other parameters. Among

cantly, for sessions that search for intellectual targets as well

, , and , we find that  and  are larger than , which

as sessions that search with amorphous goals. In our opin-

implies that theme terms and added terms may play more

ion, this is due to that intellectual tasks produce new ideas

important roles in session search than removed terms.

or new findings (e.g. learn about a topic or make decision

6.3 Aggregation for the Entire Session

based on the information collected so far) while searching. Both intellectual and amorphous sessions rely more on pre-

QCM proposes an effective way to aggregate all queries in

vious search results. Thus, users reformulate queries based

a session as in Eq.8. We compare how effective it is to prior

more on what they have retrieved, not the vague informa-

query aggregation methods. A query aggregation scheme

tion need. This is a scenario where our approach is good at

can be represented as: Score(session, d) =

n i=1

i

·

Score(qi,

d),

since

we

employ

previous

search

results

to

guide

the

search

where Score(qi, d) is the relevance scoring function of d and

engine's action. For specific and factual sessions, users are

qi and i is the query weight for qi.

clearer in search goals, query changes may come less from the

[8] proposed several aggregation schemes for TREC 2012

previous search results. In summary, our good performance

Session track. The schemes are: uniform (all queries are

on both intellectual task and amorphous task is consistent

equally weighted), previous vs. current (known as PvC;

with our efforts of modeling query changes.

all previous queries are discounted by p, while the current

Moreover, we benefit from term-level manipulation in var-

query uses a complementary and higher coefficient (1 - p),

ious aspects in our system. The first aspect is novelty. Both

and distance-based (previous queries are discounted based

the TREC best system and our system handle novelty in

on a reciprocal function of queries' positions in the session).

a session. The TREC best system only deals with novelty

We express various query aggregation schemes in terms

at the document level. They consider documents that have

of the discount factor  in order to compare them with our

been examined by the user in a previous interaction not

approach. From Table 8, we find that QCM degenerates to

novel and the rest are novel [14]. That is, they determine

uniform when  = 1. Previous queries in PvC and Distance-

novelty purely based on document identification number, not

based schemes are also discounted as they are in QCM, but

the actual content. Through studying whether query terms

with different decay functions.

appeared in previous search results, our approach evaluates

The search accuracy for different aggregation schemes are

and models novelty at the term level (or concept level),

compared in Table 9. QCM performs the best for both

which we believe better represents the evolving informa-

TREC 2011 and 2012. The PvC scheme is the second best

tion needs in a session. The second aspect is query han-

scheme, which confirms what is reported in [8]. The Distance- dling. The Nugget approach [8] treats queries at the phrase

based scheme gives the worst performance.

level and formulates structured queries based on phrase-like

We explore the best discount factor  for QCM over (0, 1)

nuggets. The approach achieves good performance, espe-

460

Table 10: nDCG@10 for different classes of sessions in TREC 2012.

TREC best Nugget QCM QCM+DUP

Intellectual 0.3369 0.3305 0.3870 0.3900

%chg 0.00% -1.90% 14.87% 15.76%

Amorphous 0.3495 0.3397 0.3689 0.3692

%chg 0.00% -2.80% 5.55% 5.64%

Specific 0.3007 0.2736 0.3091 0.3114

%chg 0.00% -9.01% 2.79% 3.56%

Factual 0.3138 0.2871 0.3066 0.3072

%chg 0.00% -8.51% -2.29% -2.10%

cially for TREC 2011. However, due to complexity in natural language, nugget detection is sensitive to dataset and the approach's performance is not quite as stable as ours on different datasets.
Lastly, our system benefits from trusting the user. Our approach does not use too much materials from other resources such as anchor texts, meta data, or click orders, as many other approaches do [8, 26]. We believe that the most direct and valuable feedback is the next query that the user enters. In this work, we manage to capture the query change and investigate the reasons behind it. We use ourselves as users to summarize possible human users' reasoning and actions. More detailed analysis about user intent might be useful for researchers to understand web users, however, it might be overwhelming (too fine-grained or too much semantics) for a search engine that essentially only counts words.
7.2 Error Analysis & Future Work
Our system retrieves nothing for 22 out of 98 sessions in TREC 2012. To analyze the reason for the poor performance for those sessions, we study their topic descriptions, queries, and ground truth documents. We summarize the types of errors as "two theme concepts", "ill query", "few relevant documents", and others. Figure 4 shows how many sessions that we fail to retrieve under each error type.
We call the first type of errors "two theme concepts". It comes from a type of session where the information need cover more than one concepts. For instance, s17 and s18 share the the same topic "... To what extent can decisions and policies of the Indian government be credited with these wins?". Queries in s17 and s18 ask about both concepts "indian politics' and "miss universe". Unfortunately, very few relevant documents about both theme concepts exist in the corpus. The retrieved documents are about either concept, but none is about both. Eight sessions belong to this type. As future work, we can improve our system by incorporating structures in queries, and enable more sophisticated operators such as Boolean and proximity search.
The second type of errors is "ill query", where in such sessions, queries themselves are ill formulated and do not well-represent the information needs indicated in the given topic. A common mistake is that the user misses some subinformation need. For example, the topic for s16 is: "... you want to reduce the use of air conditioning in your house ... you could protect the roof being overly hot due to sun exposure... Find information of ... how it could be done." A good query for this topic should include roof and air conditioning. However, the queries that the user issued for s60, "reduce airconditioning" and "attic insulation air conditioning costs", do not mention roof at all. Because of this ill query formulation, our system yields no relevant documents for s60. On the other hand, for s59, which shares the same information need with s60, our system achieves a nDCG@10 of 0.48 simply because s59 queries "cool roof". It suggests that ill queries mislead the search engine and yield poor retrieval performance. Four sessions belong to this type. As

future work, we will explore effective query suggestion by studying sessions that share the same topic.
The third type of errors is "too few relevant documents". For sessions with too few relevant documents in the ground truth, our system do not perform well. In total 2,573 relevant documents exist in CatB for all 48 TREC 2012 topics; on average 53.6 relevant documents per topic. However, topics 10, 45, 47 and 48, each has no more than 2 relevant documents and topic 47 (s92 to s95) has no relevant document in CatB (Table 5). This problem could be reduced if we index the entire ClubWeb09 CatA collection.
Figure 4 also indicates in which classes of sessions these errors lie. We find that all "two theme concept" errors belong to sessions created with amorphous goals while all "too few relevant documents" errors belong to those with specific goals. Moreover, "ill queries" tend to occur more in sessions with amorphous goals. Note that "ill query" and "few relevant documents" are errors due to either the user or the data. There might not be much room for our system to improve over them. However, "two theme concepts" is where our system can certainly make further improvements.
8. RELATED WORK
Session search is a challenging IR task [4, 8, 13, 14, 25, 32]. Existing approaches investigate session search from various aspects such as semantic meanings of search tasks [23], document novelty [14], and phrase structure in queries [8]. The best TREC system [13, 14] employs an adaptive browsing model by considering both relevance and novelty; however it does not demonstrate improvement by handling novelty. In this paper, we successfully model query and document novelty by investigating the relationship between query change and previous search results. Moreover, our analysis on query change does not require knowledge of semantic types for the sessions as [23] proposed.
Our proposed work is perhaps the most similar to the problem of query formulation [1, 9, 12, 24] and query suggestion [29]. [12] showed that certain query changes such as adding/removing words, word substitution, acronym expansion, and spelling correction are more likely to cause clicks, especially on higher ranked results. The finding is generally consistent with our view of query change. However, their work only emphasized on understanding of query changes, without showing how to apply it to help session search. [24] examined the relationship between task types and how users change queries. They classified query changes by semantic types: Generalization, Specialization, Word Substitution, Repeat, and New. Similar to [12], however, [24] stopped at understanding query changes and didn't apply their findings to help session search. This probably makes us the first to utilize query changes in actual retrieval. [1] derived queryflow graph, a graph representation of user query behavior, from user query logs. The approach detected query chains in the graph and recommended queries based on maximum weights, random walk, or just the previous query. Other mining approaches [1, 29] identify the importance of query

461

change in sessions; however, they require the luxury of large user query logs.
This research is perhaps the first to employ reinforcement learning to solve the Markov Decision Process demonstrated in session search. Reinforcement learning is complex and difficult to solve. Its solutions include model-based approaches and model-free approaches [16]. The former learn the transition model and the reward function for every possible states and actions and mainly employ MLE to estimate the model parameters. Others also use matrix inversion or linear programming to solve the Bellman equation. It works well when state spaces are small. However, in our case, the state space is large since we use natural language queries as the states; hence we could not easily apply model-based approaches in practice. In this work, we effectively reduce the search space by summarizing users' and search engine's actions into a few types and employ a model-free approach to learn value functions directly.
9. CONCLUSION
This paper presents a novel session search approach (QCM) by utilizing query change and modeling the dynamic of the entire session as a Markov Decision Process. We assume that query change is an important form of feedback. Based on this assumption, through studying editing changes between adjacent queries, and their relationship with previous retrieved documents, we propose corresponding search engine actions to handle individual term weights for both the query and the document. In a reinforcement learning inspired framework, we incorporate various ingredients present in session search, such as query changes, satisfactory clicks, desire for document novelty, and duplicated queries. The proposed framework provides a theoretically sound and general foundation that allows more novel features to be incorporated. Experiments on both TREC 2011 and 2012 Session tracks show that our approach is highly effective and outperforms the best session search systems in TREC. This research is perhaps the first to employ reinforcement learning in session search. Our MDP view of modeling session search can potentially benefit a wide range of IR tasks.
10. ACKNOWLEDGMENT
This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.
11. REFERENCES
[1] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and S. Vigna. The query-flow graph: model and applications. In CIKM '08.
[2] I. Bordino, C. Castillo, D. Donato, and A. Gionis. Query similarity by projecting the query-flow graph. In SIGIR '10.
[3] P. Bruza, R. McArthur, and S. Dennis. Interactive internet search: keyword, directory and query reformulation mechanisms compared. In SIGIR '00.
[4] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In CIKM '11.
[5] M.-A. Cartright, R. W. White, and E. Horvitz. Intentions and attention in exploratory health search. In SIGIR'11.
[6] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr., 14(5), Oct. 2011.

[7] D. Guan and H. Yang. Increasing stability of result organization for session search. In ECIR '13.
[8] D. Guan, H. Yang, and N. Goharian. Effective structured query formulation for session search. In TREC '12.
[9] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and discriminative model for query refinement. In SIGIR '08.
[10] Q. Guo and E. Agichtein. Ready to buy or just browsing?: detecting web searcher goals from interaction data. In SIGIR '10.
[11] D. S. Hirschberg. Algorithms for the longest common subsequence problem. J. ACM, 24(4), Oct. 1977.
[12] J. Huang and E. N. Efthimiadis. Analyzing and evaluating query reformulation strategies in web search logs. In CIKM '09.
[13] J. Jiang, S. Han, J. Wu, and D. He. Pitt at trec 2011 session track. In TREC '11.
[14] J. Jiang, D. He, and S. Han. Pitt at trec 2012 session track. In TREC '12.
[15] R. Jones and K. L. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In CIKM '08.
[16] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: a survey. J. Artif. Int. Res., 4(1), May 1996.
[17] Y. Kalfoglou and M. Schorlemmer. Ontology mapping: the state of the art. Knowl. Eng. Rev., 18(1), Jan. 2003.
[18] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2011 session track. In TREC'11.
[19] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2012 session track. In TREC'12.
[20] E. Kanoulas, P. D. Clough, B. Carterette, and M. Sanderson. Session track at trec 2010. In TREC'10.
[21] Lemur Search Engine. http://www.lemurproject.org/. [22] C. Liu, N. J. Belkin, and M. J. Cole. Personalization of
search results using interaction behaviors in search sessions. In SIGIR '12.
[23] C. Liu, M. Cole, E. Baik, and J. N. Belkin. Rutgers at the trec 2012 session track. In TREC'12.
[24] C. Liu, J. Gwizdka, J. Liu, T. Xu, and N. J. Belkin. Analysis and evaluation of query reformulations in different task types. In ASIST '10.
[25] J. Liu and N. J. Belkin. Personalizing information retrieval for multi-session tasks: the roles of task stage and task type. In SIGIR '10.
[26] A. M-Dyaa, K. Udo, N. Nikolaos, N. Brendan, L. Deirdre, and F. Maria. University of essex at the trec 2011 session track. In TREC '11.
[27] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Inf. Process. Manage., 40(5), Sept. 2004.
[28] S. P. Singh. Learning to solve markovian decision processes. Technical report, Amherst, MA, USA, 1993.
[29] Y. Song and L.-w. He. Optimal rare query suggestion with implicit user feedback. In WWW '10.
[30] Y. Song, D. Zhou, and L.-w. He. Query suggestion by constructing term-transition graphs. In WSDM '12.
[31] J. Teevan, S. T. Dumais, and D. J. Liebling. To personalize or not to personalize: modeling queries with variation in user intent. In SIGIR '08.
[32] R. W. White, I. Ruthven, J. M. Jose, and C. J. V. Rijsbergen. Evaluating implicit feedback models using searcher simulations. ACM Trans. Inf. Syst., 23(3), July 2005.
[33] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, Apr. 2004.

462

Toward Whole-Session Relevance: Exploring Intrinsic Diversity in Web Search

Karthik Raman
Dept. of Computer Science Cornell University Ithaca, NY, USA
karthik@cs.cornell.edu

Paul N. Bennett
Microsoft Research One Microsoft Way
Redmond, USA
pauben@microsoft.com

Kevyn Collins-Thompson
Microsoft Research One Microsoft Way
Redmond, USA
kevynct@microsoft.com

ABSTRACT
Current research on web search has focused on optimizing and evaluating single queries. However, a significant fraction of user queries are part of more complex tasks [20] which span multiple queries across one or more search sessions [26, 24]. An ideal search engine would not only retrieve relevant results for a user's particular query but also be able to identify when the user is engaged in a more complex task and aid the user in completing that task [29, 1]. Toward optimizing whole-session or task relevance, we characterize and address the problem of intrinsic diversity (ID) in retrieval [30], a type of complex task that requires multiple interactions with current search engines. Unlike existing work on extrinsic diversity [30] that deals with ambiguity in intent across multiple users, ID queries often have little ambiguity in intent but seek content covering a variety of aspects on a shared theme. In such scenarios, the underlying needs are typically exploratory, comparative, or breadth-oriented in nature. We identify and address three key problems for ID retrieval: identifying authentic examples of ID tasks from post-hoc analysis of behavioral signals in search logs; learning to identify initiator queries that mark the start of an ID search task; and given an initiator query, predicting which content to prefetch and rank.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--retrieval models, search process
General Terms
Algorithms, Experimentation
Keywords
Search session analysis, diversity, proactive search
1. INTRODUCTION
Information retrieval research has primarily focused on improving retrieval for a single query at a time. However, many complex tasks such as vacation planning, comparative
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

shopping, literature surveys, etc. require multiple queries to complete the task [20].

Initiator query snow leopards
remodeling ideas

Successor queries
snow leopard pics where do snow leopards live snow leopard lifespan snow leopard population snow leopards in captivity
cost of typical remodel hardwood flooring earthquake retrofit paint colors kitchen remodel

Table 1: Examples of intrinsically diverse search tasks, showing the first (initiator) query and several successor queries from the same search session.

Within the context of this work, we focus on one specific type of information seeking need that drives interaction with web search engines and often requires issuing multiple queries ­ namely intrinsically diverse tasks [30]. Table 1 gives examples of two intrinsically diverse tasks observed in a commercial web search engine. Intrinsic diversity, where diversity is a desired property of the retrieved set of results to satisfy the current user's immediate information need, is meant to indicate that diversity is intrinsic to the need itself; this is in contrast to techniques that provide diversity to cope with uncertainty in query intent (e.g., [jaguar]).
Intrinsically diverse tasks typically are exploratory, comprehensive, survey-like, or comparative in nature. They may result from users seeking different opinions on a topic, exploring or discovering aspects of a topic, or trying to ascertain an overview of a topic [30]. While a single, comprehensive result on the topic may satisfy the need when available, several or many results may be required to provide the user with adequate information [30]. As seen in the examples, a user starting with [snow leopards] may be about to engage in an exploratory task covering many aspects of snow leopards including their lifespan, geographic dispersion, and appearance. Likewise when investigating remodeling ideas, a user may wish to explore a variety of aspects including cost, compliance with current codes, and common redecoration options. Note that the user may in fact discover these aspects to explore through the interaction process itself. Thus intrinsic diversity shares overlap with both exploratory and faceted search [9, 37]. However, unlike the more open-ended paradigm provided by exploratory search, we desire a solution that is shaped by the current user's information need

463

and is able to discover and associate relevant aspects for a topic automatically in a data-driven fashion. For example, for the query [snow leopards], our goal is to enable deeper user-driven exploration of that topic, by proactively searching for the relevant information that the user might want during the course of a session on that topic, thus reducing the time and effort involved in manual reformulations, aspect discovery, and so on.
To this end, we aim to design a system that addresses two key problems needed for ID retrieval: detecting the start of an ID task, and computing an optimal set of ID documents to return to the user given engagement on an ID task. For the former, the system must be capable of predicting when a user is likely to issue multiple queries to accomplish a task, based on seeing their first "initiator query". To do this, we first develop a set of heuristic rules to mine examples of authentic intrinsic diversity tasks from the query logs of a commercial search engine. The resulting tasks provide a source of weak supervision for training classification methods that can predict when a query is initiating an intrinsically diverse task. With these predictive models, we characterize how ID initiators differ from typical queries. We then present our approach to intrinsically diversifying for a query. In particular, rather than simply considering different intents of a query, we incorporate queries that give rise to related aspects of a topic by estimating the relevance relationship between the aspect and the original query. Given the intrinsically diverse sessions identified through log analysis, we demonstrate that our approach to intrinsic diversification is able to identify more of the relevant material found during a session given less user effort, and furthermore, the proposed approach outperforms a number of standard baselines.
2. RELATED WORK
The distinction between extrinsic and intrinsic diversity was first made by Radlinski et al. who coined these terms [30]. In contrast to extrinsically-oriented approaches, which diversify search results due to ambiguity in user intent, intrinsic diversification requires that results are both relevant to a single topical intent as well as diverse across aspects, rather than simply covering additional topical interpretations. Existing methods like maximal marginal relevance (MMR) do not satisfy these requirements well (cf. Sec. 5.3). Most diversification research has focused primarily on extrinsic diversity: this includes learning [39, 34] and non-learning approaches [6, 40, 7, 8]. Recent work [2], however, indicates real-world Web search tasks are commonly intrinsically diverse and require significant user effort. For example, considering average number of queries, total time, and prevalence of such sessions, common tasks include: discovering more information about a specific topic (6.8 queries, 13.5 min, 14% of sessions); comparing products or services (6.8 q, 24.8 m, 12%); finding facts about a person (6.9 q, 4.8 m, 3.5%); and learning how to perform a task (13 q, 8.5 m, 2.5%). Thus, improvements in retrieval quality that address intrinsically diverse needs have potential for broad impact.
Some previous TREC tracks, including the Interactive, Novelty and QA tracks, studied intrinsic diversity-like problems in which retrieval effectiveness was partly measured in terms of coverage of relevant aspects of queries, along with the interactive cost to a user of achieving good coverage. However, our task and data assumptions differ from these tracks. For example, the Interactive tracks focused more on

coverage of fact- or website-oriented answers, while our definition of query aspect is broader and includes less-focused subtopics. In addition to optimizing rankings to allow efficient exploration of topics, we also predict queries that initiate intrinsically diverse tasks, and show how to mine candidates for ID tasks from large-scale search log data.
Session-based retrieval is a topic that has become increasingly popular. For example, Radlinski et al. [31] studied the benefit of using query chains in a learning-to-rank framework to improve ranking performance. Others have proposed different session-level evaluation metrics [17, 21]. Research in this area has been aided by the introduction of the Session track at TREC [22]; this has led to papers on session analysis and classification [27]. In particular, He et al. use a random walk on a query graph to find other related queries, which are then clustered and used as subtopics in their diversification system [16]. In our re-ranking approach, we also use related queries to diversify the results, but maintain coherence with the original query. Specifically, we identify a common type of information need that often leads to longer, more complex search sessions. However, in contrast to previous work, rather than using the session interactions up to the current point to improve retrieval for the current query, we use a query to improve retrieval for a user's future session and use sessions from query logs to evaluate the effectiveness of the proposed methods. While the TREC Session track evaluated the number of uncovered relevant examples for the final query, the emphasis was on the impact of session context up to the present query; in our case, we assume no previous context, but instead are able to characterize the need for intrinsic diversity based on the single query alone.
Session data has also been used to identify and focus on complex, multi-stage user search tasks that require multiple searches to obtain the necessary information [36, 24]. This has led to research on task-based retrieval [14, 15] where tasks are the unit of interest (as opposed to queries or sessions). Trail-finding research studies the influence of factors such as relevance, topic coverage, diversity and expertise [33, 38]. While these problems are certainly related to ours, tasks and trails tend to be more specialized and defined in terms of specific structures: e.g. tasks are characterized as a set or sequence of sub-tasks to be accomplished, while trails are defined in terms of specific paths of user behavior on the web graph. However, intrinsically diverse search sessions, e.g. as in Table 1, represent a broader, less structured category of search behavior. Similarly, our approach complements work on faceted search [23] and exploratory search [37] by providing a data-driven manner of discovering common facets dependent on the particular topic.
Query suggestions are a well-established component of web search results with a large research literature: common approaches include using query similarity (e.g.[42]) or querylog based learning approaches (e.g. [19]). Query suggestions can play an important role for intrinsically diverse needs, because they provide an accessible and efficient mechanism for directing users towards potentially multiple diverse sets of relevant documents. Therefore, query suggestion techniques that do not merely provide simple reformulation of the initial query, but correctly diversify across multiple facets of a topic may be particularly helpful for intrinsically diverse needs. Thus, recent research on diversifying query suggestions [28] has partly inspired our retrieval approach.
Our approach is also motivated by recent work on interactive ranking. Brandt et al. [5] propose the notion of dynamic

464

rankings, where users navigate a path through the search results, to maximize the likelihood of finding documents relevant to them. Our objective formulation closely relates to another recent work on two-level dynamic rankings [32], which studied the benefit of interaction for the problem of extrinsic diversity. Similarly, user interaction has been found to help in more structured and faceted search tasks [41, 13], in cases such as product search. However, while presenting interactive, dynamic rankings is one user experience that offers a way to surface the improved relevance to users, our techniques are more general: they may be used to present a summary of the topic to the user, recommend unexplored options, anticipate and then crowdsource queries to trade off latency and quality by prefetching, and more.
In contrast to previous work, we provide a way to not only identify complex search tasks that will require multiple queries but to proactively retrieve for future queries before the user has searched for them. Importantly, these future queries are neither simple reformulations nor completely unrelated, but are queries on the particular task that the user has started. Finally, we introduce diversification methods which, unlike previous methods, maintain coherence around the current theme while diversifying. Using these methods we demonstrate that we can improve retrieval relevance for a task by detecting an intrinsically diverse need and providing whole-session retrieval at that point.
3. INTRINSICALLY DIVERSE TASKS
An intrinsically diverse task is one in which the user requires information about multiple, different aspects of the same topical information need. In practice, a user most strongly demonstrates this interest by issuing multiple queries about different aspects of the same topic. We are particularly interested in identifying the common theme of an intrinsically diverse task and when a user initiated the task. We unify these into the concept of an initiator query where, given a set of queries on an intrinsically diverse task, the query among them that is most general and likely to have been the first among these set of queries is called the initiator query. If multiple such queries exist, then the first among them from the actual sequence (issued by the user) is considered the initiator. We give importance to the temporal sequence since the goal is to detect the initiation of the task and provide support for it as soon as possible.
While previous work has defined the concept of intrinsic diversity, there has been no further understanding of the problem or means to obtain data. We now identify and analyze authentic instances of intrinsically diverse search behavior, extracted from large-scale mining and analysis of query logs from a commercial search engine.
3.1 Mining intrinsically diverse sessions
Intuitively, intrinsically diverse (ID) tasks are topically coherent but cover many different aspects. To automatically identify ID tasks in situ where a user is attempting to accomplish the task, we seek to codify this intuition. Furthermore, rather than trying to cover all types of ID tasks, we focus on extracting with good precision and accuracy a set of tasks where each task is contained within a single search session. As a "session" we take the commonly used approach of demarcating session boundaries by 30 minutes of user inactivity [35]. Once identified, these mined instances could potentially be used to predict broader patterns of cross-session

intrinsic diversity tasks [24, 1], but we restrict this study to mining and predicting the initiation of an ID task within a search session and performing whole-session retrieval at the point of detection.
To mine intrinsically diverse sessions from a post-hoc analysis of behavioral interactions signals with the search results, we developed a set of heuristics to detect when a session is topically coherent but covering many aspects. These can be summarized as finding sessions that are: (1) longer ­ the user must display evidence of exploring multiple aspects; (2) topically coherent ­ the identified aspects should be related to the same overall theme rather than disparate tasks or topics; (3) diverse over aspects ­ the queries should demonstrate a pattern beyond simple reformulation by showing diversity. Furthermore, since the user's interaction with the results will be used in lieu of a contextual relevance judgment for evaluation, we also desire that we have some "satisfied" or "long-click" results where we define a satisfied (SAT) click similar to other work as having a dwell of  30s or terminating the search session [11, 12].
Given these criteria, we propose a simple algorithm to collect intrinsically diverse user sessions. Our algorithm uses a series of filters, explained in more detail below. When we refer to "removing" queries, we mean they were treated as not having occurred for any subsequent analysis steps. For sessions, with the exception of those we "remove" from further analysis in Step 4, we label all other sessions as intrinsically diverse or regular (i.e., not ID). We identify the initiator query as the first query that remains after all query removal steps, and likewise a successor query is any remaining query that follows the initiator in the session. More precisely, we use the following steps (in sequence) to filter sessions:
1. Remove frequent queries: Frequent queries ­ such as facebook or walmart ­ that are often interleaved with more complex tasks can obscure the more complex task the user is accomplishing. Therefore, we remove the top 100 queries by frequency as well as frequent misspellings related to these queries.
2. Collapse duplicates: We collapse any duplicate of a query issued later in the session as representing the same aspect but record all SAT clicks across the separate impressions.
3. Only preserve manually entered queries: To focus on user-driven exploration and search, we removed queries that were not manually entered, e.g. those obtained by clicking on a link such as by query suggestion or searches embedded on a page.
4. Remove sessions with no SAT Document: Since we would like to eventually measure the quality of rerankings for these session queries in a personal and contextual sense, we would like to ensure that there is at least one long-dwell click to treat as a relevance judgment. While this is not required for a session being an ID session, we simply require it for ease of evaluation. Thus, we removed sessions with no SAT clicks.
5. Ensure topical coherence: As ID sessions have a common topic, we removed any successor query that did not share at least one common top ten result with the initiator query. Note that this need not be the same result for every aspect. While this restricts the set of interaction patterns we identify, it enables us to be more precise, while ensuring semantic relatedness, and does not rely on the weakness of assuming one fixed static ontology.

465

6. Ensure diversity in aspects: Although we desire topical coherence across the queries, we do not want to identify simple reformulations or spelling corrections as aspects. Thus we restrict the syntactic similarity with the initiator query to avoid identifying trivial difference as substantially different aspects. To measure query similarity robust to spelling variations, we consistently use cosine similarity with character trigrams in this work. In particular, we remove queries where the similarity was more than 0.5.
7. Remove long queries: We observed a small fraction of sessions matching the above filters appear to consist of copy/paste homework questions on a common topic. While potentially interesting, we focus in this paper on completely user-generated aspects and introduce a constraint on query length, removing queries of length at least 50 characters.
8. Threshold the number of distinct aspects: Finally, to focus on diversity and complexity among the aspects, we threshold on the number of distinct successor queries. We identify a query as distinct when its maximum pairwise (trigram character cosine) similarity with any preceding query in the session is less than 0.6. Any session with less than three distinct aspects (including the initiator) are labeled as regular and those with three or more aspects are labeled as intrinsically diverse.
Putting everything together, we ran this algorithm on a sample of user sessions from the logs of a commercial search engine from the period April 1­May 31, 2012. We used log entries generated in the English-speaking United States locale to reduce variability caused by geographical or linguistic variation in search behavior. Starting with 51.2M sessions comprising 134M queries, applying all but the SAT-click filter, with the Number of Distinct Aspects threshold at two, led to more than 497K ID sessions with 7.0M queries. These ID tasks accounted for 1.0% of all search sessions in our sample, and 3.5% of sessions having 3 queries or more (14.4M sessions)1. Further applying the SAT-click filter reduced the number to 390K. Finally, focusing on the more complex sessions by setting the Number of Distinct Aspects filter to three, reduced this to 146K sessions.
Given that ID sessions require multiple queries, we hypothesize that ID sessions account for a disproportionately larger fraction of time spent searching by all users. To test this, we estimated the time a user spent in a session by the elapsed time from the first query to the last action (i.e., query or click). Sessions with a single query and no clicks were assigned a constant duration of 5 seconds. Here, the time in session includes the whole session once an ID task was identified in that session. Our hypothesis was confirmed: while ID sessions with at least 2 distinct aspects represented 1.0% of all sessions, they accounted for 4.3% of total time spent searching, showing the significant role ID sessions play in overall search activity.
To assess the accuracy of our automatic labeling process, we sampled 150 sessions (75 each from the auto-labeled regular and intrinsic sets) of length at least 2 queries. We ignored single query sessions since those are dominated by regular
1Because we do not focus on more complex ID information seeking, such as tasks that span multiple sessions, the true percentage associated with ID tasks is likely to be larger.

intents and there may be a bias in labeling. Two assessors were given instructions similar to the description in the first paragraph of Section 3, examples of ID sessions such as those in Table 1, and all of the queries in the session and asked to label each session as regular or ID. The assessors had a 79% agreement with an inter-rater  agreement of 0.5875. Using each assessor as a gold-standard and taking the average, on sessions of length two or greater our extraction method has a precision of 73.9% and an accuracy of 73.7% (overall accuracy is higher because of single query sessions always being regular). Thus, with both good agreement and a moderate to strong accuracy and precision, the method provides a suitable source of noisy supervised labels. With enough data, we can hope to overcome the noise in the labels (as long as it is unbiased) with an appropriate learning algorithm [3].
4. PREDICTING INTRINSICALLY DIVERSE TASK INITIATION
Given that we may want to alter retrieval depending on whether the user is seeking intrinsic diversity or not, we ask the question whether we can identify the initiator queries for intrinsically diverse tasks and treat this as a classification problem. In particular, while in Sec. 3 we used the behavioral signals of interaction between the initiator and successor queries of a session to automatically label queries with a (weak) supervised label, here we ask if we can predict what the label would be in the absence of those interaction signals ­ a necessary ability if we are to detect the user's need for intrinsic diversity in an operational setting. Ultimately our goal is to enable a search engine to customize the search results for intrinsic diversity only when appropriate, while providing at least the same level of relevance on tasks predicted to be regular. Recognizing that in most operative settings, it is likely important to invoke a specialized method of retrieval only when confident, we present a precision-recall tradeoff but focus on the high precision portion of the curve.
4.1 Experimental Setting
Data: We used a sample of initiator queries from the intrinsically diverse sessions described in Sec. 3.1 as our positive examples, and the first queries (after removing common queries as in Step 1 of Sec. 3.1) from regular sessions were used as negative examples. Note that since the label of a query, e.g. [foo], comes from the session context, it is possible that [foo] occurs in both positive and negative contexts. In order to only train to predict queries that were clearly either ID or regular, we dropped such conflicting queries from the dataset; this only occurred 1 out of every 5K ID sessions. Also to weigh each task equally instead of by frequency, we sample by type: i.e., we treat multiple occurrences of a query in the positive (resp. negative) set as a single occurrence. Finally, we downsample to obtain a 1:1 ratio from the positive and negative sets to create a balanced set. Unless otherwise mentioned, the dataset was sampled to contain 61K queries and split into an 80/5/15 proportion (50000 training, 3000 validation, 8000 test) with no class bias.
Classification: We used SVMs[18] with linear kernels, unless mentioned otherwise. We varied the regularization parameter (C) over the values: {10-4, 2·10-4, 5·10-4, 10-3, . . . , 500, 103}. Model selection was done using the validation

466

Feature Set Text Stats POS ODP
QLOG

Examples
Unigram Counts # Words, # Characters, # Impressions, Click Count, Click Entropy
Part-of-Speech Tag Counts Five Most Probable ODP Class Scores from Top Two Levels Average Similarity with co-session queries, Average session length, Distribution of occurrences within session (start/middle/end)

Cardinality 44140 10 37 219
55

Coverage 100% 81% 100% 25%
44%

Normalized? No Yes No Yes
Yes

Log? No Yes No Yes
No

Table 2: Features used for identification of initiator queries

set by selecting the model with the best precision using the default margin score threshold (i.e., 0).
Features: The features are broadly grouped into 5 classes as shown in Table 2. Apart from the text and POS tag features, all other features were normalized to zero mean, unit variance. Features with values spanning multiple orders of magnitude, such as the number of impressions, were first scaled down via the log function. Due to the large scale of our data, coverage of some features is limited. In particular, query classification was done similar to [4] by selecting the top 9.4M queries by frequency from a year's query logs previously in time and then using a click-based weighting on the content-classified documents receiving clicks2. Likewise Stats and QLOG features are built from four months' worth of query logs and have limited coverage as a result. The query logs chosen to build these features were from previous to April 2012 to ensure a fair experimental setting with no overlap with the data collection period of the intrinsically diverse or regular sessions. We found the coverage of these features to be roughly the same for both the positive and negative classes.
We also note that the cardinality of some feature sets will depend on the training set (e.g.,vocabulary size of Text grows with more training data); the values listed in Table 2 are for the default training set of 50,000 queries. Most of our experiments will use all of the 5 feature sets; the effect of using only a subset of the feature sets is explored in Sec. 4.3.
4.2 Can we predict ID task initiation?
To begin with, we would like to know the precision-recall tradeoff that we can achieve on this problem. Figure 1 shows the precision-recall curve for a linear SVM trained on 50K examples with all the features. The result is a curve with clear regions of high precision, indicating that the SVM is able to identify initiator queries in these regions quite accurately. Furthermore, performance is better than random (precision of 50% since classes are balanced) along the entire recall spectrum.
As Table 3 shows, we are able to achieve relatively high precision values at low recall values. For example, we can identify 20% of ID tasks with 80% precision.
4.3 Which features were most important?
We next investigate the effect of using different subsets of the features on performance. The results are shown in Figure 2 and Table 4. First, we note that Stats, QLOG and ODP feature sets help identify only a small fraction of the initiator queries but do so with high precision. On the
2For greater coverage this could be extended to a rankweighted back-off as described in that paper.

Recall @ Precision

5.9

90

9.8

85

18.3

80

30.3

75

49.0

70

61.4

65

78.8

60

Precision @ Recall

84.9

10

79.3

20

75

30

72.8

40

69.4

50

65.4

60

62.4

70

Table 3: Recall at different precision levels and viceversa for predicting ID task initiation.
Precision-Recall Graph
100
SVM Performance
90

Precision

80

70

60

50 0 10 20 30 40 50 60 70 80 90 100 Recall
Figure 1: P-R curve for predicting ID task initiation.

other hand, the Text and POS feature sets, which have high coverage, provide some meaningful signal for all the queries, but cannot lead to high precision classification. We also find that a combination of features, such as the Text and Stats features, can help obtain higher precision as well as higher recall than either alone. In fact, such combinations perform almost as well as using all features, which is the best out of all feature combinations.

Effect of Features Used

100 Text

Stats

90

POS

ODP

QLOG

80

Text+Stats

Text+Stats+POS+ODP

All 70

Precision

60

50 0 10 20 30 40 50 60 70 80 90 100 Recall
Figure 2: Change in classification performance of initiator queries as feature sets are varied.
4.4 Linguistic features of initiator queries
To further understand ID initiator queries, we identified the part-of-speech and text features most strongly associ-

467

Feature Set
T S P O Q TS TSPO TSPOQ

Rec@80%Prec
0.1 9.2 0.0 5.6 9.4 13.6 12.2 18.3

Prec@40%Rec
62.6 63.7 52.8 51.6 54.1 69.7 67.0 72.8

Table 4: Effect of feature set on precision & recall. T=Text, S=Stats, P=POS, O=ODP, Q=QLOG

ated with them, by computing each feature's log-odds ratio (LOR)3 compared to regular queries. Looking at the topranked features by LOR, we found that initiator queries are more likely to use question words (LOR=0.41); focus on proper nouns (0.40) such as places and people; use more `filler' words (particles) found in natural language (0.27); and when they use general nouns, these tend to be plural (0.13) instead of singular (-0.052). Predominant text features indicated the importance of list-like nouns such as forms, facts, types, ideas (LOR=1.59, 1.45, 1.25, 0.92); verbs that are commonly used in questions such as did (1.34); and words indicating a broad need such as information and manual (1.64, 1.18). Strong negative features tend to encode exceptions ­ such as the most negative word lyrics (-2.25) used to find words to specific songs.
5. RE-RANKING FOR INTRINSIC DIVERSITY
While the previous section discusses the identification of queries that lead to ID tasks, in this section we discuss changes that can be made to the search results page to support queries for ID tasks. Specifically, we propose a reranking scheme that looks to satisfy not only the information need of the issued query, but also the future queries that the user is likely to issue later in the session on other aspects of the task. To the best of our knowledge, we are the first to address the problem of jointly satisfying the current query as well as future queries (unlike anticipatory search [25] which focuses solely on the latter).
We will use an interactive ranking-based paradigm here, using an approach related to the two-level rankings proposed in [32]. Given an issued query representing the start of an ID task, we consider rankings where each result can be attributed to some aspect of that task. We represent each aspect of the ID task by a related query of the issued query. One way this could be surfaced on a results page for a user is by placing the related query for an aspect adjacent to its corresponding search result. In such a setting, clicking on the related query could lead to results for that query being presented, thus enabling the user to explore documents for that aspect. This brings us to the question of how we find such a ranking.
5.1 Ranking via Submodular Optimization
We first describe precisely what we consider as an interactive ranking. In response to an initial query q, an interactive ranking y = (yD, yQ) comprises two parts: a ranking
3The LOR can be thought of as an approximation to the weight in a single-variable logistic regression.

of documents yD = d1, d2, . . ., which we refer to as the primary ranking; and a corresponding list of related queries yQ = q1, q2, . . ., which represent the aspects associated with the documents of the primary ranking. The ith query in the list, qi, represents the aspect associated with di. Structurally this can also be thought of as a ranked list of (document, related query) pairs (di, qi).
Given this structure, let us consider four conditions that comprise a good interactive ranking:
1. Since the documents in the primary ranking were displayed in response to the issued query q, they should be relevant to q.
2. As document di is associated with the aspect represented by the related query qi, document di should be relevant to query qi.
3. Aspects should be relevant to the ID task being initiated by the query q.
4. At the same time, the aspects should not be repetitive i.e., there should be diversity in the aspects covered.

We now design a ranking objective function that satisfies

these four conditions to jointly optimize the selection of doc-

uments and queries (yD, yQ). Suppose we have an existing interactive ranking y(k-1) that has k - 1 (document, related

query) pairs, and our goal is to construct a new ranking

y(k) by adding an optimal (document, related query) pair to

y(k-1) ­ an operation we denote by y(k) = y(k-1)  (dk, qk).

Condition 1 above can be met by selecting dk such that

R(dk|q) is large, where R(d|q) denotes the probability of rel-

evance of document d given query q. Condition 2 can be met

by selecting dk such that its relevance to the related query

qk, R(dk|qk), is large. Conditions 3 and 4 imply a standard

diversification tradeoff, but here we have that the aspects

qk should be related to the initial query q and diverse. If

we use a similarity function between queries to estimate the

relevance between queries, Condition 3 implies that the sim-

ilarity function Sim(q, qk) between qk and q should be large.

Condition 4 requires that the diversity should be maximized

between qk and all previous queries Q = q1, . . . , qk-1. Both

Condition 3 and 4 can be jointly obtained by optimizing an

MMR-like diversity function [6], Div(qk, Q), as described

below. Intuitively, we would also like the change in the ob-

jective function on adding document-query pair (dk, qk) to

the ranking y to be no smaller than what we would gain if

adding the pair to a larger ranking y  y : that is, the ob-

jective function should be monotone and submodular. Sub-

modular objectives are desirable because they have the prop-

erty that they can be optimized using a simple and efficient

greedy algorithm which iteratively computes the next best

(d, q) pair to add to the ranking. Using the greedy algo-

rithm

ensures

that

the

computed

solution

is

at

least

(1

-

1 e

)

times as good as the optimal.

We now consider the following objective satisfying the above conditions4:

n

argmax(d1 ,q1 )···(dn ,qn )

i · R(di|q) · R(di|qi) · eDiv(qi,Q)

i=1

where Q is shorthand for the set of queries Q = {q1, . . . , qn},

4We omit the straightforward submodularity proof for space reasons.

468

and Div(·) is an MMR-like diversity function defined as

Div(qi, Q) =  · Sim(qi, Snip(q))

(1)

- (1 - ) max Sim(Snip(qi), Snip(qj)).
j<i

Here,   [0, 1] and  > 0 are parameters, where  controls

the tradeoff between related query aspect relevance and di-

versity while  controls the rate at which returns diminish

from additional coverage. Finally, i refers to the discount

factor

for

position

i:

we

use

the

common

1 log2 (i+1)

DCG

dis-

counting.

This objective can be interpreted as maximizing an ex-

pected utility (the exponential term) of covering related and

diverse aspects where the expectation is over the maximum

joint relevance of a document to both the initial query and

the related query aspect. Furthermore, the joint probability

is assumed to be conditionally independent to factor into

the two relevance terms.

In this study, we define Sim(x, y) as the cosine similarity

between word-TF representations of x and y, and Snip(qj)

is the bag-of-words representation of caption text from the

top-10 search results for qj using relevance score R(d|qj)

alone. The MMR-like term appears within the exponent to

ensure the objective is monotone.

Note that while the final objective optimizes for an inter-

active ranking, the primary ranking itself aims to present re-

sults from other aspects. We optimize this using the greedy

algorithm presented in Algorithm 1, which we refer to as

the DynRR method. In Alg. 1, the function RelQ(q) de-

notes a function that returns related queries for query q, and

T op(yD) returns the top element in the ranking yD.

Algorithm 1 Greedy-DynRR(, , P (·|·), q)

1: (yD, yQ)   2: for all q  RelQ(q) do 3: N ext(q )  Document Ranking by R(·|q) · R(·|q ).

4: for i = 1  n do

5: bestU  -

6: for all q  RelQ(q)/ yQ do

7:

d  T op(N ext(q )/ yD)

8:

v  R(d |q) · R(d |q ) · e·Div(q ,yQ)

9:

if v > bestU then

10:

bestU  v

11:

bestQ  q

12:

bestD  d

13: (yD, yQ)  (yD, yQ)  (bestD, bestQ)

14: return y

5.2 Evaluation Measures
As the problem of presenting results for both the current as well as future queries is a new one, we first discuss the evaluation methodology used. In particular, we use two kinds of evaluation metrics:
Primary ranking metrics: To compare against standard non-interactive methods of ranking, we simply evaluate the quality of the primary ranking, i.e., completely ignore the related query suggestions attributed to documents. Since our goal is whole-session relevance, documents are considered relevant if and only if they are relevant to any query in the session. Given this notion of relevance, we compute the Precision, MAP, DCG and NDCG values.

Dataset # Train # Test

MINED 8888

2219

MIXED 4120

1027

Table 5: Datasets used in re-ranking experiments

Interactive ranking metrics: To evaluate the offline effectiveness and accuracy of the predicted future aspects (queries) and results (documents), we need to assume some model of human interaction. Consider the following search user model:
1. Users begin at the top of the ranking. 2. They click/expand the related query attributed to a
document if and only if the document is relevant or the query is relevant. (We say a query is relevant if the top k results of the query contain a (new) relevant document.) 3. On expanding the related query, the user views the top k results for that related query, before returning to the original document ranking and continuing. 4. Users ignore previously seen documents, and click on all new relevant documents.
Under this user model, we can easily trace the ranking of documents that the user navigates and thus evaluate Precision@10 and DCG@10 for this ranking. We refer to these metrics as P recUk and DCGUk, and compare them with the primary Prec@10 and DCG@10 metrics.
We do not claim that this user model accurately captures all online users, nor that it is sophisticated. This is simply a well-motivated model for analyzing a rational user's actions, assuming the user is relatively accurate at predicting the relevance of an aspect based on either the top document or its related query. This in turn is intended to inform us about trends and relative differences we may see in online studies.
5.3 Experimental Setup
DATA: To evaluate the efficacy of the method, we used the data obtained from mining the search logs, as described in Section 3. We used two main datasets shown in Table 5. To analyze impact when most of the sessions are ID and more complex, the MINED dataset is obtained directly from the filtering algorithm by setting the threshold on the Number of Distinct Aspects to be 5. To determine the reranking impact when sessions may be a mixture of both ID and regular sessions, the MIXED dataset was obtained by predicting when a session was ID using the classifier from Sec. 4 over a mixture of the MINED dataset sessions and a random sample of regular sessions of the same size. More specifically, the combined sessions were split in a 45-10-45 split of training-validation and test sets. The trained classifier was used to classify the test set sessions as being ID or not, based on the initiator query. The sessions predicted as ID formed the MIXED dataset (prediction accuracy of 68.8% over the combined sessions); for those not predicted to be ID, we assume the standard ranking algorithm would be applied and thus relevance would be the same on those. The MIXED dataset is a reflection of an operational setting, where the query issued is used to predict if the resulting session will be an ID session or not, and the ones predicted to be ID are selected for re-ranking.
Obtaining Probability of Relevance: For our algorithm, we required the computation of the conditional relevance of a document given a query i.e., R(d|q). Thus, to en-

469

Query Website Baseline Ranker
URL
Anchor (Weighted) Anchor (Unweighted)

Length
Log(PageRank) Reciprocal Rank
(if in top 10) Length
# of Query Terms Covered Fraction of Query Covered
TF Cosine sim LM Score(KLD)
Jaccard Boolean AND Match Boolean OR Match
Same as URL TF-Cosine Sim
KLD Score

Table 6: The 21 features used to train R(d|q).

able easier reproducibility by others, we learned a model using Boosted Regression Trees, on a dataset labeled with the relevance-values for query-document pairs with 20,000 queries using graded relevance judgments (60 documents per query). The features used are given in Table 6. Features were all normalized to 0 mean, unit variance. To obtain the final model, we optimized for NDCG@5.
Baselines: As baselines we used the following methods:
· RelDQ: Ranking obtained by sorting as per R(d|q). · Baseline: A state-of-the-art commercial search en-
gine ranker (also used to compute the rank feature mentioned earlier).
We also computed performance of other baselines, such as MMR and relevance-based methods such as BM-25 (using the weighted anchor text), but found them to perform far worse than RelDQ and Baseline and hence do not present the results for such other baselines.
Related Queries: To study the effect of the related queries, we used four different sources:
· API: We used the publicly available API of a commercial search engine (which returns 6-10 related queries)
· Click-Graph: Using co-click data, we obtained a set of 10 - 20 related queries.
· Co-Session Graph: Using data of queries co-occurring in the same session, we obtained 10-20 related queries.
· Oracle: As an approximate upper bound, we used the actual queries issued by the user during the session.
To ensure fairness, the graphs were constructed using data prior to April 2012. For most experiments, we only use the first 3 sources or only the second and third (which we distinguish by the suffix C+S).
Settings: The parameters for DynRR were set by optimizing for DCGU3 on the training data5. All numbers reported here are for the test sets. We considered all SATclicked results in the session as relevant documents; since we compare relative to the baseline search engine, the assumption is that placing the SAT-clicked documents higher is better, rather than being an indication of absolute performance. Unless otherwise mentioned, the candidate document set for re-ranking comprises the union of the top 100 results (from the Baseline method) of the initiator query, and the top 10 results from each related query.
5We varied the  parameter from 0 to 1 in increments of 0.1, while the  parameter was varied across the values {0.1, 0.3, 1, 3, 10}.

PREC
P recU1 P recU2 P recU3 P recU5

Mined
1.093 1.247 1.347 1.401

Mixed
1.103 1.223 1.295 1.345

DCG
DC GU1 DC GU2 DC GU3 DC GU5

Mined
1.075 1.188 1.242 1.254

Mixed
1.074 1.153 1.190 1.204

Table 8: Interactive Performance of DynRR for different user models (as ratios compared to the Baseline Prec@10 and DCG@10)

Set Mined Mixed

Comp. Metric
DC GU3 DCG@10 DC GU3 DCG@10

% Gains 0.2 0.5 1.0
34.4 13.0 1.6 19.6 5.2 0.3 29.1 12.0 1.6 17.7 6.0 0.8

% Losses 0.2 0.5 1.0
9.9 2.7 0.1 12.7 3.8 0.3 10.8 3.7 0.2 12.9 4.0 0.2

Table 9: % of sessions for which the metric performance of DynRR differs from the Baseline DCG@10 by more than a certain threshold.

5.4 Results
Primary Evaluation: We first study the re-ranking without any interactivity i.e., using the primary ranking metrics to evaluate the quality of the top-level ranking. As seen in the results of Table 7, the re-ranking leads to improvements across the different metrics for both datasets. Thus, even without interactivity, the method is able to outperform the baselines in predicting future results of interest to the user, while also providing results for the current query. In particular, we found the DynRR method works best using the C+S related queries (which we return to later) with 9-11% gains over the baselines at position 10 across the various metrics with 3-5% relative gains. We also find that the method improves on the MIXED dataset supporting the question of whether the method can be robustly used in practical scenarios. Thus we improve an important segment of tasks while maintaining high levels of performance elsewhere; further improvements to the initiator classification model will improve the robustness further.
Interactive Evaluation: Next we evaluate the performance of the method while incorporating the interactivity. As seen in Table 8, the added interactivity leads to large increases in both the precision and DCG of the user paths navigated, across the different user models and datasets. In fact, we find 30-40% improvements in precision and 20-25% improvements in DCG, indicating that we are able to do a far better job in predicting future relevant results, and potentially, queries. These results also show that the method improvements are relatively robust to the user model.
Robustness: A key concern when comparing a new method against a baseline, is the robustness of the method. In particular, we are interested in the number of queries that are either improved or hurt, when switching from the Baseline method to the proposed re-ranking method. This is particularly crucial for the MIXED dataset, as we would want that the performance on non-ID sessions not be severely affected. Table 9 displays the % of examples for which the method either gains or loses above a certain threshold, compared to the Baseline method. We see that the number of gains far exceeds the number of losses, especially while comparing the interactive metric. We should also note that for

470

Set Mined Mixed

Method
RelDQ DynRR DynRR C+S RelDQ DynRR

@1 1.00 1.06 1.10 1.00 1.03

Prec @3 0.94 1.03 1.09 0.94 1.02

@10 0.97 1.02 1.09 0.99 1.04

@1 1.00 1.06 1.10 1.00 1.03

MAP @3 0.97 1.05 1.10 0.98 1.04

@10 0.98 1.04 1.10 0.98 1.03

@1 1.00 1.06 1.10 1.00 1.03

DCG @3 0.97 1.04 1.10 0.96 1.03

@10 0.99 1.04 1.11 0.98 1.03

NDCG @1 @3 @10 1.00 0.97 0.99 1.06 1.05 1.05 1.09 1.10 1.11 1.00 0.97 0.98 1.03 1.03 1.05

Table 7: Primary Performance of different methods(as a ratio compared to the Baseline)

RelQ A C S O AS AC CS
ASC ASCO

Prec 0.905 1.015 1.051 1.476 0.961 0.986 1.089 1.019 1.179

DCG 0.880 1.014 1.074 1.397 0.961 1.013 1.106 1.039 1.144

P recU3 1.082 1.333 1.248 2.211 1.271 1.244 1.413 1.347 1.580

DC GU3 0.997 1.214 1.198 1.827 1.157 1.176 1.306 1.242 1.386

SDCG 1.174 1.488 1.384 2.500 1.452 1.408 1.593 1.529 1.802

Table 10: Performance change on varying the related queries. All measures are @10 and reported as a ratio to the baseline values.

Task IsTopicID? AreQueriesID? BestInitiatorQ

Fleiss Kappa .423 .452 .694

% All agree 85.5 67.1 55.3

% 2 agree 100 100 98.7

Table 11: Annotator agreement on TREC data.

both datasets and both metrics, the DynRR method is statistically significantly better than the Baseline method, as measured by a binomial test at the 99.99% significance level.
Effect of related query set: Next we study the impact of the related queries on the method performance, using the MINED dataset. To do so, we considered different combinations of the four related query sources: API(A), ClickGraph(C), Co-Session(S) and Oracle(O). Table 10 shows the results. As we clearly see, the related query source can make a significant impact on both the primary ranking performance and the interactive performance. One thing which stands out is the extremely strong performance using the Oracle related queries, which suggest that any improvements we can make in the quality of the suggested related queries is likely to result in even better overall performance. On the other hand, we see that using the API related queries almost always hurts performance. In fact, simply using only the related queries from the click-graph and the co-session data leads to much better performance than that compared to using the API queries as well. Further analysis reveals that this is due to two reasons: (a) In many cases, the queries returned by the API are spelling corrections or reformulations, with no difference in aspect; (b) most importantly though, there is little to no diversity in the queries obtained from the API compared to those from the other sources.
5.5 TREC Session Data
We also ran experiments using the publicly available TREC 2011 Session data (which was constructed with ID topics in mind) using only publicly reproducible components. To do so, three assessors labeled the different sessions as poten-

Initiator Title Title First First Label Label

Method Baseline DynRR Baseline DynRR Baseline DynRR

Pr@1 0.58 0.71 0.53 0.5 0.55 0.61

Pr@3 0.60 0.60 0.47 0.48 0.51 0.5

DCG@1 0.84 1.39 0.94 0.92 0.87 1.13

DCG@3 2.13 2.41 1.94 1.97 1.95 2.09

Table 12: Absolute performance on TREC Session data.  indicates significance at p = 0.05 by a paired
one-tailed t-test.

tially being intrinsically diverse or not, based: a) only on

the queries issued; and b) on the narration and title of the

session as well. We also asked them to label their opinion on

the query best suited to be the initiator query, among the

queries issued. Annotators were provided the definition of

ID sessions as described at the start of Section 3. We found

good agreement among the different annotators for all the

different labeling tasks, as seen from Table 11. In fact, in 63

of the 76 total sessions all three annotators agreed the ses-

sions were ID based on the narration, title, and queries. We

used a 50-50 training-test split on all sets, with the training

data used for selecting the parameters of the ranking meth-

ods. To obtain the conditional relevance R(d|q), we trained

a regularized linear regression model with features based on

the scores of two standard ranking algorithms: BM25 and

TFIDF. As labeled data we used the TREC Web data from

2010 and 2011, by converting the graded relevance scores for

relevant and

above

from

the

{1, 2, 3}

scale

to

{

1 3

,

1, 1}.

We

used related queries from the Van Dang-Croft [10] method

(Q) on the ClueWeb '09 anchor text, where the starting seed

for the random walk would use the most similar anchor text

to the query by TFIDF-weighted cosine if an exact match

was not available. Our candidate document pool was set

similar to the previous experiments. To evaluate, we again

use the same metrics as before except using the TREC asses-

sor relevance labels instead of clicks. We considered three

different candidates for the initiator query: (a) Topic; (b)

First query in the session; and (c) Labeled initiator query.

As a baseline, we considered the method that ranked as per

R(d|q). For the DynRR method, we used the titles of the

top 10 results of a query (as per the baseline), as the snippet

of the query, since snippets were not made available. The

results for the primary metric comparison are shown in Ta-

ble 12. As we see from the table, the method improves in

precision and DCG in most cases, with particularly large im-

provements when the title of the topic is used as the initiator

query. This matches feedback the assessors gave us that the

titles looked much more like the general queries issued by

web users; in contrast, the TREC sessions would often start

with a specific query before moving to a more general query.

It could be that supplying the user with a well-formulated

topic description before starting the search task influences

471

the user to search for a particular aspect, rather than issue a more general query as they might when no topic description is explicitly formulated.
6. CONCLUSIONS AND FUTURE WORK
In this paper, we studied intrinsically diverse tasks that typically require multiple user searches on different aspects of the same information need. As this is just the first step into this problem, this also opens many interesting future directions ­ such as iterative ways to combine the mining and query identification process or extending these techniques to other related problems like exploratory search. In this work, we motivated the problem using real-world data and presented an algorithm to mine data from search logs using behavioral interaction signals within a session. We then looked at the problem of identifying the queries that start these sessions, and treated it as a classification problem, along with an analysis of these queries. Finally, we presented an approach to alter the rankings presented to the user, so as to also provide them information on aspects of the task for which the user will search in the future. We validated our approach empirically using search log data, as well as TREC data, demonstrating significant improvement over competitive baselines in both cases.
7. REFERENCES
[1] E. Agichtein, R. White, S. Dumais, and P. Bennett. Search, Interrupted: Understanding and Predicting Search Task Continuation. In SIGIR '12, 2012.
[2] P. Bailey et al. User task understanding: a web search engine perspective. http://research.microsoft.com/apps/pubs/default.aspx?id=180594, 2012.
[3] P. L. Bartlett, M. I. Jordan, and J. M. Mcauliffe. Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates. In NIPS 16, 2004.
[4] P. N. Bennett, K. Svore, and S. Dumais. Classification-Enhanced Ranking. In WWW '10, 2010.
[5] C. Brandt, T. Joachims, Y. Yue, and J. Bank. Dynamic Ranked Retrieval. In WSDM '11, 2011.
[6] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR '98, 1998.
[7] H. Chen and D. R. Karger. Less is more: Probabilistic models for retrieving fewer relevant documents. In SIGIR '06, 2006.
[8] C. L. Clarke et al. Novelty and diversity in information retrieval evaluation. In SIGIR '08, 2008.
[9] W. Dakka et al. Automatic Discovery of Useful Facet Terms. In SIGIR 2006 Workshop on Faceted Search, 2006.
[10] V. Dang, M. Bendersky, and W. B. Croft. Learning to rank query reformulations. In SIGIR '10, 2010.
[11] S. Fox, K. Kulddep, M. Mydland, S. Dumais, and T. White. Evaluating implicit measures to improve web search. ACM TOIS, 23(2):147­168, 2005.
[12] J. Gao, W. Yuan, X. Li, K. Deng, and J. Nie. Smoothing clickthrough data for web search ranking. In SIGIR '09, 2009.
[13] S. Gollapudi, S. Ieong, A. Ntoulas, and S. Paparizos. Efficient query rewrite for structured web queries. In CIKM '11, 2011.
[14] A. Hassan, Y. Song, and L.-w. He. A task level metric for measuring web search satisfaction and its application on improving relevance estimation. In CIKM '11, 2011.
[15] A. Hassan and R. W. White. Task tours: helping users tackle complex search tasks. In CIKM '12, 2012.
[16] J. He et al. CWI at TREC 2011: session, web, and medical. In TREC '11, 2012.

[17] K. J¨arvelin, S. L. Price, L. M. L. Delcambre, and M. L. Nielsen. Discounted cumulated gain based evaluation of multiple-query IR sessions. In ECIR'08, 2008.
[18] T. Joachims. Making large-scale support vector machine learning practical. In Advances in kernel methods, pages 169­184. MIT Press, 1999.
[19] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. TOIS, 25(2), Apr. 2007.
[20] R. Jones and K. Klinkner. Beyond the session timeout: Automatic hierarchical segmentation of search topics in query logs. In CIKM '08, 2008.
[21] E. Kanoulas, B. Carterette, P. D. Clough, and M. Sanderson. Evaluating multi-query sessions. In SIGIR '11, 2011.
[22] E. Kanoulas et al. Overview of the TREC 2011 Session Track. In TREC '11, 2012.
[23] C. Kohlschutter, P.-A. Chirita, and W. Nejdl. Using link analysis to identify aspects in faceted web search. In SIGIR '06, 2006.
[24] A. Kotov, P. N. Bennett, R. W. White, S. T. Dumais, and J. Teevan. Modeling and analysis of cross-session search tasks. In SIGIR '11, 2011.
[25] D. J. Liebling, P. N. Bennett, and R. W. White. Anticipatory search: using context to initiate search. In SIGIR '12, 2012.
[26] J. Liu and N. Belkin. Personalizing information retrieval for multi-session tasks: The roles of task stage and task type. In SIGIR '10, 2010.
[27] J. Liu, M. J. Cole, C. Liu, R. Bierig, J. Gwizdka, N. J. Belkin, J. Zhang, and X. Zhang. Search behaviors in different task types. In JCDL '10, 2010.
[28] H. Ma, M. R. Lyu, and I. King. Diversifying query suggestion results. In AAAI '10, 2010.
[29] D. Morris, M. R. Morris, and G. Venolia. Searchbar: A search-centric web history for task resumption and information re-finding. In CHI '08, 2008.
[30] F. Radlinski, P. N. Bennett, B. Carterette, and T. Joachims. Redundancy, diversity and interdependent document relevance. SIGIR Forum, 43(2):46­52, Dec. 2009.
[31] F. Radlinski and T. Joachims. Query chains: learning to rank from implicit feedback. In KDD '05, 2005.
[32] K. Raman, T. Joachims, and P. Shivaswamy. Structured learning of two-level dynamic rankings. In CIKM '11, 2011.
[33] A. Singla, R. White, and J. Huang. Studying trailfinding algorithms for enhanced web search. In SIGIR '10, 2010.
[34] A. Slivkins, F. Radlinski, and S. Gollapudi. Learning optimally diverse rankings over large document collections. In ICML '10, 2010.
[35] R. White and S. Drucker. Investigating behavioral variability in web search. In WWW '07, 2007.
[36] R. W. White, P. N. Bennett, and S. T. Dumais. Predicting short-term interests using activity-based search context. In CIKM '10, 2010.
[37] R. W. White, G. Marchionini, and G. Muresan. Editorial: Evaluating exploratory search systems. Inf. Process. Manage., 44(2):433­436, Mar. 2008.
[38] X. Yuan and R. White. Building the trail best traveled: effects of domain knowledge on web search trailblazing. In CHI '12, 2012.
[39] Y. Yue and T. Joachims. Predicting diverse subsets using structural SVMs. In ICML '08, 2008.
[40] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR '03, 2003.
[41] L. Zhang and Y. Zhang. Interactive retrieval based on faceted feedback. In SIGIR '10, 2010.
[42] Q. Zhao et al. Time-dependent semantic similarity measure of queries using historical click-through data. In WWW '06, 2006.

472

Pseudo Test Collections for Training and Tuning Microblog Rankers

Richard Berendsen r.w.berendsen@uva.nl

Manos Tsagkias e.tsagkias@uva.nl

Wouter Weerkamp w.weerkamp@uva.nl

Maarten de Rijke derijke@uva.nl

ISLA, University of Amsterdam, Amsterdam, The Netherlands

ABSTRACT
Recent years have witnessed a persistent interest in generating pseudo test collections, both for training and evaluation purposes. We describe a method for generating queries and relevance judgments for microblog search in an unsupervised way. Our starting point is this intuition: tweets with a hashtag are relevant to the topic covered by the hashtag and hence to a suitable query derived from the hashtag. Our baseline method selects all commonly used hashtags, and all associated tweets as relevance judgments; we then generate a query from these tweets. Next, we generate a timestamp for each query, allowing us to use temporal information in the training process. We then enrich the generation process with knowledge derived from an editorial test collection for microblog search.
We use our pseudo test collections in two ways. First, we tune parameters of a variety of well known retrieval methods on them. Correlations with parameter sweeps on an editorial test collection are high on average, with a large variance over retrieval algorithms. Second, we use the pseudo test collections as training sets in a learning to rank scenario. Performance close to training on an editorial test collection is achieved in many cases. Our results demonstrate the utility of tuning and training microblog search algorithms on automatically generated training material.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
Keywords
Simulation, pseudo test collections, learning to rank, microblog retrieval
1. INTRODUCTION
Modern information retrieval (IR) systems have evolved from single model based systems to intelligent systems that learn to combine uncertain evidence from multiple individual models [10, 22]. The effectiveness and flexibility of such systems has led to wide adoptation in IR research. A key contributor to the success of such systems is the learning phase, i.e., the training set they are given for
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

learning. Training sets have to be tailored to the task at hand and, in contrast to the systems themselves, do not generalize to other tasks. This characteristic requires compiling task-specific training sets, which is a time consuming and resource intensive process, as it usually involves human labor. Automating the process of compiling training sets has obvious advantages in reducing costs, while it simultaneously increases the size of the training set. This observation has led to a persistent interest in finding ways for generating so-called pseudo test collections, which consist of a set of queries, and for each query a set of relevant documents (given some document set). In this paper, we consider the problem of generating pseudo test collections for microblog search.
Microblog search is the task of finding information in microblogs, such as Facebook status updates, Twitter posts, etc. The task became popular with the advent of social media and is distinct from web search and from blog search due mainly to its real-time nature, the very limited length of microblog posts and the use of "microblog language," e.g., hashtags, mentions, which can provide useful information for retrieval purposes. In 2011 the Text REtrieval Conference (TREC) launched the Microblog track aimed at developing a test collection from Twitter data and evaluating systems' performance on retrieving--given a query and time-stamp--relevant and interesting tweets in a simulated real-time scenario. Several participants approached the task using learning to rank methods for combining evidence from multiple rankers [21]. This approach to microblog search comes natural because of the many dimensions available for ranking microblog posts, e.g., recency, user authority, content, existence of hyperlinks, hashtags, retweets. For training a learning to rank-based system at the TREC 2011 Microblog track, participants used a traditional supervised method: many manually labeled data for compiling a training set. What if we could generate the required training sets automatically? In 2012 a second edition of the Microblog track was organized. This gives us the opportunity to compare what yields better learning to rank performance: training on the 2011 relevance assessments, or training on automatically generated ground truth?
Our starting point is the following intuition, based upon the observation that hashtags tend to represent a topic in the Twitter domain: From tweets Th associated with a hashtag h, select a subset of tweets Rh  Th that are relevant to an unknown query qh related to h. We build on this intuition for creating a training set for microblog rankers. To this end, we take several steps, each raising research questions. First, we select hashtags h and associated relevant tweets Rh. Can we just select all hashtags and use all their associated tweets? In microblog search, time is important: what is considered relevant to a query may change rapidly over time. A

53

microblog query, then, has a timestamp, and relevant tweets must occur prior to this timestamp. As for a query, the topic a hashtag is associated with may change over time. Can we exploit this analogy, and label hashtags with a timestamp, regarding tweets prior to this timestamp as relevant? Another well-known aspect of microblog posts is that they often contain casual conversation that is unlikely to be relevant to a query. Can we improve generated training sets by selecting interesting tweets and hashtags associated with such tweets? Once we have selected a hashtag h and a set of tweets Rh, how do we generate a query qh related to h?
The main contribution of this paper is a set of methods for creating pseudo test collections for microblog search. These collections are shown to be useful as training material for tuning well-known retrieval methods from the literature, and for optimizing a learning to rank method. In particular, we contribute: (1) unsupervised pseudo test collection generation methods; (2) a supervised pseudo test collection generation method, where we learn what are interesting tweets from TREC Microblog track assessments; (3) insights into the sensitivity of our methods to parameter settings.
2. PROBLEM DEFINITION
Below, we consider a number of instantiations of our pseudo test collection generator. For the purposes of the TREC Microblog track, a test collection for microblog search consists of queries with timestamps and a set of relevant documents for these queries. A pseudo test collection for microblog search consists of a set of queries Q, in which each query q  Q is associated with a timestamp qt and a set of relevant documents Rq. Given this definition, there are three main steps for generating a pseudo test collection for microblog search: generating (a) the query; (b) the query timestamp; and (c) a set of relevant tweets for the query.
We start from the following intuition: From the tweets Th that contain a hashtag h, we can select tweets Rh that are relevant to an unknown query qh related to h. In the next section, we present three methods to generate a pseudo test collection. Each method selects hashtags and for every hashtag h it selects tweets Rh from Th that will act as relevant tweets to a suitable query related to h. In §4, we present a technique for generating queries from Rh.
3. SELECTING HASHTAGS AND TWEETS
We propose four solutions to selecting hashtags and tweets for inclusion in a pseudo test collection. (A) Random: A sanity check baseline against our hypothesis that hashtags are good sources for generating pseudo test collections. Collections are created by randomly sampling a set of relevant tweets for each topic, without replacement. All these random collections are of a fixed size, equal to our largest hashtag-based pseudo test collection. (B) Hashtags: A naive method that serves as baseline in our experiments and that considers all hashtags and tweets to be equally important (§3.1). (C) Hashtags-T: A method that creates a test collection in the microblog retrieval sense, in which queries have timestamps (§3.2). (D) Hashtags-TI: A method that aims at capturing interestingness in tweets. Interesting tweets should contain good candidate terms for a query. We present a method with which we can estimate from example queries and relevant tweets the probability of interestingness of a tweet (§3.3).
3.1 Hashtags: All hashtags, tweets are equal
We select all hashtags, with a single requirement: that they are mentioned in a reasonable amount of tweets, m (Algorithm 1). There are three reasons for this lower bound: (i) it reflects a certain consensus about the meaning of a hashtag; (ii) we generate

Algorithm 1: Generating collection Hashtags
H - {h : |Th| >= m}; for h  H do
Rh - Th; Generate query qh from Rh; end

// See §4

our queries based on word distributions in these tweets: for this to work reliably, we need a reasonable amount of tweets, see §4; (iii) we train a learning to rank retrieval algorithm on our pseudo test collection; we hypothesize that it would benefit from a relative large set of positive training examples, see §5. We normalize hashtags by lowercasing them and removing any non-alphanumeric characters. In all our experiments, we set m = 50. The pseudo test collection generated by Algorithm 1 is called Hashtags.
3.2 Hashtags-T: Generating timestamps
Microblog search is sensitive to the query issue time because of the real time nature of tweets. To generate a timestamp for a query related to a hashtag h, we make an analogy between search volume over time for a query and publishing volume over time for tweets with h. Our assumption is that users often issue queries for trending topics because they want to monitor developments, similar to certain types of blog search [28]. We generate a timestamp for hashtag h just after peaks in publishing volume. In this way, our generated queries will be about trending topics. In addition, we keep a large amount of tweets from Th, while discarding a limited number, after h stops trending. In collections that span a considerable period of time, re-occurring topics, such as Christmas or Super Bowl, may quite likely be observed. In this case, one may want to assign multiple issue times for a query, depending on the number of observed peaks. Our corpus (see §5) covers a relatively short period, and we assign only one issue time to every query sampled.
In detail, our query issue time generation works as follows. First, we group the time span of the collection in 8-hours bins. Then, for each hashtag, we count how many relevant documents belong to each bin; this results in generating the hashtag's timeseries. In our setting, timeseries are short and sparse; our peak detection method aims at coping with this challenge. We find the bin with the most counts and resolve ties by taking the earliest date. This approach allows us to return a peak even for very sparse timeseries. We call the pseudo test collection generated by Algorithm 2: Hashtags-T.
3.3 Hashtags-TI: Selecting interesting tweets
Consider the following tweet: "Hey follow me here #teamfollowback #justinbieber." We hypothesize that this tweet would not be useful for sampling terms for topics labeled #teamfollowback or #justinbieber or as a relevant document for these topics. To avoid selecting such tweets, we rank tweets by their probability of interestingness and keep the best X percent. We think of a tweet

Algorithm 2: Generating collection Hashtags-T

H - {h : |Th|  m};

for h  H do

Generate timestamp t(h);

// See §3.2

Rh - { :   Th and t( )  t(h)};

end

H - {h : h  H and |Rh|  m};

for h  H do

Generate query qh from Rh;

// See §4

end

54

as interesting if it carries some information and could be relevant to a query. We use a set of criteria to capture interestingness and present a method to learn from example queries and relevant documents from an editorial collection.
Let C1, C2, . . . , Cn be random variables associated with the criteria and let I := I( ) = 1 be the event that a tweet is interesting. We estimate P (I |C1 = c1, . . . , Cn = cn), or, shorthand: P (I |c1, . . . , cn). Following Bayes' rule, we have

P (I |c1, . . . , cn)

=

P (c1, . . . , cn|I )P (I ) , P (c1, . . . , cn)

(1)

where P (I ) is the a-priori probability that a tweet is interesting, P (c1, . . . , cn|I ) is the likelihood of observing the evidence given that a tweet is interesting, and P (c1, . . . , cn) is the probability of observing the evidence. The crucial step is to estimate P (c1, . . . , cn|I ). We hypothesize that tweets that are known to be relevant to a query are interesting and estimate P (c1, . . . , cn|I ) with P (c1, . . . , cn|R ), where R is the event that a tweet is relevant to a query in an editorial collection. We use the TREC Microblog 2011 qrels for this estimation. Since we do not have enough relevant tweets to estimate the full joint probability, we assume conditional independence of ci given that a tweet is relevant:

P (I |c1, . . . , cn) 

i P (ci|R ) P (I ) . P (c1, . . . cn)

(2)

Since we rank tweets by interestingness we do not have to estimate P (I ). Instead, we have:

rank (P (I |c1, . . . , cn)) = rank

i log(P (ci|R )) . (3) P (c1, . . . , cn)

Most of the criteria we use have discrete distributions. For those that do not, we bin their values in B bins; we set B = 10. To avoid rejecting a tweet on the basis of one measurement that did not occur in any of the relevant tweets, we add one observation to every bin of every P (ci|R ) distribution. For P (c1, . . . , cn) we use the empirical distribution of all tweets in the collection. After selecting the best X percent of tweets, we again filter out hashtags that have less than 50 interesting tweets. We build this method on top of our pseudo test collection Hashtags-T, only ranking the tweets in this collection and keeping the best 50% of them. We call the pseudo test collection generated by Algorithm 3 Hashtags-TI.

Algorithm 3: Generating collection Hashtags-TI

H - {h : |Th|  m}; for h  H do

Generate timestamp t(h);

// See §3.2

Th,t - { :   Th and t( )  t(h)};

end

H - {h : h  H and |Th,t|  m}; T - hH Th,t;

// Rank tweets by probability of being

interesting

Rank T by P (I |c1, . . . , cn( ));

// See eq. 3

Let TI be the top X percent of this ranking;

for h  H do

Rh - Th,t  TI ; end

H - {h : h  H and |Rh|  m}; for h  H do

Generate query q from Rh; end

// See §4

The criteria we use build on textual features (density and capitalization) and microblog features (links, mentions, recency). Each criterion is discussed below. The marginal distributions P (ci|R ) of three criteria are shown in Fig. 1 as white histograms. They overlap with black histograms of all tweets in our Hashtags pseudo test collection. These criteria have different distributions over relevant tweets and over tweets that have a hashtag, which motivates our idea to keep tweets with high probability of interestingness.
Links. The existence of a hyperlink is a good indicator of the content value of a tweet. TREC Microblog 2011 defines interestingness of a tweet as whether a tweet contains a link [21]. Also, a large fraction of tweets are pointers to online news [19]. Tweets with links are likely to include terms that describe the linked web page, rendering them good surrogates for query terms [5].
Mentions. Tweets with mentions (@username) signify discussions about the hashtag's topic. This type of tweet is likely to be noisy because of their personal character. They may, however, bring in query terms used by a niche of people.
Tweet length. Document length has been shown to matter in retrieval scenarios [35]. Short tweets are less likely to contain terms useful for query simulation, see Fig. 1 (Center) for the distribution of tweet length.
Density. A direct measure for probing a tweet's content quality is the density score [20]. Density is defined as the sum of tf-idf values of non-stopwords, divided by the number of stopwords they are apart, squared:

Density( )

=

K K-

1

K-1 k=1

weight(wk) + weight(wk+1 distance(wk, wk+1)2

)

,

where K is the total number of non-stopwords terms in tweet  , wk and wk+1 are two adjacent keywords in  . weight(·) denotes the term's tf-idf score, and distance(wk, wk+1) denotes the distance between wk and wk+1 in number of stopwords. Fig. 1 (Left) shows the distribution of density scores of tweets.

Capitalization. The textual quality of tweets can partially be captured through the use of capitalization [41]. Words in all capitals are considered shouting and an indication of low quality. The ratio of capitals may indicate the quality of the text. Fig. 1 (Right) shows the distribution of the fraction of capital letters over tweet length.

Direct. A tweet is direct if it is meant to be a "private" message to another user (i.e., the tweets starts with @user).

4. GENERATING QUERIES
Sampling query terms is a challenging step in the process of automatically generating pseudo test collections. Azzopardi et al. [3] propose several methods for sampling query terms from web documents for known-item search, while Asadi et al. [2] avoid the problem by using anchor texts. Neither approach is applicable in the microblog setting due to a lack of both redundancy in the tweets and anchor texts. Terms in tweets usually occur at most once, but if not, this is often a signal for spam [26]. Probabilistic sampling methods that boost terms occurring with high probability are less likely to return good candidates for query terms. Tf-idf methods emphasize rare terms, which are likely to be spelling mistakes or descriptive of online chatter (e.g., "looooool") in our setting.
The log-likelihood ratio (LLR) is suitable for our problem of sampling terms from (tweets associated with) a given hashtag [25]. LLR is defined as the symmetric Kullback-Leibler divergence of the expected and observed term probabilities in two corpora. Terms are ranked by how discriminative they are for both corpora. For

55

 Th(447957)  Rq(2388)

 Th(447957)  Rq(2388)

 Th(447957)  Rq(2388)

0

100

200

300

0 20 40 60 80 100 120 0.0

0.2

0.4

0.6

0.8

1.0

density

length

capital

Figure 1: Distribution of (Left) density scores, (Center) tweet length, and (Right) capitalization. Where the histograms for the tweets associated with hashtags (dark grey) and for the TREC MB 2011 relevant tweets (white) overlap, the color is light grey.

our purposes, we set one corpus to be a set of tweets associated with hashtag h and the other to consist of the rest of the tweets in the collection. Stopwords, spam terms, or terms indicative of online chatter will rank lower because they occur in both corpora with roughly the same frequency.
Let T be a collection of tweets  , Rh a set of relevant tweets associated with a hashtag h resulting from one of the sampling methods described in §3, and w a term in  . For every w  Rh  we compute LLR(w), given corpora Rh and B = T \ Rh:

LLR(w) =

2·

ORh (w) log

ORh (w) ERh (w)

+

OB(w) log

OB (w) EB (w)

,

where ORh (w) = tf (w, Rh) and OB(w) = tf (w, B) are the observed term frequencies of w in Rh and B, respectively. ERh (w) and EB(w) are the expected values of the term frequency of w in Rh and B, respectively.
For every hashtag h, terms are ranked in descending order of their log-likelihood ratio score. We remove terms if one of the following pertains: (1) The term is equal to the hashtag up to the `#' character. In this case, all tweets in Th contain the hashtag, making the query very easy for all rankers, which then leads to a learning to rank method having a hard time distinguishing between rankers. (2) The term occurs in fewer than 10 documents. We generate queries that consist of the top-K ranked terms. For all pseudo test collections described in §3 we set K = 10. For our most promising method, we examine the impact of this parameter by generating queries of length 1, 2, 3, 5, and 20.

5. EXPERIMENTAL SETUP
The main research question we aim to answer is: What is the utility of our test collection generation methods for tuning retrieval approaches and training learning to rank methods?
Parameter tuning. We do parameter sweeps for some retrieval runs on different (pseudo) test collections, see Table 1 for details. We ask: (a) What is better in terms of retrieval performance: tuning on a different editorial test collection or tuning on a pseudo test collection? We answer by calculating how far performance obtained by tuning on either collection is from optimal performance for each retrieval model. (b) Do scores between a pseudo test collection and an editorial test collection correlate better than scores between edi-

torial test collections? We answer by calculating Kendall's tau and expected loss in effectiveness.
Learning to rank. We compare the utility of test collections as training material for a learning to rank algorithm. We ask: (a) What is better in terms of retrieval performance: training on a different editorial test collection, or training on a pseudo test collection? We answer by calculating the difference in retrieval performance between training on each. (b) Which of our pseudo test collections is most useful? (c) Do learning to rank algorithms trained on pseudo test collections outperform the best individual feature?
Parameter sensitivity. We also analyze parameter sensitivity of our methods, focusing on two parameters. First, in generating Hashtag-TI (§3), we keep the best X = 50% percent of tweets. How sensitive are our results to this method to different values for X? We try these values: 20, 40, 60, and 80. In all our experiments,
Algorithm 4: Training an LTR system on a pseudo test collection, and testing it on an editorial collection
for i = 1  N do // -- Training phase: -Generate the pseudo test collection; for each ranker do Sweep parameters on the pseudo test collection; Randomly sample parameter vector to use from winners; end for q  Q do Merge the ranked lists into Mq; Let positive training examples  Rq,h  Mq; Randomly sample |Rq,h| negative training examples from Mq \ Rq,h; end Learn a LTR model on the training set; // -- Testing phase: -for each ranker do Run on test topics using the sampled parameter vector; end Run the learned LTR model on the test set;
end

56

Indri 5.1

Ranker LM Tf-idf
Okapi
BOW BUW Tf-idf PL2 DFR-FD
QE

Table 1: Retrieval algorithms tuned.

Description

Parameter

Language modeling with Dirichlet smoothing Indri's implementation of tf-idf
Okapi, also known as BM25 [35] Boolean ordered window Boolean unordered window

µ k1 b k1 b k3 Window size Window size

Terrier's implementation of tf-idf A Divergence from Randomness (DFR) model [1] Terrier's DFRee model with a document score modifier that takes into account co-occurence within a window
Terrier's implementation of query expansion

b c Window size
No. documents No. terms

Values
{50, 150, . . . , 10050} {0.2, 0.4, . . . , 3} {0, 0.05 . . . , 1} {0, 0.2, . . . , 3} {0, 0.05 . . . , 1} {0} {1, 2 . . . , 15, inf} {1, 2 . . . , 15, inf}
{0, 0.05 . . . , 1} {0.5, 1, 5, 10} {2, 3 . . . , 15}
{1, 5, 10, 20, 30, 50} {1, 5, 10, 20, 30, 50}

cf. Literature [42] [33]
[33]
[33] [9] [31]
[23]

Terrier 3.5

when we generate queries, we keep the top 10 terms of the ranking produced by LLR (§4). For our best pseudo test collection, we ask how parameter tuning results and learning to rank performance is influenced by different query length. We try query lengths 1, 2, 3, 5, and 20.

5.1 Dataset and preprocessing

We use the publicly available dataset from the TREC 2011 and 2012 Microblog tracks. It covers two weeks of Twitter data, from January 24, 2011­February 8, 2011, consisting of approximately 16 million tweets. We perform a series of preprocessing steps on the content of tweets. We discard non-English tweets using a language identification method for microblogs [6]. Exact duplicates are removed; among a set of duplicates the oldest tweet is kept. Retweets are discarded; in ambiguous cases, e.g., where comments were added to a retweet, we keep the tweet. Punctuation and stop words are removed using a collection-based stop word list, but we keep hashtags without the `#' character. After preprocessing we are left with 4,459,840 tweets, roughly 27% of all tweets. Due to our aggressive preprocessing, we miss 19% of the relevant tweets per topic, on average. Our 10 retrieval models avoid using future evidence by using per topic indexes. For completeness, we note that our stopword list and the idf-weights in the density feature were computed on the entire collection. Pseudo test collections and both TREC microblog test collections also contain tweets from the entire collection. For generating queries, we index the collection with Lucene without stemming or stopword removal.
Table 2 lists statistics of the pseudo test collections generated with the methods described in §3, as well as statistics of the collec-

Table 2: Statistics for pseudo test collection generated from our methods and the TREC Microblog 2011 track.

Collection

Topics # Max Min Avg.
length

Relevant documents # Max Min Avg.

TREC MB 2011 49 6 1 3.4 TREC MB 2012 59 7 1 2.9

2,965 178 1 60.5 6,286 572 1 106.5

Random-1 Hashtags Hashtags-T Hashtags-TI

1,888 10 10 10 462560 245 245 245.0 1,888 10 10 10 462,013 16,105 50 244.7
891 10 10 10 212,377 9,164 50 238.4 481 10 10 10 98,586 4,949 50 205.0

H-TI-X20 H-TI-X40 H-TI-X60 H-TI-X80

175 10 10 10 32,221 3661 50 184.1 392 10 10 10 75,287 4804 50 192.1 576 10 10 10 121,639 5277 50 211.2 740 10 10 10 166,489 6956 50 225.0

tions generated by choosing different values for X. The HashtagsTI-QL{1,2,3,5,20} pseudo test collections are of the same proportions as Hashtags-TI, apart from the query length. We have listed only one of fifteen Random collections, but these are all of the same proportions: about as large as the Hashtags collection.
5.2 Learning to rank
We follow a two-step approach to learning to rank, outlined in Algorithm 4. First, we run several retrieval algorithms, then we rerank all retrieved tweets. For retrieval, we use the retrieval algorithms listed in Table 1, optimized for MAP [24, 34] after tuning on a training collection. In case of ties among parameter vectors for a ranker, we randomly sample a parameter vector. We also use a parameter free retrieval algorithm, DFRee [1]. For re-ranking, we compute three groups of features.
Query-tweet features. These are features that have different values for each query-tweet pair. We subdivide these as follows. Rankers: the raw output of each retrieval algorithm. For LM, BOW and BUW we transform the raw output X by taking the exponent: exp(X). Ranker meta features: the number of rankers that retrieved the tweet, the maximal, average, and median reciprocal rank of the tweet over all rankers. Recency: query-tweet time difference decay, computed as exp(t( ) - qt), where t( ) is the timestamp of the tweet and qt the timestamp of the query. We linearly normalize query-tweet features over all retrieved tweets for the query.
Query features. These are features which have the same value for every retrieved tweet within the same query. We use Query clarity, a method for probing the semantic distance between the query and the collection [11]. We linearly normalize query features over the set of retrieved tweets for all queries.
Tweet features. These are features that have the same value for each tweet independent of the query. We use the Quality criteria listed in §3: link, mentions, tweet length, density, capitalization, and direct. We linearly normalize tweet features over the set of retrieved tweets for all queries.
To build a training set, one needs positive and negative training examples. Let q  Q be a query from the training collection, Rq the set of relevant tweets for query q, and Mq the set of all retrieved tweets for q. Then, for each query in the training collection we use Rq  Mq as positive examples. To have a balanced training set, we randomly sample |Rq| tweets as negative training examples from Mq \ Rq.

57

Next, we feed the training set to four state of the art learners: (a) Pegasos SVM1 [36, 37], (b) Coordinate ascent [27], (c) RankSVM [17], and (d) RT-Rank [29]. We set Pegasos SVM to optimize the area under the ROC curve using indexed sampling of training examples, with regularization parameter  = 0.1 for a maximum of 100,000 iterations. We set coordinate ascent to optimize for MAP with = 0.0001 and maximum step size of 3. We use line search to optimize each feature with uniform initialization and consider only positive feature weights without projecting points on the manifold. For RankSVM we set the cost parameter to 1 per query. In preliminary experiments, RT-Rank performed poorly and therefore we choose to leave it out from our report.
Recall that training sets are compiled using tuned rankers and that in case of ties between different parameter vectors for a ranker, a random vector is selected. When compiling test sets for TREC MB 2011 and TREC MB 2012 to evaluate the utility of a training set, we use the exact same parameter vectors, so that the same set of features are used for training and testing.
Algorithm 4 has randomness in several stages: (i) when generating the pseudo test collection (only in the case of the Random collections), (ii) when sampling a winning parameter setting for each feature, (iii) when randomly sampling negative training examples, and (iv) during model learning. To obtain a reliable estimate of of the performance when training on a pseudo test collection, this procedure is repeated N = 10 times, each time generating a new pseudo test collection (in the case of the Random test collection), selecting random parameter vectors, selecting random negative training examples, and training an LTR model.
5.3 Evaluation
We report on precision at 30 (P30) on binary relevance judgments. We choose P30 because it has been one of the main metrics in both the TREC 2011 and 2012 Microblog track. We also report on MAP, as it is a well understood and commonly used evaluation metric in information retrieval, allowing us to better understand the behavior of our pseudo test collections. Note that in the 2011 task, tweets had to be ordered by their publication date instead of by their relevance. Many top performing systems treated the task as normal relevance ranking and cut off their ranked lists at rank 30 [21]. In the 2012 track organizers decided to focus on ranking by relevance again, which is what we will focus on.
Testing for statistical significance. For each training collection, we run Algorithm 4 N = 10 times, giving rise to N scores for each topic, for each collection. We report average performance and sample standard deviation over these iterations. To also gain insight if any differences between a pair of training collections would be observed on different microblog topics from the same hypothetical population of topics, we proceed as follows. We pick for each collection the iteration of Algorithm 4 which had the smallest training error on that collection. Then, we do a paired t-test over differences per topic as usual and report the obtained p-values. Statistically significant differences are marked as (or ) for significant differences for  = .001, or (and ) for  = .05.
6. RESULTS AND ANALYSIS
First, we report on our parameter tuning results; then on our learning to rank results. We also analyze parameter sensitivity with regard to the percentage of interesting tweets kept and query length.
6.1 Parameter tuning results
The main outcomes in this section will be correlations, to an-
1http://code.google.com/p/sofia-ml/

swer the question whether relative performance of parameter values on pseudo test collections correlates with relative performance of the same values on an editorial collection. We begin with two case studies to gain a better understanding of the behavior of our pseudo text collections. We sweep (a) the document length normalization parameter b for Terrier's tf-idf implementation (Fig. 2(a)), and (b) µ for Indri's implementation of language modeling with Dirichlet smoothing (Fig. 2(b)). We only include one of our ten random pseudo test collections; all random collections behave similarly, for all retrieval systems and metrics.
Fig. 2(a) shows that on the TREC MB 2011 collection there is a general trend to prefer lower values of b, possibly because of the very small average document length, which, in turn, renders the deviation from the average length close to one. All pseudo test collections capture this trend, including the Random-1 pseudo test collection. The curves of the pseudo test collections are smoother than the curve obtained when tuning on the TREC MB 2011 topics; this is because the pseudo test collections have far more test topics. Pseudo test collections show differences in absolute scores, but most importantly, we are interested in whether pseudo test collection predictions that one parameter vector is better than another correlate to such predictions of editorial collections. Kendall's tau expresses exactly that correlation, see Table 3. In addition, we want to know the following: if we sample a random parameter vector from those predicted to yield optimal performance on a pseudo test collection, what will be the expected loss with regard to optimal performance on an editorial collection? Table 5 provides these quantities. Correlations are high across the board, and expected loss is low. All pseudo test collections can be used to reliably tune the b parameter of Terrier's TF-IDF, even the Random-1 collection.
Turning to a second case study, Indri's language modeling algorithm with Dirichlet smoothing, we see a different picture (Fig. 2(b)). The TREC MB 2011 topics show a slightly decreasing trend for larger µ values. We believe this is due to the short document length; tweets after processing are few terms long, and therefore even small µ values overshadow the document term probability with the background probability. This trend is not entirely captured by the pseudo test collections. All have a short increase for low values of µ which is much less pronounced in the TREC MB 2011 curve. After that, all except the Random-1 collection show a decline, if only in the third digit. Correlations are fair (Table 4), but the Random-1 collection fails here. Expected loss is low across the board (Table 6).
Looking at the big picture, we average the correlations and expected loss figures in Tables 7 and 8 over all nine retrieval models from Table 1. For the hashtag based collections, correlations are high, with a large variance over systems. Expected loss is low. This indicates that pseudo test collections can be used to reliably and profitably tune parameters for a variety of well established retrieval algorithms, with more or less success depending on which model is being tuned. Tuning retrieval models on all hashtag based pseudo test collections is about as reliable as tuning on editorial test collections. For most retrieval algorithms, a Random collection cannot be recommended for tuning. Thus, the idea of grouping tweets by hashtag has value for creating pseudo test collections for tuning retrieval algorithms.
6.2 Learning to rank results
In this section we evaluate the usefulness of our pseudo test collections (PTCs) by training several learning to rank algorithms on them. In Tables 9 and 10, we report P30 and MAP performance on the TREC 2011 Microblog track topics. We compare training on our PTCs with training on the TREC 2012 Microblog track top-

58

P30 0.0 0.2 0.4 0.6 0.8 1.0
P30 0.0 0.2 0.4 0.6 0.8 1.0

Tf-idf (Terrier)

TREC-MB-2011 Hashtags Hashtags-T Hashtags-TI Random-1

LM

TREC-MB-2011 Hashtags Hashtags-T Hashtags-TI Random-1

0.0 0.2 0.4 0.6 0.8 1.0

0 2000 4000 6000 8000 10000

b

mu

Figure 2: Sweeping the b parameter of Tf-idf (Terrier) (2(a)) and the µ parameter of LM (2(b)). The x-axes have parameter values, the y-axes average P30 over the topics of the respective tuning collection.

Table 3: For Tf-idf (Indri), and P30, Kendall's tau correlations of parameter sweeps on several pseudo test collections with sweeps on TREC MB 2011 and 2012 collections. Kendall's tau between sweeps over TREC MB 2011 and 2012 is 0.85.

Tune on TREC MB 2011 TREC MB 2012

Random-1

0.18

0.20

Hashtags

0.87

0.86

Hashtags-T

0.90

0.86

Hashtags-TI

0.90

0.86

Table 4: For language modeling (LM), and P30, Kendall's tau correlations of parameter sweeps on several pseudo test collections with sweeps on TREC MB 2011 and 2012 collections. Kendall's tau between sweeps over TREC MB 2011 and 2012 is 0.61.

Tune on TREC MB 2011 TREC MB 2012

Random-1 Hashtags Hashtags-T Hashtags-TI

-0.87 0.65 0.69 0.58

-0.65 0.78 0.77 0.79

ics and indicate significant differences. In Tables 11 and 12, we report P30 and MAP performance on the TREC 2012 Microblog track topics, and compare training on our PTCs with training on the TREC 2011 Microblog track topics.
A first brief glance at all four tables tells us that training on an editorial test collection is in most cases (but not all) the best strategy. Still, if training on a PTC is not substantially and significantly worse, we may conclude that in the abscence of training data, pseudo test collections are a viable alternative.
A second glance at all four tables shows us that training on the Random PTC is always significantly outperformed by training on editorial collections. Still, in most cases, there is a hashtag based PTC on which training yields performance on par with training on editorial collections. This shows that there is added value in our idea of using hashtags to group tweets by topic.
Another phenomenon we can observe in all tables is that Pegasos has remarkably stable performance over pseudo test collections, compared to the other two learning to rank algorithms. With the exception of Hashtags-TI-QL1 and Random, it seems to be able to exploit the structure in any PTC to learn a function that yields performance comparable to a function learned on editorial training data. RankSVM, on the other hand, has unstable performance. Especially in Table 12 it refuses to work on anything but manually obtained ground truth. Coordinate Ascent, a remarkably simple learning to rank algorithm holds the middle ground. When queries are

Table 5: For Tf-idf (Indri), expected loss in P30 performance of parameter sweeps on several training collections compared to optimal performance on TREC MB 2011 and 2012 collections.

Tune on

TREC MB 2011 TREC MB 2012

TREC MB 2011 TREC MB 2012 Random-1 Hashtags Hashtags-T Hashtags-TI

0.006±0.005 (3) 0.029 0.012±0.002 (2) 0.002 0.002

0.003 0.024 0.002±0.002 (2) 0.000 0.000

Table 6: For language modeling (LM), expected loss in P30 performance of parameter sweeps on several training collections compared to optimal performance on TREC MB 2011 and 2012 collections.

Tune on

TREC MB 2011 TREC MB 2012

TREC MB 2011 TREC MB 2012 Random-1 Hashtags Hashtags-T Hashtags-TI

0.006 0.039±0.001 (11) 0.010 0.013±0.004 (2) 0.014

0.010 0.014±0.001 (11) 0.001 0.001±0.001 (2) 0.002

too short, as in Hashtags-TI-QL1, Hashtags-QL2 and Hashtags-TIQL3 performance detoriates. When subsamples of tweets become too small (Hashtags-TI-X20) the same happens.
So which PTC is the best? All PTCs are significantly outperformed by training on an editorial collection in at least one of the conditions. Hashtags-TI-X60 and Hashtags-TI-X80 both yield best results in one case. Also, like Hashtags, Hashtags-T and HashtagsTI are significantly outperformed in only a small number of cases.
Learning to rank only makes sense if improvements over individual retrieval algorithms can be obtained. Tables 13 and 14 show performance of the retrieval models we use as features. In the great majority of cases our hashtag based PTCs outperform the best feature. The Random collection never achieves this.
7. RELATED WORK
We describe two types of related work: (i) searching microblog posts and (ii) pseudo test collections.
Searching microblog posts. Microblog search is a growing research area. The dominant microblogging platform that most research focuses on is Twitter. Microblogs have characteristics that introduce new problems and challenges for retrieval [12, 40]. Massoudi et al. [26] report on an early study of retrieval in microblogs, and introduce a retrieval and query expansion method to account for

59

Table 7: For all systems, and P30, Kendall's tau correlations of parameter sweeps on several pseudo test collections with sweeps on TREC MB 2011 and 2012 collections. Kendall's tau between sweeps over TREC MB 2011 and 2012 is 0.80.

Tune on TREC MB 2011 TREC MB 2012

Random-1 Hashtags Hashtags-T Hashtags-TI

0.27±0.62 (2 NA) 0.78±0.20 (1 NA) 0.80±0.20 (1 NA) 0.75±0.26 (1 NA)

0.32±0.56 (1 NA) 0.70±0.35 (1 NA) 0.78±0.30 (1 NA) 0.81±0.23 (1 NA)

Table 8: Over all retrieval models, average expected loss in P30 performance of parameter sweeps on several training collections compared to optimal performance on TREC MB 2011 and 2012 collections.

Tune on

TREC MB 2011 TREC MB 2012

TREC MB 2011 TREC MB 2012 Random-1 Hashtags Hashtags-T Hashtags-TI

0.003±0.003 0.021±0.021 0.005±0.006 0.004±0.005 0.004±0.005

0.003±0.006 0.013±0.012 0.004±0.006 0.002±0.005 0.002±0.005

microblog search challenges. Efron and Golovchinsky [13] investigate the temporal aspects of documents on query expansion using pseudo relevance feedback. Naveed et al. [30] develop a retrieval model that takes into account document length and interestingness defined over a range of features.
In 2011, TREC launched the microblog search track, where systems are asked to return relevant and interesting tweets given a query [21]. The temporal aspect of Twitter and its characteristics, e.g., hashtags and existence of hyperlinks, were exploited by many participants, for query expansion, filtering, or learning to rank [21]. Whereas teams depended on self-constructed training data to train learning to rank systems during TREC 2011, most of these systems were successfully trained on the 2011 queries during TREC 2012. Teevan et al. [40] find that over 20% of Twitter queries contain a hashtag, an indication that hashtags may be good topical surrogates, which can be leveraged for building pseudo test collections.
Pseudo test collections. The issue of creating and using pseudo test collections is a longstanding and recurring theme in IR, see, e.g., [38, 39]. Several attempts have been made to either simulate human queries or generate relevance judgments without the need of human assessors for a range of tasks. Azzopardi et al. [3] simulate queries for known-item search and investigate term weighting methods for query generation. Their main concern is not to develop training material, but to determine whether their pseudo test collection generation methods ultimately give rise to similar rankings of retrieval systems as manually created test collections. Kim and Croft [18] generate a pseudo test collection for desktop search. Huurnink et al. [15] use click-through data to simulate relevance assessments. Later, they evaluate the performance of query simulation methods in terms of system rankings [16] and they find that incorporating document structure in the query generation process results in more realistic query simulators. Hofmann et al. [14] try to smooth noise from click-through data from an audio-visual archive's transaction log using purchase information of videos. We extend previous work on pseudo test collection generation [4] with a principled method to obtain good quality training material, using knowledge derived from editorial judgements.
Asadi et al. [2] describe a method for generating pseudo test collections for training learning to rank methods for web retrieval. Their methods build on the idea that anchor text in web documents is a good source for sampling queries, and the documents that these anchors link to are regarded as relevant documents for the anchor text (query). Our work shares analogies, but in the microblog set-

Table 9: P30 performance on TREC MB 2011 topics for various LTR algorithms, trained on different collections. Features are tuned on MAP. Best run in bold. Indicated statistically significant differences are with regard to training on TREC-MB-2012. The best individual tuned ranker, DFR-FD, achieved 0.416.

Train on

Pegasos RankSVM

CA

TREC-MB-2012 Random Hashtags Hashtags-T Hashtags-TI

0.446±0.001 0.401±0.004 0.437±0.002 0.438±0.001 0.437±0.001

0.436±0.004 0.356±0.011 0.430±0.004 0.426±0.002 0.406±0.007

0.447±0.004 0.378±0.006 0.434±0.002 0.434±0.003 0.429±0.002

Hashtags-TI-X20 Hashtags-TI-X40 Hashtags-TI-X60 Hashtags-TI-X80

0.432±0.001 0.435±0.001 0.438±0.001 0.438±0.001

0.265±0.016 0.361±0.006 0.415±0.004 0.427±0.003

0.383±0.016 0.427±0.001 0.431±0.003 0.435±0.002

Hashtags-TI-QL1 0.189±0.135 Hashtags-TI-QL2 0.435±0.002 Hashtags-TI-QL3 0.443±0.001 Hashtags-TI-QL5 0.443±0.001 Hashtags-TI-QL20 0.438±0.001

0.034±0.015 0.244±0.024 0.240±0.012 0.259±0.008 0.320±0.007

0.293±0.107 0.421±0.047 0.427±0.023 0.428±0.005 0.432±0.004

Table 10: MAP performance on TREC MB 2011 topics for various LTR algorithms, trained on different collections. Features are tuned on MAP. Best performance per LTR algorithm in bold. Indicated statistically significant differences are with regard to training on TREC-MB-2012. The best individual tuned ranker, Tf-idf (Indri), achieved 0.357.

Train on

Pegasos RankSVM

CA

TREC-MB-2012 Random Hashtags Hashtags-T Hashtags-TI

0.388±0.001 0.348±0.004 0.374±0.001 0.379±0.000 0.383±0.001

0.362±0.003 0.294±0.009 0.360±0.005 0.362±0.005 0.344±0.003

0.387±0.003 0.319±0.009 0.385±0.001 0.386±0.000 0.377±0.002

Hashtags-TI-X20 Hashtags-TI-X40 Hashtags-TI-X60 Hashtags-TI-X80

0.376±0.001 0.380±0.001 0.383±0.001 0.381±0.000

0.198±0.015 0.300±0.007 0.352±0.005 0.363±0.002

0.332±0.013 0.373±0.001 0.381±0.003 0.384±0.002

Hashtags-TI-QL1 0.141±0.125 Hashtags-TI-QL2 0.370±0.001 Hashtags-TI-QL3 0.379±0.000 Hashtags-TI-QL5 0.383±0.001 Hashtags-TI-QL20 0.383±0.001

0.020±0.009 0.185±0.019 0.166±0.011 0.165±0.007 0.231±0.008

0.202±0.119 0.360±0.046 0.366±0.026 0.372±0.006 0.381±0.002

ting, there is no anchor text to sample queries. Moreover, the temporal aspect of relevance plays a bigger role in microblog search.
Carterette et al. [7] investigate what the minimal judging effort is that must be done to have confidence in the outcome of an evaluation. Rajput et al. [32] present a method for extending the recall base in a manually created test collection. Carterette et al. [8] find that test collections with thousands of queries with fewer relevant documents considerably reduce the assessor effort with no appreciable increase in evaluation errors. This finding inspired us to come up with pseudo test collection generators that are able to produce large numbers of queries: while the signal produced by an individual query may be noisy, the volume will produce a signal that is useful for learning and parameter tuning.

8. DISCUSSION AND CONCLUSION
Following the results of our experiments we list three main observations: (1) The Random pseudo test collection performs significantly worse than editorial collections in the retrieval experiments and shows low correlation to these collections in the tuning phase. (2) The top pseudo test collections are not significantly worse than editorial collections and show high correlation when tuning parameters. (3) Differences between various pseudo test collections on retrieval effectiveness are small.

60

Table 11: P30 performance on TREC MB 2012 topics for various LTR algorithms, trained on different collections. Features are tuned on MAP. Best performance per LTR algorithm in bold. Indicated statistically significant differences are with regard to training on TREC-MB-2011. The best individual tuned ranker, DFR-FD, achieved 0.351.

Train on

Pegasos RankSVM

CA

TREC-MB-2011 Random Hashtags Hashtags-T Hashtags-TI

0.392±0.001 0.341±0.003 0.379±0.001 0.372±0.001 0.381±0.001

0.391±0.007 0.289±0.008 0.330±0.011 0.336±0.005 0.326±0.007

0.392±0.007 0.314±0.006 0.378±0.001 0.382±0.001 0.393±0.002

Hashtags-TI-X20 Hashtags-TI-X40 Hashtags-TI-X60 Hashtags-TI-X80

0.377±0.001 0.379±0.001 0.379±0.001 0.373±0.001

0.248±0.013 0.294±0.009 0.348±0.003 0.337±0.006

0.353±0.008 0.389±0.001 0.394±0.006 0.388±0.007

Hashtags-TI-QL1 0.198±0.089 Hashtags-TI-QL2 0.374±0.002 Hashtags-TI-QL3 0.381±0.001 Hashtags-TI-QL5 0.385±0.001 Hashtags-TI-QL20 0.380±0.001

0.047±0.016 0.231±0.016 0.218±0.007 0.248±0.005 0.302±0.005

0.291±0.059 0.377±0.035 0.382±0.016 0.391±0.004 0.388±0.002

Table 12: MAP performance on TREC MB 2012 topics for various LTR algorithms, trained on different collections. Features are tuned on MAP. Best performance per LTR algorithm in bold. Indicated statistically significant differences are with regard to training on TREC-MB-2011. The best individual tuned ranker, DFR-FD, achieved 0.220.

Train on

Pegasos RankSVM

CA

TREC-MB-2011 Random Hashtags Hashtags-T Hashtags-TI

0.245±0.000 0.207±0.001 0.231±0.000 0.227±0.001 0.234±0.001

0.245±0.004 0.159±0.005 0.181±0.008 0.193±0.004 0.172±0.005

0.246±0.004 0.176±0.006 0.224±0.001 0.227±0.001 0.233±0.001

Hashtags-TI-X20 Hashtags-TI-X40 Hashtags-TI-X60 Hashtags-TI-X80

0.233±0.001 0.233±0.000 0.233±0.000 0.230±0.000

0.143±0.007 0.158±0.005 0.196±0.002 0.189±0.004

0.210±0.005 0.230±0.001 0.234±0.003 0.230±0.004

Hashtags-TI-QL1 0.106±0.060 Hashtags-TI-QL2 0.229±0.001 Hashtags-TI-QL3 0.234±0.000 Hashtags-TI-QL5 0.235±0.000 Hashtags-TI-QL20 0.231±0.000

0.026±0.008 0.123±0.011 0.106±0.004 0.118±0.004 0.162±0.003

0.165±0.048 0.233±0.021 0.231±0.015 0.233±0.003 0.228±0.001

Combining the top two observations leads us to conclude that our approach to constructing pseudo test collections works. We can successfully use pseudo test collections, as long as we find appropriate surrogate relevance labels. Why are these findings important? To train learning to rank methods on microblog retrieval tasks we do not have to invest in manual annotations but can use hashtags for creating training examples. Pseudo test collections can also be used successfully for tuning parameters of retrieval models.
The third observation is that the differences between our pseudo test collections are limited. More advanced methods for selecting tweets and hashtags result in performance that is only sporadically better than the naive baseline method, which treats all tweets and hashtags equally. We can look at this from two angles. (i) Collection construction: We can limit time spent on constructing a smooth and interesting pseudo test collection by substituting more advanced methods (Hashtags-T and -TI) with the naive Hashtags method. Using the naive method is faster and results in a larger collection, with similar results. (ii) Training volume: Investing in obtaining more interesting tweets and hashtags using our more advanced methods substantially reduces the number of queries in our collections. While the naive Hashtags uses over 1,800 queries and 460,000 relevant tweets, the other methods use only 890 (HashtagsT) and 480 (Hashtags-TI) queries and equally reduced sets of relevant tweets. Training efficiency improves substantially by limiting

Table 13: P30 performance on 2011 and 2012 topics for retrieval models for which MAP was tuned on 2011 topics, ordered by 2012 performance. The only parameter free retrieval model, DFRee, achieved 0.416 on the 2011 topics, and 0.346 on the 2012 topics.

Model

2011 2012

DFR-FD (Terrier) Tf-idf (Terrier) PL2 (Terrier) PRF (Terrier) Tf-idf (Indri) Okapi (Indri) LM (Indri) BUW (Indri) BOW (Indri)

0.416 0.397 0.406 0.391 0.413 0.409 0.404 0.128 0.094

0.351 0.348 0.336 0.335 0.331 0.329 0.325 0.154 0.134

Table 14: MAP performance on 2011 and 2012 topics for retrieval models for which MAP was tuned on 2011 topics, ordered by 2012 performance. The only parameter free retrieval model, DFRee, achieved 0.351 on the 2011 topics, and 0.216 on the 2012 topics.

Model

2011 2012

DFR-FD (Terrier) Tf-idf (Terrier) PL2 (Terrier) PRF (Terrier) Tf-idf (Indri) Okapi (Indri) LM (Indri) BUW (Indri) BOW (Indri)

0.352 0.352 0.348 0.335 0.357 0.354 0.346 0.075 0.060

0.220 0.215 0.209 0.201 0.194 0.191 0.188 0.069 0.061

the number of queries in the collections. In other words, we can choose between spending more time on constructing our collections, while reducing training time, or take a naive collection construction approach that results in larger collections and thus longer training times. A similar observation holds for the editorial collections, which are the smallest collections (50­60 queries), but (supposedly) with the highest quality.
Summarizing, we have studied the use of pseudo test collections for training and tuning LTR systems for microblog retrieval. We use hashtags as surrogate relevance labels and generate queries from tweets that contain the particular hashtag. These pseudo test collections are then used for (1) tuning parameters of various retrieval models, and (2) training learning to rank methods for microblog retrieval. We explore three ways of constructing pseudo test collections, (i) a naive method that treats all tweets and hashtags equally, (ii) a method that takes timestamps into account, and (iii) a method that uses timestamps and selects only interesting microblog posts. We compare their performance to those of a randomly generated pseudo test collection and two editorial collections.
Our pseudo test collections have high correlation with the editorial collections in the parameter tuning phase, whereas the random collection has a significantly lower correlation. In the LTR phase we find that in most cases our collections do not perform significantly worse than the editorial collections, while the random collection does perform significantly worse.
Looking forward, we are interested in training on a mixture of editorial and generated ground truth. Our work is related to creating ground truth in a semi-supervised way and we also aim to further explore this relation.
Acknowledgements
This research was partially supported by the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreements nr 258191 (PROMISE Network of Excellence) and 288024 (LiMoSINe project), the Netherlands Organisation for Scientific Research (NWO) under project nrs 640.004.802, 727.011.005, 612.001.116, HOR-11-10, the Center for Creation, Content and

61

Technology (CCCT), the BILAND project funded by the CLARINnl program, the Dutch national program COMMIT, the ESF Research Network Program ELIAS, the Elite Network Shifts project funded by the Royal Dutch Academy of Sciences (KNAW), and the Netherlands eScience Center under project number 027.012.105.
9. REFERENCES
[1] G. Amati and C. van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.
[2] N. Asadi, D. Metzler, T. Elsayed, and J. Lin. Pseudo test collections for learning web search ranking functions. In SIGIR '11, pages 1073­1082, 2011.
[3] L. Azzopardi, M. de Rijke, and K. Balog. Building simulated queries for known-item topics: an analysis using six european languages. In SIGIR '07, pages 455­462, 2007.
[4] R. Berendsen, M. Tsagias, M. de Rijke, and E. Meij. Generating pseudo test collections for learning to rank scientific articles. In CLEF '12, 2012.
[5] M. Bron, E. Meij, M. Peetz, M. Tsagkias, and M. de Rijke. Team COMMIT at TREC 2011. In TREC 2011, 2011.
[6] S. Carter, W. Weerkamp, and E. Tsagkias. Microblog language identification. Language Resources and Evaluation Journal, 47(1), 2013.
[7] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In SIGIR '06, pages 268­275, 2006.
[8] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation over thousands of queries. In SIGIR '08, pages 651­658, 2008.
[9] S. Clinchant and E. Gaussier. Bridging language modeling and divergence from randomness models: A log-logistic model for ir. In ECIR '09, pages 54­65, 2009.
[10] B. Croft, D. Metzler, and T. Strohman. Search Engines: Information Retrieval in Practice. Addison Wesley, 2009.
[11] S. Cronen-Townsend and W. B. Croft. Quantifying query ambiguity. In HLT '02, pages 104­109, 2002.
[12] M. Efron. Information search and retrieval in microblogs. J. Am. Soc. Inf. Sci. Technol., 62:996­1008, 2011.
[13] M. Efron and G. Golovchinsky. Estimation methods for ranking recent information. In SIGIR '11, pages 495­504, 2011.
[14] K. Hofmann, B. Huurnink, M. Bron, and M. de Rijke. Comparing click-through data to purchase decisions for retrieval evaluation. In SIGIR '10, pages 761­762, 2010.
[15] B. Huurnink, K. Hofmann, and M. de Rijke. Simulating searches from transaction logs. In SIGIR 2010 Workshop on the Simulation of Interaction, 2010.
[16] B. Huurnink, K. Hofmann, M. de Rijke, and M. Bron. Validating query simulators: an experiment using commercial searches and purchases. In CLEF '10, pages 40­51, 2010.
[17] T. Joachims. Training linear SVMs in linear time. In KDD '06, pages 217­226, 2006.
[18] J. Kim and W. B. Croft. Retrieval experiments using pseudo-desktop collections. In CIKM '09, pages 1297­1306, 2009.
[19] H. Kwak, C. Lee, H. Park, and S. Moon. What is Twitter, a social network or a news media? In WWW '10, pages 591­600, 2010.

[20] G. G. Lee et al. SiteQ: Engineering high performance QA system using lexico-semantic pattern matching and shallow NLP. In TREC 2001, pages 442­451, 2001.
[21] J. Lin, C. Macdonald, I. Ounis, and I. Soboroff. Overview of the TREC 2011 Microblog track. In TREC 2011, 2012.
[22] T.-Y. Liu. Learning to rank for information retrieval. Found. and Trends in Inf. Retr., 3:225­331, 2009.
[23] C. Lundquist, D. Grossman, and O. Frieder. Improving relevance feedback in the vector space model. In CIKM '97, pages 16­23, 1997.
[24] C. Macdonald, R. Santos, and I. Ounis. The whens and hows of learning to rank for web search. Inf. Retr., pages 1­45, 2012.
[25] C. D. Manning and H. Schütze. Foundations of statistical natural language processing. MIT Press, 1999.
[26] K. Massoudi, E. Tsagkias, M. de Rijke, and W. Weerkamp. Incorporating query expansion and quality indicators in searching microblog posts. In ECIR '11, pages 362­367, 2011.
[27] D. Metzler. A Feature-Centric View of Information Retrieval. Springer, 2011.
[28] G. Mishne and M. de Rijke. A study of blog search. In ECIR '06, pages 289­301, 2006.
[29] A. Mohan, Z. Chen, and K. Q. Weinberger. Web-search ranking with initialized gradient boosted regression trees. J. Mach. Learn. Res., Workshop and Conf. Proc., 14:77­89, 2011.
[30] N. Naveed, T. Gottron, J. Kunegis, and A. C. Alhadi. Searching microblogs: coping with sparsity and document quality. In CIKM '11, pages 183­188, 2011.
[31] J. Peng, C. Macdonald, B. He, V. Plachouras, and I. Ounis. Incorporating term dependency in the DFR framework. In SIGIR '07, pages 843­844, 2007.
[32] S. Rajput, V. Pavlu, P. B. Golbus, and J. A. Aslam. A nugget-based test collection construction paradigm. In CIKM '11, pages 1945­1948, 2011.
[33] S. Robertson and K. Jones. Simple, proven approaches to text retrieval. Technical report, U. Cambridge, 1997.
[34] S. Robertson and H. Zaragoza. On rank-based effectiveness measures and optimization. Inf. Retr., 10(3):321­339, 2007.
[35] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at trec-3. In TREC '94, pages 109­109, 1995.
[36] D. Sculley. Large scale learning to rank. In NIPS Workshop on Advances in Ranking, 2009.
[37] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In ICML '12, pages 807­814, 2012.
[38] J. Tague and M. Nelson. Simulation of user judgments in bibliographic retrieval systems. In SIGIR '81, pages 66­71, 1981.
[39] J. Tague, M. Nelson, and H. Wu. Problems in the simulation of bibliographic retrieval systems. In SIGIR '80, pages 236­255, 1980.
[40] J. Teevan, D. Ramage, and M. R. Morris. #twittersearch: a comparison of microblog search and web search. In WSDM '11, pages 35­44, 2011.
[41] W. Weerkamp and M. de Rijke. Credibility-based reranking for blog post retrieval. Inf. Retr., 15(3­4):243­277, 2012.
[42] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In SIGIR '01, pages 334­342, 2001.

62

An Incremental Approach to Efficient Pseudo-Relevance Feedback

Hao Wu
Department of Electrical and Computer Engineering
University of Delaware Newark, DE USA
haow@udel.edu
ABSTRACT
Pseudo-relevance feedback is an important strategy to improve search accuracy. It is often implemented as a tworound retrieval process: the first round is to retrieve an initial set of documents relevant to an original query, and the second round is to retrieve final retrieval results using the original query expanded with terms selected from the previously retrieved documents. This two-round retrieval process is clearly time consuming, which could arguably be one of main reasons that hinder the wide adaptation of the pseudorelevance feedback methods in real-world IR systems.
In this paper, we study how to improve the efficiency of pseudo-relevance feedback methods. The basic idea is to reduce the time needed for the second round of retrieval by leveraging the query processing results of the first round. Specifically, instead of processing the expand query as a newly submitted query, we propose an incremental approach, which resumes the query processing results (i.e. document accumulators) for the first round of retrieval and process the second round of retrieval mainly as a step of adjusting the scores in the accumulators. Experimental results on TREC Terabyte collections show that the proposed incremental approach can improve the efficiency of pseudo-relevance feedback methods by a factor of two without sacrificing their effectiveness.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Search process, Relevance feedback
General Terms: Performance; Experimentation
Keywords: Pseudo-relevance Feedback; Incremental Approach; Efficiency; Query Expansion
1. INTRODUCTION
Pseudo-relevance feedback is an important technique that can be used to improve the effectiveness of IR systems. Dif-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Hui Fang
Department of Electrical and Computer Engineering
University of Delaware Newark, DE USA
hfang@udel.edu
ferent pseudo-relevance feedback methods have been proposed and studied for various retrieval models [8, 19, 26­28, 37, 41], but they all boil down to the problem of expanding an original query with useful term selected from a certain number of pseudo-relevant documents, i.e., top-ranked documents from an initial retrieval run. In particular, the implementation of pseudo-relevance feedback methods always require two rounds of retrieval, with the first round retrieving an initial set of documents with respect to an original query and the second round retrieving documents based on an expanded query, which is generated by expanding the original query with relevant terms selected from the previously retrieved document set.
Although pseudo-relevance feedback methods can lead to large performance gain in terms of effectiveness, there is one major downside that limits their usability in real retrieval application: its low efficiency [6, 13, 18]. In particular, the second round of retrieval could significantly slow down the performance due to the time spent on selecting terms for expansion and on executing the expanded query that are often much longer than the original one [13, 18]. It is clear that such drastically increased execution cost limits the applicability of pseudo-relevance feedback methods in real IR applications.
Compared with continual efforts on improving the effectiveness and robustness of pseudo feedback methods in the past decades [8, 10, 11, 14, 15, 19­21, 21, 22, 24, 26­28, 32, 37, 38, 41], less attention has been paid to make these methods more efficient [6,13,18]. Billerbeck and Zobel [6] proposed a summary based method for efficient term selection from the feedback methods. Lavrenko and Allan [18] proposed a fast relevance model, and Cartright et. al. [13] studied approximation methods to reduce the storage and computational cost for the fast relevance model. These proposed methods were designed for either a specific feedback method [13, 18] or a bottleneck in the feedback implementation, i.e., how to efficiently select expansion terms from the feedback documents [6]. However, it remains unclear whether there is a general solution to address other bottlenecks for efficient pseudo-relevance feedback implementation.
In this paper, we propose a general solution that can improve the efficiency of pseudo-relevance feedback methods. The basic idea is to reduce the execution cost for the expanded query by using the the query processing status of the original query. Existing IR systems often implement the pseudo-feedback methods as a two-round retrieval, where the second round is processed independently to the

553

first round. However, since the query used in the second round retrieval is an expanded version of the original query used in the first round, it would be beneficial to leverage the results of the first round of retrieval to reduce the query processing time for the second round. Although this is an intuitive idea, we are not aware of any previous work that successfully implemented the idea. Experimental results on TREC Terabyte collections show that the proposed strategy can improve the efficiency by a factor of 2 to 4 without sacrificing the effectiveness.
The remainder of this paper is organized as follows. We first discuss related work on pseudo-relevance feedback in Section 2, and then provide background knowledge about indexing and query processing in Section 3. We then describe the motivation and details of the proposed incremental approach in Section 4. We explain the experiment set up, present experiment results and summarize our findings in Section 5. Finally, we conclude and discuss the future work in Section 6.
2. RELATED WORK
2.1 Pseudo-Relevance Feedback Methods
Pseudo-relevance feedback is an effective technique for improving retrieval accuracy. The commonly used feedback method for vector space models is the Rocchio algorithm [27,28]. In classical probabilistic models, the feedback method is based on the Robertson/Sparck-Jones weighting [26]. Two representative feedback methods for the language modeling approaches are the relevance model [19] and the model-based feedback [41].
Despite the difference in selecting expansion terms (i.e., estimating relevance models in language modeling framework), all these methods share similar implementation strategies, i.e., initial retrieval to find documents relevant to an original query; term selection to identify useful expansion terms from the feedback documents; and second-round retrieval to return documents relevant to the expanded query. This commonality in implementation makes it possible to derive a general optimization strategy that can improve the efficiency of these pseudo-relevance feedback methods, which is the focus of our paper.
More recently, continuous efforts have been put on improving the effectiveness and robustness of the pseudo-feedback methods [8, 10, 11, 14, 15, 20­22, 24, 32, 38]. For example, Tao and Zhai [32] extended model-based feedback methods using a regularized EM algorithm to reduce the parameter sensitivity, and Lv and Zhai [21] extended the relevance model to exploit term proximity.
2.2 Efficient Pseudo-Relevance Feedback
Compared with the effectiveness of an IR system, its efficiency is equally important. In particular, high query latency can decrease user satisfaction significantly [23]. Unfortunately, only a few previous studies focused on efficient pseudo-relevance feedback methods [5, 6, 13, 18].
Billerbeck and Zobel [5,6] examined the bottlenecks of the pseudo-relevance feedback implementation and proposed to leverage tf-idf document summaries to reduce the cost of selecting expansion terms from the feedback documents. They also explored other approaches such as query association, using reduced-size collections and carrying accumulator information from the first round of retrieval to the second round,

but all of them were unsuccessful. Our approach is in the same line of the third unsuccessful strategy they mentioned in the paper. However, we reach a different conclusion with our implementation, i.e., our method can improve the efficiency without obvious sacrificing the effectiveness. The main reason behind this fact is that our method allows the expanded terms to incrementally evaluate the accumulators in the initial retrieval rather than merely re-rank the top documents of the initial runs.
Lavrenko and Allan [18] proposed a fast relevance model which computes the original relevance-based ranking based on the cross-entropy between two documents (i.e., one representing the relevance model and the other representing the document to be scored). The re-arrangement used in the derivation makes it possible to shift the main computational cost from the query processing time to indexing time. Their experiment results show that the new method can reduce the query execution time significantly. However, the downside is the time taken to compute the document similarity matrix could be rather long (e.g., 90 hours for a small collection), which could makes it impractical for larger collections. Cartright et. al. [13] took one step further and propose approximation methods to reduce the storage and computational cost of this offline computing. Since these methods are specifically designed for relevance models, it remains unclear whether similar strategy can be leveraged for other feedback methods.
Cartright and Allan [12] have focused on optimizing the efficiency for interpolating subqueries. In particular, they investigated four cases of such an interpolation with varying amounts of accessibility for the original queries and expansion terms, and then proposed methods for each case. Our problem formulation is similar to the case when having access to both original queries and expansion terms, and our proposed approach bears similarity to the proposed Rewind algorithm. However, we use a different query processing technique, i.e,. score at a time (SAAT), from the one used in the previous study, i.e., document at a time (DAAT). Since SAAT makes it easier to resume the ranking status and adjust the document scores based on new terms, the improvement of our results is more significant than observed in the previous study.
Pseudo-relevance feedback is a specific type of query expansion methods where expansion terms are selected from highly-ranking documents. We now describe a couple of efforts on efficient query expansion [33, 35]. Theobald et. al. [33] proposed an optimization strategy for query expansion methods that are based on term similarities such as those computed based on WordNet. Their pruning method was designed based on a similarity based scoring function, and it remains unclear how to generalize it to other query expansion methods. Wang et. al. [35] proposed a solution for efficient query expansion for advertisement search. They focused on solving a domain specific challenge, i.e., the placement of similar bid phrases. The proposed strategies in these studies are very problem-specific, and can not be directly applied to pseudo-relevance feedback methods.
3. BACKGROUND ON INDEXING AND QUERY PROCESSING
Most current IR systems are based on inverted indexes [42]. An inverted index contains one inverted list for each term in the document collection. An inverted list of a term

554

contains a list of postings, and each posting describes the information of a document containing the term such as document ID and the term occurrence in the document. Query processing involves traversing the inverted lists of query terms, computing the relevance score for each document based on a retrieval function and then ranking the documents based on their scores.
Significant efforts have been made on making this process more efficient over the past decade [1­4,7,9,17,25,29,30,42]. Due to the limited space, we mainly provide some background about a few influential studies including those used in our system [1­4, 25, 29, 30].

3.1 Index Organization
Traditional inverted indexes are document sorted indexing, where the postings of an inverted list are sorted based on the document IDs [36]. The inverted lists of common terms could consist a huge number of postings. To enable faster access to the lists, various compression techniques such as delta encoding for document IDs have been used [42]. As the document IDs are usually sorted based on an increasing order, the delta codes are usually much smaller than the original IDs. It is therefore possible to reduce the space usage by storing delta codes instead of the original document IDs. Indexing compression not only saves the usage of disk and memory, but also improves the efficiency by minimizing cache/memory misses.
Another commonly used indexing organization strategy is impact-sorted indexing, where the postings are sorted based on their impact, i.e., their contribution to the relevance score of a document [1­4, 29]. With the impact-sorted indexing, the retrieval score of a document D for query Q can be evaluated by summing up the impact scores of D in the inverted lists of all the terms in Q. To further improve the efficiency, the impact scores are binned into a smaller number of distinct values and only the binned integer values are stored in the index. This strategy brings two benefits. First, the impact score can be stored using a very small number of bits (e.g., 6 bits for 64 distinct binned integers) instead of using 32 or 64 bits for a floating point variable. Second, with the binned scores, the number of documents with the same score is pretty large so that we can separate an inverted list into segments, one for each distinct value of the impact scores. Within each segment, documents are then ranked based on their document IDs, and the compression techniques used for document-sorted indexing can be applied here to achieve high level of compression. Formally, the retrieval score of document D for query Q is computed as follows,

SQ,D =

Bw,D ,

(1)

wQ

where Bw,D is the binned integer score representing the impact of query term w in D. Previous studies have shown that such binning strategy would not significantly affect the retrieval accuracy [3, 29].
Skipping is another mechanism for fast accessing the information from the postings. Skips are forward pointers within an inverted list, and make it possible to pass over non-promising information on the postings with minimal efforts. They are often inserted into the indexes and stored as additional information. With the help of the skips, the system can jump to required records without going through the posting list one record by one record.

3.2 Query Processing
When processing a query, there are three different techniques to traverse the indexes and compute the relevance score for each document.
· Document-At-A-Time (DAAT) [7, 31, 34]: It assigns a document its final score before the next document is considered. In particular, the inverted lists of all query terms are processed in parallel. The iterators of the inverted lists need to frequently compare with each other to make sure they keep the same pace. Once all the postings of a document are accessed and processed, the system can forward the iterators to deal with the next document.
· Term-At-A-Time (TAAT) [9,25,42]: It starts with the inverted list of a query term, and would finish evaluating all the documents in the list before it moves to the next query term. As a result, an accumulator is required for each document to store and accumulate the retrieval scores. Once lists of all the query terms are processed, the documents with the K largest accumulated scores are returned as the retrieval results.
· Score-At-A-Time (SAAT) [4, 29, 30]: This strategy is only suitable for impact-based indexing. It fetches all inverted lists first (as in DAAT) and processes the postings in the decreasing order of the impact values instead of document IDs as used in TAAT. Document accumulators are still needed to accumulate partial score contributions from the different inverted lists. Since SAAT places the most promising postings close to start of the lists, early termination techniques are often used to improve the efficiency [4, 16, 39].
To reduce the computational efforts, pruning has been introduced to all these three strategies [4, 9, 34] with the goal of limiting the number of accumulators and the scan depth on the index lists. The main idea is that the process can be terminated once the system identifies that further processing will not change the ranking of top k documents.
3.3 What We Use in This Paper
We follow the previous studies [4, 29] and use a state-ofthe-art four-stage SAAT query processing strategy with the impact-based indexing since it can achieve very good performance in many cases. Note that we use query generation model with Dirichlet prior smoothing as the basic retrieval model [40] (i.e., the one without using feedback) and the relevance model [19] as the pseudo-relevance feedback method. However, our proposed incremental approach can be applied to other retrieval functions and feedback methods and we plan to study it as one of our future work.
To compute the impact scores (i.e., Bw,D in Equation (1), we use the derived equation from previous study [29]:
n Bw,D = M + log(s) (logP (w|D) - Cw) ,
where n is the number of distinct values used to represent impact scores and set to 64. Cw is the minimum value of P (w|D) in the collection (i.e., Cw = minD logP (w|D)). M is used to ensure that the binned score is within the range, and s is a saturation parameter which makes sure that all the bin values, i.e., [0, n], can be fully utilized. The details

555

of the derivation can be found in the references [29]. Moreover, the term probability is estimated using Dirichlet Prior smoothing [40], i.e.,

c(w; D) + µc(w; C)/|C|

P (w|D) =

.

|D| + µ

The parameter µ is set to 2,500 and the skip length is set to 128 bytes.
For query processing, we use the SAAT with a four-stage pruning. The four stages are: OR, AND, REFINE and IGNORE. The processing begins with the OR stage by processing the postings in the inverted lists based on the decreasing order of the impact values. Document accumulators are created whenever it is necessary, i.e., when a new document is seen in one of the inverted lists. The processing switches to AND stage when we can prove that we have created accumulators for all the documents that could possibly enter the top n. In AND stage, we ignore all the documents without an accumulator. The processing continues and we update the scores for the existing accumulators with newly processed information. The processing switches to REFINE stage as soon as we know exactly what the top n documents are without knowing their exact ranking. REFINE stage works with the accumulators of the top n documents. Once we can determine that rank order of the top n documents, the process enters the final IGNORE stage, which means that all the remaining information from the inverted list can be ignored.
The basic idea of the pruning strategy is to gradually reduce the number of active accumulators when we gain more confidence on knowing that other accumulators would not change the final top-k search results. In particular, OR stage is the only one that can add accumulators, AND stage only updates existing accumulators, and REFINE stage only process accumulators that can make to the top k results. Moreover, we further speed up the process by using skipping and accumulator trimming [29]. Skipping enables fast processing of long postings, while accumulator trimming can reduce the computational cost in the AND stage by dynamically reduce the number of enabled accumulators.

4. EFFICIENT PSEUDO-RELEVANCE FEEDBACK

4.1 Overview of Existing Implementation Strategy
As described in Section 2.1, many pseudo-relevance feedback methods have been proposed and studied. Despite the differences on how to exploit feedback information, they all require the following three-step implementation:

1. Initial retrieval: finding documents relevant to an original query;

2. Term selection: identifying useful expansion terms (i.e., relevant information in language modeling framework) from the feedback documents;

3. Second-round retrieval: returning documents relevant to the expanded query (i.e., updated query model in the language modeling framework).

We now provide more details on how each of these three steps is implemented in existing IR systems.

Figure 1: Processing time of the three steps in pseudo-relevance feedback implementation

Initial Retrieval, 29.6ms

Term Selection, 43.2 ms

Second Round of Retrieval, 2624 ms

The initial retrieval step can be implemented with any existing top-k query processing techniques as described in Section 3. There have been significant efforts on optimizing the efficiency for this step [1­4, 7, 9, 17, 25, 29, 30, 42].
The term selection step is to select important terms from the feedback documents with the expectation that the selected terms can bring more relevant documents in the second round of retrieval. Traditionally, these terms are selected directly from the top ranked documents. However, this step could takes lots of time when the documents are long and the information about the documents need to be read from the disk. To solve this problem, Billerbeck and Zobel [6] proposed to generate a short tf-idf based summary for each document and select expansion terms from the summaries of the feedback documents. These summaries are small enough to be pre-loaded into the memory, and can lead to more efficient term selection. Their experimental results showed that this strategy is efficient with ignorable loss in terms of the effectiveness. In this paper, we use this strategy for term selection. One difference is that we use term probability p(t|D) instead of tf-idf weighting to generate document summaries because the retrieval function used is based on language modeling approach. And we set the length of document summary to 20 terms.
The second round retrieval step aims to retrieve final retrieval results with the expanded query. The expanded query is often formulated as a linear interpolation of the original query and the expansion terms selected from the second step. As an example, in the relevance model [19], this step is to retrieve documents with an updated query model (i.e., Qnew) by linearly combining the original query model (i.e., Q) with the relevance model estimated from the feedback documents (i.e., F ) as follows:

Qnew =  · Q + (1 - ) · F ,

(2)

where the original query model Q is estimated using the maximum likelihood estimation of query Q, the relevance model F estimated from the feedback documents F using

556

Figure 2: Computational flows for different implementations of pseudo-relevance feedback methods

Conventional

OR

AND REFINE IGNORE

Top Docs

New Query

OR

AND REFINE IGNORE

RESULT

effective but not efficient

Top-K Re-Rank

OR

AND REFINE

Incremental

OR

AND REFINE

Initial Round

Top Docs

Add Terms

Top Docs

Add Terms

RECOVERY

Query Expansion

REFINE IGNORE
AND REFINE IGNORE Second Round

RESULT
RESULT Output

efficient but not effective
efficient and effective

the methods described in previous study [19], and  is to control the amount of feedback.
In the second round of retrieval, existing IR systems such as Indri would process the expanded query in the same way as a newly submitted query. In other words, the two rounds of retrieval are processed independently.
Figure 1 shows how much time each step takes when using the existing implementation methods described above for the relevance feedback method with 20 expansion terms. It is clear that the third step takes the most of computational time while the other two stages share a very small part the time usage. Thus, the key to efficient pseudo-relevance feedback methods is to reduce the execution time for the second-round retrieval, which is the focus of our paper.
4.2 Analyzing the second round retrieval
Compared with the initial retrieval, the expanded query processed in the second round is often much longer than the original query because it has much larger number of query terms to be processed. For example, the average length of Web queries is around 3, while the number of expansion terms is often set to 20. As a result, the computational cost for expanded query is significantly higher than that for the basic retrieval. Take the implementation used in our paper as an example, a longer query means that each accumulator needs to collect more postings to produce a final result and more accumulators would be created and evaluated. As a result, the system performs much more index accesses and computing, which leads to extremely long processing time compared with that for shorter queries.
One limitation of existing implementation for feedback methods is that the two rounds of retrieval are processed independently. Each round starts with an empty set of accumulators and gradually adds new accumulators in the OR stage. When switched to AND stage, accumulators can be updated but no new accumulated can be added. When switched to REFINE stage, only top k accumulators are processes. And in the final IGNORE stage, all the information from the inverted lists can be ignored. Note that the accumulators used the two rounds of retrieval are computed from the scratch separately. This implementation is illustrated in the upper part of Figure 2

However, unlike processing a new query, the expanded query in the second round retrieval is related to the query used in the initial retrieval. In particular, the query terms in the initial retrieval is a subset of those in the second round of retrieval, and these terms will be processed twice in this two rounds of retrieval process. Moreover, the results of these two rounds of retrieval might have a great overlap. Thus, it would be interesting to study how to leverage the results of the first round of retrieval to reduce the computational cost. But how to leverage them? This is not a simple problem without significant challenges.
The idea of exploiting the results of initial retrieval to improve the efficiency of the second round retrieval was discussed in previous study [6], but was not found useful. In the next subsection, we re-visit this basic idea and propose an incremental approach that is shown to be both efficient and effective based on the experimental results.
4.3 The Proposed Incremental Approach
Our basic idea is that the initial retrieval process should be treated as part of the query processing for the second round retrieval. Instead of processing the expanded query in the second round from the scratch, we should be able to resume the query processing results of the initial query, and continue the processing for the expanded query terms. But how to resume the results?
One possible strategy is to resume the last ranking-related stage, i.e., REFINE, in the query processing results of the initial retrieval. Recall that REFINE stage is designed to process only the accumulators of top K ranked documents. Thus, if we resume the status from the REFINE stage in the second round retrieval, it is equivalent to re-ranking those top K ranked documents using the expanded query. This strategy is illustrated in the middle part of Figure 2. Since the number of accumulators used in REFINE stage is very small, this re-ranking method would be quite efficient. However, it would suffer significant loss in terms of the effectiveness because one of the major benefits of feedback methods is to find relevant documents that were not among top ranked results for the initial retrieval and the re-ranking strategy seems to disable this nice benefit. Clearly, this is not an optimal solution. Can we do better?

557

Intuitively, if more document accumulators can be included in re-ranking process, the retrieval effectiveness could be improved. On the extreme case, when all the documents are considered for the re-ranking, the cost would be the same as submitting a new query. Thus, the main challenge here is how to select a set of documents to be re-ranked so that we can increase the efficiency without sacrificing the effectiveness.
Recall that the pruning technique consists of four stages: OR, AND, REFINE and IGNORE. The number of active accumulators becomes smaller as the system switches from one stage to the other. As discussed earlier, resuming from the REFINE stage is equivalent to re-ranking only top k documents, which hurts the effectiveness. On the contrary, resuming from the OR stage would not hurt the effectiveness. However, since the number of expanded terms is much larger than the number of original query terms, we might not be able to reduce the number of accumulators significantly. Thus, we propose to resume from the AND stage.
The main idea of our incremental approach is shown in the lower part in Figure 2. REFINE is the last ranking-related stage in the initial retrieval. Thus, in order to resume from the AND stage of the initial retrieval, we have to first rewind the query processing results from the end of REFINE stage back to the end of the AND stage. This new stage is referred to as RECOVERY stage in our system. The RECOVERY stage has two tasks: (1) re-enable the accumulators that were disabled at the REFINE stage; and (2) turn back the inverted list pointers to the positions where they were at the end of AND mode. Our experimental results show that this stage takes a very short ignorable time. After the RECOVERY stage, we will switch to the new AND stage to update the accumulators based on the expanded terms, and then continue to the other two stages as usual.
4.4 Discussions
Our proposed incremental approach can improve the efficiency because of the following two reasons.
First, query processing results of the original query terms can provide useful information for effective pruning in the second round retrieval. Specifically, accumulator trimming is used in the AND stage to dynamically reduce the number of active accumulators, and a threshold is used to decide whether an accumulator should be kept active or not. The threshold is to estimate a lower bound of the relevance scores for the top K ranked documents. This threshold is set to -inf at the beginning of the retrieval process, and will be updated at the later stage of the retrieval when more information is gathered. Since initial value of the threshold is rather small, little pruning is applied at the early stage of the retrieval process. If the system could be informed with the range of the threshold, more pruning can be done, which would lead to shorter processing time. Since resuming the process of the initial retrieval can provide a much larger initial value for the pruning threshold, the proposed approach can reduce the query processing time.
Second, the efficiency is closely related to the number of accumulators that need to be processed. Long queries usually lead to a huge number of accumulators, which significantly hurt the efficiency. However, when resuming the query processing results of the initial query, we are able to start with a much smaller set of accumulators. Note that this strategy is not ranking safe when the expanded query is

Table 1: Efficiency comparison using TREC Ter-

abyte ad hoc queries (i.e., average query processing

time (ms) to retrieve 1K documents)

Data sets NoFB FB-BL FB-Incremental

TB04 105 5,522

3,023

TB05

77 4,502

2,462

TB06

71 4,542

2,082

Table 2: Comparison of the average size of accumu-

lator lists per query

FB-BL FB-Incremental

TB04 2,368K

370K

TB05 1,805K

252K

TB06 1,901K

240K

significantly different from the initial query (i.e., when  in Equation 2 is very small). However, as shown in Section 5, the optimal value of  is often large and this strategy does not affect the effectiveness significantly.
5. EVALUATION
5.1 Experiment Design
In our study, we use three standard TREC data sets which were created for TREC Terabyte tracks in 2004 to 2006. These three sets are denoted as TB04, TB05 and TB06 in this paper. All the data sets use Gov2 collection as the document set, and the collection consists of 25.2 million web pages crawled from the .gov domain. All experiments were conducted on a single machine with dual AMD Lisbon Opteron 4122 2.2GHz processors, 8GB DDR3-1333 memory and four 2TB SATA2 disks.
The basic retrieval model used in our experiments is the Dirichlet Prior smoothing method [40], where the parameter µ was set empirically to 2500. This method is labeled as NoFB. We use the relevance model [19] as the pseudorelevance feedback method. This model is chosen because of the following two reasons. First, it is a state-of-the-art pseudo-relevance feedback method that has been shown to be both effective and robust. Second, it is implemented in the Indri 1 toolkit, which makes it possible to verify the correctness of our implementation. This method is labeled as FB.
The implementation of our basic retrieval system (i.e., NoFB) is described in Section 3.3. Based on the basic retrieval system, we implemented two pseudo-relevance feedback systems: (1) FB-BL: traditional method of implementing pseudo-relevance feedback methods, i.e., the expanded query is processed independently to the original query; (2) FB-Incremental: our proposed approach that exploits the query processing results of the original query to speed up the processing of the expanded query.
Although we implemented our own indexing and query processing modules, we did not build the impact-based indexing directly from the collection. Instead, we built an initial index using Indri toolkit and use Indri API to transfer the indri index to our impact-sorted index. The size of the impact-based index for Gov2 collection is about 11.8GB with 442MB for term lookup, 7.5GB for inverted lists and
1http://www.lemurproject.org/indri/

558

Table 4: Impact of the expansion weight (i.e., ) on the efficiency (the average execution time (ms) per query)

Data sets TB04 TB05 TB06

Methods
FB-BL FB-Incremental
FB-BL FB-Incremental
FB-BL FB-Incremental

0.5 6,125 3,798 5,146 3,071 5,281 2,664

0.6 5,522 3,023 4,502 2,462 4,542 2,082

 0.7 4,435 2,130 3,577 1,762 3,571 1,406

0.8 2,723 1,221 2,466 1,113 2,412 778

0.9 1,597 826 1,323 845 1,114 560

Table 3: Effectiveness comparison using TREC Ter-

abyte ad hoc queries (MAP@1000)

Data sets NoFB FB-BL FB-Incremental

TB04 0.246 0.256

0.255

TB05 0.309 0.334

0.334

TB06 0.275 0.285

0.284

3.8GB for document summaries that are needed for the term selection step in the feedback methods. We decided to leverage the Indri index because (1) it saves our unnecessary efforts on writing codes to parse documents and count the term statistics; and (2) it enables direct comparison between our system and Indri since they now use the same information about the collection. When evaluating a query, we also leveraged Indri API to handle the tasks of converting terms/documents to their IDs.
Since we have to work with two indexes (the original Indri index and new Impact-sorted index) and all the information are read directly from disk rather than memory, our system could be slower than those using similar strategies. However, this should not affect our comparison on the two FB systems since they are implemented using the same strategy. Moreover, even working with two indexes, our developed system is still much faster than the Indri toolkit (version 5.1), which takes about 1 minute to process a query when using the relevance model on the Gov2 collection.
5.2 Experimental Results
The first two sets of experiments were conducted with the ad hoc queries used in TREC Terabyte tracks. Each data set has 50 queries, and we use title only field to formulate queries. The third set of experiments was conducted with the efficiency queries used in TREC Terabyte tracks. One data set has 50K queries while the other has 100K queries.
5.2.1 Efficiency and Effectiveness
We first examine whether the incremental approach can improve the efficiency. In particular, we set the number of feedback documents to 10, the number of expansion terms to 20 and the expansion weight  is set to 0.6. These parameters are chosen because they can lead to near-optimal performance in terms of the effectiveness. Table 1 shows the average processing time (ms) for each query when 1000 documents are retrieved. The trends on the three collections are similar. Both FB systems are slower than the NoFB system. It takes around 5 seconds for FB-BL to process a query, and takes around 2.5 seconds for FB-Incremental to do so. Clearly, FB-Incremental can achieve a speed up of 2 compared with FB-BL, which shows that the proposed incremental approach can improve the efficiency. Note that

we also evaluate the efficiency of the Indri toolkit, a widely used open source IR system. It takes around 1 minute for the Indri toolkit to process a query on the same collection. This suggests that the baseline system we implemented, i.e., FB-BL, is very efficient.
As discussed earlier, the speed up of FB-Incremental is achieved for two reasons. First, FB-Incremental leverages the query processing results of the initial retrieval, which avoids processing the original query terms twice. Second, the accumulator list inherited from the initial retrieval by FB-Incremental is much smaller. Table 2 shows the comparison of the accumulator list size between the two methods. Our incremental method creates only about 1/7 accumulators as those of baseline method. As a result, the new method avoids a lot of unnecessary index access and computing for those accumulators it does not include. Note that our baseline system, i.e., FB-BL, is a very strong baseline since it applied the accumulator trimming techniques which can reduce unnecessary accumulators during the process.
We also conduct experiments to examine whether the proposed approach would hurt the performance. Since our incremental approach leverages the accumulators used in the AND stage of the initial retrieval, it is possible that we may miss some relevant documents because they were not relevant to the original query and thus were not included in its accumulator lists. As a result, it is possible that the incremental approach might hurt the effectiveness, but it only happens when the expanded query is very different from the original query. Recall that the optimal value of  is 0.6 which indeed indicates that the original query and expanded query are similar. Table 3 shows the results measured with MAP@1000. To ensure the correct implementation of our system, we compare the baseline performance with the ones reported in the previous study [30] and find that the results are similar, which confirms our implementation of N oF B is correct. Moreover, we find that both FB methods can improve the retrieval accuracy about 4% to 8%, which is also similar to the performance improvement of the feedback method implemented in Indri. One interesting observation is that the effectiveness of the two FB systems are similar, which indicates that our proposed approach improve the efficiency without sacrificing the effectiveness.
5.2.2 Impact of Parameter Values
There are multiple parameters in the pseudo-relevance feedback methods such as the number of expansion terms, the number of feedback documents and the expansion weight (i.e.,  in Equation (2)). Since we use impact based document summary for term select, the impact of the number of feedback documents on efficiency is not very significant. Thus, we only focus on the other two parameters.

559

Table 5: Impact of the expansion weight (i.e., ) on the effectiveness (MAP@1000)

Data sets

Methods



0.5 0.6 0.7 0.8 0.9

TB04

FB-BL

0.254 0.256 0.257 0.255 0.252

FB-Incremental 0.252 0.255 0.256 0.255 0.252

TB05

FB-BL

0.333 0.334 0.332 0.328 0.321

FB-Incremental 0.333 0.334 0.332 0.327 0.321

TB06

FB-BL

0.284 0.285 0.285 0.284 0.281

FB-incremental 0.282 0.284 0.283 0.282 0.280

Table 6: Average query execute time (ms) based on different expansion weights (top 10 documents for each

query)

NoFB FB Methods



0.5 0.6 0.7 0.8 0.9

TB04 69

FB-BL

4,731 4,016 2,786 1,756 933

FB-Incremental 1,295 915 578 437 403

TB05 53

FB-BL

3730 2,923 1,999 1,131 547

FB-Incremental 983 709 451 341 310

TB06 46

FB-BL

3909 3,060 2,167 1,317 573

FB-Incremental 921 669 447 319 284

Speed-Up Speed Up

Figure 3: The speed-up rate on different expansion weight  (top 1000 documents returned)

3.2

3 TB04

2.8

TB05

TB06 2.6

2.4

2.2

2

1.8

1.6

1.4

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

Expansion Weight (Lamda)

Figure 4: The speed-up rate on different number of expansion terms
2.5
TB04 TB05 TB06 2

1.5

1

5

10

15

20

Number of Expansion Terms

As discussed earlier, the expansion weight is an important parameter and we should examine the impact of its value on both efficiency and effectiveness. The results are shown in Table 4 and Table 5. We see that the optimal value of  is around 0.6, which means that we should put more weights to the original query terms. According to Equation (2), the higher value of  means that we put more trust on the original query and less weights to the expansion terms. Given the characteristics of SAAT pruning technique, postings of terms with smaller weights are more likely to be pruned. As a result, we can observe a clear trend that both methods can improve the efficiency more when the value  is larger.
Figure 3 shows how the speed-up rate of F B-Incremental compared with F B changes with the expansion weight. The higher expansion weight we choose, the more the original query determines the final ranking list and the more time the incremental method can save. However when the expansion weight is too high (e.g. 0.9), SAAT pruning technique is rather efficient and leave small room for further improvement. As a result, the speed-up rate at high expansion weight decreases.
We further test the impact of number of expansion terms on efficiency. Table 4 shows the speed up rate of F B -

Incremental compared with F B - BL. It shows that our new method benefits more when the system adds more expansion terms into the original query. The main reason of this trend is that our method efficiently controls the grows of accumulator list size when the query becomes longer.
Another important factor that could affect the retrieval speed is the number of documents retrieved for each query. Figure 5 shows how the number of retrieved documents affect the speed up of F B - Incremental over F B - BL. The results indicate that the new method can achieve more speed-up when the system returns fewer results to users. We believe it is due to the reason that returning fewer documents brings higher cut-off threshold. And in our method, the high cut-off threshold which is got from the initial run provides a strong and efficient guidance on the accumulator selections in the resumed retrieval. As a result, the pool of candidate documents/accumulators is kept in a very small size and final result is generated soon. In opposite, the baseline method which does the second round retrieval independently cannot get the benefit from the high cut-off threshold and it still generates large number of accumulators in which most of them are unnecessary.

560

Figure 5: The speed-up rate on different number of retrieved documents per query

5

4.5

TB04

TB05

4

TB06

Speed Up Rate

3.5

3

2.5

2

1.5

101

102

103

Number of return documents per query

Figure 6: The speed-up rate on different expansion weight  (top 10 documents returned)

5

4.5

4

Speed Up

3.5

3

TB04

TB05

2.5

TB06

2

1.5

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

Expansion Weight (Lamda)

We also report the efficiency when retrieving only 10 documents for each query in Table 6. It is clear that when retrieving fewer documents, FB-Incremental can achieve a higher speed up, i.e., around four. This indicates that this method could be very useful for Web search domain since search users only need to look at 10 results per page. Finally, Figure 6 shows impact of the expansion weight on the speed-up when only 10 documents are retrieved. The trend is very similar to Figure 3.
5.2.3 Scalability
To further test the scalability of the incremental method and study how original query length affects the pseudo feedback system speed, we test both methods on 50k 2005 efficiency queries and 100k 2006 efficiency queries. For each query, 5 expansion terms are added and it only returns top 10 documents. The results show that the incremental method can stably improve the efficiency at a rate more than 2 at different original query length. For short queries, the baseline is fast and it leaves less space for further improvement. For long queries, the processing time is dominated

Table 7: Average query processing time (ms) on different query length (2005 50K queries)

Avg 1 2 3 4 5 >5

NoFB

35 4 10 26 55 91 175

FB-BL

297 38 94 302 526 749 1,102

FB-Incremental 124 18 43 97 204 316 540

Speed-up

2.4 2.1 2.2 3.1 2.6 2.4 2.0

Table 8: Average query processing time (ms) on different query length (2006 100K queries)

Avg 1 2 3 4 5 > 5

NoFB

103 3 12 39 81 132 275

FB-BL

739 39 99 357 690 1,030 1,642

FB-Incremental 300 26 43 110 248 441 797

Speed-up

2.5 1.5 2.3 3.2 2.8 2.3 2.0

by the original query terms and the optimizations in pseudo feedback part will not affect the total time too much. As a result, it is reasonable to observe a speed up rate summit at medium length query (i.e., 3-4 terms).
6. CONCLUSIONS
This paper focuses on efficient implementation of pseudorelevance feedback methods, an important yet under-studied problem. Motivated by the fact that original query terms are often processed twice in the two-round retrieval process, we proposed an incremental approach that can exploits the query processing results of the first round retrieval for computing the document scores in the second round. Although similar idea was mentioned in a previous study [6], it was concluded as a unsuccessful strategy. On the contrary, our method has been shown to be useful and can achieve a speed up of 2 over TREC collections. This paper is one of a few studies that try to bring the gap between the continuous research efforts on improving the effectiveness of the pseudorelevance feedback methods and the increasing efforts on efficient IR systems.
There could be many interesting directions for our future work. First, we plan to implement other pseudo feedback methods including both basic models such as model-based feedback [41] and more sophisticated models such as positional relevance model [22], and evaluate whether the proposed approach can improve their performance. Second, it would be interesting to study how to leverage the incremental approach to improve the efficiency of query processing for long queries. Finally, our system is built on impactsorted indexing with SAAT traverse. We plan to continue our efforts and study whether it is possible to improve the efficiency for feedback methods when using other query processing strategies such as DAAT and TAAT .
7. ACKNOWLEDGMENTS
This material is based upon work supported by the National Science Foundation under Grant Number IIS-1017026. We thank the anonymous SIGIR reviewers for their useful comments.
8. REFERENCES
[1] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space ranking with effective early termination. In Proceedings of SIGIR'01, 2001.
[2] V. N. Anh and A. Moffat. Impact transformation: efffective and efficient web retrieval. In Proceedings of SIGIR'02, 2002.
[3] V. N. Anh and A. Moffat. Simplified similarity scoring using term ranks. In Proceedings of SIGIR'05, 2005.
[4] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In Proceedings of SIGIR'06, 2006.

561

[5] B. Billerbeck and J. Zobel. Techniques for efficient query expansion. In Proceedings of String Processing and Information Retrieval Symposium, 2004.
[6] B. Billerbeck and J. Zobel. Efficient query expansion with auxiliary data structures. Information Systems, 31(7):573­584, 2006.
[7] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In Proceedings of CIKM'03, 2003.
[8] C. Buckley. Automatic query expansion using SMART: Trec-3. In D. Harman, editor, Overview of the Third Text Retrieval Conference (TREC-3), pages 69­80, 1995. NIST Special Publication 500-225.
[9] C. Buckley and A. F. Lewit. Optimization of inverted vector searches. In Proceedings of SIGIR'85, 1985.
[10] C. Buckley and S. Robertson. Relevance feedback track overview: Trec 2008. In Proceedings of TREC'08, 2008.
[11] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting good expansion terms for pseudo-relevance feedback. In Proceedings of SIGIR'08, 2008.
[12] M.-A. Cartright and J. Allan. Efficiency optimizations for interpolating subqueries. In Proceedings of the CIKM'11, 2011.
[13] M.-A. Cartright, J. Allan, V. Lavrenko, and A. McGregor. Fast query expansion using approximations of relevance models. In Proceedings of the CIKM'10, 2010.
[14] K. Collins-Thompson and J. Callan. Estimation and use of uncertainty in pseudo-relevance feedback. In Proceedings of SIGIR'07, 2007.
[15] J. V. Dillon and K. Collins-Thompson. A unified optimization framework for robust pseudo-relevance feedback algorithms. In Proceedings of CIKM'10, 2010.
[16] S. Ding and T. Suel. Faster top-k document retreival using block-max indexes. In Proceedings of SIGIR'11, 2011.
[17] R. Fagin. Combining fuzzy information: an overview. ACM SIGMOD Record, 31(2), 2002.
[18] V. Lavrenko and J. Allan. Real-time query expansion in relevance models. Technical Report IR-473, University of Massachusetts Amherst, 2006.
[19] V. Lavrenko and B. Croft. Relevance-based language models. In Proceedings of SIGIR'01, pages 120­127, Sept 2001.
[20] K.-S. Lee, W. B. Croft, and J. Allan. A cluster-based resampling method for pseudo-relevance feedback. In Proceedings of SIGIR'08, 2008.
[21] Y. Lv and C. Zhai. Positional relevance model for pseudo-relevance feedback. In Proceedings of SIGIR'10, 2010.
[22] Y. Lv, C. Zhai, and W. Chen. A boosting approach to improving pseudo-relevance feedback. In Proceedings of SIGIR'11, 2011.
[23] M. Mayer. Scaling google for every user. In Seattle Conference on Scalability, 2007.
[24] J. Miao, J. Huang, and Z. Ye. Proximity-based rocchio's model for pseudo relevance. In Proceedings of SIGIR'12, 2012.

[25] A. Moffat and J. Zobel. Self-indexing inverted files for fast text retrieval. ACM Transactions on Information Systems, 14(4):349­379, 1996.
[26] S. Robertson and K. Sparck Jones. Relevance weighting of search terms. Journal of the American Society for Information Science, 27:129­146, 1976.
[27] J. Rocchio. Relevance feedback in information retrieval. In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313­323. Prentice-Hall Inc., 1971.
[28] G. Salton and C. Buckley. Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science, 44(4):288­297, 1990.
[29] T. Strohman. Efficient processing of complex features for information retrieval. PhD thesis, University of Massachusetts Amherst, 2007.
[30] T. Strohman and W. B. Croft. Efficient document retrieval in main memory. In Proceedings of SIGIR'07, 2007.
[31] T. Strohman, H. Turtle, and W. B. Croft. Optimization strategies for complex queries. In Proceedings of SIGIR'05, 2005.
[32] T. Tao and C. Zhai. Regularized estimation of mixture models for robust pseudo-relevance feedback. In Proceedings of SIGIR'06, 2006.
[33] M. Theobald, R. Schenkel, and G. Weikum. Efficient and self-tuning incremental query expansion for top-k query processing. In Proceedings of the SIGIR'05, 2005.
[34] H. Turtle and J. Flood. Query evaluation: strategies and optimizations. Information Processing & Management, 31(1):831­850, 1995.
[35] H. Wang, Y. Liang, L. Fu, G. rong Xue, and Y. Yu. Efficient query expansion for advertisement search. In Proceedings of SIGIR'09, 2009.
[36] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. Morgan Kaufmann, 1999.
[37] J. Xu and W. B. Croft. Improving the effectivness of information retrieval with local context analysis. ACM Transactions on Information Systems, 18:79­112, 2000.
[38] Y. Xu, G. J. F. Jones, and B. Wang. Query dependent pseudo-relevance feedback based on wikipedia. In Proceedings of SIGIR'09, 2009.
[39] H. Yan, S. Shi, F. Zhang, T. Suel, and J.-R. Wen. Efficient term proximity search with term-pair indexes. In Proceedings of CIKM'10, 2010.
[40] C. Zhai and J. Lafferty. The dual role of smoothing in the language modeling approach. In Proceedings of the Workshop on Language Modeling and Information Retrieval, 2001.
[41] C. Zhai and J. Lafferty. Model-based feedback in the KL-divergence retrieval model. In Tenth International Conference on Information and Knowledge Management (CIKM 2001), pages 403­410, 2001.
[42] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Computing Surveys, 38(2), 2006.

562

Compact Query Term Selection Using Topically Related Text

K. Tamsin Maxwell
University of Edinburgh School of Informatics Edinburgh EH8 9AB, UK t.maxwell@ed.ac.uk
ABSTRACT
Many recent and highly effective retrieval models for long queries use query reformulation methods that jointly optimize term weights and term selection. These methods learn using word context and global context but typically fail to capture query context. In this paper, we present a novel term ranking algorithm, PhRank, that extends work on Markov chain frameworks for query expansion to select compact and focused terms from within a query itself. This focuses queries so that one to five terms in an unweighted model achieve better retrieval effectiveness than weighted term selection models that use up to 30 terms. PhRank terms are also typically compact and contain 1-2 words compared to competing models that use query subsets up to 7 words long. PhRank captures query context with an affinity graph constructed using word co-occurrence in pseudorelevant documents. A random walk of the graph is used for term ranking in combination with discrimination weights. Empirical evaluation using newswire and web collections demonstrates that performance of reformulated queries is significantly improved for long queries and at least as good for short, keyword queries compared to highly competitive information retrieval (IR) models.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query Formulation
Keywords
Random Walk; Verbose Queries; Query Reformulation
1. INTRODUCTION
Query reformulation is a rich area of information retrieval (IR) research, including techniques for query expansion, dependency analysis, query segmentation and term selection. For short queries, IR effectiveness is improved by smoothing and query expansion using techniques such as pseudo
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

W. Bruce Croft
University of Massachusetts Dept. of Computer Science Amherst, MA 01003, USA
croft@cs.umass.edu
relevance feedback [14, 23, 34]. Conversely, long or verbose queries contain words that are peripheral or shared across many topics so expansion is prone to query drift. Reformulation instead focuses on term weighting [18, 5], term selection [3, 2] and query reduction [15]. The selection of informative terms, defined as one or many-word units, becomes critical as the number of potentially noisy terms increases.
Techniques for term selection and term weighting automatically emphasize the `essence' of a query. Several successful techniques jointly optimize weights and term selection using both global statistics and local syntactic features [3, 33]. However, these features can fail to detect or differentiate informative terms, where an informative term represents the essential aspects of query meaning given a collection of documents. Global statistics are strong indicators of term importance [4] but do not reflect local query context. There is also evidence that they do not lead to significant improvement in query effectiveness [20]. Syntactic features precisely identify word relations but do not identify all the informative relations [22]. The ubiquity of global statistics and syntactic features in current methods for term selection suggests a continuing need for improved understanding of alternatives ways to estimate term informativeness [20].
In this paper, we present PhRank (phrase rank), an algorithm that uses pseudo relevance feedback for in-query term selection rather than expansion to terms not in a query. Compact and focused terms are selected from a list of candidates by ranking terms using a Markov chain framework and picking the top-ranked candidates. Candidate terms are all combinations of 1-3 words in a query that are not stopwords. Term scores are computed using the average word score for words in a term, combined with global discrimination weights. Word scores are computed using a random walk of a word co-occurrence graph constructed from pseudo relevant documents, combined with word salience weights for the query and global contexts. This approach selects terms that achieve significant gains in both recall and precision compared to the most effective techniques for query reformulation that do not use term weighting, expansion, or stratified dependencies [4]. This is achieved by focusing on a limited number of word relationships with a core concept.
PhRank has three advantages compared to previous work. First, to our knowledge PhRank is the first method to use pseudo relevance feedback for in-query term selection. Feedback was applied initially to estimate weights for independent query words without selection [8] and is predominantly used for query expansion. Previous approaches to in-query term selection use highly localized word context, in the form

583

Query: Locations of volcanic activity which occurred within the present day boundaries of the U.S. and its territories.

PhRank

Sequential Dependence

Key Concept

Subset Distribution

volcanic volcanic boundaries volcanic territories volcanic activity volcanic occurred

locations volcanic volcanic activity activity which which occurred occurred within within present present day day boundaries boundaries us us territories

present day boundaries volcanic activity

volcanic day boundaries day boundaries territories volcanic activity occurred day boundaries present day boundaries volcanic boundaries territories volcanic activity occurred activity occurred day boundaries volcanic activity occurred boundaries volcanic present day boundaries volcanic occurred boundaries
+ 20 bigrams (if weights collapsed)

Table 1: Terms selected by four highly effective query reformulation models for TREC GOV2 topic #756.

of syntactic relations and co-occurrence, and global context in the retrieval collection. They do not consider query context, such as a general query topic identified from pseudo relevant documents. The intuition behind a random walk of a query context graph is that it reinforces words that capture query `essence' more strongly than words that are peripheral to query meaning. For this reason, informative terms are more readily apparent if query context is considered.
Second, PhRank achieves significant performance gains with a small number of compact terms while retaining the flexibility to select more and longer terms if required. Other approaches use a robust, but less effective, distribution over many imprecise but approximately relevant terms. Alternatively, they take a relatively inflexible, high-risk approach that prefers a few exact terms and is prone to mistakes. For example, Table 1 shows the terms selected for TREC topic #756 by three top performing IR models. The sequential dependence (SD) model is straightforward and robust [24]. The key concept (KC) model [3] aims at a highly succinct representation but is hampered by a requirement that terms are predefined syntactic units (noun phrase length). The subset distribution (SDist) model [33] optimizes over many term and weight variables and is highly effective but is biased towards longer terms of 3-6 words. PhRank demonstrates that for a majority of queries, a few precise terms, in addition to a standard query likelihood representation, are more effective than term distributions. They also result in queries that have up to 90% fewer terms, and these terms are typically only 1-2 words long.
Finally, an affinity graph captures aspects of both syntactic and non-syntactic word associations in an integrated manner. A co-occurrence affinity graph shares the same structure as a global dependency graph in which edges are defined by linguistic relations. Specifically, the most connected vertices are high frequency functional words and less frequent content-bearing words tend towards the edges of the graph [10, 11]. By consequence, the semantic significance of a word is correlated with the degree of the corresponding vertex. We infer that the shared structure of dependency and affinity graphs captures aspects of both syntactic and non-syntactic word associations. Moreover, an affinity graph can be used to estimate the semantic significance of words.
To summarize, unlike existing models of term selection, PhRank integrates three characteristics that we believe are important to accurately identify the most informative terms: query context, compactness, and integration of syntactic and semantic knowledge. We show that consolidating these

characteristics delivers up to 14% performance improvement compared to highly competitive methods for TREC description topics and is comparable to the state-of-the-art for TREC keyword (title) queries.
The rest of this paper is organized as follows. In Section 2 we review related work and its connection to PhRank. Section 3 defines the problem of term selection and its key characteristics. In Section 4 we formally describe the PhRank algorithm. Section 5 presents the evaluation framework. In Section 6 we discuss the results of empirical experiments, and Section 7 concludes the paper.

2. RELATED WORK

Markov chain frameworks and spreading activation networks for a network of words are well-studied in IR with origins in associative word networks [7]. They include research on webpage authority [26], e.g. PageRank, as well as query expansion [17, 6, 23, 14]. However, they are novel for unexpanded term selection.
The Markov chain framework uses the stationary distribution of a random walk over an affinity graph G to estimate the importance of vertices in the graph. Vertices can represent words, in which case edges represent word associations. If the random walk is ergodic, affinity scores at vertices converge to a stationary distribution that can be used to establish a ranking, e.g. over words.
A random walk describes a succession of random or semirandom steps between vertices vi and vj in G. Let ij be the transition probability (or edge weight) between vi and vj. The path of the walk is determined by a square probability matrix H = (hij ) with size n, where n is the number of unique vertices in G. The probability hij = ij if vi and vj are connected, and hij = 0 otherwise. Affinity scores are computed recursively. Let jt be the affinity score associated with vj at time t. Then jt+1 is the sum of scores for each vi connected to vj , weighted by the possibility of choosing vj as the next step on the path from vi:

jt+1 =

ihij

(1)

i

It is usual to introduce some minimal likelihood that a path from vi at time t will randomly step to some vj at time t + 1 that may be unconnected to vi. Otherwise, clusters of vertices interfere with the propagation of weight through the graph. This likelihood is often defined to be the uniform probability vector u = 1/n, although any other vector can be chosen [14]. A corresponding factor reflects the likeli-

584

hood that a path will follow the structure of edges in G. A damping factor  controls the balance between them:

t+1 = tH + (1 - )u

(2)

The Markov chain framework has has been used in a principled way to smooth and expand queries in a language modeling framework [34], but application in query reformulation has been limited to selection of individual words that do not appear in the original query. By contrast, PhRank ranks terms containing one or more words that do appear in the original query. Moreover, while expansion techniques can exacerbate problems with unrelated terms, PhRank reduces the problem of query drift through improved term selection.
Markov chain processes have also been applied in text summarization for keyphrase extraction. This is a task similar to term detection for automated indexing. TextRank [25], SingleRank [32] and ExpandRank [32] use a random walk to identify salient sequences of nouns and adjectives. They improve over earlier unsupervised methods for this task but achieve only 30-40% task accuracy and may be outperformed by a tf.idf metric [13]. ExpandRank supplements text with pseudo relevant documents but does not improve performance compared to SingleRank [13]. PhRank is similar to these algorithms but is more flexible and better suited to IR. It uses multiple sources of co-occurrence evidence and the discriminative ability of terms in a collection. It also produces an unbiased ranking over terms of mixed lengths, does not rely on syntactic word categories such as nouns, and permits terms to contain words with long distance dependencies.
Other related work focuses on techniques for identification of dependent terms [28], key concepts [3], or sub-queries [16, 33]. This includes techniques for the removal of stop structure [15]; reduction of narrative queries to word sequences associated with part of speech blocks [19]; selection of candidate sub-queries using noun phrases [3]; query term ranking using dependency tree relations [27]; and optimized ranking over possible subqueries [33]. There is also a significant body of work on learning individual term weights [18, 5]. Much of this work incorporates syntactic and statistical features in machine learning.

3. PRINCIPLES FOR TERM SELECTION
We hypothesize that the following principles define word and term informativeness. These principles motivate the PhRank algorithm detailed in the next Section.
An informative word:
1. Is informative relative to a query: An informative word should accurately represent the meaning of a query. However, queries do not provide much context with which to determine meaning. Pseudo relevance used in PhRank is an established means of enhancing a query representation [29].
2. Is related to other informative words: The Association Hypothesis [31] states that, "if one index term is good at discriminating relevant from non-relevant documents, then any closely associated index term is also likely to be good at this". PhRank uses a Markov chain framework in which the value assigned to a word i is determined by the value of other words connected to i, and the number of connections to i.

An informative term:
3. Contains informative words: Consider a base case in which a term has only one word. It is obvious that this term must also display the properties of an informative word. We deduce that all terms must contain informative words. PhRank considers the informativeness of individual words when ranking terms.
4. Is discriminative in the retrieval collection: A term that occurs many times within a small number of documents gives a pronounced relevance signal. PhRank weights terms with a normalized tf.idf inspired weight.
4. THE PHRANK ALGORITHM
PhRank captures query context with an affinity graph constructed from stopped, stemmed pseudo-relevant documents. Vertices in the graph represent unique stemmed words (or simply, stems). Edges connect stems that are adjacent in the processed pseudo relevant set. Graph transition probabilities (edge weights) are computed using a weighted linear combination of stem co-occurrence, the certainty that the document in which they co-occur is relevant, and the salience of sequential bigram factors in the pseudo relevant set. The edge weights thus represent the tendency for two stemmed words wi and wj=i to appear in close proximity in documents that that reflect a query topic.
Stems in the affinity graph are scored using a random walk algorithm. Following convergence, stem scores are weighted by a tf.idf style weight that further captures salience in the pseudo relevant set. This aims to compensate for potential undesirable properties of the random walk. Finally, term scores are computed using the average score for stemmed words in a term, weighted by term salience in the retrieval collection. The m highest scoring terms are employed to reformulate Q. Pseudo code for the algorithm is shown in Figure 1. The rest of this section describes the algorithm in more detail, including three heuristic weights (factors r, s and z). A number of choices for these factors could have been made and specific choices are analyzed in Section 6.1.
1) Graph construction (principle 1): Let a query Q = {w1, ...wn} and C be a document col-
lection. The top k documents retrieved from C using Q are assumed to describe a similar topic to Q. We define C to be the retrieval collection plus English Wikipedia. We use Wikipedia since it improves IR results for query expansion using a random walk [6], but also explore the effectiveness of using the retrieval collection alone. The top k documents in C, together with Q itself encoded as a short document d0, comprise neighboring documents in the neighborhood set N = {d0, ....dk}.
Documents in N are stopped using a minimal list of 18 words [21] and stemmed using the Krovetz stemmer. This improves co-occurrence counts for content-bearing stems and reduces the size of an affinity graph G constructed from the processed documents. Stoplisting with a longer list hurt IR effectiveness. Edges in G connect stemmed words i and j at vertices vi and vj if i and j are adjacent in N . Documents in N with only one word (e.g. some queries) are discarded to ensure that all vertices have at least one connecting edge.
2) Edge weights (principle 1): Transition probabilities (edge weights) ij are based on a
weighted linear combination of the number of times i and

585

k = 5 resourceList = [ C, wikipedia ]

for q in queryList:

N = set() for rsc in resourceList:
N.add( retrieve_top_k( q, rsc ) ) N = retrieve_top_k( q, N ) N.add( q )

# one word type per row and column G = arrayStruct() for ( doc, docRel ) in N:
doc.stopStem() G.grow( buildGraph( doc, docRel ) )

G.idfWeightEdge() G.normalize() G.iterate() G.weightVertex()

# bigram wt r # word wt s

T = q.terms for term in q:
term.wt = G.score( term ) term.wt *= term.globalWt( C ) # term wt z T.sortByWeight()

def buildGraph( doc, docRel ): docG = index( doc ) docG.linearWt( uw2, uw10 ) docG.weight( docRel )
return docG
def score( term ): S = 0
for w in term.wordSplit(): S += self.affinityScore( term )
return S /= term.length()
def globalWt( C ): l = self.length() wt = C.tfidf( self ) * l^l
return wt

Figure 1: Pseudocode for the PhRank algorithm.

j co-occur in windows W of size 2 and 10. This is motivated by the idea that different degrees of proximity provide rich evidence for word relatedness in IR [25, 32, 24]. Edge weights are defined by:

ij = r 

p(dk|Q)(cijW2 + (1 - )cijW10 )

dk N

where p(dk|Q) is the probability of the document in which the stems i and j co-occur given Q, and cijW2 and cijW10 are the counts of stem co-occurrence in windows of size 2 and 10 in N .  is set to 0.6. We set the relevance of d0 to Q to be high but reasonable (-4 for Indri log likelihood scores). The exact setting has very little effect on term ranking.
Factor r is a tf.idf style weight that confirms the importance of a connection between i and j in N . G includes many stemmed words, so unweighted affinity scores can be influenced by co-occurrences with highly frequent, but possibly uninformative, stems such as `make'. Factor r minimizes this effect. Since the tf component is already accounted for by cijW2 + (1 - )cijW10 , we reduce r to the idf component:

rij

=

log2

ijN
1+

cijW2 cijW2

3) Random Walk (principle 2): A random walk of G follows the standard Markov chain
framework presented in Section 2. Edge weights are normalized to sum to one and j is the affinity score of the stem associated with vj . j indicates the importance of a stem in the query context. Iteration of the walk ceases when the difference in score at any vertex does not exceed 0.0001. This translates to around 15 iterations but may be optimized for efficiency. The damping factor  = 0.85 is equivalent to a walk along five connected edges in G before the algorithm randomly skips to a possibly unrelated vertex. The average sentence length in English is around 11-15 words so this equates to skipping at or near the boundary of a sentence around one half of the time.

4) Vertex weights (principle 3):

Following the random walk, stemmed words in G are fur-

ther weighted to capture both the exhaustiveness with which

they represent a query, and their global saliency in the col-

lection [30]. Exhaustivity indicates whether a word w1 is a

sufficient representation of the query. If w1 appears many

times in N then it is less likely that a term x containing w1

will benefit from additional words w2...wn. For example, the

term geysers quite exhaustively represents the TREC query

#840, `Give the definition, locations, or characteristics of

geysers'. A term containing additional words, e.g. defini-

tion geysers, is not more informative. However, common

stems, such as `make', tend to have high affinity scores be-

cause they co-occur with many words.

Factor s balances exhaustivity with global saliency to iden-

tify stems that are poor discriminators been relevant and

non-relevant documents. Specifically, swn = wnfavg  idfwn , where wnfavg is the frequency of a word wn in N , averaged

over k + 1 documents (the average frequency) and normal-

ized by the maximum average frequency of any term in N .

As usual, idfwn is the inverse document frequency of wn in

the collection, so idfwn

=

log2

|C| 1+dfwn

where |C| is the vo-

cabulary of stemmed words in the collection C, and dfwn is

the number of documents in C containing wn.

An advantage of factor s is that it enables PhRank to be

independent of an IR model. A model may treat the com-

ponent words of terms as independent or dependent. Factor

s helps to ensure that the selected terms are informative

irrespective of this representation.

5) Term ranking (principles 3, 4): To avoid a bias towards longer terms, a term x is scored
by averaging the affinity scores for its component words {w1, [...wn]}. Term rank is determined by the average score multiplied by a factor zx that represents the degree to which the term is discriminative in a collection:

zx = fxe  idfxe  lx
Let xe be a proximity expression such that the component words of x appear in an unordered window of size W = 4 per word. Thus, a term with two words appears in an 8-word window, and a term with three words appears in a 12-word window. The frequency of xe in C is fxe and idfxe is defined analogously to idfwn above. lx is an exponential weighting factor proposed for the normalization of ngram frequencies during query segmentation [12]. This factor favors longer ngrams that tend to occur less frequently in text. Multiplication of ngram counts by lx enables comparison of counts for terms of varying length. Let |x| be the number of words in x, then lx = |x||x|.
In summary, the PhRank algorithm describes how informative a term x is for Q compared to other terms. This is computed using the function:

wn

f (x,

Q)

ra=nk

zx



wn x
n

(3)

586

4.1 Diversity filter
PhRank often assigns a high rank to multi-word terms that contain only one highly informative word. This is due to use of an average word affinity score, and is desirable because informative terms can contain uninformative words. For example, given a query about `the destruction of Pan Am Flight 103 over Lockerbie, Scotland ' (TREC #409), the term `pan flight 103 ' is informative even if the polysemous word `pan' is uninformative by itself. However, this can result in low diversity of top ranked terms used in query reformulation. To increase diversity, we apply a simple, heuristic filtering technique with top-down constraints.
Given a ranked list, all terms with a score of zero are discarded. The lowest ranked term, xn, is checked against the list of terms with a better rank xm<n. Let A be the set of component words in xn and B be the set of component words in any single term xm<n. If A : A  B  A  B then we discard xn iff every component word of xn is contained in at least one xm=n that is in the retained list of ranked words at the time B is evaluated. For example, if xn =`birth rate' and we find some xm<n =`birth rate china' then we discard xn on the assumption that the longer term better represents the information need. If xn =`declining birth rate' and we find some xm<n =`birth rate' and some xm<n =`declining birth' then we discard xn on the assumption that the shorter terms better represent the information need and the longer term is redundant. Note that the top-ranked term is always retained. This process is adequate to increase diversity in the ranked list and ensures that no vital information is lost, but clearly presents an opportunity for further improvement.
5. EVALUATION FRAMEWORK
This section describes comparative models and query reformulations used to assess the degree to which PhRank queries are robust, precise and succinct, and represent word dependency. The main point of comparison is a robust and highly effective IR model (SD) that uses term selection and is employed as a baseline in related work [3, 28, 33]. We also compare against the model with the highest mean average precision of which we are aware that is relevant to a discussion of term selection with query expansion (sDist). Finally, since compact queries are a feature of PhRank, we compare against a succinct yet competitive model that selects only two terms (KC). We note that superior IR effectiveness is possible with term weighting, but we focus on results using unweighted terms to more clearly demonstrate the effect of term selection alone. We also report results for query likelihood (QL) for reference even though this model uses no term selection. This is because the other models reported include a query likelihood component. We do not compare against models that use pseudo relevance feedback for expansion. Pseudo relevance feedback without expansion is a novel feature of our work that contributes to PhRank performance.
5.1 Robustness
Evaluation across three TREC collections using both description topics and title queries requires a strong, robust baseline. We use a sequential dependence (SD) variant of the Markov random field (MRF) model [24]. SD uses a linear combination of three cliques of terms, where each clique

is prioritized by a weight c. The first clique contains individual words (query likelihood QL), 1 = 0.85. The second clique contains query bigrams that match document bigrams in 2-word ordered windows (`#1 '), 2 = 0.1. The third clique uses the same query bigrams as clique 2 with an 8-word unordered window (`#uw8 '), 3 = 0.05. For example, the query `new york city' in Indri1 query language is:
#weight( 1 #combine(new york city) 2 #combine(#1(new york) #1(york city)) 3 #combine(#uw8(new york) #uw8(york city)))
Because it is very simple to generate SD queries, this model is regularly used as a baseline. Highly effective weighted variants have also been developed [4, 33, 28]. We compare SD with a PhRank model (PR-.F) that uses the same query format, except the second and third cliques contain PhRank terms instead of query bigrams. In addition, because PhRank terms may be 1-3 words long, we adjust the unordered window operator in the manner proposed for the full dependence variant of the MRF model [24]. Namely, the window size is 4 multiplied by the number of words in a term. Note that for a term with only one word i, the operators #1(i) and #uw8(i) equate to a search for the word i in a document. So, if two terms `york ' and `new york city' are selected by PhRank, the PR-.F model has the form:
#weight( 1 #combine(new york city) 2 #combine( york #1(new york city)) 3 #combine( york #uw12(new york city)))
PR-.F uses five terms for description topics and feature analysis experiments, and three terms for title queries (or less, if the required number of terms is not available after rank filtering).
5.2 Precision
Highly competitive performance compared to SD can be achieved by jointly optimizing possible subqueries and subquery weights using syntactic and statistical features. Among the models of which we are aware, the subset distribution model (sDist) [33] achieves the highest mean average precision on long queries using term selection with no higher order dependencies [4]. However, it is not entirely fair to compare sDist with PR-.F since sDist uses heavily optimized weights for ten subqueries. A subquery in sDist is a linear combination of a standard SD query and one selected term treated as a bag-of-words. This compares with the flat  weights used in SD and PR-.F. Despite this, sDist is the most effective model we can use for stringent comparison that ensures real progress has been made. We therefore include sDist in our evaluation even though queries for Robust04 are not available from the authors.
5.3 Succinctness
Queries formulated with PhRank have few terms and a maximum of three words per term. To evaluate highly succinct queries we compare against Key Concepts (KC) [3]. KC is another succinct weighted linear feature model that combines two cliques. The first clique (1 = 0.8) contains a bag-of-words query representation of the original query,
1http://www.lemurproject.org/

587

and the second clique (2 = 0.2) combines a weighted bagof-words representation for each of two selected terms. The top terms are selected from the set of query noun phrases using a decision tree with frequency-based features [3]. The model reduces to a weighted representation of the original query with word independence. If `city' and `new york ' are the top two terms, it takes the following form, where  is the decision tree confidence score associated with a term:
#weight( 1 #combine(new york city) 2 #weight(  #combine(new york)  city ))
To compare against KC, we present a model (PR-zF2) that takes the same form but does not benefit from term weights . We use the two top terms selected by PhRank.
5.4 Word dependence
Assumptions of word dependence are an important issue in IR. To clarify the dependence assumptions made by PhRank we refer to four models of phrase belief presented by [9] (Figure 2, a-d). These models show how belief in a document dc  C flows to belief in a query Q in an inference network, and thus how words and terms can be dependent. In PhRank, we do not perform inference, but by analogy these models aid interpretation of PhRank features.
Of the four models in Figure 2, the more general dependence assumption (d) is used by PhRank to score words, and term ranks are computed using an independence assumption (b). Even if component words of terms are not connected in G, weight is propagated through the graph such that word dependencies affect evidence for a term. PhRank factors z, s and r reflect Figure 2 models a, b and c respectively.
We speculate that optimal term selection occurs when 1) a high rank is assigned to terms that are important under all four interpretations of phrase belief according to evidence in N , 2) the rank of terms that have less evidence under one or more interpretation decreases gracefully, and 3) the ranking meets the principles of term selection proposed in Section 3.
6. EXPERIMENTS
We examine the performance of PhRank in three ways. First, we compare versions of the algorithm in which we omit specific features. Second, we compare performance of queries reformulated using PhRank top ranked terms against highly effective models for both TREC description topics and title queries. Third, we compare on a query by query basis the robustness and performance error for PhRank versus a distributed approach to term selection (SD).
We evaluate on three TREC collections using version 4.12 of Indri with Dirichlet smoothing,  = 2500. The Robust04, WT10G and GOV2 newswire and open web text collections have queries that vary substantially in length and known difficulty. Together they provide a diverse platform for experiments (Table 2). Topic 672 is excluded from the Robust04 evaluation as the collection contains no relevant documents. All collections and queries are stopped and stemmed using the INQUERY stoplist and Krovetz stemmer. Queries are further stopped to exclude 18 TREC stopwords such as `describe' [1]. Candidate terms are all units of 1-3 words in the power set P(q) of content-bearing words in Q. IR models are defined in Section 5. Pseudo relevant documents are retrieved using a sequential dependence model. Mod-

Name ROBUST04 WT10G GOV2

# Docs 528,155 1,692,096 25,205,179

Topic Numbers 301-450, 601-700 (-672) 451-550 701-850

Table 2: TREC collections and topics

ROBUST04

MAP R-Pr

Description topics

rTsTzT 26.65

zF

27.32

sF

26.03

rF

26.67

30.05 30.32 29.61 30.02

Title queries

rTsTzT 24.87

zF

26.14

sF

25.90

rF

26.32

29.04 30.13 30.03 30.25

WT10G MAP R-Pr

0.00 23.68 21.00 22.44

0.00 26.71 25.10 25.70

21.78 20.85 20.72 21.81

25.73 24.72 24.30 25.70

GOV2 MAP R-Pr

28.83 28.64 27.93 28.93

34.55 34.13 33.67 34.65

31.49 30.73 31.30 31.59

37.26 36.26 36.91 37.42

Table 3: Feature analysis results. Description topics perform best with omission of the global term weight z (zF). Title queries perform best with the omission of bigram salience weight r (rF).

els `.F' exclude the feature represented by `.' and models `.T' include the feature. Thus, model rTsTzT includes all features.

6.1 Feature analysis
In this section we explore the impact of PhRank feature removal on IR effectiveness assessed using model PR-.F. Results in Table 3 show that PhRank is highly effective in selecting informative terms for a query. However, not all the features proposed consistently improve term ranking for IR. Description topics are most effective when factor z is omitted, and title queries are most effective when r is omitted.
1) Factor r: words dependent on term Factor r imperfectly captures belief in component words
dependent on belief in a term (Figure 2c). It uses global bigram statistics to scale edge weights in G. During a random walk, this affects the affinity scores for individual stemmed words. However, bigram statistics are only an approximate measure of term unity. More problematically, r relies on words in a term being co-occurrent in N . Highly informative terms are likely to have their component words connected in G, but this is not guaranteed. For terms with more than two words, edge weights in G also must be factored. Perhaps due to these limitations, r had minimal impact on IR effectiveness for title queries and could be omitted to improve algorithm efficiency.
However, we note that r is useful for description topics. We speculate that this is because the query words for description topics may be peripheral to the core information need. Spurious adjacent word dependencies in Q tend to appear in the pseudo relevant set because bigrams feature in the IR model employed for initial retrieval. Thus, if word cooccurrence in Q reflects query meaning, as typically occurs with title queries, the edges and weights used to initialise G are likely to be adequate. If word co-occurrence is spurious, the initialisation may be suboptimal. Factor r ameliorates misleading initial edge weights for description topics.

588

dc

wi wj

x

Q

(a) Phrase as elemental unit

dc

wi

wj

x

dc

x

wi

wj

dc

wi

wj

Q

Q

Q

(b) Independent belief in component words

(c) Component words dependent on phrase

(d) Dependent belief in component words

Q , dk

x

wi

wj

x

C

Q' (e) PhRank

Figure 2: Four models of phrase belief proposed by [9] (a-d). Word dependence in PhRank can be understood as a hybrid with features of all these models (e) for term x ={wi, wj} and documents dk  N .

2) Factor s: word independence Factor s contributes to belief in a term dependent on be-
lief in individual words (Figure 2b). It weights each vertex in an affinity graph by its salience in the query context N balanced by its salience in the document collection. Omission of s substantially hurt IR effectiveness. Among all the features tested it had the most impact on overall performance, perhaps because independent belief in words is the most important factor in IR effectiveness [24]. In addition, work with random walk algorithms for query expansion has found that words co-occurring with high frequency are of low value if they are not semantically close to the query [6]. We suggest that salience in N as captured by s represents semantic closeness to the query, and salience in the collection helps to identify high frequency co-occurrent words.
3) Factor z: term as elemental unit Factor z represents belief in a term independent of belief
in its component words (Figure 2a). It resembles a standard tf.idf weight and reflects the principle that a term should be discriminative in the retrieval collection. Given the established effectiveness of tf.idf weighting, it is surprising that omission of z improves IR effectiveness for description topics. However, it is based on observations of a term in an unordered proximity window in the retrieval collection. The way such observations are made implies a dependence assumption that may not provide an accurate estimate of term salience. In addition, it has recently been suggested that global statistics rarely improve retrieval performance and that local, document level evidence is sufficient [20].
We also note that both r and z account for the discrimination ability of multi-word units in the collection: r applies to bigrams and z applies to words in unordered windows. This encoding is partially redundant, so description queries may not require z because they use r, and title queries may require z because they do not use r. We remove z for our final runs for description queries, and retain it for title queries.
4) Factor k: pseudo relevant documents Results in Table 4 show that the most improvement in IR
effectiveness is achieved with 2 to 5 pseudo relevant documents. Higher k decreases effectiveness due to the introduction of non-relevant information. However, PhRank is quite robust to variation in k due to the weighting of co-occurrence relations by document relevance. Even with construction

¬PRF k2 k5 k10 k50 k100

ROBUST04 MAP R-Pr 26.44 29.59 26.86 30.05 27.32 30.32 27.29 30.05 27.09 30.11 26.80 29.82

WT10G MAP R-Pr 21.88 25.36 22.76 25.42 23.68 26.71 22.33 25.02 23.04 26.21 22.78 26.11

GOV2 MAP R-Pr 27.85 33.28 28.81 34.38 28.64 34.13 28.64 34.16 28.82 34.27 28.34 33.91

Table 4: IR effectiveness for description topics using k pseudo relevant documents. Best IR effectiveness is achieved using the top few documents only.

Figure 3: IR effectiveness with feature analysis and variable threshold. In many cases PhRank achieves performance gains with two terms, and is robust to variance in the number of terms selected.
of the affinity graph from the original query only (¬PRF), PhRank performs better than sDist and comparably with SD. This suggests that most important information is retained by the term selection process.
6.2 Retrieval performance
We present our best results for runs using description topics and title queries on three TREC collections.
6.2.1 Robustness
For description topics, the results in Table 5 show highly significant or significant improvement in mean average precision (MAP) and R-precision compared to the SD baseline for

589

(a) TREC description topics

ROBUST04

WT10G

GOV2

MAP R-Pr MAP R-Pr MAP R-Pr

Robust and precise

QL

25.25

SD

26.57

sDist

-

PR-zF 27.32

PR¬W 27.19

28.69 30.02 - 30.32 30.12

19.55 22.77 25.77 31.26 20.63 24.31 28.00 33.30 21.14 24.93 27.64 33.50 23.68 26.71 28.64 34.13 22.90 26.57 28.18 33.77

Succinct
KC PR-zF2 PR¬W2

25.62 25.91 25.76

28.89 28.92 28.33

20.15 22.58 26.88 22.02 25.69 27.04 21.43 25.40 26.05

32.73 32.75 31.75

QL SD PR-rF PR¬W

(b) TREC title queries

ROBUST04 MAP R-Pr 24.37 28.52 26.16 30.25 26.32 30.25 26.44 30.40

WT10G

GOV2

MAP R-Pr MAP R-Pr

19.48 23.08 28.55 34.41

20.97 23.75 31.25 36.88

21.81 25.70 31.59 37.42

21.76 25.57 31.50 37.14

Table 5: Retrieval results for description topics and title queries. PhRank significantly outperforms a highly effective baseline for description topics and is strongly competitive for title queries.  shows significant (p < .05) and  highly significant (p < .01) results compared to SD and KC respectively as determined by a sign test.
GOV2 and WT10G. Substantial improvements in precision on Robust04 are just short of significance. For title queries, improvement is highly significant for WT10G and comparable to the baseline for other collections. Increased precision occurs for top ranked documents (top 5 and 10) as well as being a general trend in the results. Exclusion of Wikipedia has a small negative effect as shown by PR¬W and PR¬W2 corresponding to PR-.F and PR-zF2 respectively.
To assess the quality of the ranked list of terms without a measure of ground truth for term informativeness, we explore the impact of varying the number of terms included in query reformulations. The results in Figure 3 show that the quality of top terms output by PhRank are stable as more terms are selected. Further, a large part of the gain in precision is attributed to the top two terms.
To investigate performance further, for each collection we manually reviewed the ranked term lists for queries that perform significantly better or worse than SD (>100% change in MAP), and 10 queries with comparable performance. Across all queries observed, there is a strong tendency for PhRank to single out one word, or a pair of words, as the main concept of the query, and rank all terms that contain the main concept highly. Remaining terms are ranked according to the contributions of their additional words. This high risk, high reward strategy negatively affects the robustness of PhRank on a query by query basis as shown in Figure 4 for description topics. Title queries exhibit similar behavior.
For example, one of the best performing queries for GOV2 is #756 as shown in Table 1. For this query, identification of `volcano' as the main concept greatly helped IR. On the other hand, the same strategy for query #780, one of the worst performing queries for GOV2 (see Table 6), selected

Figure 4: MAP difference compared to SD baseline per query for description topics. PhRank does slightly better on harder queries. The strategy of focus around one concept usually helps, but can significantly hurt some queries.
`earth' as the main concept that should be included in all top terms. This resulted in terms that were representative of the query, but not well distributed.
Nevertheless, Figure 4 shows consistent improvement for queries that are known to be harder (Robust04 HARD track) or easier (high baseline MAP). It is more likely that PhRank selects an appropriate main concept for easy queries because the pseudo relevant documents are of high quality. Difficult queries are less clearly defined and often benefit from the strong directional focus provided by PhRank terms.
In comparison, models like SD and sDist, take a more robust approach to term selection with a distribution of possibly relevant terms. This presents a very different term selection strategy, so one potential avenue for improvement is interpolation of PhRank term selection with bigrams in SD. However, the robustness of a distributed term selection approach can come with a tradeoff in overall effectiveness. Initial interpolation experiments with a weighted linear combination of SD and PhRank terms did not appear to yield any benefit over PhRank terms alone.
Alternatively, the properties of G may be turned to advantage. It has been observed that a Markov field framework selects more general and robust query expansion terms than competing methods [6]. A combination of query expansion and term selection using a Markov field framework may balance complementary high reward and robust query reformulation strategies and result in significant overall gains.
6.2.2 Precision
Results in Table 5 show significant improvement in MAP and R-precision for PhRank compared to sDist for both GOV2 and WT10G. PhRank terms are significantly more precise on average than the highest precision models for unweighted term selection. Unfortunately, the focus on one aspect of query meaning has unpredictable effects and some queries are significantly hurt by a high precision strategy.
There are two potential causes for negative results. First, PhRank may be picking a suboptimal concept. This does occur, particularly in the presence of polysemous or highly co-occurrent words in the query, or irrelevant documents in N . This is demonstrated with the high rank for `earth' in query #780 (Table 6). In the case of highly co-occurrent

590

Q: How much of planet Earth is arable at present? Area must have plenty of water, sun and soil to support plant life.

PhRank terms earth earth arable planet earth earth life earth water

SD terms

planet earth

water sun

earth arable

sun soil

arable present soil support

present area

support plant

area water

plant life

Table 6: TREC query #780: poor performance for PhRank compared to SD.

Term Length

1

2

3 45

6

7

KC

37% 40% 15% 6% 1% <1% <1%

PhRank 22% 54% 24%

Table 7: Percentage of PhRank and KC terms with various lengths.

words, these have a higher in-degree in G so they tend to accumulate weight during a random walk. A reduction in the number of iterations may help address this problem.
Irrelevant documents in G also hurt performance. The adequacy of an affinity graph G constructed using N is highly reliant on the quality of the initial query, the precision of the document similarity metric, and the adequacy of the collection being searched. If non-relevant documents occur in N there will be reduced connectivity in G, and this has an undesirable impact on the balance of word affinity scores. One solution to this problem may be to merge ranked lists computed by PhRank using different resources. The mistakes made by different instances of PhRank for the same query are likely to be less consistent than accurate assessments of term informativeness.
Second, more than one focus can occur, particularly in long queries. For example, there are two focal concepts of query #336: `A relevant document would discuss the frequency of vicious black bear attacks worldwide and the possible causes for this savage behavior '. The two core concepts are `black bear ' and `savage behavior ' but PhRank largely misses the importance of black bears. Instead, its top ranked terms for this query are {savage, savage behavior, bear savage, vicious savage, attacks savage}. This has a negative effect on IR effectiveness.
6.2.3 Succinctness
Results show that the performance of the top two PhRank terms in the same query structure as KC but with no term weighting performs comparably to KC with term weighting. The length of the terms is similar in both models, with around 75% of terms having a length of 1-2 words. This suggests that improved performance of unweighted PR-zF2 queries is more likely to be due to differences in the strategy for term selection than differences in term length. Note that KC shares the distributed approach to term selection with SD and sDist. KC selects two distinct concepts, whereas the top two terms selected by PhRank typically overlap.
More generally, it is observed that the succinct terms selected by PhRank are also novel. Table 8 shows that although PhRank and KC have the same number of 1-2 word terms overall, they display less than half of their potential overlap (we account for fewer terms in KC in this figure). Moreover, around 50% of PhRank terms contain two words, but only around half of them are also selected by SD.

PhRank (1-3 words) ROBUST04 WT10G GOV2

SD (2 words)
23% 28% 27%

sDist (3-6 words)
11% 15%

KC (1-7 words)
12% 18% 16%

Table 8: Percentage of PhRank terms selected by other models. Low figures show that PhRank is detecting novel terms with long-range dependencies.

Terms that are three words long dominate sDist (69% of all terms) yet less than half of the terms with three words in PhRank are also found in sDist queries. One likely explanation for these findings is that PhRank is not limited by syntactic or adjacency relations that are used in the other models. It detects distant word dependencies because repeat co-occurrences of word combinations reflect the associations in which they take part.
We hypothesize that distant associations may be present in queries because users condense information by relying on the ability of a search engine to infer links between words. The frequency of such textual economy was assessed in a sample of 100 queries randomly selected from Robust04 and GOV2. We assume that title queries capture a succinct information need and have informative associations between query words. By aligning description and title vocabulary, we discovered that 22% of description topics contain at least one informative word association that cannot be detected using any form of syntax or word adjacency, and a further 11% of topics contain at least one association that can only be detected using dependency relations.
7. CONCLUSION
We have presented PhRank, a novel term ranking algorithm that extends work on Markov chain frameworks for query expansion to select focused and succinct terms from within a query. PhRank captures query context with an affinity graph constructed using word co-occurrence in pseudorelevant documents. A random walk of the graph is used for term ranking in combination with discrimination weights.
We showed that PhRank focuses on a limited number of words associated with a core query concept. Overall, this is more effective for both description topics and title queries than a distributed approach to term selection, and can generate queries with up to 90% fewer terms. However, this term selection strategy is risky and less robust than competing methods. For all collections, around 26% of queries have more than 5% decrease in MAP compared to SD (significant change is around 3-6%).
The two main issues affecting robustness are the handling queries with multiple concepts, and variation in the quality of pseudo relevance feedback. The first issue may be addressed by a diversity constraint on top ranked terms that adjusts the number of selected terms permitted to include the highest scoring query word. Improved sensitivity of the ranking algorithm may also improve results. For example, the present implementation does not consider the degree of connection between two words in the affinity graph when scoring terms. A third approach might apply a non-linear interpolation of SD and PhRank that backs off to distributed terms where required. Adaptive methods for the selection of k can address challenges with the depth of coverage in a collection or occasions when evidence for multiple concepts is widely dispersed.

591

Finally, the high precision strategy of term selection might be combined with the more conservative and robust expansion terms generated with a Markov chain approach to query expansion. On this point, we note that although weighted variants of an affinity graph have been proposed before, our concrete suggestion for a vertex weight s based on word salience in pseudo-relevant documents improves the informativeness of affinity scores and may benefit other techniques that use a Markov chain framework.
More generally, the work in this paper may be applicable to lexical feature selection methods for other areas of IR, including text-based image and multimedia retrieval or matching of search advertisements. Efficiency considerations surrounding the time to construct an affinity graph may be ameliorated by off-line indexing to precompute a language model for each document in a collection.
8. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
References
[1] J. Allan, L. Ballesteros, J. P. Callan, W. B. Croft, and Z. Lu. Recent experiments with INQUERY. In Fourth Text REtrieval Conference (TREC-4). 1995.
[2] N. Balasubramanian, G. Kumaran, and V. R. Carvalho. Exploring reductions for long web queries. In Proc. of SIGIR 2010, pages 571­578, New York, NY, USA, 2010. ACM.
[3] M. Bendersky and W. B. Croft. Discovering key concepts in verbose queries. In Proc. of SIGIR 2008, pages 491­498, New York, NY, USA, 2008. ACM.
[4] M. Bendersky and W. B. Croft. Modeling higher-order term dependencies in information retrieval using query hypergraphs. In Proc. of SIGIR 2012, pages 941­950, New York, NY, USA, 2012. ACM.
[5] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In Proc. of WSDM 2010, pages 31­40, New York, NY, USA, 2010. ACM.
[6] K. Collins-Thompson and J. Callan. Query expansion using random walk models. In Proc. of CIKM 2005, pages 704­711, New York, NY, USA, 2005. ACM.
[7] F. Crestani. Application of spreading activation techniques in information retrieval. Artif. Intell. Rev., 11(6):453­482, Dec. 1997.
[8] W. B. Croft and D. Harper. Using probabilistic models of document retrieval without relevance information. Journal of Documentation, 35(4):285 ­ 295, 1979.
[9] W. B. Croft, H. R. Turtle, and D. D. Lewis. The use of phrases and structured queries in information retrieval. In Proc. of SIGIR 1991, pages 32­45, New York, NY, USA, 1991. ACM.
[10] R. Ferrer i Cancho and R. Sol´e. The small world of human language. In Proc. of the Royal Society of London B, volume 268, 2001.
[11] R. Ferrer i Cancho, R. V. Sol´e, and R. K¨ohler. Patterns in syntactic dependency networks. Physical Review E, 69(5), 2004.
[12] M. Hagen, M. Potthast, B. Stein, and C. Br¨autigam. Query segmentation revisited. In Proc. of WWW 2011, pages 97­106, New York, NY, USA, 2011. ACM.
[13] K. S. Hasan and V. Ng. Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art.

In Proc. of COLING 2010: posters, pages 365­373, Stroudsburg, PA, USA, 2010. ACL.
[14] Y. Huang, L. Sun, and J.-Y. Nie. Query model refinement using word graphs. In Proc. of CIKM 2010, pages 1453­1456, New York, NY, USA, 2010. ACM.
[15] S. Huston and W. B. Croft. Evaluating verbose query processing techniques. In Proc. of SIGIR 2010, pages 291­298, New York, NY, USA, 2010. ACM.
[16] G. Kumaran and V. R. Carvalho. Reducing long queries using query quality predictors. In Proc. of SIGIR 2009, pages 564­571, New York, NY, USA, 2009. ACM.
[17] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proc. of SIGIR 2001, pages 111­119, New York, NY, USA, 2001. ACM.
[18] M. Lease, J. Allan, and W. B. Croft. Regression rank: Learning to meet the opportunity of descriptive queries. In Proc. of ECIR 2009, pages 90­101, Berlin, Heidelberg, 2009. Springer-Verlag.
[19] C. Lioma and I. Ounis. A syntactically-based query reformulation technique for information retrieval. Inf. Process. Manage., 44:143­162, January 2008.
[20] C. Macdonald and I. Ounis. Global statistics in proximity weighting models. In Proceedings of Web N-gram 2010 Workshop at SIGIR, 2010.
[21] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, 2008.
[22] K. Maxwell. Term selection in information retrieval (forthcoming). PhD thesis, University of Edinburgh, 2013.
[23] Q. Mei, D. Zhang, and C. Zhai. A general optimization framework for smoothing language models on graph structures. In Proc. of SIGIR 2008, pages 611­618, New York, NY, USA, 2008. ACM.
[24] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proc. of SIGIR 2005, pages 472­479, New York, NY, USA, 2005. ACM.
[25] R. Mihalcea and P. Tarau. Textrank: Bringing order into texts. In Proc. of EMNLP 2004, pages 404­411, 2004.
[26] L. Page, S. Brin, R. Motwani, and T. Winograd. The PageRank citation ranking: Bringing order to the web. Technical report, Stanford University, Stanford, CA, 1999.
[27] J. H. Park and W. B. Croft. Query term ranking based on dependency parsing of verbose queries. In Proc. of SIGIR 2010, pages 829­830, New York, NY, USA, 2010. ACM.
[28] J. H. Park, W. B. Croft, and D. A. Smith. A quasi-synchronous dependence model for information retrieval. In Proc. of CIKM 2011, pages 17­26, New York, NY, USA, 2011. ACM.
[29] J. Rocchio. Relevance feedback in information retrieval. In The SMART Retrieval System - Experiments in Automatic Document Processing. Prentice Hall, 1971.
[30] K. Sparck Jones. A statistical interpretation of term specificity and its application in retrieval. J. of Documentation, 28(1):11­21, 1972.
[31] C. J. van Rijsbergen. Information retrieval: theory and practice. In Proceedings of the Joint IBM/University of Newcastle upon Tyne Seminar on Data Base Systems, pages 1­14, 1979.
[32] X. Wan and J. Xiao. Single document keyphrase extraction using neighborhood knowledge. In Proc. of AAAI 2008, volume 2, pages 855­860. AAAI Press, 2008.
[33] X. Xue, S. Huston, and W. B. Croft. Improving verbose queries using subset distribution. In Proc. of CIKM 2010, pages 1059­1068, New York, NY, USA, 2010. ACM.
[34] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In Proc. of CIKM 2001, pages 403­410, New York, NY, USA, 2001. ACM.

592

Sentiment Diversification with Different Biases

Elif Aktolga and James Allan
Center for Intelligent Information Retrieval School of Computer Science
University of Massachusetts Amherst Amherst, Massachusetts
{elif, allan}@cs.umass.edu

ABSTRACT
Prior search result diversification work focuses on achieving topical variety in a ranked list, typically equally across all aspects. In this paper, we diversify with sentiments according to an explicit bias. We want to allow users to switch the result perspective to better grasp the polarity of opinionated content, such as during a literature review. For this, we first infer the prior sentiment bias inherent in a controversial topic ­ the `Topic Sentiment'. Then, we utilize this information in 3 different ways to diversify results according to various sentiment biases: (1) Equal diversification to achieve a balanced and unbiased representation of all sentiments on the topic; (2) Diversification towards the Topic Sentiment, in which the actual sentiment bias in the topic is mirrored to emphasize the general perception of the topic; (3) Diversification against the Topic Sentiment, in which documents about the `minority' or outlying sentiment(s) are boosted and those with the popular sentiment are demoted.
Since sentiment classification is an essential tool for this task, we experiment by gradually degrading the accuracy of a perfect classifier down to 40%, and show which diversification approaches prove most stable in this setting. The results reveal that the proportionality-based methods and our SCSF model, considering sentiment strength and frequency in the diversified list, yield the highest gains. Further, in case the Topic Sentiment cannot be reliably estimated, we show how performance is affected by equal diversification when actually an emphasis either towards or against the Topic Sentiment is desired: in the former case, an average of 6.48% is lost across all evaluation measures, whereas in the latter case this is 16.23%, confirming that bias-specific sentiment diversification is crucial.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms: Experimentation, Algorithms, Measurement
Keywords: Diversity, Opinions, Sentiment, Proportionality
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1. INTRODUCTION
In previous work diversification has mainly been applied for better topical variety in search results [9, 25, 26, 27]. Equal preference is typically given to all aspects. How can opinionated content exhibiting sentiments be diversified? Initial approaches have been presented [10, 17, 18]; however these only consider equal diversification. In this paper, we view the problem from a high-level perspective to allow for sentiment diversification according to different biases, which will be vital for situations like a literature review on a controversial topic.
Consider the topic `global warming.' In a typical use case, a user engages in a comprehensive literature review with the aim of understanding the positions on this topic. This involves ­ besides searching and finding relevant opinionated documents [16] ­ understanding and mentally categorizing opinionated content. This can be done by organizing the discussed arguments by topical content; or, they can also be grouped by sentiment, such as positive, negative, neutral, and mixed [18]. We focus on facilitating the latter approach for the user. For some topics that can clearly be generalized into `pro' versus `con' arguments, this sentiment categorization is more natural, whereas it can be less obvious for topics like global warming that are associated with various arguments. Focusing on the sentiment dimension of these arguments, we can see that negative sentiments for global warming typically express criticism and concern about it and its effects on the environment. Those with positive sentiment often claim that worries about global climate change are unjustified ("there is no such issue"), playing down the concerns in a `calming' (i.e., positive) way. Mixed or neutral statements either express no sentiments or contain an equal amount of positive and negative arguments. Those could be "I don't care", or "It's a serious problem but we're handling it" kind of stances towards global warming.
Getting back to our use case: while a balanced and unbiased presentation of the results helps the user understand various viewpoints on a topic, discerning the topic's polarity is harder if minority opinions are `buried' in the results [18]. Therefore, the user should be able to switch the result perspective as needed. This way, she can either obtain a balanced or a biased view on majority or minority opinions, make her own comparisons across the representations, and perform this task in a more informed manner. Note that this is different from showing all positive or all negative or all neutral/mixed documents at a time: with such a representation the user would still need to draw her own conclusions about which sentiments form majority or minority

593

neutral + mixed

TOPIC T

negative

positive
Figure 1: Topic Sentiment: Dots represent relevant documents for this topic, which are grouped according to their sentiments. The obtained sentiment distribution is used for sentiment diversification.
opinions. Our aim is to analyze this information for the user and to match the inferred trend as closely as possible in the results. For this, we need to have a good grasp of the topic a priori: we consider a large pool of relevant documents about the topic, which are grouped by sentiments as visualized in Figure 1. Then, we can infer the topic's sentiment distribution or inherent bias ­ Topic Sentiment ­ from this analysis. We categorize the aspects `mixed' and `neutral' together to represent the `balanced' aspect, whereas `positive' and `negative' refer to arguments that are clearly biased towards one side only. If Figure 1 represented the Topic Sentiment for global warming, this could be interpreted as the issue being perceived with great concern since negative sentiments constitute the majority, and while there are some `balanced' positions on it, the positive sentiments form a clear minority. By utilizing this information during diversification, three biases are emphasized in search results: (1) Equal diversification by preferring all sentiments equally. This allows for a balanced representation of all sentiments on the topic; (2) Diversification towards the Topic Sentiment, in which the resulting reranked list mirrors the actual sentiment bias in a topic. This approach highlights the general perception of a topic; (3) Diversification against the Topic Sentiment, in which documents about the minority sentiment(s) are boosted whereas those with the majority sentiment are demoted. Such a list highlights unusual and outlying opinions on the topic.
In this paper we propose different diversification models for sentiment diversity with these 3 biases, and perform experiments using the TREC Blog Track data [23]. Since sentiment classification is an essential tool for this task, we experiment by gradually reducing the accuracy of a perfect classifier down to 40%, and show which diversification approaches prove most stable in this setting. Further, in case the Topic Sentiment cannot be reliably estimated, we show how performance is affected by equal diversification when actually an emphasis either towards or against the Topic Sentiment is desired. The results reveal that particularly when highlighting minority sentiments, diversifying with the corresponding bias yields significant improvements.
2. RELATED WORK
There is a large amount of work in the area of topical diversity: the main aim is to eliminate topical redundancy

in results while maximizing the number of documents containing novel information [1, 3, 6, 9, 25, 26, 27]. One of the earliest works is the maximal marginal relevance (MMR) approach [3], which employs content-based similarity measures to balance the tradeoff between novelty and relevance. More recently, researchers have shown explicit diversification approaches to be superior over implicit diversification techniques: well-known algorithms are xQuAD [25, 26, 27], IAselect [1], and more recently PM-1 and PM-2 [9]. Other approaches to topical diversity are language modeling based [31], probabilistic [4] and correlation-based [30]. Very recent research addresses personalized diversification [29], blog feed diversity [19, 28], and combined implicit and explicit aspect diversification [15].
Among these approaches, it is common to equally or uniformly diversify across all query aspects or subtopics due to the lack of data [9, 25, 26, 27]. Although the TREC Web Track diversity task provides topical query aspects [8], distributions over these aspects are not included. Agrawal et al. [1] use their own classifiers and judgments for obtaining query intent aspect distributions. Further, the NTCIR-9 Intent task provides non-uniform aspect probabilities [24]. One of our contributions in this paper is to present alternatives to the equal distribution approach (Section 3.4): a query's topic's sentiment distribution can be employed in various ways to yield an emphasis for a certain bias. Topical diversity could also benefit from these ideas.
The proportionality-based approaches PM-1 and PM-2 [9] distinguish themselves from prior research by explicitly matching the aspect distribution in the diversified list to the overall popularity of these aspects, thus yielding a proportionally diversified list. We adapt this approach to sentiment diversity, and propose a minor variation for dealing with retrieval limitations (Section 3.3).
Extensive work on opinion detection and retrieval has dealt with techniques to boost opinionated documents in retrieval [14, 28, 32, 33]. Prior work focusing on opinion diversity is very recent: Demartini and Siersdorfer describe a study about opinions in search results as given by popular search engines for controversial queries [11]. In later work, Demartini then tackles opinion diversification [10]: his approach is based on the xQuAD framework [25]. Retrieved search results are classified into the sentiment categories positive, negative, or objective. Diversification ensures maximum variety among these aspects with uniform preference. We implement this approach as the SCS model and combine it with the 3 biases (Section 3.2.1). The SCSF model is a further extension, presented in Section 3.2.2.
Kacimi and Gamper propose a different opinion diversification framework for controversial queries [17, 18]: three criteria are considered for diversification: topical relevance, semantic diversification, and sentiment diversification. Their model favors documents most different in sentiment direction and in the arguments they discuss. The sentiments are again one of positive, negative, and neutral. In the model the components are linearly combined; however, in order to find the documents maximizing the distances for all criteria the authors consider all subsets of documents. Our work differs from this work in several points: (1) We perform sentiment diversification only and not opinion diversification. Opinions refer to topical content, whereas sentiments are a non-topical aspect that we focus on in this paper. (2) This choice allows us to study sentiment diversification perfor-

594

mance with different biases, which has not been researched in prior work.
In this context, unlike topical diversity we make a simplifying assumption that each query belongs to one topic and therefore represents one topical aspect. We avoid dealing with ambiguity by using long and specific queries in our experiments, as explained in Section 4. That is, the topical dimension is kept static so we can focus on the varied sentiment dimension. We leave it to future work to explore the interplay of topical and sentiment aspects together for diversification.

3. SENTIMENT DIVERSIFICATION
3.1 Introduction
In order to diversify a retrieved list with respect to the distribution of sentiments in a query's topic, we need to introduce a few concepts first. Let Q be a query, and let T be the query's topic T (Q), abbreviated as T for simplicity. As visualized in Figure 1, we define T to include all the relevant documents that can be retrieved for Q, i.e., T = rel(Q). Further, let each document D in T have a sentiment, i.e., each document is positive, negative, neutral or mixed. These can be generalized to countable sentiment criteria   sent(T ). We will use this sentiment information from T in our models to diversify search results according to the distribution of sentiments in the topic.
Sentiment criteria of the form positive, negative, and neutral/mixed can take different shapes when converted into a sentiment score. In the literature [10, 17, 23] we identified a document to either have a single discrete sentiment from {-1, 0, 1}, or the sentiment is broken down into three scores positivity, negativity, and neutrality such that they sum to 1.0 for a single document. We refer to these latter ones as finer grained "fractional scores" in the rest of the paper. Our models are designed for these kinds of scores, but discrete scores can also be handled by simple conversion as we will show later.
Below we consider two different diversification frameworks and present several modifications to them.
3.2 Retrieval-Interpolated Diversification

Algorithm 1 Retrieval Interpolated Diversification Framework.

1 S=

2 while|S| <  and |R| > 0

3

do

4

D = arg maxDR RetC(Q) + (1 - )SentC(T )

5

R = R \ {D}

6

S = S  {D}

7 return S

Algorithm 1 shows the Retrieval-Interpolated Diversification Framework, which is similar to xQuAD, first introduced by Santos et al. [25] for topical diversity. In this diversification framework, documents retrieved in R are iteratively added to the new ranked list S. The  documents are chosen according to the maximization objective function in line 4:
D = argmaxDR  · RetC(Q) + (1 - ) · SentC(T ) (1)

where RetC(Q) is the retrieval contribution, which is always estimated with P (D|Q) ­ how likely D is to be relevant to Q by content, and SentC(T ) is the sentiment contribution, which we will define in two different ways below. The scores from these two components are interpolated for diversity estimation.
3.2.1 Sentiment Contribution by Strength (SCS)
In this version of the model we estimate the sentiment contribution in the maximization objective function (Equation 1) as follows:

SentC(T ) = P (D, S¯|T )

(2)

Here P (D, S¯|T ) measures how much D can contribute to the sentiment diversity of S. Structurally, this resembles xQuAD [25] with the difference that the estimation is conditioned on the query's topic T .
In order to make the model more flexible towards sentiment scores, we define each document to have a fractional score for each sentiment criterion   sent(T ). For example, a document may be classified as positive with 75% confidence. Then, this can be converted into a trinary score P (D| = positive) = 0.75, P (D| = neutral) = 0.25, and P (D| = negative) = 0. Fractional classification scores directly obtained from a classifier (such as logistic regression) fit in nicely into this framework. If documents are manually judged, they are often associated with only one `dominant' sentiment score from {-1, 0, 1} such as -1, which can be converted into a 100% negative score. Given this information, we can further decompose P (D, S¯|T ) as follows:

P (D, S¯|T ) =

P (D, S¯|) · P (|T )

(3)

sent(T )

ra=nk

P (D|) · P (S¯|) · P (|T ) (4)

sent(T )

where P (S¯|) denotes the likelihood of  not being satisfied by the documents already chosen into S (see below for further derivation) and P (|T ) stands for the importance of sentiment  to topic T . This is discussed in detail in Section 3.4. From Equation 3 to Equation 4 we make the same independence assumption as Santos et al. [25]: the diversity estimation of D with respect to the sentiments  can be made independently of the documents already selected into S. We continue with Equation 4:

P (D|) · P (S¯|) · P (|T )
sent(T )

=

P (D|) · P (|T ) ·

P (Dj |)

sent(T )

Dj S

=

P (D|) · P (|T ) ·

1 - P (Dj |) (5)

sent(T )

Dj S

Here we make another independence assumption for P (Dj|) as Santos et al. [25]: the likelihood of not sampling Dj's sentiment from T is independent of the sentiments of the other documents in S. Since each Dj was independently chosen into S, this is a reasonable assumption.
To summarize, Equation 5 estimates the diversity of a document D by considering how well D represents each sentiment criterion, which is weighted by how important that

595

Algorithm 2 Diversity by Proportionality (PM-2).

1 S=

2  s = 0

3 while|S| <  and |R| > 0

4

do

5

for   sent(T )

6

do

7 8



=

quotient[]

=

v 2s +1

arg max quotient[]

9

D = arg maxDR  · quotient[] · P (D|) + (1 - )

10

R = R \ {D}

11

S = S  {D}

12

for   sent(T )

13 14 15 return S

do s = s +

P (D |) sent(T ) P (D |)

= quotient[] · P (D|)

sentiment criterion is to T . This whole part is demoted according to how many documents of the same sentiment S already contains.
3.2.2 Sentiment Contribution by Strength and Frequency (SCSF)
We consider an alternative formulation of the sentiment contribution component above in Equation 1 in which the punish/reward factor is estimated slightly differently:

SentC(T ) = P (D|T ) · (1 - P (S|T ))

(6)

where P (D|T ) stands for how important D's sentiment is for T , and 1 - P (S|T ) describes how well the sentiments from T are already represented in S. We further derive:

P (D|T ) · (1 - P (S|T )) = P (D|T ) - P (D|T ) · P (S|T )

=

P (D|) · P (|T ) - P (D|) · P (|T ) · P (S|) (7)

sent(T )

Here we apply the Bayes' Rule to P (S|):

P (S|) = P (|S) · P (S) ra=nk P (|S)

(8)

P ()

which is rank-equivalent since P (S) is a constant across all documents in an iteration, and P (), the prior probability of a particular sentiment, is equal across all sentiments. Hence we obtain from Equation 7:

P (D|) · P (|T ) - P (D|) · P (|T ) · P (|S)
sent(T )

=

P (D|) · P (|T ) · (1 - P (|S))

sent(T )

=

P (D|) · P (|T ) · P (¯|S)

(9)

sent(T )

Now we can see that the first part of Equation 9 is identical to Equation 5. We can estimate the components P (D|) · P (|T ) the same way as described in Sections 3.2 and 3.4. However, P (¯|S), the likelihood of S not having sentiment , is new. We define its complement as follows:

P (|S) = sent(, S)

(10)

|S|

which is the number of documents in S having dominant sentiment . Each document in S can be mapped into its dominant or most confident sentiment class   sent(T ), typically positive, negative, or neutral/mixed. Given this, we count the number of times a particular sentiment  occurs in S as sent(, S). We set P (|S) = 0 if S =  to avoid zero division in the first iteration.
To summarize, this formulation calculates the punish/ reward factor directly from the frequency of documents present in the whole set S with certain sentiments. Contrarily, in the SCS model the strength of sentiments of each document in S is considered individually, whereas the frequency of such documents is implicit in the multiplication over all documents in S. In the experiments we empirically verify the effectiveness of the two models in sentiment diversification to draw conclusions about their usefulness.
3.3 Diversity by Proportionality
As a second diversification framework we consider Algorithm 2, the best-performing approach in Dang and Croft's work [9]. This framework is based on the Sante-Lagu¨e method for seat allocation and is adapted here to sentiment diversification. In each iteration documents are chosen based on the proportionality of the diversified list. We only describe modified components due to space limitations. Instead of applying the algorithm to topical aspects, here it is employed together with sentiments   sent(T ). Further, P (D|) is estimated by means of fractional sentiment scores as defined in Section 3.2 instead of estimating the relevance of the document with respect to a (sub)topical aspect. Note that under this modification, a document is purely evaluated on the basis of its sentiments and not according to topical relevance.
The variables v and s in the quotient are important. The former is the number of relevant documents the sentiment  should have, whereas the latter represents the estimated number of documents actually present in the list for . v at a particular rank i can easily be inferred from P (|T ) as follows:

v = i · P (|T ) + 0.5

(11)

According to PM-2, s is updated with the fractional sentiment scores of the chosen document, since each sentiment takes up a `portion' of the seats in S. This denotes how well the chosen document represents each sentiment. For

596

the relationship between document sentiments and the topic sentiment distribution, please refer to Section 3.4.
3.3.1 Diversity by Proportionality with Minimum Available Votes (PM-2M)
Unlike the seat allocation problem in a voting system, in a retrieved list of documents there is an additional constraining factor. The top K documents retrieved from a search system constitute the source for diversification, so it is possible that a particular sentiment is underrepresented in this list. Unless the system requests more documents, the desired proportionality in the diversified list may not be optimally achieved with the current set of documents. In this situation, with respect to PM-2 the given votes v overestimate l, the actual number of documents with sentiment  in the retrieved top K set. For a large enough rank K, this may result in a suboptimally diversified list where documents with an over-emphasized sentiment are exploited early in the ranks. Therefore, we propose a small change to the quotient defined in Algorithm 2:

quotient[] = min(v, l)

(12)

2s + 1

which ensures that the quotient does not over-emphasize the importance of a sentiment if data is missing in the retrieved list. This technique has a remote resemblance to disproportionate stratified sampling in that documents are chosen slightly differently than dictated by the topic sentiment distribution in favor of improved overall diversity. We refer to this modified diversification approach as PM-2M and compare its effectiveness to PM-2, SCS and SCSF in the experimental section 4.

3.4 Favoring Different Biases in Search Results
In the presentation of the diversification models above P (|T ) plays a central role in defining which sentiment bias is favored in search results. Intuitively, this component stands for the importance of sentiment  to topic T . Below we present three different possible biases in search results that the estimation of P (|T ) impacts.

3.4.1 Equal Sentiment Diversification (BAL)
This is our baseline approach, which does not give preference to any sentiment, but weights them equally or uniformly. Therefore, this approach does not utilize information from the query's topic about its prior sentiment distribution. We set

P (|T ) = 1

(13)

|sent(T )|

which results in each sentiment criterion   sent(T ) to be considered equally important. We refer to this bias method as `Balance' (BAL) in Section 4.
We assume that with this balanced estimation the SCS model is equivalent to Demartini's [10] approach. Since this detail is not explicitly described in their work, it is most reasonable to assume an equal bias as in prior research.

3.4.2 Diversifying Towards the Topic Sentiment (CRD)
In this approach we choose to diversify the retrieved list towards the distribution of sentiments in the query's topic.

Such results strongly represent the crowd's opinion(s). For this, we need information about the sentiments in T . Recall from Section 3.1 that T is defined as a topic space to include all the relevant documents that can be retrieved for the query Q, i.e., T = rel(Q). Then, we can map each relevant document into its dominant or most confident sentiment class   sent(T ). Given this, we count the number of times a particular sentiment  occurs in T as sent(, T ). This allows us to interpret P (|T ) as the likelihood of sentiment  being drawn from T :

P (|T ) = sent(, T )

(14)

|T |

which represents the fraction of documents in T with dominant sentiment ; for instance the fraction of positive documents in T . We name this bias as `Crowd' (short: CRD).

3.4.3 Diversifying Against the Topic Sentiment (OTL)
What if a user is interested in viewing minority sentiments on the topic? For favoring outlying sentiments, we need to diversify the search results against the Topic Sentiment. For this, we introduce one minor modification to CRD above: Let the n sentiment estimations for   sent(T ) be sorted in increasing order of P (|T ). Then, for each  at rank i we swap its estimation P (|T ) with the one at rank n - i. This `reverses' the values in the topic distribution without changing the properties of the distribution. Consequently, if originally in T positive documents are strongly favored and negative documents are least favored, this trend is reversed through the value swap in T so that outlying sentiments (negative documents) will be strongly preferred during diversification. We refer to this bias as `Outlier' (OTL) in the experiments (Section 4).
Irrespective of the preferred bias, we apply Add-1 Smoothing [5] to P (|T ) estimates to account for zero probabilities. In order to correct such unrealistic estimations, an unobserved sentiment class is assigned a very small probability, and the estimations for the other sentiment classes are adjusted accordingly.

4. EXPERIMENTS
4.1 Setup
Retrieval Corpus As retrieval corpus we use the TREC Blog Track data from 2006 and 2008 [23] for all our experiments. For preparation, the DiffPost algorithm is applied to the corpus for better retrieval as shown in prior work [20, 22]. Further, we perform stop word removal and Porter stemming.

Queries and Retrieval Model We split the 150 TREC Blog Track 2008 queries into 3 non-overlapping randomly chosen sets of size 50 each in order not to bias training or testing towards a specific year: split 1 is used for training and tuning parameters; the results in this paper are reported on split 2, and split 3 is reserved for sentiment classifier training. For our diversification experiments, we use a strong retrieval baseline: the queries' stopped title and description texts are combined for use with the Sequential Dependence Model in Lemur/Indri [21], smoothed using Dirichlet (µ = 10, 000). All diversification models are applied to the top K = 50 retrieved documents as determined

597

during training. The retrieval scores are normalized to yield document likelihood scores.
Sentiment Classification The sentiment classifier is trained as a logistic regression model using Liblinear [13] with default settings. For this, we utilize the judged documents from the 50 split 3 TREC Blog Track queries. Training is done for three classes ­ positive, negative, and neutral to obtain probability estimates that are employed as fractional scores for sentiment estimation (Section 3.2.1). As features we extract Sentiwordnet 3.0 terms with their lengthnormalized term frequencies in the documents [12].
Topic Sentiment Estimation Given a query, the topic sentiment distribution can be estimated in various ways: (1) in the form of opinion relevance judgments for a pool of documents where all judged relevant documents are included in the distribution; (2) by retrieving the top M documents from a separate corpus or web search engine and tagging them with sentiment judgments. We experimented with both approaches but only present the results for (1) here due to space limitations: we use the relevance judgments from the TREC 2008 Blog Track [23], which are divided into the same sentiment aspects as required in the models.

where v is the number of relevant documents the sentiment  should have, s is the number of relevant documents actually found for , nNR is the number of documents that are non-relevant (to any sentiment), and c = 1 if v  s, 0 otherwise. This measure allows us to assess how proportional the diversified list is with respect to the desired topic distribution. v can be inferred from the true topic sentiment distribution P (|T ) in the same way as detailed in Equation 11. As noted by Dang and Croft [9], CPR penalizes the under-representation of aspects (here: sentiments) and the over-representation of non-relevant documents.
4.3 Results
In this section we discuss the results of the retrieval baseline SDM and all the proposed diversification models in Section 3, SCS, SCSF, PM-2 and PM-2M, with the three biases, Crowd (CRD), Balance (BAL) and Outlier (OTL). The interpolation parameter   {0.0, ..., 1.0} is tuned in 0.1 steps separately for each model and bias on our training split. The results are presented with fixed parameters K and  on test split 2, and the evaluation is performed with the TREC 2008 Blog Track judgments at rank 20. -NDCG, ERR-IA, and NRBP require parameters, which are set to  = 0.5 and  = 0.5.

4.2 Evaluation Measures
The sentiment diversification approaches are evaluated using standard evaluation measures that were designed for topical diversity: Precision-IA [1], s-recall [31], -NDCG [6], ERR-IA [2], and NRBP [7]. The former two measures are set-based, whereas the remaining ones are cascade measures as described by Ashkan and Clarke [2], punishing redundancy through parameters  (-NDCG, ERR-IA, NRBP) and additionally  (NRBP), which represents user patience. In order to measure sentiment diversity with a chosen bias, we implement all the measures in their intent-aware (or for us, `sentiment-aware') version [1, 2]. Hence, the weighted average over the sentiment-dependent scores of a measure is computed as given by measure-IA for a query Q and topic T:

measure-IA(Q, T ) =

P (|T ) · measure(Q|) (15)

sent(T )

where P (|T ) defines the weight for the sentiment-specific result yielded by measure(Q|).
Intent-aware measures can be rank-specific such as PrecisionIA@k or -NDCG@k for example, or rank-independent as NRBP. We utilize another rank-specific measure defined by Dang and Croft [9], Cumulative Proportionality (CPR) at rank K:

1K

CP R@K =

P R@i

(16)

K

i=1

in which P R@i is computed as the inverse normalized disproportionality at rank i (see [9] for details). Here, we define the disproportionality at rank i as follows:

DP @i =

c (v

-

s )2

+

1 2

n2N

R

sent(T )

(17)

4.3.1 Straight-Bias Experiments
Our primary aim in the experiments is to evaluate sentiment diversification performance. Sentiment classification is an important part of the system since both the to-bediversified documents need to be tagged with sentiments, as well as those for the topic sentiment distribution estimation. Since a `full evaluation' of sentiment diversification techniques on a publicly available dataset has not been done yet in prior work, it is important to understand how sentiment classification quality affects diversification performance. Therefore, we start with a "perfect system" in which classification accuracy is 100% for judged documents. For unjudged documents the trained sentiment classifier described in Section 4.1 is applied. We then gradually reduce the overall classification performance in 10% steps until 40% as follows: given the top K = 50 retrieved documents for a query, before diversification we randomly sample the ranks at which the true classification label is switched to another label randomly to achieve the desired classification error for each query.
Figure 2 shows the results for the straight-bias experiment, in which the topic sentiment distribution employed in experiment and evaluation underlies the same favored bias. For instance, the left-most column in Figure 2 shows the results for diversifying towards Crowd in the experiments, and measuring performance for Crowd in the evaluation (shortCRD-CRD). The middle column shows the same for Balance (short: BAL-BAL), and the right-most column is for the Outlier bias (short: OTL-OTL).
At the top-most row in the Precision-IA@20 graphs we observe a big gap between the SDM baseline and SCS model versus the rest of the models. For Crowd, the SCSF model only dominates when classification accuracy is at least 60% while it achieves the best (however not statistically significant) numbers in the Outlier graph. PM-2M and PM-2 also perform well and dominate some of the lower accuracy ranges. Statistical significance with the paired two-sided ttest (p-value< 0.05) is indicated in the graphs with circles:

598

PrecisionIA@20, CRD

s-recall@20, CRD

alpha-NDCG@20, CRD

0.35 0.345
0.34

SDM SCS SCSF PM-2 PM-2M

0.335

0.33

0.325

0.32

0.315

0.31

0.305 40

50

60

70

80

90

100

sentiment classifier accuracy

0.86 SDM

0.84

SCS SCSF

0.82

PM-2 PM-2M

0.8

0.78

0.76

0.74

0.72

0.7

0.68 40

50

60

70

80

90

100

sentiment classifier accuracy

0.68 SDM

0.67

SCS SCSF

0.66

PM-2 PM-2M

0.65

0.64

0.63

0.62

0.61

0.6

0.59 40

50

60

70

80

90

100

sentiment classifier accuracy

0.59 SDM

0.58

SCS SCSF

0.57

PM-2 PM-2M

0.56

0.55

0.54

0.53

0.52

0.51

0.5 40

50

60

70

80

90

100

sentiment classifier accuracy

0.52 SDM

0.51

SCS SCSF

0.5

PM-2 PM-2M

0.49

0.48

0.47

0.46

0.45

0.44

0.43 40

50

60

70

80

90

100

sentiment classifier accuracy

0.82 SDM

0.81

SCS SCSF

0.8

PM-2 PM-2M

0.79

0.78

0.77

0.76

0.75

0.74

0.73

0.72 40

50

60

70

80

90

100

sentiment classifier accuracy

CPR@20, BAL

NRBP@20, BAL

ERR@20, BAL

alpha-NDCG@20, BAL

s-recall@20, BAL

PrecisionIA@20, BAL

0.235 0.23

SDM SCS SCSF PM-2 PM-2M

0.225

0.22

0.215

0.21

0.205 40

50

60

70

80

90

100

sentiment classifier accuracy

0.86 SDM

0.85

SCS SCSF

0.84

PM-2 PM-2M

0.83

0.82

0.81

0.8

0.79

0.78

0.77

0.76 40

50

60

70

80

90

100

sentiment classifier accuracy

0.56 SDM

SCS

0.54

SCSF PM-2

PM-2M

0.52

0.5

0.48

0.46

0.44

0.42 40

50

60

70

80

90

100

sentiment classifier accuracy

0.44 SDM

0.43

SCS SCSF

0.42

PM-2 PM-2M

0.41

0.4

0.39

0.38

0.37

0.36

0.35

0.34 40

50

60

70

80

90

100

sentiment classifier accuracy

0.36 SDM

0.35

SCS SCSF

PM-2

0.34 PM-2M

0.33

0.32

0.31

0.3

0.29

0.28 40

50

60

70

80

90

100

sentiment classifier accuracy

0.8 SDM

SCS

0.78

SCSF PM-2

PM-2M

0.76

0.74

0.72

0.7

0.68

0.66 40

50

60

70

80

90

100

sentiment classifier accuracy

CPR@20, OTL

NRBP@20, OTL

ERR@20, OTL

alpha-NDCG@20, OTL

s-recall@20, OTL

PrecisionIA@20, OTL

0.19 SDM

0.18

SCS SCSF

PM-2

0.17 PM-2M

0.16

0.15

0.14

0.13

0.12

0.11 40

50

60

70

80

90

100

sentiment classifier accuracy

0.86 SDM

0.85

SCS SCSF

0.84

PM-2 PM-2M

0.83

0.82

0.81

0.8

0.79

0.78

0.77

0.76 40

50

60

70

80

90

100

sentiment classifier accuracy

0.55 SDM

SCS

0.5

SCSF PM-2

PM-2M

0.45

0.4

0.35

0.3

0.25 40

50

60

70

80

90

100

sentiment classifier accuracy

0.5 SDM

SCS

0.45

SCSF PM-2

PM-2M

0.4

0.35

0.3

0.25

0.2 40

50

60

70

80

90

100

sentiment classifier accuracy

0.4 SDM

SCS

SCSF

0.35

PM-2 PM-2M

0.3

0.25

0.2

0.15 40

50

60

70

80

90

100

sentiment classifier accuracy

0.66 SDM

0.64

SCS SCSF

PM-2

0.62 PM-2M

0.6

0.58

0.56

0.54

0.52

0.5 40

50

60

70

80

90

100

sentiment classifier accuracy

ERR@20, CRD

NRBP@20, CRD

CPR@20, CRD

Figure 2: Straight-Bias Experiment over test split varying sentiment classifier accuracies on the x-axis and each one measure and bias on the y-axis. The leftmost column is for the Crowd bias (CRD), the middle one for Balance (BAL), and the rightmost one for Outlier (OTL). Dark purple dotted circled points indicate statistical significance over the SDM baseline with p-value < 0.05 using the paired two-sided t-test, whereas light blue circled points indicate the same over the SCS and SDM models.

599

Measure Exp-Eval SDM baseline SCS SCSF PM-2 PM-2M

Precision-IA@20

BAL-CRD
0.312 0.308 0.298 0.302 0.298

CRD-CRD
0.312 0.309 0.348 0.341 0.341

-NDCG@20

BAL-CRD
0.593 0.642 0.648 0.642 0.639

CRD-CRD
0.593 0.650 0.647 0.674 0.674

ERR-IA@20

BAL-CRD
0.501 0.532 0.533 0.526 0.521

CRD-CRD
0.501 0.543 0.545 0.570 0.570

NRBP

BAL-CRD
0.440 0.453 0.456 0.446 0.440

CRD-CRD
0.440 0.471 0.477 0.504 0.504

CPR@20

BAL-CRD
0.731 0.750 0.774 0.772 0.767

CRD-CRD
0.731 0.755 0.801 0.813 0.813

Table 1: Cross-Bias Experiment over test split with perfect sentiment classifier to compare performance loss when diversifying equally (BAL-CRD) if actually diversification for the Crowd bias is desired (CRD-CRD). Bold entries in CRD-CRD columns are statistically significant over corresponding entries in BAL-CRD with p-value < 0.004 using the paired two-sided t-test.

the lighter blue circles refer to the result being significant over the SCS and SDM models, whereas the darker dotted circles indicate significance over the SDM model only. In the Precision-IA@20 graphs the results for SCSF and the proportionality-based methods are significant over SCS and SDM even for lower accuracies. We conclude that if precision is important, the SCSF diversification model should be used.
Among the s-recall@20 graphs the one for Crowd is the most arbitrary one. Performance drops well below the baseline for the SCSF and proportionality-based methods with medium quality classification: this indicates that the majority sentiment(s) are being emphasized too strongly, whereas minority sentiments appear much later in the ranked list for the first time, which is when the subtopic-recall measure is affected. This is expected, since we explicitly diversify in favor of majority sentiments. In the Balance and Outlier graphs for s-recall@20 there is no such trend, however precision is not as high for those biases as it is for Crowd. This is a typical precision versus recall tradeoff observation.
The next row shows results for -NDCG@20, followed by ERR-IA@20 and NRBP: we note that the trends in these graphs look very similar, although the ranges of the values differ greatly. It is interesting to observe that the peak performance for the proportionality-based methods for the Crowd bias is not at 100% classification accuracy, but at 90%. What these three measures have in common is punishing redundancy based on the rank and sentiment criterion in addition to non-relevance. Since usually there are many documents with the majority sentiment in the retrieved list to start with, a strong emphasis on a single sentiment criterion results in more redundancy. With the 10% error in classification documents with other sentiments are slightly boosted, yielding better overall varied ranking. In the Balance and Outlier graphs this trend cannot be observed, since the Balance bias does not strongly emphasize a single sentiment criterion to begin with. Concerning the Outlier bias, there are fewer documents with minority sentiments in the retrieved list to cause the same `clustered' ranking effect as for Crowd. Summarizing the trends across the -NDCG@20, ERR-IA@20, and NRBP graphs we make the following conclusion: if ranking is important, the PM-2 and PM-2M methods should be chosen.
Finally, we look at the last row of graphs with the CPR@20 results: this measure evaluates how proportional the overall list is with respect to the chosen bias. PM-2 and PM-2M achieve the best results, which is closely followed by SCSF. PM-2 and SCSF are more appropriate for lower classification accuracies ( 70%), whereas PM-2M performs slightly better with better classification quality.
Looking at the fixed values of the interpolation parameter  during training for this experiment, the following insights

can be drawn: for the SCS model, across all classifier accuracies and biases generally   0.6 values are preferred. So this model performs best with a weaker emphasis on diversity, which pulls it closer to the SDM baseline as observed in the graphs of Figure 2. SCSF on the other hand has a good mixture of higher and lower  values across classifier accuracies and biases, with many of them being < 0.5, particularly when the classifier is more accurate. So a heavier emphasis on the diversification part helps this model. The distinguishing feature between SCS and SCSF is the consideration of sentiment frequencies in addition to sentiment strength contributions. When the classifier is noisy however (< 60%) and thus sentiment frequency counts are not accurate, SCSF also benefits from higher  values. In the PM-2 and PM-2M models the role of  is different: it balances the emphasis on the chosen aspect  versus all the other aspects   sent(T ),  = . Here, consistently higher  values are preferred for both models, i.e., a high emphasis on the chosen aspect and a minimal weight on the other ones seems most beneficial. The effectiveness of these two models solely relies on sentiment estimations: given our adaption of PM-2 from its original definition ([9]) to sentiment diversity, the retrieval scores are not used for building the diversified list.
4.3.2 Cross-Bias Experiments
Consider the following real-world setting: for certain topics, it may not be feasible to collect data for calculating Topic Sentiment estimations, or suitable corpora may currently not be available. This could happen if the topic is very new and the data is not substantial enough for drawing general conclusions. If judgments shall be obtained, the data tagging effort may also be a burden. In such a situation we can fall back to the Balance bias or equal diversification approach [9, 25, 26, 27]. Naturally, the next question to answer is how much performance is lost when diversifying with Balance instead of the desired bias such as Outlier. The cross-bias experiments in this section investigate this case, and enable us to draw conclusions about the value of collecting and using information about topic sentiment distributions for controversial topics.
We analyze two cases. The first, presented in Table 1 shows the results for equally diversifying for Balance, but performance is measured for the Crowd bias (BAL-CRD). This is contrasted with diversifying for the Crowd bias, and evaluating for the same (CRD-CRD). Bold entries in CRDCRD indicate statistical significance over BAL-CRD with a p-value of < 0.004 (t-test, as before). The SDM baseline is included for comparison. We omit s-recall@20 due to space limitations. All CRD-CRD results for the proportionalitybased methods are significant over BAL-CRD results, whereas for the SCSF and SCS models there are a few exceptions.

600

Measure Exp-Eval SDM baseline SCS SCSF PM-2 PM-2M

Precision-IA@20

BAL-OTL
0.120 0.126 0.164 0.166 0.166

OTL-OTL
0.120 0.126 0.188 0.184 0.184

-NDCG@20

BAL-OTL
0.277 0.413 0.433 0.447 0.446

OTL-OTL
0.277 0.436 0.462 0.540 0.540

ERR-IA@20

BAL-OTL
0.202 0.309 0.320 0.337 0.336

OTL-OTL
0.202 0.338 0.358 0.465 0.465

NRBP

BAL-OTL
0.155 0.237 0.243 0.262 0.261

OTL-OTL
0.155 0.268 0.287 0.388 0.388

CPR@20

BAL-OTL
0.501 0.562 0.624 0.632 0.634

OTL-OTL
0.501 0.567 0.632 0.651 0.651

Table 2: Cross-Bias Experiment over test split with perfect sentiment classifier to compare performance loss when diversifying equally (BAL-OTL) if actually diversification for the Outlier bias is desired (OTL-OTL). Bold entries in OTL-OTL columns are statistically significant over corresponding entries in BAL-OTL with p-value < 0.05 using the paired two-sided t-test.

Rank 1 2 3 4 5 6 7 8 9 10
Rank 1 2 3 4 5 6 7 8 9 10

SDM baseline Excerpt The Religious Policeman: Mutt the Muttawa Happy Feminist: PROTESTING GENDER... Between tradition and demands for change Saudi mobile carriers ban SMS voting... Saudi Arabia, Ever Our Friends And Allies Orientalism and Islamophobia Laws discriminate against women... ...who urged SA to improve women's rights... Being a Child in Saudi Arabia Depressing Post: ...woman filed a case against...
SCSF Excerpt The Religious Policeman: Mutt the Muttawa Happy Feminist: PROTESTING GENDER... Saudi Arabia, Ever Our Friends And Allies Orientalism and Islamophobia First women to win in Saudi elections Laws discriminate against women... Depressing Post: ...woman filed a case against... Their shabby treatment of women... Oprah is being smuggled into Saudi Arabia... Between tradition and demands for change

Sent. o o o o o -
Sent. o o + o

SCS Excerpt The Religious Policeman: Mutt the Muttawa Happy Feminist: PROTESTING GENDER... First women to win in Saudi elections Between tradition and demands for change Saudi mobile carriers ban SMS voting... Saudi Arabia, Ever Our Friends And Allies Orientalism and Islamophobia Laws discriminate against women... ...who urged SA to improve women's rights... Being a Child in Saudi Arabia
PM-2 Excerpt The Religious Policeman: Mutt the Muttawa Saudi Arabia, Ever Our Friends And Allies First women to win in Saudi elections Happy Feminist: PROTESTING GENDER... Orientalism and Islamophobia Laws discriminate against women... Depressing Post: ...woman filed a case against... Their shabby treatment of women... Thumbs up for the Saudi ladies. Between tradition and demands for change

Sent. o + o o o o
Sent. + o o + o

Table 3: Crowd Bias: Top 10 results with 4 models for query number 1007, `women in Saudi Arabia.' - denotes a negative document, o refers to mixed/neutral, and + to positive.

We observe a maximum loss of 16.92% for Precision-IA@20 with SCSF, and an average loss of 6.48% across all measures and diversification approaches.
The second case is presented in Table 2: we observe the results for equally diversifying for Balance, but performance is measured for the Outlier bias (BAL-OTL). This is contrasted with diversifying for the Outlier bias, and evaluating for the same (OTL-OTL). Similar to Table 1 the results are statistically significant for OTL-OTL over BAL-OTL, but the losses with equal diversification are more heavily pronounced here: there is a maximum loss of 48.79% for NRBP with PM-2M, and an average loss of 16.23% across all measures and diversification approaches. So for highlighting minority sentiments through diversification it is even more important to collect biased data about topic sentiment distributions than it is for emphasizing majority sentiments as observed in Table 1. This way diversification can be performed with the intended bias rather than with equal diversification, which yields significantly worse results.
We presented the cross-bias experiments with perfect sentiment classification to reveal the maximum performance loss. As classification accuracy degrades, the losses become smaller but remain noticeable.
4.3.3 Analysis with Specific Queries
To see the models in action, we look at the output for one query in Table 3, number 1007 from the TREC Blog Track: `women in Saudi Arabia', asking for opinions about the treatment of women in Saudi Arabia. We show titles

or characteristic excerpts from the documents together with their overall sentiment. The topic of this query has the following Topic Sentiment: 67% negative, 17% mixed/neutral, and 16% positive. Here we diversify for the Crowd Bias, so the aim is to mirror this distribution in the results. The top 10 retrieved results with the SDM baseline are presented at the top left: this result list does not include any positive documents, and an equal amount of negative and mixed/neutral documents, which is clearly unsatisfactory for a Crowd bias representation of the results. The SCS model includes one positive document at rank 3, since lower ranked documents through the SDM baseline can be pulled up by the diversification models. Although the documents are nicely shuffled around across ranks, the ratio of the sentiments is still not close to the Topic Sentiment. The SCSF model is able to correct this, explicitly considering the frequency of documents with their dominant sentiments: we have 6 negative documents, 3 mixed/neutral, and 1 positive. But 4 negative documents are clustered right after each other, which slightly affects measures such as -NDCG@10. The PM-2 results (bottom right) use the overall proportionality of the sentiments in the list as a guidance for choosing further documents: here, a second positive document is pulled up from lower ranks, yielding the best CPR@10 score among the 4 models for this query at a cost of slightly lower Precision-IA@10 than SCSF. With 5 negative documents, 3 mixed/neutral ones, and 2 positive documents we are very close to the desired distribution of sentiments.

601

5. CONCLUSIONS & FUTURE WORK
In this paper we demonstrate how to diversify search results according to sentiments by considering a pre-defined bias. This allows us to emphasize either majority or minority sentiments during diversification, or to give an unbiased representation across all sentiment aspects. For this, we introduce several diversification models that use sentiments and topic sentiment distributions. Diversifying the output of a strong retrieval baseline, the results on the TREC Blog Track data reveal that the proportionality-based methods and the SCSF model perform best according to most measures, but an individual choice should be made based on the quality of the sentiment classifier at hand. Finally, we demonstrate the value of using biases and collecting topic sentiment distribution estimations by means of cross-bias experiments in which equal diversification is performed instead of the desired bias.
The ideas presented in this paper are not only valuable for sentiment diversity, but can also be applied to topical diversity with modifications. To what extent does it make sense to consider biases for topical diversity? For instance, with an Outlier bias-like approach underrepresented query aspects could be highlighted in search results. Further, we have proposed different extensions to existing diversification models such as xQuAD and PM-2 with the SCSF and PM2M models, which may be effective for topical diversity as well.
There are many directions for future work: (1) Exploring other biases applicable for sentiment diversity; (2) We found that our trained 3-class sentiment classifier and ready-to-use classifiers on the web perform rather poorly at documentlevel sentiment classification. State-of-the-art sentiment classification works better on sentences or short text, but interpreting the overall sentiment of a document is more difficult, particularly on the web. Therefore, advances in this area would greatly benefit sentiment diversification so that it can be applied to the web beyond the TREC Blog Track; (3) In case this is difficult to realize, how can the diversification models be adapted to yield higher gains with noisy classification input; (3) Analyzing opinion or topical arguments and sentiments together with biases. One question to solve is what kind of biases could be defined to capture both, and whether more fine-grained topic-specific biases would be required.
6. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval, in part under subcontract #19000208 from SRI International, prime contractor to DARPA contract #HR0011-12-C-0016, and in part by NSF grant #IIS-11217281. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
7. REFERENCES [1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proc. of WSDM, 2009. [2] A. Ashkan and C. L. Clarke. On the informativeness of cascade and intent-aware effectiveness measures. In Proc. of WWW, 2011. [3] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR, 1998. [4] H. Chen and D. R. Karger. Less is more: probabilistic models for retrieving fewer relevant documents. In Proc. of SIGIR, 2006.

[5] S. F. Chen and J. Goodman. An Empirical Study of Smoothing Techniques for Language Modeling. In Proc. of ACL, 1996.
[6] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proc. of SIGIR, 2008.
[7] C. L. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. In Proc. of ICTIR, 2009.
[8] C. L. A. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 Web Track. In Proc. of TREC-2009, 2009.
[9] V. Dang and W. B. Croft. Diversity by proportionality: an election-based approach to search result diversification. In Proc. of SIGIR, 2012.
[10] G. Demartini. Ares: a retrieval engine based on sentiments sentiment-based search result annotation and diversification. In Proc. of ECIR, 2011.
[11] G. Demartini and S. Siersdorfer. Dear search engine: what's your opinion about...?: sentiment analysis for semantic enrichment of web search results. In Proc. of SEMSEARCH, 2010.
[12] A. Esuli and F. Sebastiani. Sentiwordnet: A publicly available lexical resource for opinion mining. In In Proc. of LREC, pages 417­422, 2006.
[13] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871­1874, 2008.
[14] B. He, C. Macdonald, and I. Ounis. Ranking opinionated blog posts using opinionfinder. In Proc. of SIGIR, 2008.
[15] J. He, V. Hollink, and A. de Vries. Combining implicit and explicit topic representations for result diversification. In Proc. of SIGIR, 2012.
[16] X. Huang and W. B. Croft. A unified relevance model for opinion retrieval. In Proc. of CIKM, 2009.
[17] M. Kacimi and J. Gamper. Diversifying search results of controversial queries. In Proc. of CIKM, 2011.
[18] M. Kacimi and J. Gamper. Mouna: mining opinions to unveil neglected arguments. In Proc. of CIKM, 2012.
[19] M. Keikha, F. Crestani, and W. B. Croft. Diversity in blog feed retrieval. In Proc. of CIKM, 2012.
[20] Y. Lee, S.-H. Na, J. Kim, S.-H. Nam, H.-Y. Jung, and J.-H. Lee. Kle at trec 2008 blog track: Blog post and feed retrieval. In E. M. Voorhees and L. P. Buckland, editors, Proc. of TREC, Gaithersburg, Maryland, USA, November 18-21, 2008, volume Special Publication 500-277. National Institute of Standards and Technology (NIST), 2008.
[21] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proc. of SIGIR, 2005.
[22] S.-H. Nam, S.-H. Na, Y. Lee, and J.-H. Lee. Diffpost: Filtering non-relevant content based on content difference between two consecutive blog posts. In Proc. of ECIR, 2009.
[23] Ounis, M. de Rijke, C. Macdonald, G. Mishne, and Soboroff. Overview of the TREC-2006 Blog Track. In Proc. of TREC, 2006.
[24] T. Sakai and H. Joho. Overview of ntcir-9, 2011.
[25] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proc. of WWW, 2010.
[26] R. L. Santos, C. Macdonald, and I. Ounis. Selectively diversifying web search results. In Proc. of CIKM, 2010.
[27] R. L. Santos, C. Macdonald, and I. Ounis. Intent-aware search result diversification. In Proc. of SIGIR, 2011.
[28] R. L. T. Santos, C. Macdonald, R. McCreadie, I. Ounis, and I. Soboroff. Information retrieval on the blogosphere. Found. Trends Inf. Retr., 6(1), Jan. 2012.
[29] D. Vallet and P. Castells. Personalized diversification of search results. In Proc. of SIGIR, 2012.
[30] J. Wang and J. Zhu. Portfolio theory of information retrieval. In Proc. of SIGIR, 2009.
[31] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proc. of SIGIR, 2003.
[32] W. Zhang, L. Jia, C. Yu, and W. Meng. Improve the effectiveness of the opinion retrieval and opinion polarity classification. In Proc. of CIKM, 2008.
[33] W. Zhang, C. Yu, and W. Meng. Opinion retrieval from blogs. In Proc. of CIKM, 2007.

602

Term Level Search Result Diversification

Van Dang and W. Bruce Croft
Center for Intelligent Information Retrieval Department of Computer Science University of Massachusetts Amherst, MA 01003
{vdang, croft}@cs.umass.edu

ABSTRACT
Current approaches for search result diversification have been categorized as either implicit or explicit. The implicit approach assumes each document represents its own topic, and promotes diversity by selecting documents for different topics based on the difference of their vocabulary. On the other hand, the explicit approach models the set of query topics, or aspects. While the former approach is generally less effective, the latter usually depends on a manually created description of the query aspects, the automatic construction of which has proven difficult. This paper introduces a new approach: term-level diversification. Instead of modeling the set of query aspects, which are typically represented as coherent groups of terms, our approach uses terms without the grouping. Our results on the ClueWeb collection show that the grouping of topic terms provides very little benefit to diversification compared to simply using the terms themselves. Consequently, we demonstrate that term-level diversification, with topic terms identified automatically from the search results using a simple greedy algorithm, significantly outperforms methods that attempt to create a full topic structure for diversification.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ retrieval models
General Terms
Algorithms, Measurement, Performance, Experimentation.
Keywords
Search result diversification, term level, topic level.
1. INTRODUCTION
Search result diversification has been studied as a task of re-ordering an initial ranking of documents retrieved for a
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

query. The goal is to produce a more diverse ranked list with respect to some set of topics or aspects associated with this query. Existing approaches to diversification have been classified as either implicit or explicit [30]. The implicit approach includes MMR [4] and its probabilistic variants [31]. These techniques do not assume any explicit representation of the underlying topics for a query. Instead, they assume each document represents its own topic. As a result, diversity is achieved by iterating over the input ranking and selecting documents based on the difference of their vocabulary, as measured by document similarity. These methods are generally less effective [1, 30, 18] as there are no guarantees that the topics covered by the resulting documents correspond to query aspects.
The explicit approach, on the other hand, models the set of query aspects and select documents for each of them. This includes algorithms such as IA-Select [1], xQuAD [30] and Proportionality Model [18]. The success of these methods, however, has been observed mostly with descriptions of query aspects that have been created manually, either as a concise list of topics [30, 18], a larger taxonomy from which the query topics can be inferred [1], or a list of topics obtained directly from commercial search engines [30, 18].
Generating the query aspects or topic descriptions automatically, on the other hand, is not as well understood. Although there have been a number of attempts to do this [5, 27, 17], only more recent techniques that build topic descriptions by combining information from several sources have been shown to be effective on web copora [19, 20].
In the literature, a query topic or aspect is usually identified as a single phrase or unit. More generally, a topic is a coherent group of what we call topic terms. Fig. 1 shows an example TREC query: joints (topic 82) with two topics: treat joint pain and woodwork joint type. These topics contain five topic terms: treat, joint, pain, woodwork and type. The question that we address in this paper is whether diversification with respect to these topics benefits from the additional structure or grouping of terms or would diversification using the topic terms directly be just as effective?
We investigate the problem of term level diversification empirically. Instead of modeling the set of query aspects, each of which is a coherent group of terms, this approach directly models these terms without their topical grouping. Thus, it still explicitly models the user intents, but it uses a weaker representation of them. Our experiments on the ClueWeb collection using two existing diversification frameworks [30, 18] confirm that discarding the topic structure does not result in any significant loss in diversification effec-

603

Figure 1: Two different levels for diversification: topic level and term level.
tiveness. Therefore, instead of trying to recover the topics for a query, we only need to identify a set of terms that cover most of the query topics. This is, in fact, the main task for multi-document summarization (e.g., [29, 23, 24]).
Consequently, we propose to use a simple greedy algorithm from the document summarization literature for identifying topic terms for diversification from the initial ranking of documents [23, 24]. Our results show that this simple method significantly outperforms many existing approaches for estimating the full topic structure from the same data on a wide ranges of both relevance and diversity measures. To the best of our knowledge, our method is the first one that can provide statistically significant improvement over standard relevance-based retrieval models in both relevance and diversity measures, without relying on any external data source or manually created topic set.
In summary, the main contribution of this paper is term level diversification. It simplifies the current topic level approach by taking as input a set of terms as opposed to a set of topic descriptions. This is important since automatic topic generation has proven challenging. We show that our approach with terms generated automatically, using a summarization technique [23, 24], significantly outperforms its topic level counterpart with topics generated using existing methods. When ground-truth query topics are available, our approach remains comparable to the topic level alternative.
In the next section, we briefly mention related work. Section 3 presents the current topic level diversification frameworks, which will also be used for term diversification. Section 4 describes in more detail the notion of term level diversification as well as our algorithm for identifying topic terms. Section 5 and 6 contains the experimental setup and results, as well as analysis and discussions. Finally, Section 7 concludes.
2. RELATED WORK
Search result diversification has been studied as the task of retrieving documents covering multiple possible topics, aspects, or interpretations of a query. Existing work can be categorized using two orthogonal criteria: their representation of these topics and their notion of diversity.
Query Topic Representation. Proposed techniques are usually classified as either implicit or explicit. The implicit approach, in fact, does not assume any of such representation. Instead, it assumes each document has its own topic. It promotes diversity by selecting documents that are different to one another in terms of vocabulary, as captured by document similarity such as cosine [4] or Pearson's correlation [28] between the document vectors and KL divergence between their language models [31]. As the selected documents do not necessary cover any of the query topics, this

approach often fails to provide consistent improvement over standard relevance-based retrieval model on large web corpora [19, 18].
The explicit approach, on the other hand, models the set of query aspects and returns documents for each of them [5, 1, 30, 17]. Our term level diversification scheme belongs to this second category. The difference is, instead of modeling the set of query topics, each of which is a group of terms, we model these terms directly without their grouping structure. As we will show later on, the grouping provides very little benefit to diversification compared to the presence of the topic terms. This effectively reduces the task of finding a set of topics into finding a simple set of terms.
The success of the explicit approach, in fact, has been observed primarily with query topics that are either created manually (e.g. TREC subtopic descriptions [30, 18] or a larger predefined taxonomy [1]) or obtained directly from related queries provided by commercial search engines [30, 18]. Generating these aspects automatically, on the other hand, is not as well understood. For example, while clustering queries from logs [27] or anchor text and ngrams from the web [17] can produce interesting looking clusters of text, their effectiveness for diversification has yet been confirmed. Topics extracted from clustered documents, either deterministically or probabilistically via topic modeling [5], were only evaluated on a very small collection. In addition, their effectiveness is concluded to be only comparable to MMR [4], the canonical technique from the implicit approach [5]. Only more recent work [19, 20] has achieved some success, but they generally build topic descriptions by combining informations from several sources of data.
Instead of trying to generate a set of topics for a query, we apply a simple greedy algorithm [23, 24] to extract a diverse set of topic terms automatically from the input ranking. We then evaluate and compare their effectiveness for diversification (term level) with topics generated using some of the subtopic mining techniques mentioned above that utilize the same data (topic level) [5].
Notion of Diversity. There are two notions of diversity in the current literature: diversity by redundancy and by proportionality. The concepts of redundancy and novelty are based on the cascade user model which assumes users will scan the result list from top to bottom [14]. Therefore, documents at any position in the result list that provide the same information as those at earlier ranks are considered redundant. Similarly, novel documents are those that provide new information. A ranking is more diverse if it contains less redundancy, or equivalently, more novelty. Common to these techniques [4, 31, 7, 1, 5, 28, 30, 32, 18] are the greedy framework which sequentially selects documents with minimal redundancy, the measure of which is where they differ. For example, MMR [4] (implicit) measures redundancy of a document by its cosine similarity to the documents selected previously. IA-Select [1] and xQuAD [30] (explicit) measures how much it covers the query topics that have not been well covered by those chosen earlier.
On the other hand, a proportional ranking of documents with respect to a topic popularity distribution is a ranking in which the number of documents on each topic is proportional to its popularity [18]. By this definition, perfectly proportional search results would naturally be diverse. The main algorithm in this class is PM-2 [18], which selects doc-

604

uments in a similar greedy fashion, except that it maximizes proportionality using the Sainte-Lagu¨e formula.
In this paper, we compare term level diversification to the topic level counterpart using both frameworks. In particular, we choose xQuAD (redundancy-based) and PM-2 (proportionality-based) simply because they have been demonstrated to be effective on the ClueWeb collection, which we also use to conduct experiments.

3. DIVERSIFICATION FRAMEWORK
In this section, we first formally describe the problem of diversification at the topic level. Then we will present the two frameworks for diversification in the current literature: redundancy-based and proportionality-based diversification. These frameworks will later be used for term diversification.
3.1 Topic Level Diversification
Let q indicate a user query and T = {t1, t2, ..., tn} indicate the set of topics for q. Let W = {w1, w2, ..., wn} denote the weights for each of the topics ti  T . These weights can be interpreted as the importance [30] or popularity [18] depending on the diversification techniques. In addition, let R = {d1, d2, ..., dm} indicate a ranked list of documents initially retrieved for q and P (d|t) denote some probabilistic estimate of d's relevance to a topic t. The task of topic level diversification is to select a subset of R using {T, W, P (d|t)} to form a diverse ranked list S of size k.
It is worth noting that the type of topics T = {t1, t2, ..., tn} will determine the relevance measure P (d|t). For example, if T is a set of short textual descriptions (e.g. queries), P (d|t) is often the relevance score of d to t given by some retrieval models [30, 18].
3.2 Framework

3.2.1 Diversity by Redundancy
This framework promotes diverse rankings of documents by penalizing redundancy at every rank. It does so by greedily selecting documents in R to put into S. At each step, it selects the document that is most different to those previously selected (thus minimizing redundancy), while remains relevant to the query q:
d = arg max (1 - ) × P (dj|q) +  × D(dj, S) (1)
dj R

where D(dj, S) is a measure of novelty, which indicates the difference between the candidate document dj and each of the documents in S. Different choices of D(dj, S) correspond to different instantiations of this framework [1, 5, 30]. In this paper, we choose xQuAD [30] simply because it has proven effective on several TREC Web Track query sets [30, 18], which we use to carry out our evaluation. Our findings, nevertheless, should apply to all techniques within this framework.
xQuAD measures the difference between documents by the topics they cover. It defines pi to be the "portion" of the topic ti that has not been covered by documents in S:

pi = (1 - P (dj |ti))

(2)

dj S

Higher pi indicates that most of the documents in S are not relevant to ti. As such, ti is less substantially covered and

it should have higher "priority" in getting more documents. With this, D(dj, S) is calculated as follows:

D(dj , S) = wi × P (dj |ti) × pi

(3)

ti T

which means the novelty of a document is its ability to cover the topics that need covering (i.e. higher pi) weighted by the importance of the topics wi.

3.2.2 Diversity by Proportionality

The main algorithm in this class is the proportionality model PM-2 [18]. It is a probabilistic adaptation of the Sainte-Lagu¨e method for assigning seats to members of competing political parties such that the number of seats for each party is proportional to the votes they receive. PM-2 starts with a ranked list S with k empty seats. For each of these seats, it computes the quotient qti for each topic ti following the Sainte-Lagu¨e formula:

qti

=

wi 2si + 1

According the the Sainte-Lagu¨e method, this seat should be awarded to the topic with the largest quotient in order to best maintain the proportionality of the list. Therefore, PM2 assigns the current seat to the topic ti with the largest quotient. The document to fill this seat is the one that is not only relevant to ti but to other topics as well:

d = arg max  × qti × P (dj |ti ) + (1 - ) qti × P (dj |ti)

dj R

i=i

(4)

After the document d is selected, PM-2 increases the "por-

tion" of seats occupied by each of the topics ti by its normalized relevance to d:

si = si +

P (d|ti) tj T P (d|tj )

This process repeats until we get k documents for S or we are out of candidate documents. The order in which each document is put into S determines its ranking.

4. TERM LEVEL DIVERSIFICATION
4.1 Problem Statement
Diversification at the term level is very similar to the topic level. Let ti = {t1i , t2i , ..., t|iti|} be the set of terms for topic ti. Instead of diversifying R using the set of topics T = {t1, t2, ..., tn}, we propose to perform diversification using T  = {t1i , t2i , ..., t|iti|, ..., t1n, t2n, ..., t|ntn|}, in effect treating each tji as a topic.
Let us reuse the example query provided in Fig. 1 earlier to illustrate this. Instead of diversifying the initial ranking for the query joints with respect to two of its topics: treat joint pain and woodwork joint type, we propose to perform diversification with respect to its topic terms: treat, joint, pain, woodwork and type.
We will now compare diversification at the topic level to its term level counterpart at an intuitive level, using both frameworks, to provide some justification for why one can expect similar performance from these two paradigms. This is based on the assumption that if a document is more relevant to one topic than another, it is also more relevant to the terms associated with this topic than any of the terms

605

from the other topic. In other words, if a document is more relevant to treat joint pain than it is to woodwork joint type, we assume that it is also more relevant to treat and pain than it is to woordwork and type. Since both topics have the term joint, we will ignore it for the ease of explanation.
Let us first explain using the xQuAD's framework. At either the topic or term level, it follows from Eq. (1) that the first selected document d1 is the one that is most relevant to the user query. Let us assume this document d1 is more relevant to treat joint pain than it is to woodwork joint type. At the second step at the topic level, woodwork joint type will have higher "priority" (higher value for p) to get documents and thus, xQuAD will favor documents on this topic. At the same time at the term level, d1 should be also more relevant to treat and pain than it is to woodwork and type (because of our assumption). Therefore, woodwork and type will have higher "priority" than either treat or pain. It follows that, if the topic level system is able to find a document d for its relevance to woodwork joint type, this same document should also emerge at the term level. Once this document is selected, the "priority" of woodwork and type decreases as does that of woodwork joint type. As the algorithm proceeds, the two approaches may select different documents due to the different numbers of "topics", each of which is downweighted by a different amount (by Eq.(2)). Nevertheless, the general idea still applies, that if a document is selected for its relevance to any particular topic, that same document should be at least a highly potential candidate at the term level due to its relevance to the corresponding terms.
Similarly, in the framework of PM-2, let us also assume the first document d1 is more relevant to treat joint pain than it is to woodwork joint type at both levels. As a result, the "portion" of seats occupied by treat and pain is also higher than that of both woodwork and type, just as treat joint pain will have a higher portion than woodwork joint type. At the second step, woodwork joint type should be assigned a higher quotient by the Sainte-Lagu¨e formula, which again indicates woodwork joint type has higher "priority" similarly to what happens in the xQuAD framework.

4.2 Choice of P (d|t)
As mentioned earlier, diversification frameworks assume {T, W, P (d|t)} as inputs and the choice of T will determine P (d|t). An obvious choice for P (d|t) for term level diversification is P (tki |d), the probability that the document d generates the topic term tki . This is, however, highly problematic. At the term level, in addition to those true query topics which have become latent, there are also "false" latent topics formed by the wrong combinations of terms. In the context where we identify topic terms for a query automatically, some of them might be generic and ineffective. As the number of bad terms increases, the number of "false" topics will grow exponentially. Combined with the fact that there are likely many non-relevant documents in the baseline ranking, term diversification under the effects of these "false" topics might end up promoting non-relevant documents.
Assuming any document that is relevant to a true query topic should be relevant to the query itself, we propose to expand each topic term with the query. Let {q1, q2, ..., qn} be the set of terms of the query q. P (d|tki ) is estimated as follows:

1

1

P (d|tki ) = (P (tki |d)P (q|d)) |tki |+|q| = (P (tki |d) qj q P (qj |d)) |tki |+|q|

which is essentially the query likelihood model for ranking d with respect to the query {tki , q1, q2, ..., qn} [15] normalized by the query length to avoid biased towards shorter terms
(i.e. terms can include both unigrams or phrases). In the
case where all terms have the same length, the normalization
is certainly not necessary.
Note that the inclusion of the query is not the only mech-
anism to keep non-relevant documents under control. Interpolating P (tki |d) with P (q|d) as has been done in the redundancy-based framework (Eq. (1)) is another possibil-
ity. We do not do so because we do not want to introduce
more parameters into the framework. In principle, any combination of P (tki |d) and P (q|d) should be applicable. Since P (q|d) can be obtained directly from the baseline rankings,
this does not increase computational complexity.

4.3 Automatic Identification of Topic Terms
We now present DSPApprox, the topic term extraction algorithm proposed by Lawrie and Croft [23, 24] for hierarchical multi-document summarization. The goal of the algorithm is to select from a collection of documents a small set of highly representative terms that best summarize them. This algorithm is applied hierarchically, resulting in an hierarchical topic structure.
Since we only need a single diverse set of topic terms, we only apply the algorithm once on the initial ranking of documents R = {d1, d2, ..., dm} retrieved for the query q. The algorithm first identifies a set of vocabulary from these documents, from which it forms a set of more specific topic terms. It then measures for these terms their topicality and how well they predict the occurrences of other terms. Finally, it greedily selects a subset of topic terms, aiming to maximize both their topicality and their coverage of the vocabulary.

Vocabulary Identification. We consider as vocabulary all terms that (1) appear in at least two documents, (2) have at least two characters and (3) are not numbers. In our experiments, we test two types of terms: unigrams and phrases. We use a very simple method for phrase extraction. We scan through terms in each document and at each position, we select the longest sequence of terms that matches a wikipedia title as a phrase.

Topic Terms Identification. All vocabulary terms that co-occur with any of the query terms within a proximity window of size w is selected as topic terms.

Topicality and Predictiveness. Topicality of a term measures how informative it is at describing the set of documents. To compute topicality, a relevance model PR(t|q) [25] is first estimated from the initial set of documents R:

PR(t|q) =

P (t|di)P (di|q)

di R

where P (t|d) is the probability that di generates the term t and P (di|q) is relevance of di to the query. The topicality T P (t) of a term t is estimated as its contribution to the KL divergence between this relevance model and the language model for the entire retrieval collection:

T

P

(t)

=

PR(t|q)log2

PR(t|q) Pc(t)

It is equivalently t's contribution to the clarity score of the query q [16].

606

Predictiveness, on the other hand, measures how much the occurrence of a term predicts the occurrences of others. Let Pw(t|v) indicate the probability that a term t occurs within a window of size w of another term v and Ct indicate the set all such v. The predictiveness of t is estimated as follows:

P R(t) = 1 Z

Pw (t|v)

vCt

where Z is the hierarchy level specific normalization factor. In our case, we set it to the size of the vocabulary.

Greedy Algorithm. Pseudo-code for this algorithm is presented as Algorithm 1. It iteratively selects terms from the candidate topic term set T . The utility of each term is the product of its topicality and predictiveness. At each step, the algorithm selects the topic term t  T with maximum utility. Then, it decreases the predictiveness of other topic terms that predict the same vocabulary. This makes sure topic terms that cover the uncovered part of the vocabulary will emerge for selection in the next iteration. The algorithm stops once the utility of all candidate topic terms reaches 0, indicating that all vocabulary has been covered. Some example topic terms (both unigrams and phrases) generated by DSPApprox for the query joints are provided in Table 1.

Algorithm 1 DSPApprox for identifying topic terms.

1: V = {v1, v2, ..., vn}: the set of vocabulary

2: T = {t1, t2, ..., tm}: the set of candidate topic terms

3: Cti : set of terms occurring within a window to ti 4: Pw(ti|v): co-occurrence (within window of size w) statistics

5: Compute topicality T P (ti), ti  T

6: Compute predictiveness P R(ti), ti  T

7: DT T : the output diverse set of topic terms

8: P REDV : vocabulary that has been predicted by DT T

9: DT T  

10: P REDV  

11: while P REDV  V and |T | > 0 do

12: 13:

t  DT T

argDmTaxTtiTtT

P

(ti)

×

P

R(ti)

14: T  T \ {t}

15: pred  Ct

16: for all v  pred \ P REDV do

17:

for all ti  T do

18:

P R(ti)  P R(ti) - Pw(ti|v)

19:

end for

20: end for

21: P REDV = P REDV  pred

22: end while

5. EXPERIMENTAL SETUP
Query and Retrieval Collection. Our query set consists of the 147 queries with relevance judgments from three years of the TREC Web Track's diversity task (2009 [10], 2010 [11] and 2011 [12]). Our evaluation is done on the ClueWeb09 Category B retrieval collection1, which contains roughly 50 million web pages in English. This collection is stemmed using the Krovetz stemmer [22]. Stopword removal is only performed on the query using a small stopword list.
Baseline Retrieval Model. We use the standard querylikelihood model [15] implemented in Indri2 to conduct the
1http://boston.lti.cs.cmu.edu/Data/clueweb09/ 2http://www.lemurproject.org

initial retrieval run. This run serves not only as a means to provide a set of documents for the diversification systems but also as a baseline to verify their usefulness.
Spam filtering is known to be an important component of web retrieval [2]. In addition, documents with too few stopwords are found to have poor readability [21, 26]. Therefore, we incorporate both of these into our baseline ranking. We use the spam filtering technique described by Cormack et al. [13], which assigns a "spamminess" percentile S(d) to each document d in the collection. Let (d) be the stopword to non-stopwords ratio in d and p(d|q) indicate the score the retrieval model assigns to the document d. Following Bendersky et al. [2], the final score of d is given by:

P (d|q) =

p(d|q) -

if S(d)  60 and (d)  0.1 otherwise

Diversification Frameworks. We compare term level diversification to topic level diversification using both xQuAD [30] and Proportionality Model (PM-2) [18], which we have described in Section 3. While xQuAD obtains diversity by penalizing redundancy at every position in the ranked list, PM-2 does so by promoting proportionality at every rank.
Evaluation Metric. We report our evaluation results using several standard metrics that have been used in the official evaluation of the diversity tasks at TREC [11, 12]: -NDCG [8], ERR-IA (a variant of ERR [6]) and NRBP [9]. These metrics penalize redundancy at each position in the ranked list based on how much of that information the user has already seen from documents at earlier ranks. In addition, we also report our results using Precision-IA [1] and subtopic recall, which indicate respectively the precision across all topics of the query and how many of those topics are covered in the search results. All of these measures are computed using the top 20 documents retrieved by each model to be consistent with official TREC evaluation. Statistically significant differences are measured using two-tailed t-test with p-value < 0.05.
Most diversification mechanisms are evaluated using only diversity measures [1, 30, 18]. It is unclear if diversity is achieved at a cost to relevance. Therefore, in addition to all diversity measures above, we also report our results using two standard relevance-based metrics for web retrieval: NDCG and ERR, which are also evaluated at the top 20 documents.
Parameter Settings. All of the diversification approaches under evaluation are applied on the top K retrieved documents. We set K = 50 to be consistent with existing research which found that both xQuAD and PM-2 achieve their highest performance at K = 50 [18]. Consequently, all topic and term extraction techniques will also operate on these top 50 documents.
Each topic and term extraction technique, as we will show later, has several free parameters that require tuning. xQuAD and PM-2 also have one parameter  to tune. To enforce fair comparison, all parameters are selected via 3-fold cross validation.
We consider for  values in the range of [0.05, 1.0] with a increment of 0.05. Value ranges for parameters of the topic and term extraction methods will be presented in their respective sections.

607

Table 1: Some example outputs of DSPApprox for the query "joints" (topic number 82). Important terms from the

original TREC subtopics for this query are also provided for easy references.

TREC Sub-topic

DSPApprox[Unigram] DSPApprox[Phrase]

1) joints in human body

spine articulate

elbow joint knee joint

2) woodworking joints types

miter planter

mitter joint mitter box

3) treat joint pain

symptom grease

joint pain joint anti inflamory

6. EVALUATION

6.1 Term Level Diversification: Effectiveness

We first compare the term level diversification approach to the topic level approach using the set of true topics associated with each query (TREC "sub-topics"). A topic is a coherent group of terms. These topics represent the oracle grouping of the oracle topic terms. By comparing the diversification effectiveness of this set of topics (topic level diversification) with that of the corresponding set of unigram topic terms (term level diversification), we can separate the benefit diversification algorithms get from the grouping with the benefit they get from the presence of topic terms.
In addition, related queries provided by commercial search engines have been demonstrated to be very effective for diversification [30, 18]. These queries too can be considered good underlying topics for the original query. As a result, we also compare the two diversification paradigms using this topic set. It is worth noting that the search engine provides no suggestions for three of the queries in our set. The query set for this experiment only contains 144 queries (out of 147).
Similar to existing work [18], the document-topic relevance function P (d|t) for topic level diversification is implemented as the query-likelihood score for d with respect to t (each topic t is treated as a query). In particular, let ti = {t1i , t2i , .., tni } indicates the set of terms for the topic ti. P (d|ti) is computed using the geometric mean to avoid biased towards shorter topics:

 P (d|ti) = 

1
 |ti| P (tki |d)

tik ti

For term level diversification, P (d|t) is calculated as described in Section 4.2.
Table 2 compares term diversification to topic diversification using both topic sets and both diversification frameworks. The first thing to notice is that both topic and term diversification, using both PM-2 and xQuAD, significantly outperform the baseline in all metrics. This is certainly unsurprising since we are using the oracle data. Nevertheless, it confirms the effectiveness of both of these frameworks at providing relevant and diverse results.
What is more interesting from Table 2 is that the set of topic terms maintains a highly comparable level of performance to the topic structures. There are no statistically significant differences in all cases. These results are consistent across different diversification techniques and topic sets. This suggests that existing diversification frameworks are capable of returning relevant documents for topics without the explicit topical grouping.
We notice, however, that some of the query topics are different to the query itself by only one term. For example,

topics for the query "south africa" include "history of south africa" and "maps of south africa". Both of these topics have only one key term, which is "history" and "maps" respectively. It is possible that term level diversification is competitive with the topic level alternative because of queries like this.
To investigate this issue, we use the notion of key term to indicate the number of non-stopword terms in a query topic that are different to the query text. To quantify the impact the number of key terms has on our approach, we plot the number of topics where each approach is able to provide at least one relevant document against the number of key terms for these topics. In addition, we also plot the actual number of relevant documents retrieved for each topic (on log scale) against the number of key terms it contains. These plots are presented by Fig. 2 (a) and (b) respectively. Note that we only show the plots for PM-2 because the analysis with xQuAD is very similar.
Fig. 2 reveals that not only is our approach comparable with its topic counterpart on topics with a single key term, it also remains competitive consistently across different numbers of key terms. In particular, term level diversification has a slight advantage with topics that have only one single key term. With topics that have two and three key terms, although the topic level systems perform better, the difference is very small. The two approaches become comparable with larger numbers of key terms. Given that the term level systems do not need the topical structure, this very slight performance loss seems reasonable.
In summary, our experiments with the "oracle topics"show that the benefits for diversification of grouping topic terms are minimal compared to just using the terms themselves. The existing frameworks, PM-2 and xQuAD to be specific, can perform topical diversification at the term level. Together, these findings indicate that term level diversification is worth pursuing.
6.2 Automatically Generated Topics vs. Terms
We now evaluate the effectiveness of DSPApprox for automatically extracting topic terms, considering unigrams and phrases separately, from the initial ranking of documents. The total number of unigrams and phrases the algorithm returns are approximately 100 and 500 respectively. Since using too many terms is inefficient and unlikely to be effective, we use a parameter T to control the number of terms used for diversification. The second parameter is w, which determines the size of the window in which (1) a term has to co-occur with at least one query term in order to be considered a candidate topic term, and (2) prediction boundary: a term cannot predict terms that are more than w words away. We consider w  {20, 30, 40, 50} and T  {5, 10, 20, 40, 60, 80, 100}.

608

Table 2: Performance comparison between term level (marked as [Term]) diversification, topic level ([Topic]) and the

non-diversification baseline query-likelihood (QL) using two frameworks: PM-2 and xQuAD. They are evaluated on

both the oracle topic set (TREC) and the set obtained from a commercial search engine (C.S.E) using a wide range

of diversity and relevance measures. Win/Loss (W/L) is with respect to -NDCG. Q indicates statistically significant

differences to QL.

Diversity -NDCG W/L ERR-IA Prec-IA S-Recall NRBP

Relevance NDCG ERR

TREC

PM-2 xQuAD

QL [Topic] [Term] [Topic] [Term]

0.3927 0.4742Q 0.4635Q 0.4447Q 0.4366Q

84/41 92/30 85/40 89/33

0.2807 0.3536Q 0.344Q 0.3207Q 0.3143Q

0.1829 0.2021Q 0.2064Q 0.2035Q 0.2067Q

0.5824 0.6341Q 0.6268Q 0.6259Q 0.6171Q

0.2446 0.3212Q 0.3094Q 0.2856Q 0.2777Q

0.2742
0.2979 0.3048Q 0.3002Q 0.3054Q

0.1371
0.1468 0.1535Q 0.1490Q 0.1515Q

C.S.E.

PM-2 xQuAD

QL [Topic] [Term] [Topic] [Term]

0.3884 0.4308Q 0.4305Q 0.4024Q 0.4118Q

62/60 69/52 63/51 70/48

0.2783 0.3137Q 0.3162Q
0.2888 0.2965Q

0.1789 0.1815 0.1874 0.183 0.1883

0.5735 0.6084Q 0.6084Q
0.5884 0.5954Q

0.2428 0.2784Q 0.2804Q
0.252 0.2603Q

0.2739 0.2790 0.2872 0.2797 0.2906

0.1394 0.1415 0.1471 0.1423 0.1464

Figure 2: Figure (a) provides the total number of topics (across all queries) the term level the topic level diversification systems (under the PM-2's framework) as well as query-likelihood cover with respect to their number of key terms. Figure (b) shows the number of relevant documents (on log scale) retrieved for each of these topics.

6.2.1 Baselines
Baseline 1. Our first baseline for comparison, first proposed by Carterette and Chandar [5], estimates topic models using LDA [3] from the documents and uses the resulting clusters for diversification. This model only has one parameter, which is the number of latent topics c  [2..10]. The relevance between a document to a topic is provided by the LDA framework. We use the multi-threaded implementation of LDA that is publicly available 3.
Baseline 2. Our second baseline technique, also proposed by Carterette and Chandar [5], applies k-nearest neighbor (KNN) first to cluster the documents. After that, it estimates a relevance model [25] from each of the clusters and use it as a topic model. Its parameters include k  [2, 10] and T  {5, 10, 20}, which are the number of neighbors and the number of top terms from the relevance model to be used as topic description respectively. The topic descrip-
3https://sites.google.com/site/rameshnallapati/software

tion is treated as an Indri weighted query. The relevance between a document and this topic is obtained directly via Indri's output relevance score.
Baseline 3. MMR [4] has become a canonical baseline in the diversity literature [31, 30, 18]. Though it does not explicitly model topics, it fits into the class of algorithms that relies solely on the set of documents. The framework of MMR is very similar to the one presented in Eq. (1). The novelty component D(d, S), which indicates the different between a document d and those previously selected in S, is aggregated over its difference to each of the document dj  S. The difference between two documents is implemented based on the cosine similarity. We experimented with three aggregation functions: max, min and average and report results with max since it is the most effective.
6.2.2 Results
Table 3 presents the results comparing the systems mentioned above. All of their parameters are determined using 3-fold cross validation. The letters Q, M, L, K indicate sta-

609

Table 3: Performance comparison among (1) topic terms (both unigrams and phrases) generated by DSPApprox (ab-

breviated as DSP), (2) topics generated by LDA and KNN, (3) MMR which does not explicit model query topics,

and (4) the non-diversification baseline query-likelihood (QL) using two frameworks: PM-2 and xQuAD. Evaluation is

done using a wide range of diversity and relevance measures. Win/Loss (W/L) is with respect to -NDCG. Q, M, L,

K indicate statistically significant differences (p-value < 0.05) to QL, MMR, LDA and KNN respectively. Bold face

indicates the best performance in each group.

Diversity

-NDCG

W/L ERR-IA

Prec-IA

S-Recall

NRBP

Relevance

NDCG

ERR

QL

0.3927

0.2807

0.1829

0.5824

0.2446

0.2742

0.1371

MMR

0.393

30/39 0.2804

0.1829

0.5855

0.2445

0.2700Q

0.1365

PM-2

LDA[Topic] KNN[Topic] DSP[Unigram]
DSP[Phrase]
LDA[Topic] KNN[Topic] DSP[Unigram]
DSP[Phrase]

0.3762 0.3991 0.4161QL ,M 0.4159QL ,M
0.3905 0.3897
0.3906
0.3943

56/70 41/62 71/54
68/57
55/59 46/42 54/57
56/63

0.2592Q 0.2882Q 0.3085QL,,KM 0.3131QL,,KM
0.2798 0.2786
0.2837
0.2888

0.1579 0.1808 0.1953L,K
0.1953L,K
0.154Q 0.1846 0.1844L
0.1923L

0.5977 0.5825 0.5789
0.5684
0.5884 0.5824 0.5594QL,,KM 0.561M

0.2192Q 0.2536Q 0.2788QL,,KM 0.2867QL,,KM
0.2453 0.2426 0.252QK,M 0.2587QL,,KM

0.2395Q 0.2711 0.2981QL,,KM 0.3011QL,,KM
0.2350Q 0.2752 0.2780M L
0.2889L

0.1226Q 0.1373 0.1440L
0.1480L
0.1288Q 0.1369 0.1386L
0.1408L

xQuAD

tistically significant differences (p-value < 0.05) to querylikelihood, MMR, LDA and KNN respectively. Among the three baseline techniques for topic generation, MMR's performance is the most similar to the baseline. It is interesting to see that while LDA has the best results in S-Recall, it performs poorly on all other measures, regardless of the diversification techniques. This can be explained by the fact that an LDA topic is a distribution over the entire vocabulary of the initial set of documents. Thus, each topic has a higher chance of matching its documents, but at the same time it also matches several non-relevant documents. Overall, KNN is the only technique among the three baselines that can provide some improvement over query-likelihood (with PM2). Nevertheless, KNN has a trade-off between relevance and diversity: while KNN topics used by PM-2 help most of the diversity measures, it hurts relevance. On the other hand, the topics it generates, when used by xQuAD, helps relevance but hurt most of the diversity measures. Overall, the difference between these baselines and query-likelihood is mostly not statistically significant.
In contrast, both the unigrams and phrases generated using DSPApprox when used by PM-2 substantially outperform all other systems under comparison on many measures. Statistically significant differences are observed in many cases. In fact, it is the only system that optimizes for diversity measures yet outperforms query-likelihood in both relevance measures. Between unigrams and phrases, the former appears to be slightly more robust by improving more queries and hurting fewer, but the latter manages to retrieve more relevant results. In addition, their performance with xQuAD is still slightly higher than all three baselines on most precision-based measures.
The limited performance of our method when using xQuAD can be explained by xQuAD's vulnerability to large numbers of topics. Let us revisit Eq. (3) and assume that at the kth step, the topic ti has the highest "priority" pi. We can rewrite Eq. (3) with respect to ti as follows:

D(d, S) = wi × P (d|ti) × pi +

wj × P (d|tj ) × pj

tj T tj =ti

There is an implicit uncontrolled trade-off here between the

relevance of a document to ti, the topic with the highest priority, and its relevance to other lower priority topics. As the size of T increases, it becomes possible that a document d that is relevant to many tj will be selected even though xQuAD should be selecting documents for ti. This is certainly not a big problem for topic level diversification since the number of topics is relatively small. At the term level, however, our algorithm generates hundreds of terms, many of which can be very generic. As such, some non-relevant documents can appear randomly relevant to many of such terms, dominating the topic term with the highest priority.
Note that PM-2 has the same trade-off as xQuAD (Eq. (4)). The difference is that it is controlled by the parameter . If a topic term set is too noisy, cross-validation should be able to specify a larger value for  to put more emphasis on the topic with the highest priority .
6.2.3 Improvement and Failure Analysis
We focus our analysis of DSPApprox results using PM2. As can be seen from Table 3, although DSPApprox has slightly lower subtopic recall compared to query-likelihood (QL), the difference is not significant. Our investigation suggests that not only do DSPApprox and QL cover about the same number of topics, they cover almost the same set of topics (97% overlap). This high percentage of overlap suggests that the terms generated by DSPApprox are biased towards topics covered by the top ranked documents in the initial ranking.
We believe the cause of this bias is the way DSPApprox computes topicality. We observe that the topicality of a term is relatively proportional to the probability that it is given by the relevance model [25] estimated from the top 50 documents. This model usually assigns higher probabilities to frequent terms from higher ranked documents since they are assumed more relevant. If a document at a very low position covers topics that are different from those at early ranks, chances are their topic terms do not appear in these documents with high frequency. Therefore, their chance to be included in the resulting set of terms is relatively small, causing these topics to be excluded from the coverage of the final set. This is the main reason why subtopic recall was not improved.

610

Table 4: Contribution of (1) better topic coverage and (2)

within topic coverage and ranking of relevant documents to

the overall improvement on -NDCG. WIN and LOSS indi-

cate the sets of queries whose -NDCG DSPApprox (DSP for

short) improves and hurts respectively. S.Rec  is the sub-

set of WIN on which subtopic recall is also improved and

REST is its complement. S.Rec  is the subset of LOSS on

which subtopic recall is also lowered and REST is its comple-

ment. P is the difference of -NDCG compared to query-

likelihood. [U] and [P] indicate terms and phrases respec-

tively. P

#q P

DSP[P] DSP[U]

WIN

+0.2682

S.Rec  REST

10 61

LOSS

-0.1400

S.Rec  REST

10 44

+0.0652 +0.2030 -0.0616 -0.0784

WIN

+0.3285

S.Rec  REST

11 57

LOSS

-0.2034

S.Rec  REST

14 43

+0.0749 +0.2536 -0.1170 -0.0864

Regardless, DSPApprox still manages to outperform QL in both -NDCG and Precision-IA. This indicates that while both of them have the same topic coverage, DSPApprox retrieves more relevant documents for these topics as well as provides better ranking for them. More quantitative analysis on this is provided in Table 4. WIN and LOSS indicate the set of queries where DSPApprox helps and hurts -NDCG compared to QL. P denotes the performance difference in NDCG. S.Rec  indicates the subset of WIN where S-Recall is also improved and REST indicates the remaining of the set. Similarly, S.Rec  indicates the subset of LOSS where S-Recall is also lower and REST indicates the remaining. It can be seen that the increase of S-Recall contributes very little to the overall improvement on -NDCG (the S.Rec  sets). At the same time, they are also responsible for some performance loss (the S.Rec  sets). On the other hand, a significant chunk of improvement is observed on the sets of queries that cover no more topics than QL (the REST sets).
The analyses above suggest that the terms provided by DSPApprox, though unable to recover additional topics due to the bias issue, correctly represent most of those covered by QL. Consequently, they help surface more documents on theses topics, significantly improving -NDCG.
It is worth noting that, diversification with both unigrams and phrases provided by DSPApprox also significantly improves the relevance of the results (NDCG). Our approach, in fact, turns out to be very similar to pseudo-relevance feedback. The difference is that traditional relevance feedback uses the extracted terms to update the query model to retrieve new documents. Our approach, on the other hand, only attempts to re-order the input ranking, pushing more relevant documents to earlier ranks. As such, diversification can be considered a precision-driven framework for relevance feedback.
6.2.4 Parameter Sensitivity
As mentioned earlier, our topic term identification algorithm has two parameters. T controls how many of the top output terms to use for diversification and w determines how many words away can a term predict as well as how far can a topic term be from the query term. The best parameter

values selected for DSPApprox (using 3-fold cross-validation) with unigrams is {T = 40, w = 20} and the best values for phrases are {T = 80, w = 40}.
We first vary T from 10 to 200 and and keep w = 20 for unigrams and w = 40 for phrases. Note that the line for unigrams stops at T = 100 since our algorithms generates at most 100 unigram terms. Fig. 3 shows that regardless of the value of T , their is always some improvement. The set of unigrams, in particular, is very robust: it provides substantial improvement for most of T 's values. As for phrases, two few (e.g. less than 50) or too many (more than 100) terms result in very minor improvement.
We then vary w from 20 to 50 and keep T constant. Fig. 3 shows the sensativity of this parameter. Similarly, improvement is observed at every value.
7. CONCLUSIONS AND FUTURE WORK
This paper introduces a new approach to topical diversification: diversification at the term level. Existing work models a set of aspects for a query, where each aspect is a coherent group of terms [30, 18]. Instead, we propose to model the topic terms directly. Our experiments, using both TREC subtopics and related queries provided by a commercial search engine, show that the two approaches achieve highly comparable results in all diversity and relevance measures. It indicates that the topical grouping provides little benefit to diversification compared to the presence of the terms themselves. The reason for this is that if a document is selected by the topic level system for its relevance to some particular topics, it is often relevant to the corresponding topic terms as well. Thus, this document also appears as a highly potential candidate to term level system. Term level diversification, in fact, works in the same principles as the topic counterpart.
This effectively reduces the task of finding a set of query topics, which has proven difficult, into finding a simple set of terms. Consequently, we propose to use a simple greedy algorithm from the literature of multi-document summarization [23, 24] to identify a diverse set of topic terms (unigrams and phrases). Our results demonstrate that, diversification using these terms significantly outperforms its topic level alternative with automatically extracted topics, as well as the standard relevance-based retrieval models on various diversity and relevance measures.
For future work, we will consider applying DSPApprox on not only the initial retrieved documents but also on external data such as Wikipedia and anchor text collections. We believe this will help alleviate the current bias issue, improving sub-topic recall. In addition, note that DSPApprox itself is a term diversification algorithm: it selects a set of terms that best cover the vocabulary. It is worth examining the possibility of replacing it with techniques such as PM-2.
8. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval and in part under subcontract #19-000208 from SRI International, prime contractor to DARPA contract #HR0011-12-C-0016. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.

611

Figure 3: Sensitivity of the two parameters of DSPApprox (DSP for short): T (the number of topic terms used for diversification) and w (the proximity parameter). [U] and [P] indicate unigrams and phrases respectively.

9. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of WSDM, pages 5-14, 2009.
[2] M. Bendersky, D. Fisher, and W.B. Croft. UMass at TREC 2010 Web Track: Term dependence, spam filtering and quality bias. In Proceedings of TREC, 2010.
[3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. In JMLR, (3):993­1022, 2003.
[4] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings SIGIR, pages 335-336, 1998.
[5] B. Carterette and P. Chandar. Probabilistic models of ranking novel documents for faceted topic retrieval. In Proceedings of CIKM, pages 1287-1296, 2009.
[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of CIKM, pages 621-630, 2009.
[7] H. Chen and D. R. Karger. Less is more: probabilistic models for retrieving fewer relevant documents. In Proceedings of SIGIR, pages 429­436, 2006.
[8] C.L.A. Clarke, M. Kolla, G.V. Cormack, O. Vechtomova, A. Ashkan, S. Buttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of SIGIR, pages 659-666, 2008.
[9] C.L.A. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. In Proceedings of ICTIR, pages 188-199, 2009.
[10] C.L.A. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 Web track. In TREC, 2009.
[11] C.L.A. Clarke, N. Craswell, I. Soboroff, and G.V. Cormack. Overview of the TREC 2009 Web track. In TREC, 2010.
[12] C.L.A. Clarke, N. Craswell, I. Soboroff, and E.M. Voorhees. In TREC, 2011.
[13] G.V. Cormack, M.D. Smucker, and C.L.A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Apr 2010.
[14] N. Craswell, O. Zoeter, M.J. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In Proceedings of WSDM, pages 87-94, 2008.
[15] W.B. Croft, D. Metzler, and T. Strohman. Search Engines: Information Retrieval in Practice. Addison-Wesley, 2009.
[16] Predicting query performance. S. Cronen-Townsend, Y. Zhou and W.B. Croft. In Proceedings of SIGIR, pages 299­306, 2002.

[17] V. Dang, X. Xue, and W.B. Croft. Inferring query aspects from reformulations using clustering. In Proceedings of CIKM, pages 2117-2120, 2011.
[18] V. Dang and W.B. Croft. Diversity by proportionality: An election-based approach to search result diversification. In Proceedings of SIGIR, pages 65-74, 2012.
[19] Z. Dou, S. Hu, K. Chen, R. Song, and J.R. Wen. Multidimensional search result diversification. In Proceedings of WSDM, pages 475-484, 2011.
[20] J. He, V. Hollink, and A. de Vries. Combining implicit and explicit topic representations for result diversification. In Proceedings of SIGIR, pages 851-860, 2012.
[21] T. Kanungo and D. Orr. Predicting the readability of short web summaries. In Proceedings of WSDM, pages 202­211, 2009.
[22] R. Krovetz. Viewing morphology as an inference process. In Proceedings of SIGIR, pages 191^aA S¸-202, 1993.
[23] D. Lawrie, W.B. Croft, and A. Rosenberg. Finding topic words for hierarchical summarization. In Proceedings of SIGIR, pages 349­357 , 2001.
[24] D. Lawrie and W.B. Croft. Generating hierarchical summaries for web searches. In Proceedings of SIGIR, pages 457­458 , 2001.
[25] V. Lavrenko and W.B. Croft. Relevance-Based Language Models. In Proceedings of SIGIR, pages 120­127, 2001.
[26] A. Ntoulas and M. Manasse. Detecting spam web pages through content analysis. In Proceedings of WWW, pages 83-92, 2006.
[27] F. Radlinski, M. Szummer, and N. Craswell. Inferring query intent from reformulations and clicks. In Proceedings of WWW, pages 1171-1172, 2010.
[28] J. Wang and J. Zhu. Portfolio theory of information retrieval. In Proceedings of SIGIR, pages 115-122, 2009.
[29] M. Sanderson and W.B. Croft. Deriving concept hierarchies from text. In Proceedings of SIGIR, pages 206-213 1999.
[30] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proceedings of WWW, pages 881-890, 2010.
[31] C. Zhai, W.W. Cohen, and J. Lafferty. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. In Proceedings of SIGIR, pages 10-17, 2003.
[32] W. Zheng, X. Wang, H. Fang and H. Cheng. Coverage-based Search Result Diversification. In Information Retrieval, 15(5): 433-457, 2012.

612

Search Result Diversification in Resource Selection for Federated Search

Dzung Hong
Department of Computer Science Purdue University
250 N. University Street West Lafayette, IN 47907, USA
dthong@cs.purdue.edu

Luo Si
Department of Computer Science Purdue University
250 N. University Street West Lafayette, IN 47907, USA
lsi@cs.purdue.edu

ABSTRACT
Prior research in resource selection for federated search mainly focused on selecting a small number of information sources that are most relevant to a user query. However, result novelty and diversification are largely unexplored, which does not reflect the various kinds of information needs of users in real world applications.
This paper proposes two general approaches to model both result relevance and diversification in selecting sources, in order to provide more comprehensive coverage of multiple aspects of a user query. The first approach focuses on diversifying the document ranking on a centralized sample database before selecting information sources under the framework of Relevant Document Distribution Estimation (ReDDE). The second approach first evaluates the relevance of information sources with respect to each aspect of the query, and then ranks the sources based on the novelty and relevance that they offer. Both approaches can be applied with a wide range of existing resource selection algorithms such as ReDDE, CRCS, CORI and Big Document. Moreover, this paper proposes a learning based approach to combine multiple resource selection algorithms for result diversification, which can further improve the performance. We propose a set of new metrics for resource selection in federated search to evaluate the diversification performance of different approaches. To our best knowledge, this is the first piece of work that addresses the problem of search result diversification in federated search. The effectiveness of the proposed approaches has been demonstrated by an extensive set of experiments on the federated search testbed of the Clueweb dataset.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Keywords
Federated Search, Resource Selection, Diversification
1. INTRODUCTION
Federated search, also known as distributed information retrieval [28, 4, 14], focuses on searching information distributed across multiple information sources such as local repositories or verticals. There are three major sub-problems in federated search: resource representation obtains information about contents and other key properties of each individual information source, resource selection selects a small number of most useful sources given a user query, and result merging integrates individual ranked lists from selected sources into a single final list. A large body of research has been conducted for resource selection in federated search [4, 28]. However, little is known about selecting a set of sources that balances relevance and novelty. This substantially limits the usability of federated search in many applications.
On the other side, search result diversification has been studied extensively in ad hoc search in order to offer more coverage for ambiguous and multifaceted queries. In several occasions, users' intents in their queries may not be expressed explicitly. For example, an ambiguous query such as "Jaguar" may refer to an animal or a car model; or a multifaceted query such as "Batman" may refer to a name of a movie, a comic character, or the comic itself. Search result diversification circumvents this problem by explicitly or implicitly considering probable aspects of the query and presenting the search results in a way that is easier for users to find the needed information. Since 2009, the TREC Web track has incorporated diversification in the evaluation of the Web track [9]. Several evaluation metrics have been developed in ad hoc search to measure the effectiveness of different approaches of search result diversification.
Search result diversification in federated search may not be as simple as diversifying the final ranked list obtained from the selected sources. In a federated environment where documents of a same source cover similar topics, selecting a set of sources that balances relevance and novelty becomes crucial. As for the example of the query "Jaguar" above, if the sources related to "Jaguar" as a car dominate the resource selection result, it will be much harder to obtain a diversified ranked list in the end.
This paper proposes new approaches in diversifying results of resource selection in federated search. To the best of our knowledge, this is the first study that tackles the issue. First, a set of new metrics is designed for measuring result diver-

613

sification in resource selection. The metrics can incorporate any diversity measure that has been developed in ad hoc search, including the intent-aware expected reciprocal rank (ERR) [7], -nDCG@k [13] and MAP-IA [1]. Second, two general approaches are proposed for diversifying resource selection. The first approach extends the ReDDE framework and utilizes a ranked list of documents on the centralized sample database. By reranking the sample documents with respect to result diversification, a better set of sources can be obtained in term of relevance and aspect coverage. The second approach offers a different view. Instead of reranking the sample documents based on their relevance to the query aspects, the information sources are reranked in a similar process. This can be done by estimating the relevance of each information source with respect to different aspects of the query by any existing resource selection algorithm. Furthermore, a learning based classification approach is proposed to combine multiple resource selection algorithms for a better estimation of source relevance with respect to query aspects. With some training data, the learning based classification approach can improve the effectiveness of resource selection for search result diversification.
An extensive set of experiments has been conducted with the federated search testbed of the Clueweb dataset to evaluate the proposed research. In many experimental settings, the new approaches can successfully improve result diversification over traditional approaches that only consider document relevance. In particular, the new approaches provide superior performance on two test levels: source-level results of resource selection and document-level results of the final ranked list of federated document retrieval. Finally, the learning based approach, which combines results from multiple resource selection algorithms, outperforms each individual algorithm in result diversification.
The rest of the paper is organized as follows. Section 2 offers the literature review on both resource selection in federated search and result diversification in ad hoc search. Section 3 discusses our proposed metric to measure the diversity of selected sources. Section 4 proposes the two result diversification approaches for resource selection in federated search. Section 5 presents the learning based classification approach. The new proposed research is examined by an extensive set of experiments in Sections 6 and 7. Section 8 concludes our work and points out some potential research directions in the future.
2. RELATED WORK
Considerable research have been conducted for all three sub-problems of federated search as resource representation, resource selection and result merging [4, 28, 15]. This section provides a discussion of prior research on resource selection, as well as a brief review on resource representation and result merging. We also discuss some popular ad hoc search algorithms for search result diversification.
Resource representation is the first step of federated search for obtaining important properties of distributed information sources such as content and size statistics. Query-based sampling [4] is a common approach as it obtains sample documents from available sources with randomly generated queries. The sample documents obtained in this process can be placed together in a centralized sample database.
Resource selection selects a small number of most relevant information sources for a user query. Some early resource

selection algorithms such as CORI [4], CVV [37] and KL [36] treat each source as a big document and derive useful statistics to rank available sources with respect to a user query. However, these algorithms have limitation of losing the boundaries of individual documents, and thus may underestimate a big source with many relevant documents. Topic modeling has been proposed by recent work to overcome this limitation [3].
Other resource selection algorithms such as ReDDE [32], DTF [17], CRCS [27] and SUSHI [33] step away from the big document assumption by modeling individual documents of a source. Several selection algorithms in this category rely on the centralized sample database to build a ranked list of sample documents for a user query, and then assign a relevance score to available sources based on the scores of their sample documents in the list. Different algorithms use different methods for aggregating document contribution to available sources. Recent work by Markov et al. follows a similar approach, but attempts to minimize uncertainty in the centralized sample database by sampling different queries, retrieval systems, or rankings [22].
Learning based models have also been proposed for resource selection. They treat resource selection in federated search [20], or vertical search [2] as a classification problem. In particular, given a set of training queries and some relevance judgment, a classification model can learn to predict the relevance of an information source. In some experiments, the classification approaches have been shown to provide more accurate resource selection results than traditional algorithms without the training process.
Result merging is the last step in federated search, which merges documents returned by selected sources into a single ranked list. Modern methods such as SSL [31] and SAFE [30] both attempt to merge documents by approximating the centralize retrieval results in different ways.
Existing research in federated search have not explored an important issue of result novelty and diversification, which limits their abilities in representing the various information needs of users. The research work in [29] estimates the degree of document overlap among available sources, but its focus is only on duplicate documents and does not directly address the diversification problem related with multiple aspects of user queries. Other research work in [25] and [39] address diversification in aggregated search, which is similar to federated search, but operates in cooperative environments. Most importantly, those work do not target diversification in selecting relevant verticals (or sources) directly.
On the other hand, result diversification has been a popular research topic in ad hoc search. Its goal is to make a trade-off between relevance and novelty in ranking documents [38, 5, 1, 6, 26]. In order to achieve the desired effect of covering sufficient aspects of a user query (so that user will likely find the sought information), diversification algorithms target on discovering novel aspects that have not been covered in the ranked list, and reducing the redundancy information shared between multiple documents.
The earlier generation of diversification algorithms do not explicitly consider multiple aspects of a query [5, 38]. Instead, they build the ranked list from top to bottom, and make a choice of whether to include a document based on its similarity with existing documents in the list. More recent diversification algorithms directly incorporate query aspects into consideration. Agrawal et al. proposed the use of tax-

614

onomy to classify query aspects, in order to discover novel and redundant information [1]. Carterette and Chandar directly optimized the ranked list with respect to evaluation measures based on diversity [6]. Santos et al. proposed the eXplitcit Query Aspect Diversification (xQuAD) method, which estimates the surplus information that a document can add to a ranked list, using the probability of relevance with respect to all aspects of the query [26]. Recently, Dang and Croft proposed a different view on diversification by preserving the proportionality of document presence with respect to each aspect of the query [16]. Their proposed algorithm PM-2 has proven to achieve superior performance over several other algorithms of the same category. Other recent work on search result diversification include a combination of implicit and explicit topic representations [19], personalized diversification [34], and explicit relevance model [35].

3. DIVERSIFICATION METRICS IN RESOURCE SELECTION FOR FEDERATED SEARCH
There exists several standard diversification metrics for ad hoc search. However, no evaluation metric has been developed to compare the diversification results of different resource selection algorithms in federated search. This section proposes a new metric in order to fill that gap. Our metric is based on the popular R-metric [4] for resource selection in federated search. It is calculated as the ratio between the number of relevant documents contained in sources selected by a particular algorithm, over the number of relevance documents in sources selected by an ideal algorithm. In particular, the R-metric is defined as:

Rk =

k i=1

Ei

k i=1

Bi

where Ei denotes the number of relevant documents of the i-

th source according to the ranking E by a particular resource

selection algorithm, and Bi denotes the same quantity with

respect to the optimal ranking B. In this case, the optimal

ranking B should order sources by the true number of their

relevant documents. We adopt the idea to our new generic

metric, which is called R-based diversification metric, as fol-

lows.

M(optimal ranking of documents in S) RM(S) = M(optimal ranking of documents in all sources)

where S is the set of selected sources for comparison and M is a diversity metric of a ranked list of documents such as ERR-IA, -nDCG, Prec-IA, S-Recall and NRBP. The optimal ranking of documents in S is the list that achieves the best score with respect to metric M. For most of the aforementioned metrics, finding the optimal ranked list is an NP-hard problem, but this can be acceptably approximated by a greedy algorithm (i.e., repeatedly select the next document that maximizes the metric given the current ranking; cf. [8, 13]). The intuition of the proposed metric is that, if we can select a minimal set of sources that contains enough diversified documents to reach the optimal measure, then the R-based diversification metric is maximized to 1. Otherwise, it gives us an estimation of how far our selected sources are from the optimal ones.
Like R-metric, the proposed R-based diversification metric is independent of the retrieval algorithm utilized by each

source. The R-based diversification metric returns 1 if all available sources are selected. In general, for comparison between different resource selection algorithms, the maximum number of sources is determined beforehand.
4. TWO APPROACHES FOR DIVERSIFICATION IN RESOURCE SELECTION
This section proposes two approaches for selecting diversified information sources. The first approach extends the ReDDE framework by ranking sample documents with consideration to diversity. The second approach estimates the source relevance to each aspect of the query and ranks the sources based on the estimations of their aspect relevance.
4.1 Diversification on Sample Documents under ReDDE framework
We first describe the Relevant Document Distribution Estimation (ReDDE) framework [32] for ranking sources based on the centralized sample database. In this framework, a given query is issued to the centralized sample database to retrieve a ranked list of sample documents. ReDDE makes an assumption that each sample document in this list represents a number of (unseen) documents from the source that it belongs to. Based on that, a source score is calculated by aggregating all contribution from its sample documents. The amount of contribution is scaled up depending on the source's size. Original ReDDE assigns a constant score for all documents on the top part of the returned list, and multiplies that constant with the ratio of the estimated source size over the sample size. The obtained quantity is then used for aggregating source scores. The CRCS resource selection algorithm [27] follows the same approach, but varies the amount of contribution of each document by an exponential decay function, as documents further down the ranked list have less contribution to its source.
ReDDE and many other algorithms of the same family such as CRCS and SUSHI [33] utilize a ranking on the centralized sample database to estimate the relevance of available sources. For algorithms of this family, they mostly vary in the way of defining a utility function for each document in the list. Having said that, the original ranked list from the centralized sample database plays an important role. Nondiversification algorithms target on building a centralized ranked list that covers as many relevant documents to the query as possible. In many cases, this may not pay enough attention to sources that cover multiple query aspects. For diversification purpose, we should be careful when selecting a source that mainly contains documents relevant to an aspect that has been covered before.
The above observation suggests a way to achieve good search diversification results by constructing a ranked list that covers several aspects of the query. We call this approach Diversification approach based on sample Documents (DivD). Instead of building a centralized ranked list that focuses only on relevance, we construct a ranked list that offers more diversity. The goal is to reduce the contribution of a document (on behalf of its source) that may be relevant to the query, but offers less novelty in the overall ranking.
This approach can combine a wide range of resource selection algorithms with any diversification algorithm that has been developed before, for instance, PM-2 [16] and xQuAD [26], which were mentioned in Section 2. A typical example

615

Algorithm 1 Diversification Approach based on Sample Documents using ReDDE and PM-2
1: Initialize scores of all sources to 0 2: Rank documents of the centralized sample database by
an effective retrieval algorithm, e.g. Indri 3: Rerank that list by PM-2 4: for each document on top of the ranked list do 5: Add a constant score c to the source containing the
document 6: end for 7: return the ranked list of all sources based on their
scores

Algorithm 2 Source-based Diversification using ReDDE and PM-2
1: Rank all sources using standard ReDDE 2: for each aspect qi of query q and each source sj do 3: Estimate P (sj|qi), the probability of relevance of
source sj with respect to qi using the ReDDE algorithm 4: end for 5: Rerank the list obtained in step 1 using PM-2 algorithm with the estimated P (sj|qi) as inputs 6: return the ranked list of all sources based on their diversification scores

of combining the standard ReDDE and PM-2 is shown in Algorithm 1.
Some resource selection algorithms utilize the relevance score of each document in the centralized sample ranked list for ranking sources (e.g. ReDDE.top [2]). In the diversification approach that has been discussed so far, this relevance score can be replaced by the diversity score given by a diversification algorithm for ad hoc search. There are different interpretations about those diversity scores, depending on the assumptions made by the diversification algorithms. In our experiments, diversity scores can be used with most resource selection algorithms effectively.
4.2 Diversification Approach based on Sourcelevel Estimation
The second approach follows a different strategy than the first one. As presented in the previous section, the diversification approach based on sample documents works directly on the ranked list of sample documents, which is not a natural component for resource selection algorithms similar to CORI. Indeed, several resource selection algorithms utilize summary statistics of a source to estimate its relevance to a query. It is not straightforward to apply a diversification method based on individual sample documents for those kinds of algorithms.
This paper proposes another diversification approach for resource selection that operates at the source level. More specifically, many existing diversification algorithms for ad hoc search rank documents by estimating their relevance with respect to each aspect of the query, and then harvesting this information in order to produce a ranked list that balances multiple query aspects. It is possible to design a similar process that uses the estimated source relevance with respect to each aspect of the query. More specifically, instead of building a diversified list of documents for a given query as in ad hoc search, we can build a diversified list of available sources in a similar manner. All estimations with respect to the documents can be replaced by estimations with respect to the sources. This resource selection approach for results diversification is called Diversification approach based on Source-level estimation (DivS).
An important step in DivS is therefore to compute the probability of relevance of a source with respect to a query aspect. Many existing resource selection algorithms are able to provide such information. CORI can directly provide a relevance estimation based on the big document assumption. ReDDE and CRCS do not provide direct estimations, but it is possible to use their source scores aggregated from sample documents for such a purpose. An example of the

diversification approach based on source-level estimation using ReDDE and PM-2 algorithms is presented in Algorithm 2.
Compared with diversification approach based on sample documents, the diversification approach based on sourcelevel estimation can work with a wider range of resource selection algorithms. On the other hand, it requires multiple runs of a resource selection algorithm for all different aspects of a query, which is more time consuming than the former approach. In real-world applications, it is possible to design a parallel solution for multiple resource selection runs to speed up the process.
5. A CLASSIFICATION APPROACH FOR COMBINING DIVERSIFICATION RESULTS
The two diversification approaches based on sample documents and source-level estimation both utilize a specific resource selection algorithm and a diversification algorithm for ad hoc search. It is possible to combine the results of multiple resource selection algorithms for search result diversification, which may provide better results by modeling complementary results from different algorithms. This section proposes a learning based classification approach. With some training information, this method can learn how to combine evidence supporting available sources from different resource selection algorithms for result diversification.
In particular, we adapt the classification-based approach that has been used for both vertical search [2] and federated search [20]. For collecting training information in learning the classification model, this approach generates a pseudorelevant judgment of a source given a query by counting the number of relevant documents that the source contains. If the number is higher than some threshold value  , the source is considered to be relevant. In this paper, the training dataset for result diversification consists of multiple instances, each of them represents a pair of a source and a query aspect. A source is considered relevant to a query aspect if it contains at least one document relevant to that aspect of the query. For each pair of a source and a query aspect, we utilize the source score information from the following resource selection algorithms as features:
· ReDDE.top [2] This is quite similar to the original ReDDE algorithm, but uses the relevance scores of documents in calculating source scores, instead of a step function as in traditional ReDDE. ReDDE.top has

616

been chosen since it provides more consistent results than ReDDE in our dataset.
· CRCS [27] with exponential decay function for estimating probability of relevance of sample documents. This method has been reported by several studies to be better than the linear decay version [33].
· Big Document This is a traditional approach that collapses all sample documents of a source into a big document, which represents this source and is used for resource selection. Our retrieval algorithm for this approach is Indri [23].
· CORI [4] CORI is another type of big document selection approach with a different tf.idf weighting scheme. It shares the same assumption that considers each source as a big document of its sample documents.

More specifically, the learning based approach attempts to naturally integrate evidence from two different views of resource selection algorithms, one based on big document assumption (Big Document and CORI), and the other based on aggregated information of sample documents (ReDDE.top and CRCS). In the experimental section, we will provide more analysis and some examples about why this combination strategy may outperform each individual method.
All the features provided by the above algorithms are normalized for each query in order to achieve more consistency. Given the training dataset, it is possible to design a learning method that estimates P (s|qk) for a new query aspect qk. We choose logistic regression model as it has been shown to be among the best in many practical applications such as text categorization [18].
Let sji be the binary variable that indicates the relevance of the i-th source to the query qj, i.e. sji = 1 indicates relevance, and sji = 0 indicates otherwise. Let fij be the vector of all features returned by the resource selection algorithms mentioned above. We can then represent the relevance probability of source si given fij by a sigmoid function :

P (sji

=

1|qj )

=

exp(fij · w) 1 + exp(fij · w)

=

(fij

· w)

where w denotes the combination weight vector. Learning the combination weight w can be conducted by
maximizing the log-likelihood function using the iterative reweighted least squares method. The learned parameter can be then used to estimate the relevance probability P (s|qk) for any particular aspect of a new user query. This probability becomes inputs for the diversification approach based on source-level estimation to rank the sources. Our hypothesis is that, if the learning model can provide a more accurate estimation than those produced by a single resource selection algorithm, we can expect the learning based approach to generate more accurate results. This approach is denoted as LR-DivS, as it applies logistic regression within the diversification approach based on source-level estimation.

6. EXPERIMENTAL METHODOLOGY

Dataset.
The experiments in the paper have been conducted on the federated search testbed of the Clueweb English dataset.

Table 1: Document statistics of the federated search testbed based on Clueweb English

# of

total # min # max # avg #

sources of docs of docs of docs of docs

2,780 151,161,188 48 3,417,805 54,364.27

The Clueweb dataset1 is a large collection of web pages that has been used in several official tasks in the Text REtrieval Conference (TREC) tracks. Furthermore, the available information of queries with multiple aspects and the corresponding relevance judgment has enabled the evaluation of search result diversification. The federated search testbed derived from Clueweb is publicly available2 as an attempt to offer a large and realistic testing environment for federated search. This collection contains 2, 780 information sources and about 151 million documents, which is much larger than most other testbeds that have been previously used in federated search. Information sources are created by collecting web documents of the same domains (e.g. blogspot.com, about.com), and the Wikipedia documents are clustered into 100 collections using the K-means clustering algorithm with two passes/iterations. More statistics information of this dataset is given in Table 1. It is noticed that Clueweb has also been used in distributed environment, albeit in a different problem setting [21].
We are aware of a recent dataset that has been proposed for federated search in web search environment [24]. However, this dataset does not provide relevance judgments on multiple aspects of a query, and thus does not fully support experiments of this research at the moment.
Each information source in the testbed has been assigned a retrieval algorithm, which was chosen from a set of algorithms such as Inquery, Language Model and tf.idf in a round-robin manner. This strategy simulates the behavior that information sources in real world applications may use different types of retrieval algorithms. In order to build the centralized sample database, 300 documents have been sampled from each source via the source's specific retrieval algorithm. The Indri retrieval algorithm [23] was used in all retrieval processes on the centralized sample database.
The queries used in our experiments consist of 148 queries from the TREC Web Track 2009[9], 2010[10], and 2011[11].
Diversification Algorithm.
Diversity by Proportionality [16] (PM-2) and eXplicit Query Aspect Diversification (xQuAD) [26], which are among the state-of-the-art diversification algorithms, have been examined in our study. In particular, we notice that the performance of PM-2 tends to be better than xQuAD in most of the metrics, thus we only report our results based on PM-2. In our implementation of this algorithm, we have chosen to rerank the top K = 500 documents in the centralized sample database, as well as in the final step of diversifying results from the centralized complete database (i.e., all documents in available sources). The parameter  in PM-2 is set to be 0.5 in all settings.
1http://boston.lti.cs.cmu.edu/Data/clueweb09 2http://www.cs.purdue.edu/homes/dthong/clueweb/

617

An important component of PM-2 and many other search result diversification algorithms is the estimation of P (d|qi), which is the relevance score of a document d with respect to a particular aspect qi of a query. These aspects are usually not available in real-world applications. We follow the work of [16, 26] to report our results on two scenarios: all the aspects of a query are provided; or we can retrieve the aspects from a commercial search engine such as Google or Bing. For the first scenario, we use subtopics that come with TREC queries as aspects. For the second scenario, we send the original query to the search engine (Google in our experiments), and adopt its suggestions as query aspects. The second set contains 144 queries, as 6 original queries do not have search engine's suggestions at the moment of our work. We name the first scenario "Given Subtopics" and the second "Suggestions" in the results presented in Section 7.
Resource Selection Algorithms.
A set of commonly used resource selection algorithms as described in section 5 has been utilized in the new research of result diversification in resource selection. They consist of ReDDE.top, CRCS with exponential decay function, Big Document and CORI.
With CRCS, the top 500 sample documents returned by the centralized sample database are considered. The exponential decay function of CRCS makes it more stable on the Clueweb collection, which exhibits a highly skewed distribution of source sizes. On the other side, ReDDE.top is more sensitive to noise in such an environment, as a notso-relevant sample document from a really big source may result in too much bias in favor of that source and affects the final ranked list. Therefore, for ReDDE.top, we set the number of top sample documents for each query to be smaller than CRCS's, chosen from the set {50, 100, 150, · · · }. We report the results using top 50 documents for ReDDE.top as it provides the most consistent performance.
To name the different methods, we use a prefix "D" for a diversification resource selection algorithm to indicate an approach based on sample documents, and a prefix "S" to indicate an approach based on source-level estimation. As discussed before, the Big Document and CORI algorithms have only the S versions. For all methods reported in the next section, we select up to 10 sources for each query.
Training and Testing.
All the proposed resource selection algorithms for search result diversification do not need any training data except the final approach of combining multiple resource selection algorithms in a learning model (LR-DivS). Therefore, for the algorithms other than LR-DivS, we report the results on all 148 queries from TREC Web Track 2009-2011. For LR-DivS, since it requires a training dataset, we use queries with TREC id less than or equal to 75 for training, and the rest for testing. We also report the results on the testing set of all other algorithms for comparison. For the scenario with suggested aspects of user queries, since there is no corresponding relevance judgment, we train our model using the provided query subtopics/aspects and their corresponding relevance judgments. However, in the testing phase, the model is applied with features derived from the suggested aspects, i.e., we estimate P (s|qi) where qi is a query aspect suggested by a commercial search engine. Finally, we evaluate all approaches with respect to TREC's provided

subtopics/aspects. This strategy is consistent with the evaluation process in TREC Web Track [11].
Evaluation Metric.
The proposed new research has been evaluated at two levels: source selection and federated document retrieval.
· Diversification results with R-based diversification metric for source selection: We evaluate the resource selection results using the R-based diversification metric described in Section 3. In particular, five popular metrics in result diversification such as ERRIA[7], -nDCG[13], NRBP[12], P-IA (intent aware precision [1]) and S-Recall (subtopic recall, for the number of subtopics/aspects covered by top documents) have been adopted with the R metric in resource selection and used in the experiments.
· Diversification results for federated document retrieval: To make the evaluation independent from a specific result merging algorithm, the Indri algorithm is used to perform document retrieval on the centralized complete database. Only documents from the selected sources for each query have been retained, which is consistent with prior research in [32]. This ranked list of documents is reranked again using the PM-2 algorithm. We then evaluate the final ranked list by the five metrics mentioned above as ERR-IA, -nDCG, NRBP, P-IA and S-Recall. All of these metrics are computed at the top 20 documents, which is consistent with the official TREC evaluation of search result diversification for ad hoc search [11] and consistent with the commonly used high-precision metric in federated document retrieval.
7. EXPERIMENTAL RESULTS
An extensive set of experiments has been conducted for evaluating several approaches proposed in this paper, which are: the approach based on sample documents (DivD), the approach based on source-level estimation (DivS) and the learning based approach (LR-DivS). We conduct experiments on two levels for different purposes: source-level results for resource selection and document-level results for federated document retrieval. More specifically, the first subsection compares the performance of a diversification approach with standard resource selection algorithms. The second set of experiments in 7.2 compares multiple resource selection algorithms adapting DivD and DivS approaches. The third set of experiments in 7.3 demonstrates the advantage of the learning based approach (LR-DivS) over approaches with a single resource selection algorithm. The last set of experiments in 7.4 compares the document-level diversification results across all proposed approaches.
7.1 Diversification versus Standard Resource Selection Algorithms
This subsection compares the performance of two standard resource selection algorithms ReDDE.top and CRCS with their diversification counterparts at source selection level. In particular, we choose to study the first diversification approach based on sample documents (DivD) in this subsection as they are more related with standard ReDDE.top and CRCS algorithms, while more results of both diversification approaches will be presented shortly.

618

Table 2 shows the performance using all the R-based diversification metrics described in the previous section. Without diversification, it can be observed that the standard CRCS significantly outperforms standard ReDDE.top in all metrics. This may be attributed to the fact that CRCS generally selects more relevant sources to the query, which leads to a wider range of aspects being covered. The advantage of CRCS may come from using the exponential decay function for document utility, which tends to be better than using document score as utility in ReDDE.top. When the document-based diversification approach is applied, it further increases the performance of the standard algorithms: D-ReDDE.top significantly outperforms ReDDE.top in its capacity of selecting diversified sources. As for CRCS, its diversification version (i.e., D-CRCS) is also consistently better than the standard CRCS algorithm. The same observation can be seen in both scenarios when the query aspects are given, or suggested by Web search.
7.2 Diversification with Different Resource Selection Algorithms
This subsection studies the performance of the two proposed resource selection approaches (i.e., DivD and DivS) using several resource selection algorithms, including ReDDEtop, CRCS, Big Document and CORI. The results are presented in Table 3. In all settings, the standard Big Document and standard CORI algorithms are outperformed by the other methods. Furthermore, both S-Big Document and S-CORI, which are under the same assumption of collapsing sample documents within a source, are inferior to SReDDE.top and S-CRCS. These results indicate that ReDDEtop and CRCS tend to be more effective in resource selection for result diversification than Big Document and CORI, which is consistent with previous research in federated search for resource selection without diversification.
Both DivD and DivS approaches produce comparable results when applied to ReDDE.top and CRCS. The D-CRCS version based on sample documents is better than its counterpart based on source-level estimation (i.e., S-CRCS), whereas the contrary is observed for ReDDE.top. In case of ReDDE.top, the difference is significant, which may be explained by the fact that when the original ranked list is short (only 50 for ReDDE.top), it is more difficult for the diversification algorithm to find a document that covers many query aspects, rather than finding a source that covers many aspects.
The results using the provided query aspects and suggested ones reveal an interesting observation. We notice that the performance of the two settings are quite comparable with little difference. One possible explanation is that, in federated environment, it may not need a perfect set of query aspects for selecting a diversified set of information sources, as sources are already somehow divided by different types of semantic topics. Since our goal is to select sources that can cover as many query aspects as possible, the resource selection algorithms can do a reasonably good work as long as the suggestions of query aspects provide some meaningful interpretations of different aspects of the query.
7.3 Classification Approach for Combining Diversification Results
This subsection compares the performance of the learning based classification approach with all other diversification approaches mentioned above. Since the classification ap-

proach requires a set of training queries and does the testing on another set, we also report the results of all previous methods on the testing queries for comparison. Table 4 presents the results. The performance of the standard resource selection algorithms and their diversification counterparts are better on the set of testing queries than on the set of all queries, due to the particular division of training and testing queries. The comparison between standard algorithms' performance and those of diversification approaches on the testing queries raises similar observations as mentioned in the previous subsections 7.1 and 7.2.
It can be seen that the classification approach provides the best performance over all metrics. It can be attributed to the fact that the learning based classification approach can harness the advantage of different algorithms, and combine them in an effective way. A typical example from our training set is the query "Obama family tree" with its provided subtopic "Find the TIME magazine photo essay Barack Obama's Family Tree". For ReDDE.top and CRCS, it is almost impossible to find a sample document containing all the keywords of the subtopic/aspect, if such a document does not exist in the sample database. On the other hand, Big Document and CORI can provide some useful hints for selecting sources by looking at all sample documents from each source as whole. For example, several different sample documents of a source may contain various parts of the query. This is particularly useful for sources that cover a wide range of topics, for instance, Wikipedia. Another example is the query "Volvo" with its provided subtopic "Find a Volvo dealer". Big Document and CORI can give hint to the sources that contain the words "Volvo" and "dealer" from different sample documents. For those sources, the classification approach can utilize the complementary results for improving search results diversification.
7.4 Diversification Results of Federated Document Retrieval
This subsection compares the search diversification results in document-level of federated document retrieval. Given the top ten sources selected by the proposed algorithms for each query, the final ranked list generated from the sources is evaluated. One set of results compares algorithms without training over all queries. The other set of results is included for the test queries to compare LR-DivS with other algorithms. We omit the results of Big Document and CORI on the first set due to space limitation.
Table 5 provides the results on all queries. The standard ReDDE.top algorithm falls behind the other models in its diversification capacity. Among all the methods without training, D-CRCS consistently outperforms the other methods, which is consistent with its performance on the R-based diversification metrics in the source-level.
Table 6 provides the results on the test queries. Again, the same trend with algorithms without training can be observed. When some training information is available to learn how to combine multiple evidence, the classification approach LR-DivS consistently provides the best documentlevel diversification performance among all models.
8. CONCLUSION AND FUTURE WORK
Resource selection is an important research problem in federated search for selecting a small number of relevance sources for a given query. Various algorithms have been pro-

619

Table 2: R-based diversification metric of resource selection on all 148 queries. Symbols  and  indicate statistical significance under paired t-test with respect to ReDDE.top baseline and CRCS baseline (p < 0.05).

Given Suggestions
Subtopics

Baseline DDiv Baseline DDiv

ReDDE.top CRCS D-ReDDE.top D-CRCS ReDDE.top CRCS D-ReDDE.top D-CRCS

R-ERR
0.549 0.677 0.624 0.699
0.542 0.673 0.614 0.698

R--nDCG
0.529 0.652 0.598 0.672
0.522 0.649 0.589 0.671

R-NRBP
0.566 0.698 0.645 0.721
0.558 0.694 0.635 0.720

R-P-IA
0.406 0.484 0.438 0.499
0.396 0.477 0.434 0.491

R-S-Recall
0.545 0.658 0.612 0.674
0.541 0.659 0.604 0.677

Table 3: R-based diversification metric on resource selection on multiple diversification approaches. Letters r, c, R, C, B, O indicate statistical significance under paired t-test to D-ReDDE.top, D-CRCS, S-ReDDE.top, S-CRCS, S-BigDoc, and S-CORI respectively (p < 0.05).

Suggestions Given Subtopics

DDiv SDiv DDiv SDiv

D-ReDDE.top D-CRCS S-ReDDE.top S-CRCS S-BigDoc S-CORI
D-ReDDE.top D-CRCS S-ReDDE.top S-CRCS S-BigDoc S-CORI

R-ERR 0.624B 0.699r B O 0.680r B O 0.675r B O 0.490 0.569B
0.614B 0.698r B O 0.677r B O 0.673r B O 0.490 0.566B

R--nDCG 0.598B O 0.672r B O 0.656r B O 0.649r B O 0.468 0.541B
0.589B 0.671r B O 0.652r B O 0.647r B O 0.469 0.538B

R-NRBP 0.645B 0.721r B O 0.701r B O 0.698r B O 0.506 0.591B
0.635B 0.720r B O 0.698r B O 0.695r B O 0.508 0.587B

R-P-IA 0.438B O 0.499rC B O 0.496rC B O 0.462r B O 0.346 0.351B
0.434B O 0.491rC B O 0.485rC B O 0.458B O 0.342 0.347B

R-S-Recall 0.612B 0.674r B O 0.664r B O 0.664r B O 0.486 0.561B
0.604B 0.677r B O 0.666r B O 0.665r B O 0.490 0.562B

Table 4: R-based diversification metric of resource selection on multiple diversification approaches on test queries. Symbols , , r, c, R, C, B, O indicate significant improvement under paired t-test to ReDDE.top, CRCS, D-ReDDE.top, D-CRCS, SReDDE.top, S-CRCS, S-BigDoc, and S-CORI respectively. (p < 0.05).

Given Subtopics

Baseline DDiv
SDiv LR-DivS Baseline DDiv
SDiv LR-DivS

ReDDE.top CRCS D-ReDDE.top D-CRCS S-ReDDE.top S-CRCS S-BigDoc S-CORI
ReDDE.top CRCS D-ReDDE.top D-CRCS S-ReDDE.top S-CRCS S-BigDoc S-CORI

R-ERR
0.618 0.769rBO 0.711B 0.778rBO 0.775rBO 0.786rBO 0.615
0.653 0.873rcRCBO
0.614 0.768BO 0.724B 0.781rBO 0.799rBO 0.787rBO 0.614
0.647 0.821rCBO

R--nDCG
0.604 0.751rBO 0.694B 0.762rBO 0.757rBO 0.769rBO 0.594
0.634 0.853rcRCBO
0.600 0.751BO 0.705B 0.765rBO 0.779rBO 0.771rBO 0.593
0.629 0.805rCBO

R-NRBP
0.630 0.783rBO 0.725B 0.791rBO 0.789rBO 0.800rBO 0.632
0.669 0.889rcRCBO
0.626 0.782BO 0.739B 0.794rBO 0.814rBO 0.801rBO 0.632
0.662 0.834rBO

R-P-IA
0.461 0.550rBO 0.507BO 0.557rBO 0.567rcCBO 0.544rBO 0.415
0.396 0.705rcRCBO
0.463 0.555BO 0.525BO 0.558rBO 0.590rcCBO 0.550rBO 0.411
0.396 0.629rcRCBO

R-S-Recall
0.635 0.768BO 0.726B 0.782rBO 0.778rCBO 0.802rRBO 0.621
0.676 0.856rcRCBO
0.629 0.768BO 0.735B 0.787rBO 0.800rBO 0.803rBO 0.624
0.672 0.827rBO

Suggestions

620

Table 5: Diversification results on document retrieval on all queries. The symbols , , r, c, R, C indicate statistical significance under paired t-test to ReDDE.top baseline, CRCS baseline, D-ReDDE.top, D-CRCS, S-ReDDE.top, and S-CRCS respectively (p < 0.05).

Suggestions Given Subtopics

Baseline DDiv SDiv Baseline DDiv SDiv

ReDDE.top CRCS D-ReDDE.top D-CRCS S-ReDDE.top S-CRCS
ReDDE.top CRCS D-ReDDE.top D-CRCS S-ReDDE.top S-CRCS

ERR
0.246 0.380rR 0.319 0.383rR 0.357r 0.365r
0.249 0.339r 0.304 0.356rR 0.323 0.342r

-nDCG
0.276 0.406r 0.345 0.410rR 0.387r 0.395r
0.277 0.378r 0.333 0.390rR 0.359r 0.376r

NRBP
0.226 0.365rR 0.304 0.365rR 0.340r 0.346r
0.230 0.315 0.286 0.335rR 0.300 0.320

P-IA
0.111
0.152rR 0.123 0.151rR 0.141r 0.142r
0.114 0.157rR 0.129 0.157rR 0.144r 0.147r

S-Recall
0.409 0.530r 0.477 0.535r 0.526r 0.534r
0.400 0.535r 0.469 0.530rR 0.512r 0.530r

Table 6: Diversification results on document retrieval on test queries. The symbols , , r, c, R, C, B, O indicate statistical significance under paired t-test to ReDDE.top baseline, CRCS baseline, D-ReDDE.top, D-CRCS, S-ReDDE.top, S-CRCS, S-BigDoc, and S-CORI respectively (p < 0.05).

Given Subtopics

Baseline DDiv
SDiv LR-DivS Baseline DDiv
SDiv LR-DivS

ReDDE.top CRCS D-ReDDE.top D-CRCS S-ReDDE.top S-CRCS S-BigDoc S-CORI
ReDDE.top CRCS D-ReDDE.top D-CRCS S-ReDDE.top S-CRCS S-BigDoc S-CORI

ERR

0.304 0.478rBO 0.422 0.485rBO 0.463BO 0.473rBO 0.370

0.368

0.515

 rRCBO

0.337
0.469rBO 0.434B 0.487rBO 0.462BO 0.472BO 0.347

0.370 0.509rRCBO

-nDCG
0.345 0.512rBO 0.457 0.523rBO 0.501rBO 0.515rBO 0.390
0.393 0.555rRCBO
0.373 0.509rBO 0.471BO 0.524rBO 0.504rBO 0.512rBO 0.374
0.400 0.543rRCBO

NRBP
0.278 0.457rBO 0.400 0.460rBO 0.438BO 0.446BO 0.359
0.354 0.489rRCBO
0.311 0.445BO 0.410B 0.463rBO 0.435BO 0.448BO 0.330
0.349 0.487rRBO

P-IA
0.151 0.217rRBO 0.178O 0.219rRBO 0.200rBO 0.206rBO 0.151
0.129 0.243rcRCBO
0.167 0.235rBO 0.202O 0.233rBO 0.224BO 0.219BO 0.157
0.150 0.249rBO

S-Recall
0.509 0.668BO 0.622B 0.677rBO 0.678rBO 0.687rBO 0.519
0.545 0.722rRBO
0.518 0.671BO 0.637B 0.675BO 0.674BO 0.675BO 0.511
0.560 0.690rBO

Suggestions

posed for resource selection in federated search, but limited attention has been paid to result novelty and diversification, which affects the effectiveness of existing algorithms. As far as we know, this paper proposes the first piece of research for incorporating search result diversification in resource selection for federated search.
A family of new evaluation metrics is first proposed for measuring search result diversification in resource selection, which combines some popular diversification metrics in ad hoc search with the recall-based evaluation metric in resource selection. Two general approaches are then proposed for diversification in selecting relevant sources. The first approach is based on sample documents, which ranks sample documents with respect to result diversification, and then

utilizes the ReDDE framework for ranking the sources. The second approach is based on source-level estimation, which directly ranks each information source as a whole for result diversification. Furthermore, a learning based classification approach is proposed to combine multiple resource selection algorithms for more accurate diversification results.
An intensive set of empirical studies has been conducted to evaluate the proposed research on the Clueweb federated search dataset. Both the approach based on sample documents and on source-level estimation can outperform traditional resource selection algorithms in result diversification in both source-level for resource selection and in the document-level for federated document retrieval. Moreover, the learning based approach, which combines outputs

621

of multiple resource selection algorithms for result diversification, has been shown to generate the best results when some training data is available.
There are several possible directions to pursue in the future. The learning based method in this paper utilizes a simple model for combining outputs of multiple algorithms for result diversification, while a more sophisticated learning method may be more effective. Furthermore, it is an interesting topic to design new result merging algorithms with the focus on result diversification.
9. ACKNOWLEDGMENTS
This work is partially supported by NSF research grants IIS-0746830, CNS- 1012208 and IIS-1017837. This work is also partially supported by the Vietnam Education Foundation, the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370, and a travel grant from the ACM Special Interest Group on Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.
10. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of the Second ACM International Conference on Web Search and Data Mining pages 5­14, 2009.
[2] J. Arguello, J. Callan, and F. Diaz. Classification-based resource selection. CIKM'09, pages 1277­1286, 2009.
[3] M. Baillie, M. Carman, and F. Crestani. A multi-collection latent topic model for federated search. Information Retrieval, 14(4):390­412, 2011.
[4] J. Callan. Distributed information retrieval. Advances in Information Retrieval, pages 127­150, 2000.
[5] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR'98, pages 335­336, 1998.
[6] B. Carterette and P. Chandar. Probabilistic models of ranking novel documents for faceted topic retrieval. In CIKM'09, pages 1287­1296, 2009.
[7] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In CIKM'09, pages 621­630. ACM, 2009.
[8] H. Chen and D. Karger. Less is more: probabilistic models for retrieving fewer relevant documents. In SIGIR'06, pages 429­436, 2006.
[9] C. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 Web Track. TREC, pages 1­9, Jan. 2009.
[10] C. Clarke, N. Craswell, I. Soboroff, and G. V. Cormack. Overview of the TREC 2010 Web Track. TREC, pages 1­9, Jan. 2010.
[11] C. Clarke, N. Craswell, I. Soboroff, and E. Voorhees. Overview of the TREC 2011 Web Track. pages 1­9, Jan. 2011.
[12] C. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. Advances in Information Retrieval Theory, pages 188­199, 2009.
[13] C. L. A. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR'08, pages 659­666, 2008.
[14] N. Craswell. Methods for Distributed Information Retrieval. PhD thesis, The Australian National University, 2000.
[15] F. Crestani and I. Markov. Distributed Information Retrieval and Applications. In Proceedings of ECIR, Jan. 2013.

[16] V. Dang and W. B. Croft. Diversity by proportionality: an election-based approach to search result diversification. In SIGIR'12, pages 65­74. ACM, 2012.
[17] N. Fuhr. Resource Discovery in Distributed Digital Libraries. In In Digital Libraries '99: Advanced Methods and Technologies, Digital Collections, 1999.
[18] A. Genkin, D. D. Lewis, and D. Madigan. Large-scale Bayesian logistic regression for text categorization. Technometrics, 49(3):291­304, 2007.
[19] J. He, V. Hollink, and A. de Vries. Combining implicit and explicit topic representations for result diversification. In SIGIR'12, pages 851­860. ACM, 2012.
[20] D. Hong, L. Si, P. Bracke, M. Witt, and T. Juchcinski. A joint probabilistic classification model for resource selection. SIGIR'10, pages 98­105, 2010.
[21] A. Kulkarni and J. Callan. Document allocation policies for selective searching of distributed indexes. CIKM'10, pages 449­458, 2010.
[22] I. Markov, L. Azzopardi, and F. Crestani. Reducing the Uncertainty in Resource Selection. In Proceedings of ECIR, 2013.
[23] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Information Processing and Management, 40(5):735­750, 2004.
[24] D. Nguyen, T. Demeester, D. Trieschnigg, and D. Hiemstra. Federated Search in the Wild. In CIKM '12, pages 1874­1878, 2012.
[25] R. L. Santos, C. Macdonald, and I. Ounis. Aggregated search result diversification. Advances in Information Retrieval Theory, pages 250­261, 2011.
[26] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proceedings of the 19th international conference on World wide web, pages 881­890. ACM, 2010.
[27] M. Shokouhi. Central-rank-based collection selection in uncooperative distributed information retrieval. Advances in Information Retrieval, 2007.
[28] M. Shokouhi and L. Si. Federated Search. 2011.
[29] M. Shokouhi and J. Zobel. Federated Text Retrieval From Uncooperative Overlapped Collections. SIGIR'07, pages 789­790, 2007.
[30] M. Shokouhi and J. Zobel. Robust result merging using sample-based score estimates. ACM Transactions on Information Systems (TOIS), 27(3):1­29, 2009.
[31] L. Si and J. Callan. A semisupervised learning method to merge search engine results. ACM Transactions on Information Systems (TOIS), 21(4):457­491, 2003.
[32] L. Si and J. Callan. Relevant document distribution estimation method for resource selection. SIGIR'03, pages 298­305, 2003.
[33] P. Thomas and M. Shokouhi. Sushi: Scoring scaled samples for server selection. In SIGIR'09, pages 419­426. ACM, 2009.
[34] D. Vallet and P. Castells. Personalized diversification of search results. In SIGIR'12, pages 841­850. ACM, 2012.
[35] S. Vargas, P. Castells, and D. Vallet. Explicit relevance models in intent-oriented information retrieval diversification. In SIGIR'12, pages 75­84. ACM, 2012.
[36] J. Xu and W. B. Croft. Cluster-based language models for distributed retrieval. In SIGIR'99, pages 254­261, 1999.
[37] B. Yuwono and D. L. Lee. Server ranking for distributed text retrieval systems on the internet. In Proceedings of the Fifth International Conference on Database Systems for Advanced Applications (DASFAA), pages 41­50, 1997.
[38] C. X. Zhai, W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR'03, pages 10­17, 2003.
[39] K. Zhou, R. Cummins, M. Lalmas, and J. M. Jose. Evaluating aggregated search pages. In SIGIR'12, pages 115­124, 2012.

622

The Effect of Threshold Priming and Need for Cognition on Relevance Calibration and Assessment

Falk Scholer
School of Computer Science and Information Technology
RMIT University Melbourne, Australia
falk.scholer@rmit.edu.au

Diane Kelly, Wan-Ching Wu, Hanseul S. Lee
School of Information and Library Science
University of North Carolina Chapel Hill, NC, USA
dianek@email.unc.edu

William Webber
College of Information Studies University of Maryland
College Park, Maryland, USA
wew@umd.edu

ABSTRACT
Human assessments of document relevance are needed for the construction of test collections, for ad-hoc evaluation, and for training text classifiers. Showing documents to assessors in different orderings, however, may lead to different assessment outcomes. We examine the effect that threshold priming, seeing varying degrees of relevant documents, has on people's calibration of relevance. Participants judged the relevance of a prologue of documents containing highly relevant, moderately relevant, or non-relevant documents, followed by a common epilogue of documents of mixed relevance. We observe that participants exposed to only non-relevant documents in the prologue assigned significantly higher average relevance scores to prologue and epilogue documents than participants exposed to moderately or highly relevant documents in the prologue. We also examine how need for cognition, an individual difference measure of the extent to which a person enjoys engaging in effortful cognitive activity, impacts relevance assessments. High need for cognition participants had a significantly higher level of agreement with expert assessors than low need for cognition participants did. Our findings indicate that assessors should be exposed to documents from multiple relevance levels early in the judging process, in order to calibrate their relevance thresholds in a balanced way, and that individual difference measures might be a useful way to screen assessors.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and software--performance evaluation.
Keywords
Evaluation, assessors, relevance assessments, relevance behavior, need for cognition, threshold priming, order effects, calibration
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1. INTRODUCTION
Relevance is a fundamental concept in information retrieval (IR) and forms the basis of most methods of evaluating IR systems. Relevance assessments, however, are subject to human judgment. Different assessors may make different assessments of the relevance of the same document to the same topic, and one assessor may make different assessments of the same document at different times and under different conditions.
Relevance assessments are used for retrieval evaluation, either through reusable test collections such as those constructed by TREC and similar efforts, or through more ad-hoc evaluation. While differences in relevance assessments may have a limited effect on comparative system evaluation, they have a serious impact on the absolute evaluation of system effectiveness, as the level of agreement between human assessors places a practical upper bound on measurable system quality. This distortion is consequential in environments where absolute measures of retrieval completeness and accuracy are required, such as e-discovery, patent retrieval, and research literature surveys. Relevance assessments are also used to train text classifiers and learning to rank systems, and assessor variability may work to degrade the effectiveness of such systems.
While many causes of human variability in relevance assessment are inescapable, some are within the control of the evaluation designer [26]. One such factor is the order in which the assessor is asked to assess documents for relevance. A long sequence of irrelevant documents, for instance, might cause an assessor to lower their threshold of relevance, or alternatively to lose concentration and miss relevant documents--an effect evaluation designers could seek to counteract by seeding likely-relevant documents more evenly. The overall density of likely-relevant documents in the set of documents for assessment may be under the control of the evaluation designer, through sampling or pooling decisions. For instance, a uniform sampling of a document corpus will give a much lower density of relevant documents than a stratified sampling that weights samples towards retrieved or highly-ranked documents.
As assessors evaluate documents against a topic, whether of their own or of another's creation, they are building internal relevance models that guide their decision-making process about whether a document is relevant or not, and (where graded relevance is used) about the level of relevance to assign to documents. We posit that these resulting relevance models will develop differently depending on the relevance levels of documents that assessors encounter, and that this differential development of relevance models will result in different calibrations, or thresholds for relevance. We refer to this posited effect as threshold priming. For example, an as-

623

sessor who encounters few or no relevant documents would (under our hypothesis) have a lower threshold for relevance than a person who encounters a large number of highly relevant documents. Subsequently shown the same documents, the former assessor would tend to assign a higher relevance grade than the latter.
A further factor influencing assessment reliability that may be under the evaluator's control is the choice of the assessors themselves. Studies in the e-discovery field have found that there is a great deal of variability in assessor reliability, but this does not seem to correlate with training or (at least legal) expertise (Section 2.2). Perhaps more fundamental personality traits and capacities are at play. One possible trait identified in the psychological literature is need for cognition, a measure of the extent to which individuals enjoy engaging in concentrated intellectual activity. We believe that assessing relevance is an example of concentrated intellectual activity, since it requires sustained attention, close and careful reading, and good memory. There are standardized scales for need for cognition; if this trait is correlated with assessor reliability, it could provide a useful filter for assessor recruitment.
We investigate the hypotheses that threshold priming and need for cognition influence assessor relevance judgments by asking the following three research questions:
1. Does threshold priming, based on the relevance level of documents that are encountered early in the judging process, affect how an assessor assigns relevance to documents that are seen later on?
2. Does threshold priming affect how well an assessor is able to form a stable conception of relevance for a topic, based on the self-consistency of ratings over time, and the level of agreement between assessors?
3. Does the need for cognition (NFC) characteristic of an assessor predict how they assign relevance to documents, or how long they take to make judgments?
2. BACKGROUND
We first review research from the information behavior literature about defining and measuring relevance. This research has primarily focused on the behaviors of searchers working on search tasks (both assigned and natural). Next, we review work examining how assessors make relevance judgments when engaged in the development of test collections or similar evaluation tasks. Finally, we review work describing individual differences and relevance behavior, and specifically need for cognition.
2.1 Relevance
A trilogy of articles by Saracevic [20, 21, 22] provide a comprehensive overview and synthesis of relevance research spanning over 30 years. Saracevic [21] conceptualized five classes of relevance: (1) system or algorithm; (2) topical; (3) pertinence or cognitive; (4) situational; and (5) motivational or affective. In our research, we focus on topical relevance, since this is the type of relevance that is modeled by the relevance assessments that accompany most test collections. Topical relevance is defined as "an intellectual assessment of how an information object corresponds to the topical area required and described by the request(s) for information" [2, p. 915]. We are also concerned with cognitive relevance, as our research is motivated by the theory that as people encounter documents, they are calibrating their internal relevance models of the topic, which serve as a basis for subsequent assessments. This is related to the idea of psychological relevance, which posits that relevance assessments are made in relation to a person's current psy-

chological state: ". . . relevance judgments are a function of one's mental state at the time a reference is read. They are not fixed; they are dynamic" [12, p. 612]. Specifically, we are interested in how the order in which documents are encountered impacts people's relevance calibrations and subsequent assessments.
While it is generally accepted that order effects occur during document assessment, Saracevic [22] notes that only a few studies have systematically addressed this issue. The results of these studies have been inconsistent, likely due to variability in research methods. Eisenberg and Barry [9] experimented with two document orders for a set of 15 documents and a single query: one ranked high to low relevance, the other low to high. The authors found that in the high­low condition, people underestimated the relevance of documents at the higher end (assigning lower relevance scores to the highly relevant documents), while those in the low­high condition overestimated the relevance of documents in the low to middle range (assigning higher relevance scores to less relevant documents). This behavior was explained as a "hedging phenomenon" (p. 296), where participants working with a fixed scale and an unknown set of stimuli are reluctant to initially assign high or low scores to items. Purgaillis Parker and Johnson [19] did not find a systematic order effect in their study, but they used citations, rather than full-text, and like Eisenberg and Barry [9] only experimented with sets of 15 objects. Later, Huang and Wang [14] found that set size matters, with order effects being detected for sets of size 15, 30, 45 and 60, but not 5 and 75. They also found that relevance scores assigned by people in the low to high treatment were higher than those assigned by people in the high to low treatment, which is consistent with the results of Eisenberg and Barry [9].
In our research, we also focus on degree of relevance. Degree of relevance refers to the rating or indication of the relevance value given to documents. Borlund [2] provides an overview of different degrees of relevance including binary relevance, tripartite relevance, scale-based relevance and graded relevance. Borlund observes that degrees of relevance can also refer to whether the object as a whole is considered relevant, or only a part of it. Saracevic [22] indicates that previous research shows that people prefer to judge document relevance on a continuum, and comparatively. If people construct internal relevance models as they assess documents, then it seems reasonable to assume that the degree of relevance of the documents encountered will impact the formation of people's relevance thresholds, or the amount of evidence needed to associate documents with each degree of relevance.
In addition to Saracevic's conceptualizations, many researchers have also documented the various criteria (e.g., novelty, recency, depth) that people use when making relevance assessments [1, 25] and observed that the process of assigning relevance to documents is dynamic and inter-dependent, and impacted by a number of variables, including how much one knows about the topic and the search stage [6, 27]. There has also been a persistent body of research about how assessors make relevance judgments for test collections or other system evaluations. This work is reviewed in more detail in the next section.
2.2 Assessor Behavior
High levels of assessor disagreement about document relevance have been observed in a number of studies. Voorhees [29] reports a mean positive agreement between TREC assessors of 0.58. (Positive agreement can also be interpreted as the F1 score that one assessor would achieve if evaluated by the other assessor.) Oard and Webber [18] survey a number of studies on assessor agreement, observing positive agreement between 0.33 and 0.76. Analyzing TREC assessment data, Scholer et al. [23] find that a single assessor

624

will make at different times a different assessment of the binary relevance of the same document between 15% to 19% of the time, and 19% to 24% of the time for trinary relevance. Voorhees [29] finds that assessor disagreement has a limited effect on relative measures of system quality, but observes that it sets an upper bound on the practically measurable absolute quality. Carterette and Soboroff [5] find that (randomly simulated) optimistic assessors (those tending to mark more documents relevant) disrupt evaluation reliability more than pessimistic assessors do.
Webber [31] finds considerable variance between assessors' agreement with an authoritative assessor; nevertheless, Oard and Webber [18] conclude that assessor disagreement differs more between topics than between assessors, suggesting that topic difficulty is a major component in assessment variability. Grossman and Cormack [11] argue that assessor disagreement is due in 90% of cases to inarguable assessor error; however, their dataset has a strong selection bias towards such errors. Webber et al. [32] find that giving more detailed assessment guidelines does not improve assessor agreement or reliability. Webber et al. [32], Wang and Soergel [30], and Efthimiadis and Hotchkiss [8] all compare the reliability for e-discovery of legally trained assessors (lawyers or law students) with that of assessors who lack legal training, and find no difference between the two groups. Kazai et al. [16] find systematic biases between assessor groups in Web search towards, for instance, Wikipedia pages or documents rich in query keywords.
The impact of document order on relevance assessments has also been investigated in the context of crowd-sourcing. Le et al. [17] studied how the impact of the distribution of answers in a training set influences worker performance, and found that the accuracy of workers increases when the training set more closely reflects the underlying distribution of items that are matches or mismatches in a categorisation task. Kazai et al. [15] demonstrated that when workers are asked to assess a series of 10 documents ordered by their expected relevance (as derived from experimental IR systems) for an INEX book track, their overall accuracy compared to expert judgments is lower than when documents are ordered randomly.
Sormunen [24] investigates multi-level relevance criteria, carrying out extensive re-judging of documents for 38 topics from the TREC 7 and 8 newswire collections. Judgments were made on a 4-point relevance scale: highly relevant; relevant; marginally relevant; and non-relevant. The analysis shows that around half of the relevant documents were only marginally relevant, defined as not containing information beyond what is already available in the topic description. The assessments were made by students enrolled in a Masters degree program, and involved the pre-reading of topical documents before starting judgments. We make use of these expert judgments in this paper, using them as an underlying gold standard for our experiment.
2.3 Need for Cognition
While there has been some research about the impact of individual differences on search behavior (e.g., Ford et al. [10]), according to Saracevic [22] there has been little research exploring how individual differences impact relevance behavior. The exception is Davidson [7] who studied openness to information, which was measured by a number of cognitive style variables such as openmindedness, rigidity and locus of control. Davidson [7] found that about 30% of the variance in relevance assessments was attributable to variances in openness to information.
In this study, we focus on one individual difference measure, need for cognition (NFC), which has been extensively studied in other fields as a way to understand differences in the extent to which people process information [3]. Cacioppo et al. [4] define

NFC as "a stable individual difference in people's tendency to engage in and enjoy effortful cognitive activity" (p. 198). Empirical studies have found that individuals with high NFC are more motivated to process information, pay more attention to argument quality, perform better on cognitive tasks, react more positively to complex rules, generate more task-related and thoughtful responses, and recall more information about tasks (see Cacioppo et al. [4] for a detailed review). In this study, we seek to determine if and how NFC impacts relevance behavior.
3. METHODS
A between-subjects laboratory experiment with 82 participants was conducted. Each participant was given a single topic and asked to judge the relevance of 48 documents presented in a pre-specified order. The experimental sessions lasted approximately one hour. Participants were compensated with US$10.
3.1 Treatment
The treatment, or experimental manipulation, occurred within the lists of documents presented to participants. The document lists contained two parts: the prologue, consisting of the first 20 documents; and the epilogue, consisting of the last 28 documents. Documents in the prologue contained the experimental manipulations, while those in the epilogue were held constant for all participants for each topic. These manipulations and lists can be viewed in Table 1. Note that participants were presented with one document at a time, rather than a clickable list.
Within the prologue, one of three treatments was represented, low, medium or high, which corresponded to the degree of relevance of documents in the `X' positions in the list in Table 1. In the low treatment, X was replaced by non-relevant documents. In the medium treatment, X was replaced by marginally relevant or relevant documents. In the high treatment, X was replaced by highly relevant documents. Documents shown in all positions other than those marked with an X were identical across treatments for each topic. The documents shown in positions 46-48 were duplicates of those shown in positions 21, 22 and 24. One document list was instantiated for each topic and treatment level combination; all participants who completed a specified treatment for a particular topic worked through the same list.
The documents, topics and relevance judgments came from a subset of the TREC-7 and TREC-8 test collections, which was created by Sormunen [24]. Marginally relevant and relevant documents were merged into one class and used in our treatments. However, participants were provided with the four-point relevance scale used in Sormunen [24] to make their judgments. The definitions of these various levels of relevance were:
· Highly relevant (3): The document discusses the themes of the topic exhaustively. In case of a multi-faceted topic, all or most sub-themes or viewpoints are covered. Typical extent: several text paragraphs, at least 4 sentences or facts.
· Relevant (2): The document contains more information than the topic description but the presentation is not exhaustive. In case of a multi-faceted topic, only some of the sub-themes or viewpoints are covered. Typical extent: one text paragraph, 2-3 sentences or facts.
· Marginally relevant (1): The document only points to the topic. It does not contain more or other information than the topic description. Typical extent: one sentence or fact.
· Not relevant (0): The document does not contain any information about the topic.

625

Prologue

1X

11 NR

2X

12 X

3 NR 13 NR

4 NR 14 X

5X

15 NR

6 NR 16 X

7X

17 X

8 NR 18 NR

9X

19 NR

10 NR 20 X

21 MR-R 22 MR-R 23 NR 24 MR-R 25 MR-R 26 HR 27 NR 28 HR 29 NR 30 NR

Epilogue 31 HR 32 MR-R 33 MR-R 34 MR-R 35 HR 36 MR-R 37 HR 38 MR-R 39 NR 40 HR

41 MR-R 42 NR 43 MR-R 44 NR 45 HR 46 MR-R, 21 47 MR-R, 22 48 MR-R, 24

Table 1: Design template for document lists. X indicates documents that differed among treatments in the Prologue. NR=nonrelevant document; MR-R=relevant or marginally relevant document; HR=highly relevant document. Documents in positions 46-48 are duplicates of those in positions 21, 22, and 24.

When participants logged in to the system, they were presented with a description of what they would be doing during the experimental session. Participants were instructed: "The aim of this task is to assess the relevance of a set of documents for a particular search topic. You will be provided with a search topic to work on. You will then be presented with a series of documents, one at a time. Read each document, and decide if it is relevant for the topic." Participants were presented with the relevance definitions and scale, and were instructed to judge each document independently and on its own merit ("If a document contains information that makes it anything other than not relevant, then you should choose the appropriate relevance category, even if you have seen that information previously in another document."). Participants were further told that the documents were from the late 1980s and 1990s and that when a topic referred to "current" they should interpret this as "current to the time the article was written."
The assessment interface, which presented the search topic at the top of the screen (title, description and narrative), a document in the middle of the screen (one document was presented at a time), and the relevance assessment scale at the bottom of the screen, was also explained in the instructions. Participants were instructed that once they submitted an assessment they could not return to revise it later. During the assessment, participants could hover their mouse over each choice on the relevance scale to see the relevance definitions.
After participants finished reviewing the instructions, they completed a training task with a topic not used in the main study and two documents, so they could become familiar with the assessment process and interface. Several pilot tests were conducted during the development of the instructions and experimental infrastructure.
3.2 Topics
After completing the training task, participants were presented with one of three search topics. Our choice of topics was guided by several concerns. First, we needed to select topics for which there was a good assortment of documents at various relevance levels. Second, we needed to select topics about which most participants would know little, so that participants would start the study with the same basic models of the topics. Finally, we wanted to select topics that might interest participants. Several pilot tests were conducted during the topic and document selection phases.
Three topics were used in this study: 385 (hybrid fuel cars), 396 (sick building syndrome), and 415 (drugs, Golden Triangle). These topics were counter-balanced across the experimental treatments; that is, there were high, medium and low treatments for each topic. We understand that the inclusion of only three topics limits our ability to generalize, but including a larger number of topics was not

Frequency of past search Prior knowledge Interest Relevance to life

385 Never A little Slightly Moderately

396 Never A little Slightly Slightly

415 Never Nothing Somewhat Not at all

Table 2: Participants' Mode Responses to Topic Questions.

possible since it would have required a prohibitively large number of participants to achieve statistical power.
Participants were presented with the title, description and narrative fields and asked (1) How many times they had searched for information about the topic in the past (never, 1-2 times, 3-4 times, 5 or more times); (2) How much they knew about the topic (nothing, a little, some, a great deal); (3) How interested they were to learn more about the topic (not at all, slightly, somewhat, moderately, very); and (4) The relevance of the topic to their lives (not at all, slightly, somewhat, moderately, very). Following this, participants started the assessments.
Table 2 presents participants' mode responses to the topic items. Overall, most participants had not ever searched for information about these topics, knew little or nothing about them, were somewhat or slightly interested in learning more about them and found them moderately (385, hybrid fuel cars), slightly (396, sick building syndrome) and not at all (415, drugs, Golden Triangle) relevant to their lives. While we had hoped participants' interests in the topics might be slightly higher, our main objective was to select topics about which most participants would have no prior knowledge, so from this perspective, our topic selection was satisfactory.
3.3 Exit Questionnaire
After participants completed the main assessment task, they were directed to an exit questionnaire. The first question asked them to indicate what, if anything, was challenging about deciding which relevance levels to associate with each document. Next, they were asked to indicate their confidence in the relevance judgments they made (not at all confident, slightly confident, somewhat confident, moderately confident, very confident).
The next set of items consisted of the Need for Cognition scale (Cacioppo and Petty [3]). This 18-item scale contains statements such as "The notion of thinking abstractly is appealing to me," and "I like to have the responsibility of handling a situation that requires a lot of thinking." Participants indicated the extent to which the statements were characteristic of them using five choices (extremely uncharacteristic of me, somewhat uncharacteristic of me,

626

Average relevance rating 0.0 0.5 1.0 1.5 2.0 2.5 3.0

uncertain, somewhat characteristic of me, and extremely characteristic of me).
The last part of the exit questionnaire contained demographic questions about participants' sex, age, student and occupational statuses, as well as whether English was their native language.
3.4 Participants
Participants were recruited through the mass email service at the University of North Carolina, Chapel Hill. A power analysis performed before the study indicated that approximately 25-30 participants were needed per treatment. Eighty-two people completed the experiment. Participants were randomly assigned to condition: 26 were in Treatment 1 (Low), 27 were in Treatment 2 (Medium) and 29 were in Treatment 3 (High). The majority of participants were female (85.37%). Their ages ranged from 18 to 55 years (M = 23.70, SD = 8.96). Seventy (85.36%) participants were students (57 undergraduates and 13 graduates); seven (8.54%) were university employees; four (4.88%) were both students and employees and one was neither. Student participants' majors were: social sciences (32%); professional schools (29%); sciences (19%); humanities (15%); and undecided (5%). For participants who were fullor part-time employees at the university (11, 13.42%), job titles included research assistant, IT support specialist, librarian, accountant, examiner and administrative manager/supporter. Seventy-one participants (86.59%) indicated they were native English speakers.
4. RESULTS
This paper investigates the effects of threshold priming and need for cognition on relevance assessments. In this section we present the results of our experiments as they relate to the three main research questions.
4.1 Relevance Assessments
The first research question aims to analyze how threshold priming, as operationalized by the relevance level of documents that participants see early in the judgment process, impacts how relevance scores are assigned to documents later on.
Recall that our experiments included three treatment conditions for documents in the prologue (the first 20 items that were judged): participants were shown either highly relevant (high), marginally relevant and relevant (medium), or non-relevant (low) documents. All participants then saw a consistent set of 28 documents in the epilogue. In this section, we investigate the impact of these threshold priming treatments in four ways: the impact on the relative assignment of relevance scores between study participants; the impact on relevance score assignments relative to underlying expert judgments; whether possible differences endure over time; and, whether the amount of time taken to make assessments differ according to treatment.
Before presenting these results, we first perform a treatment check to verify that participants in each of the three groups experienced the intended treatments. Participants' mean (standard deviation) ratings of prologue documents were 0.56 (0.40), 1.08 (0.34) and 1.42 (0.22) for each of the groups low, medium and high, so it appears that participants experienced the intended treatments. If participants were marking documents exactly as the underlying experts, we would expect those in the low group to have a mean of 0, those in the medium group to have a mean around 1 and those in the high group to have a mean of 1.5. These results show while participants experienced the intended treatments, those in the low group also up-marked some non-relevant documents. We explore this behavior in a subsequent section.

Low

Medium

High

Figure 1: Distribution of mean relevance scores assigned to documents in the epilogue by participants in each of the three prologue treatments.

Impact on relative assignment of relevance scores. A
convenient way to characterize the overall behavior of participants in the epilogue is to consider the mean relevance scores assigned to documents in positions 21­48. The statistic captures differences in how participants assigned relevance scores, and enables the analysis of relative judging behavior. (Since individual document assessments were made using a four-point ordinal scale, the mean scores should not be interpreted in absolute terms.)
A boxplot of these average relevance scores assigned by participants in each treatment is shown in Figure 1. The boxes show the data points of the 25th to 75th percentiles, with the solid black line representing the median, and mean values shown by solid black circles. The whiskers show the range of data and outlier values (data points that lie more than 1.5 times the inter-quartile range away from the box) are shown as circles. The means (and standard deviations, in parentheses) of the average relevance scores assigned to documents in the epilogue is 1.67 (0.38), 1.57 (0.33) and 1.40 (0.42) for the low, medium and high treatments, respectively.
Another possible source of variation that could impact the average relevance scores that were assigned in the epilogue comes from the search topics that participants were working on. The mean average relevance scores assigned by participants for each Topic was 1.57 (0.42) for Topic 385, 1.58 (0.37) for Topic 396, and 1.47 (0.39) for Topic 415.
A two-way ANOVA using type II sums of squares was conducted to investigate the statistical significance of both the treatment and topic effects. The treatment effect was found to be statistically significant (F (2, 73) = 3.63, p = 0.031) while the topic effect was not significant (F (2, 73) = 0.61, p = 0.548). The interaction effect was also not significant (F (4, 73) = 0.64, p = 0.636). Follow-up pairwise t-tests were conducted to further investigate the treatment effect (using the Bonferroni-Holm correction for multiple comparisons [13]). The results indicated that the difference between average scores assigned by participants in the low and high treatments differed significantly (t(53) = 2.56, p = 0.025). The differences between the other conditions were not significant (p > 0.05). In terms of relative behavior, it appears that participants who initially saw no relevant documents compensated by assigning higher relevance scores to items in the epilogue, while those who were exposed to highly relevant documents assigned more moderate scores.

627

Relevance rating 0.0 0.5 1.0 1.5 2.0 2.5 3.0

HM L 0

HM L 1

HM L 2

HM L 3

Figure 2: Mean relevance assessments of documents in the epilogue, grouped by the expert relevance rating of the document (0­3) and by prologue treatment.

Topic 385 396 415
Total

Prologue
low med high
1.36 1.06 0.61 0.15 0.12 0.09
0.31 0.12 0.26 0.11 0.05 0.09
0.29 0.26 0.13 0.06 0.13 0.04
0.67 0.44 0.33 0.12 0.10 0.06

Epilogue
low med high
0.59 0.71 0.34 0.07 0.15 0.11
0.29 0.30 0.40 0.11 0.11 0.21
0.17 0.24 0.21 0.06 0.06 0.06
0.35 0.40 0.32 0.06 0.07 0.07

Table 3: Mean of relevance assessments (standard error across assessors in italics) of common non-relevant prologue and epilogue documents, by topic and treatment.

Figure 2 shows the mean relevance assessments that participants assigned to documents (with a 95% confidence interval), grouped first by the expert relevance assessment and second by the experimental treatment. It can be seen that the treatment conditions had the strongest impact on ratings that participants assigned to the mid-range documents (that is, moderately relevant or relevant). These observations are supported by an analysis of the differences between the treatment conditions within each of the four expert relevance groups using a Kruskal-Wallis test (p = 0.773, p < 0.001, p < 0.001, and p = 0.065 for expert relevance levels 0 to 4, respectively).
The prologues contained ten non-relevant documents which were the same within each topic. Participants' assessments of these nonrelevant documents can be seen in Table 3. The mean assessed relevances for these non-relevant documents were 0.65, 0.48, and 0.33 for the low, medium, and high treatments. A two-way ANOVA finds the difference between treatments statistically significant (F (2, 73) = 9.78, p < 0.001), with topic and topic­treatment interactions also being significant (F (2, 73) = 68.01, p < 0.001, and F (4, 73) = 4.79, p = 0.002). Post-hoc two-way ANOVAs between each pair of levels find significance for the contrasts of high with low treatment and medium with low treatment, but not high with medium treatment (respectively, F (2, 49) = 20.03, p <

0.001; F (2, 50) = 6.91, p < 0.05; and F (2, 47) = 2.46, p > 0.05). For the seven non-relevant documents in the epilogue, mean assessed relevances were 0.35, 0.42, and 0.32 for the low, medium, and high treatments, which was not significant in a two-way ANOVA, though there was still a significant topic effect (F (2, 73) = 7.37, p = 0.001). Assessors who saw only non-relevant documents in the prologue were disproportionately inclined to mark some of these documents as relevant; this effect disappeared in the epilogue, however, most likely due to having seen some genuinely relevant documents.
Impact on relevance scores in comparison to expert judgments. In addition to considering the impact that the treat-
ments had on how participants in the different groups assigned relevance scores relative to each other, it is also possible to analyze how these scores compare to the underlying expert relevance judgments. As explained in Section 3, each epilogue consisted of an equal number of documents at each of the four relevance levels. The mean relevance level of each epilogue is therefore 1.5, by design. This expert mean score can be compared to the mean scores that were assigned under each of the treatment conditions; as indicated previously, these were 1.67 for low, 1.56 for medium, and 1.40 for high. Comparing each of these groups against the expert mean of 1.5 using a t-test shows that the scores of the low group differed significantly (t(25) = 2.31, p = 0.0293), while the medium and high groups did not show significant differences (p = 0.354 and p = 0.193, respectively).
A difference in the average rating that a participant assigns, relative to an expert rating, could arise due to a large difference in the assigned score for a small number of documents, or a smaller difference in the assigned score across a large set of documents. Figure 3 shows the frequency of the differences between the score that a participant assigned and the underlying expert judgment. All three treatment groups agreed with the expert judgments around 50% of the time. However, when differences occurred, the low group tended to assign relevance scores that were higher than that given by the experts, while the high group tended to assign scores that were lower than those given by the experts. Moreover, it can be seen that the observed differences in average scores were not simply due to the presence of a small number of items for which there were extreme differences of opinion.
The increase in average scores from low treatment participants on epilogue documents can be further analyzed by the expert relevance of each document. Due to bounding effects, there is a natural tendency for documents with low expert relevance to be assigned higher relevance by participants, and vice versa. We control for this by taking as the baseline the mean assessment of medium and high treatment participants for that document and topic. Compared to this baseline, low treatment participants on epilogue documents on average scored (expert-judged) irrelevant documents 0.01 points lower, marginally relevant documents 0.38 points higher, relevant documents 0.29 points higher, and highly-relevant documents 0.14 points higher. The difference between these score increases is significant in a one-way ANOVA test (F (3, 724) = 8.47, p < 0.0001). That is, the low treatment participants were not up-voting all documents in the epilogue, but only those with some evidence of relevance (as determined by the expert assessments). Moreover, they tended to boost the relevance of low-relevant documents by more than that of high-relevant documents.
Impact on assignment of relevance scores over time.
While the prologue treatment conditions had an impact on the relative assignment of relevance scores to documents in the epilogue,

628

80

0.5

60

High Medium Low

Mean time (seconds)

0.4

40

0.3

Frequency

0.2

20

0.1

0

0.0

-3

-2

-1

0

1

2

3

Difference in relevance ratings (participant - expert)

Figure 3: Frequency of the difference in relevance scores assigned by participants and experts to all epilogue documents.

Average relevance rating 0.0 0.5 1.0 1.5 2.0 2.5 3.0

Early

Late

Low

Early

Late

Medium

Early

Late

High

Figure 4: Distribution of mean per-participant relevance scores assigned to documents in the epilogue, for each of the three treatments.

this raises the question of whether such an effect was enduring over the entire 28 epilogue documents, or whether it changed over time. To investigate this, the epilogue was divided into halves, giving a group of 14 early documents and 14 late documents.
The results of this time-based split are shown in Figure 4, where the low, medium and high treatment conditions have been further partitioned into early and late groups. The graph suggests that variability (based on the inter-quartile range) is higher for the late documents than for the early documents. However, the differences between early and late relevance assignments within a particular treatment group are not statistically significant (t-test, p > 0.1).
This suggests that participants continue to refine their mental relevance models over time. Even if they do not have any reference points to begin with, they are able to re-calibrate once they begin to see documents that are relevant to different degrees.
Time taken to judge relevance. It is possible that the occur-
rence of documents with different levels of relevance has an effect on the amount of attention--and in particular, time--that partici-

Low

Medium

High

Figure 5: Distribution of the mean time that participants took to judge the relevance of documents in the epilogue, for each of the three treatments.
pants devote to examining documents. For example, if an individual reads a number of documents and they are all non-relevant, the person might become disheartened, and as a result pay less attention to subsequent documents that are presented.
The boxplot in Figure 5 shows the mean time in seconds that participants took to make relevance judgments for the 28 documents in the epilogue, split by the three treatment conditions. The time was measured in seconds, from when the document was first displayed, until the participant entered and saved a response in the judging interface. While the mean judging times show slight variation (36.60, 37.02, and 33.51 seconds for the low, medium and high groups respectively), these differences are not statistically significant (one-way ANOVA, F (2, 79) = 0.40, p = 0.669).
4.2 Agreement
The second research question focuses on whether threshold priming affects how well participants are able to form a stable conception of relevance for a topic, based on the self-consistency of ratings over time, and the level of agreement between participants. Two types of agreement are considered: agreement among participants; and participant self-agreement when making repeat judgments of the same documents at different points in time.
Overall agreement among participants. Our experimental
framework was constructed with reference to multi-level relevance judgments created by Sormunen [24], which were created through a careful assessment process.
Consider two sets of relevance judgments made by different assessors. The overlap (or percentage agreement) between these judgments is defined as the intersection divided by the union of the two sets (that is, the number of documents that were given the same relevance score by both assessors, divided by the total number of documents that were assessed). This mean pairwise percentage agreement among all participants is 44.80%.
When people read documents in response to a topic, their understanding of the topic changes. It is possible that exposure to different documents will influence their conception of relevance. For example, an individual who sees many relevant documents early might be able to more quickly develop a model of topical relevance; conversely, a person who has mostly seen non-relevant documents might find it more challenging to establish a stable model. To ex-

629

Average relevance rating 0.0 0.5 1.0 1.5 2.0 2.5 3.0

plore this issue, we investigate the relationship between study treatment and the level of agreement between participants when judging documents in the epilogue.
The treatment groups show only minor differences in mean pairwise agreement levels, with 46.45% (12.62), 45.28% (10.17) and 44.25% (12.85) for the low, medium and high groups, respectively. A two-way ANOVA was conducted to investigate the significance of treatment and topic effects. The results show that the treatment main effect was not statistically significant (F (2, 326) = 0.99, p = 0.373). However, differences in agreement due to the topic main effect were significant (F (2, 326) = 5.70, p = 0.004); the corresponding mean pairwise agreement levels were 42.76 (12.33), 44.79 (12.44), and 47.99 (10.65) for topics 385, 396, and 415, respectively.
This finding of a significant topic effect is consistent with other studies [18]. The interaction between topic and treatment was not significant.
Self-agreement over time. A second perspective on agree-
ment is whether participants agree with their own relevance assessments, over time. Recall that the epilogue was constructed so that the first three "medium level" documents that a participant encountered in the epilogue recurred as the final three documents in the list. Self-agreement is calculated as the overlap between the relevance ratings assigned to these documents.
The average self-agreement across participants was 51.62%. Based on the different treatment levels, only slight variations were introduced, with self-agreement of 52.56%, 49.38% and 52.87% for the low, medium and high groups. The differences between the groups are not statistically significant, based on a one-way ANOVA (F (2, 79) = 0.09, p = 0.918).
4.3 Need for Cognition
Our third research question was whether need for cognition (NFC) influences relevance judgements. In this section, we consider the relationship between NFC and the relevance ratings participants assigned to the epilogue documents; the time taken to make relevance judgements; and the level of agreement between participants and experts.
To examine the effect of NFC, participants' responses to each of the 18 items on the NFC scale were averaged to arrive at a composite NFC score for each participant. Their composite scores from the scale ranged from 1.84 to 4.37. The mean and median of NFC composite scores were 3.16 and 3.24, respectively, and the standard deviation was .56. Participants were divided into a high need for cognition group (HNFC, n=41) and a low need for cognition group (LNFC, n=41) based on a median split [28]. The distribution of HNFC and LNFC participants across treatments was not statistically different (2(2) = 2.741, p = 0.254).
Impact on assignment of relevance scores. Overall, LNFC
participants tended to assign lower relevance scores (M = 1.49, SD = 0.39) than HNFC (M = 1.58, SD = 0.39). A two-way ANOVA (treatment x NFC) was used to investigate the potential main and interaction effects of NFC and treatment on the relevance scores of documents in the epilogue (Figure 6). Results showed a significant main effect for treatment (F (2, 76) = 3.89, p = 0.025), but not NFC group (F (1, 76) = 1.118, p = 0.294). As the prologue treatment varied from low to high, the difference in relevance scores between HNFC participants and LNFC began to diverge, but not significantly (F (2, 76) = 0.267, p = 0.767).

+o

+o

o

+

NFC group
o High + Low

Low

Medium

High

Figure 6: Main and interaction effects of NFC and prologue treatments on mean relevance scores assigned to epilogue documents.

50

40

o

o

+

+o

+

30

Mean time (seconds)

20

10

0

NFC group
o High + Low

Low

Medium

High

Figure 7: Main and interaction effects of NFC and Prologue treatments on mean time taken to judge the relevance of documents in the epilogue.
Impact on time to judge relevance. Results showed that
HNFC participants spent more time making relevance judgements (M = 37.97, SD = 16.33) than LNFC participants (M = 33.36, SD = 15.24) (Figure 7). Results also showed that as treatment varied from low to high, the differences between the time taken by participants in the HNFC and LNFC groups converged. A two-way ANOVA found no significant differences (Treatment: F (2, 76) = 0.473, p = 0.625; NFC: F (1, 76) = 2.154, p = 0.146; Interaction: F (2, 76) = .772, p = 0.466).
Impact on agreement with expert judgements. Finally,
the effect of NFC on participant agreement with the expert assessors was examined. Agreement was measured as the proportion of documents in the epilogue to which participant and expert assessor gave the same relevance value. HNFC participants had higher agreement (M = 51.8%, SD = 10.2%) with the experts than LNFC participants (M = 46.7%, SD = 9.0%). A three-way ANOVA (treatment x NFC group x topic) was performed to test this difference for significance. (Topic was included in the analysis due to the strong topic effect on agreement between assessors

630

noted in Section 4.2.) The NFC effect was found to be significant (F (1, 70) = 5.48, p = 0.022); none of the other effects or interactions achieved significance.
4.4 Assessment Challenges
At the end of the study, participants were asked what, if anything, they found challenging about deciding which relevance levels to associate with documents. The most common challenge identified by participants was related to the extent to which the topic was represented in the document. Participants noted that they struggled with documents that contained relevant terms, but no real discussion of the issues. Some mentioned difficulties assessing documents that only mentioned a single aspect of the topic and those where the topic was not the main focus. Others struggled to assess documents that only contained a few sentences about the topic. Specifically, participants commented that the proportion of the document "about" the topic was something they had a difficult time dealing with when making relevance assessments. Another frequently mentioned challenge was document length, which according to many participants made skimming difficult and also placed a burden on their memory as they moved through the document trying to identify and track relevant information. Only two participants commented about fatigue.
Although it is generally assumed that assessors will (and can) base their judgements on topical relevance alone, many participants' comments were related to other types of relevance. With respect to cognitive relevance, participants indicated that their lack of background knowledge made the assessment task challenging. One participant commented that if he did not understand a document, he categorized it as not relevant or marginally relevant. Another participant observed that it was difficult to ignore novelty when making assessments. With respect to situational relevance, several participants indicated a desire for a more thorough topic description including information about why the information was needed and how it would be used. Finally, several participants commented that they did not like some of the document genres and formats because the display was unappealing, and this was difficult to ignore when making assessments (affective relevance). Overall, these comments challenge the notion that in assessment situations, relevance judgements are based purely on external representations (i.e., topic description) and that other types of relevance can be controlled.
5. CONCLUSION AND FUTURE WORK
This study investigated threshold priming, or the extent to which the relevance of documents viewed early during the assessment process impacted subsequent assessments. This study also investigated how need for cognition, an individual difference measure, impacted relevance assessments.
Our first research question examined how threshold priming impacted participants' relevance scores. We found that participants in the low treatment group assigned significantly higher mean relevance scores to documents in the epilogue than participants in the high treatment group. In particular this change in behavior was significant for documents that were in the middle of the relevance range (marginally relevant or relevant). To investigate the impact on relevance scores over time, we sub-divided documents in the epilogue into two halves (early and late), but found that mean relevance ratings did not differ significantly. In comparing participants' assessments of identical non-relevant documents in the prologue, we found a significant difference in the scores, with those in the low treatment group assigning the highest scores, followed by those in the medium and high treatment groups. This difference disappeared in the epilogue. Taken together, these results provide

evidence that people's internal relevance models are impacted by the relevance of the documents they initially view and that they can re-calibrate these models as they encounter documents with more diverse relevance scores.
With respect to agreement among scores assigned by participants and the underlying expert relevance scores, we found that scores assigned by participants in the low group significantly differed from the expert judgments, while those assigned by participants in the medium and high groups did not. In looking more closely at the disagreements, we found that participants in the high group tended to assign scores that were lower than those given by experts, while participants in the low group tended to assign scores that were higher than those given by experts. In particular, participants in the low group tended to boost the relevance of low-relevant documents more than high-relevant documents, which is consistent with past research [9, 14]. These participants were not up-voting all documents in the epilogue, but rather only those with some evidence of relevance. Given that these participants also assigned higher ratings to non-relevant documents in the prologue, it is likely that they developed lower relevance thresholds for at least some categories of relevance.
Our second research question focused on whether threshold priming impacted how well participants were able to form a consistent conception of relevance for a topic. We found no significant differences in self-agreement levels, agreement between assessors, or agreement with experts, according to treatment condition. However, regardless of treatment condition, a reasonable level of disagreement remains, indicating that making relevance assessments is a challenging task and subject to substantial variation. An interesting question for future work is to investigate whether the low self-agreement for individual assessors over time is due to a genuine change in their internal relevance model, or is representative of general inherent variability in such models, perhaps due to factors such as mental fatigue.
Our final research question focused on whether need for cognition (NFC) influenced relevance judgments. Although high NFC participants assigned higher relevance scores to documents, this was not significant. We also observed a divergence in mean relevance score assigned by high and low NFC participants as the treatment groups varied from low to medium to high, but this also was not significant. Similar results were found for time taken to judge relevance. Overall, high NFC participants spent more time assessing documents, and the time spent by high and low NFC participants converged as treatments varied from low to medium to high. Finally, we found that high NFC participants' level of agreement with the expert assessors was significantly higher than that of low NFC participants. While not all of these results are statistically significant, the results suggest that individual difference measures might provide insight into assessor variability and could be a potentially useful way to screen assessors for characteristics that are associated with more consistent judgments.
Although we did not pose a research question about the challenges participants experience when making assessments, we included a question at the close of the study about this issue. Common challenges identified by participants concerned the depth of treatment of the topic, focus of the document, and the proportion of the document devoted to the topic. We observed many comments that indicated participants struggled to base their assessments purely on topical relevance, and instead wanted to consider cognitive, situational and affective relevance as well. These results show the difficulty in trying to restrict human judgments to topical relevance.
We are conducting a follow-up study using the same design and infrastructure except that participants are asked to justify their rel-

631

evance assessments explicitly after viewing each document. This will allow us to elicit and monitor the formation of participants' internal relevance models and evaluate how this procedure impacts relevance assessments. Since the study design is the same, we will be able to compare the judgments of these participants with a randomly selected subset of participants from the current study to determine how this reflective procedure impacts the judgment process across experimental treatments. If this procedure results in more consistency in judgments across treatments, then it might be included as part of assessors' initial training.
Acknowledgements
This work was supported in part by the ICT Centre of Excellence program, National ICT Australia (NICTA), and in part under the Australian Research Council's Discovery Projects funding scheme (project number DP130104007). This material is based in part upon work supported by the National Science Foundation under Grant No. 1065250. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
References
[1] Carol Barry. User-defined relevance criteria: an exploratory study. Journal of the American Society for Information Science, 45: 149­159, 1994.
[2] Pia Borlund. The concept of relevance in IR. Journal of the American Society for Information Science and Technology, 54(10): 913­925, 2003.
[3] John T. Cacioppo and Richard E. Petty. The need for cognition. Journal of Personality and Social Psychology, 42(1):116­131, 1982.
[4] John T. Cacioppo, Richard E. Petty, Jeffery A. Feinstein, and W. Blaire G. Jarvis. Dispositional differences in cognitive motivation: The life and times of individuals varying in need for cognition. Psychological Bulletin, 119(2):197­253, 1996.
[5] Ben Carterette and Ian Soboroff. The effect of assessor errors on IR system evaluation. In Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, pages 539­546, Geneva, Switzerland, 2010.
[6] Carlos A. Cuadra and Robert V. Katter. Opening the black box of relevance. Journal of Documentation, 23(4):291­303, 1967.
[7] David Davidson. The effect of individual differences of cognitive style on judgments of document relevance. Journal of the American Society for Information Science, 28(5):273­284, 1977.
[8] Efthimis N. Efthimiadis and Mary A. Hotchkiss. Legal discovery: does domain expertise matter? Proceedings of the American Society for Information Science and Technology, 45:1­2, 2008.
[9] Michael Eisenberg and Carol Barry. Order effects: a study of the possible influence of presentation order on user judgments of document relevance. Journal of the American Society for Information Science, 39:293­300, 1988.
[10] Nigel Ford, David Miller, and Nicola Moss. Web search strategies and human individual differences: cognitive and demographic factors, internet attitudes and approaches. Journal of the American Society for Information Science and Technology, 56:741­756, 2005.
[11] Maura Grossman and Gordon V. Cormack. Inconsistent assessment of responsiveness in e-discovery: difference of opinion or human error? In DESI IV: The ICAIL Workshop on Setting Standards for Searching Electronically Stored Information in Discovery Proceedings, pages 1­11, June 2011.
[12] Stephen P. Harter. Psychological relevance and information science. Journal of the American Society for Information Science, 43: 602­615, 1992.
[13] Sture Holm. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6(2):65­70, 1979.
[14] Mu-hsuan Huang and Hui-yu Wang. The influence of document presentation order and number of documents judged on users' judgments of relevance. Journal of the American Society for Information Science and Technology, 55(11):970­979, 2004.

[15] Gabriella Kazai, Jaap Kamps, Marijn Koolen, and Natasa Milic-Frayling. Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking. In Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, pages 205­214, Beijing, China, 2011. ACM.
[16] Gabriella Kazai, Nick Craswell, Emine Yilmaz, and S. M. M. Tahaghoghi. An analysis of systematic judging errors in information retrieval. In Proceedings of the ACM International Conference on Information and Knowledge Management, pages 105­114, Maui, Hawaii, USA, 2012.
[17] John Le, Andy Edmonds, Vaughn Hester, and Lukas Biewald. Ensuring quality in crowdsourced search relevance evaluation: The effects of training question distribution. In SIGIR 2010 workshop on crowdsourcing for search evaluation, pages 21­26, 2010.
[18] Douglas W. Oard and William Webber. Information retrieval for e-discovery. 2013. In submission. http://ediscovery.umiacs.umd.edu/pub/ow12fntir.pdf.
[19] Lorraine M. Purgaillis Parker and Robert E. Johnson. Does order of presentation affect users' judgment of documents? Journal of the American Society for Information Science, 41(7):493­494, 1990.
[20] Tefko Saracevic. Relevance: a review of and a framework for the thinking on the notion of information science. Journal of the American Society for Information Science, 26(6):321­343, 1975.
[21] Tefko Saracevic. Relevance: a review of the literature and a framework for thinking on the notion in information science. part II: Nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58:1915­1933, 2007.
[22] Tefko Saracevic. Relevance: a review of the literature and a framework for thinking on the notion in information science. part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58:2126­2144, 2007.
[23] Falk Scholer, Andrew Turpin, and Mark Sanderson. Quantifying test collection quality based on the consistency of relevance judgements. In Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, pages 1063­1072, Beijing, China, 2011.
[24] Eero Sormunen. Liberal relevance criteria of TREC ­ counting on negligible documents? In Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, pages 324­330, Tampere, Finland, 2002.
[25] Anastasios Tombros, Ian Ruthven, and Joemon M. Jose. How users assess web pages for information seeking. Journal of the American Society for Information Science and Technology, 56(4):327­344, 2005.
[26] Christian Unkelbach, Vanessa Ostheimer, Frowin Fasold, and Daniel Memmert. A calibration explanation of serial position effects in evaluative judgments. Organizational Behavior and Human Decision Processes, 119:103­113, 2012.
[27] Pertti Vakkari. Changes in search tactics and relevance judgments when preparing a research proposal: a summary of the findings of a longitudinal study. Information Retrieval, 4(3):295­310, 2001.
[28] Bas Verplanken. Need for cognition and external information search: Responses to time pressure during decision-making. Journal of Research in Personality, 27(3):238­252, 1993.
[29] Ellen M. Voorhees. Variations in relevance judgements and the measurement of retrieval effectiveness. Information Processing and Management, 36(5):697­716, 2000.
[30] Jianqiang Wang and Dagobert Soergel. A user study of relevance judgments for e-discovery. Proceedings of the American Society for Information Science and Technology, 47:1­10, 2010.
[31] William Webber. Re-examining the effectiveness of manual review. In Proc. SIGIR Information Retrieval for E-Discovery Workshop, pages 2:1­8, Beijing, China, July 2011.
[32] William Webber, Bryan Toth, and Marjorie Desamito. Effect of written instructions on assessor agreement. In Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, pages 1053­1054, Portland, Oregon, USA, 2012.

632

A General Evaluation Measure for Document Organization Tasks

Enrique Amigó
E.T.S.I. Informática UNED Juan del Rosal, 16 Madrid, Spain
enrique@lsi.uned.es

Julio Gonzalo
E.T.S.I. Informática UNED Juan del Rosal, 16 Madrid, Spain
julio@lsi.uned.es

Felisa Verdejo
E.T.S.I. Informática UNED Juan del Rosal, 16 Madrid, Spain
felisa@lsi.uned.es

ABSTRACT
A number of key Information Access tasks ­ Document Retrieval, Clustering, Filtering, and their combinations ­ can be seen as instances of a generic document organization problem that establishes priority and relatedness relationships between documents (in other words, a problem of forming and ranking clusters). As far as we know, no analysis has been made yet on the evaluation of these tasks from a global perspective. In this paper we propose two complementary evaluation measures ­ Reliability and Sensitivity ­ for the generic Document Organization task which are derived from a proposed set of formal constraints (properties that any suitable measure must satisfy).
In addition to be the first measures that can be applied to any mixture of ranking, clustering and filtering tasks, Reliability and Sensitivity satisfy more formal constraints than previously existing evaluation metrics for each of the subsumed tasks. Besides their formal properties, its most salient feature from an empirical point of view is their strictness: a high score according to the harmonic mean of Reliability and Sensitivity ensures a high score with any of the most popular evaluation metrics in all the Document Retrieval, Clustering and Filtering datasets used in our experiments.
Categories and Subject Descriptors
B.8 [Performance and Reliability]: General
General Terms
Measurement, Performance
Keywords
IR effectiveness measures
1. INTRODUCTION
Some key Information Access tasks can be seen as instances of a generic document organization problem that es-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

tablishes priority (ranking) and relatedness (clustering) relationships between documents. Let us think of a generic document organization system as a function from document pairs d, d into one of these possible relationships: , which means that d has more priority than d , and , which means that d and d have some kind of topical equivalence. We will use the notation for the cases in which the other two relations do not hold, and the notation {d1, d2 . . . dn} to indicate that d1 . . . dn are all related via the topical equivalence relation .
This general problems subsumes: Document Ranking. This case is illustrated in Table 1. The gold standard establishes (at least) two priority levels (relevant versus non-relevant), and the system output returns an ordered list with one priority level per document. For clarity, in the table we use a bold font for relevant documents. Note that, in this problem, it is assumed that there is an unlimited amount of irrelevant documents, while the set of relevant documents is limited. Both the gold standard and the system output contain, implicitly, an unlimited set of documents in the last level. The gold standard contains two priority levels (with the documents manually judged) while the system output contains as many levels as returned documents. Document Filtering. Table 2 illustrates this case; now, both the gold standard and the system output consist of two priority levels (relevant and irrelevant). Document Clustering is exemplified in Table 3. Now there is only one priority level, and a set of clusters which contain related documents. Both the gold standard and the system output have the same form. Table 4 illustrates the variant of overlapping clustering, where a document may simultaneously appear in more than one cluster. Evaluation metrics have been extensively discussed for each of these tasks. There are, however, many practical problems where the system must be able both to detect topical relationships (clustering documents) and relative priorities (some clusters are more relevant than others). Let us give a couple of examples: Alert detection. A number of practical information access problems involve detecting, in an incoming stream of documents, new information that is both novel and of high priority. Online Reputation Management, for instance, involves monitoring online information about an entity (a company, brand, product, person, etc.), clustering texts into the main topics, and establishing which of them have higher priority (for instance, those that may potentially damage the reputation of the entity).

643

Gold Standard d1, d2, d3
d4, d5, d6, d7 ..dn

System Output
d1 d2 d6 d3 d14 d4, d5, d7, ..dn

Table 1: Example of Document Ranking task. Vertical ordering indicates relative priorities

Goldstandard d1, d2 , d3
d4, d5, d6, d7, d8 System output d1 , d2 , d4
d3, d5, d6, d7, d8
Table 2: Example of filtering task

Search Results Organization. Given the set of top ranked documents retrieved for a query, a mechanism to group related documents and assign priorities between clusters can be used to improve search results in many ways: (i) to enhance search results diversity by maximizing the number of highly relevant topics represented in the top results; (ii) to provide keyword suggestions to refine the query, sampled from the most relevant clusters; (iii) to directly display search results as a ranked list of relevant topics corresponding to alternative query interpretations, subtopics or facets in the retrieved documents.
This type of tasks (and others such as composite retrieval) ­ which are more complex than the standard ranking and clustering problems ­ still match the generic Document Organization task as we have defined it. An example is shown in Table 5. In the gold standard there is, for instance, one topic with two documents (d1  d2) at the top priority level, and two topics at the second level (d3  d4 and d5  d6). In the next three priority levels there are two single documents and another topic. The example includes overlapping clusters: for instance, d5 is relevant for two topics, one in the second priority level and another one in the fourth.
Finally, the (potentially long) list of documents at the bottom of the gold standard ranking (d11, d12, d13, ... dn) represents irrelevant documents which are not judged in terms of topical similarity, and are therefore represented via the empty relationship .
Evaluation measures have been proposed to evaluate clustering outputs in the context of document retrieval [14, 8, 18] and to include the notion of diversity in search results[15, 24, 9]. But, to the best of our knowledge, no measure has been previously designed to evaluate the general document organization problem and all the tasks subsumed by this one.
In order to find appropriate evaluation measures for the generic Document Organization problem, we will focus on the specification of the formal constraints that they should satisfy in each of the subsumed tasks (filtering, clustering and ranking). Our goal is finding an evaluation measure

Gold standard {d1, d2, d3}, {d4,d5,d6}, d7
System output {d1,d2}, d3, {d4, d5, d6, d7}
Table 3: Example of Clustering task.
Gold standard {d1, d2, d3}, {d4, d5, d6}, {d4, d7}
System output {d1, d2}, d3, {d4, d5, d6, d7}, {d6, d7}
Table 4: Example of overlapping clustering

for the general document organization task that (i) satisfies formal constraints for all the tasks; (ii) turns into suitable existing measures when mapped into each of the subsumed tasks.
Our research leads to propose Reliability and Sensitivity which are, in short, precision and recall over document pair relationships established in the gold standard, with a suitable weighting scheme. Comparing them with state of the art measures for each particular scenario, we find that Reliability and Sensitivity satisfy more formal constraints than previously existing measures in most cases. In addition, its harmonic mean is stricter than previously existing measures ­ a high score implies high scores with respect to all other standard measures ­, which is an interesting property when the application scenario does not clearly prescribe a more specific evaluation measure.
We will start by establishing the set of formal constraints that any suitable metric should satisfy (Section 2); then we present our proposed measures (Section 3) and discuss its application to the different tasks (Section 4). We end by showing how the measures apply to the more complex document organization task, and discussing the main implications of our results.
2. INFORMATION ACCESS MEASURES AND FORMAL CONSTRAINTS
Our approach to measure design is to start defining a set of formal constraints, i.e. a set of formal, verifiable properties that any suitable measure should satisfy. In addition, they explain the nature of different evaluation measure families. We will start by reviewing (or proposing) formal constraints for each of the subsumed tasks, and then merge all collected constraints into a single list of properties for the generalized measures. Table 6 summarize the results of this analysis.

Gold standard
{d1, d2} {d3, d4}, {d5, d6}
d7 {d5, d8, d9}
d10 d11, d12, d13, ... , dn

System output
d1 d2 {d3, d4}, {d5, d6} {d5, d7, d9} d8 d12 d10, d11, d13, ... dn

Table 5: An example of gold standard and system output for the generic information retrieval task.

644

Measure

Homo. Comp. Rag Size vs Priority Deepness Deepness Closeness Confidence

Bag Quantity

Threshold Threshold

Set matching









Entropy based









Edit distance









Counting pairs









Bcubed





 (overlap)

MAP, DCG, Q measure











P10











MRR











RBP











UTILITY, F, LAM%



R*S



















Table 6: Formal constraints satisfied by standard measures and R*S

2.1 Document Clustering
[2] provides a detailed analysis of clustering evaluation measures (grouped by families) and the constraints they should satisfy. We briefly describe here these constraints and refer to [2] for its justification and formal description. We will say that the system output produce clusters while the gold standard is composed by classes. Q represents the quality of a clustering distribution and a, b, c, d.. are documents from different classes:
Cluster Homogeneity: This restriction was firstly proposed in [22]. Given a certain system output document distribution, splitting documents that do not belong to the same class, must increase the output quality:
Q({a, a, a}, {b, b}...) > Q({a, a, a, b, b}...)
Although it seems a very basic constraint, in [2] it is shown that measures based in editing distance do not satisfy it.
Cluster Completeness: The counterpart to the first constraint is that documents belonging to the same assessed class should be grouped in the same cluster [2, 22]:
Q({a, a, a, a, a, }...) > Q({a, a, a}, {a, a}...)
Measures based on set matching, such as Purity and Inverse Purity do not satisfy this contraint.
Rag Bag: This constraint states that introducing disorder into a disordered cluster (rag bag) is less harmful than introducing disorder into a clean cluster. That is:
Q({a, a, a, a}, {b, c, d, e}..) > Q({a, a, a, a, b}, {c, d, e}..)
In general, all traditional measures fail to comply with this constraint.
Cluster size vs. quantity: A small error in a big cluster is preferable to a large number of small errors in small clusters [2, 19, 22]. This constraint prevents the problem that measures based on counting pairs [19, 13], overweight big clusters. That is:
Q({a, a, a, a}, {a}, {b, b}{c, c}{d, d}{e, e}...) >
Q({a, a, a, a, a}, {b}, {b}, {c}, {c}, {d}, {d}, {e}, {e}..)
Measures based on counting pairs are sensitive to the combinatory explosion of pairs in big clusters, failing on this constraint.
In [2] it is shown how the above contraints discriminate between four families of evaluation measures for the clustering problem: measures based on set matching (e.g. F

measure, Purity and Inverse Purity), entropy-based measures (Entropy, class Entropy, Mutual Information), measures based on counting pairs and edit distance measures. Interestingly, there is only one pair of measures, BCubed Precision and Recall, that satisfies all constraints simultaneously, and forms an independent family. However, we will show in this paper that the extended version of Bcubed for overlapping clustering does not satisfy the last constraint.
2.2 Document Filtering
Filtering is a binary classification problem where there is a relative priority between the two classes: the system must classify each document as positive or negative, being the positive class the one that stores the relevant information. In the filtering scenario all existing measures satisfy a basic constraint:
Priority Constraint Moving a positive (relevant) document from the predicted negative set to the predicted positive set, or moving a negative (irrelevant) document from the predicted positive to the predicted negative set must increase the system output quality. Being dr and d¬r a judged relevant/irrelevant document respectively:
Q({dr..}, {..}) > Q({..}, {dr..})
Q({..}, {d¬r..}) > Q({d¬r..}, {..})
This constraint is satisfied by every standard measure. Filtering is a basic binary classification task that can be evaluated in multiple ways, but it is difficult to define objectively desirable boundary constraint that discriminate measures. However, there are descriptive properties which are mutually exclusive and explain the nature of different measure families [3]. These properties focus on how the metrics score non-informative outputs D¬inf depending on the size of the positive set S(D¬inf ). A non-informative output is a random distribution of the documents that is independent on the content of the input documents. An "all positive" baseline, for instance, is a non-informative output where the size of the positive set is equivalent to the size of the input set.
For instance, there is a large set of filtering measures that assign a fixed score to every non-informative output: Lam [11], the Macro Average Accuracy [20] and, in general, measures based on correlation such as the Kappa statistic [10]. (Q(D¬inf ) = constant). Other family of measures assumes that, if the filtering system does not know any-

645

thing, returning all is better than removing documents randomly. In this case, the score for a non-informative system is correlated with the size of the positive output set: (Q(D¬inf )  S(D¬inf )) This is the case of the harmonic mean of Precision and Recall over the positive class. Finally, measures such as Accuracy or Utility assign relative weights to documents in each of the cases in the confusion matrix; Depending on how these parameters are set and on the distribution of classes in the input, the optimal positive class size for a non-informative output varies.
2.3 Document Ranking
In order to define a set of constraints for the document ranking (or document retrieval) task, we have to take into account some aspects. First, documents at the top of the ranking must have more weight in the evaluation process: even if the system is able to sort all documents, the user will not be able to explore all of them. The sizes of the relevant and the irrelevant set are expected to be heavily unbalanced: potentially, the amount of irrelevant documents for a given query is (in practice) unlimited. Second, traditional document retrieval is a mixture of filtering and ranking tasks: an optimal system should not only rank the documents, but also decide on the size of the output set, depending on the amount of relevant documents in the collection and the selfassessed quality of the system output. These two features ­ which are related the fact that the gold standard and the system output take different forms, unlike the filtering and clustering problems ­ make document retrieval evaluation harder than it seems a priori.
There is a large number of proposed measures in the state of the art. Some of the most popular are: precision at certain recall levels or ranking positions, AUC [12], MAP (Mean Average Precision), Discounted Cumulative Gain [16], Expected Reciprocal Rank (ERR)[25], Q-measure, Binary Preference [4], or Rank Biased Precision [21], among many others. Let us analyze them in terms of formal constraints.
First, the Priority Constraint from the filtering problem also applies here (see previous subsection) and it is satisfied by most measures. A notable exception is P@10 (precision at the top ten documents retrieved), given that it is not sensitive to relationships after the tenth position in the system output ranking, and to internal reorderings in the top ten setWe can express this constraint in the context of Document Retrieval evaluation as, being r and ¬r relevant and irrelevant documents respectively, and being {d1, d2, d3..dn} an output ranking:
Q({..ri+1, ¬ri+2..}) > Q({..¬ri+1, ri+2..})
Deepness Constraint: The more we go to a deeper point in the output ranking, the less the probability of documents being explored by the user. Therefore, the effect of a document priority relationship in the system quality depends on the depth of the documents in the ranking. Being i<j:
Q({..ri, ¬ri+1..}) - Q({..¬ri, ri+1..}) <
Q({..rj , ¬rj+1..}) - Q({..¬rj , rj+1..})
Measures based on traditional correlation, such as AUC or Kendall, do not satisfy this contraint, because they give the same weight to all elements in the ranking. Also, P@10 obviously does not satisfy this contraint.
Deepness Threshold Constraint: Although P@10 does not satisfy the previous two constraints, one motivation for

using it is that in some cases it is advisable to assume a practical deepness threshold. In other words, there is a ranking area which will never be explored by the user. We can express this as a constraint by saying that there exists a value n large enough such that retrieving one relevant document at the top of the rank is better than retrieving n relevant documents after n irrelevant documents:

Q({r1, ¬r2, ¬r3..¬r2n}) > Q({¬r1, ¬r2..¬rn-1, rn..r2n})

We will not include all the formal proofs on how measures

satisfy this constraint, due to space availability; we will only

discuss the proof that MAP does not satisfy it.

The score for the leftmost distribution in the constraint

is

1 Nr

,

assuming

that

there

are

Nr

relevant

documents

in

the collectionWe can prove that the score for the rightmost

distribution is always higher:

1 i=n i

1 i=n i

M AP =

>

=

Nr i=1 n + i Nr i=1 2n

1 i=n

1 (n - 1)n (n - 1)

i=

=

2nNr i=1

2nNr

2

4Nr

which

is

bigger than

1 Nr

for

any

n > 4.

DCG does

not

com-

ply with this constraint either: DCG for the first distribution

is 1, before normalization. And for the second distribution

we have:

i=n

1

i=n

1

n

DCG =

>

=

>1

i=1 log2(i + n) i=1 log2(2n) log2(2n)

For instance, according to MAP or DCG, finding 1,000 relevant documents after 1,000 irrelevant documents is better than having only one relevant document, but at the top of the rank. This is counterintuitive in many practical settings.
The Q-measure is an extension of MAP for multigraded relevance, having a similar behavior. However, the measure ERR does satisfy this constraint (as well as P@10), due to the strong relevance discount for positions deeper in the rank. Measures with similar weighting schemes also satisfy this constraint, but at the cost of failing to satisfy the next one.
Closeness Threshold Constraint: There exists a (short) ranking area which is always explored by the user. For instance, we can assume that the top three documents returned by a search engine for informational queries are always inspected. We formalize this constraint as the counter part of the previous constraint: There exists a value n small enough such that retrieving one relevant document in the first position is worse than n relevant documents after n irrelevant documents:

Q({r1, ¬r2, ¬r3..¬r2n}) < Q({¬r1, ¬r2..¬rn-1, rn..r2n})

In the case of P@10, n is 9 (i.e. for any n lower than 9, the constraint is satisfied). ERR, on the other hand, does not satisfy the constraint: given its strong discount with ranking depth, one relevant document at the first position has always more weight than n relevant documents after the position n.
The measures RBP and the discounting function proposed by Smucker and Clarke [23] satisfy all the previous constraints. The common characteristic of both measures is that they are based on a probabilistic user behavior model.

646

However, all proposed measures fail on the following constraint.
Confidence Constraint. The output ranking does not necessarily include all documents in the collection and, therefore, the amount of documents returned is also an aspect of the system quality. The classical TREC ad-hoc evaluation does not consider this aspect, given that the length of the rank is fixed; nevertheless, there is research focused on the prediction of ranking quality, in order to determine when an output rank must be shown to the user. We include this aspect in our constraints by stating that extending the rank with irrelevant documents should decrease the output score:
Q({d1, d2..dn}) > Q({d1, d2..dn, ¬rn+1})
Given that most measures are based on accumulative relevance weighted by the location in the ranking [5], we can conclude that irrelevant documents at the bottom of the ranking do not affect the score and the constraint is not satisfied. As far as we know, current evaluation measures do not consider this aspect.
In summary, for the tasks subsumed under our document organization problem, the result of our analysis is that (i) in clustering, only the Bcubed measure satisfies all constraints, with the exception of one of them in the case of overlapping clustering; (ii) in the filtering scenario, all measures satisfy the priority constraint, but behave in very different manners with respect to how they score non-informative outputs; and (iii) Finally, in the case of document retrieval, we have detected a few measures that satisfy all constraints except the last one, but none that satisfies all constraints.
Our goal is finding a measure that can be applied to all of the subsumed tasks and to any combination of them (i.e. to the general document organization problem), and that satisfies all constraints coming from each of the subsumed tasks. In the next section we introduce our proposal.

3. PROPOSAL: RELIABILITY AND SENSITIVITY
Our proposal consists of two complementary measures, Reliability and Sensitivity, with a straightforward initial definition. Let us consider a system output X and a gold standard G, which are both a set of document relationships r(d, d )  { , , }. The Reliability (R) of relationships in the system output is the probability of finding them in the gold standard. Reversely, the Sensitivity (S) of predicted relationships is the probability of finding them in the system output when they appear in the gold standard. In other words, R and S are precision and recall of the predicted set of relationships with respect to the true set of relationships:
R(X )  P (r(d, d )  G|r(d, d )  X )

S(X )  P (r(d, d )  X |r(d, d )  G)

We can express Reliability as a sum of probabilities pondered by the weight of each relationship in X :

R(X ) 

P (r(d, d )  G)wX (r(d, d ))

r(d,d )X

We want to observe three restrictions on relationship weights: (i) wX (r(d, d )) = f (wX (d), wX (d )), i.e., the weight of a relationship is a function of the weights of the documents involved. In Document Retrieval, the weight of a document

will be related to its probability of being inspected by the user, which is related at least to its position in the ranking; (ii) w(d) = d w(r(d, d )), i.e., the weight of all relations starting from a document d determines the weight of document d; this restriction prevents the quadratic effect of counting binary relationships and is related to the "size vs quantity" restriction for the clustering problem that we want to satisfy; and finally (iii) the contribution w(d, d ) of each d related to d should be proportional to the weight of d .
Then, we can express R and S in terms of weights of single documents. Being wX (d) the weight of d in X and being Wx,d the sum weight of documents related with d:

WX ,d =

wx(d )

d /r(d,d )X

we can compute R and S as:

R(X ) =

P

(r(d,

d

)



G)

wx(d ) Wx,d

wx(d)

r(d,d )X

S(v)

=

r(d,d

)G

P

(r(d,

d

)



X

)

wg(d ) Wg,d

wg (d)

If all documents have the same weight in the distributions, R and S simply turn into the average R(d) and S(d) associated to each document:

R(X ) = AvgdP (r(d, d )  G|r(d, d )  X ) S(X ) = AvgdP (r(d, d )  X |r(d, d )  G)

3.1 Estimating Document Weight Discounting

In the generic document organization task we must as-

sume that there exists a virtually unlimited amount of doc-

uments in the collection. Therefore, we must weight docu-

ments according to their priority in the system output or in

the gold standard. There exist several studies on predict-

ing the weight of a document in the ranking in terms of the

probability to be explored by the user. Most of them are

based on assumptions over user behavior [6, 7, 23]. How-

ever, there is no clear consensus yet on how to model the

empirical user behavior. Rather than this, we focus on ba-

sic constraints and interpretability as the main criteria to

choose a weighting scheme.

We model the weight of the document in the i position

as the weight integration between i - 1 and i. The first

constraint is that the sum weight for all documents must be

finite. Therefore, we must employ a function with a converg-

ing

integral.

We

select

1 i2

because

it

is

a

simple,

soft

decay,

integrable and convergent function. We leave the refinement

of the discounting curve for a latter parameterization step;

ideally, our evaluation measure should be as general as pos-

sible, and therefore it must have parameters to establish how

much of the ranking (v.g. the top 10 vs the top 100) carries

on how much of the weight (v.g. 50% or 99% of the overall

score) in the evaluation.

According

to

1 i2

,

the

weight

of

the

document

in

position

i

in the priority ordering is:

c+i 1

1

1

wX (d) = c1

c+i-1

dx x2

=

c1

- c+i-1 c+i

(1)

where c1 is a normalization factor to ensure that the sum is 1. c is another parameter that moves the function in order

647

to give more or less weight to the high priority documents. Stating that the total sum weight is one:

1

1

c1
c

dx x2

=

c1 c

=

1

Therefore, c = c1. The next constraint is that we should be able to parameterize the weighting curve in an interpretable way. Ideally, we want to be able to set two parameters n and Wn which mean that the first n documents must cover a Wn weight ratio of the overall evaluation score. For instance, we may want to state that the first 30 positions in the ranking (n = 30) will have an 80% of the total weight in the evaluation measure (Wn = 0.8). Therefore:

c

c+n c

1 dx
x2

=

Wn

=

c

=

(1 - Wn)n Wn

Now, we can estimate the weight of each document in the system output or in the gold-standard according to Formula 1. Documents at the same priority level share the interval weight. Being n and n= the amount of documents with more and equal relevance than d respectively we can estimate the weight of each document as:

wX (d)

=

c1 n=

c+n +n= 1

dx

c+n

x2

(2)

Smucker and Clarke proposed a discounting model based on exploration time calibration[23] which considers additional aspects such as the relevance and length of documents. Actually, this model is compatible with our proposal: we can incorporate this by replacing the i position of documents with a time function. We leave this analysis for future work.

3.2 Overlapping Clusters
As we mentioned earlier, a document may appear in multiple clusters (corresponding, for instance, to different information nuggets in the document), and therefore it may appear at multiple priority levels. If overlapping between clusters is allowed, a document has potentially a different number of occurrences in the gold and system output distribution. If there exists only one instance of d and d in both the gold standard and the system output, the probability of coocurrence is 1 when the relationships match. Otherwise, following the extended Bcubed measure proposed in [2], we assume the best possible correspondence between relationships in X and G. For instance, if two documents are related in the system output less times than in the gold standard, then all the predicted relationships are assumed to be correct. Otherwise, the probability is the ratio of gold relationships per system relationships. Formally, being |rG(d, d )| and |rX (d, d )| the number of occurrences of r(d, d ) in G and X respectively:

P (r(d, d )  G) = min(|rG(d, d )|, |rX (d, d )|) (3) |rX (d, d )|

P (r(d, d )  X ) = min(|rG(d, d )|, |rX (d, d )|) (4) |rG(d, d )|
3.3 Measure Computation
Here we state the method to compute R and S over the general document organization task. We assume that there is a set of prioritized documents Xr organized by levels and clusters and a special, bottom level containing an unlimited

amount of irrelevant/discarded documents X¬r (see Table 5). The weight of a single document in the set of prioritized documents Xr is computed as in Equation 2. The weight of prioritized documents Xr in the system output is:

c+|Xr | 1

c

W (Xr) = c
c

x2 = 1 - c + |Xr|

The weight of the long tail of non prioritized documents X¬r in the system output is:

1

c

WX (X¬r) = c

= c+|Xr| x2 c + |Xr|

The sum weight of documents priority related with d is 1 minus the sum weight of documents in the same priority level:

WX ,d, = 1 -

wx(d )

d /¬(d  X d)

The probability P (r(dij, dkl)  G) for a document relationship between two document occurrences in Xr is computed as in Equation 3 for both the clustering and priority relationships.
As for the relationships between the unlimited tail X¬r and documents in Xr, we must consider that all documents in the infinite set X¬r have the same weight. Therefore, the finite amount of relevant documents in the long tail has no effect. Then, any relevant document in Xr is correctly related with all documents in X¬r if it appears between the prioritized documents in Gr. According to Equation 3, being dij the j occurrence of document di in the system output and being Di its set of occurrences1:

P (dij

G

X¬r )

=

min(|Di  Gr|, |Di |Di  Xr|

 Xr|)

According to all of this, Reliability over priority relationships can be computed as follows2

R

=
dij dklXr

P (r(dij , dkl)  G)wx(di,j )wx(dk,l) + Wx,dij ,

r(dij ,dkl)X

P (dij
dij Xr

g X¬r)wx(dij )W (X¬r)

1 Wx,dij ,

1 +
Wx (Xr )

Assuming that the documents in the long tail do not have clustering relationships with each other, Reliability over clustering relationships can be computed as follows:

R =

dij Xr

P (dij

g

dkl)wx(dij )wx(dkl) Wx,dij ,

+ Wx(X¬r)

dkl Xr

dij xdkl

Sensitivity is computed in the same way, but exchanging
X by G and x by g. The complexity of this computation is O(n2), being n the amount of ranked documents Xr or prioritized documents Gr.
1For the sake of simplicity, we notate P (dij X¬r  G) as P (dij G X¬r) 2The first component covers the relationships within documents in Xr. The second and third components cover the relationship Xr  X¬r and X¬r  Xr respectively.

648

Figure 1: Comparing Evaluation Measures over Document Filtering Task.

4. RELIABILITY AND SENSITIVITY: METAEVALUATION
As far as we know, Reliability and Sensitivity are the first measures which are applicable to the general document organization task. For this reason, our comparison will focus on how R and S behave in each of the subsumed tasks (filtering, clustering, ranking) with respect to previously existing measures.
4.1 Meta-evaluation Criteria
There are many ways of meta-evaluating a new measure. The most direct one consists of comparing measure scores vs. human assessments of quality, or system results in some extrinsic task. Other meta-evaluation criteria focus on the hability to capture information from limited data sets. Some examples are discriminativeness [23], statistical significant differences between systems[21], stability method, swap method, robustness against noisy data, correlation between rankings over different data sets [5], etc. The main drawback of these methods is that a measure can be, for instance, perfectly discriminative under limited data sets without giving information about quality. As an extreme example, the ranking length can be perfectly discriminative but not useful for evaluation purposes.
in this study we want to investigate how useful is to evaluate measures in terms of a basic, intuitive set of formal constraints. According to this, our first meta-evaluation criterion is the ability to satisfy the stated formal constraints. After that formal analysis, and in order to compare measures empirically over data sets, we will assume that current standard measures used by the community are, to a certain extent, reliable: we assume that all of them give some useful information about system quality in certain scenarios. The problem is that, in most cases, we do not know exactly the real scenarios in which the system will be employed. In previous experiments, particularly in the case of clustering and filtering, it has been shown that there can be a very low correlation between measure results [3]. Therefore, we cannot expect to find a measure which is correlated with all of them. However, we can at least ensure that a high score according to the measure implies a high score according to all measures. This is strictness. In other words, a good score in a reliable measure should ensure a good system regardless of the environmental conditions. Note that strictness itself is not enough as a meta-evaluation criterion (a measure that always scores zero is the strictest of all). Note also that strictness could easily be achieved by computing a harmonic mean of the most popular measures, but we would have to solve scale-normalization issues and we would end

up with a measure that would be hard to interpret, and would not cover all quality aspects in an homogeneous manner. Therefore, and ad-hoc combination of metrics is not the best solution to have a strict measure.
Let us quantify Strictness in the following way: we compute, for each measure, all the rank positions obtained by each system output o  O for each topic and measure. Then we define strictness as the largest difference between a high rank assigned by our measure and a low rank assigned by a traditional measure:
Strictness(m) = - M axo,m (Rankm(o) - Rankm (o)) |O|
In order to maintain a correspondence with standard metaevaluation criteria, we will also compute the robustness of measures in terms of the average Spearman correlation of system scores across topics. Being Rnk(m, ti) the ranking of systems produced by metric m for topic ti:
Robustness(m) = Avgi,j(Spearman(Rnk(m, ti), Rnk(m, tj)))
We now discuss each of the subsumed tasks.
4.2 Clustering Scenario
In the non overlapped clustering scenario where all documents has the same weight, R and S turn into Bcubed Precision and Bcubed Recall:
RX ,G  AvgdP (r(d, d )  G|r(d, d )  X ) = BR(X)
SX ,G  AvgdP (r(d, d )  X |r(d, d )  G) = BR(X)
Bcubed measures are the only ones that satisfy all the clustering constraints [2]. However, in the extended version for overlapping clustering, the Cluster Size vs Quantity restriction is no longer satisfied. This is due to the fact that, in the extended Bcubed version, all the relationships from one document are computed as a single unit, even when it belongs to several clusters at the same time. The result is that if two documents d and d are related to each other several times (e.g. they share more than one information nugget), breaking all these relationships is penalized only once. Therefore, splitting n clusters can have less effect than splitting one document from a cluster with size n. The solution provided by R and S consists of considering the document in each different cluster as a separate document. BCubed measures are well-known and have already been studied formally and empirically and compared with other measures in previous work [2]. Therefore, we will focus on the other subsumed tasks for our meta evaluation.

649

Accuracy Utility Lam% F Measure R*S

Rob.

0.3

0.39 0.38

0.49

0.70

Strict. -0.91 -0.91 -0.96

-0.92 -0.78

Table 7: Robustness and Strictness achieved by Filtering Evaluation Measures, WEPS2 test set

The measures F, Utility and Accuracy achieve -0.92 of Strictness. There exists a clear difference in robustness between R*S and other measures. In this scenario, robustness seems to be correlated with strictness.
4.4 Document Retrieval

4.3 Document Filtering Task
4.3.1 Formal Constraints
The Filtering scenario consists of distributing a finite set of documents into two priority levels (binary classification). Being Xr and Gr the sets of prioritized documents in the system output and gold standard respectively, the correct relationships in the system output are priority relationships between relevant documents in Xr and irrelevant documents in ¬Xr. Therefore, R corresponds with:
R(X) = AvgdP (rg(d, d )|rx(d, d )) = P (Gr|Xr)P (Xr)P (¬Gr|¬Xr) + P (¬Gr|¬Xr)P (¬Xr)P (Gr|Xr) =
P (¬Gr|¬Xr)P (Gr|Xr)(P (Xr) + P (¬Xr)) = P (¬Gr|¬Xr)P (Gr|Xr)
This corresponds with the product of precisions over positive and negative sets in the output. Analogously, Sensitivity corresponds with the product of Recalls over the positive and negative sets.
S(X) = P (¬Xr|¬Gr)P (Xr|Gr)
Just like any other filtering measure, the combination of R and S (using the F measure) satisfies the priority constraint. Let us denotate the harmonic mean of R and S as R*S. With respect to how R*S scores non-informative outputs, its behavior is a mixture of the other measure families: it assigns a zero-score to the all-relevant and all-irrelevant baselines, because they are not able to distinguish any useful priority relationship between documents. Other arbitrary partitions receive scores that depend on how the ground truth partitions the test set.
4.3.2 Empirical meta-evaluation
For the filtering scenario we employ the evaluation corpus and system results from the second task in the WePS3 competition [1]. The task consisted of classifying Twitter entries [17] that contain a company name as relevant when they do refer to the company and irrelevant otherwise. The test set includes tweets for 47 companies and the training set includes 52 company names. For each company, around 400 tweets were retrieved using the company name as query. The ratio of related tweets per company name is variable, covering both extremely low and high ratios. We will refer to a company tweet set retrieved by a query in a time slot as an input stream or topic. Five research teams participated in the competition, and sixteen runs were evaluated. The organizers included two baseline systems: the placebo system (all true) and its opposite (all false).
Figure 1 shows the correspondence between R*S and standard measures employed in different evaluation campaigns; F represents the harmonic mean of precision and recall over the positive class. The most relevant fact is that a high score in all standard measures is necessary to achieve a high score according to R*S. Table 7 shows the Robustness and strictness of measures (see Section 4.1) in this test set. The strictest measure is R*S (-0.78), followed by Lam% (-0.89).

4.4.1 Formal Constraints
Just like MAP and DCG, Reliability and Sensitivity satisfy the first two constraints, Priority and Deepness. Adding an incorrect relationship produces a score decrease, and the effect of an incorrect relationship depends on the deepness of the related documents in the system output ranking. However, unlike MAP or DCG, Reliability and Sensitivity also satisfy the third constraint, Deepness Threshold. And, unlike MRR, they also satisfy the fourth constraint, Closeness Threshold. Due to space constraints, we do not include here the formal proofs.
Note that the parameters n and Wn offer great flexibility, and permit to accomodate scenarios where only the top documents in the rank matter (as in Web search) as well as recall-oriented scenarios, simply adjusting the parameters accordingly. The formal constraints are satisfied for any paratemer setting. For example, with the setting W30 = 0.8, ranking 30 relevant documents after 30 irrelevant documents is worse than retrieving one document in the first position, but retrieving five relevant documents in the first 10 positions is better than retrieving only one document at the top 1. The cut point is n=20.
As for the confidence constraint, note that the more we include irrelevant documents in the ranking, the more we include incorrect priority relationships between the priorized documents and the long tail. We could also satisfy this property with measures like RBP by using a discounting score for each irrelevant document, but then the nature of RBP would change and its formal properties would not hold any longer. Table 6 summarizes the results of the formal constraint analysis.
4.4.2 Empirical Meta-evaluation
We have used queries 701 to 750 in the GOV-2 collection, which were employed in the TREC 2004 Terabyte Track. The GOV-2 corpus consists of approximately 25 million documents. Therefore, we can assume that the amount of documents in the collection is unlimited for practical purposes. Relevant documents were manually annotated for each query. We consider the results of 60 retrieval systems developed by the participants in the track. Two relevance levels (high and medium) were considered in the human annotation.
Table 8 shows the strictness and robustness3 of measures. We consider the strictness of all measures against the standard measures MAP, DCP, P@10, MRR, RBPp=0.8 and RBPp=0.95. The strictest measure is R*S with W80 = 30 (-0.58) followed by MRR (-0.63), MAP (-0.74) and P@10 (0.75). However, R*S with W80 = 30 achieves low robustness, while R*S with n80 = 800 and DCG have higher robustness at the cost of strictness. It seems that there is a trade-off between strictness and robustness. A possible explanation is that the robustness of measures across test cases depends on the amount of data that is considered for the evalua-
3Discriminating systems that return short rankings is easy. In order to avoid this noise, we only consider in this test those systems that return at least 1000 documents

650

Measure
Robustness Strictness

MAP
0.55 -0.89

DCG
0.60 -0.68

P@10
0.3 -0.76

MRR
0.4 -0.63

RBP p=0.8 0.28 -0.92

RBP p=0.95
0.39 -0.65

R*S W30 0.35 -0.58

R*S W800 0.57 -0.73

Table 8: Robustness and strictness of Document Retrieval Evaluation Measures, GOV2 test set.

Gold Standard
d1 {d2 d3 d4} {d4 d5} {d6 d7}
{d6 d7} R =1 S =1
R=1 S=1
System Output 2
d1 {d2 d3}{ d4} {d4 d5} {d6 d7}
{d6 d7} R =1 S =1 R=1 S=0.8
System Output 4
d1 {d3 d4 } {d4 d5} {d6 d7}
{d6 d7 d8} R =0.95 S =0.85
R=0.96 S=0.74

System Output 1
d1{d2 d3 d4} {d4 d5}{d6 d7}
d6 d7 R =1 S =1 R=1 S=0.97
System Output 3
d1 {d3 d4 } {d4 d5} {d6 d7}
{d6 d7} R =1 S =0.86
R=1 S=0.74
System Output 5
{d6 d7} {d4 d5} {d6 d7}
d1 {d2 d3 d4} R =0.64 S =0.59
R=1 S=1

Table 9: Sensitivity and Reliability examples for the generic Document Organization scenario.

tion. Therefore, measures focused on the top of the ranking are less robust. Given that the max function in the strictness definition is very sensitive to outlier systems, we have also computed strictness considering the 10 maximum differences; results were the same.
5. GENERIC DOCUMENT ORGANIZATION SCENARIO
As far as we know, there is no measure that can be directly compared with R*S in the generic document organization scenario as we have defined it. In this section, we illustrate the behavior of R*S across several instances of system outputs. See Table 9. The gold standard consists of seven relevant documents distributed along three priority levels. Each priority level contains two clusters (or information nuggets) and documents 4,6 and 7 appear in two clusters and priority levels simultaneously. The Reliability and Sensitivity over priority and clustering relationships have been computed with W10 = 0.8, i.e., we require that the first 10 occurrences represent 80% of the score.
Of course, Reliability and Sensitivity are maximal for the gold standard. Starting from this, we can identify in the table the following behaviors: (System 1) breaking clusters at low priority levels decreases slightly S, (System 2) breaking clusters at high priority levels decreases S to a greater extent; (System 3) Removing one document (d2) decreases priority and clustering sensitivity; (System 4) removing and adding noisy documents (d2 and d8) decreases both sensitivity and reliability scores and in (System 5) we swap all priority levels. Then, clustering is perfect and the priority R and S decrease, but not to zero. Notice that the 10 first documents have only a 80% of weight in the evaluation. There exists a long tail of documents from which this set is priorized.

6. CONCLUSIONS
In this paper we have discussed how some prominent Information Access tasks ­ Document Retrieval, Clustering and Filtering ­ can be subsumed under a generic Document Organization task that establishes two kinds of binary relationships between documents: relatedness (which forms clusters) and priority (ranking). We have then introduced two evaluation measures for the document organization problem: Reliability and Sensitivity, which are precision and recall of the predicted set of relationships with respect to the true relationships, with a specific relative weighting scheme between relations. The main contribution of this paper is that R and S can be applied to complex tasks which involve ranking, clustering and filtering at the same time. An example task is online reputation monitoring, where systems have to (i) filter out irrelevant information, (ii) organize relevant information in topics, and (iii) decide which topics have more priority from the point of view of reputation management. R and S are able to provide a unique evaluation measure for this combined problem.
In addition, R and S satisfy all formal constraints in each of the subsumed tasks; in particular, they satisfy more formal constraints than any previous measure in the Document Retrieval task, and they are able to accomodate several retrieval scenarios (from precision-oriented to recall-oriented) via two parameters that establish that the first n levels in the rank carry on a fraction Wn of the overall quality score. Our empirical study indicates that R and S are stricter than standard measures, i.e., a high result with R and S ensures a high result with any other standard measure in all the subsumed tasks. That makes R and S a preferable choice in cases where the application scenario does not clearly point towards any of the previously existing measures, because it guarantees that a good result will still hold according to any other standard measure.
For its application to combined tasks, future work involves extending the set of binary relations ­ the general principle of R and S can be applied to any kind of relations ­ together with a a careful analysis on how to assign relative weights to different types of relations; for instance, in certain application scenarios one type of relation (priority or relatedness) may dominate and obscure what is going on with the other, making R and S less transparent and/or useful.
Code to use R and S is available at http://nlp.uned.es.
Acknowledgments
This work has been partially funded by EU FP7 project Limosine (grant number 288024), a Google Faculty Research Award (Axiometrics, project Holopedia (grant from the Spanish goverment) and project MA2VICMR (grant from the government of Comunidad de Madrid).

651

7. REFERENCES
[1] E. Amigo´, J. Artiles, J. Gonzalo, D. Spina, B. Liu, and A. Corujo. WePS3 Evaluation Campaign: Overview of the On-line Reputation Management Task. In 2nd Web People Search Evaluation Workshop (WePS 2010), CLEF 2010 Conference, Padova Italy, 2010.
[2] E. Amig´o, J. Gonzalo, J. Artiles, and F. Verdejo. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Inf. Retr., 12:461­486, August 2009.
[3] E. Amig´o, J. Gonzalo, and F. Verdejo. A comparison of evaluation metrics for document filtering. In Proceedings of CLEF'11, CLEF'11, pages 38­49, Berlin, Heidelberg, 2011. Springer-Verlag.
[4] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '04, pages 25­32, New York, NY, USA, 2004. ACM.
[5] B. Carterette. System effectiveness, user models, and user utility: a conceptual framework for investigation. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR '11, pages 903­912, New York, NY, USA, 2011. ACM.
[6] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In CIKM, pages 611­620, 2011.
[7] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In WWW, pages 1­10, 2009.
[8] J. M. Cigarra´n, A. Pen~as, J. Gonzalo, and F. Verdejo. Automatic selection of noun phrases as document descriptors in an fca-based information retrieval system. In ICFCA, pages 49­63, 2005.
[9] C. L. A. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR, pages 659­666, 2008.
[10] J. Cohen. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37, 1960.
[11] G. Cormack and T. Lynam. Trec 2005 spam track overview. In Proceedings of the fourteenth Text Retrieval Conference 8TREC 2005), 2005.
[12] G. V. Cormack and T. R. Lynam. TREC 2005 Spam Track Overview. In Proceedings of the fourteenth Text REtrieval Conference (TREC-2005), 2005.
[13] M. Halkidi, Y. Batistakis, and M. Vazirgiannis. On Clustering Validation Techniques. Journal of Intelligent Information Systems, 17(2-3):107­145, 2001.
[14] M. A. Hearst and J. O. Pedersen. Reexamining the cluster hypothesis: Scatter/gather on retrieval results. pages 76­84, 1996.
[15] B. Hu, Y. Zhang, W. Chen, G. Wang, and Q. Yang. Characterizing search intent diversity into click models. In Proceedings of the 20th international conference on World wide web, WWW '11, pages 17­26, New York, NY, USA, 2011. ACM.
[16] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based

evaluation of ir techniques. ACM Trans. Inf. Syst., 20:422­446, October 2002.
[17] B. Krishnamurthy, P. Gill, and M. Arlitt. A few chirps about twitter. In WOSP '08: Proceedings of the first workshop on Online social networks, pages 19­24, New York, NY, USA, 2008. ACM.
[18] A. Leuski. Evaluating document clustering for interactive information retrieval. In CIKM, pages 33­40, 2001.
[19] M. Meila. Comparing clusterings. In Proceedings of COLT 03, 2003.
[20] T. M. Mitchell. Machine learning. McGraw Hill, New York, 1997.
[21] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):2:1­2:27, Dec. 2008.
[22] A. Rosenberg and J. Hirschberg. V-measure: A conditional entropy-based external cluster evaluation measure. In Proceedings of EMNLP-CoNLL 2007, pages 410­420, 2007.
[23] M. D. Smucker and C. L. Clarke. Time-based calibration of effectiveness measures. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, SIGIR '12, pages 95­104, New York, NY, USA, 2012. ACM.
[24] S. Vargas and P. Castells. Rank and relevance in novelty and diversity metrics for recommender systems. In 5th ACM Conference on Recommender Systems (RecSys 2011), pages 109­116, Chicago, Illinois, October 2011.
[25] E. M. Voorhees. The trec-8 question answering track report. In In Proceedings of TREC-8, pages 77­82, 1999.

652

Modeling Term Dependencies with Quantum Language Models for IR

Alessandro Sordoni

Jian-Yun Nie

sordonia@iro.umontreal.ca nie@iro.umontreal.ca

DIRO, Université de Montréal Montréal, H3C 3J7, Québec

Yoshua Bengio bengioy@iro.umontreal.ca

ABSTRACT
Traditional information retrieval (IR) models use bag-ofwords as the basic representation and assume that some form of independence holds between terms. Representing term dependencies and defining a scoring function capable of integrating such additional evidence is theoretically and practically challenging. Recently, Quantum Theory (QT) has been proposed as a possible, more general framework for IR. However, only a limited number of investigations have been made and the potential of QT has not been fully explored and tested. We develop a new, generalized Language Modeling approach for IR by adopting the probabilistic framework of QT. In particular, quantum probability could account for both single and compound terms at once without having to extend the term space artificially as in previous studies. This naturally allows us to avoid the weight-normalization problem, which arises in the current practice by mixing scores from matching compound terms and from matching single terms. Our model is the first practical application of quantum probability to show significant improvements over a robust bag-of-words baseline and achieves better performance on a stronger non bag-of-words baseline.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords
Density Matrices; Language Modeling; Retrieval Models
1. INTRODUCTION
The quest for the effective modeling of term dependencies has been of central interest in the information retrieval (IR) community since the inception of first retrieval models. However, the gradual shift towards non bag-of-words models is strewn with modeling difficulties. One of the central
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

problems is to find an effective way of representing and scoring documents based on such dependencies. As pointed out by Gao et al. [9], dependencies can be handled in two ways.
The first approach is to extend the dimensionality of the representation space. In early geometrical retrieval models such as the Vector Space Model (VSM), dependencies arising from phrases (compound terms) are represented by defining additional dimensions in the space, i.e. both the phrase and its component single terms are regarded as representation features [8, 21, 28]. For example, computer architecture is considered as disjoint from computer and architecture, which is a strong modeling assumption, and does not take advantage of the semantic relation that generally exists between a compound phrase and its component terms.
The second approach is more principled in such that simple terms are kept as representational units and term dependencies are modeled statistically as joint probabilities, i.e. p(computer, architecture). Proposed dependence models such as n-gram Language Model (LM) for IR [30], biterm LM [31] or the dependence LM [9] adopt such a representation. However, the gain from integrating dependencies was smaller than hoped [35] and it came with higher computational costs due to dependency parsing or n-gram models [13, 30], or unsupervised iterative methods for estimating the joint probability [9].
Recently, non bag-of-words models such Markov random field (MRF) [19], quasi-synchronous dependence model [24] and the query hypergraph model [2] have been proposed. Most of these retrieval models take a log-linear form, which offers a very flexible way of taking into account term dependencies by integrating different sources of evidence, such as proximity heuristics and exact matching. However, the LM is used as a black box to estimate single-term and compoundterm influences separately and then the model combines them to compute the final score. We believe that, from a representational point of view, these models have implicitly made a turn back to the first VSM approach in the sense that the dependencies are assumed to represent additional concepts, i.e. atomic units for the purpose of document and query representation, thus disjoint from the component terms [2, 3]. This choice indeed allows for flexible scoring functions. However, the retrieval model boils down to a combination of scores obtained separately from matching single terms and from matching compound dependencies. This is the main cause of the weight-normalization problem [9, 11] which is that a dependency may be counted twice, as a compound and as component terms. In the context of phrases, Sparck Jones et al. note that "the weight of the

653

phrase should reflect not the increased odds of relevance implied by its presence as compared to its absence, as a whole unit, but the increased odds compared to the presence of its components words" [11]. When integrating the evidence, the weights for the combination are usually estimated by optimizing a retrieval measure such as mean average precision (MAP). In this sense, a principled probabilistic interpretation of these models is difficult.
The pioneering work by Van Rijsbergen [33] officially formalized the idea that Quantum Theory (QT) could be seen as a "formal language that can be used to describe the objects and processes in information retrieval". The idea of QT as a framework for manipulating vector spaces and probability is appealing. However, the methods that stem from this initial intuition provided only limited evidence about the usefulness and effectiveness of the framework for IR tasks. For example, Piwowarski et al. [25] test if acceptable performance for ad-hoc tasks could be achieved with a quantum approach to IR. The authors represent documents as subspaces and queries as density operators. However, both documents and queries representations are estimated through passage-retrieval like heuristics, i.e. a document is divided into passages and is associated to a subspace spanned by the vectors corresponding to document passages [25]. Different representations for the query density matrix are tested but none of them led to good retrieval performance. Successively, a number of works took inspiration from quantum phenomena in order to relax some common assumption in IR [37, 38]. Zuccon and Azzopardi [38] introduce interference effects into the Probability Ranking Principle (PRP) in order to rank interdependent documents. Although this method achieves good results, it does not make principled use of the quantum probability space and cannot be considered as evidence towards the usefulness of the enlarged probabilistic space. In general, these methods made heuristic use of the concepts of the theory and no clear probabilistic interpretation can be given.
The intrinsic heuristic flavor in preceding approaches motivated some authors to provide evidence to the hypothesis that there exists an IR situation in which classical probabilistic IR fails, or it is severely limited, and it is thus necessary to switch to a more general probabilistic theory [16, 17, 34]. Although these works are theoretically grounded and heavily influenced our general vision of the theory, no clue is given on how to operationalize such results in real-world applications.
In this paper, we propose a novel retrieval framework for modeling term dependencies based on the probabilistic calculus offered by QT. In our model, both single terms and compound dependencies are mathematically modeled as projectors in a vector space, i.e. elementary events in an enlarged probabilistic space. In particular, a compound dependency is represented as a superposition event which is a special kind of projector that is neither disjoint from its component terms, nor a joint event. Documents and queries are represented as a sequence of projectors associated to a quantum language model (QLM), encapsulated in a particular matrix. The scoring function is a divergence between query and document QLMs. We will show that our model is a generalization of classical unigram LMs. To our knowledge, this work can be seen as the first work to use the quantum probabilistic calculus in order to achieve improvements over state-of-the-art models.

Our contributions are as follows:
1. We propose a novel application of quantum probability to IR.
2. Using this approach, we show significant improvements over a strong baseline bag-of-words model and a strong non bag-of-words model.
3. We propose a new way of representing dependencies without artificially extending the term space and without estimating expensive n-gram probabilities.
4. We show how the new representation of the dependency permits to specify how the dependency behaves with respect to its component terms.
5. In our model, the dependency information is not integrated in the scoring phase, but in the estimation phase. Hence, our model does not suffer the weightnormalization problem.

2. A BROADER VIEW ON PROBABILITY

2.1 The Quantum Sample Space

In quantum probability, the probabilistic space is natu-

rally encapsulated in a vector space, specifically a Hilbert

space, noted Hn, but for the sake of simplicity, in this pa-

per we limit ourselves to finite real spaces, noted Rn. We

will be using Dirac's notation restricted to the real field, for

which a unit vector u  Rn, u 2 = 1 and its transpose u are respectively written as a ket |u and a bra u|. Using

this notation, the projector onto the direction u writes as

|u u|. The inner product between two vectors writes as

u|v . Moreover, we note by |ei the elements of the standard basis in Rn, i.e. |ei = (1i, . . . , ni), where ij = 1 iff

i = j.

Events are no more defined as subsets but as subspaces,

more specifically as projectors onto subspaces [23, 34]. Given

a 1-dimensional subspace spanned by a ket |u , the projector

onto the unit norm vector |u , |u u|, is an elementary event

of the quantum probability space, also called a dyad. A dyad

is always a projector onto a 1-dimensional space. Given the

bijection between subspaces and projectors, it is correct to

state that |u is itself an elementary event. For example,

if n = 2, the quantum elementary events |e1 = (1, 0),

|f1

=

(

1 2

,

1 2

),

can

be

represented

by

the

following

dyads:

|e1 e1| =

1 0

0 0

, |f1

f1| =

0.5 0.5

0.5 0.5

.

(1)

Generally, any ket |v = i i|ui is called a superposition of the {|ui } where {|u1 , . . . , |un } form an orthonormal basis. In order to see the generalization that is taking place, one has to consider that in Rn there is an infinite number of
vectors even if the dimension n is finite. Hence, contrary to
the classical case, an infinite number of elementary events
can be defined.

2.2 Density Matrices
A quantum probability measure µ is the generalization of a classical probability measure such that (i) for every dyad |u u|, µ(|u u|)  [0, 1] and (ii) it reduces to a classical probability measure for any orthonormal basis {|u1 , . . . , |un }, i.e. i µ(|ui ui|) = 1. Gleason's Theorem [10] states that,

654

(a)



=

(

0.75 0

0 0.25

)

(b)



=

(

0.5 0.5

0.5 0.5

)

(c)



=

(

0.5 0.25

0.25 0.5

)

Figure 1: The ellipses depict the set of points {|u : |u  R2}. The eigenvalues of  define how much each ellipse is stretched along the corresponding eigenvectors. To the left,  corresponds to a classical probability distribution. To the center,  is a pure state, thus the ellipse degenerates along the eigenvector corresponding to its unit eigenvalue. To the right, a general density matrix for which we vary both the eigenvalues and the eigensystem.

for any real vector space with dimension greater than 2, there is a one-to-one correspondence between quantum probability measures µ and density matrices . The form of this correspondence is given by:

µ(|v v|) = tr(|v v|).

(2)

A real density matrix is symmetric,  = , positive semidefinite,   0, and of trace 1, tr  = 11. From now on, the set of n × n real density matrices would be noted Sn.
By Gleason's theorem, a density matrix can be seen as the proper quantum generalization of a classical probability distribution. It assigns a quantum probability to each one of the infinite dyads. For example, the density matrix:

=

0.5 0.5

0.5 0.5

,

(3)

assigns probabilities tr(|e1 e1|) = 0.5 and tr(|f1 f1|) = 1. Hence, the event |f1 f1| is certain and still there is non-classical uncertainty on |e1 e1|. Only if {|u1 , . . . , |un } form an orthonormal system of Rn can the dyads |ui ui| be understood as disjoints events of a classical sample space, i.e.

their probabilities sum to one. The relation that ties |e1 e1| and |f1 f1| is purely geometrical and cannot be expressed using set theoretic operations.

Any classical discrete probability distribution can be seen

as a mixture over n elementary points, i.e. a parameter  =

(1, . . . , n), where i  0 and i i = 1. The density matrix is the straightforward generalization of this idea by con-

sidering a mixture over orthogonal dyads  = i i|ui ui| where i  0 and i i = 1. Given a density matrix , one can find the components dyads by taking its eigende-

composition and building a dyad for each eigenvector. We

note such decomposition by  = RR =

n i=1

i|ri

ri|,

where |ri are the eigenvectors and i their corresponding

eigenvalues. This decomposition always exists for density

matrices [23].

Conventional probability distributions can be represented

by diagonal density matrices. The sample space corresponds to the standard basis E = {|ei ei|}ni=1. Hence, the density

matrix corresponding to the parameter  above can be repre-

sented as a mixture over E, i.e.  = diag() = i i|ei ei|.

1The trace is equal to the sum of the diagonal terms in a matrix.

Consider a vocabulary of two terms V = {a, b}. A unigram
language model  = (0.75, 0.25) defined on V is represented by:



=

3 4

|ea

ea|

+

1 4

|eb

eb| =

0.75 0

0 0.25

.

Hence, term projectors are orthogonal, i.e. terms correspond

to disjoint events. For example, the probability of the term

a is computed by tr(|ea ea|) = 0.75. As conventional

probability distributions are restricted to the identity eigen-

system, they differ in their eigenvalues, which correspond to

diagonal entries. On the contrary, general density matrices

can differ also in the eigensystem. For example, the density

matrix  of Eq. 3 has eigenvector |f1

=

(

1 2

,

1 2

)

with

eigenvalue 1 and the eigenvector |f2

=

(

1 2

,

-

1 2

)

with

eigenvalue 0. Hence, it can be represented as a one-element

mixture containing the projector  = |f1 f1|. When the

mixture weights are concentrated into a single projector, the

corresponding density matrix is called pure state. Otherwise,

it is called mixed state.

When defined over Rn, density matrices can be seen as el-

lipsoids, i.e. deformations of the unit sphere (Figure 1) [34].

Classical probability distributions, i.e. diagonal density ma-

trices, are ellipsoids stretched along the identity eigensys-

tem. As quantum probability has access to an infinite num-

ber of eigensystems, the ellipsoid can be "rotated", i.e. de-

fined on a different eigensystem. In this work, we will use

this additional feature in order to build a more reliable rep-

resentation of documents and queries taking into account

more complex information than single terms.

3. QUANTUM LANGUAGE MODELS
The approach Quantum Language Modeling (QLM) retains the classical Language Modeling for IR as a special case. Hereafter, we will present in details the quantum counterpart of unigram language models. Although it is not explicitly developed in this paper, we argue that arbitrary n-gram models could be modeled as well.
3.1 Representation
In classical bag-of-words language models, a document d is represented by a sequence of i.i.d. term events, i.e.

655

Wd = {wi : i = 1, . . . , N }, where N is the document length. Each wi belongs to a sample space V, corresponding to the vocabulary, of size n. It is assumed that such sequences cor-
respond to a sample from an unknown distribution  over the vocabulary V, for which we want to gain insight.
A quantum language model assigns quantum probabilities to arbitrary subsets of the vocabulary. It is parametrized by an n × n density matrix ,   Sn, where n is the size of the vocabulary V. In QLM, a document d is considered as a sequence of M quantum events associated with a density matrix :

Pd = {i : i = 1, . . . , M },

(4)

where each i is a general dyad |u u| and represents a sub-
set of the vocabulary. Note that the number of dyads M can
be different from N , the total number of terms in the document. The sequence Pd is constructed from the observed terms Wd: we have to define how to map subsets of terms to
projectors. Separating the observed text from the observed
projectors constitutes the main flexibility of our model. In
what follows, we define a way of mapping single terms and
arbitrary dependencies to quantum elementary events. Formally, we seek to define a mapping m : P(V)  L(Rn), where P(V) is the powerset of the vocabulary and L(Rn) is the set of dyads on Rn. As an initial assumption, we set m() = O, where O is the projector onto the zero vector.

3.1.1 Representing Single Terms
In Section 2.2, we showed that unigram sample spaces can be represented as the set of projectors on the standard basis E = {|ei ei|}ni=1 and unigram language models can be represented as mixtures over E, i.e. diagonal matrices. Therefore, a straightforward mapping from single terms to quantum events is:

m({w}) = |ew ew|,

(5)

where w  V. This choice associates the occurrence of

each term to a dyad |ew ew|, and these dyads form an or-

thonormal basis. Hence, occurrences of single terms are still

represented as disjoint events. Consider n = 3 and V =

{computer, architecture, games}. If Wd = {computer, archi-

tecture} and one applies m to each of the terms, the sequence

of corresponding projectors is Pd = {Ecomputer, Earchitecture}

where Ew = |ew ew|:





100





000

Ecomputer = 0 0 0 , Earchitecture = 0 1 0 . (6)

000

000

Note that if we decide to observe only single terms, Pd turns out to be the quantum counterpart of classical observed terms Wd, i.e. M = N .

3.1.2 Representing Dependencies
In this paper, by dependency, we mean a relationship linking two or more terms and we represent such an entity abstractly by a subset of the vocabulary, i.e.  = {w1, . . . , wK }. We define the following mapping for an arbitrary dependency :

K
m() = m({w1, . . . , wK }) = | |, | = i|ewi , (7)
i=1
where the coefficients i  R must be chosen such that i i2 = 1, in order to ensure the proper normalization of

Figure 2: The dependency ca is modeled as a projector onto |ca , i.e. as a superposition event.

| . The well-defined dyad | | is a superposition event.

As we showed in Section 2.2, superposition events are justifi-

able only in the quantum probabilistic space. They are nei-

ther disjoint from their constituents |ewi ewi | nor do they solely constitute joint events in the sense of n-grams: here,

the compound dependency is not considered as an additional

entity, as done in previous models [2, 3, 19, 21]. The pro-

posed mapping allows for the representation of relationships

within a group of terms by creating a new quantum event

in the same n-dimensional space.

In addition, superposition events come with a flexible way

in quantifying how much evidence the observation of depen-

dency  brings to its component terms. This is achieved by

changing the distribution of the i: if one wants to attempt

a classical interpretation, the i can be viewed as relative

pseudo-counts, i.e. observing | | adds fractional occur-

rence to the events of its component terms |ewi ewi |. To our knowledge, until now this feature has been only mod-

eled heuristically, or not modeled at all. In our framework,

it fits nicely in the quantum probabilistic space by specify-

ing how a compound dependency event and its constituent

single terms events are related.

As an example, one could model the compound depen-

dency between computer and architecture, ca = {computer,

architecture}, by the dyad Kca = |ca ca|, where |ca =

2/3|ec + 1/3|ea (Figure 2). With respect to the exam-

ple taken above, the event is represented by the matrix:

2

Kca

=



3 2

3

 2 3 1 3

 0 0 .

(8)

0 00

The superposition coefficients entail that observing Kca adds more evidence to |ec ec| than to |ea ea|.

3.1.3 Choosing When and What to Observe
Once we have defined the mapping m, one must ask three questions:

1. Which compound dependencies to consider?
2. When does such a compound dependency hold in a document?
3. When the compound dependency is detected, should we also consider the projectors for its subsets as observed events?

Regarding the first question, one may (a) use a dictionary of phrases or frequent n-grams, or (b) assume that any

656

w1

w2 w3

w4

w5 w6

w7

w8

Sakamura says he created Tron a computer architecture

1

2

3

4

5

6

7

8

ESakamura Esays Ehe Ecreated ETron Ea Ecomputer Earchitecture

9
Kca

1

2

3

4

5

6

ESakamura Esays Ehe Ecreated ETron Ea

7
Kca

Figure 3: Two possible quantum sequences Pdi of an excerpt Wd from a TREC collection. The observation of computer architecture is associated to a superposition projector Kca = |ca ca| while Ew = |ew ew| are classical projectors. For Pd2 we observed only the compound while in Pd1 we also added its subsets.

subset of terms that appear in short queries are candidate compound dependencies to capture. In this paper, we want to make the approach as independent as possible of any linguistic resource. So the second approach (b) is used. This will also allow us to make a fair comparison with the previous approaches using the same strategy (such as the MRF model [19]).
The second question regards whether such selected compound dependencies hold in a given document. In other words, one has to decide when to add the selected dependency projector into a document sequence Pd. This can be done for example by assuming that the components terms in the dependency appear as a bigram in a document, as biterm or in a unordered window of L terms. Convergent evidence from different works [1, 12, 14, 18, 31, 36] confirms that proximity is a strong indicator of dependence. Therefore, in this work we choose to detect a dependency if its component terms appear in a fixed-window of length L.
The third question regards how to apply the mapping m and can be more easily understood by a practical example. Consider a document Wd = {computer, architecture} and a query Wq = {computer, architecture}. Once the dependency ca = {computer, architecture} has been detected in the document, i.e. the component terms appear next to each other, one can further decide:
1. to map only the dependency, i.e. Pd = {Kca},
2. to map both the dependency and the component terms, i.e. Pd = {Ecomputer, Earchitecture, Kca}.
These two choices are illustrated in Figure 3. The first choice is a highly non-classical one because it completely steals the occurrence of its component terms. Nevertheless, it becomes a valid choice in our framework. Differently from classical approaches, the fact that we only consider a count for the compound computer architecture does not mean that we assume that the terms computer and architecture do not occur. The dependency event is not disjoint from the single term events, and its occurrence partially entails the occurrence of its component terms. However, this choice is more dangerous because it over-penalizes the component terms: we should know very precisely when such a strong dependency is observed and which coefficients to assign to it.
The second choice is implicitly done in current dependency models and is at the basis of the weight-normalization problem. From this point of view, the sequence Pd could be seen as composed by concepts as recently formalized by Bendersky et al. [2, 3]. However, there are crucial differences from

that work: (1) we give a clear probabilistic status to such concepts and (2) we do not assume that concepts are atomic units of information, completely unrelated from each other. In classical dependence models, single terms and compound dependencies are scored separately and then the scores are combined together [2, 19, 35]. A critical aspect of such models is that the occurrence of the phrase computer architecture will be counted twice - as single terms and as a compound. That is why the score on compound dependencies must be reweighed before integrating it with the independence score [9, 11, 19]. Contrary to classical models, our model does not suffer from such a problem because the evidences brought by the compound dependency as a whole and by its component terms are integrated in the estimation phase. Even if not reported explicitly in the experiments section, conducted experiments show that including projectors for both the dependency and its subsets is much more effective for the ad-hoc task evaluated here and thus this strategy will be preferred throughout this paper. In addition, an algorithm building the sequence of projectors from the document sequence will be presented in Section 4.3.1.
3.2 Estimation

3.2.1 Maximum Likelihood Estimation
Given that a document is represented by a set of observed projectors, one has to find ways to learn a quantum language model  to associate with a document. In QT, a number of objective functions have been proposed to estimate an unknown density matrix from a set of projectors: Linear Inversion [23] and Hedged ML [4] are notorious examples. In this work, we use the Maximum Likelihood (ML) formulation proposed in [15], because (1) it can easily be seen as a quantum generalization of a classical likelihood function (2) contrary to linear inversion, ML generates a well-defined density matrix, i.e.   Sn, and (3) proposed estimation methods remain computationally affordable in high-dimensional spaces.
Given the observed projectors Pd = {1, . . . , M } for document d, we define as training criterion for the quantum language model  the maximization of the following product proposed in [15] and corresponding in the unigram case to a proper likelihood:

M

LPd () = tr(i).

(9)

i=1

657

The estimate  can be obtained by approximately solving the following maximization problem:

maximize


log LPd ()

(10)

subject to   Sn.

This maximization is difficult and must be approximated by using iterative methods. In [15], the following iterative scheme is proposed, also called the "RR algorithm". One introduces the operator:

R()

=

M i=1

1 tr(i)

i

,

(11)

and updates an initial density matrix (0) by applying repetitive iterations:

(k+1)

=

1 Z

R((k)

)(k)

R((k)

),

(12)

where, Z = tr(R((k))(k)R((k))) is a normalization factor in order to ensure that (k+1) respects the constraint of unitary trace [15]. Despite the RR algorithm being a quantum generalization of the well-behaving Expectation Maximization (EM) algorithm, the likelihood is not guaranteed to increase at each step because the nonlinear iteration may overshoot, similarly to a gradient descent algorithm with a too big step size. Characterizing such situations still remains an open problem [27]. In this work, in order to ensure convergence, if the likelihood is decreased at k + 1, we use the following damped update:

(k+1) = (1 - )(k) + (k+1),

(13)

where   [0, 1) controls the amount of damping and is optimized by linear search in order to ensure the maximum increase of the training objective2. As Sn is convex [23], (k+1) is a proper candidate density matrix. The process stops if the change in the likelihood is below a certain threshold or if a maximum number of iterations is attained.
From an IR point of view, the metric divergence problem [22] tells us that the maximization of the likelihood does not mean that the evaluation metric under consideration, such as mean average precision, is also maximized. In the experiments section, we address the two following questions from a perspective closer to IR concerns:

1. Which initial matrix (0) to choose? 2. When to stop the update process?

As the estimation of a quantum document model requires an iterative process, one may believe that the complexity will make the process intractable. In Section 4.5, we provide an analysis of the complexity of the proposed computation, which will show that the process is quite tractable.

3.2.2 Smoothing Density Matrices
The ML estimation presented above suffers from a generalization of the usual zero-probability problem of classical ML, i.e. the estimator assigns zero probability to unseen data [35]. This is also called the zero eigenvalue problem [4]. Bayesian smoothing for density matrices has not yet been proposed. This may be because Bayesian inference
2Similar damped updates were successfully used in [26] to improve convergence and stability of the loopy belief propagation algorithm.

in the quantum setting has just started to be the subject of intensive research [5, 34]. In this work, we propose to smooth density matrices by linear interpolation [35]. If d is a document quantum language model obtained by ML, its smoothed version is obtained by interpolation with the ML collection quantum language model c:

d = (1 - d) d + d c,

(14)

where d  [0, 1] controls the amount of smoothing. As

the set of density matrices Sn is convex, the resulting d

is a proper density matrix. In this work, we assume that

d

=

µ (µ+M

)

,

which

is

the

well-known

form

of

the

parameter

for Dirichlet smoothing [35].

3.3 Scoring
The flexibility of the Kullback Liebler (KL) divergence approach in keeping distinct query and document representations makes it attractive for a candidate scoring function in our new framework. The direct generalization of classical KL divergence was introduced by Umegaki in [32] and is called quantum relative entropy or Von-Neumann (VN) divergence. Given two quantum language models q and d for the query and a document respectively, our scoring function is the negative query-to-document VN divergence:

-V N (q d) = - tr(q(log q - log d))

ra=nk

tr(q log d),

(15)

where log applied to a matrix denotes the matrix logarithm, i.e. the classical logarithm applied to the matrix eigenvalues. Rank equivalence is obtained by noting that tr(q log q) does not depend on the particular document. Denote by q = i qi |qi qi|, d = i di |di di| the eigendecompositions of the density matrices q and d respectively. By substituting into the above equation, the scoring function rewrites as:

-V N (q||d) ra=nk

qi

log dj qi|dj 2.

i

j

(16)

Compared to a classical KL divergence, the additional term qi|dj 2 quantifies the difference in the eigenvectors between the two models. Following the representation introduced in Section 2.2, the VN divergence compares two ellipsoids not only by differences in the "shape" but also by differences in the "rotation".
If a VSM-like interpretation is attempted, one can think about {|qi }, {|dj } as semantic concepts for the query and the document respectively, whereas the vectors of eigenval-
ues q, d denote the importance of the corresponding semantic concepts in the two models. The VN divergence offers a way of matching query concepts by analyzing how much such concepts are related to documents concepts, i.e. i, j, qi|dj 2. Particularly, j qi|dj 2 = 1. Thus, qi|dj 2 can be interpreted as the quantum probability associated with the pure state |qi qi| for the elementary event |dj dj|, i.e. µqi (|dj dj |) = tr(|qi qi|dj dj |) = qi|dj 2. Hence, one could rewrite Eq. 16 as:

-V N (q||d) ra=nk

qi Eµqi log d .

i

(17)

Therefore, the VN divergence scores a document based on the expectation of how important concept |qi is in document d even if it does not appear in it explicitly.

658

(a) -V N (q d1 )  -.76

(b) -V N (q d2 )  -1.06

o

Wo

Po

q

{computer, architecture}

{Ec, Ea, Kca}

d1 {computer, architecture, and, games} {Ec, Ea, Kca, Eg}

d2 {computer, games, and, architecture} {Ec, Eg, Ea}

Figure 4: A synthetic example of QLM with a vocabulary of n = 3 terms. The orthogonal rays are the eigenvectors of the ellipsoids. q is not smoothed thus degenerates onto a ray. d1 rotates towards the direction of observed query dependencies and is thus ranked higher.

3.4 Final Considerations
The estimation and scoring process of quantum language models retains classical unigram LMs and KL divergence as special cases. The classical unigram LM is recovered by restricting the maximization in Eq. 10 to diagonal density matrices and including into the sequence of projectors Pd only an orthonormal basis, such as the elements of E. Classical KL divergence is recovered by noting that if q and d are diagonal density matrices, they share the same eigensystem. Hence, |qi = |di and qi = qi, di = di, where q, d are the parameters of classical unigram LMs for the query and the document respectively. In this setting, qi|dj 2 = 0 for i = j and the VN divergence reduces to classical KL, i.e. -V N (q d) = -KL(q d) ra=nk i qi log di.
In Figure 4, we report a synthetic example of the application of the model. We plot the density matrices obtained by the MLE (Section 3.2.1) on the sequence of projectors reported in the table. As usual in ad-hoc tasks, we smooth only the QLMs of the documents. The model corresponding to the query is a projector, i.e. it has two zero eigenvalues, because we did not apply smoothing. If the dependencies are included in the sequence Po, the MLE rotates the corresponding QLM towards the direction spanned by the observed projector (i.e. Kca). This entails that the model d1 is considered more similar to the query than the model d2 which corresponds to a classical language model.
4. EVALUATION
4.1 Experimental Setup
All the experiments reported in this work were conducted using the open source Indri search engine (version 5.3)3. The test collections used are reported in Table 1. We choose the
3http://www.lemurproject.org

collections in order to vary (1) the collection size and (2) collection type. This will produce a comprehensive test set in order to verify the properties of our approach. All the

Name SJMN TREC7-8 WT10g ClueWeb-B

Content Newswire Newswire Web Web

# Docs 90,257 528,155 1,692,096 50,220,423

Topic Numbers 51-150 351-450 451-550 51-200

Table 1: Summary of the TREC collections used to support the experimental evaluation.

collections have been stemmed with the Krovetz stemmer. Both documents and queries have been stopped using the standard INQUERY stopword list. For all the methods, the Dirichlet smoothing parameter µ is set to the default Indri value (µ = 2500). The optimization of all the other free parameters for the proposed model and the baselines is done using five-fold cross validation using coordinate ascent [18] with mean average precision (MAP) as the target metric. The performance is measured on the top 1000 ranked documents. In addition to MAP, for newswire collections we report the early precision metric @10 (precision at 10) and for web collections with graded relevance judgements we report the recent ERR@10, which correlates better with click metrics than other editorial metrics [6]. The statistical significance of differences in the performance of tested methods is determined using a two-sided Fisher's randomization test [29] with 25,000 permutations evaluated at  < 0.05.
4.2 Methodology
Our experimental methodology goes as follows. In a first step, we compare our QLM approach to a unigram Language Modeling baseline (denoted LM) based on Dirichlet smoothing [35], which is a strong bag-of-words baseline. This comparison is done by assigning uniform superposition weights to each dependency , i.e. i = 1/ ||, where || is the cardinality of  (denoted QLM-UNI). This step has two main objectives: (1) to test if quantum probability can bring better performance than a standard bag-of-words model and (2) to test if uniform superposition weights are a reasonable baseline setting.
As a second step, we test the proposed model against the strong non bag-of-words MRF model, which has shown to be highly effective especially for large scale web collections [19, 20]. We test the full dependence version of the model (denoted MRF-FD) which captures dependencies between all the query terms and thus is the most natural choice for a comparison with our model. However, MRF-FD exploits both proximity (#uw) and exact matching (#1). As our model only exploits proximity as an indicator of dependence, we also propose to test the variant MRF-FD-U, which is a MRF using only the proximity feature. This could provide interesting insights on how the models score based upon the same evidence.
Finally, we propose a slightly more elaborate version of our model (denoted QLM-IDF) in which the superposition weights are no more assumed to be uniform. Instead, we assign to each i the normalized idf weight of the corresponding term wi. The objective is to test if a more reasonable parametrization of superposition weights can improve the retrieval effectiveness.

659

Figure 5: Plots of MAP (QLM-UNI and LM) and MLE objective against the number of updates of the density matrix for SJMN, TREC7-8 and WT10g (left, center and right).

All the results exposed in this paper have been obtained by reranking. We rerank a pool of 20000 documents retrieved using LM in order to make a fair comparison between our method and the baselines.
4.3 Setting up QLM
4.3.1 Building the Sequence of Projectors
Very similarly to MRF-FD, given a query Q = {q1, . . . , qn}, we assume that the interesting dependencies to consider correspond to the power set P(Q)4. In order to build the set of projectors for the given document we apply Algorithm 1.

Algorithm 1 Builds the sequence Pd given Wd, Q

Require: Wd, Q

1: Pd  

2: for   P(Q) do

3: for #(, Wd) do

4:

Pd  Pd  m() %Adds the projector to the sequence

5: end for

6: end for

7: return Pd

For each dependency  in P(Q), the algorithm scans the document sequence Wd. For each occurrence of , it adds a projector m() to the sequence Pd. The function #(, Wd) returns how many times the dependency  is observed in Wd. Therefore, the algorithm adds as many projectors as the number of detected compound dependencies. Note that by looping on P(Q), we are actually implementing the strategy exposed in Section 3.1.3, i.e. adding both the dependence and all of its subsets. Following Section 3.1.3, we choose to parametrize # as the unordered window operator in Indri (#uwL). Therefore, a given dependency  will be detected if the component terms appear in any order in a fixed-window of length L = l||. This kind of adaptive parametrization of the window length is state-of-the-art for dependence models such as MRF-FD [2, 19]. For all the dependence models, the coordinate ascent for l spans {1, 2, 4, 8, 16, 32}, which is a robust pool covering different window lengths, including the standard value (l = 4) for MRF-FD.
4In order to keep the retrieval complexity reasonable both for MRF and QLM, we limit ourselves to query term subsets with at most three terms.

4.3.2 MLE Convergence Analysis

Before doing any comparisons, we answer the questions

related to the construction of a quantum language model,

i.e. (1) how to initialize (0)? (2) when to stop the update

process? In order to help the maximum likelihood process to

converge faster, we initialize the matrix (0) to the density

matrix corresponding to the classical maximum likelihood

language model ML of the document or query under con-

sideration. This is a diagonal matrix (0) = diag(ML). We

also tested with the uniform density matrix, as suggested

in [15], but we found that the MAP was severely harmed.

In order to address the second question, we analyze the

variation of MAP with respect to the maximum number of

iterations nit  [1, 50]. The damping factor  is optimized

over the set of values  = {0, 0.1, ..., 0.9}. The iterative

process stops before nit if the change in the likelihood is below 10-4. In order to check for possible variations due

to the collection type, we plot the iteration-MAP curve for

two similar collections, i.e. SJMN and TREC7-8, and a web

collection, WT10g. We also plot the training objective in

Eq. 10 over the set of topics:

1 |R|

dR log LPd (d), where

R is the multiset of retrieved documents. The trend is shown

in Figure 5. Generally, at any number of iterations, the

MAP stays significantly above the baseline. It seems that

there is a good correlation between likelihood maximization

and MAP, although one can note some overfitting at high

number of iterations. Capping by 10  nit  20 seems a

good trade-off between likelihood maximization and MAP.

However, to provide a fair comparison with the baselines,

we choose to include nit as a free parameter to train by

coordinate ascent.

4.4 Results
The results discussed in this section are compactly reported in Table 2.

4.4.1 Language Modeling Baseline
From the comparisons with the LM baseline, one can see that QLM-UNI outperforms LM significantly, with relative improvements in MAP going up to 12.1% in the case of WT10g collection and 19.2% for the ClueWeb-B collection. This seems to be in line with the hypothesis formulated in [19], for which dependence models may yield larger improvements for large collections.
The weight-normalization problem seems to be addressed automatically: our model does not need for any combina-

660

LM MRF-FD-U MRF-FD QLM-UNI
QLM-IDF

SJMN

P@10

MAP

.3064

.1995

.3138

.2071

.3074

.2061

.3181
(+1.4/+3.5)
.3170
(+1.0/+3.1)

.2077
(+0.3/+0.8)
.2093
(+1.1/+1.6)

TREC7-8

P@10

MAP

.4230

.2120

.4350

.2228

.4460

.2243

.4480
(+3.0/+0.4)
.4450
(+2.3/-0.2)

.2240
(+0.5/-0.1)
.2254
(+1.2/+0.5)

WT10g

ERR@10

MAP

.1068

.1975

.1136

.2097

.1147

.2146

.1162

.2215

(+2.2/+1.3) (+5.6/+3.2)

.1176

.2264

(+3.5/+2.6) (+7.9/+5.5)

ClueWeb-B

ERR@10

MAP

.0718

.1003

.0828

.1103

.0881

.1137

.1015

.1196

(+22.6/+15.2) (+8.4/+5.2)

.0997

.1189

(+20.4/+13.1) (+7.8/+4.5)

Table 2: Evaluation of the performance for the five methods tested. Best results are highlighted in boldface.
Numbers in parentheses indicate relative improvement (%) in MAP over MRF-FD-U/MRF-FD. All the results for dependence models are significant with respect to the baseline LM. The symbols , means statistical
significance over MRF-FD-U, MRF-FD respectively.

tion weights. Moreover, it is robust across the folds. From an analysis of the optimal values of the parameters obtained across the different folds, we found that optimal window sizes were l  {1, 2}. This can be explained by considering that in the current version of QLM, it is possible to decide if the dependency is detected or not, but the model cannot discriminate its "importance". If one decides to increase l, more inaccurate dependencies will be detected and the performance will be deteriorated. However, even with a larger window size, statistical significance over LM is maintained. From these considerations, we suggest l = 2 as a default setting for our model. Finally, the results endorse that our QLM does not need an engineered estimation of superposition weights to perform well.
4.4.2 Markov Random Fields Baseline
As a second test, we report the results obtained for the MRF-FD and MRF-FD-U baselines. These have proved to be very robust non bag-of-words baselines [2, 19, 20]. Contrary to our model, MRF does not handle dependency information in the estimation phase. One has to specify the coefficients (T , O, U ) for the combination of dependence and independence scores. To limit per-fold overfitting, for the dependence models, we first train combination parameters (f  {0, 0.01, ..., 1}) then l for each fold. For MRF-FD-U, we set O = 0.
Results show that for SJMN and TREC7-8, QLM-UNI, MRFFD and MRF-FD-U are essentially equivalent. However, for the two Web collections, our model significantly outperforms both MRF variants. On ClueWeb-B, statistical significance is attained for the two reported measures. As conjectured in [19], noisy web collections could be a more discriminative testbed for dependence models. Optimal l values for MRFFD were very small for SJMN (l  {1, 2}) in contrast to the optimal setting for ClueWeb-B (l  {16, 32}). In [19], the authors suggest that for homogenous newswire collections a small window is enough to capture useful dependencies, while for large, noisy web collections, a larger span must be set. However, the performances obtained by our model seem to suggest that it can greatly benefit from term dependencies, on a variety of collections, even when a small window size is used. This elucidates the fact that even short range information can be extremely useful if integrated in the estimation phase. In order to get a more comprehensive view on such issues, we trained on the entire set of ClueWeb-B topics three versions of MRF-FD-U, each obtained by clamp-

ing a different value of l  {1, 2, 4}. The best performing model obtained a MAP of 10.91. It seems that our model can exploit this short range information in a better way than MRF models.
4.4.3 Setting Superposition Weights
Our last test aimed at verifying if a more reasonable setting of the superposition weights could further improve retrieval performance. For a dependency {w1, . . . , wK }, we
set i = idfwi / i idfwi . This has the effect of attributing a larger count to the more "important" term in the dependency. QLM-IDF generally increases MAP. However, this is not the case for ClueWeb-B. From a query-by-query analysis, we noticed that QLM-IDF increases the performance for noisy queries by promoting the most "important" terms in unnecessary subsets. For multiword expressions such as ClueWebB topics continental plates and rock art, weighting by idf may be misleading by assigning more weight to one of the terms. In this cases, a uniform parametrization is far more effective. This demonstrates that there is still room for improvement by a clever tuning of superposition parameters, for example by leveraging feature functions [2, 3].
4.5 Complexity Analysis
Complexity issues can be tackled by noting that it is not necessary to manipulate n × n matrices. We associate a dimension for each query term and an additional dimension for a "don't care" term that will store the probability mass for the other terms in the vocabulary. Therefore, a multinomial over n points is reduced to a multinomial over |Q| + 1 points, where |Q| is the number of unique terms in the query and the additional dimension is simply a relabeling of the other term events. In this way, the QLM to manipulate is k × k, where k = |Q| + 1. The eigendecomposition generally requires O(k3). The iterative process requires at most |P(Q)| = 2|Q| matrix multiplications for the expectation step, where 2|Q| is the maximum number of unique projectors in Pd and 2 matrix multiplications for the maximization step. In the case the likelihood is decreased, || more iterations are done giving a worst-case complexity of O(nit||2k + k3), i.e. if each iteration needs damping. We showed that 10  nit  20 is enough; we use || = 10 and k is very small for title queries, which make the process computationally tractable. In practice, we observed that the damping process is very effective and dramatically improves convergence speed. As an example, the mean number of iter-

661

ations for ClueWeb-B when nit = 15 is 7.02 which is orders of magnitude less than nit|| = 150. Finally, we conjecture that such process could be executed at indexing time, thus eliminating any additional on-line costs.
5. CONCLUSION
In this paper, we presented a principled application of quantum probability for IR. We showed how the flexibility of vector spaces joined with the powerful tools of probabilistic calculus can be mixed together for a flexible, yet principled account of term dependencies for IR. In our model, dependencies are neither represented as additional dimensions, nor stochastically as joint probabilities. They assume a new status as superposition events. The relationship of such an event to the traditional term events are encoded by the off-diagonal values in the corresponding projection matrix. Both documents and queries are associated to density matrices estimated through the maximization of a product, which in the classical case reduces to a likelihood. As our model integrates the dependencies in the estimation phase, it has no need for combination parameters. Experiments showed that it performs equivalently to the existing dependence models on newswire test collections and outperforms the latter on web data.
To our knowledge, this work provides the first experimental result showing the usefulness of this kind of probabilistic calculus for IR. The marriage between vector spaces and probability can be endlessly improved in the future. One straightforward direction is to relax the assumption that single terms represent orthogonal projectors. This could lead to a new way of integrating latent directions as estimated by purely geometric methods such as Latent Semantic Indexing (LSI) [7] into a probabilistic model. In this work, we did not exploit the full machinery of complex vector spaces. We do not have a practical justification for the use of the complex field for IR tasks. However, we speculate that this could bring improved representational power and thus remains an interesting direction to explore. At last, we believe that our model could be potentially applied to other fields of natural language processing only by means of a principled Bayesian calculus capable of manipulating density matrices. We hope that this work will foster future research in this direction.
6. ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for their valuable comments and suggestions.
7. REFERENCES
[1] J. Bai, Y. Chang, H. Cui, Z. Zheng, G. Sun, and X. Li. Investigation of partial query proximity in web search. In Proc. of WWW, pages 1183­1184, 2008.
[2] M. Bendersky and W. B. Croft. Modeling higher-order term
dependencies in information retrieval using query hypergraphs. In Proc. of SIGIR, pages 941­950, 2012. [3] M. Bendersky, D. Metzler, and W. B. Croft. Parametrized concept weighting in verbose queries. In Proc. of SIGIR, pages 605­614, 2011.
[4] R. Blume-Kohout. Hedged maximum likelihood estimation. Phys. Rev. Lett., 105:200504, 2010.
[5] R. Blume-Kohout. Optimal, reliable estimation of quantum states. New J. Phys., 12:043034, 2010.
[6] O. Chapelle, D. Metzler, Y. Zhang, P. Grinspan. Expected reciprocal rank for graded relevance In Proc. of CIKM, 2009.
[7] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer,
and R. Harshman. Indexing by latent semantic analysis. JASIST, 41:391­407, 1990.

[8] J. L. Fagan. Automatic phrase indexing for document retrieval. In Proc. of SIGIR, pages 91­101, 1987.
[9] J. Gao, J. Y. Nie, G. Wu, and G. Cao. Dependence language model for information retrieval. In Proc. of SIGIR, pages 170­177, 2004.
[10] A. Gleason. Measures on the closed subspaces of a hilbert space. Journ. Math. Mech., 6:885­893, 1957.
[11] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval: development and comparative experiments. Inf. Proc. Manag., pages 779­840, 2000.
[12] M. Lease. An improved markov random field model for supporting verbose queries. In Proc. of SIGIR, pages 476­483, 2009.
[13] C. Lee, G. G. Lee, and M. G. Jang. Dependency structure applied to language modeling for information retrieval. ETRI, 28(3):337­346, 2006.
[14] Y. Lv and C. Zhai. Positional language models for information retrieval. In Proc. of SIGIR, pages 299­306, 2009.
[15] A. I. Lvovsky. Iterative maximum-likelihood reconstruction in quantum homodyne tomography. Journ. Opt. B6, pages S556­S559, 2004.
[16] M. Melucci. Deriving a quantum information retrieval basis. The Computer Journal, 2012.
[17] M. Melucci and K. Rijsbergen. Quantum mechanics and information retrieval. Advanced Topics in Information Retrieval, 33:125­155, 2011.
[18] D. Metzler and W. Bruce Croft. Linear feature-based models for information retrieval. Inf. Retr., 10(3):257­274, 2007.
[19] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proc. of SIGIR, pages 472­479, 2005.
[20] D. Metzler, T. Strohman, Y. Zhou, and W. B. Croft. Indri at TREC 2005: Terabyte Track. In Proc. of TREC, 2005.
[21] M. Mitra, C. Buckley, A. Singhal, and C. Cardie. An analysis of statistical and syntactic phrases. In Proc of RIAO, pages 200­217, 1997.
[22] W. Morgan, W. Greiff, and J. Henderson. Direct maximization of average precision by hill-climbing, with a comparison to a maximum entropy approach. In Proc. of HLT-NAACL, pages 93­96, 2004.
[23] M. A. Nielsen and I. L. Chuang. Quantum Computation and Quantum Information. Cambridge University Press, 2004.
[24] J. H. Park, W. B. Croft, and D. A. Smith. A quasi-synchronous dependence model for information retrieval. In Proc. of CIKM, pages 17­26, 2011.
[25] B. Piwowarski, I. Frommholz, M. Lalmas, and K. van Rijsbergen. What can quantum theory bring to information retrieval. In Proc. of CIKM, pages 59­68, 2010.
[26] M. Pretti. A message-passing algorithm with damping. J. Stat. Mech., page P11008, 2005.
[27] J. R eh´acek, Z. Hradil, E. Knill, A. I. Lvovsky. Diluted maximum-likelihood algorithm for quantum tomography. Phys. Rev. A, 75:042108, 2007.
[28] G. Salton, C. S. Yang, and C. T. Yu. A Theory of Term Importance in Automatic Text Analysis. JASIST, 26(1):33­44, 1975.
[29] M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In Proc. of CIKM, pages 623­632, 2007.
[30] F. Song and W. B. Croft. A general language model for information retrieval. In Proc. of SIGIR, pages 279­280, 1999.
[31] M. Srikanth and R. Srihari. Biterm language models for document retrieval. In Proc. of SIGIR, pages 425­426, 2002.
[32] H. Umegaki. Conditional expectation in an operator algebra. Kodai Mathematical Seminar Reports, 14(2):59­85, 1962.
[33] K. van Rijsbergen. The Geometry of Information Retrieval. Cambridge University Press, 2004.
[34] M. K. Warmuth and D. Kuzmin. Bayesian generalized probability calculus for density matrices. Machine Learning, 78(1-2):63­101, 2009.
[35] C. Zhai. Statistical language models for information retrieval a critical review. Found. Trends Inf. Retr., 2(3):137­213, 2008.
[36] J. Zhao and Y. Yun. A proximity language model for information retrieval. In Proc. of SIGIR, pages 291­298, 2009.
[37] X. Zhao, P. Zhang, D. Song, and Y. Hou. A novel re-ranking approach inspired by quantum measurement. In Proc. of ECIR, pages 721­724, 2011.
[38] G. Zuccon and L. Azzopardi. Using the quantum probability ranking principle to rank interdependent documents. In Proc. of ECIR, page 357­369, 2010.

662

Copulas for Information Retrieval

Carsten Eickhoff
Delft University of Technology Delft, The Netherlands
c.eickhoff@acm.org

Arjen P. de Vries
CWI Amsterdam Amsterdam, The Netherlands
arjen@acm.org

Kevyn Collins-Thompson
Microsoft Research Redmond, WA, USA
kevynct@microsoft.com

ABSTRACT
In many domains of information retrieval, system estimates of document relevance are based on multidimensional quality criteria that have to be accommodated in a unidimensional result ranking. Current solutions to this challenge are often inconsistent with the formal probabilistic framework in which constituent scores were estimated, or use sophisticated learning methods that make it difficult for humans to understand the origin of the final ranking. To address these issues, we introduce the use of copulas, a powerful statistical framework for modeling complex multi-dimensional dependencies, to information retrieval tasks. We provide a formal background to copulas and demonstrate their effectiveness on standard IR tasks such as combining multidimensional relevance estimates and fusion of results from multiple search engines. We introduce copula-based versions of standard relevance estimators and fusion methods and show that these lead to significant performance improvements on several tasks, as evaluated on large-scale standard corpora, compared to their non-copula counterparts. We also investigate criteria for understanding the likely effect of using copula models in a given retrieval scenario.
Categories and Subject Descriptors
Information Systems [Information Retrieval]: Retrieval models
Keywords
Relevance models; Multivariate relevance; Ranking; Probabilistic framework; Data fusion.
1. INTRODUCTION
In response to user queries, today's search systems typically return lists of documents ranked by system estimates of relevance. In traditional IR retrieval models, each document's relevance towards the query is expressed as term overlap between query and document [42]. Early on, re-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

searchers began exploring alternative, non-topical document quality criteria such as document recency, credibility or monetary cost. More recently, through a combination of improved algorithms and greatly increased data scale, significant gains in ranking quality and user satisfaction based on employing non-topical factors such as textual complexity [12] or suitability for children [17] have begun influencing the ranking process. Given a scenario such as child-friendly information search, non-topical quality criteria can clearly have a strong influence on usefulness of a document for a specific user. A perfectly relevant document that is not understandable due its complex sentence structure or excessive use of jargon will have significantly diminished user relevance.
Beyond the value of individual relevance factors, there can be complex, non-linear dependencies between relevance factors. For example, relevance criteria such as topicality and credibility might appear independent for some document subsets, but extreme values in one dimension may influence the other in a way that is not easily captured by state-of-the-art approaches. As a concrete example, take TREC 2010's faceted blog distillation task [32], that aims at retrieving topically relevant non-factual blog feeds. Here, the relevance space has two dimensions: topicality and subjectivity. Figure 1 shows the distribution of relevance scores for Topic 1171, "mysql ", across these two relevance dimensions. We can note an apparent correlation in the lower left part of the graph that weakens as scores increase. To underline this, we computed Pearson's  between the two dimensions for the lower score third ( = 0.37), the upper region ( = -0.4), as well as the overall distribution ( = 0.18). Apparently, the dependency structure of the joint distribution of relevance, in this case, is not easily described by a linear model. Consequently, we can expect dissatisfying performance of linear combination models. And, indeed, when inspecting the performance of a linear combination model with empirically learned mixture parameters , Topic 1171 receives an average precision of only 0.14, well below the method's average across all topics of 0.25. In the course of this work, we will discuss practical means of addressing cases like the present one and will finally revisit this example to demonstrate the effect of our proposed method.
While the machine learning, information retrieval, data mining and natural language processing communities have significant expertise in estimating topical document relevance and additional criteria in isolation, the commonly applied combination schemes have tended to be ad hoc and ignore the problem of modeling complex, multi-dimension dependencies. In practice, they follow statically weighted lin-

663

Figure 1: Distribution of bivariate relevance scores for TREC 2010 Blog Track Topic 1171, "mysql".
ear combinations with empirically determined mixture parameters [42] or deploy sophisticated learning to rank techniques that tend to offer only limited insight to humans about why they were weighted highly for relevance. Ideally, we would demand realistic, yet formally-grounded combination schemes that can lead to results that are both effective and with human-interpretable justification.
In a different context, the field of quantitative risk management has devised copulas, a flexible, varied class of probability density functions that are designed to capture rich, non-linear dependencies efficiently in multi-dimensional distributions. Copulas work by decoupling the marginal distributions of the data from the underlying dependency structure of the joint distribution. In particular, copulas can account for so-called tail dependencies, i.e., dependencies that play up at the extreme values of the interacting distributions. As an example, let us consider two commodities traded on the stock market, such as rare earth metals and pork bellies. The two commodities are sufficiently different to make the related market segments quasi-independent. However, extreme market situations have been shown to cause investor panics that reach across otherwise independent segments and cause previously unseen interrelationships [9].
This work makes three contributions to the state of the art in relevance modelling. (1) We give a detailed introduction to the formal framework of copulas and describe how to estimate them from empirical data. (2) Based on a number of sizeable standard data sets such as the Blogs08 collection [32], we demonstrate the merit of using copulas for multivariate relevance estimation. (3) In a related effort, we address the task of score fusion based on historic submissions to the TREC ad hoc task. The remainder of this paper is structured as follows: Sec-

tion 2 gives a historic overview of IR relevance frameworks, prior work on multidimensional relevance models, score fusion approaches, as well as, examples of copula applications from different fields. Section 3 formally introduces the theoretical foundation of copulas and details key techniques in their application. In Sections 4 and 5 we demonstrate their merit at the tasks of estimating multidimensional relevance scores as well as fusing prior TREC runs. Section 6 further discusses the experimental results and aims at identifying those domains of IR for which copulas are most promising. Section 7 concludes the paper with a concise summary of our findings.
2. RELATED WORK
Over the past decades, a wide range of partially overlapping relevance frameworks have been proposed, a few prominent examples include [44, 22, 34, 8]. They unanimously consider relevance as a complex, potentially multidimensional concept that may be composed from a number of constituents. In the further course of this section, we will focus on the practical implementation of formal relevance estimation schemes employed in information retrieval and related disciplines. Schamber et al. [45] radically revised the definition of relevance, causing a growing interest in probabilistic relevance modelling in the research community. First openly applied at the third TREC competition, the BM25 retrieval model [43] represents a performance landmark that is still valid today (with slight variations such as the 2004 integration of multiple weighted fields [42]). In 1996, Persin et al. [38] introduced the idea of retrieval result lists ranked by their probability of relevance, as an alternative to the previously dominant binary retrieval scenario. Two years later, Ponte and Croft proposed the use of language modelling techniques to determine topical relevance [39]. One of the first notions of non-topical relevance was expressed in Kleinberg's work on hubs and authorities [26] in which the author introduces two document-specific relevance notions independent of the query. Lavrenko and Croft [28, 29] pursued a line of work on dedicated relevance models. While the formal combination of several individual relevance facets in one model has not been extensively studied, there has been an interesting thread of research on score fusion. The task is to combine the result rankings of multiple independent retrieval systems in order to compensate for local inaccuracies of single engines. Early approaches to the task were based on evidence aggregation in the form of products and sums of scores across individual systems [19]. The fused ranking is based on the absolute value of the cross-system aggregates. Vogt et al. [51] first introduced linear interpolation of multiple model rankings for system fusion. Aslam and Montague [5] proposed a probabilistic rank-based method for direct combination of multiple engines. Later on, they devised a similar method based on a majority voting scheme between various retrieval systems [35]. [36] proposed a score normalization scheme that is more robust to outliers in the distribution of relevance than the previously used min/max technique. There has been an extensive body of work on estimating the distribution of relevance scores for document ranking. Recent examples include the work by Arampatzis and Stephenson [4], Kanoulas et al. [25], and, Cummins [14]. Manmatha et al. [33] estimated a search engine's score distribution as a mixture of normal and exponential distributions, for relevant and non-relevant documents respectively. They

664

used the resulting distributions for score fusion across multiple engines, but did not attempt to model dependencies in the joint score distribution, instead treating the scores as independent and averaging probabilities, or discarding `bad' engines altogether. In 2002, Wu and Crestani [52] introduced the first of what would become a group of fusion approaches that define an explicit weighting scheme under which the original result lists are combined. [7] and [15] employ various quality notions such as the degree to which a document satisfies a given relevance criterion to dynamically adapt the weighting scheme to the underlying distribution of relevance. In 2005, Craswell et al. investigated relevance model combination by linearly combining constituent scores in the log domain [13]. Tsikrika and Lalmas applied Dempster-Shafer theory for the aggregation of independent relevance criteria in web retrieval in the form of belief functions [49]. Gerani et al. [21] propose non-linear score transformations prior to the standard weighted linear combination step. Their solid results demonstrate the need for models whose capabilities go beyond linear dependency structures between relevance dimensions. In recent years, the variety of IR applications has become significantly more diverse. As a consequence, universal relevance models have become less viable in many areas. Tasks such as legal IR, expert finding, opinion detection or the retrieval of very short documents (e.g., tweets) have brought forward strongly customised relevance models tailored towards satisfying a given task (e.g., [24, 6]). Especially for the retrieval of structured (XML) documents, score combination schemes are of central importance to combine evidence across multiple structural fields within a document. Despite the numerous potential issues pointed out by Robertson et al. [42], most state-of-the-art approaches to XML retrieval rely on linear models [31]. An advance towards the formal combination of several independent relevance criteria in the form of prior probabilities for language models has been made by Kraaij et al. [27] for the task of entry page search. To date, however, most universally applicable relevance models still rely on pure linear combinations of relevance criteria that disregard the underlying data distribution or potential dependencies between the considered dimensions. Learning to rank (L2R) has been established as an alternative approach for signal combination. The aim is to apply machine learning methods to either directly infer a document ranking or a ranking function from a wide range of features, potentially including the previously-discussed relevance criteria [10, 40, 30]. The downside of this approach is that the resulting models tend to yield only limited insight for humans. The classic approach of developing a unifying formal retrieval model would in our view provide better means to increase not just overall performance, but also our qualitative understanding of the problem domain. By introducing copulas for information retrieval, this work proposes a way for closing the gap between linear combinations (that break with the probabilistic framework in which the constituent scores were estimated) and non-linear machine-learned models (that offer only limited insight to scientists and users). Copulas have been traditionally applied for risk analyses in portfolio management [18] as well as derivatives pricing [9] in quantitative finance. Recently, however, there are several successful examples from unrelated disciplines. Renard et

al. estimate water flow behaviour based on Gaussian copulas [41]. Onken et al. apply copulas for spike count analysis in neuroscience [37]. In meteorology, copulas have been used to combine very high-dimensional observations for the task of climate process modelling [47]. To the best of our knowledge, there has been no prior application of the copula framework to information retrieval problems.
3. COPULAS
At this point, we will give a brief introduction of the general theoretical framework of copulas, before applying them to various IR tasks in subsequent sections. For a more comprehensive overview, please refer to [46] for more detail and pointers to further reading. The term copula was first introduced by Sklar [48] to describe multivariate cumulative distribution functions (cdfs) that allow for a formal decoupling of observations from dependency structures. Formally, given
X = (x1, x2, . . . , xk)
a k-dimensional random vector with continuous margins
Fk(x) = P[Xk  x]
we can map our observations to the unit cube [0, 1]k as
U = (u1, u2, . . . uk) = (F1(x1), F2(x2), . . . Fk(xk)).
This is where our copulas come into play. A k-dimensional copula C describes the joint cumulative distribution function of random vector U with uniform margins.
C : [0, 1]k  [0, 1]
This approach has two obvious practical benefits: (1) Separating marginals and dependency structure allows for more straightforward estimation or approximation of each component in isolation. (2) An explicit model of dependency is scale-invariant. The copula describes a reference case of dependency on the unit cube [0, 1]k that can be applied to arbitrary random vectors without further adjustment. A number of key properties make copulas an appealing theoretical framework for a wide number of applications, so we summarize those now.
· Like all cdfs, a copula C(u1, u2, . . . , uk) is increasing in each component ui
· A marginal component ui can be isolated by setting all remaining components to 1:
C(1, . . . , 1, ui, 1, . . . , 1) = ui
· If a single component ui in U is zero, the entire copula is zero:
C(u1, . . . , ui-1, 0, ui+1, . . . , uk) = 0
· Most importantly, we can assume general applicability of the copula framework, since, as a consequence of Sklar's Theorem [48], for each k-dimensional cdf F and all xi in [-, ] and 1  i  k, there exists a copula C with
F (x1, . . . , xk) = C(F1(x1), . . . , Fk(xk))

665

3.1 Extreme conditions
Before applying the copula framework to problems in information retrieval, let us visit a number of extreme conditions of dependency that frequently occur in IR scenarios. (1) Independence of observations is a frequently assumed simplification in IR theory that leads to convenient (if na¨ive) probabilistic models. In the copula framework, independence of events can be captured by the so-called independence copula Cindep:
k
Cindep (U ) = exp(- - log ui)
i=1
which is equivalent to the product across all constituent probabilities in U . (2) Co-monotonicity describes the case of perfect positive correlation between observations u:
CcoMono (U ) = min{u1, . . . , uk}
(3) counter-monotonicity of observations is given in the opposite case of perfect negative correlation:
k
CcounterMono (U ) = max{ ui + 1 - k, 0}
i=1
Consequently, each copula lies within the so-called Fr´echetH¨offding bounds [23]:
CcounterMono (U )  C(U )  CcoMono (U )
3.2 Copula families
After having covered the foundations of copula theory let us inspect some concrete examples of copulas that will be used in the course of this work. Three general families of standard copulas have been proposed in the literature, whose corresponding equations are given right after their introduction in this paragraph: (1) Elliptical copulas are directly derived from known distributions and are based on standard distribution functions such as the Gaussian distribution or Student's t distribution. Equation 1 shows the Gaussian copula that requires the observed covariance matrix   Rk×k as a parameter.  denotes the cdf of a standard normal distribution and -1 its inverse. (2) Archimedean copulas are popular as they can be explicitly stated (note that due to their distribution dependency that is not the case for elliptical copulas) and typically depend on only a single degree of freedom. The parameter  expresses the strength of dependency in the model. Equation 2 shows the Clayton copula whose -range is [-1, )\{0}.  = -1 represents counter-monotonicity,   0 gives the independence copula and    approaches co-monotonicity. Finally, (3) Extreme value copulas are robust in cases of extreme observations. The Gumbel copula (Equation 3) has a parameter space of  in [1, ). For  = 1 we obtain the independence copula, and, for    we approach co-monotonicity.

CGaussian (U ) = (-1(u1), . . . , -1(uk))

(1)

k
CClayton (U ) = (1 + (

1 

(u-i 

-

1)))

-1 

(2)

i=1

k

CGumbel (U ) = exp(-(

(-

log(ui

))

)

1 

)

(3)

i=1

Figure 3.2 shows contour plots of a number of bivariate standard copulas. The concrete choice of copula family and instantiation has been frequently reported to depend on the application domain [46]. If no prior knowledge about the dependency structure, e.g., prevalence of asymptotic or tail dependencies, is available, practitioners often resort to goodness-of-fit tests or measures of tail dependency in order to choose an appropriate model. We will describe the use of these techniques in the subsequent sections when applying copulas for information retrieval problems.
3.3 Fitting copulas to observations
In the case of elliptical copulas, the fitting process is limited to calculating means and covariance matrices from the available observations. Here, the only degree of freedom is the concrete choice of distribution function (e.g., Gaussian vs. Student) that best approximates the original distribution that generated the observations. In the non-elliptical case, the task is to determine optimal settings of . Commonly, this is achieved by means of maximum likelihood estimates based on the available observations. This is also the approach chosen in this work. It should be noted that there are methods for direct empirical estimations of entire copula functions. The interested reader can find a good overview by Charpentier et al. [11] as a starting point for this line of research, the inclusion of which would however go beyond the scope of this initial exploration of copulas for information retrieval.

4. RELEVANCE ESTIMATION
In the previous section, we described the theoretical foundations of copulas including concrete ways of computing C(U ) from multivariate observations U . We now detail their application for relevance estimation in information retrieval. First, we separately estimate the probability of relevance Pr(ekl)(d) and non-relevance Pn(okn) (d) for a document d, under each of the k criteria (dimensions) ­ for example, topicality, recency, readability, etc. Next, we assume random observations Urel and Unon to derive from these distributions and base two distinct copulas, Crel and Cnon on them. Recall that these copulas should capture the dependencies between relevance criteria, in either the relevant (Crel) or the non-relevant (Cnon) documents retrieved. Since it is difficult to predict where these dependencies have the most effect, it is natural to consider three different general approaches of combining multivariate observation scores U into a single probability of relevance that can be used for resource ranking. (1) CPOS (Urel ) multiplies the independent likelihood of observing Urel with the relevance copula Crel , capturing only dependencies between the likelihoods of relevance. (2) CNEG(Urel , Unon ) normalizes the probability of relevance by the non-relevance copula Cnon (Unon ), capturing only the dependencies between the likelihoods of non-relevance. (3) CODDS (Urel , Unon ), finally, multiplies the probability of relevance by the ratio of the two copulas, modelling simultaneously the dependencies between both previous notions.
k
CPOS (Urel ) = Crel (Urel ) urel,i
i=1

CNEG(Urel , Unon )

=

k i=1

urel

,i

Cnon (Unon )

666

Figure 2: Examples of bivariate copula contour plots. (a) Gaussian copula, (b) Clayton copula with  = 2.0, (c) Gumbel copula with  = 2.0.

CODDS (Urel , Unon )

=

Crel (Urel ) Cnon (Unon )

k
urel ,i
i=1

As performance baselines, we will compare to three popular combination methods from the literature: (1) SUM (Urel ) sums up the relevance scores across all criteria k and uses the sum as the final ranking criterion [19]. (2) PROD(Urel ) builds the product across all constituents [19]. Probabilistically, this combination scheme assumes independence across all criteria and can be expected to be too na¨ive in some settings where dependence is given. (3) Weighted linear combinations LIN(Urel ) build a weighted sum of constituents urel,i with mixture parameters i optimized by means of a parameter sweep with step size 0.1 [51]. It should be noted that all optimizations and parameter estimations, both for the baselines as well as for the copula models are conducted on designated training sets that do not overlap with the final test sets. We relied on the original training portion of the respective corpora. In the case that the original corpus did not specify a dedicated training set, we used a stratified 90%/10% split.

k
SUM (Urel ) = urel,i
i=1
k
PROD (Urel ) = urel,i
i=1
k
LIN(Urel ) = iurel,i
i=1
Based on three different standard datasets and tasks, we will highlight the merit of using copulas over the traditional approaches. Each of the settings specifies 2 individual relevance criteria (k = 2) which are crucial for user satisfaction given the retrieval task. Table 1 gives a high-level overview of the relevant corpora that we used. Each of them will be described in more detail in the three following sections. Depending on the strength of tail dependency in the data, we will see varying improvements for the three inspected settings. Comparable as the scenarios appear, there seem to be significant underlying differences in the distribution of relevant documents that influence the benefit from the use

Table 1: Overview of experimental corpora. ID # docs # topics # labels year

Blogs08 1.3M

100

Delicious 339k

180

ODP

22k

30

38.2k 3.8k 1k

2008 2012 2009

of copulas. In Section 6, we will dedicate some room to a detailed investigation of when the use of copula-based retrieval models is most promising.
4.1 Opinionated blogs
When conducting marketing analyses for businesses, researching customer reviews of products or gauging political trends based on voter opinions, it can be desirable to focus the search process on subjective, non-factual documents. The Text REtrieval Conference (TREC) accounted for this task within the confines of their Blog Track between the years 2006 and 2010 [32]. The aim of the task is to retrieve blog feeds that are both topically relevant and opinionated. Our experimental corpus for this task is the Blogs08 collection specifically created for the venue. The dataset consists of 1.3 million blog feeds and is annotated by more than 38k manually created labels contributed by NIST assessors.
Each document is represented as a two-component vector Ur(e2l). The first component refers to the document's topical relevance given the query and the second represents its degree of opinionatedness. In order for a document to be considered relevant according to the judges' assessments, it has to satisfy both conditions. Topical relevance was estimated by a standard BM25 model and opinionatedness was determined using the output of a state-of-the-art open source classifier [1]. After an initial evaluation of the domain, we chose Clayton copulas (Equation 2) to represent the joint distribution of topicality and opinionatedness. Table 2 shows a juxtaposition of performance scores for the baselines as well as the various copula methods. The highest observed performance per metric is highlighted by the use of bold typeface, statistically significant improvements (measured by means of a Wilcoxon signed-rank test at  = 0.05-level) over all competing approaches are denoted by an asterisk. Of the baseline methods, the score product PROD performs best. However, introducing the use of copulas, we observe that the highest performance was achieved using the CPOS copula,

667

Table 2: Copula-based relevance estimation perfor- Table 3: Copula-based relevance estimation perfor-

mance for opinionated blogs (k = 2).

mance for personalized bookmarks (k = 2).

Method
PROD SUM LIN CPOS CNEG CODDS

P@5
0.413 0.400 0.387 0.413 0.373 0.373

P@10
0.360 0.333 0.333 0.400* 0.373 0.360

p@100
0.181 0.154 0.162 0.182 0.181 0.182

BPREF
0.289 0.255 0.262 0.306* 0.290 0.283

MRR
0.692 0.689 0.689 0.692 0.545 0.544

MAP
0.275 0.238 0.245 0.287* 0.245 0.242

Method
PROD SUM LIN CPOS CNEG CODDS

P@5
0.084 0.095 0.126 0.105 0.137* 0.116

P@10
0.079 0.095 0.100* 0.068 0.090 0.074

p@100
0.011 0.011 0.011 0.01 0.010 0.01

BPREF
0.051 0.071 0.077 0.056 0.079* 0.066

MRR
0.192 0.192 0.219* 0.190 0.184 0.202

MAP
0.043 0.055 0.063 0.047 0.065 0.058

which gave statistically significant gains in MAP, Bpref and precision at rank 10 over all the baseline methods.
At this point, we revisit the example query (Topic 1171) that was discussed in the introduction and depicted in Figure 1. For this topic, we observed a clear non-linear dependency structure alongside a lower-than-average linear combination performance of AP = 0.14. When applying CPOS to the topic, however, we obtain AP = 0.22, an improvement of over 50%.
4.2 Personalized bookmarks
Finding and re-finding resources on the Internet are frequently accompanied and aided by bookmarking. What started as a local in-browser navigation aid, has in recent years become an active pillar of the social web society. Collaborative bookmarking platforms such as Delicious, Furl, or Simpy allow users to maintain an online profile along with bookmarks that can be shared among friends and collaboratively annotated by the user community. Research into tagging behaviour [2] found that a significant amount of the tags assigned to shared media items and bookmarks are of subjective nature and do not necessarily serve as objective topical descriptors of the content. This finding suggests that bookmarking has a strong personal aspect which we will cater for in our experiment. Vallet et al. [50] compiled a collection of more than 300k Delicious bookmarks and several million tags to describe them. For a share of 3.8k bookmarks and 180 topics, the authors collected manual relevance assessments along two dimensions, topical relevance of the bookmark given the topic and personal relevance of the bookmark for the user. This dataset is one of the very few corpora whose personalized relevance judgements were made by the actual users being profiled. We conduct a retrieval experiment in which we estimate topical and personal relevance for each document and use Gumbel copula models to model the joint distribution of facets. The set of relevant documents comprises only those bookmarks that satisfy both criteria and were judged relevant in terms of topicality and personal relevance. Table 3 shows an overview of the resulting retrieval performances. CNEG stands out as the strongest copula-based model but the overall ranking of systems depends on the concrete metrics evaluated. For some metrics such as precision at rank 10 and MRR, the linear combination baseline prevails, BPREF and precision at 5 documents favour CNEG.
4.3 Child-friendly websites
The third application domain that we will inspect is concerned with the retrieval of child-friendly websites. Children, especially at a young age, are an audience with specific needs that deviate significantly from those of standard web users. Even for adult users it has been shown that focussing

Table 4: Copula-based relevance estimation perfor-

mance for child-friendly websites (k = 2).

Method P@5 P@10 p@100 BPREF MRR MAP

PROD SUM LIN CPOS CNEG

0.240 0.246 0.320* 0.238 0.242

0.143 0.157 0.187* 0.140 0.140

0.051 0.052 0.071* 0.053 0.048

0.221 0.213 0.275* 0.215 0.223

0.349 0.340 0.357 0.351 0.349

0.196 0.200 0.235* 0.200 0.194

CODDS 0.241 0.143 0.052 0.220 0.349 0.196

the retrieval process on material of appropriate reading level can benefit user satisfaction [12]. In the case of children, this tendency can be expected to be even more pronounced since young users show very different modes of interaction with search engines that reflect their specific cognitive and motor capabilities [16]. Consequently, dedicated web search engines for children should focus their result sets on topically relevant, yet age-appropriate documents. [17] constructed a corpus of 22k web pages, 1,000 of which were manually annotated in terms of topical relevance towards a query as well as the document's likelihood of suitability for children. According to the authors, the class of suitable documents encompasses those pages that were topically relevant for children, presented in a fun and engaging way and textually not too complex to be understood. In our retrieval experiment, we account for both criteria and require documents to be both on topic as well as suitable for children in order to be considered relevant. Table 4 gives an overview of the resulting retrieval performance. In this setting, the various copula models show comparable result quality as the non parametric baselines. Linear combinations with empirically learned weights, however, were consistently the strongest method. We intend to explore the reasons for this in future work. However we note that the distribution of child-suitable ratings has a very large mode at zero, with only a small number of non-zero scores taking a limited number of possible discrete values - limiting the amount of useful dependency information available that copulas could exploit.
5. SCORE FUSION
Previously, we investigated the usefulness of copulas for modelling multivariate document relevance scores based on a number of (largely) orthogonal document quality criteria. Now, we will address a different, closely related problem: score fusion (also known as an instance of data fusion). In this setting, rather than estimating document quality from the documents, we attempt to combine the output of several independent retrieval systems into one holistic ranking. This challenge is often encountered in the domains of metasearch or search engine fusion. To evaluate the score fusion performance of copula-based methods, we use historic submissions

668

to the TREC Adhoc and Web tracks. We investigate 6 years of TREC (1995 - 2000) and fuse the document relevance scores produced by several of the original participating systems. Intuitively, this task closely resembles the previously addressed relevance estimation based on individual document properties. In practice, as we will show, the scenario differs from direct relevance estimation in that retrieval systems rely on overlapping notions of document quality (e.g., a variant of tf/idf scoring) and are therefore assumed to show stronger inter-criteria dependencies than individual facets of document quality might. Systematically, however, we address a set of document-level scores Ur(ekl), originating from k retrieval systems, exactly in the same way as we did document quality criteria in the previous section. As performance baselines, we will rely on two popular score fusion schemes, CombSUM and CombMNZ [19]. CombSUM adds up the scores of all k constituent retrieval models and uses the resulting sum as a new document score. CombMNZ tries to account for score outliers by multiplying the crosssystem sum by NZ (U ), the number of non-zero constituent scores.
k
CombSUM (Urel ) = urel,i
i=1

k
CombMNZ (Urel ) = NZ (Urel ) urel,i
i=1

We introduce statistically principled, copula-based extensions of these established baseline methods: corresponding to CombSUM and CombMNZ, we define CopSUM and CopMNZ that normalize the respective baseline methods by the non-relevance copula.

CopSUM (Urel , Unon )

=

k i=1

urel ,i

Cnon (Unon )

CopMNZ (Urel , Unon ) =

NZ (Urel )

k i=1

urel ,i

Cnon (Unon )

Due to the close relationship to the baseline methods, the effect of introducing copulas is easily measurable. Based on empirical evidence, we employ Clayton copulas to estimate Cnon (Unon ).
Table 5 compares the baselines and copula methods in terms of MAP gain over the best, worst and median historic system run that were fused. Each performance score is averaged over 200 repetitions of randomly selecting k individual runs with k ranging from 2 to 10 for each year of TREC. Statistically significant improvements over the respective baseline method, i.e. of CopSUM over CombSUM and CopMNZ over CombMNZ, are determined by a Wilcoxon signed-rank test at  = 0.05 level and are denoted by an asterisk.
Regarding the baseline methods, CombSUM and CombMNZ perform equally well on average, but with a clear dataset bias. On TREC 4, 8 and 9, CombSUM performs consistently better than CombMNZ. For TREC 5, 6 and 7, the inverse is true. With the exception of TREC 4, the fused rankings do not match the performance of the single strongest run that contributed to the fusion.
Introducing the copula methods led to consistent improvements over their non-copula baseline counterparts. In 104 out of 168 cases, the copula-based fusion methods gave statistically significant gains, with only 14 out 168 performing

worse than the corresponding baseline method. The copulabased methods achieved, on average, 7% gains over the corresponding baseline when comparing to the strongest fused system, 4% gain on median systems and 2% gain on the weakest systems.
Fusion robustness
There are significant differences in fusion effectiveness between individual editions of TREC. Comparing TREC 4 and TREC 6, for example, we observe that TREC 6 fusion results typically showcase performance losses in comparison to the best original run and very high gains for the weakest systems. We seek an explanation in the imbalance in performance of the original systems. Very weak systems have the potential of decreasing the overall quality of the fused result list by boosting the scores of non-relevant documents. As the number of very weak systems increases, so does the chance for performance losses introduced by fusion. When inspecting the number weak submissions (defined as having an MAP score that is at least 2 standard deviations lower than the average score across all participants) included in our fusion experiments, we find that, indeed, our TREC 6 sample includes 27% more weak systems than that of TREC 4.
In order to further investigate the influence of weak runs on overall fusion performance and to measure the proposed methods' robustness against this effect, we turn to the 10system fusion scenario and inject more and more weak systems among the regular ones. Figure 3 shows how the fusion improvement over the single strongest system of TREC 4 is affected as the number of weak submissions ranges from 0 to 9 out of 10. As before, each data point is an average across 200 fusions of randomly drawn runs. In the ideal setting, in which there are no weak systems, we note higher performance gains than in the uncontrolled scenario that was shown in Table 5. As the number of weak systems injected into the fusion increases, performance scores quickly drop. As noted earlier, CombSUM performs slightly better on TREC 4 than CombMNZ. This difference, however, is not further influenced by the number of weak systems. The copula-based fusion methods are more resistant to the influence of weak systems. We note the divide between copulamethods and baseline approaches growing as the number of weak systems increases. Each baseline system score is wellseparated from the respective copula-based variant. Error bars in Figure 3 were omitted to prevent clutter.
6. DISCUSSION
In Section 4, we investigated three different domains in which we apply copulas to model the joint distribution of multivariate relevance scores. For each of these settings, we could observe varying degrees of usefulness of the proposed copula scheme. While for child-friendly web search, the linear baseline performed best, we achieved significant improvements in the opinionated blog retrieval setting. At this point, we investigate the reason for this seeming imbalance in performance gains in order to find a way of deciding for which problem domains the application of copulas is most promising.
One of the key properties of copulas is their ability to account for tail dependencies. Formally, tail dependence describes the likelihood that component urel,i within the observation vector Ur(ekl) will take on extremely high or low values,

669

Table 5: Score fusion performance based on historic TREC submissions. Evaluated in percentages of MAP improvements over the best, median, and worst original systems that were fused.

TREC 4
CombSUM CopSUM CombMNZ CopMNZ

Best
-9.8 -9.6* -9.5 -9.5

2 runs Med.
-

Worst
118 116 116 115

Best
-4.2 -4.2 -5.4 -5.5

4 runs Med.
20 20.5* 18.3 18.2

Worst
1128 1136 1071 1080

Best
0.0 0.0 -1.1 -1.0

6 runs Med.
33.5 33.8* 31.6 31.9*

Worst
1709 1721 1675 1689*

Best
3.0 3.2* 2.1 1.8

8 runs Med.
39.6 40.0* 38.3 38.6*

Worst
2344 2350 2310 2318*

Best
3.9 4.0 3.6 3.8*

10 runs Med. Worst
48.5 3116 49.2* 3125* 48.0 3106 48.0 3117*

TREC 5
CombSUM CopSUM CombMNZ CopMNZ

Best
-5.6 -5.2* -4.6 -4.5

2 runs Med.
-

Worst
268 274* 269 274*

Best
-10.6 -9.9* -6.7 -6.5

4 runs Med.
12.5 13.0* 17.4 17.8*

Worst
614 613 652 667*

Best
-6.9 -6.7* -3.5 -3.1*

6 runs Med.
26.5 28.0* 30.9 32.2*

Worst
955 972* 986 991

Best
-5.3 -4.9* -2.5 -2.4

8 runs Med.
34.3 35.0* 38.2 38.7*

Worst
1031 1050* 1074 1092

Best
-5.6 -5.2* -3.3 -3.0*

10 runs

Med. Worst

40.1 43.3* 43.5 46.0*

1479 1503* 1526 1554*

TREC 6
CombSUM CopSUM CombMNZ CopMNZ

Best
-18.5 -17.7* -17.0 -16.3*

2 runs Med.
-

Worst
486 471 491 490

Best
-24.6 -23.1* -18.6 -17.2*

4 runs Med.
7.8 9.1* 15.5 17.4*

Worst
2235 2279* 2537 2601*

Best
-24.0 -22.9* -18.1 -17.9

6 runs Med.
29.6 32.1* 38.8 40.5*

Worst
3950 4075* 4386 4458*

Best
-22.8 -21.2* -16.7 -16.7

8 runs Med.
44.9 48.3* 55.0 59.6*

Worst
5585 5699* 6111 6202*

10 runs Best Med. Worst

-22.1 56.9 7685

-20.8* 58.2* -17.3 65.0 -16.4* 66.8*

7702 8117 8170

TREC 7
CombSUM CopSUM CombMNZ CopMNZ

Best
-9.3 -9.4 -8.8 -8.8

2 runs Med.
-

Worst
132 145* 130 139*

Best
-16.2 -15.8* -13.7 -13.3*

4 runs Med.
6.2 6.5* 9.4 10.1*

Worst
303 321* 347 363*

Best
-11.7 -11.1* -10.1 -10.2

6 runs Med.
25.9 27.2* 28.1 30.5*

Worst
504 538* 538 565*

Best
-12.8 -12.3* -10.9 -10.7

8 runs Med.
30.0 34.1* 32.8 34.7*

Worst
708 734* 745 786*

10 runs Best Med. Worst
-14.5 36.3 863 -13.8* 39.1* 877 -13.1 38.5 891 -12.4* 40.4* 922

TREC 8
CombSUM CopSUM CombMNZ CopMNZ

Best
-15.9 -16.1 -17.2 -17.3

2 runs Med.
-

Worst
475 488 421 447*

Best
-11.6 -10.1* -11.8 -11.2

4 runs Med.
8.1 8.3 7.6 7.9*

Worst
1188 1201 1273 1292

Best
-11.5 -10.9* -12.9 -12.8

6 runs Med.
16.9 16.7 15.1 14.9

Worst
3194 3195 3209 3216

Best
-7.7 -7.3* -9.8 -9.2*

8 runs Med.
21.8 22.3* 18.6 19.7*

Worst
2739 2755 2660 2685

Best
-5.4 -4.3* -7.2 -6.7*

10 runs

Med. Worst

21.8 22.4 19.2 20.5*

3372 3397 3266 3301*

TREC 9
CombSUM CopSUM CombMNZ CopMNZ

Best
-9.0 -8.5* -11.0 -10.7

2 runs Med.
-

Worst
173 188* 155 167

Best
-14.9 -13.7* -19.0 -17.9*

4 runs Med.
20.4 21.2* 14.5 16.0*

Worst
473 499* 435 432

Best
-15.6 -15.3 -17.4 -17.1

6 runs Med.
17.4 17.9 14.4 14.7

Worst
178 182 172 176

Best
-21.3 -20.9 -25.3 -24.8*

8 runs Med.
18.9 19.2* 12.6 13.0*

Worst
202 207 186 190

10 runs Best Med. Worst
-27.9 12.6 204 -26.6* 13.1* 206 -32.7 4.7 184 -30.4* 5.1* 187

as another component urel,j with i = j also takes an extreme value. The strength of this correlation in extreme regions is expressed by the tail dependency indices IU and IL for upper and lower tail dependency, respectively. Higher values of I signal stronger dependencies in the respective tail regions of the distribution.
IU = P {X1 > Fi-1(ui)|X2 > Fj-1(uj )}
IL = P {X1  Fi-1(ui)|X2  Fj-1(uj )}
The literature has brought forward a number of estimators of the tail indices. We use the R implementation of Frees et al.'s method [20].
Tail index estimates serve as good tools for separating domains where we are likely to observe performance gains (blog and bookmark retrieval) and those that do not match linear combination performance (child-friendly search). Based on the respective copula models that we fit to our observations, the blog retrieval (IL = 0.07) and personalized bookmarking (IU = 0.49) show moderate tail dependencies while the

child-friendly web search task has no recognizable dependency among extrema (IL = IU = 0). Since the comparison of absolute tail index scores across observations is not meaningful, we are interested in a method to further narrow down the expected performance. To this end, we took a closer look at the actual data distribution, and investigated goodness-of-fit tests that are used to determine how well an assumed theoretical distribution fits the empirical observations. The higher the likelihood of our observations to have been generated by the copula models that we estimated, the higher resulting performance we can expect. We apply a standard Anderson-Darling test [3] to determine how well the observations are represented by the copula models. In the personalized bookmarking setting, we obtain p = 0.47 and for the blog data p = 0.67 for the null hypothesis of the observations originating from the present copula model. As we suspected based on the tail dependency strength, the child-friendly web search data only achieved a probability of fit of p = 0.046.

670

Figure 3: Performance in terms of MAP when 0 . . . 9 out of 10 fused original systems are weak.
To summarize, in this section, we have shown how a combination of tail dependence indices and goodness-of-fit tests can be used to help differentiate between domains that may benefit from copula-based retrieval models and those that may not.
7. CONCLUSION
In this work we introduced the use of copulas, a powerful statistical framework for modeling complex dependencies, for information retrieval tasks. We demonstrated the effectiveness of copula-based approaches in improving performance on several standard IR challenges. First, we applied copulas to the task of multivariate document relevance estimation, where each document is described by several potentially correlated relevance criteria. We learned and evaluated copula models for three different IR tasks, using largescale standard corpora: (1) opinionated blog retrieval; (2) personalized social bookmarking; and (3) child-friendly web search, obtaining significant improvements on the first two of these tasks. Second, we introduced copula-based versions of two existing score fusion methods, COMB-Sum and COMBMNZ, and showed that these improve the performance of score fusion on historic TREC submissions, in terms of both effectiveness and robustness, compared to their non-copula counterparts. Finally, we investigated the performance differences of copula models between different domains, and proposed the use of tail dependency indices and goodnessof-fit tests to understand the likely effect of using copulas for a given scenario.
In future work, there are a number of interesting challenges remaining in applying copula-based models to information retrieval. (1) The independence assumption between individual terms in queries and documents is a longstanding simplification in document and language modelling.

Most attempts at incorporating more powerful dependency models into the retrieval process resulted in limited performance improvements at best. We would like to investigate the use of copulas in order to more realistically approximate the complex underlying term dependency structure. (2) During our investigation of the blog retrieval scenario, we encountered examples of non-linear multivariate distributions of relevance and briefly pointed out the different correlation regimes that exist within the joint distribution. While the current single-copula models have been shown to outperform linear combination models at capturing such structures, we would like to proceed to inspecting mixture models in which individual copulas account for certain data ranges to represent the underlying regimes better than a single holistic model could. (3) This work represents an exploratory study that aims to introduce the copula framework to the information retrieval community. For reasons of simplicity and brevity, it is based on data-driven estimation of copula parameters . It would, however, be interesting to build on the large body of previous work on formal modelling of the probability of relevance, to derive custom information retrieval copulas from the assumed distribution of relevance among documents.
8. REFERENCES
[1] Alias-i. LingPipe 3.9.2. http://alias-i.com/lingpipe, 2013.
[2] M. Ames and M. Naaman. Why we tag: motivations for annotation in mobile and online media. In SIGCHI 2007. ACM.
[3] TW Anderson and D.A. Darling. A test of goodness of fit. Journal of the American Statistical Association, 49, 1954.
[4] Avi Arampatzis and Stephen Robertson. Modeling score distributions in information retrieval. Information Retrieval, 2011.
[5] J.A. Aslam and M. Montague. Bayes optimal metasearch: a probabilistic model for combining the results of multiple retrieval systems (poster session). In Proceedings of SIGIR 2000, pages 379­381. ACM.
[6] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert finding in enterprise corpora. In Proceedings of SIGIR 2006, pages 43­50. ACM.
[7] G. Bordogna and G. Pasi. A model for a SOft Fusion of Information Accesses on the web. Fuzzy Sets and Systems, 148(1):105­118, 2004.
[8] P. Borlund. The concept of relevance in IR. JASIST, 2003.
[9] J.P. Bouchaud and M. Potters. Theory of financial risk and derivative pricing: from statistical physics to risk management. Cambridge University Press, 2003.
[10] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In ICML, pages 89­96. ACM, 2005.
[11] A. Charpentier, J.D. Fermanian, and O. Scaillet. The estimation of copulas: Theory and practice. Copulas: From theory to Application in Finance. Risk Publications, 2007.
[12] K. Collins-Thompson, P.N. Bennett, R.W. White, S. de la Chica, and D. Sontag. Personalizing web search results by reading level. In CIKM 2011. ACM.

671

[13] N. Craswell, S. Robertson, H. Zaragoza, and M. Taylor. Relevance weighting for query independent evidence. In Proceedings of SIGIR 2005, pages 416­423. ACM.
[14] Ronan Cummins. Measuring the ability of score distributions to model relevance. In Information Retrieval Technology. Springer, 2011.
[15] C. da Costa Pereira, M. Dragoni, and G. Pasi. Multidimensional relevance: A new aggregation criterion. ECIR 2009.
[16] A. Druin, E. Foss, L. Hatley, E. Golub, M.L. Guha, J. Fails, and H. Hutchinson. How children search the internet with keyword interfaces. In Proceedings of the 8th International Conference on Interaction Design and Children, pages 89­96. ACM, 2009.
[17] C. Eickhoff, P. Serdyukov, and A.P. de Vries. A combined topical/non-topical approach to identifying web sites for children. In WSDM 2011. ACM.
[18] P. Embrechts, F. Lindskog, and A. McNeil. Modelling dependence with copulas and applications to risk management. Handbook of heavy tailed distributions in finance, 8(329-384):1, 2003.
[19] E. Fox and J. Shaw. Combination of multiple searches. NIST Special Pub., 1994.
[20] E.W. Frees and E.A. Valdez. Understanding relationships using copulas. North American actuarial journal, 2(1), 1998.
[21] S. Gerani, C.X. Zhai, and F. Crestani. Score transformation in linear combination for multi-criteria relevance ranking. ECIR 2012.
[22] S.P. Harter. Psychological relevance and information science. JASIS, 43(9):602­615, 1992.
[23] W. H¨offding. Scale-invariant correlation theory. Schriften des Mathematischen Instituts und des Instituts fur Angewandte Mathematik der Universit¨at Berlin, 5(3):181­233, 1940.
[24] X. Huang and W.B. Croft. A unified relevance model for opinion retrieval. In Proceeding of CIKM 2009, pages 947­956. ACM.
[25] Evangelos Kanoulas, Keshi Dai, Virgil Pavlu, and Javed A Aslam. Score distribution models: assumptions, intuition, and robustness to score manipulation. In SIGIR 2010. ACM.
[26] J.M. Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM (JACM), 46(5):604­632, 1999.
[27] W. Kraaij, T. Westerveld, and D. Hiemstra. The importance of prior probabilities for entry page search. In SIGIR. ACM, 2002.
[28] V. Lavrenko and W.B. Croft. Relevance based language models. In Proceedings of SIGIR 2001, pages 120­127. ACM.
[29] V. Lavrenko and W.B. Croft. Relevance models in information retrieval. Language modeling for information retrieval, pages 11­56, 2003.
[30] T.Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 2009.
[31] W. Lu, S. Robertson, and A. MacFarlane. Field-weighted xml retrieval based on bm25. Advances in XML Information Retrieval and Evaluation, pages 161­171, 2006.

[32] C. Macdonald, R.L.T. Santos, I. Ounis, and I. Soboroff. Blog track research at trec. In SIGIR Forum 2010. ACM.
[33] R. Manmatha, Toni M. Rath, and Fangfang Feng. Modeling score distributions for combining the outputs of search engines. In SIGIR 2001.
[34] S. Mizzaro. Relevance: The whole history. JASIS, 1997.
[35] M. Montague and J.A. Aslam. Condorcet fusion for improved retrieval. In Proceedings of CIKM 2002, pages 538­548. ACM.
[36] M. Montague and J.A. Aslam. Relevance score normalization for metasearch. In CIKM 2001. ACM.
[37] A. Onken, S. Gru¨new¨alder, M.H.J. Munk, and K. Obermayer. Analyzing short-term noise dependencies of spike-counts in macaque prefrontal cortex using copulas and the flashlight transformation. PLoS computational biology, 5(11):e1000577, 2009.
[38] M. Persin, J. Zobel, and R. Sacks-Davis. Filtered document retrieval with frequency-sorted indexes. JASIS, 47(10):749­764, 1996.
[39] J.M. Ponte and W.B. Croft. A language modeling approach to information retrieval. In Proceedings of SIGIR 1998, pages 275­281. ACM.
[40] F. Radlinski and T. Joachims. Query chains: learning to rank from implicit feedback. In SIGKDD, pages 239­248. ACM, 2005.
[41] B. Renard and M. Lang. Use of a gaussian copula for multivariate extreme value analysis: Some case studies in hydrology. Advances in Water Resources, 30(4):897­912, 2007.
[42] S. Robertson, H. Zaragoza, and M. Taylor. Simple BM25 extension to multiple weighted fields. In CIKM 2004.
[43] S.E. Robertson, S. Walker, M.M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. Gaithersburgh, MD, 1994.
[44] T. Saracevic. Relevance reconsidered. In Conference on Conceptions of Library and Information Science, 1996.
[45] L. Schamber, M.B. Eisenberg, and M.S. Nilan. A re-examination of relevance: toward a dynamic, situational definition. IPM, 26(6):755­776, 1990.
[46] T. Schmidt. Coping with copulas. Risk Books: Copulas from Theory to Applications in Finance, 2007.
[47] C. Schoelzel, P. Friederichs, et al. Multivariate non-normally distributed random variables in climate research­introduction to the copula approach. Nonlin. Processes Geophys., 15(5):761­772, 2008.
[48] A. Sklar. Fonctions de r´epartition a` n dimensions et leurs marges. Publ. Inst. Statist. Univ. Paris, 8(1):11, 1959.
[49] T. Tsikrika and M. Lalmas. Combining evidence for relevance criteria: a framework and experiments in web retrieval. ECIR 2007.
[50] D. Vallet and P. Castells. Personalized diversification of search results. In SIGIR 2012. ACM.
[51] C.C. Vogt and G.W. Cottrell. Fusion via a linear combination of scores. Information Retrieval, 1(3):151­173, 1999.
[52] S. Wu and F. Crestani. Data fusion with estimated weights. In CIKM 2002. ACM.

672

Taily: Shard Selection Using the Tail of Score Distributions

Robin Aly, Djoerd Hiemstra
University Twente
The Netherlands
{r.aly,hiemstra}@ewi.utwente.nl

Thomas Demeester
Ghent University - iMinds Belgium
thomas.demeester@intec.ugent.be

ABSTRACT
Search engines can improve their efficiency by selecting only few promising shards for each query. State-of-the-art shard selection algorithms first query a central index of sampled documents, and their effectiveness is similar to searching all shards. However, the search in the central index also hurts efficiency. Additionally, we show that the effectiveness of these approaches varies substantially with the sampled documents. This paper proposes Taily, a novel shard selection algorithm that models a query's score distribution in each shard as a Gamma distribution and selects shards with highly scored documents in the tail of the distribution. Taily estimates the parameters of score distributions based on the mean and variance of the score function's features in the collections and shards. Because Taily operates on term statistics instead of document samples, it is efficient and has deterministic effectiveness. Experiments on large web collections (Gov2, CluewebA and CluewebB) show that Taily achieves similar effectiveness to sample-based approaches, and improves upon their efficiency by roughly 20% in terms of used resources and response time.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Search process
Keywords
Distributed Retrieval, Database Selection
1. INTRODUCTION
For large collections, search engines have to shard their index to distribute it over multiple machines. Search efficiency can be further increased by shard selection, that is, by querying a small number of promising shards for each query [5]. An important research challenge in this domain is the definition of shard selection algorithms that have to address the following two issues. First, selective shard search
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

should be more efficient than searching all shards, and second, selective shard search should be as effective as searching all shards. In this paper, we investigate the tradeoff between efficiency and effectiveness of sharded search. We propose Taily, a new shard selection algorithm that represents shards by using parametric distributions of the document scores for a query. Taily is much more efficient while showing similar effectiveness, as compared to an exhaustive search.
State-of-the-art shard selection algorithms use a central sample index (CSI) that contains randomly selected documents from each shard [21, 22, 14]. The algorithms use the results of an initial search against the CSI to select the shards to be used in a second, sharded search. In the literature, the efficiency of shard selection algorithms is usually measured by resources used in the sharded search. In this paper, we argue that efficiency measures should also consider the resources spent during the initial search on the CSI, which can be substantial. For example, a common sample size in the literature is four percent [14], which results in a CSI that is bigger than an average shard once there are more than 25 shards. Searching a CSI that is as large as the average shard uses a considerable percentage of the resources required. For the common case that the algorithm selects two shards, the initial search uses roughly one third of the resources required for answering a query. In our experiments we show that our algorithm is more efficient than current sample-based methods, especially when also considering the resources of the initial search, while maintaining similar effectiveness.
Although the query response time is seldom investigated in the shard selection literature, it is often considered more important than the used resources, which are relatively cheap nowadays [9]. Whereas the search in a CSI can use a substantial amount of the total resources, the influence of its execution time on the total query response time can be more severe. In the above example, the initial search would roughly double the response time, assuming the parallel execution of the sharded search. We show that Taily's improvement over the query response time of current shard selection algorithms is particularly strong.
One aspect of sample-based methods that has not been studied so far is the effect of the particular random sample in the CSI on the search effectiveness. One might expect that, if samples are truly random and sufficiently large, different random samples would produce stable effectiveness of the search system in terms of precision or nDCG. We show in this paper that this expectation does not hold in practice. As we show in Section 5, different sample sizes of up to 4%

673

of each shard, lead to substantially different effectiveness of the sharded search system. We believe that this variation in effectiveness is a drawback of the sample-based methods tested in this paper.
Like Kulkarni et al. [14] and Arguello et al. [3], Taily is used on clusters of machines in a cooperative search environment. Taily selects one or more shards by estimating the number of documents of a shard that are highly scored in the collection, which we model by the right tail of the score distributions in the collection and the shards. The basic idea of this approach has been proposed by Markov [15]. To estimate these score distributions, we follow the approach by Kanoulas et al. [12], who derive score distributions from statistics of features related to a query's terms (e.g. a term's language model score). Being based on statistics for each term in a vocabulary, Taily belongs to the class of vocabulary-based selection algorithms, which are often biased to larger shards and often show weaker performance than sample-based methods. Our contribution to the shard selection literature are threefold: first, experiments show that its effectiveness is similar or stronger than the effectiveness of sample-based methods, second, compared to a search in a CSI, its processing of feature statistics is more efficient, and results in a lower number of resources used in total and a faster query response time, and finally, compared to sample-based methods, the efficiency and effectiveness do not depend on the size or the sampled documents in a CSI.
The remainder of this paper is structured as follows. Section 2 describes related work on shard selection. Section 3 elaborates on Taily's shard selection algorithm. Section 4 defines the measures that we used to compare our method to state-of-the-art algorithms. Section 5 describes the experiments that we conducted to explore the performance of our search method. Section 6 concludes this paper.
2. RELATED WORK
In this section, we present shard selection algorithms from two classes that are most related to this work. Note that there is also a significant number of works on other approaches to shard selection. Because of space limitations, we refer the interested reader to Kulkarni et al. [14] where a more exhaustive list of related work is presented.
Most of the early shard selection algorithms are vocabularybased: they represent shards by collection statistics about the terms in the search engine's vocabulary. The popular CORI algorithm by Callan et al. [6] represents a shard by the number of its documents that contain the terms of a vocabulary. The shards are ranked using the INDRI version of the tf · idf score function using the mentioned number as the frequency of each query term. CORI selects a fixed number of shards from the top of this ranking. Xu and Croft [23] build upon this approach by representing them by topical language models. The shards are ranked by the Kullback-Leibner divergence between the language models of the query and the topics. CORI and the algorithm by Xu and Croft characterize a shard by statistics over all its documents. We propose that shard selection algorithms should focus on documents with high scores because they contribute most to the system's effectiveness. The gGloss algorithm by Gravano and Garcia-Molina [10] selects shards based on the distribution of the vector space weights, which we refer to as features values. They assume that the distribution of the features is uniform. The algorithm ranks a

shard by its expected score value calculated from the expectation of all feature values for a query. In this paper, we also consider the expected score, which we also infer through the expected feature values of the query terms. However, unlike the gGloss algorithm, we assume a gamma distribution of the score instead of a uniform distribution, and focus on the tail of the distribution, which contains the highly scored documents.
Algorithms from the second main class of are called samplebased algorithms because they use a central sample index (CSI) of documents from each shard for shard selection. The REDDE algorithm [21] ranks shards according to the number of the highest ranked documents that belong to this shard, weighted by the ratio between the shard's size and the size of the shard's documents in the CSI. The SUSHI algorithm [22] determine the best fitting function from a set of possible functions that between the estimated ranks of a shard's documents in the central index and their observed scores from the initial search. Using this function, the algorithm estimates the scores of the top-ranked documents of each shard. SUSHI selects shards based on their estimated number of documents among a number of top-ranked documents in the global ranking. REDDE and SUSHI assume that a shard's documents in the CSI are at equidistant ranks in the ranking of the shard. We believe that this assumption may be too strong because the actual ranks vary widely depending on the randomly sampled documents in the CSI, as we found in preliminary experiments. Kulkarni et al. [14] present a family of three algorithms that model shard selection as a process of the CSI documents voting for the shards that they represent. The vote of a shard is the sum of the votes of its documents. The algorithms select shards that have an accumulated vote higher than 0.0001. The three algorithms differ in how they model the strength of a document's vote. In the most effective method Rank-S, the strength of the votes is based on the score from the initial search and decays exponentially according to the rank of the document in this ranking. The base of the exponential decay function is a tuning parameter of the model.
The shard selection algorithm proposed in this paper is vocabulary-based. However, instead of considering all documents in a shard for ranking, it selects shards based on the highly scored documents of a shard similar to the described sample-based methods.
3. SHARD SELECTION USING THE TAIL OF SCORE DISTRIBUTIONS
Before formally introducing our shard selection algorithm in this section, we will explain our reasoning and the intuition behind it.
3.1 Intuition and Reasoning
Abstracting from sharded search, a search engine uses a score function that assigns each document in the collection a score. The documents are then ranked based on that score, and for most effectiveness measures, the top-ranked documents are the most influential. Now, a shard selection algorithm has to identify those shards whose documents can be left out from the complete ranking without hurting the effectiveness. We therefore design our shard selection algorithm to leave out shards with no or only few documents in the top of the complete document ranking. The number of

674

25000 20000

at least one query term

at least one query term all query terms 300

count count

15000

200

10000
100 5000

0

-25

-20

-15

-10

Score s

0

-25

-20

-15

-10

Score s

(a) Score distributions of documents with (b) Score distributions focusing on the (c) Score distribution in two shards of doc-

at least one query term.

right tail.

uments with all query terms.

Figure 1: Intuition of the shard selection process for the web-track query 843 pol pot. The vertical bar indicates the cut-off score of the nc = 100 highest scored documents. The shown distributions are Gamma distributions fitted using the maximum likelihood and multiplied by the number of documents in the distribution.

considered top-ranked documents can vary depending on the search scenario. Our algorithm therefore considers a number of nc highly scored documents. Expressed differently, these documents are the right tail of the collection's score distribution in response to the given query, and hence we name our shard selection algorithm Taily. Figure 1a shows the score frequency distribution of the query 843 pol pot in the Gov2 collection using language model scores. A search engine may want to preserve, e.g., nc = 100 top-ranked documents. For our example, this corresponds to the documents that are assigned a language model score of -14.6 or higher for this query.
The more accurate our model is in the right tail of the score distribution, the more accurate we can expect our shard selection to be. Score distributions are typically dominated by low scores of documents that contain no or only few of the query terms. We do not expect that the tail of these score distributions can be accurately modeled. Instead, we model the score distribution of documents that contain all query terms, which include the top-ranked documents for most queries and empirically leads to a better fit of the right tail, see Figure 1b.
Taily selects shards based on the number of documents with a score above the cut-off score of the top-ncdocuments. To estimate this number, Taily fits the score distribution in each of the shards, from which the probability of a document in this shard with a score above that cut-off point can be readily calculated. Because shards differ in size and absolute numbers of high-scoring documents, a shard with a low right-tail probability might still have a reasonable number of documents with scores above cut-off. We therefore also estimate the total number of documents that participate in the considered score distribution and select shards based on the expected number of documents that are above the cut-off score. For example, Figure 1c shows the empirical and fitted score distribution1 of the shards 19 and 41 of topical shards
1Note that Fig. 1 displays histograms with absolute frequencies. The fitted lines are the estimated density functions (based on the Gamma distribution), rescaled by the total number of documents included and its bin width, in order to allow visual comparison with the histograms.

generated by Kulkarni and Callan [13]. Most documents in the selected tail of the collection's score distribution belong to shard 41. Therefore, Taily prefers shard 41 over shard 19 for this query.
A popular way to estimate score distributions is to use scores of document samples from the top of the ranking [1]. However, because we avoid the use of a central sample index, this type of methods is not applicable here. Instead, following Kanoulas et al. [12], we infer the query dependent score distribution from query independent feature distributions that are summed in the score function. The parameters of the feature distributions form Taily's shard representation, which can be calculated offline.
In the following, we develop the Taily algorithm more formally. Section 3.2 introduces the used score function and Section 3.3 describes the statistics that form Taily's shard representation. Section 3.4 shows how these statistics are used to estimate the parameters of the score distributions for the shards, and for the whole collection. Section 3.5 describes how the number of documents with all query terms in the whole collection and per shard can be estimated. Using the estimates for the score distribution and the number of documents, we define Taily's shard selection criterion in Section 3.6.
3.2 Notation and Score Functions
We use the following notation throughout this paper. Queries and documents are denoted by lower case q and d respectively. Sets of documents are denoted by D, and particular set is indicated by a subscript. In particular, let Dc be the set of documents in the total considered collection, and let D1, ..., DN be the sets of documents of the N shards of this collection. We often refer to either the set of documents in the collection or the shards, for which we use the subscript i. Terms are denoted by lower case t, the query terms of a query q are denoted by q. The length of document d is denoted by dl(d), the frequency of term t in document d is written c(t, d), and the number of documents from set Di that contain term t at least once is given by c(t, Di).
Taily infers a query's score distribution from the distributions of the features that constitute the query's score func-

675

tion. In general, our algorithm can be used with any score function that is a weighted sum of term-related feature values. Note that we consider score functions independently from their theoretical motivation. To facilitate experiments, which require a particular score function, we focus in this paper on the query likelihood model, as implemented in the Indri search engine2. The query likelihood model uses for a term t in a document d a term feature ft(d), which is defined as follows:

c(t, d) + µP (t|D)

ft(d) = log

dl(d) + µ

(1)

where P (t|D) =

d c(t,d) d dl(d)

is

the

collection

prior

of

term

t,

and

µ is the Dirichlet smoothing parameter. Note that the term

features in (1) are query independent. The score function

s(d) of a document d for a query q is a sum of the features

for the query terms:

s(d) = ft(d).

(2)

tq

In this paper we focus on score functions based on features that are related to a single query term. In future work, we plan to extend Taily to capture multi-term features such as ones used in the full dependency model [16], priors such as PageRank or spam scores, see [19] for a possible integration of these features into score functions.

3.3 Statistical Shard Representation

In order to infer the score distributions in shards and the collection, we represent each of them by the distribution parameters of term features. The main statistics of a feature ft for term t in document set Di are the expected value Ei[ft] and the variance vari[ft] of the feature, which can be calculated as follows:

Ei[ft] =

dDi ft(d) c(t, Di)

(3)

Ei[ft2] =

dDi ft(d)2 c(t, Di)

vari[ft] = Ei[ft2] - Ei[ft]2

(4)

where Ei[ft2] is the expected squared feature value. These quantities can be calculated by a single scan through the collection.
The language model score function used in this paper produces negative values. However, the Gamma distribution that we use for the score function is defined for positive values. To be able to shift the score distribution in the next section, we also store for each feature f its minimum value in the collection c:

minc[f ] = min{f (d)|d  Dc, c(t, d) > 0}

The expected feature values from (3), the feature variances in (4), and the above minimum values, form the representation used to calculate the score distribution in the shards and the total collection.

3.4 Inferring Score Distributions
Given the shard representation described in the previous section, we derive the distribution parameters of the query specific score distribution. Because the score function used
2http://www.lemurproject.org/indri/

in this paper produces negative scores, we instead consider a score distribution that is shifted by its minimum value, similar to Arampatzis et al. [2]:

|fq |
s(d) = s(d) + minc[fj].
j=1
To keep our notation lean, we continue using s instead of s for the score function, keeping in mind that it is now positive defined. For a document set i, the expected score Ei[s] and the score variance vari[s] can be derived from the definition of the score function in (2)

|fq |

|fq |

Ei[s] =

Ei[fj ] + minc[fj ]

(5)

j=1

j=1

|fq |

vari[s] =

vari[fj ]

(6)

j=1

where fq is the feature vector of the query terms in (2), and fj is the jth feature in this vector. Equation 6 uses the simplifying assumption that the sum of covariances is zero. Note that we verified the validity of this assumption by repeating our experiment taking covariances into account, which did not result in a significant increase in effectiveness.
According to Kanoulas et al. [12], the distribution of language model scores is gamma distributed. The parameters of the distribution in document set i can be derived from the expected score and the variance by using the method of moments:

ki

=

Ei[s]2 vari[s]

(7)

i

=

vari[s] Ei[s]

(8)

where we used the definition of these parameters. Having
the parameters ki and i for a document set i, we can define its cumulative score distribution function, which yields the
probability of documents having a score greater than a score s in a document set i:3

cdfi(s

)

=

Pi(s

>

s

)

=

1-

1 (ki)



s ki, i

(9)

where  is the Gamma function,  is the incomplete Gamma function, and ki and i are the distribution parameters defined above. For the case of the whole collection and the example introduced previously, the values of the cumulative distribution function can be visualized as the percentage of documents with a higher score than -14.6 in Figure 1c.

3.5 Documents With All Query Terms
To make the probabilities from the cumulative density functions comparable, Taily uses the number of documents with all query terms in this set. To reduce the strength of assuming independence between the occurrence of query terms [7], we first estimate the number of documents that contain at least one any query term Anyi in a document
3Note that cumulative distributions are usually defined in terms of the probability in the left tail. We differ from this practice because it simplifies the mathematical formalism used to describe Taily.

676

set i:





|q|
Anyi = |Di| 1 -
j=1

1

-

c(tj , Di) |Di|



where the term

|q| j=1

(1

-

c(t,Di |Di |

)

)

estimates

the

number

of

documents in document set i that have none of the query

terms. Among the Anyi documents that contain at least

one query term, we estimate the number of documents that

contain all query terms All i by assuming independence of

the term occurrences:

All i

=

Anyi

|q| j=1

c(tj , Di) . Anyi

(10)

where c(tj, Di)/Anyi is the probability that a document with term tj appears in the documents in Di that have at least one of the query term.
Our experiments show that this estimate produces strong and stable results. Important to note here, is that we want an efficient and lightweight algorithm, also during the preprocessing stage. Therefore, even for two-term queries, instead of counting the mutual term occurrences, quadratic in the vocabulary size, we estimate these based on the singleterm occurrences.

3.6 Shard Ranking and Selection Criterion

Given the cumulative score distribution cdfi and estimated

number of documents that contain all query terms All i for

both the whole collection and each shard separately, we de-

fine Taily's shard selection criteria. Based on our intuition

in Figure 1, we first estimate the cut-off score of a fixed num-

ber of top-ranked documents in the collection that at least

should be in the sharded ranking. Let this number be nc,

which is a parameter of Taily. The probability of a document

in the collection to be among the top-ranked documents can

be calculated as:

pc

=

nc All c

(11)

where All c is the estimated number of documents in the collection with all query terms. The cut-off score sc of the top-nc documents can be estimated using the inverse of the cumulative density function: sc = cdfc-1(pc) where pc is the probability defined above.
Using the score distribution in a shard i, we can calculate
the probability that a document in this shard has a score higher than the cut-off score sc: pi = cdfi(sc). The number of documents in shard i that have a score above sc, written ni, can then be readily estimated4 by ni = All i pi. The number of documents in shard i with all query terms is a mere estimation (see (10)), and the sum of estimates All i for all shards not necessarily equals the overall estimate All c. Experimentally, this appeared to introduce inaccuracies in the
results. As the improvement of score distribution estimates
is an ongoing research topic [1], we limit ourselves here to a
simple solution. We assume that the estimation of the expected number of documents in the collection nc is accurate

4In fact, we estimate the number of documents that have a score above sc and contain all query terms. This means that we assume that for the shards to be selected, most
documents above cut-off contain all query terms. Experi-
mentally, this appears to hold if sc is reasonably high, see e.g. Figure 1b.

such that (11) holds. A suitably normalized estimate of ni

is hence

ni

= All i pi

nc sumNj=1pj Allj

(12)

where the term All i pi is the unnormalized number of documents in Di above score sc and the right term is a normalization constant ensuring that the estimated number of documents above sc from each shard j add up to the corresponding number of documents in the whole collection, which is nc.
We are now able to define Taily's shard selection criterion sel(q) for a query q that selects shards with an estimated number of documents in the top-m above a threshold:

sel(q) = i i  1...N, ni > v

(13)

where i is a shard index, and v is the selection threshold. Note that it can be beneficial for v to be higher than 0 because of the computational costs for including a shard with only very few estimated documents in the top ranks.

4. EFFICIENCY MEASURES

Before we can evaluate Taily, we have to define measures that quantify the efficiency of shard selection algorithms in terms of used resources and response time. To be comparable to related work, we base our measure on the measure by Kulkarni et al. [14], which calculates the resources used by a shard selection algorithm for a query q by the number of documents that the sharded search has to access:

|sel(q)|

CR(q) =

Di(q)

i=1

(14)

where sel(q) are the shards selected by the algorithm and Di(q) is the number of documents in shard i that match at least one of the query terms q. As discussed in Section 1, the selection algorithm itself can require substantial resources, which should be reflected in the efficiency measure. We therefore extend the above efficiency measure by a component that reflects the costs of executing the selection algorithm. We arrive at our resource efficiency measure CRES(q) of a query q:

CRES(q) = CSEL(q) + CR(q)

(15)

where CR(q) is measure by Kulkarni et al. from (14), and CSEL(q) are the costs for executing the selection algorithm for query q. The selection costs CSEL depend on the type of selection algorithm. For sample-based methods, we set the selection costs CSEL(q) = CSI(q), where CSI(q) is the number of documents in the CSI that have at least one query term. For vocabulary based algorithms, we set CSEL(q) = N , where N is the number of shards in the collection, which is the upper bound of the number of entries in the shard representation for any query term.
Additional to the resource usage, the query response time is another important efficiency aspect to consider [18]. In contrast to the used resources, measures for the response time have to take into account that the selected shards are usually processed in parallel, once the selection algorithm has finished. For the search in the selected shards, the costs therefore mainly depend on the shard with the most matching documents. Similar to the methodology in the evaluation of database systems, we measure the response time by the

677

Table 1: Statistics about the collections used in the experiments of this paper (In entries of the form X±Y , X is the average and Y is the standard deviation. TB=terrabyte track, WT=web track.).

Collection Gov2 CluewebB CluewebA

Documents 25M 50M
250M

Shards 50
100 1000

Query set TB '04-'06 (701-750) WT '09+'10 (1-100) WT '09+'10 (1-100)

Query length
3.1(±0.97) 2.1(±1.36) 2.1(±1.36)

Rel. Docs
180(±148) 49(±42)
124(±75)

number of accessed data items on the longest execution path (ignoring implementation dependent aspects):

CTIME(q) = CSEL(q) + max|is=e1l(q)| {Di(q)} .

(16)

where 1, ..., |sel(q)| are the selected shards and the other symbols carry the same definition as in (15). We report average values CRES and CTIME over a considered query set, similar to reporting the mean average precision instead of individual average precision values. Note that the measures in this section consider the number of accessed items, which are documents in shards or CSIs, and shards in vocabulary-based methods. Another choice would have been to consider the number of accessed postings for these items in the inverted files of the query terms.

5. EXPERIMENTS
In this section, we describe the experiments that we conducted to evaluate our shard selection algorithm Taily. We aligned our experiments to the ones from the recent publication by Kulkarni et al. [14], to ensure comparability of our work to the state-of-the-art in shard selection. We proceed as follows: first, we describe the experimental setup, second, we describe each experiment and its results, and finally, we discuss the findings.
5.1 Setup
Table 1 describes the collections and query sets that we used. The collections represent modern retrieval collections of a medium to large size. We used the shards defined by the topical clustering algorithm by Kulkarni and Callan [13]. Due to spam, the effectiveness of the search on the full CluewebA collection of 500M documents was weak. We therefore removed the documents whose spam scores were among the 50% highest scores according to the fusion method by Cormack et al. [8].
We implemented the experiments using the hadoop mapreduce framework, directly answering queries and generating statistics from the full text of the collection; similarly to the approach described in [11]. We used the Krovetz stemmer for both the document text and the queries. We did not use stopwording. For the exhaustive search and the searches in the central sample index (CSI), we used the full dependency model [16] with the parameter setting (0.8, 0.1, 0.1) for single term features, unordered term features and ordered term features respectively as recommended by Metzler et al. [17] and used in Kulkarni et al. [14]. The Dirichlet smoothing parameter was set to µ = 2500.
We chose one baseline from each of the two classes of selection algorithms presented in Section 2. As a vocabularybased algorithm, we used the popular CORI algorithm [6]. As a sample-based algorithm we chose Rank-S by Kulkarni et al. [14], which showed significantly stronger performance than REDDE [21] and SUSHI [22], two other state-of-the-art shard selection algorithms. Note that we added for Rank-S

 gov2 cluewebB cluewebA

30

0.5

70


60


55 50


45 40




P@30

0.4

60 55 50 45 40 70

30

0.3

70 6505 50 45 40

30

4e+05

5e+05

6e+05

CRES

7e+05

Figure 2: Sensitivity of Taily with nc=400 according to the threshold parameter v (as indicated in the plot).

the minimum score of the score of the full dependency to make the scores positive. This was not reported by Kulkarni et al. [14] but it was important to achieve results comparable to the ones in the original publication. The documents for the central sample index used by Rank-S were uniformly sampled without replacement from each shard until a percentage P of the shard's size was reached. We ensured that each shard was represented by at least 100 documents. To rule out random effects, we repeated the runs with Rank-S 50 times, producing 50 CSIs with potentially different shards selections. Unless stated otherwise, the reported performance measures are the averages of the 50 repetitions. The average approximates the expected performance for a random CSI of this size. Note that performing statistical significance tests using these expected performance values is mathematically speaking problematic. We still report the results of these tests as an indication of the strength of the performance change. Note that Kulkarni et al. [14] use only a single CSI in their results. Therefore, their numerical results do not necessarily correspond to ours.
The size of the CSI can influence the performance of Rank-S. Unless stated otherwise, we used P = 0.02, 0.01 and 0.01 for Gov2, CluewebB and for CluewebA respectively. For Gov2 and CluewebB these settings resulted in a CSI that was roughly as big as an average shard in the respective collection. For CluewebA we chose P = 0.01 because using P = 0.001, which corresponds to the size of an average shard, caused poor effectiveness.
We used the effectiveness measures precision at ten, thirty and hundred (P @10, P @30, P @100), mean average precision (map), and ndcg at ten (ndcg@10). When we focused on a single effectiveness measure, we chose P @30 because it was more stable than P @10 for all selection algorithms but still reflected a precision oriented search task. To mea-

678

P@30 P@30 P@30

 CORI Rank-S Taily
0.52

0.50

0.48



0.46



0.44 4e+05

6e+05 CRES

8e+05

(a) Gov2

0.325

 CORI Rank-S Taily

0.300

0.275

0.250 

0.225 3e+05

5e+05


7e+05 CRES

9e+05

(b) CluewebB

 CORI Rank-S Taily

0.25

0.20



0.15 400000



800000

1200000

CRES

1600000

(c) CluewebA

 CORI Rank-S Taily
0.52 0.50

0.325 0.300

 CORI Rank-S Taily

 CORI Rank-S Taily
0.25

P@30 P@30 P@30

0.48



0.46



0.44

300000

350000 400000 CTIME

450000

(d) Gov2

0.275 0.250

0.20 

0.225

300000

350000 400000 CTIME

 450000

0.15

(e) CluewebB



4e+05

6e+05 CTIME

 8e+05

(f) CluewebA

Figure 3: Efficiency-effectiveness tradeoff for CORI, Rank-S, and Taily. The following tradeoff parameter settings of each method were used. CORI: n  {2, 3} (higher values were always outside the limits of the x-axis), Rank-S: B  {2, 5, 10, 30, 50, 70, 100, 200, 500} (lower values caused lower efficiency), Taily: nc  {200, 250, 300, 350, 400, 600, 800, 1000, 1500, 2000}.

sure efficiency, we used the resource efficiency CRES and the response time CTIME described in Section 4.
5.2 Threshold Parameter
The threshold parameter v, defined in Section 3.6, specifies the minimum number of documents in the right tail of the collection's score distribution that a shard should have to be selected. Figure 2 shows a sensitivity analysis of Taily towards changes of v by displaying the resulting P @30 and CRES measures (here, we used a fixed tradeoff parameter of nc = 400 but the results were similar for other values of nc). The effectiveness of Taily was robust against changes of v, and increased slightly for Gov2. The parameter setting v = 50 caused efficiency and effectiveness around the median of the tested values. We chose this parameter setting for the rest of our experiments.
5.3 Efficiency-Effectiveness Comparison
An important characteristic of a shard selection algorithm is the tradeoff it provides between efficiency and effectiveness. A search engine operator may want to invest more resources to ensure high effectiveness, or to make more efficient use of resources and accept worse effectiveness. CORI, Rank-S and Taily each have a parameter that determines the tradeoff between efficiency and effectiveness. For CORI the parameter n states a fixed number of the highest ranked shards that are selected, second, Rank-S uses the parameter B that determines the influence of a CSI document on the

selection of the shard it belongs to (see Section 2), finally, Taily uses the parameter nc that determines the number of top-ranked documents that should be included in the results of the sharded search.
Figure 3 compares the tradeoff that CORI, Rank-S, and Taily provided in the indicated parameter range. The xaxes show the resource usage CRES or the response time CTIME. The y-axes show the effectiveness in terms of precision P @30.
Figure 3a-Figure 3c show the tradeoff between CRES and P @30. For Gov2 the tradeoff is similar for CORI, Rank-S and Taily at low efficiency values. Rank-S and Taily provide a similar tradeoff between effectiveness and efficiency over all parameter settings. For CluewebB the efficiency of CORI was always lower than the one of Rank-S and Taily. The effectiveness of Rank-S was lower than the one of Taily for a resource usage of CRES < 60, 000. With higher resource usage, both methods had similar effectiveness. For CluewebA Taily showed a higher effectiveness than Rank-S until CRES = 90, 000.
Figure 3d-Figure 3f show the tradeoff between CTIME and P @30. For Gov2 CORI performed similar to Taily. All parameter settings of Rank-S had a larger response time than the ones of Taily. To achieve comparable effectiveness, the response time of Rank-S was roughly 15% larger than the one from Taily. For CluewebB CORI's performance was low. Taily showed higher effectiveness than Rank-S until

679

Table 2: Effectiveness and efficiency comparison between Taily and Rank-S for the indicated parameter settings. ( and indicate statistically significant improvement or regression respectively compared to the Exhaustive search, using a two-
sided t-test with p-value<0.05. Percentages state the efficiency change compared to the Rank-S method.)

Method Exhaustive CORI (n=3) Rank-S (B=50 P=0.02) Taily (nc = 400, v = 50)

P @10 0.58 0.57 0.55 0.56

P @30 0.52 0.48 0.48 0.48

P @100 0.42 0.36 0.37 0.38

Method Exhaustive CORI (n=3) Rank-S (B=50 P=0.01) Taily (nc = 400, v = 50)

P @10 0.29 0.25 0.31 0.31

P @30 0.32 0.24 0.28 0.33

P @100 0.22 0.16 0.18 0.22

Method Exhaustive CORI (n=3) Rank-S (B=50 P=0.01) Taily (nc = 400, v = 50)

P @10 0.29 0.20 0.29 0.30

P @30 0.27 0.17 0.24 0.27

P @100 0.18 0.11 0.15 0.17

map ndcg@10

0.34

0.49

0.25

0.48

0.24

0.45

0.27

0.46

(a) Gov2

Shards 50.0 3.0 1.5 2.6

CRES 4.92M 0.71M 0.45M 0.55M

22.1%

CTIME 0.51M 0.33M 0.38M 0.32M

-15.7%

map ndcg@10

0.20

0.24

0.13

0.21

0.15

0.27

0.18

0.27

(b) CluewebB

Shards 100.0 3.0 1.5 2.7

CRES 10.15M
0.90M 0.40M 0.51M

26.1%

CTIME 0.47M 0.45M 0.34M 0.32M

-6.5%

map ndcg@10

0.11

0.19

0.06

0.14

0.08

0.19

0.09

0.20

(c) CluewebA

Shards 1000.0
3.0 2.2 2.5

CRES 48.78M
1.46M 0.90M 0.47M

-47.6%

CTIME 1.02M 0.78M 0.77M 0.33M

-57.3%

roughly CTIME = 40, 0000. For CluewebA CORI's performance is again low. Compared with CluewebB the difference of Rank-S and Taily in terms of CTIME is larger.
The results in Figure 3 show that the effectiveness of Rank-S varies with different settings of B, unlike the results by Kulkarni et al. [14] (their Figures 2 and 3) that suggest that the effectiveness of Rank-S is almost unaffected by the parameter. Because the particular sampled documents in the central indices used by Kulkarni et al. were not available, we did not further investigate these differences. They might be explained by two important differences in the experimental setup: first, we use a smaller CSI size (P = 0.02 and P = 0.01 vs. P = 0.04 by Kulkarni et al.), and second, we report the average performance over random samples, instead of results on a single sample.
5.4 Detailed Effectiveness Analysis
We also investigated the effectiveness in terms of multiple measures at a fixed parameter settings. For CORI we chose a shard cut-off value of n = 3. Although larger values sometimes produced better effectiveness, the efficiency was too low to be comparable to Rank-S and Taily. We set the Rank-S parameter B = 50 for all three collections because Kulkarni et al. also used this value for CluewebB. For Taily we set nc = 400 and v = 50. The efficiency and effectiveness for these parameters is shown as larger points in Figure 3. We also display the performance for the exhaustive search, as a reference. For the displayed efficiency of the exhaustive search, we assumed a parallel search of all shards with zero costs for the selection (CSEL = 0 in (15) and (16)).
Table 2a shows the results for the Gov2 collection. CORI shows similar performance than Rank-S and Taily in terms of P @10. The resource efficiency was the lowest among the three methods. The response time CTIME was comparable to the one of Taily. Rank-S showed comparable effectiveness to Taily with only map being lower. The resource usage of Rank-S was the lowest among the three runs. The response

time was the largest. On average, Taily and Rank-S selected 2.6 and 1.5 shards on average respectively.
Table 2b shows the results for the CluewebB collection. CORI showed weaker efficiency and effectiveness than Rank-S and Taily. Taily's effectiveness was stronger for all efficiency measures. In terms of CRES, Taily used 26.1% more resources than Rank-S, while the response time improved by 6.5%. Note that Rank-S and Taily improved significantly upon exhaustive search for ndcg@10. This is possible if topranked non-relevant documents in the exhaustive ranking are from shards, which are ignored by the shard selection algorithm. We consider this phenomenon as a property of the data and therefore did not make any further investigations.
Table 2c shows the results for the CluewebA collection. CORI showed weak effectiveness and efficiency. Rank-S performed significantly worse than exhaustive search for P @30, P @100 and map. Taily showed similar or better effectiveness compared to Rank-S. Compared to Rank-S, the efficiency improved 47.6% and 57.3% upon Rank-S in terms of CRES and CTIME respectively.
Note that although Taily used more resources in Gov2 and CluewebB than Rank-S for the indicated parameter settings, there exist other parameter settings where this was not the case, see Figure 3. Furthermore, the used effectiveness measures depend on a high coverage of judgments in the topranked documents. For CluewebA only roughly 50% of the top-30 documents were judged. This coverage was similar for all investigated shard selection algorithms. As a result, the absolute effectiveness values are not exact. However, we believe that the relative effectiveness comparison among the shard selection algorithms would be similar under complete coverage because the selective rankings are derived from the exhaustive ranking and only differ by few added or removed shards per query, which is likely to average out.
5.5 Influence of Document Sample on Rank-S
The performance of Rank-S depends on the CSI it uses

680

P@30 P@30 P@30

0.50


  

0.48

0.46





0.44



0.010

0.020

0.040

Taily

Relative CSI Size

(a) Gov2


0.32

0.30

0.28

0.26

0.24



0.010

0.020

0.040

Taily

Relative CSI Size

(b) CluewebB

0.250

0.225

0.200 0.175







0.001 0.005 0.010 0.020 0.040 Taily Relative CSI Size

(c) CluewebA

Figure 4: Influence of the document sample on the effectiveness of Rank-S. (Rank-S used B = 50. Taily does not use of a CSI and his parameters were nc = 400 and v = 50).

for the initial search in two ways: first, the number of documents, assuming that a larger CSI also causes a more accurate selection, and second, exactly which documents are sampled. Variation in performance can influence the comparison of Rank-S and Taily. Therefore, we investigated the performance variability of Rank-S along these axes.
Figure 4 summarizes the results of this experiment, using the P @30 measure. As a reference, we also plot the effectiveness of Taily. Figure 4a shows the results for Gov2. The median effectiveness increased by roughly 5% between P = 0.01 and P = 0.04. The lowest achieved performance was roughly 8% weaker than the median performance. Taily showed similar effectiveness as the median performance of Rank-S with P = 0.04.
Figure 4b shows the results for CluewebB. The median effectiveness was more stable when changing the CSI size than with Gov2. The lowest achieved search performance was roughly 10% lower than the median performance. Taily showed better effectiveness than all measurements for Rank-S.
Figure 4c shows the results for CluewebA. The median search performance increases by roughly 18% between P = 0.001 and P = 0.04. The lowest achieved performance was roughly 15% lower than the median performance. Taily's effectiveness was en par with the best-measured effectiveness of Rank-S with P = 0.02 and P = 0.04.
5.6 Discussion
We discuss the results presented in this section. Taily depends on the following two parameters: the tradeoff parameter nc that determines the number of high scored documents of the collection, and the threshold parameter v that determines minimum of this estimate for are a shard to be selected, which was introduced to suppress estimation errors. Section 5.2 and Section 5.3 showed that both parameters can be set reliably within a range of values, resulting in strong performance. Nevertheless, the parameter values are unintuitive. For example, the setting of nc = 400 and v = 50, used in Section 5.4, means according to the definition of the parameters that shards with 50 documents in the top 400 documents should not be selected. A likely reason for these unintuitive estimates is the crude normalization of the expected values in (12). Therefore, for future work we propose the research of more accurate normalization methods, which possibly improves the performance of Taily further.

The results of the comparison of CORI, Rank-S and Taily yielded a number of findings. First, the performance of the vocabulary-based algorithms shows opposite trends depending on the collection size. While CORI's effectiveness decreases, Taily effectiveness increases. A likely explanation for CORI's performance decrease is its known bias towards bigger shards, which do not necessarily contain many relevant documents, which causes efficiency and effectiveness to drop. Therefore, Taily's approach to model the tail of the score distribution in each shard selects substantially smaller shards with more relevant documents than CORI's approach to model all documents. Second, for each parameter setting of Rank-S there was a parameter setting of Taily with higher efficiency and similar or higher effectiveness compared to the former's expected performance. This was in particular true for the response time. This shows that the selection costs CSEL for executing Taily were substantially lower than the ones for Rank-S and Taily's vocabulary-based approach can select shards with a comparable number of relevant documents as Rank-S. Finally, because of the shape of the achieved efficiency - effectiveness tradeoff combinations, we propose that finding tradeoff settings for a given efficiency is easier to achieve for Taily than for Rank-S. An exception is the response time of Taily in Gov2, which increases at a high rate, causing small differences in efficiency to cause large changes in effectiveness.
The effectiveness of Rank-S is strongly correlated with the CSI size. For example, for CluewebA an increase of the CSI size from P = 0.001 to P = 0.01 improved the median effectiveness by 16%. At the same time, larger CSI sizes consume more storage space, which also makes them less efficient. Therefore, it is likely that Rank-S does not scale to collections lager than CluewebA.
6. CONCLUSIONS AND FUTURE WORK
We introduced Taily, a novel shard selection algorithm that is based on highly scored documents in the tail of the collection's score distribution. The scores are assumed to be Gamma distributed and estimated from statistics about features related to the query terms of the language model score function. Taily is therefore a member of vocabularybased shard selection algorithms that represent shards by statistics of terms in the vocabulary.
We evaluated Taily on three large web collections (Gov2,

681

CluewebB and CluewebA) using topically clustered shards defined by Kulkarni and Callan [13]. Compared to the popular vocabulary-based method CORI [6], Taily showed better efficiency and effectiveness. Compared to Rank-S [14], a state-of-the-art, sample-based shard selection algorithm, Taily achieved similar or better effectiveness using less resources. Especially for larger collections and CSIs, the vocabulary-based Taily used less resources than the sample-based Rank-S although it selected on average more shards. The improvement of the response time compared to Rank-S was larger than the improvement of the resource usage. Taily showed his highest effectiveness with a shorter response time than the shortest response time measured for Rank-S over a wide range of parameters. For CluewebA, Taily achieved its best performance in roughly 50% of the response time of Rank-S.
Taily does not use document samples and therefore does not need to answer some of the questions that sample-based methods need to answer, such as what samples to take, and what would be a reasonable size of the central sample index (CSI). We investigated possible answers to these questions for Rank-S, and found that the effectiveness of Rank-S decreases with the CSI size. For example, the effectiveness decreased by 20% between using a commonly used CSI size of 4% and 0.1% for the CluewebA collection, where 0.1% corresponded to the size of an average shard. We also found that, for a given CSI size, the lowest effectiveness of Rank-S in 50 CSIs consisting of different document samples was roughly 10% lower than the median performance. The dependence of effectiveness on CSI size and CSI sample is a weakness of sample-based methods. This weakness can also be seen as an advantage of vocabulary-based methods, like Taily, which do not depend on a CSI.
This work focused on shard selection in a cooperative environment using topically clustered shards. We believe that the basic ideas behind Taily can also be applied to database selection in a uncooperative, federated search scenario [4, 20]. In future work we plan to investigate methods to gather the statistics required by Taily in a federal search scenario, to evaluate whether its performance also applies to this setting.
Acknowledgments
The work reported in this paper was funded by the EU Project AXES (FP7-269980), the Netherlands Organisation for Scientifc Research (NWO) under project 639.022.809, the University of Twente in The Netherlands, Ghent University in Belgium, and iMinds (Interdisciplinary institute for Technology), a research institute founded by the Flemish Government. This work is part of the programme of BiG Grid, the Dutch e-Science Grid, which is financially supported by the Nederlandse Organisatie voor Wetenschappelijk Onderzoek (Netherlands Organisation for Scientific Research, NWO). The authors want to thank the Big Grid team for their outstanding technical support, as well as Jamie Callan and the anonymous reviewers for their fruitful input.
Bibliography
[1] A. Arampatzis and S. E. Robertson. Modeling score distributions in information retrieval. Information Retrieval, 14:1­21, 2010. doi: 10.1007/s10791-010-9145-5.
[2] A. Arampatzis, N. Nussbaum, and J. Kamps. Where to stop reading a ranked list? In TREC'08, 2008.

[3] J. Arguello, J. Callan, and F. Diaz. Classification-based resource selection. In CIKM'09, pages 1277­1286. ACM, 2009. doi: 10.1145/1645953.1646115.
[4] R. Baeza-Yates, C. Castillo, F. Junqueira, V. Plachouras, and F. Silvestri. Challenges on distributed web retrieval. In ICDE'07, pages 6­20, 2007. doi: 10.1109/ICDE.2007.367846.
[5] R. Baeza-Yates, V. Murdock, and C. Hauff. Efficiency trade-offs in two-tier web search systems. In SIGIR'09, pages 163­170. ACM, 2009. doi: 10.1145/1571941.1571971.
[6] J. P. Callan, Z. Lu, and W. B. Croft. Searching distributed collections with inference networks. In SIGIR '95, pages 21­28. ACM, 1995. doi: 10.1145/215206.215328.
[7] W. S. Cooper. Some inconsistencies and misidentified modeling assumptions in probabilistic information retrieval. ACM Trans. Inf. Syst., 13(1):100­111, 1995. doi: 10.1145/195705.195735.
[8] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. CoRR, abs/1004.5168, 2010.
[9] S. Ganguly, W. Hasan, and R. Krishnamurthy. Query optimization for parallel execution. In SIGMOD'92, pages 9­18, NY, USA, 1992. ACM. doi: 10.1145/130283.130291.
[10] L. Gravano and H. Garcia-Molina. Generalizing gloss to vector-space databases and broker hierarchies. In VLDB'95, pages 78­89. Morgan Kaufmann Publishers Inc., 1995. ISBN 1-55860-379-4.
[11] D. Hiemstra and C. Hauff. Mapreduce for information retrieval evaluation: "let's quickly test this on 12 tb of data". In Multilingual and Multimodal Information Access Evaluation. Springer Verlag, 2010.
[12] E. Kanoulas, K. Dai, V. Pavlu, and J. A. Aslam. Score distribution models: assumptions, intuition, and robustness to score manipulation. In SIGIR'10, pages 242­249. ACM, 2010. doi: 10.1145/1835449.1835491.
[13] A. Kulkarni and J. Callan. Document allocation policies for selective searching of distributed indexes. In CIKM '10, pages 449­458. ACM, 2010. doi: 10.1145/1871437.1871497.
[14] A. Kulkarni, A. S. Tigelaar, D. Hiemstra, and J. Callan. Shard ranking and cutoff estimation for topically partitioned collections. In CIKM'12. ACM, 2012. doi: 10.1007/s10791-006-9014-4.
[15] I. Markov. Modeling document scores for distributed information retrieval. In SIGIR'11, pages 1321­1322. ACM, 2011. doi: 10.1145/2009916.2010180.
[16] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In SIGIR'05, pages 472­479. ACM, 2005. doi: 10.1145/1076034.1076115.
[17] D. Metzler, T. Strohman, H. Turtle, and W. Croft. Indri at trec 2004: Terabyte track. In TREC 2004, 2004.
[18] A. Moffat, W. Webber, J. Zobel, and R. Baeza-Yates. A pipelined architecture for distributed text query evaluation. In Information Retrieval, volume 10, pages 205­231. Kluwer Academic Publishers, 2007. doi: 10.1007/s10791-006-9014-4.
[19] D. Nguyen and J. Callan. Combination of evidence for effective web search. In TREC 2010, 2010.
[20] M. Shokouhi and L. Si. Federated search, volume 5. Now Publishers Inc, 2011. doi: 10.1561/1500000010.
[21] L. Si and J. Callan. Relevant document distribution estimation method for resource selection. In SIGIR'03, pages 298­305. ACM, 2003. doi: 10.1145/860435.860490.
[22] P. Thomas and M. Shokouhi. Sushi: scoring scaled samples for server selection. In SIGIR'09, pages 419­426. ACM, 2009. doi: 10.1145/1571941.1572014.
[23] J. Xu and W. Croft. Cluster-based language models for distributed retrieval. In SIGIR'99, pages 254­261. ACM, 1999.

682

A Mutual Information-based Framework for the Analysis of Information Retrieval Systems

Peter B. Golbus Javed A. Aslam
College of Computer and Information Science Northeastern University Boston, MA, USA
{pgolbus,jaa}@ccs.neu.edu

ABSTRACT
We consider the problem of information retrieval evaluation and the methods and metrics used for such evaluations. We propose a probabilistic framework for evaluation which we use to develop new information-theoretic evaluation metrics. We demonstrate that these new metrics are powerful and generalizable, enabling evaluations heretofore not possible.
We introduce four preliminary uses of our framework: (1) a measure of conditional rank correlation, information  , a powerful meta-evaluation tool whose use we demonstrate on understanding novelty and diversity evaluation; (2) a new evaluation measure, relevance information correlation, which is correlated with traditional evaluation measures and can be used to (3) evaluate a collection of systems simultaneously, which provides a natural upper bound on metasearch performance; and (4) a measure of the similarity between rankers on judged documents, information difference, which allows us to determine whether systems with similar performance are in fact different.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation (efficiency and effectiveness)
General Terms
Experimentation; Theory; Measurement
Keywords
Information Retrieval, Search Evaluation
1. INTRODUCTION
In order to improve search engines, it is necessary to accurately measure their current performance. If we cannot measure performance, how can we know whether a change
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2034-4/13/07 ...$15.00.

was beneficial? In recent years, much of the work on information retrieval evaluation has focused on user models [7, 18] and diversity measures [1, 10, 24] which attempt to accurately reflect the experience of the user of a modern internet search engine. However, these measure are not easily generalized. In this work, we introduce a probabilistic framework for evaluation that encompasses and generalizes current evaluation methods. Our probabilistic framework allows us to view evaluation using the tools of information theory [11]. While our framework is not designed to coincide with user experience, it provides immediate access to a large number of powerful tools allowing for a deeper understanding of the performance of search engines.
Our framework for evaluation is based on the observation that relevance judgments can also be interpreted as a preference between those documents with different relevance grades. This implies that relevance judgments can be treated as a retrieval system, and that evaluation can be considered as the "rank" correlation between systems and relevance judgments. To this end, we develop a probabilistic framework for rank correlation based on the expectation of random variables, which we demonstrate can also be used to compute existing evaluation metrics. However, the true value of our framework lies in its extension to new information-theoretic evaluation tools.
After a discussion of related work (Section 2), we introduce our framework in Section 3. In Section 4, we demonstrate that our framework allows for an information theoretic understanding of Kendall's  [17], information  , which we use to define a conditional version of the rank correlation between two lists conditioned on a third. In Section 5, we define a new evaluation measure based on our framework: relevance information correlation. We validate our measure by showing that it is highly correlated with existing measures such as average precision (AP) and normalized discounted cumulative gain (nDCG). As a demonstration of the versatility of our framework when compared to, for example, user models, we show that our measure can be used to evaluate a collection of systems simultaneously (Section 6), creating an upper bound on the performance of metasearch algorithms. Finally, in Section 7, we introduce information difference, a powerful new tool for evaluating the similarity of retrieval systems beyond simply comparing their performance.
This material is based upon work supported by the National Science Foundation under Grant No. IIS-1256172. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation (NSF).

683

2. RELATED WORK
Search systems are typically evaluated against test collections which consist of a corpus of documents, a set of topics, and relevance assessments--whether a subset of those documents are relevant with respect to each topic.1 For example, the annual, NIST-sponsored Text REtrieval Conference (TREC) creates test collections commonly used in academic research. The performance of systems is assessed with regards to a specific task. A traditional search task is to attempt to rank all relevant documents above any nonrelevant documents. For this task, systems are evaluated in terms of the average trade-off between their precision and recall with respect to multiple topics. For a given topic, Let gi  {0, 1} be the relevance grade of the document at rank i, and let R be the number of relevant documents in the collection. At rank k,

k

gi

precision@k = i=1

(1)

k

k

gi

recall@k = i=1

(2)

R

The trade-off between the two is measured by average precision, which can be interpreted as the area under the precisionrecall curve.



gi × precision@i

AP = i=1

(3)

R

Average precision does not include information about document quality and degrees of relevance, and is an inherently recall-oriented measure. It is therefore not suitable for evaluating commercial web search engines.
With the growth of the World Wide Web, test collections began to include graded, non-binary relevance judgments, e.g. G = {non-relevant, relevant, highly relevant} or G = {0, . . . , 4}. To make use of these graded assessments, J¨arvelin and Kek¨al¨ainen developed normalized discounted cumulative gain (nDCG) [15]. nDCG also has the advantage that it can be evaluated at arbitrary ranks, and can therefore be used for precision-oriented tasks like web search.
Unlike average precision, which has a technical interpretation, nDCG can be best understood in terms of a model of a hypothetical user. In this model, a user will read the first k documents in a ranked list, deriving utility from each document. The amount of utility is proportional to the document's relevance grade and inversely proportional to the rank at which the document is encountered. We first define discounted cumulative gain (DCG).

k 2gi - 1

DCG@k =

(4)

i=1 log2(i + 1)

Since the range of DCG will vary from topic to topic, it is necessary to normalize these scores so that an average can

1For historical reasons, the set of relevance assessments is often referred to as a QREL.

be computed. Normalization is performed with regard to an ideal ranked list. If DCG @k is the maximum possible DCG of any ranked list of documents in the collection then

nDCG@k = DCG@k

(5)

DCG @k

However, one does not always know how many documents are relevant at each level, and therefore the ideal list used for normalization is only an approximation. Moffat and Zobel [18] introduced a measure, rank-biased precision (RBP), that addresses this issue. In RBP, the probability that a user will read the document at rank k is drawn from a geometric distribution, whose parameter,   [0, 1), models the user's persistence. Given a utility function u : G  [0, 1], commonly defined as

2g - 1

u(g) = 2d

(6)

where d is the maximum possible relevance grade, RBP is defined as the expected utility of a user who browses according to this model.



RBP = (1 - ) u(gi) × i-1

(7)

i=1

Since RBP is guaranteed to be in the range [0,1) for any topic and , it does not require normalization.
Craswell et al. [12] introduced the Cascade model of user behavior. In this model, a user is still assumed to browse documents in order, but the probability that a user will view a particular document is no longer assumed to be independent of the documents that were viewed previously, i.e. a user is not assumed to stop at a particular rank, or at each rank with some probability. Instead, the user is assumed to stop after finding a relevant document. This implies that if a user reaches rank k, then all of the k - 1 documents ranked before it were non-relevant. Craswell et al. demonstrated empirically that this model corresponds well to observed user behavior in terms of predicting the clickthrough data of a commercial search engine.
Chapelle et al. [7] developed an evaluation measure, expected reciprocal rank (ERR), based on the Cascade model. Let Ri denote the probability that a user will find the document at rank i to be relevant. Then in the Cascade model, the likelihood that a user will terminate his or her search at rank r is

r-1

Rr (1 - Ri).

(8)

i=1

If we interpret the previously defined utility function (Equation 6) as the probability that a user will find a document relevant, i.e. Ri = u(gi), then we can computed the expected reciprocal rank at which a user will terminate his or her search as

 1 r-1

ERR = r Rr (1 - Ri).

(9)

r=1

i=1

In this work, we propose an alternative, information-theoretic framework for evaluation. The first step is to reformulate these measures as the expected outcomes of random experiments. Computing evaluation measures in expectation is

684

not uncommon in the literature, and we are not the first to suggest that reformulating an evaluation measure as an expectation allows for novel applications. For example, Yilmaz and Aslam [30] formulated average precision as the expectation of the following random experiment:
1. Pick a random relevant document,
2. Pick a random document ranked at or above the rank of the document selected in step 1.
3. Output 1 if the document from step 2 is relevant, otherwise output 0.
Their intention was to accurately estimate average precision while collecting fewer relevance judgments (a process also applied to nDCG [32]). However, this formulation led to new uses, such as defining an information retrieval-specific rank correlation measure, AP [31], and a variation of average precision for graded relevance judgments, Graded Average Precision (GAP) [21].
Our work uses pairwise document preferences rather than absolute relevance judgments. The use of preferences is somewhat common in IR. For example, many learning-torank algorithms, such as LambdaMart [3] and RankBoost [13], use pairwise document preferences in their objective functions. Carterette et al. [4, 5] explored the collection of preference judgments for evaluation, showing that they are faster to collect and have lower levels of inter-assessor disagreement. More recently, Chandar and Carterette [6] crowdsourced the collection of conditional document preferences to evaluate the standard assumptions underlying diversity evaluation, for example that users always prefer novel documents. Relative document preferences can also be inferred from the clickthrough data collected in the logs of commercial search engines [16]. These preferences can be used for evaluation without undertaking the expense of collecting relevance judgments from assessors.
3. A PROBABILISTIC FRAMEWORK FOR EVALUATION
Mathematically, one can view the search system as providing a total ordering of the documents ranked and a partial ordering of the entire collection, where all ranked documents are preferred to unranked documents but the relative preference among the unranked documents is unknown. Similarly, one can view the relevance assessments as providing a partial ordering of the entire collection: in the case of binary relevance assessments, for example, all judged relevant documents are preferred to all judged non-relevant and unjudged documents, but the relative preferences among the relevant documents and among the non-relevant and unjudged documents is unknown. Thus, mathematically, one can view retrieval evaluation as comparing the partial ordering of the collection induced by the search system with the partial ordering of the collection induced by the relevance assessments.
To formalize and instantiate a framework for comparing such partial orderings, consider the simplest case where we have two total orderings of objects, i.e., where the entire "collection" of objects is fully ranked in both "orderings." While such a situation does not typically arise in search system evaluation (since not all documents are ranked by the retrieval system nor are they fully ranked by relevance assessments), it does often arise when comparing the rankings

of systems induced by two (or more) evaluation metrics; here

Kendall's  is often the metric used to compare these (total

order) rankings.

In what follows, we define a probabilistic framework within

which to compare two total orderings, and we show how tra-

ditional metrics (such as Kendall's  ) are easily cast within

this framework. The real power of such a framework is

shown in subsequent sections: (1) the framework can be

easily generalized to handle the comparison of two partial

orderings, such as arise in search system evaluation, and

(2) well-studied, powerful, and general information-theoretic

metrics can be developed within this generalized framework.

Consider two total orderings of n objects. There are

n 2

(unordered) pairs of such objects, and a pair is said to be

concordant if the two orderings agree on the relative rankings

of the objects and discordant if the two orderings disagree.

Let c and d be the number of concordant and discordant

pairs, respectively. Then Kendall's  is defined as follows:

c-d

=

.

(10)

c+d

If we let C and D denote the fraction of concordant and discordant pairs then Kendall's  is defined as

 = C - D.

(11)

Note that c + d =

n 2

if there are ties.2

To define a probabilistic framework, we must specify three

things: (1) a sample space of objects, (2) a distribution over

this sample space, and (3) random variables over this sample

space. Let our sample space  be all possible 2 ·

n 2

ordered

pairs of distinct objects, and consider a uniform distribution

over this sample space. For a given ranking R, define a

random variable XR :   {-1, +1} that outputs +1 for

any ordered pair concordant with R and -1 for any ordered

pair discordant with R.

XR [(di, dj )] =

1 if di appears before dj in R. -1 otherwise.

(12)

We thus have a well-defined random experiment: draw an ordered pair of objects at random and output +1 if that ordered pair agrees with R's ranking and -1 otherwise. Since all ordered pairs of objects are considered uniformly, the expected value E[XR] of this random variable is zero.
Given a second ranked list S, one can similarly define an associated random variable XS. Now consider the random experiment of multiplying the two random variables: the product XR · XS will be +1 precisely when the pair is concordant--i.e. both lists agree that the ordering of the objects is correct (+1) or incorrect (-1), and the product will be -1 when the pair is discordant--i.e. the lists disagree. In this probabilistic framework, Kendall's  is the expected value
2Kendall defined two means by which  can account for ties, depending on the desired behavior. Imagine comparing two ranked lists, one of which is almost completely composed of ties. A, defined above, approaches 1. B includes the number of ties in the denominator, and therefore approaches 0. We believe that the former approach is appropriate in this context. Since QRELs are almost exclusively composed of ties (recall that all pairs of unjudged documents in the corpus are considered to be tied), using the latter would mean that effect of the relatively rare meaningful comparisons would be negligible.

685

of the product of these random variables:

 = E[XR · XS].

(13)

The real power of this framework is in the definition of these random variables: (1) the ability to generalize them to compare partial orderings as arise in system evaluation, and (2) the ability to measure the correlation of these random variables using information-theoretic techniques.

4. INFORMATION-THEORETIC RANK CORRELATION
In Section 3, we defined Kendall's  as the expected product of random variables. The following theorem allows us to restate Kendall's  equivalently as the mutual information between the random variables.

Theorem

1.

I(XR; XS) =

1+ 2

log(1+

)+

1- 2

log(1- ).

(For a proof of Theorem 1, see Appendix). Unlike Kendall's  , the mutual information between ranked lists ranges from 0 on lists that are completely uncorrelated to 1 on lists that are either perfectly correlated or perfectly anti-correlated.
If we restrict our attention to pairs of lists that are not anti-correlated, then the relationship is bijective. Given this fact, we define a variant of Kendall's  , information  :

I (R, S) = I(XR; XS)

(14)

where XR is the ranked list random variable defined in Equation 12 observed with respect to the uniform probability distribution over all pairs of distinct objects. By reframing Kendall's  equivalently in terms of mutual information, we immediately gain access to a large number of powerful theoretical tools. For example, we can define a conditional information  between two lists given a third. For lists R and S given T ,

I (R, S | T ) = I(XR; XS | XT ).

(15)

Kendall's  can tell you whether two sets of rankings are similar, but it cannot tell you why. Information  can be used as a meta-evaluation tool to find the underlying cause of correlation between measures. We demonstrate the use of information  as a meta-evaluation tool by using it to analyze measures of the diversity of information retrieval systems. In recent years, several diversity measures (e.g. [1, 10, 24]) have been introduced to evaluate how well systems perform in response to ambiguous or underspecified queries that have multiple interpretations. These measures conflate several factors [14], including: a diversity model that rewards novelty and penalizes redundancy, and a measure of ad hoc performance that rewards systems for retrieving highly relevant documents. We wish to know not only whether two diversity measures are correlated, but also the similarity between their component diversity models. Using Kendall's  , we can observe whether the rankings of systems by each measure are correlated. But even if they are correlated, this could still be for one of two reasons: either both the diversity and the performance components evaluate systems similarly; or else one of the components is similar, and its effect on evaluation is dominant. However, if the measures are correlated when conditioned on their underlying performance components, then this must be due to similarities in their models of diversity.

Figure 1: Per-query information  (conditional rank correlation) between the TREC and NTCIR gold standard diversity measures conditioned on their underlying performance measures.
We measured this effect on the the TREC 2011 and 2012 Web collections [8, 9]. Note that the performance measures are evaluated using graded relevance, while the diversity measures use binary judgments for each subtopic. All evaluations are performed at rank 20. Figure 1 shows the rank correlation between ERR-IA and D#-nDCG, the primary measures reported by TREC and NTCIR [26], when conditioned on their underlying performance models. Each query is computed separately, with each datapoint in the figure corresponding to a different query. Table 1 shows the results of conditioning additional pairs of diversity measures (now averaged over queries in the usual way) on their performance models. The results in Figure 1 are typical of all pairs of measures on a per-query basis.
Our results confirm that while diversity measures are very highly correlated, most of this correlation disappears when one conditions on the underlying performance model. This indicates that most of the correlation is due to the similarity between the performance components and not the diversity components. For example, in TREC 2010, ERR-IA and -nDCG have an information  of almost 0.9. However, when conditioned on ERR, the similarity falls to only 0.25. This means that while these two measures are mostly ranking systems for the same reason, that reason is simply ERR. However, of the 0.9 bits that are the same, 0.25 are due to some factor other than ERR. This other factor must presumably be the similarity in their diversity models.

686

I (ERR-IA ; -nDCG) I (ERR-IA ; -nDCG | nDCG) I (ERR-IA ; -nDCG | ERR) I (ERR-IA ; -nDCG | nDCG, ERR) I (ERR-IA ; D#-nDCG) I (ERR-IA ; D#-nDCG | nDCG) I (ERR-IA ; D#-nDCG | ERR) I (ERR-IA ; D#-nDCG | nDCG, ERR)

TREC 2010 0.8290 0.4860 0.2499 0.2451 0.6390 0.3026 0.1222 0.1239

TREC 2011 0.8375 0.4434 0.3263 0.2805 0.5545 0.1728 0.1442 0.1003

Table 1: TREC 2010 and 2011 information  (conditional rank correlation) between diversity measures conditioned on ad hoc performance measures.

5. EVALUATION MEASURE
In this section, we demonstrate an extension of our probabilistic framework for evaluation to measuring the correlation between a system and the incomplete ranking generated by a set of relevance judgments. This allows us to define an information-theoretic evaluation measure, relevance information correlation. While our measure has novel applications, we will demonstrate that the evaluations produced are consistent with those of existing measures.
To compute mutual information, we must define a sample space, a probability distribution, and random variables. Let the sample space,  = {(di, dj)}, be the set of all ordered pairs of judged documents. This means that we are ignoring unjudged documents, rather than considering them non-relevant. This is equivalent to computing an evaluation measure on the condensed list [23] created by removing all non-judged documents from the list. We define the probability distribution in terms of the QREL to ensure that all ranked lists will be evaluated using the same random experiment. Let P = U |I(gi=gj), where gi represents the relevance grade of document di, be the uniform probability distribution over all pairs of documents whose relevance grades are not equal. We define a QREL variable Q over ordered pairs of documents as

Q [(di, dj )] =

1 0

if gi > gj otherwise.

(16)

Note that this definition can be applied to both graded and binary relevance judgments.
We now turn our attention to defining a ranked list random variable over ordered pairs of documents (di, dj). If both document di and dj appear in the ranked list, than our output can simply indicate whether di was ranked above dj. If document di appears in the ranked list and dj does not, then we will consider di as having been ranked above dj, and vice versa. If neither di nor dj is ranked, we will output a null value. If we were to instead restrict our attention only to judged document pairs where at least one document is ranked, then a ranked list consisting of a single relevant document followed by some number of non-relevant documents would have perfect mutual information with the QREL--all of the ranked relevant documents appear before all of the ranked non-relevant documents. However, this system must be penalized for preferring all of the ranked non-relevant documents to all of the unranked relevant documents. If we instead use a null value, our example ranked

list would almost always output null. This behavior would

be independent of the QREL, meaning the two variables will

have almost no mutual information. In effect, the null value

creates a recall component for our evaluation measure; no

system can have a large mutual information with the QREL

unless it retrieves most of the relevant documents.

Another problem we must consider is that mutual infor-

mation is maximized when two variables are completely cor-

related or completely anti-correlated. Consider an example

ranked list consisting of a few non-relevant documents fol-

lowed by several relevant documents and then many more

non-relevant documents. Since this example ranked list will

disagree with the QREL on almost all document pairs, its

random variable will have a very high mutual information

with the QREL variable. The system is effectively being

rewarded for finding the subset of non-relevant documents

that happen to be present in the QREL. To address this,

we truncate the list at the last retrieved relevant document

prior to evaluation.

Let ri represent the rank of document di in the list S. Then the ranked list variable RS is defined as

 1  RS [(di, dj )] = 0
-1

if ri < rj if neither di nor djwere retrieved otherwise.

(17)

We define our new measure, Relevance Information Correlation, as the mutual information between the QREL variable Q and the truncated ranked list variable R

RIC(System) = I(RSystem; Q).

(18)

RIC is computed separately for each query, and then averaged, as with mean average precision.
In order to compute RIC we must estimate the joint probability distribution of document preferences over Q and R. This could be done in various ways. In this work, we use the maximum likelihood estimate computed separately for each query. Since the MLE requires a large number of observations, RIC is only accurate for recall-oriented evaluation. In future work, we intend to explore other means of estimating P (Q, R) that will allow RIC to be used for precision-oriented evaluation as well.
We also note that RIC has no explicit rank component, and would therefore seem to treat all relevant documents equally independent of the rank at which they were observed. However, there is an implicit rank component in that a relevant document that is not retrieved early in the list must be incorrectly ranked below many non-relevant documents. This argument is similar in spirit to Bpref [2].
Our measure is quite novel in its formulation, and makes many non-standard assumptions about information retrieval evaluation. Therefore it is necessary to validate experimentally that our measure prefers the same retrieval systems as existing measures. Note that for two evaluation measures to be considered compatible, it is sufficient that they rank systems in the same relative order; it is not necessary that they always assign systems similar absolute scores. For example, a system's nDCG is often higher than its average precision.
To show that RIC is consistent with AP and nDCG, we computed the RIC, AP, and nDCG of all systems submitted to TRECs 8 and 9. Figure 2 shows the output of RIC plotted against AP (top) and nDCG (bottom) on TRECs 8 (left) and 9 (right) [28, 29]. TREC 8 uses binary relevance

687

Figure 2: Correlation between RIC and AP (top) and nDCG (bottom). TREC 8 (left) uses binary relevance judgments. TREC 9 (right) uses graded relevance judgments.

(G)AP nDCG
MI

TREC 8 0.716 0.713 0.719

TREC9 0.648 0.757 0.744

Table 2: Discriminative power of (graded) AP and nDCG vs. RIC

judgments. TREC 9 uses graded relevance judgments, requiring the use of graded average precision. Inset into each plot is the output of the measures on the top ten systems. For each experiment, we report the Kendall's  and Spearman's  [27] rank correlations for all systems, and for the top ten systems. With Kendall's  values of at least 0.799 on all systems and 0.644 on top ten systems, the ranking of systems by RIC is still highly correlated with those of both AP and nDCG. However, RIC is not as highly correlated with either AP or nDCG as AP and nDCG are with each other. Note that the correlation between RIC and GAP on TREC 9 is highly monotonic, even if is not particularly linear. This implies that the two measures do rank systems in a consistent relative order, even if RIC is a biased estimator of GAP.
To further validate our measure, we also compute the discriminative power [22] of the various measures. Discriminative power is a widely used tool for evaluating a measure's sensitivity i.e. how often differences between systems can be

detected with high confidence. A high sensitivity can be seen as a necessary, though not sufficient, condition for a good evaluation measure. Discriminative power is defined as the percentage of pairs of runs that are found to be statistically significantly different by some significance test. As per Sakai, we use a two-tailed paired bootstrap test with 1000 bootstrap samples per pair of systems. Our results are displayed in Table 2. As measured by discriminatory power, we see that RIC is at least as sensitive, if not more so, than AP and nDCG.

6. UPPER BOUND ON METASEARCH
In Section 5, we defined an evaluation measure in terms of mutual information. One advantage of this approach is that collections of systems can be evaluated directly by considering the output of their random variables jointly, without their needing to be combined. For a collection of systems, denoted S1 through Sn, the relevance information correlation can be defined as

RIC(S1, . . . , Sn) = I(RS1 , . . . , RSn ; Q)

(19)

In this section, we will show that this produces a natural upper bound on metasearch performance that is consistent with other upper bounds appearing in the literature.
We compare our upper bound against those of Montague [19]. Montague describes metasearch algorithms as sorting functions whose comparators, as well as the documents to be sorted, are defined in terms of collections of input systems.

688

By also using the QREL as input, these algorithms can estimate upper bounds on metasearch performance. These bounds range from the ideal performance that cannot possibly be exceeded by any metasearch algorithm, to descriptions of reasonable metasearch behavior that should be similar to the performance of any quality metasearch algorithm.
Montague defines the following upper bounds on metasearch:
1. Naive: Documents are sorted by comparison of relevance judgments, i.e. the naive upper bound is created by returning all relevant documents returned by any system in the collection above any non-relevant document. Relevant documents not retrieved by any system are not ranked.
2. Pareto: If document A is ranked above document B by all systems, then document A is considered "greater" than document B. Otherwise, the documents are sorted by comparison of relevance judgments.
3. Majoritarian: If document A is ranked above document B by at least half of the systems, then document A is considered "greater" than document B. Otherwise, the documents are sorted by comparison of relevance judgments.
We will compare our direct joint evaluation with these upper bounds, and several metasearch algorithms commonly used as baselines in the IR literature: the CondorcetFuse metasearch algorithm [20], and the comb family of metasearch algorithms [25].

TREC 8 ANZ MNZ
Condorcet Majoritarian
Pareto Naive

 0.221 0.587 0.519 0.552 0.657 0.788

 0.330 0.764 0.689 0.735 0.836 0.931

RMSE 0.481 0.351 0.362 0.340 0.044 0.039

Table 3: Correlation between joint distribution and metasearch algorithms (Kendall's  , Spearman's , root mean square error).

perform comparably to the Majoritarian bound, and the Naive bound is not appreciably better than the Pareto bound. If direct evaluation and the Naive bound are both reasonable estimates of the actual upper bound, then these results should be confirmed by Figure 3 and Table 3, as indeed they are. Note that there is almost no correlation between the joint evalution and the weakest metasearch algorithm, combANZ: combANZ does not approximate the upper bound on metasearch. The correlation improves as the quality of the metasearch algorithm improves, and it does so in a manner consistent with Montague. The correlations between the joint evaluation and the output of combMNZ, CondorcetFuse, and the Majoritarian bound are similar; while they are still biased as estimators, the correlation is beginning to approach monotonicity. Finally, with a root mean square error of 0.039, the joint evaluation estimation of the upper bound is essentially identical to that of the Naive upper bound. If the Naive upper bound is a reasonable estimate of the upper bound on metasearch performance, then so is the joint evaluation of the input systems.

Figure 3: RIC of systems output by metasearch algorithms (Fusion System) versus RIC of systems computed directly (S1, . . . , S10) without combining.
We examined the direct evaluation and metasearch performance of collections of ten randomly selected systems. Experiments were performed on TREC 8 and 9, with both binary and graded relevance judgments. To conserve space, we only show the results from TREC 8. The results from TREC 9 were highly similar, both when using binary and graded relevance judgments.
Figure 3 shows the RIC of the system output by a metasearch algorithm plotted against the joint RIC of the input systems, and Table 3 shows various measures of their correlation. Montague found that combANZ is inferior to CondorcetFuse and combMNZ, CondorcetFuse and combMNZ

7. INFORMATION DIFFERENCE
In this section, we introduce a novel application of our probabilistic framework. Imagine that you are attempting to improve an existing ranker. On what basis do you decide whether or not your changes are beneficial? One typically evaluates both systems on a number of queries, and measures the difference in average performance. If one system outperforms the other, whether you have made an improvement is clear. But what happens when the systems perform similarly? It could be that your new system is essentially unchanged from your old system, but it is also possible that the two systems chose highly different document and just happened to have very similar evaluation scores. In the latter case, it may be possible to create a new, better system based on a combination of the two existing systems.
We propose to measure the magnitude of the difference between systems in their ranking of documents for which we have relevance information, rather than the magnitude of the difference between their performance. We denote this new quantity as the information difference between systems. Our definition of information difference is inspired by the Boolean Algebra symmetric difference operator as applied to information space (see Figure 4).

id(S1, S2) = I(S1; Q | S2) + I(S2; Q | S1)

(20)

689

System 1

System 2

QREL
Figure 4: Information difference corresponds to the symmetric difference between the intersections of the systems with the QREL in information space (red portion of the Venn diagram).
As a preliminary validation of information difference, we analyzed the change in AP and information difference between pairs of systems submitted to TREC 8, selected at random. We expect the two to be somewhat directly correlated, since, in general, if two systems rank documents similarly, we would expect them to have similar AP. However, we expect that they will not be highly correlated, since we believe that information difference is much more informative. Our intuition is supported by Figure 5, which shows the magnitude of the change in AP on the horizontal axis, and the information difference on the vertical axis.

Figure 5: Scatter plot of information difference and the magnitude of change in AP of random pairs of TREC 8 systems.
To demonstrate the utility of information difference, we sorted all the systems submitted to TREC 8 by AP and separated them into twenty equal-sized bins. By construction, each bin contained systems with small differences in performance. Our goal is to distinguish between similar and dissimilar systems within each bin. To this end, all systems within each bin were compared with one other (see Table 4). When the system pairs were sorted by their information difference, both systems in the first 27 pairs were submitted by the same group, whereas sorting by | AP| produced no discernible pattern. It is reasonable to assume that these systems were different instantiations of the same underlying

technology. We can therefore conclude that information difference is able to determine whether systems with the same underlying performance are in fact similar, as desired.

Rank
1 2 3 4 5
28 29 30 31 32

System 1
UB99T unc8al32 fub99tt nttd8al ibmg99a
isa25t CL99SD ok8amxc tno8d4 uwmt8a2

System 2
UB99SW unc8al42 fub99tf nttd8alx ibmg99b
...
cirtrc82 CL99SDopt2 ok8alx MITSLStd uwmt8a1

id
0.010 0.012 0.017 0.023 0.027
0.084 0.086 0.086 0.088 0.089

| AP|
0.005 0.002 0.000 0.002 0.012
0.004 0.000 0.006 0.016 0.002

Table 4: The systems from TREC 8 were binned by average precision. Information difference and  AP were computed for all system pairs within each bin. Sorting by information difference, both systems in the first 27 pairs were submitted by the same group.

8. CONCLUSION
In this work, we developed a probabilistic framework for the analysis of information retrieval systems based on the correlation between a ranked list and the preferences induced by relevance judgments. Using this framework, we developed powerful information theoretic tools for better understanding information retrieval systems. We introduced four preliminary uses of our framework: (1) a measure of conditional rank correlation, information  , which is a powerful meta-evaluation tool whose use we demonstrated on understanding novelty and diversity evalution; (2) a new evaluation measure, relevance information correlation, which is correlated with traditional evaluation measures and can be used to (3) evaluate a collection of systems simultaneously, which provides a natural upper bound on metasearch performance; and (4) a measure of the similarity between rankers on judged documents, information difference, which allows us to determine whether systems with similar performance are actually different.
Our framework is based on the choice of sample space, probability distribution, and random variables. Throughout this work, we only used a uniform distribution on appropriate pairs of documents. However, not all document pairs are equal. The use of additional distributions is an immediate avenue for improvement that we intend to explore in future work. For example, a geometric distribution may be employed to force our evaluation tools to concentrate their attention at the top of a ranked list.
The primary limitation of our evaluation measure as implemented in this work is that it is only applicable to recalloriented retrieval tasks. In future work, we intend to develop a precision-oriented version that is applicable to web search. Given such a measure, judgments can be combined in the way systems were in our upper bound on metasearch. In that way, a small number of expensive to produce nominal relevance judgments, a somewhat larger number of somewhat less expensive preference judgments, and a gold-stander ranker could all be used simultaneously to evaluate systems.

690

Finally, we intend to explore the application of information difference to the understanding of information retrieval models. For example, BM25 and Language Models have long been used as baselines in information retrieval experiments. On the surface, these two models appear to be completely different. And yet, the two share deep theoretical connections [33]. Using information difference, we can determine whether their theoretical similarities outweigh their superficial differences in terms of how they rank documents.
9. REFERENCES
[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. Diversifying search results. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, WSDM '09, pages 5­14, New York, NY, USA, 2009. ACM.
[2] Chris Buckley and Ellen M. Voorhees. Retrieval evaluation with incomplete information. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '04, 2004.
[3] Christopher J.C. Burges. From ranknet to lambdarank to lambdamart: An overview. Technical Report MSR-TR-2010-82, Microsoft Research, 2010.
[4] Ben Carterette and Paul N. Bennett. Evaluation measures for preference judgments. In SIGIR, 2008.
[5] Ben Carterette, Paul N. Bennett, David Maxwell Chickering, and Susan T. Dumais. Here or there: preference judgments for relevance. In Proceedings of the IR research, 30th European conference on Advances in information retrieval, ECIR'08, 2008.
[6] Praveen Chandar and Ben Carterette. Using preference judgments for novel document retrieval. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, SIGIR '12, 2012.
[7] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM '09, pages 621­630, New York, NY, USA, 2009. ACM.
[8] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and Gordon V. Cormack. Overview of the TREC 2010 Web Track. In 19th Text REtrieval Conference, Gaithersburg, Maryland, 2010.
[9] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and Ellen M. Voorhees. Overview of the TREC 2011 Web Track. In 20th Text REtrieval Conference, Gaithersburg, Maryland, 2011.
[10] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Bu¨ttcher, and Ian MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '08, pages 659­666, New York, NY, USA, 2008. ACM.
[11] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley-Interscience, 1991.
[12] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. An experimental comparison of click position-bias models. In Proceedings of the 2008

International Conference on Web Search and Data Mining, WSDM '08, pages 87­94, New York, NY, USA, 2008. ACM.
[13] Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer. An efficient boosting algorithm for combining preferences. J. Mach. Learn. Res., 4, December 2003.
[14] Peter B. Golbus, Javed A. Aslam, and Charles L.A. Clarke. Increasing evaluation sensitivity to diversity. In Journal of Information Retrieval, To Appear.
[15] Kalervo J¨arvelin and Jaana Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4):422­446, October 2002.
[16] Thorsten Joachims. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '02, 2002.
[17] M. G. Kendall. A New Measure of Rank Correlation. Biometrika, 30(1/2):81­93, June 1938.
[18] Alistair Moffat and Justin Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):2:1­2:27, December 2008.
[19] Mark Montague. Metasearch: Data Fusion for Document Retrieval. PhD thesis, Dartmouth College. Dept. of Computer Science, 2002.
[20] Mark Montague and Javed A. Aslam. Condorcet fusion for improved retrieval. In Proceedings of the eleventh international conference on Information and knowledge management, CIKM '02, 2002.
[21] Stephen E. Robertson, Evangelos Kanoulas, and Emine Yilmaz. Extending average precision to graded relevance judgments. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, SIGIR '10, 2010.
[22] Tetsuya Sakai. Evaluating evaluation metrics based on the bootstrap. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '06, 2006.
[23] Tetsuya Sakai. Alternatives to Bpref. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, 2007.
[24] Tetsuya Sakai and Ruihua Song. Evaluating diversified search results using per-intent graded relevance. In SIGIR, pages 1043­1052, 2011.
[25] Joseph A. Shaw and Edward A. Fox. Combination of multiple searches. In The Second Text REtrieval Conference (TREC-2), pages 243­252, 1994.
[26] Ruihua Song, Min Zhang, Tetsuya Sakai, Makoto P. Kato, Yiqun Liu, Miho Sugimoto, Qinglei Wang, and Naoki Orii. Overview of the ntcir-9 intent task. In Proceedings of the 9th NTCIR Workshop, Tokyo, Japan, 2011.
[27] C. Spearman. The proof and measurement of association between two things. The American Journal of Psychology, 1904.
[28] E. M. Voorhees and D. Harman. Overview of the eighth text retrieval conference (TREC-8). In Proceedings of the Eighth Text REtrieval Conference (TREC-8), 2000.

691

[29] E. M. Voorhees and D. Harman. Overview of the ninth text retrieval conference (TREC-9). In Proceedings of the Ninth Text REtrieval Conference (TREC-9), 2001.
[30] Emine Yilmaz and Javed A. Aslam. Estimating average precision with incomplete and imperfect judgments. In Proceedings of the 15th ACM international conference on Information and knowledge management, CIKM '06, 2006.
[31] Emine Yilmaz, Javed A. Aslam, and Stephen Robertson. A new rank correlation coefficient for information retrieval. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '08, 2008.
[32] Emine Yilmaz, Evangelos Kanoulas, and Javed A. Aslam. A simple and efficient sampling method for estimating AP and nDCG. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '08, 2008.
[33] Chengxiang Zhai and John Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, April 2004.

10. APPENDIX

Theorem

1.

I(XR; XS) =

1+ 2

log(1+

)+

1- 2

log(1- ).

Proof. Denote XR and XS as X and Y . Consider the following joint probability distribution table.

Y -1 1 X -1 a b
1 cd

Observe that: a + b + c + d = 1; C = a + d, D = b + c, and therefore  = a + d - b - c; and since document pairs appear in both orders, a = d and b = c.
The joint probability distribution can be rewritten as follows.

Y

-1 1

X -1

C 2

D 2

1

DC

2

2

Observe that the marginal probability P (X) = P (Y ) =

C 2

+

D 2

,

C 2

+

D 2

=

1 2

,

1 2

.

I(X; Y ) = KL(P (X, Y )||P (X)P (Y ))

p(x, y)

=

p(x, y) lg

p(x)p(y)

x,y

1

=

p(x, y) lg p(x, y) + p(x, y) lg

.

p(x)p(y)

x,y

x,y

Since P (X, Y ) =

C 2

,

D 2

,

C 2

,

D 2

and P (X)P (Y ) =

1 4

,

1 4

,

1 4

,

1 4

,

I(X, Y ) = 2 · C lg C + 2 · D lg D + 2 · C lg 4 + 2 · D lg 4

22

22

2

2

= C lg C + D lg D + 2C + 2D

2

2

= C lg C - C + D lg D - D + 2C + 2D

= C lg C + D lg D + 1

= C lg C + (1 - C) lg(1 - C) + 1

Since C + D = 1 and  = C - D, we have that  = 2C - 1,

C=

1+ 2

and D = 1 - C =

1- 2

.

In terms of C, if H2 represents the entropy of a Bernoulli

random variable ,3

I(X; Y ) = -H2(C) + 1

1+ = -H2 2 + 1

1+ 1+ 1- 1-

=

lg

+

lg

+1

2

2

2

2

= 1 +  lg(1 +  ) - 1 +  + 1 -  lg(1 -  )

2

2

2

-1- +1 2

1+

1-

=

lg(1 +  ) +

lg(1 -  )

2

2

Corollary 1. For two ranked lists R and S, I(XR; XS) =

1 - H2(K) where K

=

1- 2

is the normalized Kendall's 

distance between R and S.

3H2(p) = -p lg p - (1 - p) lg(1 - p). Note that H2(p) = H2(1 - p).
692

A Candidate Filtering Mechanism for Fast Top-K Query Processing on Modern CPUs

Constantinos Dimopoulos, Sergey Nepomnyachiy, Torsten Suel
Computer Science & Eng. Polytechnic Institute of NYU
constantinos@cis.poly.edu, snepom01@students.poly.edu, suel@poly.edu

ABSTRACT

and the number of queries submitted by users. As discussed

A large amount of research has focused on faster methods for finding top-k results in large document collections, one of the main scalability challenges for web search engines. In this paper, we propose a method for accelerating such top-k queries that builds on and generalizes methods recently proposed by several groups of researchers based on Block-Max Indexes [15, 10, 13]. In particular, we describe a system that uses a new filtering mechanism, based on a combination of block maxima and bitmaps, that radically reduces the number of documents that have to be further evaluated. Our filtering mechanism exploits the SIMD processing capabilities of current microprocessors, and it is optimized through caching policies that select and store suitable filter structures based on properties of the query load. Our experimental evaluation shows that the mechanism results in very significant speed-ups for disjunctive top-k queries under several state-of-the-art algorithms, including a speed-up of more than a factor of 2 over the fastest previously known methods.

in [12], each of these measures increased by several orders of magnitude over the course of a decade. This creates performance challenges in many parts of the search engine architecture, including data acquisition, data analysis, and index maintenance. However, these challenges are particularly acute in the query processing component, whose workload grows with both data size and query load. In fact, one might naively expect query processing costs to increase with the product of these measures, though in practice various techniques are used to limit this increase. In this paper, we focus on such techniques for improving query processing efficiency.
Query processing in search engines is a fairly complex process, typically involving hundreds of features used for ranking, multiple phases that identify promising documents based on subsets of the features, and a distributed architecture that routes queries and results within and between clusters of thousands of machines. However, most systems appear to process a query by first evaluating on each machine a fairly simple ranking function over an inverted index. This results in an initial set of say a few thousand results

Categories and Subject Descriptors

that is then further processed to identify the best ten or more results to return to the user [26]. We focus on im-

H.3.3 [INFORMATION STORAGE AND RETRIEVAL]: proving this initial step, which is responsible for a significant

Information Search and Retrieval

fraction of the overall work. We assume that this initial step

General Terms

is a ranking function of the form r(q, D) = tq s(t, D), i.e., it is the sum of the scores of all the terms occurring in the
query. This is a fairly common assumption, and accommo-

Algorithms, Performance, Experimentation

dates popular functions such as BM25 as well as approaches

based on automatically learning term scores that approxi-

Keywords
top-k query processing, early termination, block-max inverted index, docID-oriented block-max index, candidate filtering mechanism, posting bitset, live area computation

mate more complex ranking functions [1]. A straightforward way to execute such a simple disjunctive
ranking function is to completely traverse the index structures of all the query terms and compute or retrieve the scores of all the postings. Under this exhaustive method,

1. INTRODUCTION

the cost of each query increases linearly with the number of documents, making it very expensive for large collections.

One of the major problems for large search engines is to keep up with the tremendous growth in the size of the web

To overcome this problem, many researchers have proposed so-called early-termination techniques, i.e., techniques for finding the top results without computing or retrieving all

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not

posting scores [25, 9, 6, 23, 15, 10]. We focus on safe earlytermination techniques, which must return the same top re-

made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland.

sults as the exhaustive method [22]. Content of this Paper: Recent work by several groups
of researchers [10, 15, 19, 13] has shown significant improvements in performance based on a Block-Max Index, first proposed in [10, 15]. This is an index structure that stores maximum scores for groups of postings, thus allowing quick

Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

723

skipping, without full evaluation, of sets of postings with low scores. In this paper, we build on this approach to derive new algorithms with additional very significant performance gains. In particular, our contributions are as follows:
(1) We design a general mechanism that can improve the performance of a whole class of query processing algorithms. Given a space budget, we maintain a set of block-max and posting-bitmap structures that can be used to transparently accelerate any DAAT index traversal algorithm. We show how to exploit the aggregated information from the auxiliary structures for aggressive candidate filtering.
(2) We present a space efficient caching policy for storing an optimal set of structures given a query distribution.
(3) We propose an optimized implementation of the mechanism that exploits SIMD instructions of modern CPUs and restricts the critical data structures to L1 cache.
(4) We provide an extensive experimental evaluation that shows significant improvements in query throughput for multiple algorithms. Our fastest results are more than a factor of two faster than the best previous results.
The remainder of this paper is organized as follows. In Section 2 we outline the background and related work. Section 3 describes the proposed filtering mechanism that uses Block-Max Indexes. Next, in Section 4 we present how to augment the mechanism with posting information. Section 5 explains our proposed solutions for the space overhead of the auxiliary structures and Section 6 presents preliminary results on our proposed methods. In Section 7 we show the performance of the filtering mechanisms on several scenarios, whereas in Section 8 we discuss caching policies for our mechanism. Finally, Section 9 presents additional results and Section 10 concludes and discusses future work.
2. BACKGROUND AND PREVIOUS WORK
In this section, we provide some background on inverted indexes, query processing and early termination techniques, Block-Max Indexes, and describe previous work.
2.1 Inverted Indexes
Inverted Indexes: An inverted index [27, 30] is a simple and efficient data structure widely used by large search engines for query processing. This structure allows finding the documents where specific terms occur and can be formally defined as follows. Given a collection of N documents, we assume each document is identified by a unique document ID (docID) between 0 and N -1. An inverted index contains an inverted list Lw for each distinct term w in the collection. Each Lw is a list of postings describing all documents where term w appears in the collection. More specifically, each posting contains the ID of the document (docID) that contains the term w, the number of occurrences of w in the document (the frequency), and potentially additional information such as the exact positions of these occurrences of term w in the document (positions) or other data. Unless stated otherwise, we store docIDs and frequencies such that each posting is of the form (di, fi). There are several ways to layout the inverted lists, but in this work we focus on document-sorted indexes, where postings in each list are

sorted by their docIDs di; see [18, 16, 6, 8, 23] for work using other layouts.
Index Compression: The inverted lists of frequent terms may contain millions or even billions of postings. To decrease space requirements, inverted lists are usually kept in compressed form. A common approach for docID-sorted indexes is to store the differences between consecutive docIDs in a list in suitably compressed form [30]. Index compression is crucial for search engine performance, and many compression methods have been proposed; see [29, 28, 21] for some recent work. For fast access, inverted lists are usually compressed in blocks of, say, 64 or 128 postings, such that each block can be accessed and decompressed individually. This is done by storing the uncompressed last docIDs and sizes of all blocks in a separate array that can then be used to locate and decompress individual blocks. Here, we compress the inverted lists into blocks of 64 docIDs followed by the corresponding 64 frequencies.
Index Quantization: We usually store each posting as a pair (di, fi). However, it may sometimes be preferable to store postings as (di, si) where si is a score of di with respect to w. There are two main motivations for this: (1) to save the cost of computing scores from frequency values and other statistics such as document size and global term frequency, and (2) for scores that cannot be easily computed on the fly as they are derived from complex ranking functions with hundreds of features, using techniques similar to those in [1]. However, storing each si as a 32- or 64-bit number would result in very large index sizes. To reduce the size, socalled index quantization techniques are used that basically round the floating point score values to one out of a fixed set of, say, 256 distinct values. This may lead to a slight loss in precision during ranking, but guarantees that each score can be stored in only 8 bits. There are several quantization techniques in the literature; see [2, 3, 5, 4]. In this paper, we use the Global-by-Value technique proposed in [5], and also used in [11], using 8 bits for each score. All necessary score accumulations can be done directly in integer space, as in [2]. We refer to such an index structure as a quantized index, in contrast to a standard index storing frequencies values.
Index access: Document-sorted indexes allow fast index traversal during query execution based on a Document-AtA-Time (DAAT) approach. In DAAT index traversal, every list has a pointer to a current posting, and all pointers move forward in a synchronized manner as a query is processed. Thus, all postings (and thus documents) to the left of the pointers have already been processed, while postings to the right are yet to be considered. In DAAT, we typical have functions for opening and closing inverted lists for reading, and a function nextGEQ that, given a docID d and an open inverted list L, moves the pointer in L forward to the first posting with docID greater or equal to d. In addition, there is a function for retrieving other data (such as frequency or score) associated with the current posting. All decompression operations in the inverted lists are hidden within these functions. In this work, we focus on algorithms that perform DAAT traversal on document-sorted indexes.
2.2 Query Processing and Early Termination
The simplest way to query an inverted index is to ask for all documents containing some (disjunctive) or all (conjunctive) of a set of query terms. Of course, this would return too many results on large document collections, and thus IR

724

systems supporting ranked queries return the highest scoring documents among those that satisfy the condition. Many simple ranking functions have been proposed for this task, including BM25 and the Cosine measure [7]. These functions are typically of the form r(q, D) = tq s(t, D), meaning that the score of a document with respect to a query is the sum (or some other simple combination) of the query term scores of the document. In this paper, as in almost all previous work, we assume disjunctive queries with a ranking function of this form, that is, our goal is to return the highest scoring among all documents containing at least one of the query terms.
We note that such disjunctive queries tend to be more expensive than conjunctive queries that only need to score documents containing all query terms. For this reason, many search systems try to use conjunctive queries whenever possible. However disjunctive queries are known to return better results in many situations, and thus it is important for search engines to also support these more expensive types of queries.
Cascade Ranking: The above simple ranking on top of a disjunctive or conjunctive filter is only the first phase of query processing in current search engines. These engines will typically take a limited number k of top-scoring results from the simple ranking, say a few thousand overall, or as little as tens per node on a large cluster of machines, and evaluate a second, more complicated but more precise, ranking function on these results. This process may be repeated with an even more evolved ranking function applied to the top results from this ranking. Thus, we expect our top-k algorithms to be the first phase of such an architecture, and will look at the effect of varying k from 10 to a few thousand.
Early Termination: The main performance bottleneck that we have to deal with is the length of the inverted lists, which can grow to many MBs or even GBs on large collections. As mentioned, a simple algorithm for this problem would compute the score of any document containing at least one of the query terms; we call such an algorithm exhaustive. To avoid scoring many of these documents, earlytermination (ET) algorithms have been proposed. We say that an ET algorithm is safe if it outputs exactly the same top k results as an exhaustive algorithm [22]. (In the case of a quantized index, safeness is of course defined with respect to an exhaustive algorithm on the full quantized index.)
There are many safe and unsafe ET algorithms in the literature; see, e.g., [25, 9, 6, 23, 15, 10] for a small sample. Some algorithms keep inverted lists sorted by docID, while others reorganize the lists by score s(q, D), so that all highscoring postings are encountered early during query processing. In this paper, we focus on safe early termination. All our algorithms keep the inverted lists sorted by docID and use DAAT for index traversal ­ but our running times also outperform all safe methods with other index layouts.
2.3 ET Algorithms
In this part, we first describe a simple exhaustive algorithm and then outline two basic safe ET algorithms that use DAAT traversal on document-sorted indexes, the WAND algorithm in [9] and the Maxscore algorithm in [25]. Both algorithms store for each inverted list the highest impact score of any posting, called the maxscore of the list. Recall that in DAAT traversal, we maintain a pointer to the current posting of each list. Also, we use  to refer to the current

threshold, which is the smallest score that can still make it into the top k results during the execution of the algorithm.
Exhaustive: The simplest query processing algorithm first picks the smallest current docID across all query terms, called cID. This can be done using either a loop or a heap, and cID is then fully evaluated. Afterwards, all pointers are moved to at least the next posting after docID cID. We also maintain a heap of the current top-k results, and add cID to this heap if it has a score larger than the current kth highest score.
WAND: This algorithm, proposed in [9], consists of three phases: pivot selection, alignment check, and evaluation. In every iteration, a pivot term is selected by summing up the maxscores of the participating lists in order of increasing current docID, until the sum becomes larger or equal to . Next, WAND tries to align the current docIDs of the preceding lists in the ordering with the pivot docID, by moving the pointers in these lists forward to the pivot docID. If this succeeds, and all pointers up to the pivot point to the same docID, then this docID is evaluated, if necessary inserted into the top-k heap, and the current pointer in the pivot list is moved forward. Then we go to the next iteration.
Maxscore: This algorithm was first described in [25]. We focus here on the DAAT versions of the algorithm in [24, 17, 13]. Maxscore splits the query terms into essential and nonessential terms as follows. We sum up the maxscores of the lists from lowest to highest while the sum remains less than . The terms that contribute to this sum form the nonessential terms, and the others are the essential terms. The basic idea is that any document that can make it into the top k results must contain at least one essential term. Now we select as candidate, the smallest current docID of any essential list, and evaluate it. Thus, we basically run the exhaustive algorithm on the essential lists only. Of course, as the threshold  for making it into the top k increases during execution of the algorithm, we may move additional terms from the essential to the non-essential set.
2.4 Block-Max Indexes
The idea of storing and exploiting the maximum impact score of each inverted list was recently and independently extended by two research groups [10, 15], who proposed an augmented index structure called Block-Max Index (BM). This structure stores the highest impact score for each block in the inverted lists, where the blocks are defined by the index compression method. Recently in [13], the blocks of the Block-Max structures were decoupled from the index compression and defined on docID space rather than posting distribution. We will refer to Block-Max structures whose blocks are defined by postings as posting-oriented, while the ones defined on docID space as docID-oriented.
2.5 Previous Work Using Block-Max Indexes
In this paper, we build on the work of [15, 10, 13], where we generalize and extend these ideas by proposing a filtering mechanism that uses Block-Max Indexes to further improve the query processing for disjunctive queries.
More specifically, [10, 15] proposed the posting-oriented BM structure to approximate the score of a block in posting space. The Block-Max Indexes provide score approximations in posting block resolution that enables skipping of compressed blocks. The approach in [15] proposed an enhancement of the WAND [9] algorithm that uses the Block-

725

Max index, called BM W , and operates as follows. After the sorting and pivot selection step as in [9], there is an additional block-max check that tests whether the pivot docID can make it into the top-k results. In case the block-max test fails, BMW can safely skip until the end of the blocks. The work in [10] suggested an algorithm that is built on top of the Maxscore and also uses posting-oriented BM structures. In particular, given a query, the algorithm reads the stored BM information, generates aligned intervals in the docID domain, and computes upper bound scores for each interval. During query processing, each interval's upper bound score is checked whether is larger or equal to the kth highest scoring document seen so far and if the test fails, the algorithm can skip blocks and safely save computations.
Both previous studies show experimentally that these auxiliary indexes provide fast query response times with small space requirements. However, the skipping power offered by posting-oriented BM indexes is limited because the blocks are defined by the index compression method. More recently, a new layout of the BM was proposed [13], where the blocks are defined on the docID domain. The blocks of the BM structure are decoupled from the index compression and their block size can be assigned per-term using the fixed, the expected or the variable block size selection method. The docID-oriented BM indexes provide very fast lookups, because the block-max access can be performed with bit shift operations. The authors in [13] use the variable size selection scheme to deal with the space overhead of the structure and show that several algorithms perform faster using the proposed BM index. We refer to [15, 10, 13] for more details on Block-Max indexes and query processing algorithms using such structures.
In this work, we extend the idea of constructing aligned intervals in docID space and computing interval upper bounds as in [10] by generalizing it on the docID-oriented BM structure [13] with fixed block size. The result is aligned BM blocks for all lists, allowing fast block-maxscore accesses and aggregation of "aligned block" upper bounds. We build on this scheme and propose a filtering mechanism that utilizes docID-oriented BM indexes to discover the "live" blocks in consecutive windows of docID space, which leads to significant performance gains over a series of DAAT algorithms for disjunctive queries. For the remainder of the paper we assume docID-oriented BM using the fixed block size selection method.
3. LIVE BLOCK MECHANISM
In this section we describe our proposed candidate filtering method that exploits the Block-Max index.
Live Area Computation: We can model DAAT query processing algorithms as a sequence of rounds, where in each one we examine whether a specific candidate docID can make it into the top-k results (by evaluating it or safely early terminating). Since our goal is to retrieve the topk results fast, we can reduce the cost of query processing by minimizing the number of candidate docIDs. In previous works [10, 15, 13], the BM structure is utilized for candidate filtering in an online fashion, meaning that in each round there is a block-maxscore test checking whether the candidate has a chance to be added in the result heap. However, in our approach the aggregation of the upper bound blockmaxscores across query terms for consecutive aligned blocks is performed in batch mode and then exploited to aggres-

sively skip blocks. Due to the docID-oriented BM layout, there is no need for the alignment step as needed in [10].
In typical DAAT query processing algorithms, at the end of each round, the candidate docID for the next round is selected. The selection of this document is always preceded by invocation of the nextGEQ() on some lists. Since nextGEQ() usually demands expensive block decompression, we would prefer to make this decision based on the available blockmax information. Therefore, given a query with m terms and a threshold , we define a block as live if the sum of block-max scores across m terms is greater or equal to . Thus, we can safely skip the block and avoid redundant decompressions and score calculations when a block is "dead". Remark that in [13] a similar optimization was suggested by the BMM-NLB algorithm, but in their setting there is no enforced block-max alignment.
We extend this idea by computing the live blocks of a window of docID space, where window refers to a contiguous docID region. Thus, we calculate block-maxscore sums for all blocks of a window. By definition, only blocks that summed up to more or equal than  (of the specific window) are live blocks. We will refer to this procedure as Live Area computation (LA) of a window. This per-window liveness information is stored in a structure called Liveness Bitset (LB), where each bit corresponds to a block of the window. LB is used for candidate filtering by skipping "dead" blocks. In particular, our mechanism first obtains the next live block from the LB and then provides the first docID of this block to the nextGEQ(). The live area computation is performed prior to query processing to provide efficient filtering oracle for candidate selection. Note that at the end of query processing of each window, we re-use the memory occupied by the LB structure to avoid the costly memory initializations.
Windows: The window size under this setting is crucial since the number of live blocks depends on the most updated . When the size of the window is too small, we pay for the construction overhead (LA). On the other hand, if the window size is too large, the effectiveness of the LB would drop due to outdated thresholds. In practice we observed that the appropriate window size should be a function of L1 or L2 cache size. As you advance in docID space (and  grows), LB becomes more sparse, and hence, more effective. In preliminary experiments we observed that the convergence rate of  is quite fast.
SIMD: The computation of live areas is performed during query processing (online) per window, thus it must be fast. This process includes the summation of the aligned blockmaxscores across terms, the comparison of this sum with , and the encoding of the liveness information in LB. Since all inverted lists block-maxes are aligned in docID space, the LA calculation is vectorizable and SIMD capabilities of modern CPUs can be exploited to accelerate this computation. Our calculation of LA is coded for the SSE instruction set and performs the following operations: (i) load 4 consecutive block-maxscores from each query term, (ii) sum the blockmaxscores vectors, (iii) compare the results with , and (iv) set the corresponding bits in LB. In a nutshell, LA's calculation overhead is negligible because of the natural speed-up of vectorized operations (SSE) and the limitation of LB to L1 cache. Note that SSE also applies to the integer operations used in the quantized index scenario.
In Figure 1, we see an example of LA computation for the 2-term query "cat squirrel" of 4 consecutive block-maxscores.

726

In this example, the BM scores for both terms are shown in grey, whereas their sum is colored green if the block is live, otherwise it is red. Under this scenario, only the third bit in LB is set. Since we are using SSE for the computation, every step is performed in one instruction. Thus, for any 2-term query we only need 5 instructions instead of 20 to compute the liveness of 4 consecutive blocks. The obtained information can filter all blocks in the window that could not make it into the top-k results and therefore, save decompressions and evaluations.

block-max

squirrel

cat

block

window

Figure 1: LA computation using terms' BM structures.

Varying Block Size and Threshold: The windowbased LA calculation is computed using the most updated . The early knowledge of a good threshold makes the filtering mechanism very effective since less blocks will be live. In Figure 2, we show the percentage of live blocks for various block sizes and for different fixed thresholds ( is not updated). We observe that the number of live blocks decreases when (i) decreasing the size of the block and (ii) providing higher . Since larger blocks provide worse blockscore approximations, with higher threshold they have better chances of being live. These observations provide a strong evidence about the effectiveness of the proposed filtering mechanism. In practice, when  is updated per window, the decrease in liveness is more significant (not linear).

Figure 2: Live block percentage varying block size and fixing the starting threshold, for the TREC06 query trace.
Filtering Mechanism Interface: Our proposed windowbased candidate filtering mechanism provides a simple interface that can be easily used by any DAAT query processing algorithm. We now describe the various low-level components of our framework so its adaptation to existing or new algorithms becomes trivial.
In this part, we outline the computation of LB structure and how to utilize it during query processing. First, the LA computation is performed whenever the termination condition of DAAT algorithms occurs. More specifically, these

algorithms terminate when the docID of the candidate document is greater than the largest possible docID in the collection. At this point, the algorithms using our mechanism should be adapted to operate in the boundaries of the current window. Thus, whenever the docID of the candidate document exceeds the current window boundaries, the LA computation should be performed with the current . This consists of block-maxscores summation, comparison to  for all blocks of the window, and setting the appropriate bits in the LB. Note that if the BM scores are not available, our mechanism needs direct access to the index to compute them. The liveness information of the LB can be accessed by a core function called nextPotentialLiveDid, which given a docID returns the first docID of the current or the next live block. The returned docID and a specific list are then provided to the nextGEQ(). We hide the complexity of this procedure by introducing a new function called newNextGEQ, which encapsulates both nextLivePotentialDid() and nextGEQ(). Note that the original nextGEQ() performed on the "live" candidate can still return a docID in a "dead" block. Thus, instead of calling the nextGEQ() the algorithms using our filtering mechanism could just use the new function newNextGEQ.
To sum up, algorithms could use our filtering mechanism by first changing the halting condition to be windows aware, then performing the described LA computation whenever the candidate docID is out of the current window boundary, and finally, by calling the newNextGEQ() instead of nextGEQ(). Hence, our mechanism offers a simple interface to query processing algorithms, by hiding all the complexity behind few functions.
Architecture of the Filtering Mechanism: We now describe how our technique communicates with the available data during query processing. Figure 3 shows our query processing pipeline, which consists of the Index, the Query Processor, the Filter Mechanism and the Filter Cache. The Filter Mechanism has direct bulk access to the Index so that it can generate the augmented structures on-the-fly for lists out of the Filter Cache, as we will describe in Section 5. Hence, the Filter Cache contains augmented structures (such as the BM Index and the LB) that are accessed and exploited by the Filter Mechanism for effective skipping during query processing. The Query Processor uses the core function nextGEQ() to access the Index and move the pointers of the inverted lists. It may also use the Filter Mechanism to request the liveness bit of a docID or the maxscore of a block. In that case, the information is served by the Filter Cache and returned to the Query Processor. Remark that the communication between the Query Processor and the Filter Mechanism can in fact be encapsulated within the newNextGEQ() operator.

docIDs, blocks
Filter Mechanism

Query Processor

Filter Cache

bulk access

nextGEQ()
Index

Figure 3: Filtering mechanism architecture.

727

4. POSTING BITSET (PB)
The LB provides a smart exploitation of the BM structure with performance gains as we will experimentally show in the next section. Its effectiveness to filter candidate docIDs relies solely on the approximation quality of the blockmaxscores. The early termination methods of algorithms using BM structures are mainly based on the score approximation. However, we could also filter candidate docIDs using additional information about the postings existence in a block. For example, a block without postings does not contribute to the block-maxscore aggregation in LA. In fact, a block-maxscore is non-zero if and only if the block contains postings. In order to exploit the additional posting existence information of a term we encode it as follows: for a contiguous docID region called sub-block, we store a bit describing whether the term contains any posting in it. Thus, for each term we store a Posting Bitset (PB), where each bit represents the posting liveness of a specific sub-block. This encoding offers extremely fast accesses to the PB structure (because sub-blocks are docID-oriented).
The PB can be seen as a per-term filtering method that query processing algorithms can benefit from. In particular, this filtering mechanism consists of a basic function called nextLivePotentialPosting that given a docID and a list, returns a docID belonging to the current or next populated sub-block of the given list. Moreover, this function can be used to answer whether a term has any postings in a specific region, thus saving redundant computations such as decompressions of blocks and score calculations. Similar to the LB mechanism, the PB filtering technique also requires direct access to the index for construction. The PB structure can be easily combined with the BM index during LA computation to provide finer granularity candidate filtering as we will shortly see.
4.1 LB-PB Mechanism
In the previous section, we proposed the PB structure that provides information about the posting liveness. This structure can be utilized during the LA computation to provide finer granularity filtering. In order to obtain a useful combination of the BM index and the PB, we need to meaningfully select the block size of the sub-blocks. Note that having a sub-block larger than the BM block does not introduce new information. We experimentally observed that a good size for the sub-blocks is 2m-3, where 2m is the size of the BM block. For the rest of the paper, we assume that the sub-block size is 2m-3.
Our assumption about the sub-block size implies that every BM block is split into 8 sub-blocks. Therefore, for each BM block we have additional information about the posting liveness of its sub-blocks. Utilizing the PB structure in LA computation requires the following steps: first every block-maxscore is duplicated 8 times and masked with 8 sub-block bits, then the sub-block level scores are summed across terms, and finally, we compare the sums with the current most updated  and set the appropriate bit in the liveness bitset. Note that in our case, the temporary PB structure is 8 times larger than the corresponding structure we used for LB. We will refer to this structure that provides an effective candidate filtering mechanism for the query processing algorithms, as LB-PB. Similarly to LB, during query processing the algorithm can provide a docID to the LB-PB to obtain the next live sub-block in the window, avoiding

redundant computations such as decompressions and score calculations. Given a docID, the mechanism returns a docID in the current or the next live sub-block. Remark that this filtering technique can exploit both nextLivePotentialDid and nextLivePotentialPosting functions, that now operate on sub-block level, and its complexity can be hidden as well behind the newNextGEQ().
In Figure 4, we describe the live area computation of the query "cat squirrel" for 4 BM blocks using the LB-PB mechanism. The BM scores of both terms are colored in grey, and only the PB of the third block is shown. The nonempty sub-blocks are shaded. The available PB information enables finer block-maxscore granularity on sub-block level. Focusing on the third block, the LA-PB computation includes the multiplication of the posting bitset of the third block of each term with the corresponding BM score. The result is depicted as grey sub-blocks, whereas the summation of the aligned sub-blocks across terms is shown as red or green sub-blocks sums. We can alternatively think this procedure as an alignment step in sub-block resolution. After the LB-PB filtering, only the sixth sub-block of third block was found to be live, while the original LB mechanism would label the entire third block (8 sub-blocks) as live. This example shows the effectiveness of the LB-PB mechanism and its superiority over the simple LB filtering technique.

squirrel

cat

block

window

Figure 4: Example of the LB-PB filtering mechanism.

SIMD: Similarly with the LA computation, the calculations of LA when combined with the PB structure can exploit the SIMD opportunities of modern CPUs. All steps of the LA computation of the LB-PB mechanism are vectorizable and therefore, the SSE instruction set is used in the implementation to speed up the calculations.
Varying Block Size and Threshold: Figure 5 presents the percentages of the live sub-blocks for various block sizes and different fixed thresholds. As in Figure's 2 observations, smaller block sizes and higher starting threshold result in less live sub-blocks, as expected. Comparing the percentages of Figure 2 and 5, we observe that the combination of LB and PB leads to less live blocks. These statistics provide evidence that the proposed candidate filtering mechanisms could give significant performance gains.

5. SPACE OVERHEAD
In this section we discuss how to deal with the space overhead of the auxiliary structures.
5.1 Block-Max Index
So far we have not mentioned the space overhead of the BM index needed during the LA computation and we assumed BM resides in memory. In practice, the BM structure

728

Figure 5: Live sub-block percentage varying block size and fixing the starting threshold, for the TREC06 query trace.
with fixed block size is too large to be memory-resident, so we address the space overhead using the approach in [13], and we will refer to this as Length-based. Under this method, the BM for terms with few postings are not stored at all, but generated on-the-fly (OTF), while the BM scores for the rest are undergoing linear quantization (BMQ), such that every block-maxscore is 1-byte (instead of 4). The difference in our setting is that the OTF is now performed per window. The window-based OTF computation involves (i) the decompression of the blocks (docIDs and frequencies), (ii) the score calculation of these docIDs, and (iii) the Block-Max Generation (BMG). We further extend the length-based approach by compressing the quantized Block-Max values (BMQC) of the medium-sized lists. More specifically, we first create a skiplist for non-zero values and then we compress them using a version of PForDelta [29]. This requires decompression of the compressed quantized values during query processing, but its overhead is minimal compared to its space gains.
5.2 Posting Bitset
The size of the sub-block plays a crucial role on the effectiveness of the posting bitset structure. Very large subblocks provide poor posting existence information, whereas too small sub-blocks add significant space overhead. Overall, we address the space overhead of the posting bitset structure by following a length-based approach as in the case of the BM structure. A first observation is that for long lists we expect most sub-blocks to be non-empty, hence a PB with all bits set is a decent approximation for it. Instead of storing the actual PBs for such lists, we "fake" them by pointing to a static array of 1's, thus saving this space. We experimentally observed that this approximation of PB does not impair filtering abilities and does not cause a slow down. Moreover, since we already decompress the short lists in OTF BMG, we can use the docIDs to construct the PBs on-the-fly as well. The PB structures are stored in main-memory only for the medium-sized terms. Note that the construction of PB should be also considered under the window-based scenario as described in the previous section.
We will revisit the length-based approach for both structures in Section 8 to achieve a better space/time tradeoff.
6. PRELIMINARY EXPERIMENTS
In this section, we evaluate and compare the performance of the simplest query processing algorithm, the exhaustive, when our filtering mechanisms are applied, with the best previously reported numbers to our knowledge [13] under the scenario of a standard index.

6.1 Experimental Setup
For our experiments, we use the TREC GOV2 dataset that contains 25.2 million web pages. We build inverted index structures with 64 docIDs and frequencies per block, using a version of the PForDelta compression method [31] described in [29], but our ideas also apply to other compression methods. The size of the uncompressed size of the data set is 426GB, whereas its compressed standard index size is 8.75GB and its compressed quantized index 11.97GB. We evaluate our methods using 1000 queries randomly selected from the TREC 2006 and 2005 Efficiency track query set.
We implemented all the algorithms in C++ using BM25 as our ranking function, and return top-10 results unless stated otherwise. For the rest of the paper, we assume docIDoriented BM using the fixed block size selection method with 64 docIDs per block. The experiments were conducted on a single core of an Intel Xeon server with 2.27Ghz and all data structures reside in memory.
6.2 Baseline algorithms
Setup: We begin with a performance comparison of the exhaustive algorithm and its versions that use the LB and the LB-PB mechanisms, called EX, EX-LB and EX-LB-PB respectively, on the standard index. For this experiment we use the following length-based policies for the BM: (i) OTF for lists with length less than 95042, (ii) BMQC for lists between 95042 and 218, and (iii) BMQ for the remaining ones. Similarly, for the PB structure we use the subsequent length-based rules: (i) OTF for lists with length less than 179758, (ii) maintaining in main memory the posting liveness information for lists with length between 179758 and 222, and (iii) faking of bitsets for lists larger than 222. The space requirement for the described policy for the GOV2 data set is 2GBs, less than 25% of the compressed index, while the use of PB increases the space to 4GBs. We also compare the performance of the exhaustive algorithm when the filtering methods are applied with the fastest previous results to our knowledge [13]. More specifically, the approach in [13] uses docID-oriented BM structures, variable block size selection methods and both OTF BMG (up to lists of size 215) and BMQ techniques (on the remaining lists), with 2.76GBs space overhead. The authors of [13] were willing to share the code for their fastest algorithm, called BM-OPT, which selects based on the number of query terms the fastest algorithm between BMM, BMM-NLB and BMW.
Exhaustive speed-up: In Table 1, we present the query response times for the BM-OPT algorithm and all versions of the exhaustive algorithm for the TREC06 query trace, when the number of query terms varies. We can see that the LB filtering mechanism makes the exhaustive algorithm faster by a factor of 10, whereas the availability of PB boosts the exhaustive 16 times! The exhaustive algorithm is significantly accelerated by the availability of the LB mechanism achieving average query response time of 13.47ms. When the LB-PB is used it further reduces the average time to 8.59ms, 16 times faster over EX. The performance boost of the mechanisms is consistent across varying number of query terms. Table 2 shows the radical decrease in term score evaluations and calls to nextGEQ() when the proposed mechanisms are applied. The reason for the significant reduction in query response times, evaluations and nextGEQ() is the following: at the end of each iteration of any query processing algorithm, the pointers of some or all lists must

729

be moved using the expensive nextGEQ() (which usually includes decompression). Our mechanism checks the LB (or the LB-PB) structure in order to obtain the next live block (or sub-block) and then provides to the nextGEQ() the first docID of this block (or sub-block). Therefore, this test before the call of nextGEQ() moves the pointers much further in docID space and results in much less redundant expensive decompressions, which is the main reason for the significant performance speed-up of our mechanisms.
Comparison with BM-OPT: Furthermore, we see that the performance of both EX-LB and EX-LB-PB outperforms that of BM-OPT. Concerning the space requirements of the algorithms, as mentioned, the EX-LB occupies 2GBs, less than the 2.76GBs needed by the best previous approach. Thus, our filtering method uses less space to achieve faster times compared to the state-of-the-art algorithm, BM-OPT. When the PB structure is also available the space increases to 4GBs and EX-LB-PB achieves much faster query response times compared to BM-OPT. Remark that our filtering mechanisms accelerate significantly the simplest query processing algorithm, the exhaustive, which outperforms the state-of-the-art BM-OPT algorithm.

Algorithm EX
EX-LB EX-LB-PB
BM-OPT

avg 138.98 13.47
8.59
14.96

2 37.68 2.71 2.65
3.81

3 97.06 5.89 5.07
9.91

4 166.39 16.63 10.72
18.8

5 301.24 35.08 18.69
34.29

Table 1: Performance of the exhaustive algorithm on the availability of our mechanisms for the standard index and the TREC06 query trace.

Algorithm EX
EX-LB EX-LB-PB

Time (ms) 138.98 13.47 8.59

# nextGEQ/query 4, 489, 430 195, 080 86, 792

# evals/query 4, 489, 430 194, 741 86, 454

Table 2: Number of term score evaluations and calls to nextGEQ for the standard index and the TREC06 query trace.
7. EXPERIMENTS
In this section, we present a performance comparison of widely used DAAT query processing algorithms when our filtering techniques are applied. These algorithms either use (BMM-NLB, BMW) or not (WAND, Maxscore) the BM structure for early termination during query processing.

7.1 LB-PB on DAAT algorithms
In particular, the WAND algorithm was implemented based on [9], Maxscore and BMM-NLB were coded according to [13], while the BMW algorithm based on [15]. All algorithms were modified in order to use the filtering mechanisms interface as previously described.
The performance comparison of these algorithms for the TREC06 and TREC05 query trace when we vary the number of query terms is presented in Table 3 and 4. The significant speed-up of our candidate filtering mechanism over several query processing algorithms shows the effectiveness of the filtering. Table 3 shows that Maxscore outperforms WAND when any filtering method is applied. On algorithms that use the BM index to early terminate documents, BMW is consistently faster from BMM-NLB. We observe that the performance of Maxscore is similar to BMM-NLB and the

reason is that BMM-NLB spends too much time during its several cascading filtering steps, which are not very useful when our mechanisms are used. BMW outperforms all algorithms with 6.45ms using the LB and 5.74ms when PB is also available. A similar performance behavior is observed for the TREC05 query trace in Table 4. Enforcing candidate selection with LB (and LB-PB) in all algorithms consistently accelerates them. However, this was apparent from preliminary experiments, where the simplest algorithm outperformed the state-of-the-art BM-OPT. Due to space limitation, for the remainder of the paper we omit any results for the TREC05.

Algorithm WAND Maxscore
BMM-NLB BMW
WAND Maxscore BMM-NLB
BMW

LB (2GBs)

avg 2

3

11.53 2.51 4.97

8.1 2.3 4.17

8.29 2.13 3.97

6.45 2.03 3.45

LB-PB (4GBs)

8.19 2.59 4.7

6.22 2.34 3.93

6.21 2.21 3.73

5.74 2.18 3.59

4 13.86 10.08 10.2 7.63
10.05 7.89 7.88 7.04

5 31.15 19.4 20.38 15.44
18.23 12.68 13.06 12.03

Table 3: Performance comparison of several DAAT query processing algorithms when LB and LB-PB mechanisms are applied on standard index, for the TREC06 query trace.

LB (2GBs)

Algorithm avg 2

3

4 5

WAND 10.22 2.51 5.02 7.56 47.19

Maxscore 5.62 2.18 4.13 5.75 20.37

BMM-NLB 6.19 2.03 3.98 5.66 24.88

BMW

5.35 1.96 3.58 4.89 20.79

WAND

LB-PB (4GBs) 7.66 2.62 5.02 6.9 29.98

Maxscore

5 2.32 4.21 5.41 15.03

BMM-NLB 5.21 2.18 4.06 5.24 17.22

BMW

5.12 2.15 3.93 5.03 17.11

Table 4: Performance comparison of several DAAT query processing algorithms when LB and LB-PB are applied on standard index, for the TREC05 query trace.

8. SPACE/TIME TRADEOFF
In this section, we discuss how we can further optimize both space and time constraints by moving from simple length-based caching policies to cost-based.
8.1 Length-based Caching
In the previous experiments, the performance of our methods and their space requirements were based on decisions governed by the length of the lists, called Length-based policies. These policies include (i) OTF computation of BM and PB for short lists, (ii) storing compressed quantized BM for medium lists (BMQC), (iii) faking of PB for long lists, and (iv) storing quantized BM and PB for all remaining lists.
For the previous experiments, the length-based parameters were fixed to specific values using acceptable space requirements to show the effectiveness of our mechanism. Although the length-based caching provides a good space/time tradeoff, it is solely depends on the length of the list. More specifically, it forces short lists' augmented structures (BM and PB) to be computed OTF, whereas it always stores in the Filter Cache these structures for large-sized lists. This is

730

not always optimal, since it is likely that some short terms will occur frequently in the query trace and thus, it may be more beneficial to store their augmented structures rather than computing them OTF. Therefore, we would like to move to policies that take into account the frequency of terms in the query workload and offer better space/time tradeoffs.

8.2 Cost-based Caching

It is common that large web search engines use caching

mechanisms so their performance adapts and optimizes to

the query workload. Such mechanisms maintain statistics

about the terms appearances in the query logs and based on

them, decide whether to cache the additional information.

The main idea behind such approach is to achieve perfor-

mance speed-up by using less space smartly. Therefore, we

try to maximize the obtained performance gains by following

optimal policies for each term according to query statistics

and under specific space requirements. We will refer to the

proposed policies that are based on a benefit function that

takes into account the frequency of the terms in the training

query trace, as Cost-based policies. In particular, if a short

term occurs very frequently in the query log, it would be

reasonable to maintain its augmented structures in memory.

On the other hand, if a long term does not appear too often

in the queries, we refrain from storing it in memory.

The proposed cost-based caching policy follows the next

steps: first, for each term we compute a rank under various

policies

Pi

as

follows

time(OT F )-time(Pi) space(Pi )

 f req(term).

The

policies for the BM index include the length-based policies

(i) OTF, (ii) BMQC and (iii) BMQ. Similarly, the policies

for the PB structure consist of (i) OTF, (ii) PB, and (iii)

faking of PB. Then, given a space budget, we select the

terms (from highest benefit rank to lowest) and store their

structures in the Filter Cache. This selection procedure ends

when the space requirements are met and thus, for all terms

that were not selected by the caching policy, we use OTF

(regardless of their length). We obtained the frequency of

terms in the query log using as our training set the 99k

queries from the TREC 2006 Efficiency track query trace

and then, applied our method to the remaining 1k queries.

In Figure 6, we compare the space/time tradeoff of both

caching methods on WAND, BMM-NLB and BMW, when

only the LB filtering mechanism is available for the stan-

dard index. As expected, given a specific space budget, the

cost-based policy always achieves faster times for all the al-

gorithms compared to the length-based approach. Our pro-

posed method is very effective even in scenarios with very

limited space constraints. In particular, for space budget of

0.5GBs, BMW achieves 15.75ms using the length-based pol-

icy, whereas 10.22ms using the cost-based. For the remain-

der of the paper we will use the cost-based caching approach.

Figure 7 shows that when more space is available, using the

PB structure accelerates the query processing time. More-

over, we observe that the quantized index can further reduce

the query response times, for example Q-BMW-LB-PB for

2GBs achieves 4.36ms.

9. EXTENSIONS
In this section, we provide some additional results. More specifically, we look the performance of our methods for reodered indexes and the impact of increasing top-k on the performance of our techniques.

Figure 6: Space/Time tradeoff of Length-based and Cost-based caching on the standard index.

Figure 7: Space/Time tradeoff of Cost-based caching on the availability of LB and LB-PB on the standard and the quantized index.

DocID Reordering: There has been a huge amount of work to reduce the size of the compressed inverted indexes by reordering the docIDs. Under docID reordering, the documents are assigned docIDs using different techniques in order to minimize the index size. The main idea is to assign closeby docIDs to documents sharing lots of terms. A simple and effective method proposed by [20], which first sorts the URLs alphabetically and then assigns docIDs based on this ordering. More sophisticated reordering techniques were proposed in [14], whereas [28] showed that reordering speeds up the conjunctive query processing. Recent work in [15, 13] showed that docID reordering also accelerates the query response times of disjunctive queries. In Table 5 we show the performance of BMW using the cost-based caching policy under the simple reordering method of [20] when varying the number of query terms. For the remainder of the paper, the space requirement of the algorithms using the LB structure is 2GBs, and 4GBs when the PB is also used. As expected reordering gives a speed-up for both filtering mechanisms. In particular, BMW-LB achieves 3.02ms average times, whereas BMW-LB-PB performs similarly with 3.04ms. The reason the performance of BMW-LB-PB is not better is that the reordering changes the distribution of postings in docID space.

Algorithm avg 2

3

4 5

BMW-LB 3.02 1.31 2.16 3.25 6.06

BMW-LB-PB 3.04 1.33 2.23 3.3 5.89

Table 5: Impact of docID reordering on BMW on the standard index.

731

Varying k in Top-k: In this experiment, we show the performance of our methods when k is increased. In Table 6 we see the performance of the filtering mechanisms on BMW as we increase k and vary the number of query terms for the standard and the quantized index. We observe that both mechanisms scale very well, but LB-PB performs better as k increases in both indexes. BMW-LB returns the top-100 results in 13.5ms. In the quantized index scenario, the query times are further reduced since calculations operate in the integer domain. In particular, Q-BMW-LB-PB returns the top-10 results in 3.83ms, while the top-1000 in 24.8ms.

Algorithm

10 50 100 500 1000

BMW-LB

5.93 10.31 13.5 26.88 36.27

BMW-LB-PB 5.06 8.53 11.23 23.33 32.34

Q-BMW-LB 4.32 7.59 10.01 19.72 26.32

Q-BMW-LB-PB 3.83 6.65 8.89 18.09 24.8

Table 6: Impact of top-k on BMW performance on the standard and the quantized index.

10. CONCLUSIONS AND FUTURE WORK
In this paper, we proposed a general filtering mechanism based on docID-oriented Block-Max indexes that significantly improves the performance of several widely used algorithms for safe early termination in disjunctive queries. We present a simple interface that can be easily used by several query processing algorithms. Moreover, we augment the mechanism with an auxiliary structure that maintains posting liveness information for each term and show how it can be exploited to achieve faster query times. We also propose caching techniques that address the space/time tradeoff of our mechanisms and overall, we achieve more than a factor of 2 speed-up over the fastest previous methods.
The future work will introduce additional query processing algorithms that exploit the advanced parallel instruction sets available in modern CPUs (AVX) and faster compression methods that use SIMD operations of current CPUs.

Acknowledgement
This research was supported by NSF Grant IIS-1117829 "Efficient Query Processing in Large Search Engines", and by a grant from Google. Sergey Nepomnyachiy was supported by NSF Grant ITR-0904246 "The Role of Order in Search".
11. REFERENCES
[1] D. Agarwal and M. Gurevich. Fast top-k retrieval for model based recommendation. In Proc. of the Fifth Int. Conf. on Web Search and Data Mining, pages 483­492, 2012.
[2] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space ranking with effective early termination. In Proceedings of the 24th Annual Int. ACM SIGIR Conference on Research and Development in Inf. Retrieval, 2001.
[3] V. N. Anh and A. Moffat. Impact transformation: effective and efficient web retrieval. In Proc. of the 25th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 3­10, 2002.
[4] V. N. Anh and A. Moffat. Improved retrieval effectiveness through impact transformation. In Proc. of the 13th Australasian Database Conference, 2002.
[5] V. N. Anh and A. Moffat. Simplified similarity scoring using term ranks. In Proc. of the 28th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 226­233, 2005.
[6] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In Proc. of the 29th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2006.
[7] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern Information Retrieval. ACM Press / Addison-Wesley, 1999.

[8] H. Bast, D. Majumdar, R. Schenkel, M. Theobald, and G. Weikum. IO-Top-K: Index-access optimized top-k query processing. In Proceedings of the 32th International Conference on Very Large Data Bases, 2006.
[9] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Y. Zien. Efficient query evaluation using a two-level retrieval process. In Proc. of the 12th ACM Conf. on Information and Knowledge Management, 2003.
[10] K. Chakrabarti, S. Chaudhuri, and V. Ganti. Interval-based pruning for top-k processing over compressed lists. In Proc. of the 27th Int. Conf. on Data Engineering, 2011.
[11] R. Cornacchia, S. H´eman, M. Zukowski, A. P. de Vries, and P. A. Boncz. Flexible and efficient ir using array databases. VLDB J., 17(1):151­168, 2008.
[12] J. Dean. Challenges in building large-scale information retrieval systems. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, 2009.
[13] C. Dimopoulos, S. Nepomnyachiy, and T. Suel. Optimizing top-k document retrieval strategies for block-max indexes. In Proc. of the Sixth ACM International Conference on Web Search and Data Mining, 2013.
[14] S. Ding, J. Attenberg, and T. Suel. Scalable techniques for document identifier assignment in inverted indexes. In Proc. of the 19th Int. Conf. on World Wide Web, 2010.
[15] S. Ding and T. Suel. Faster top-k document retrieval using block-max indexes. In Proc. of the 34th Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2011.
[16] R. Fagin. Combining fuzzy information: an overview. SIGMOD Record, 31:2002, 2002.
[17] S. Jonassen and S. E. Bratsberg. Efficient compressed inverted index skipping for disjunctive text-queries. In Proc. of the 33th European Conf. on Information Retrieval, 2011.
[18] M. Persin, J. Zobel, and R. Sacks-davis. Filtered document retrieval with frequency-sorted indexes. Journal of the American Society for Information Science, 47:749­764, 1996.
[19] D. Shan, S. Ding, J. He, H. Yan, and X. Li. Optimized top-k processing with global page scores on block-max indexes. In Proc. of the Fifth Int. Conf. on Web Search and Data Mining, 2012.
[20] F. Silvestri. Sorting out the document identifier assignment problem. In Proc. of the 29th European Conf. on Information Retrieval, 2007.
[21] F. Silvestri and R. Venturini. Vsencoding: efficient coding and fast decoding of integer lists via dynamic programming. In Proc. of the 19th ACM Conf. on Information and Knowledge Management, 2010.
[22] T. Strohman. Efficient Processing of Complex Features for Information Retrieval. PhD thesis, University of Massachusetts Amherst, 2007.
[23] T. Strohman and W. B. Croft. Efficient document retrieval in main memory. In Proc. of the 30th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2007.
[24] T. Strohman, H. R. Turtle, and W. B. Croft. Optimization strategies for complex queries. In Proc. of the 28th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005.
[25] H. R. Turtle and J. Flood. Query evaluation: Strategies and optimizations. Inf. Processing and Management, 31(6):831­850, 1995.
[26] L. Wang, J. J. Lin, and D. Metzler. A cascade ranking model for efficient ranked retrieval. In Proc. of the 34th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2011.
[27] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images, Second Edition. Morgan Kaufmann, 1999.
[28] H. Yan, S. Ding, and T. Suel. Inverted index compression and query processing with optimized document ordering. In Proc. of the 18th Int. Conf. on World Wide Web, 2009.
[29] J. Zhang, X. Long, and T. Suel. Performance of compressed inverted list caching in search engines. In Proc. of the 17th Int. Conf. on World Wide Web, 2008.
[30] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Comput. Surv., 38(2), 2006.
[31] M. Zukowski, S. Heman, N. Nes, and P. Boncz. Super-scalar RAM-CPU cache compression. In Proceedings of the 22th Int. Conf. on Data Engineering, 2006.

732

A Test Collection for Entity Search in DBpedia

Krisztian Balog
University of Stavanger
krisztian.balog@uis.no

Robert Neumayer
NTNU Trondheim
robert.neumayer@idi.ntnu.no

ABSTRACT
We develop and make publicly available an entity search test collection based on the DBpedia knowledge base. This includes a large number of queries and corresponding relevance judgments from previous benchmarking campaigns, covering a broad range of information needs, ranging from short keyword queries to natural language questions. Further, we present baseline results for this collection with a set of retrieval models based on language modeling and BM25. Finally, we perform an initial analysis to shed light on certain characteristics that make this data set particularly challenging.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval
Keywords
Entity retrieval, test collections, semantic search, DBpedia
1. INTRODUCTION
Many information needs revolve around entities as has been observed in different application domains, including question answering [14, 21], enterprise [1], and web [19] search. This is reflected by the recent emergence of a series of benchmarking campaigns focusing on entity retrieval evaluation in various settings. The INEX 2007-2009 Entity Retrieval track [8, 9] studies entity retrieval in Wikipedia. The Linked Data track at INEX 2012 also considers entities from Wikipedia, but articles are enriched with RDF properties from both DBpedia and YAGO2 [22]. The TREC 2009-2011 Entity track [1, 3] defines the related entity finding task: return homepages of entities, of a specified type, that engage in a specified relationship with a given source entity. In 2010, the Semantic Search Challenge introduced a platform for evaluating ad-hoc queries, targeting a particular entity, over a diverse collection of Linked Data sources [11]. The 2011 edition of the challenge presented a second task, list search, with more complex queries [4]. Finally, the Question Answering over Linked Data challenge fo-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

cuses on natural language question-answering over selected RDF datasets, DBpedia and MusicBrainz [14].
Finding new challenges and tasks for entity search was one of the main topics of discussion at the recently held 1st Joint International Workshop on Entity-oriented and Semantic Search (JIWES) [2]. The following action points were identified as important priorities for future research and development:
(A1) Getting more representative information needs and favouring long queries over short ones.
(A2) Limiting search to a smaller, fixed set of entity types (as opposed to arbitrary types of entities).
(A3) Using test collections that integrate both structured and unstructured information about entities.
In this paper we address the above issues by proposing an entity search test collection based on DBpedia. We synthesise queries from all these previous benchmarking efforts into a single query set and map known relevant answers to DBpedia. This results in a diverse query set ranging from short keyword queries to natural language questions, thereby addressing (A1). DBpedia has a consistent ontology comprising of 320 classes, organised into a 6 levels deep hierarchy; cf. (A2). Finally, as DBpedia is extracted from Wikipedia, there is more textual content available for those who wish to combine structured and unstructured information about entities, thereby addressing (A3).
On top of all these, there is one more important, yet still open question: To what extent can methods developed for a particular test set be applied to different settings? To help answer this question we evaluate standard document retrieval models (language models and BM25) and some of their fielded extensions. We make the somewhat surprising finding that, albeit frequently used, none of these extensions is able to substantially and significantly outperform the document-based (single-field) models. Our topic-level analysis reveals that while often a large number of topics is helped, an approximately identical number of topics is negatively impacted at the same time. Developing methods that can realise improvements across the whole query set appears to be an open challenge.
Our contributions in this paper are threefold. First, we create and make publicly available a data set for entity retrieval in DBpedia.1 Second, we evaluate and compare a set of baseline methods on this data set. Third, we perform a topic-level analysis and point out certain characteristics that make this data set particularly challenging.
The remainder of this paper is organised as follows. In Section 2 we introduce our test collection. Next, in Section 3 we present and evaluate baseline methods. This is followed by a topic-level analysis in Section 4. We summarise our findings in Section 5.
1http://bit.ly/dbpedia-entity

737

2. TEST COLLECTION
We consider a range of queries from various benchmarking evaluation campaigns and attempt to answer them using a large knowledge base. In our case this knowledge base is DBpedia, as described in Section 2.1. Further, we describe both queries and relevance judgements in Section 2.2. To conclude the description of the test collection, we give an overview of the evaluation metrics we use in Section 2.3
2.1 Knowledge base
We use DBpedia as our knowledge base, specifically, version 3.7. DBpedia has--apart from being one of the most comprehensive knowledge bases on the web--the advantage of using a consistent ontology to classify many of its entities via a type predicate. The ontology defines 320 classes, organised into a 6 levels deep hierarchy. This version of DBpedia describes more than 3.64M entities, of which 1.83M are classified in the DBpedia ontology.
2.2 Queries and relevance assessments
We consider queries from the following benchmarking evaluation campaigns (presented in temporal order):
· INEX-XER: The INEX 2009 Entity Ranking track seeks a list of entities (e.g., "US presidents since 1960"), where entities are represented by their Wikipedia page [9]. We map Wikipedia articles to the corresponding DBpedia entry.
· TREC Entity: The related entity finding task at the TREC 2009 Entity track focuses on specific relationships between entities (e.g., "Airlines that currently use Boeing 747 planes") and requests entity homepages from a Web corpus to be retrieved [1]. The Wikipedia page of the entity may also be returned in the answer record; we mapped these to the corresponding DBpedia entry.2 We use 17 out of the original 20 queries as for the remaining 3 queries there are no relevant results from DBpedia.
· SemSearch ES: Queries in the ad-hoc entity search task at the 2010 and 2011 Semantic Search Challenge refer to one particular entity, albeit often an ambiguous one (e.g., "Ben Franklin," which is both a person and a ship), by means of short keyword queries. The collection is a sizeable crawl of Semantic Web data (BTC-2009) [4, 11]. DBpedia is part of this crawl, in fact, 59% of the relevant results originate from DBpedia. 130 queries (out of the total of 142) have relevant results from DBpedia.
· SemSearch LS: Using the same data collection as the ES task, the list search task at the 2011 Semantic Search Challenge targets a group of entities that match certain criteria (e.g., "Axis powers of World War II") [4]. Out of the original 50 queries, 43 have results from DBpedia.
· QALD-2: The Question Answering over Linked Data challenge aims to answer natural language questions (e.g., "Who is the mayor of Berlin?") using Linked Data sources [14]. We used the query sets that were developed for DBpedia, and collapsed both training (100) and testing (100) queries into a single set. We filtered out queries where answers are not DBpedia pages (for example, "How many students does the Free University in Amsterdam have?" where the answer is a number). This leaves us with 140 queries in total.
2In the 2010 edition, Wikipedia pages are not accepted as entity homepages, therefore, those results cannot be mapped to DBpedia with reasonable effort. We did not include REF 2011 queries as the quality of the pools there is found to be unsatisfactory [3].

Table 1: Queries used for experimental evaluation.

Query set
INEX-XER TREC Entity SemSearch ES SemSearch LS QALD-2 INEX-LD
Total

#queries
55 17 130 43 140 100
485

avg(|q|)
5.5 6.7 2.7 5.4 7.9 4.8
5.3

avg(#rel)
29.8 13.1
8.7 12.5 41.5 37.6
27.0

· INEX-LD: The ad-hoc search task at the INEX 2012 Linked Data track uses IR-style keyword queries (e.g., "England football player highest paid") over a collection of Wikipedia articles enriched with RDF properties from both DBpedia and YAGO2 [22]. We mapped relevant Wikipedia pages to DBpedia; all 100 of the original queries were usable .
The selection above covers a broad range of information needs, ranging from short keyword queries to natural language questions. In all cases, we use only the keyword part of the query and ignore any additional markup, type information, or other hints (like example entities) that may be available as part of the topic definition according to the original task setup. Also, we take relevance to be binary, that is, both relevant and primary for the TREC Entity queries, and fair and excellent for SemSearch queries count as correct. We normalised all URIs to conform with the encoding used by the official DBpedia dump, replaced redirect pages with the URIs they redirect to, and filtered out URIs that are not entity pages (e.g., categories or templates). Table 1 provides an overview.
2.3 Evaluation metrics
We use standard IR evaluation metrics: Mean Average Precision (MAP) and Precision at rank 10 (P@10). To check for significant differences between runs, we use a two-tailed paired t-test and write / and / to denote significance at the 0.05 and 0.01 levels, respectively.
3. BASELINE METHODS AND RESULTS
This section presents our baseline methods (Section 3.2), followed by and experimental comparison (Section 3.3). We start out by introducing our experimental setup (Section 3.1).
3.1 Experimental setup
We indexed all entities that have a label (i.e., a "name") but filtered out redirect pages. We considered the top 1000 most frequent predicates as fields; this was done to ensure that all fields occur in sufficiently many entity descriptions. Note that this number is two magnitudes larger than what was considered in prior work (6 in [17] and 11 at most in [12, 13]). We employ a heuristic to identify title fields; following [16], attributes names ending in "label", "name," or "title" are considered to hold title values. For each entity, we store a content field, collapsing all its predicates. We kept both relations (i.e., links pointing to other DBpedia pages) and resolved relations (i.e., replacement of the link with the title of the page it points to) in our index.
3.2 Baseline methods
We consider two sets of baseline methods. One is based on language modeling the other is based on BM25. This particular choice

738

Table 2: Results and baseline comparison. Significance for rows 2-4 is tested against row 1; for rows 6-7 tested against row 5.

Model
LM MLM-tc MLM-all PRMS
BM25 BM25F-tc BM25F-all

INEX-XER MAP P@10

.1672 .1585 .1589 .1897

.2618 .2345 .2273 .2855

.1830 .2891 .1720 .2655 .1810 .2836

TREC Entity MAP P@10

.0970 .0855 .0641 .1206

.1294 .1176 .0882 .1706

.0882 .1000 .0848 .0882 .0824 .0824

SemSearch ES MAP P@10

.3139 .3541 .3010 .3228

.2508 .2838 .2454 .2515

.3262 .2562 .3337 .2631 .3286 .2585

SemSearch LS MAP P@10

.1788 .1738 .1514 .1857

.1907 .1744 .1581 .2093

.1785 .2116 .1718 .2163 .1789 .2163

QALD-2 MAP P@10

.1067 .0989 .1204 .1050

.0507 .0507 .0593 .0693

.1184 .0657 .1067 .0621 .1189 .0686

INEX-LD MAP P@10

.1057 .1044 .0857 .0840

.2360 .2320 .1850 .2030

.1178 .2470 .1169 .2490 .1155 .2470

Total MAP P@10

.1750 .1813 .1668 .1764

.1816 .1847 .1639 .1862

.1856 .1936 .1820 .1922 .1855 .1942

is made because we consider both families of methods state-of-theart that are frequently applied in the context of various entity search tasks, see, e.g., [5­7, 10, 15, 18]. Here, we confine ourselves to a basic approach where a (fielded) document-based representation is built for each entity. This representation makes limited use of entity-specific features, such as type information and related entities; we leave these to future work.
Specifically, we use the following language modeling based methods: LM: the standard query likelihood approach [23]; MLM-tc: the Mixture of Language Models [17], with two fields: title and content. Following [16] we set the title weight to 0.2 and the content weight to 0.8; MLM-all: the Mixture of Language Models [17], where all fields are considered with equal weight; PRMS: the Probabilistic Retrieval Model for Semistructured Data. The difference to MLM-all is that field weights are determined dynamically for each query term [13]. All methods use Dirichlet smoothing with the smoothing parameter set to the average (document or field) representation length.
We also use BM25: with standard parameter settings (k1 = 1.2, b = 0.8) [20]; BM25F-tc: the fielded extension of BM25 [20], we consider title and content fields, the title weight is set to 0.2 and the content weight to 0.8 [16]; BM25F-all: all fields are considered with equal weight. We use the same b value for all fields in the fielded variant BM25F, analogous to [18].

4. ANALYSIS
In this section we perform a topic-level analysis in order to gain some insights into the differences between the various methods (or lack of thereof). Given the space limitations, we focus on (some of) the LM-based approaches; also, according to Table 2 these exhibit more differences than their BM25-based counterparts.
We compare the MLM-all (fielded language models, with equal field weights) to the baseline (single-field) LM method in Figure 1 and to a more sophisticated PRMS method (with query term-specific field weighting) in Figure 2. In both figures the X-axis represents individual query topics, ordered by AP differences (shown on the Y-axis). MLM-all is taken to be the baseline, that is, positive values indicate that the other method outperforms MLM-all and negative values mean the advantage of MLM-all on that particular topic.
First, we observe that a large number of topics is affected, esp. on the easier query subsets (Figures 1(a)­1(d) and 2(a)­2(d)). These improvements, however, do not add up; many of the topics that are improved by moving from LM to MLM-all are hurt when a transition from MLM-all to PRMS is made. When looking into the individual topics with little to no performance differences (i.e., the ones "in the middle" of Figures 1(e)­1(f) and 2(e)­2(f)) we find that both methods that are being considered do equally bad on these topics--in many cases they fail to find any relevant results.

3.3 Results
Table 2 reports the results. We observe that the various query sets exhibit different levels of difficulty; this is indeed what we would have liked to achieve by considering different types of information needs. SemSearch ES queries (that look for particular entities by their name) are the easiest ones, while natural language queries (TREC Entity, QALD-2, and INEX-LD) represent the difficult end of the spectrum. List-type queries (INEX-XER and SemSearch LS) stand halfway in between, both in terms of query formulation (mixture of keyword and natural language) and retrieval difficulty. While a direct comparison of the scores to the official results of these benchmarks is not possible (due to the different collection used and/or that only a subset of the original queries is used here), based on manual inspection of a randomly selected subset, these results appear to be very reasonable.
When looking for significant differences in Table 2, we cannot find many. MLM-tc represents the only case when a significant improvement is observed on the whole query set; the absolute score difference compared to LM, however, is less then 5% and most likely it is a consequence of the improvements on a particular subset of queries (SemSearch ES). In all other cases, there is either no significant improvement or only a given subset of queries are significantly helped while another subset is significantly hurt.

5. SUMMARY
In this paper we made several contributions to three main topics that were identified as important priorities for future research and development for the field of entity search [2]: (A1) getting more representative information needs and favouring long queries over short ones, (A2) limiting search to a smaller, fixed set of entity types (as opposed to arbitrary types of entities), and (A3) using test collections that integrate both structured and unstructured information about entities.
We developed and made publicly available a test collection based on DBpedia and synthesised queries from a number of previous benchmarking evaluation efforts, resulting in a set of nearly 500 queries and corresponding relevance judgments. To initiate further research, we provided baseline results and showed some of the limitations of existing methods based on language models and BM25. Additionally, we provided topic-level analysis and insights on how the choice of retrieval models is bound to the characteristics of different query sub-sets.
The resources developed as part of this study are made available at http://bit.ly/dbpedia-entity. It is our plan to maintain "verified" experimental results, a list of papers using this test collection, and pointers to additional related resources (e.g., source code) at the same website.

739

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6
(a)

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6
(b)

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6

(c)

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6
(d)

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6

(e)

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6
(f)

Figure 1: Topic-level differences for LM vs. MLM-all. Y-axis displays AP differences. Positive values indicate LM is better. 1(a) INEX-XER, 1(b) TREC Entity, 1(c) SemSearch ES, 1(d) SemSearch LS, 1(e) QALD-2, 1(f) INEX-LD.

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6
(a)

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6
(b)

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6

(c)

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6
(d)

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6

(e)

0.8 0.6 0.4 0.2
0 -0.2 -0.4 -0.6
(f)

Figure 2: Topic-level differences for PRMS vs. MLM-all. Y-axis displays AP differences. Positive values indicate PRMS is better. 2(a) INEX-XER, 2(b) TREC Entity, 2(c) SemSearch ES, 2(d) SemSearch LS, 2(e) QALD-2, 2(f) INEX-LD.

References
[1] K. Balog, A. P. de Vries, P. Serdyukov, P. Thomas, and T. Westerveld. Overview of the TREC 2009 entity track. In Proc. of the 18th Text REtrieval Conference (TREC'09). NIST, 2010.
[2] K. Balog, D. Carmel, A. P. de Vries, D. M. Herzig, P. Mika, H. Roitman, R. Schenkel, P. Serdyukov, and T. Tran Duc. The first joint international workshop on entity-oriented and semantic search (JIWES). SIGIR Forum, 46(2), December 2012.
[3] K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the TREC 2011 entity track. In Proc. of the Twentieth Text REtrieval Conference (TREC'11). NIST, 2012.
[4] R. Blanco, H. Halpin, D. M. Herzig, P. Mika, J. Pound, H. S. Thompson, and T. T. Duc. Entity search evaluation over structured web data. In Proc. of the 1st International Workshop on Entity-Oriented Search (EOS'11), pages 65­71, 2011.
[5] R. Blanco, P. Mika, and S. Vigna. Effective and efficient entity search in rdf data. In Proceedings of the 10th international conference on The semantic web - Volume Part I, ISWC'11, pages 83­97. SpringerVerlag, 2011.
[6] M. Bron, K. Balog, and M. de Rijke. Ranking related entities: components and analyses. In Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM '10, pages 1079­1088. ACM, 2010.
[7] M. Bron, K. Balog, and M. de Rijke. Example based entity search in the web of data. In Proceedings of the 35th European conference on Advances in Information Retrieval, ECIR'13, pages 392­403. Springer-Verlag, 2013.
[8] A. P. de Vries, A.-M. Vercoustre, J. A. Thom, N. Craswell, and M. Lalmas. Overview of the INEX 2007 entity ranking track. In INEX, volume 4862, pages 245­251, 2008.
[9] G. Demartini, T. Iofciu, and A. P. De Vries. Overview of the inex 2009 entity ranking track. In Proc. of the Focused retrieval and evaluation, and 8th international conference on Initiative for the evaluation of XML retrieval (INEX'09), pages 254­264. Springer-Verlag, 2010.
[10] S. Elbassuoni, M. Ramanath, R. Schenkel, M. Sydow, and G. Weikum. Language-model-based ranking for queries on RDF-graphs. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM '09, pages 977­986. ACM, 2009.
[11] H. Halpin, D. M. Herzig, P. Mika, R. Blanco, J. Pound, H. S. Thompson, and D. T. Tran. Evaluating ad-hoc object retrieval. In Proc. of the International Workshop on Evaluation of Semantic Technologies
(IWEST'10), 2010.

[12] J. Kim and W. B. Croft. A field relevance model for structured document retrieval. In Proc. of the 34th European conference on Information Retrieval (ECIR'12), pages 97­108. Springer, 2012.
[13] J. Kim, X. Xue, and W. Croft. A probabilistic retrieval model for semistructured data. In Proc. of the 31st European Conference on Information Retrieval (ECIR'09), pages 228­239. Springer, 2009.
[14] V. Lopez, C. Unger, P. Cimiano, and E. Motta. Evaluating question answering over Linked Data. Journal of Web Semantics, to appear.
[15] R. Neumayer, K. Balog, and K. Nørvåg. On the modeling of entities for ad-hoc entity search in the web of data. In Proc. of the 34th European Conference on Information Retrieval (ECIR'12), pages 133­ 145. Springer, 2012.
[16] R. Neumayer, K. Balog, and K. Nørvåg. When simple is (more than) good enough: Effective semantic search with (almost) no semantics. In Proc. of the 34th European Conference on Information Retrieval (ECIR'12), pages 540­543. Springer, 2012.
[17] P. Ogilvie and J. Callan. Combining document representations for known-item search. In Proc. of the 26th annual international ACM SIGIR conference on Research and development in Information Retrieval (SIGIR'03), pages 143­150. ACM, 2003.
[18] J. R. Pérez-Agüera, J. Arroyo, J. Greenberg, J. P. Iglesias, and V. Fresno. Using BM25F for semantic search. In Proc. of the 3rd International Semantic Search Workshop (SemSearch'10), pages 1­8, 2010.
[19] J. Pound, P. Mika, and H. Zaragoza. Ad-hoc object retrieval in the web of data. In Proc. of the 19th international conference on World wide web (WWW'10), pages 771­780. ACM, 2010.
[20] S. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval, 3:333­389, 2009.
[21] E. Voorhees. Overview of the TREC 2004 question answering track. In Proc. of the 13th Text Retrieval Conference (TREC'04). NIST, 2005.
[22] Q. Wang, J. Kamps, G. Ramirez Camps, M. Marx, A. Schuth, M. Theobald, S. Gurajada, and A. Mishra. Overview of the INEX 2012 linked data track. In CLEF 2012 Evaluation Labs and Workshop, Online Working Notes, 2012.
[23] C. Zhai. Statistical language models for information retrieval a critical review. Found. Trends Inf. Retr., 2:137­213, 2008.

740

Document Features Predicting Assessor Disagreement

Praveen Chandar
Department of Computer & Information Sciences University of Delaware Delaware, USA pcr@cis.udel.edu

William Webber
College of Information Studies University of Maryland Maryland, USA wew@umd.edu

Ben Carterette
Department of Computer & Information Sciences University of Delaware Delaware, USA carteret@cis.udel.edu

ABSTRACT
The notion of relevance differs between assessors, thus giving rise to assessor disagreement. Although assessor disagreement has been frequently observed, the factors leading to disagreement are still an open problem. In this paper we study the relationship between assessor disagreement and various topic independent factors such as readability and cohesiveness. We build a logistic model using reading level and other simple document features to predict assessor disagreement and rank documents by decreasing probability of disagreement. We compare the predictive power of these document-level features with that of a meta-search feature that aggregates a document's ranking across multiple retrieval runs. Our features are shown to be on a par with the meta-search feature, without requiring a large and diverse set of retrieval runs to calculate. Surprisingly, however, we find that the reading level features are negatively correlated with disagreement, suggesting that they are detecting some other aspect of document content.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and software--performance evaluation.
General Terms
Measurement, performance, experimentation
Keywords
Retrieval experiment, evaluation
1. INTRODUCTION
Human assessors are used in information retrieval evaluation to judge the relevance of a document for a given topic. Assessors frequently disagree on the relevance of a document to a topic, however. A study by [7] found that the probability that a second assessor would agree with a first assessor's judgment that a document was relevant was only two in three. A survey of such studies done by [2] found similar results as well. While [7] found that assessor
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2034-4/13/07 ...$15.00.

disagreement had limited effect on the comparative evaluation of systems, it does have a major impact upon the evaluation of their absolute effectiveness. Moreover, a simulation study by [4] suggests that the effect on comparative evaluation depends upon the nature of disagreement, and that an overly liberal (or careless) assessor introduces considerable noise even to the comparison of retrieval systems.
While assessor disagreement has been frequently observed, and its effect on retrieval evaluation somewhat studied, less work has been done on the factors that lead to assessor disagreement. [9] observes that there is great variability in disagreement between different assessor pairs and on different topics. Regarding assessor-level effects, [8] find that assessor training has little effect on reliability (legally trained assessors no more than untrained assessors on ediscovery tasks). Regarding topic-level effects, [11] find that more detailed assessor instructions do not seem to increase disagreement.
In addition to assessor-level and topic-level effects on assessor disagreement, there may be document-level effects: some documents may be more likely to provoke assessor disagreement than others. [10] have begun work in this direction, using metarank information across multiple runs to predict disagreement. If one assessor finds a document relevant, but it is generally lowly ranked by retrieval systems, then a second assessor is likely to disagree with the original assessor, and conversely with originally-irrelevant but highly-ranked documents.
In the current paper, we investigate the relation between assessor disagreement and various topic-independent document features. One set of such features are various metrics of the reading level or reading difficulty of a document. Our hypothesis is that documents that are more difficult to read will provoke higher levels of assessor disagreement. We also consider document length (hypothesizing that longer documents will provoke more disagreement) and document coherence (hypothesizing that less coherent documents will provoke more disagreement). Finally, we extend the metarank method of [10] by considering not only average rank across different retrieval systems, but also the variability in the ranking--using disagreement between retrieval systems as a predictor of disagreement between human assessors.
If reliable document-level predictors of assessor disagreement can be found, then they can be used to efficiently direct multiple assessments towards those documents most likely to provoke assessor disagreement. We consider this as a ranking problem, in which documents must be ranked by decreasing probability of assessor disagreement, examining the case in which this ranking must be made without any initial relevance assessment having been performed. Our experimental results indicate that document-level features give a significant improvement over random choice in predicting assessor disagreement. Moreover, where initial relevance

745

assessments are not available, document-level features predict assessor disagreement as strongly as meta-rank features, without requiring a large and diverse set of retrieval runs to calculate.
One surprise of the study is that while reading level features are predictive of assessor disagreement, the correlation is the opposite of that posited in our hypothesis: documents scored as easier to read are more, not less, likely to provoke assessor disagreement than those scored as difficult to read. This suggests that reading level features are themselves correlated with some other aspect of document construction or content, which if more directly identified could lead to even stronger predictors of assessor disagreement; a question which is left to future work.
The remainder of the paper is structured as follows. A description of our logistic regression model along with all the documentlevel features is given in Section 2. Section 3 describes our experiments along with the dataset used in this work, and a detailed analysis of our results is given in Section 4. Section 5 summarizes our findings and sketches future work.

2. ASSESSOR DISAGREEMENT
Our approach to the problem of predicting assessor disagreement consists of two main components: identifying features, and developing a modeling technique.

2.1 Logistic regression

We predict the probability that a document will attract divergent binary relevance assessments from two or more assessors (D), based upon various document level features s = si , as p(D = 1|s). As we are predicting a probability, it is natural to apply a logistic regression to this problem:

p(D

=

1|s)

=

e0+Pi isi 1 + e0+Pi isi

(1)

where si is the score for feature i, and the probability p is the predicted value. The fitted value 0 in Equation 1 is the intercept, which gives the log-odds of disagreement when the score is 0, while i is the score coefficient for feature i, which gives the change or "slope" in log odds of disagreement for every one point increase in the given feature scores. The slope gives the strength of relationship between feature scores and probability of disagreement, while intercept the shifts the regression curve up or down the score axis.
A model can be built for each topic individually, or a universal model can be built using all queries in our dataset. The degree to which a universal model is a good approximation for per-topic models depends upon the strength of per-topic factors in influencing disagreement. The closer the universal model is to the pertopic models, the more likely it is that a generalized model can be built, that is able to predict assessor disagreement on new collections based only the feature scores.

2.2 Document Features
In this section, we discuss in detail the various predictors that we use in Equation 1 to estimate assessor disagreement. The logistic model described in Section 2.1 relies heavily on the feature scores and identifying good predictors of disagreement is critical. We use a combination of simple document characteristic features and reading level features to estimate disagreement.

2.2.1 Simple Document Features
The simple document quality features are described below:

- docLength Total number of terms in a document is a simple feature that estimates the amount of information available in the document.

- aveWordLength Average word length (number of characters) is a very simple estimate of readability of a document.

- Entropy An estimate of document cohesiveness can be obtained using the entropy of the document [3]. Document entropy is computed over the words in the document as follows:

E (D) = - X P (w)log(P (w))

(2)

wD

where P (w) can be estimated by the ratio of frequency of the word to the total number of words in the document. Lower entropy reflects a document that is focused on a single topic, while higher entropy indicates a more diffuse document.

2.2.2 Reading Level Features
We employ a number of standard metrics of reading level, based upon simple textual statistics. More complicated statistical and language model approaches are left for future work [5].

- FleschIndex and Kincaid are designed to capture the comprehension level of a passage. The two measures use word and sentence length with different weighting factors. FleschIndex is a test of reading ease with higher scores indicating text that is easier to read. Kincaid is a grade score that is negatively correlated to FleschIndex. A generic formula for both metrics is given below:

a

·

words sentences

+

b·

syllables words

+c

(3)

where the values of a,b, and c are as follows: FleschIndex (a = -1.01, b = -84.6, c = 206.83) and Kincaid (a = 0.39, b = 11.8, c = -15.59).

- FogIndex relies on average sentence length and the percentage of complex words for each passage of 100 words. Words with three or more syllables are identified as complex words.

"

#

0.4

,,

words sentences

«

+

100

complexWords words

(4)

- SMOG (Simple Measure of Gobbledygook) was designed as an easier and more accurate substitute to FogIndex, and is more prevalent in the medical domain. It relies on two factors: the number of polysyllables (words with 3 or more syllables) and the number of sentences.

r 1.043 numOfPolysyllables

×

30 sentences

+

3.129

(5)

- Lix is a simple measure of readability computed by adding average sentence length and number of long words. Words with 6 or more letters are considered as long words.

words sentences

+

(longwords × 100) words

(6)

- ARI (Automated Readability Index) is computed by combining the ratio of the number of characters per word and number of words per sentence. ARI relies on the number of characters per word instead of syllables per word.

4.71 characters + 0.5 words - 21.43 (7)

words

sentences

746

- Coleman-Liau is very similar to the ARI, computed by a linear combination of average number of letters per 100 words and average number of sentences per 100 words.

0.059L - 0.296S - 15.8

(8)

where, L is the average number of letters per 100 words and S is the average number of sentences per 100 words.

2.2.3 Metarank Feature
[10] propose using the metarank of a document across multiple retrieval runs as a predictor that a second assessor would disagree with an original assessor, given the original assessor's judgment. The metarank method used was the meta-AP score of [1], which is a document's implicit average precision (AP) weight in a ranking. [10] used average meta-AP score as their predictor. We add to this, maximum meta-AP score and standard deviation of metaAP scores, the last of which is a measure of the disagreement between retrieval systems over what rank a document should be returned at. Note also that [10] assume that the assessment of the original assessor was available, and build separate models for the originally-relevant and originally-irrelevant conditions; in this paper, however, we assume no assessments have been made, and build a single model to predict assessor disagreement.

3. EXPERIMENT DESIGN

3.1 Data
We use the multiply-assessed TREC 4 AdHoc dataset described by [7]. The dataset consists of 48 topics, with up to 200 relevant and 200 irrelevant pooled documents selected for multiple assessment by two alternative assessors, additional to the assessment of the topic author (who we refer to as the original assessor). We restrict ourselves only to documents from the Associated Press subcollection to avoid biases introduced by the non-random method of selecting documents for multiple assessment, and follow [7] in dropping Topics 201 and 214, as the original assessor found no documents relevant for the former, and the first alternative assessor found none relevant for the latter. We regard the assessment of a document as "disagreed" if the three relevance assessors do not all give the same assessment; this is the condition that our model will attempt to predict.
3.2 Implementation
We build per-topic models (Section 2.1) for performing feature analysis (Section 4.1), but a universal model for ranking by predicted disagreement (Section 4.2), since we assume that it is redundant to perform multiple assessments just to train up per-topic models in practice; learning-to-rank methods that adapt models for topics is left to future work. The model construction and evaluation method used in the disagreement ranking stage is described below.

- Normalization - Prior research has found the range of reading level scores to vary greatly with the topic [6]. It is a reasonable approach to normalize feature scores, making scores and models more stable across topics. We use the following L1 normalization to normalize the scores of each feature for each topic.

norm(x) = x/sum(x)

(9)

- Cross-Validation - We test the generalizability of the predictive ranking method using cross-validation. The dataset of

48 topics is split into 5 folds; one fold is held out for testing, and the other four used to develop a universal model. This avoids having a topic in both training and testing sets.
- Training - Each query in the training data is used to build a logistic model as described in Section 2.1. The maximum likelihood approach to fit the data provides us with values of intercept 0 and coefficients i in Equation 1. Finally, the intercept and coefficients of our training model are obtained by computing the mean intercept and coefficients values over all training queries.
- Testing - The feature scores are computed for each document in the unseen test query. The probability of disagreement is obtained using Equation 1 by substituting si from the computed feature scores, and intercept 0 and coefficients 1 from the trained model. Sorting documents by decreasing order of probability of disagreement gives the final ranked list.
We evaluate the quality of the rankings of documents by probability of disagreement using 11 point precision­recall curves, mean average precision, and precision at various cutoffs, with the ground truth being documents that the three assessors disagree upon the relevance of.
4. RESULTS AND ANALYSIS
We first analyze the relationship between individual features and assessor disagreement by performing per-topic regressions (Section 4.1), then investigate the usefulness of these features as predictors of disagreement by building and testing universal (crossvalidated) models (Section 4.2).
4.1 Feature Analysis
We test our hypotheses that: (1) documents with higher comprehension difficulty, (2) longer documents, and (3) documents that are less focused on a topic (less cohesive), are more likely to be disagreed upon. For each feature, we build a logistic regression model on each topic with that feature as the single predictor, and observe the coefficients that the feature achieves across the 48 topics (the  values in Equation 1). We calculate the average coefficient, and perform a two-sided, one sample t-test to test whether this coefficient differs significantly from zero across the 48 topics.
Table 4.1 reports our results. The metarank features are all highly significant. Entropy is also a significant positive predictor. In so far as entropy measures topic diffuseness, this confirms our hypothesis that more diffuse documents provoke higher levels of disagreement. Many of the reading level predictors also prove significantly correlated with disagreement. Surprisingly, however, the correlation is in the opposite direction from the hypothesis. Documents that get lower reading level scores, and therefore are marked as being easier to reading, in fact provoke higher levels of assessor disagreement. (Recall that FleschIndex is the only reading level feature where higher scores mean easier comprehension.)
4.2 Modeling Disagreement
Next, we investigate how useful our method is at predicting assessor disagreement, using a universal (cross-validated) model to rank the documents of each topic by decreasing probability of assessor disagreement. Table 4.2 summarizes performances for average precision and precision at various cutoffs. We add as a baseline the expected precision achieved by a random sorting of the documents, which is just the macroaveraged proportion of disagreed documents per topic. A universal model that combines all our

747

Predictor
FleschIndex ColemanLiau SMOGGrading Lix Kincaid ARI FogIndex
docLength aveWordLength Entropy
metaAPSum metaAPStDev metaAPMax

p-value
0.108 0.163 0.077 0.012 0.022 0.006 0.018

i
139.4 -164.4 -166.4 -241.7 -133.3 -156.0 -159.2

0.052 51.2 0.225 -374.7 < 0.001 832.1

< 0.001 < 0.001 < 0.001

159.7 206.8 321.2

Table 1: Results of significance test using two-sided one sample t-test with p-values and mean co-efficient scores across all 48 topics.

Predictor
random metaAP docLength Entropy aveWordLength ReadingLevel
All Combined

P@5
0.216 0.317* 0.229 0.258 0.200 0.246
0.321*

P@10
0.216 0.350* 0.229 0.254 0.190 0.252
0.329*

P@20
0.216 0.357* 0.235 0.241 0.215 0.229
0.341*

MAP
0.216 0.372* 0.255* 0.261* 0.240* 0.239*
0.362*

Table 2: Performance Comparison at various ranks with significant improvement over expected random scores indicated by * (paired t-test). The results are based on 5-fold cross validation across 48 topics.

features (denoted by "All Combined") and a model that uses the metarank features significantly improves over random ordering under all measures. All the other features give a significant improvement over random order for MAP only, suggesting that top-ofranking performance is mediocre. Entropy does best, as in Table 4.1, whereas the combined reading levels, despite being significant correlated with disagreement give very little benefit in terms of predicting disagreement under a universal model.
5. CONCLUSION
We started this paper with three hypotheses, namely that the documents that assessors are more likely to disagree on are: (1) documents with higher comprehension difficulty; (2) longer documents; and (3) documents that are less cohesive. At least in so far as these three conditions are captured by the measures we have used, our results have been mixed. The correlation between entropy and disagreement confirms the third hypothesis, and provides a weakly useful practical predictor of disagreement. The relationship between document length and disagreement (our second hypothesis), if it exists, is too weak for our experiments to detect as significant. Most surprisingly of all, our first hypothesis, that difficult documents would provoke more disagreement, has not only failed to be

confirmed, but in fact the reverse has been observed: it is easier documents that provoke the most disagreement.
As it seems intuitively hard to believe that it is in fact easilycomprehended documents that assessors disagree the most about, a more likely interpretation of our results is that the reading level measures are picking up some other aspect of document content, syntax, or representation that tends to provoke disagreement in assessors. An informal examination of disagreed-upon documents that attracted easy reading level scores, for instance, suggests that a disproportionate number of them are transcripts of spoken text-- presidential debates, speeches, interviews, and the like. These tend to have short sentences, but diffuse topics, and may be difficult to read quickly. Further work is to determine whether there are other text metrics that can more directly and accurately target the aspects of a document that predict assessor disagreement.
Acknowledgments: This material is based in part upon work supported by the National Science Foundation under Grant No. 1065250. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
6. REFERENCES
[1] J. A. Aslam, V. Pavlu, and E. Yilmaz. Measure-based metasearch. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval - SIGIR '05, page 571, New York, New York, USA, 2005. ACM Press.
[2] P. Bailey, P. Thomas, N. Craswell, A. P. D. Vries, I. Soboroff, and E. Yilmaz. Relevance assessment: are judges exchangeable and does it matter. In Proceedings of SIGIR, SIGIR '08, pages 667­674. ACM, 2008.
[3] M. Bendersky, W. Croft, and Y. Diao. Quality-biased ranking of web documents. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 95­104. ACM, 2011.
[4] B. Carterette and I. Soboroff. The effect of assessor error on ir system evaluation. In Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 539­546. ACM, 2010.
[5] K. Collins-Thompson and J. Callan. Predicting reading difficulty with statistical language models. J. Am. Soc. Inf. Sci. Technol., 56(13):1448­1462, Nov. 2005.
[6] J. Y. Kim, K. Collins-Thompson, P. N. Bennett, and S. T. Dumais. Characterizing web content, user interests, and search behavior by reading level and topic. In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM '12, pages 213­222, New York, NY, USA, 2012. ACM.
[7] E. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5):697­716, Sept. 2000.
[8] J. Wang and D. Soergel. A user study of relevance judgments for e-discovery. Proceedings of the American Society for Information Science and Technology, 47:1­10, 2010.
[9] W. Webber. Re-examining the effectiveness of manual review. In Proc. SIGIR Information Retrieval for E-Discovery Workshop, pages 2:1­8, Beijing, China, July 2011.
[10] W. Webber, P. Chandar, and B. Carterette. Alternative assessor disagreement and retrieval depth. In Proceeding 21st International Conference on Information and Knowledge Management - CIKM'12, pages 125­134, 2012.
[11] W. Webber, B. Toth, and M. Desamito. Effect of written instructions on assessor agreement. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, SIGIR '12, pages 1053­1054, New York, NY, USA, 2012. ACM.

748

Building a Web Test Collection using Social Media

Chia-Jung Lee
Center for Intelligent Information Retrieval School of Computer Science
University of Massachusetts, Amherst
cjlee@cs.umass.edu

W. Bruce Croft
Center for Intelligent Information Retrieval School of Computer Science
University of Massachusetts, Amherst
croft@cs.umass.edu

ABSTRACT
Community Question Answering (CQA) platforms contain a large number of questions and associated answers. Answerers sometimes include URLs as part of the answers to provide further information. This paper describes a novel way of building a test collection for web search by exploiting the link information from this type of social media data. We propose to build the test collection by regarding CQA questions as queries and the associated linked web pages as relevant documents. To evaluate this approach, we collect approximately ten thousand CQA queries, whose answers contained links to ClueWeb09 documents after spam filtering. Experimental results using this collection show that the relative effectiveness between different retrieval models on the ClueWeb-CQA query set is consistent with that on the TREC Web Track query sets, confirming the reliability of our test collection. Further analysis shows that the large number of queries generated through this approach compensates for the sparse relevance judgments in determining significant differences.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.m [Information Storage and Retrieval]: Miscellaneous--Test Collections
General Terms
Experimentation, Performance
Keywords
Test collection, social media, community question answering
1. INTRODUCTION
The most difficult part of building a test collection is perhaps creating a set of queries with associated relevance judgments. Click data can be used as a substitute in some cases,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specif c permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Figure 1: An example query with relevance judgments generated from social media data.
but this is not available for general use in academic environments. In this paper, we explore an approach to generating a set of queries and relevance judgments for a collection of web documents by exploiting the information in social media, and more specifically Community Question Answering services. CQA sites such as Yahoo! Answers1 and Baidu Zhidao2 provide social platforms for users to raise questions, to obtain useful information and to share potential answers. The collaborative nature of such platforms motivates interested users to voluntarily engage in providing useful answers to many topics. Answerers sometimes provide URLs as part of their answers. These links are used to provide additional information, to explain more complicated concepts that can not be detailed in short paragraphs, or to present reliable citations, etc. Figure 1 shows an example of an answer that incorporates several URLs.
The central hypothesis of this paper is that these links can be regarded as relevant documents for the CQA question (or query). Although the relevance judgments obtained in this way are likely to be sparse, our expectation is that the large number of queries obtained will compensate for this.
In this paper, we collected a large number of questionanswer pairs from existing and newly crawled CQA collections. We then reduced this set to include only questions whose answers contained links, where the links pointed to documents in the ClueWeb09 collection. The final ClueWebCQA (CW-CQA) set of queries and relevance judgments was produced after some additional filtering to deal with issues
1http://answers.yahoo.com/ 2http://zhidao.baidu.com/

757

such as spam. To test the validity of this new test collection, the relative effectiveness of some well-known benchmark retrieval models is evaluated and compared. Our CW-CQA results show that a term dependency model significantly outperforms a bag of words model, and pseudo-relevance feedback techniques can be helpful in most cases. These findings are consistent with the results using standard TREC Web Track query sets. As expected, the relevance judgments are sparse and incomplete. Carterette et al. [4] demonstrated that, up to a point, evaluation over more queries with fewer judgments is as reliable as fewer queries with more judgments. Similarly, we show that evaluating using a sufficient number of queries shows significant differences between retrieval models despite the incomplete relevance judgments.
The rest of paper is laid out as follows. Section 2 summarizes related work and Section 3 describes the methodology of building the CW-CQA test collection. We discuss the experimental results in Section 4. Section 5 makes closing remarks and discusses several future directions.
2. RELATED WORK
There have been several web test collections created for supporting reproducible experiments at TREC. Examples include .GOV2, WT2g, and WT10g as well as a recent larger collection ClueWeb09. For large collections, relevance judging is mostly done through pooling techniques [7]. Such techniques assemble and judge results from multiple searches and systems, with the assumption that most relevant documents will be found.
Even with the use of pooling methods, creating relevance judgments is often costly and the judged results can be biased and incomplete [2]. Buckley and Voorhees [1] proposed the metric bpref that is both highly correlated with existing measures when complete judgments are available and more robust to incomplete judgment sets. For the Million Query Track at TREC 2007, Carterette et al. [4] presented two document selection algorithms [3] to acquire relevance judgments. Their results suggested that, up to a point, evaluation over more queries with fewer judgments is more costeffective and as reliable as fewer queries with more judgments.
3. METHODOLOGY
3.1 Test Collection
We build the CW-CQA test collection using large CQA datasets and the web collection ClueWeb09 3. We obtain a large number of question-answer pairs from the CQA corpora and harvest all links provided in answers. We then reduce this set to include only questions whose answers contained links pointing to ClueWeb09 documents. We obseve that some of the links contained in CQA answers can be considered to be spam pages. To ensure a reliable test collection, we filter the reduced question sets based on two spamcontrolling parameters SR and SA. Cormack et al [5] proposed a content-based classifier that quantifies the "spamminess" of a document based on a scale of 0 to 100, where a lower score indicates that the page has a higher likelihood to be spam. Accordingly, SA calculates the average spam score of all links LQ extracted for a question Q and SR records
3http://lemurproject.org/clueweb09/

the ratio of spam links among LQ 4. Varying SR and SA affects the final number of queries and the proportion of spam links. After filtering, we can establish our final CW-CQA test collection by using the remaining questions and associated links as test queries and relevance judgments. We dicuss the parameter settings in Section 4.

3.2 Retrieval Models
We test four existing retrieval models including query likelihood model (QLM), relevance model (RM) [8], sequential dependency model (SDM) [9] and a query expansion model using latent concept expansion (LCE) [10]. We choose these models because they include both common baselines used in other papers and methods that are state-of-the-art in terms of effectiveness.
QLM computes the likelihood of generating query texts based on documents models, and can often be written as:

P (Q|D) ra=nk

log(P (qi|D))

qi Q

RM ranks documents according to the odds of their being observed in the relevant class. P (w|qi . . . qk) can be effectively used to approximate P (w|R) with w as a word in the collection.

P (D|R) P (D|N )



P (w|R) P (w|N )

wD

SDM is an effective instantiation of the Markov random field for information retrieval (MRF-IR) that makes the sequential dependence assumption. It ranks documents by:

P (D|Q) ra=nk T +O +U

qiQ fT (qi, D) qiQ fO (qi, qi+1, D) qiQ fU (qi, qi+1, D)

LCE is a robust query expansion technique based on MRFIR. This technique provides a mechanism for modeling term dependencies during expansion. The central idea is to compute a probability distribution over latent concepts using a set of pseudo-relevant documents in response to Q. Retrieval is then done by incorporating the top k latent concepts with the highest likelihood into original MRF model. Details of these models can be found in the appropriate papers.

4. EXPERIMENTS
4.1 Building The Collection
CQA datasets. We used two large CQA datasets, Yahoo Webscope L6 (Y6) 5 and a recently crawled Yahoo! Answers dataset (YA). Corpus Y6 provides a 10/25/2007 Yahoo! Answers dump. We additionally crawled the YA corpus by using the Yahoo! Answers API. Specifically, we collected up to 10,000 questions for each of the 26 Yahoo root categories as well as their corresponding answers 6. Table 1 shows the number of questions NQ, the average number of associated answers per question NAavg, and the average number of links per question NLavg in first row.
4We consider a page with spam score below 60 to be spam. 5http://webscope.sandbox.yahoo.com/ 6The collection contains approximately one month Yahoo! Answers data starting from 7/31/12 to 9/5/12.

758

log(n Qurl)
0 2 4 6 8 10
log(n Qurl)
0 2 4 6 8 10
log(n Qurl)
02468
log(n Qurl)
02468

CatA-Y6

CatB-Y6

CatA-YA

CatB-YA

y

= r

s-q3u.3a3re07=

x + 13.8248 0.9199

0

1

2

3

4

log(n url)

y

= r

s-q3u.3a7re87=

x + 13.2745 0.9147

0

1

2

3

4

log(n url)

y = -3.4497 x + 9.5577 r square = 0.9482

0.0

0.5

1.0

1.5

2.0

2.5

log(n url)

y = -3.5153 x + 8.9705 r square = 0.9808

0.0

0.5

1.0

1.5

2.0

2.5

log(n url)

Figure 2: Linear fit of the logarithms of links per question (nurl) and frequency of these questions (nQurl ).

CQA CW-CQA (CatA) CW-CQA (CatB)

NQ NAavg NLavg
NQ NLavg NQ NLavg

Y6 4,483,032 7.11 1.95 272,619 1.74 186,651 1.64

YA 216,474 3.42 1.92 8,386 1.44 5,567 1.33

Table 1: Dataset statistics.

Figure 3: Query length (x-axis) and the frequency of queries (y-axis) for query sets 10k, 3k and TREC.
Connecting CQA and ClueWeb. To find connections, we then compared the CQA links with two subsets of ClueWeb09 pages, namely Category A (CatA) and Category B (CatB), which contain approximately 500M and 50M English documents, respectively. The second and third rows in Table 1 summarize the number of questions whose answers contained links to the ClueWeb data NQ and their corresponding NLavg 7. Figure 2 shows the relation between the number of links each question has (nurl) and the frequency of questions with nurl links (nQurl ). Since this is a log-log plot, Figure 2 shows that nurl and nQurl follow a power law distribution; that is, questions with few links occupy a significant portion of entire population. R2 statistics show the goodness of the fit. The connection distributions for CatA and CatB resemble each other; in the following, we focus on evaluation using the ClueWeb09 CatB connections and searching on CatB for computational efficiency.
Queries and Relevance Judgments. To build the final test collection, we aggregate questions from Y6 and YA constrained by SR  0.1, SA  90 and nurl > 1. The constraint nurl > 1 is used to avoid the extremes in the power law distributions shown in Figure 2. A final set of 9988 questions are selected as the CW-CQA test queries and the associated links are regarded as relevance judgments. This set is denoted as the 10k set. We construct an additional query set by selecting queries from 10k that have at least one relevant document returned by any of the four retrieval models described in Section 3, resulting in the query set 3k
7Multiple appearances of the same URL for a question is considered only once.

containing 3440 questions. The maximum nurl in query sets 10k and 3k are respectively 22 and 19, and the minimum for both is 2. The relation of nurl and nQurl remains as power law distributions for both query sets. The nature of CQA questions can make the queries quite long. We apply stop structure removal techniques [6] to the CW-CQA queries and Figure 3 compares the query length distributions.
For comparison, we use 148 standard TREC web track 2009, 2010 and 2011 title and description (desc) queries. We search these TREC queries on ClueWeb09 CatB and evaluate the results using standard TREC relevance judgments. Figure 3 suggests that the CW-CQA queries are more similar to TREC description queries in terms of query length.
Retrieval Setup. Indri 8 is used for indexing and searching. We use Dirichlet smoothing with µ = 2500 for all runs without tuning. We apply spam filtering to all retrieval runs based on [5]. We evaluate using the top 1000 documents and report mean average precision (MAP), precision at 10/100 (P@10, P@100), mean reciprocal rank (MRR) and bpref.
4.2 Retrieval Performance
Table 2 shows the retrieval performance of the CW-CQA query sets 10k and 3k where the top performing runs are underlined. We perform paired t-tests on pairs of retrieval models (QLM, SDM), (QLM, RM) and (SDM, LCE). Specifically, RM and SDM are marked  if p-value < 0.05 compared to QLM. LCE is marked  if p-value < 0.05 compared to SDM. We observe that SDM significantly outperforms QLM in both query sets for every metric. RM can significantly improve QLM for most metrics, showing the utility of pseudo-relevance feedback. LCE seems to improve SDM, but the significant difference is only observed for metrics P@100 and bpref. In general, models SDM and LCE are the most effective compared to others. The performance of the 10k and 3k query sets show similar trends. For query set 3k, MRR shows that on average all models rank the first known relevant document above rank 20.
Table 3 shows the retrieval performance of TREC title and desc queries. Similar to CW-CQA results, SDM significantly outperforms QLM in all cases. For title queries, unlike CWCQA results, pseudo-relevance feedback techniques such as RM and LCE sometimes can hurt performance of QLM and SDM, respectively. The utility of pseudo-relevance feedback is more evident for desc queries. The similarity of the query length for CW-CQA queries and TREC descriptions provides a possible explanation for the higher level of consistency between their results.
In general, the relative effectiveness between retrieval models is similar for CW-CQA and TREC queries. The improvements based on term dependency modeling are significant
8http://www.lemurproject.org/indri/

759

Model MAP P@10 P@100 MRR bpref

QLM .0107 .0047 .0018 .0195 .1815

10k

SDM RM

.0114 .0051 .0019 .0114 .0050 .0019

.0208 .1866 .0204 .1942

LCE .0114 .0051 .0020 .0203 .2014

QLM .0312 .0137 .0051 .0566 .5271

3k

SDM .0331 .0149 .0054 .0605 .5417 RM .0330 .0144 .0055 .0593 .5639

LCE .0331 .0149 .0057 .0590 .5849

Table 2: Retrieval results for CW-CQA query sets 10k and 3k.

Model MAP P@10 P@100 MRR bpref

QLM .1804 .3628 .1853 .4860 .2715

title

SDM RM

.1989 .1810

.3831 .3622

.1928 .1848

.5171 .2877 .4808 .2747

LCE .2037 .3830 .1926 .4910 .2926

QLM .1309 .2892 .1147 .4559 .2953

desc

SDM RM

.1471 .1365

.2932 .2896

.1184 .1168

.4611 .4482

.3030 .2975

LCE .1463 .3000 .1214 .4537 .3049

Table 3: Retrieval results for TREC query sets.

for all query sets. The absolute retrieval performance in Table 2 is rather
low compared to Table 3. This is to be expected given the sparseness of relevance judgments. Carterette et al. [4] suggested that evaluation over more queries with fewer judgments can be reliable. Similarly, an interesting question from our perspective is: how many queries do we need to confirm the existence of significant differences between retrieval models? To this end, we compute the p-value between QLM and SDM using different number of CW-CQA queries. Specifically, from the 3k set, we randomly sample k queries where k ranges from 100 to 3400 in steps of 100. We perform 20 random samples at each k and report the average p-value in Figure 4. All metrics share a tendency that using more queries results in smaller p-values. For metrics such as MAP, MRR and P@10, stable significance (i.e., p-value < 0.05) is reached when the sample size grows beyond 2100. For other metrics such as P@100 and bpref, a sample size of more than 1000 queries is sufficient to confirm a significant difference. These observations support [4] in a sense that evaluating using a sufficient number of CW-CQA queries distinguishes retrieval model effectiveness despite incomplete judgments.
5. CONCLUSIONS
We proposed a novel way of building a test collection for web search by considering CQA questions as queries and the associated URLs as relevant documents. This approach has the advantage that a large number of queries and relevance judgments can be gathered automatically and efficiently. We filtered CW-CQA queries based on the spam scores of their links. Experimental results on the CW-CQA query sets show that the relative effectiveness between different retrieval models is consistent with previous findings using the TREC queries, showing the reliability of the test collection. The relevance judgments for the CW-CQA queries are incomplete and the absolute retrieval performance is relatively low. However, we demonstrated that evaluation using a sufficient number of queries ensures that significant

average p-value 0.0 0.1 0.2 0.3 0.4

MAP P@10 P@100 MRR bpref

p-value = 0.05

0

500

1000

1500

2000

2500

3000

3500

sample size

Figure 4: Average p-value of 20 times of sampling at each sample size.

differences can be found. These initial experimental results indicate several direc-
tions for future work. Validating consistency with manual relevance judgments will be important for our study. We plan to select a small set of queries for human assessors to judge, and compare the results with the automated approach. In addition, we will evaluate the same method using the newly constructed ClueWeb12 collection. The CW-CQA query sets for both ClueWeb09 and ClueWeb12 will be distributed through the Lemur project.
Acknowledgements
We thank Samuel Huston for his professional suggestions. This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-1160894. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
6. REFERENCES [1] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In Proc. of SIGIR, SIGIR '04, pages 25­32, 2004. [2] S. Bu¨ttcher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboroff. Reliable information retrieval evaluation with incomplete and biased judgements. In Proc. of SIGIR, SIGIR '07, pages 63­70, 2007. [3] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In Proc. of SIGIR, SIGIR '06, pages 268­275, 2006. [4] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation over thousands of queries. In Proc. of SIGIR, SIGIR '08, pages 651­658, 2008. [5] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. CoRR, abs/1004.5168, 2010. [6] S. Huston and W. B. Croft. Evaluating verbose query processing techniques. In Proc. of SIGIR, SIGIR '10, pages 291­298, 2010. [7] K. Jones, C. Van Rijsbergen, B. L. Research, and D. Dept. Report on the Need for and Provision of an Ideal Information Retrieval Test Collection. British Library Research and Development reports. 1975. [8] V. Lavrenko and W. B. Croft. Relevance based language models. In Proc. of SIGIR, SIGIR '01, pages 120­127, 2001. [9] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proc. of SIGIR, SIGIR '05, pages 472­479, 2005. [10] D. Metzler and W. B. Croft. Latent concept expansion using
markov random fields. In Proc. of SIGIR, SIGIR '07, pages 311­318, 2007.

760

Is Relevance Hard Work? Evaluating the Effort of Making

Relevant Assessments

Robert Villa
Information Retrieval Group Information School
University of Sheffield, UK
r.villa@sheffield.ac.uk

Martin Halvey
Interactive and Trustworthy Technologies Group School of Engineering and Built Environment Glasgow Caledonian University, UK
Martin.halvey@gcu.ac.uk

ABSTRACT
The judging of relevance has been a subject of study in information retrieval for a long time, especially in the creation of relevance judgments for test collections. While the criteria by which assessors' judge relevance has been intensively studied, little work has investigated the process individual assessors go through to judge the relevance of a document. In this paper, we focus on the process by which relevance is judged, and in particular, the degree of effort a user must expend to judge relevance. By better understanding this effort in isolation, we may provide data which can be used to create better models of search. We present the results of an empirical evaluation of the effort users must exert to judge the relevance of document, investigating the effect of relevance level and document size. Results suggest that "relevant" documents require more effort to judge when compared to highly relevant and not relevant documents, and that effort increases as document size increases.
Categories and Subject Descriptors
H.3.3 Information Search and Retrieval
General Terms
Measurement, Experimentation, Human Factors
Keywords
User studies, user models, relevance.
1. INTRODUCTION
The judgment of relevance has been a heavily studied topic within the Information Retrieval (IR) field, with a considerable number of papers concerned with the definition and modeling of relevance [1,2,3]. Relevance judgment is important to both the search process itself [4], and in the creation of test collections [3,5]. With regard to the latter, there is a considerable body of work which has investigated the criteria assessors use to judge relevance for the creation of text collections [3,5].
Within the context of user evaluations, the judgment of relevance is often an inseparable part of a wider information seeking process [6]. On the other hand, when generating relevance assessments for test collections, the behavior of assessors is not normally considered as important, beyond the overall time taken to create a set of relevance judgments [5,7]. Given the importance of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright © 2013 ACM 978-1-4503-2034-4/13/07...$15.00.

relevance assessment to the information seeking process, the relative lack of research studying assessors is perhaps surprising.
In this paper we address this current gap by considering the behavior and effort of relevance assessors as an important subject of study by itself. Learning more about the relevance judgment process has potential applications to a number of areas of continuing research in IR, and in particular, has potential application to user simulation and modeling. When simulating and modeling users, a range of simplifications must inevitably be used in modeling user search behavior, such as assuming that users will linearly look through a ranked list in order, from top to bottom [8]. By isolating and empirically investigating the judgment of relevance from the wider information seeking process, this study aims to provide insights which can be applied to such simulations, allowing the introduction of more realistic user behavior in a controlled manner. The approach taken is to apply a narrow, controlled, user study to one aspect of search, rather than considering the user search process as a complex undividable entity.
In this work, we investigate the following two research questions:
RQ1: Does the size of the document being judged affect the effort and accuracy of the judging process?
RQ2: Does the degree of relevance of a document to a topic affect the effort and accuracy of the judging process?
In both cases we are interested in two main responses: the effort required (including the users perceived effort), and the accuracy by which the relevance assessments can be made. In research question one the focus is on document length, while research question two considers the level of relevance. With regard to this latter question, our working hypothesis is that documents which are clearly either highly relevant or not relevant will require less effort to judge than relevant or partially relevant documents.
2. PREVIOUS WORK
Relevance has been a central focus of IR research from the inception of the field [1], with much research effort expended on defining and modeling relevance [1,2]. While research into relevance has been undertaken from a wide range of different perspectives [1,2] one important strand has been the generation of relevance judgments for use in developing sets of "qrels" for test collections [3,5]. The effort involved in the assessment process itself has not generally been a focus for this work, however, except when reflecting on the time and costs of generating relevance assessments for test collections as a whole [3], in order to minimize the test collection building effort. One exception is the work of Wang [7] which investigated the speed and perceived difficulty of relevance assessment in E-Discovery. Much past work has investigated the inter-assessor reliability of relevance

765

Figure 1: Screen shots of the experimental system showing the topic page (left), document page (center) and NASA TLX (right)

assessments [2,5], and the criteria by which assessors judge documents [5]. Relevance judgments have also been studied as part of the overall information seeking process. Again, the focus has often been on the criteria by which users judge the relevance of a document to a task [4,9] but much of this work has also investigated how a user's conception of relevance changes over time, as the search process develops [4,6,9]. The effect of relevance level (i.e. multi-dimensional relevance judgments, partial relevance) has also been studied (e.g. [2]).
From the perspective of user studies, work has also investigated the effort involved in search as a whole. For example, in [10,11] the cognitive load in search was investigated, using a secondary task to indirectly measure the user's cognitive load on a main task. In [12] the NASA TLX [13] instrument was used to measure the task load of blind and non-blind users searching on a book search web site, finding that task load correlated with task time. Many other measures of task performance and effort have also been used in interactive studies, such as time and number of queries [14].
3. EXPERIMENTAL DESIGN
3.1 Design
In the experiment we are primarily interested in manipulating two independent variables: document size and relevance level. The study was designed as a relevance judgment task only, with users being presented with a search topic, and then a document. The task of the users was then to judge the relevance of the document to the topic. As users judged documents, the system would record the user's actions, and after each judgment the user's perception of task effort was gathered using a NASA TLX [13].
3.2 Data and topics
For the purposes of this study the AQUANT collection was chosen, along with the topics and relevance assessments (qrels) from the TREC HARD task from 2005 [15]. All 50 topics were used in the study (an example topic is shown in Figure 1). The relevance assessments available in the collection had three levels: "not relevant", "relevant" and "highly relevant", with all three relevance levels being used. Two independent variables were used: relevance, with three levels corresponding to the three TREC relevance levels, and document size, also with three levels (small, medium and large).
To classify AQUANT documents into the three sizes, word counts for all documents in the qrels were generated and sorted, and then split into three equally sized groups. The ten documents closest to

the median length of each group were then extracted for each topic, and taken as representative small, median, and large documents. Not all topics had a full ten documents for each category. For the not relevant category, only documents judged as not relevant by TREC were used. The experiment used a randomized block design was used, where for each combination of document length and relevance level, a random topic and document was selected and presented to the participant.
3.3 Procedure
The study was implemented online, and was distributed to staff and students at Sheffield University, UK, as well as through social media channels. The webpages consisted of a short demographic questionnaire, a set of instructions, followed by nine topic and document combinations (Figure 1). The system first displayed the topic description to the participant, along with a button which could be used to display the document. At the bottom of the document page participants could then select the degree of relevance of the document (not relevant, relevant, and highly relevant), and then click to move on to the follow up NASA TLX. A button also allowed the participant to return to the topic description: they could move between topic and document as many times as they wished, but the view document button had to be clicked at least once.
After making a relevance judgment a NASA TLX questionnaire would be displayed. Only part 1 of the questionnaire was utilized, which is composed of six semantic differentials (mental demand, physical demand, temporal demand, performance, effort and frustration, all rated between 0 and 100). After completing this questionnaire the next topic would be displayed, and this process would continue for each of the nine topic and document combinations. No payment was made for participation. The study webpage was designed to control the size of page which would be viewed by the participant, as far as possible. On starting the study a new browser window would be opened with a specified width and height (1000x800 pixels), and a simple page design was used to ensure consistency between browsers. Not all web browsers allow a window to be fixed, although if the browser window was resized it would result in a logged event. A range of other events were also tracked, such as page scrolling.
4. RESULTS
In total 49 participants completed the survey: 27 females and 22 males with an average age of 29. The participants were multinational and all indicated that they had advanced proficiency

766

in English. In total the participants judged 409 unique documents across all 50 topics.
As much of the data analysed showed significant differences for Levene's Test, non-parametric statistical tests were used. Friedman tests were used, with pairwise comparisons made using Wilcoxon sign ranked tests (adjusted alpha = 0.0167). To determine the level of effort involved in making document judgments, both behavioral and subjective data was recorded. Behavioral data included time to make a judgment, and number of topic view clicks (i.e. number of times a user reviewed a topic). Subjective data was recorded with the NASA TLX.

4.1 Performance
Performance was compared using accuracy of judgment, true positive rate (TPR) and false positive rate (FPR), with the relevance assessments from the TREC HARD track used as the gold standard. The overall performance of users is comparable to users in a similar experimental set up by Smucker et al. [16] in terms of TPR and FPR indicating that our participants are representative. The accuracy of judgments was not influenced by document size (2(2)=0.545 p=0.761). The accuracy of judgment was influenced by document relevance level (2(2)=11.091 p=0.004). With significant differences between non-relevant and relevant documents (Z=-2.474 p=0.013) and highly relevant and relevant documents (Z=-2.889 p=0.004).

Table 1: Number of true negatives (TN), true positives (TP), false negatives (FN), false positives (FP), and accuracy of
judgment by both document size and relevance. True positive rate and false positive rate by document size are also shown.

TN TP FN FP Acc.

TPR FPR

All 119 221 72 28 0.7727 0.7543 0.1905

Sml 42 72 25 7 0.7808 0.7423 0.1429

Med 37 73 25 12 0.7483 0.7449 0.2449

Lrg 40 76 22 9 0.7891 0.7755 0.1837

Not 119 --

--

28 0.8095 --

--

Rel --

99 47 -- 0.6781 --

--

High --

122 25 -- 0.8299 --

--

4.2 Effort

4.2.1 Time and topic views
Document size had a significant effect on time to make a relevance judgment (2(2)=73.658 p<0.001). With pairwise comparisons showing differences between small and large (Z=8.030 p<0.001) and medium and large (Z=-6.439 p<0.000). Document size did not influence topic views.

Table 2: Mean time (SD) to judge document and mean topic view clicks (SD), by document size and document relevance

Time secs (SD) Topic View (SD)

Sml

63.28 (49.8)

0.40 (0.59)

Med

86.97 (78.6)

0.40 (0.58)

Lrg

145.59 (24.8)

0.33 (0.54)

Not

100.65 (20.8)

0.25 (0.48)

Rel

95.40 (85.2)

0.47 (0.61)

High

100.73 (15.3)

0.40 (0.40)

Document relevance had a significant effect on time to make a relevance judgment (2(2)=7.575 p=0.023). No significant differences were found in the pairwise comparisons at the adjusted alpha. Document relevance had an influence on topic views (2(2)=12.444 p=0.002). There were significant differences between not-relevant and relevant (Z=-3.641 p<0.000) and notrelevant and highly relevant (Z=-2.868 p=0.004).
4.2.2 Subjective Effort
Each of the 6 NASA TLX semantic differentials was compared across document size and document relevance level. The general trend for most of the categories is that demand increases as size of document increases, the exception being perceived performance where the values decrease as document size increases. For mental demand the differences were found to be significant (2(2)=21.669 p<0.001. Post hoc tests showed differences between small and large documents (Z=-4.270 p<0.001). For physical demand the differences were found to be significant (2(2)=29.903 p<0.001). Post hoc tests showed differences between small and large documents (Z=-5.370 p<0.001) as well as medium and large documents (Z=-4.440 p<0.001). For temporal demand the differences were found to be significant (2(2)=35.804 p<0.001). Post hoc showed differences between small and medium documents (Z=-3.804 p<0.001), small and large documents (Z=-5.698 p<0.001) and medium and large documents (Z=-3.476 p=0.002). Differences in effort were found to be significant (2(2)=13.386 p=0.001). Post hoc tests showed differences between small and large documents (Z=-3.732 p<0.001) and medium and large documents (Z=-2.567 p=0.010). Differences in frustration were also found to be significant (2(2)=18.922 p<0.001). Post hoc tests showed differences between small and medium documents (Z=-3.488 p<0.001) and small and large documents (Z=-4.449 p<0.001). There was also a significant difference in terms of perceived performance (2(2)=8.646 p=0.013). Post hoc tests showed differences between small and medium documents (Z=-2.476 p=0.013) and small and large documents (Z=-2.773 p=0.006).
Figure 2: Median subjective ratings for each of the 6 semantic differentials by document size (y-axis: user rating 0-100)
When the results for document relevance were analysed the general trend is that the relevant documents required the highest workload to judge. For mental demand the differences were found to be significant (2(2)=11.499 p=0.003). Post hoc tests showed differences between non-relevant and relevant documents (Z=3.445 p=0.001) and highly relevant and relevant documents (Z=2.550 p=0.011). For physical demand the differences were found to be significant (2(2)=7.154 p=0.028). Post hoc showed

767

differences between not relevant and relevant documents (Z=2.483 p=0.013). Differences in effort were found to be significant (2(2)=12.725 p=0.002). Post hoc tests showed differences between not relevant and relevant documents (Z=-3.198 p=0.001).
Figure 3: Median subjective ratings for each of the 6 semantic differentials by document relevance (y-axis: user rating 0-100)
5. DISCUSSION
Considering RQ1, which looked at the relationship between document length and both effort and accuracy, it can be seen from Table 1 (5th column) that accuracy is not affected by document size. However, looking at Table 2, it can be seen that document size does have a significant effect on how long participants took to judge a document: as might be expected, longer documents took longer to judge (Table 2, 1st column). Looking next at subjective effort, the general trend is for effort to increase as document size increases (Figure 2) with the exception of perceived performance, which shows the reverse. This suggests that participants did perceive the judging of longer documents as requiring more effort. Considering RQ2, first considering accuracy (Table 1), there were significant differences between relevant documents and both not relevant and highly relevant documents. For this latter case, Table 1 shows accuracy for relevant documents decreasing to 67.8%, from 80.1% and 83.0% for the not relevant and highly relevant cases. While a significant overall effect was found between time and document relevance level, no significant pairwise comparisons were found. Perhaps surprisingly, on average participants judged relevant documents quicker than not relevant and highly relevant, although these pairwise differences are not significant. Topic view clicks were higher for relevant documents when compared to not relevant and highly relevant, suggesting that participants tended to switch between the topic and document more when judging relevant documents. Lastly, looking at the subjective effort (Section 4.4.2), results are more complex. Looking at document relevance (Figure 3), the results suggest that it is the relevant documents which require most effort to judge (significant differences were found for mental demand, physical demand, and effort). As can be seen in Figure 3, a similar non-significant trend can be seen for temporal demand and frustration. Interestingly, for performance this trend is reversed: the trend is for users to be less secure in their performance for relevant documents.
6. CONCLUSIONS AND FUTURE WORK
From the results presented in this paper, we can make the following two conclusions: (1) document length does affect the effort required to judge a document, but does not affect the

accuracy; and (2) the degree of relevance of a document does affect both accuracy and effort: the trend is for accuracy to decrease for relevant documents, and perceived effort to increase.
Implications: simulations and evaluation metrics should take account of both document size and relevance level (where possible) when simulating users. While length does not appear to affect accuracy, it does affect effort, and simulations should take account of this. Similarly, the effort required to judge the relevance of a document varies based on its degree of relevance. In future work we aim to consider how the results of this study can be integrated into simulations of the search process.
7. REFERENCES
[1] Mizzaro. S., 1997. Relevance: The whole history. Journal of the American Society for Info. Science, 48(9), 810-832.
[2] Spink, A., Greisdorf, H., and Bateman, J. 1998. From highly relevant to not relevant: examining different regions of relevance. Inf. Process. Manage. 34, 5 (Sept 1998), 599-621.
[3] Carterette, B., Allan, J. and Sitaraman, R. 2006. Minimal test collections for retrieval evaluation. SIGIR 2006, 268­275.
[4] Tang R., Solomon P. 1998. Toward an understanding of the dynamics of relevance judgment: An analysis of one person's search behavior, Inf. Process. Manage, 34 (2-3), 237-256.
[5] Sormunen, E. 2002. Liberal relevance criteria of TREC: counting on negligible documents? SIGIR 2002, 324-330.
[6] Taylor, A. 2011. User relevance criteria choices and the information search process. IP&M. 48, 136-153.
[7] Wang, J. 2011. Accuracy, agreement, speed, and perceived difficulty of users' relevance judgments for e-discovery. SIGIR 2011 Information Retrieval for E-Discovery (SIRE) Workshop, Beijing, China, July 28, 2011.
[8] Carterette, B. 2011. System effectiveness, user models, and user utility: a conceptual framework for investigation. ACM SIGIR '11. ACM, New York, NY, USA, 903-912.
[9] Vakkari, P. 2000. Relevance and contributing information types of searched documents in task performance. SIGIR 2000, 2-9.
[10] Jacek Gwizdka. 2010. Distribution of cognitive load in Web search. J. Am. Soc. Inf. Sci. Technol. 61, 11 (Nov 10), 21672187.
[11] Gwizdka, J. (2009). Assessing Cognitive Load on Web Search Tasks. The Ergonomics Open Journal. Bentham Open Access.
[12] Iizuka, J., Okamoto, A., Horiuchi, Y., Ichikawa, A. 2009. Considerations of Efficiency and Mental Stress of Search Tasks on Websites by Blind Persons. UAHCI '09, 693-700.
[13] Hart, S.G., Staveland, L.E. 1988. Development of a NASATLX (Task load index): Results of empirical and theoretical research. In: Hancock, P.A., Meshkati, N. (eds.) Human Mental Workload, 139­183.
[14] Louise T. Su. 1992. Evaluation measures for interactive information retrieval. IP&M 28, 4 (March 1992), 503-516.
[15] Allan, J. 2005. HARD Track Overview, TREC 2005
[16] Smucker M.D., Jethani, C.P. 2011. Measuring assessor accuracy: a comparison of nist assessors and user study participants. SIGIR 2011, 1231-1232.

768

A Weakly-Supervised Detection of Entity Central Documents in a Stream

Ludovic Bonnefoy
University of Avignon CERI-LIA / iSmart
ludovic.bonnefoy@alumni. univ-avignon.fr

Vincent Bouvier
Aix-Marseille University LSIS CNRS / Kware
vincent.bouvier@lsis.org

Patrice Bellot
Aix-Marseille University LSIS CNRS
patrice.bellot@lsis.org

ABSTRACT
Filtering a time-ordered corpus for documents that are highly relevant to an entity is a task receiving more and more attention over the years. One application is to reduce the delay between the moment an information about an entity is being first observed and the moment the entity entry in a knowledge base is being updated. Current state-of-the-art approaches are highly supervised and require training examples for each entity monitored. We propose an approach which does not require new training data when processing a new entity. To capture intrinsic characteristics of highly relevant documents our approach relies on three types of features: document centric features, entity profile related features and time features. Evaluated within the framework of the "Knowledge Base Acceleration" track at TREC 2012, it outperforms current state-of-the-art approaches.
Categories and Subject Descriptors
H.3.1 [Information Storagei and Retrieval]: Information filtering
General Terms
Experimentation
Keywords
data stream, entity linking, information filtering, kba, named entity disambiguation, time
1. INTRODUCTION
Information about popular entities on knowledge bases (KB) like Wikipedia are almost updated in real-time. According to [5] the median time-lag between the first appearance of a new information about an entity and its publication on Wikipedia is 356 days. This delay may however be reduced if relevant documents are automatically found as soon as they are published and then presented to the contributors.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

A two step process is then required: entity disambiguation (resolving to which entity in the KB a name in a document is referring to) and the evaluation of the importance of the information contained in the document with regards to the given entity.
State-of-the-art approaches are highly supervised and require a training set for each entity. A real-world system, however, must work without additional examples for new entities whatever its type, its degree of popularity, and its evolution through time are.
We propose to automatically assess relevance of a document from a data stream with regards to a given entity without requiring additional data. This approach relies on three complementary types of features to capture characteristics of relevant documents: time-related, document and entities centric features.
We evaluate it in the framework provided by the "Knowledge Base Acceleration" task at TREC 2012 and it performs better than existing approaches. Moreover, we draw preliminary conclusions about characteristics of highly relevant documents, independently of which entity is monitored.
2. RELATED WORK
Thanks to the Text Analysis Conference (TAC)1 with the "Knowledge Base Population" task [7], a lot of work have been done on named entity disambiguation. Best approaches rely on similarity between a mention's context and a candidate knowledge base entry [1], name string matching [10], query expansion [6], topic modeling [14] or coreference resolution [3]. Most of theses approaches rely on computationally expensive features to evaluate the importance of the information in the documents.
Recently, named entity disambiguation in data stream have emerged relying on data from Twitter. [9] for instance followed the evolution of big and short terms events, like natural disasters, in real-time. Unfortunately, because of the characteristics of Twitter around which such approaches have been built (very short texts, hashtags, user profiles, etc. [4]), methods cannot be transposed to our problem.
A decade ago, a TREC task called "Filtering" [11] had the following definition: finding documents relevant to a query in a stream of data. Several effective approaches were inspired by information retrieval techniques to score documents (Okapi [12], Rocchio [13], ...) with the use of a learned threshold to filter out non relevant documents [15]. Most successful approaches rely on machine learning with exten-
1http://www.nist.gov/tac/2012/KBP/index.html

769

sive use of SVMs with words as features [2]. Systems were, however, in an ideal scenario: after each decision, they were notified of the annotator's label, allowing them to reevaluate their models and do not propagate their mistakes.
In 2012, a new TREC task called "Knowledge Base Acceleration" (KBA) [5] started with a similar definition: filtering a time-ordered corpus for documents that are highly relevant to a predefined list of 29 entities from Wikipedia and chosen for their ambiguity. The main differences between the two tasks are: a collection 10 times larger, various types of documents, finer-grained time unit.
As for the Filtering task, the best performing approach at KBA 2012 is highly supervised: one classifier (SVM) by entity tracked with "binary features, representing whether or not a term was present in a document, regardless of its frequency" [8]. In this setup, training data have to be provided for each new entity "followed" and even for an already monitored entity, new training examples are required to prevent a performances decrease due to concept drift through time.
Our approach, while achieving better results, differs from this work in that rather than trying to determine characteristics of a relevant document for a given entity, we focus on features of relevant documents in general.
3. LEARNING TO DETECT HIGHLY RELEVANT DOCUMENTS IN A STREAM
Let us consider a stream of documents from various types (news, forum, blogs, web pages, etc.). We want to monitor this stream, detect documents referring to a given entity e and then select highly relevant documents.
We tackle this challenge as a binary classification problem: highly relevant documents vs. mentioning, non mentioning and spams. Numerous works on text and genre classification proposed a wide range of features to associate a class to a document. Generally, the wider the set of features is, the better the results are (some approaches rely on thousands features). A good classification approach, however, have to find the good trade-off between good results (depending of the amount of features) and runtime. In our scenario, a fast approach is required to deal with the large amounts of incoming documents. Moreover, we do not want to use entity specific features. We rely on a set of 35 computationally inexpensive features falling in three categories: time related, document centric, entity's profile related.
3.1 Document centric features
The first source of information is the content of the document itself. To design effective features we looked at summarization works as they look for evidence to determine the topic(s) of a document. The frequency of the tracked entity is a first indicator about the relevance of the document. We also count the number of sentences mentionning the entity as how well distributed the occurrences are seems important. Titles, as well as beginning or ending of documents (especially news) have been shown to carry a lot of information about the main topic of a document so we count the number of mention of the entity in these different parts. We compute these features using strict name matching. How much the document is focused on a single or a few topics is important to and may be reflected by its entropy. The type of the document seems important too as different types of

documents may not follow the same "rules". The complete list of document centric features is presented in Table 1.

T F (e, d)
hasT itle(d)
T F (e, td) T F10%(e, d) T F20%(e, d) C(sent, e, d)
entropy(d)
length(d)
is(social, d)
is(news, d)

Term frequency of the entity e in d Does the document have a title? Term frequency of e in the title Term frequency of e for each 10% part of d Term frequency of e for each 20% part of d Count of sentences mentioning e Entropy of document d Count of words in d Is d a blog entry or a forum thread? Is d a news?

Table 1: Document centric features. TFs are normalized by the size of the document.

3.2 Entity related features
Previous features look at the nature of the document itself, independently of the entity considered. But how the document fits to what we know about the entity seems important too. We suppose that one representative document about the entity is provided to (or retrieved by) the system. This document will be called the "source document". In our experiments the source document of an entity is its Wikipedia page. A candidate document is judged on how much related entities appear in it and how similar the document is to the source document. We apply a named entity recogniser 2 to extract a first set of related entities; a second set is created by filtering out, from the first one, entities not embedded in a link. Similarity between documents is measure with the cosine similarity with tf-idf weigths based unigrams or bigrams and without prepossessing or smoothing. Features are listed in Table 2.

SIM1g(d, sd)
SIM2g(d, sd) T F (re, d) T F (reL, d)

Cosine similarity between d and the source document sd based on unigrams Cosine similarity with bigrams Term frequency of related entities in d Term frequency of related entities (embedded in links) in d

Table 2: Entity related features. If applicable, features are normalized by the size of the document

3.3 Time features
Exploring the impact of time related features is one of the most interesting characteristics of the studied scenario. Our hypothesis is that if something important about an entity happens, in (and for) a short time-span, the number of documents mentioning the entity may suddenly grow. We designed a set of features based on this intuition, listed in Table 3.
4. EXPERIMENTS
4.1 TREC KBA 2012 Framework
We evaluate the proposed approach within the framework of the KBA track which provided evaluation on a corpus of 500 million documents ( 9Tb). Documents are either blog or forum posts (social ), news or web pages. Documents were
2StanfordNER

770

T F (e, d).IDF (e, 1h)
DF (e, 1day) DF (e, 7d) V ar(DF (e, 7d)) T F (e, 7d) T F (e, title, 7d)

Term frequency in d and inverse document frequency for an hour Number of documents with e this day Number of documents with e in 7 days Variance of the DF in 7 days Term frequency of e in 7 days TF of e in titles in 7 days

Table 3: Time features.

crawled from October 2011 to April 2012 and to each document is associated a time-stamp corresponding to its date of crawl. For training purpose, the corpus have been split with documents from October to December as examples (with only social and news documents) and the remainder for the evaluation. The 29 entities correspond to persons and organizations that exist in Wikipedia. Two evaluations were provided: finding documents mentionning an entity and finding centrally relevant documents defined as "documents containing information which may worth a knowledge base update". In this work we focused on finding centrally relevant documents as it is the harder task. Participants must provide one ranked list of documents for each entity. The official metric was the F-measure (harmonic mean between precision and recall)3.
4.2 Classification
We proposed a set of features exploring characteristics of centrally relevant documents. A random forest classifier (RF) is composed of several decision trees, each one using a subset of the features and the examples. For a test document, the class receiving the most votes (one tree = one vote) is associated to it. We report results obtained with RF as they are among the best we got and give some insight on what features do well.
We decide to use two classifiers in cascade to evaluate documents: one for filtering out non mentioning documents and the other to dissociate poorly relevant documents from centrally relevant ones. Each classifier relies on all the features presented.
4.3 Results
Table 4 shows results of our approach against the best system at KBA 2012, the median system among participants and the mean. Our approach, listed as "All", achieves stateof-the-art results: with a score of .382, it performs better than the best 2012 TREC KBA system (+6%) and far better than median system (+32%) and mean (+73%).

Run

F-measure Run

F-measure

Our approach .382 Median KBA .289

Best KBA

.359 Mean KBA

.220

Table 4: F-measure of our approach against best, median and mean at KBA 2012.
In addition to its good performances, we claim that the huge advantage of this approach is that it does not require additional training data for new entities. The best KBA system used one SVM classifier by entity [8] with words
3Evaluation is made for different subsets of the result lists and the best score is selected. The cutoff is the same for all entities.

as features and requires training examples for each entity tested. We evaluate how well our system does without specific training data for a monitored entity by removing from the training set, examples associated to it. Under this configuration, our system gets an F-measure of .361 (reported as "1vsAll"). This result, still above the one of the best KBA system, shows that our approach succeed to capture intrinsic characteristics of centrally relevant documents, independently of the entity evaluated. The decrease may be explained by the non uniform amounts of examples associated to each entities (see [5]): setting aside training data associated to some entities can dramatically decrease the number of examples to train the models.

Run 1vsAll cross10 cross5

F-measure .361 .355 .350

Run cross3 cross2

F-measure .354 .339

Table 5: F-measure using different folds. Scores are the mean of the scores of 10 runs.

To test the robustness of our approach, with regard to the amount of training data, we evaluate several configurations by partitioning the set entities in n sets and evaluated each part with the training data associated to the others n - 1 ones (as for cross-validation). To reduce variability, for each n  {10, 5, 3, 2}, 10 runs are made and averaged results are listed Table 5 (crossn). As expected, the smaller the set of training examples is, the lower the performances are. The results are however still high and above the median and mean: for n = 10 it would be ranked 2nd and in 3rd position for n  5, 3, 2.
Random forests provide information on how well features helps to separate classes and give insight on which ones help to characterize centrally relevant documents about an entity in a stream. The mean decrease Gini score associated by a random forest to a feature is an indicator of how much this feature helps to separate documents from different classes in the trees. Figure 1 reports these scores. Not surprisingly, top 3 criterion are profile related features: the similarity of a document with the source document (the Wikipedia page of the entity) and the presence of related entities seems to be good predictors of the importance of an information in a document. In the top 10, the three types of features are represented (time, document centric and profile), showing that all the sources are complementary. Beyond this rank, features seem not to be quite useful to distinguish classes of documents. We re-evaluated our system using only these top 10 features and results confirmed this observation : .355 versus .361 with all of them. More surprisingly, the title seems useless: its presence in a document does not seems to influence the output of the classification and the presence of the entity in it is the less discriminative feature. This result is not in line with research on summarization which showed that titles are good indicators of the content of a document. The presence of an entity in a title with regards to relevance and the correlation is indeed very weak: only 53% of documents with the entity in a title are relevant. Positions of mentions in the document are not discriminative either. Finally, knowing the nature of a document does not help to take decisions even if our approach gets better results

771

for documents from news and social categories than for web pages (probably because there is no document of this type in training data).
Figure 1: Mean decrease Gini score for nonmentioning/mentioning (black) and mentioning/centrally relevant (grey)
5. CONCLUSIONS AND FUTURE WORK
We propose to detect documents containing highly relevant information about an entity in a stream of data by capturing intrinsic characteristics of these documents. This weakly supervised approach rely on time, document centric and entity profile related features. It showed state-of-the-art performances and do not require additional training data to work with new entities. Moreover, it is robust enough to still be competitive by only using half training data than state-of-the-art approaches.
Features analysis showed that using only the ten most discriminative, representing all three categories, works well. Some features based on strong evidence on others tasks were not as useful as expected: the presence of the entity in the title (which is known for being a good summary of the document) and position of the entity in documents.

A lot of things remain unexplored: the time dimension needs further investigation (is the profile of the entity must be updated over time?, does burstiness help? only for some kind of documents? ...); the characteristics of each type of document might to be considered separately; lastly, is filtering highly relevant documents is helpful for automatic knowledge base population tasks like slot-filling?
6. REFERENCES
[1] R. Bunescu and M. Pasca. Using encyclopedic knowledge for named entity disambiguation. Proceedings of the 11th Conference EACL, 2006.
[2] N. Cancedda, C. Goutte, J.-M. Renders, N. Cesa-Bianchi, A. Conconi, Y. Li, J. Shawe-Taylor, A. Vinokourov, T. Graepel, and C. Gentile. Kernel methods for document filtering. Proceedings of The 11th TREC, 2002.
[3] T. Cassidy, Z. Chen, J. Artiles, H. Ji, H. Deng, L. Ratinov, J. Han, D. Roth, and J. Zheng. Cuny-uiuc-sri tac-kbp2011 entity linking system description. Proceedings of the Fourth TAC, 2011.
[4] A. Davis, A. Veloso, A. da Silva, W. M. Jr., and A. Laender. Named entity disambiguation in streaming data. Proceedings of the 50th meeting of the ACL, 2012.
[5] J. Frank, M. Kleiman-Weiner, D. Roberts, F. Niu, C. Zhang, and C. R´e. Building an entity-centric stream filtering test collection for trec 2012. Proceedings of The 21th TREC, 2012.
[6] S. Gottipati and J. Jiang. Linking entities to a knowledge base with query expansion. Proceedings of the Conference EMNLP, 2011.
[7] H. Ji, R. Grishman, and H. Dang. Overview of the tac2011 knowledge base population track. Proceedings of the Fourth TAC, 2011.
[8] B. Kjersten and P. McNamee. The hltcoe approach to the trec 2012 kba track. Proceedings of The 21th TREC, 2012.
[9] J. Lee. Mining spatio-temporal information on microblogging streams using a density-based online clustering method. Expert Systems with Applications, 2012.
[10] P. McNamee, J. Mayfield, V. Stoyanov, D. Oard, T. Xu, W. Ke, and D. Doermann. Cross-language entity linking in maryland during a hurricane. Proceedings of the Fourth TAC, 2011.
[11] S. Robertson and I. Soboroff. The trec 2002 filtering track report. Proceedings of The 11th TREC, 2002.
[12] S. Robertson, S. Walker, H. Zaragoza, and R. Herbrich. Microsoft cambridge at trec 2002: Filtering track. Proceedings of The 11th TREC, 2002.
[13] R. Schapire, Y. Singer, and A. Singhal. Boosting and rocchio applied to text filtering. Proceedings of the 21st annual international ACM SIGIR, 1998.
[14] W. Zhang, Y. C. Sim, J. Su, and C. L. Tan. Entity linking with effective acronym expansion, instance selection and topic modeling. Proceedings of the Twenty-Second IJCAI, 2011.
[15] Y. Zhang and J. Callan. Maximum likelihood estimation for filtering thresholds. Proceedings of the 24th annual international ACM SIGIR, 2001.

772

Exploiting Semantics for Improving Clinical Information Retrieval
Atanaz Babashzadeh, Jimmy Xiangji Huang, Mariam Daoud
Information Retrieval and Knowledge Management Research Lab School of Information Technology York University, Toronto, Canada
{atanaz, jhuang, daoud}@yorku.ca

ABSTRACT
Clinical information retrieval (IR) presents several challenges including terminology mismatch and granularity mismatch. One of the main objectives in clinical IR is to fill the semantic gap among the queries and documents and go beyond keywords matching. To address these issues, in this paper we attempt to use semantic information to improve the performance of clinical IR systems by representing queries in an expressive and meaningful context. To model a query context initially we model and develop query domain ontology. The query domain ontology represents concepts closely related with query concepts. Query context represents concepts extracted from query domain ontology and weighted according to their semantic relatedness to query concept(s). The query context is then exploited in query expansion and patients' records re-ranking for improving clinical retrieval performance. We evaluate our approach on the TREC Medical Records dataset. Results show that our proposed approach significantly improves the retrieval performance compare to classic keyword-based IR model.
Categories and Subject Descriptors
H.3.3 [Information search and Retrieval]: Retrieval models, Query formulation
General Terms
Algorithms, Performance, Experimentation
Keywords
Clinical IR, Semantic relatedness, Query context modeling
1. INTRODUCTION
Due to increasing volume of digitalized medical patient records, the need for advanced information retrieval systems increases. Digitalized medical patient records contain valuable information that are usually embedded in medical reports in the form of medical history, allergies, lab results, discharge summaries and progress notes. This makes it difficult for clinicians to rapidly and effectively access the desired records and the need for effective IR system arises.
One of the main objectives in clinical IR is to fill the semantic gap between the queries and documents and go beyond keywords matching. One of the characteristics of semantic search is
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright © 2013 ACM 978-1-4503-2034-4/13/07...$15.00.

conceptual representation rather than simple keywords to enhance the representation of queries and documents. Modeling and developing domain ontologies is a fundamental framework for representing knowledge using a set of concepts and the relationships among the concepts. Medical domain knowledge is developed by several different ontologies including Unified Medical Language System (UMLS). The UMLS Metathesaurus contains millions of biomedical and health related concepts. This thesaurus is classified by concepts and connects each concept to its alternative names and views in other source vocabularies. It also determines appropriate relationships among concepts. The structural framework of UMLS ontologies aids us in categorizing concepts and locating related and similar concepts automatically. Achieving an IR model capable of exploiting semantics requires proper relatedness/similarity measures [6]. Semantic relatedness and semantic similarity in medical domain are useful measures for effective natural language processing, artificial intelligence and IR. Similarity measures determine how similar two concepts are by calculating how close they are in the UMLS semantic network hierarchy. Relatedness measures determine how related two concepts are by using concepts definition information.
Clinical IR presents several specific challenges such as granularity mismatch [12] and terminology mismatch. In [12] authors tackle the issue of granularity mismatch by exploiting subsumption relationships extracted from medical domain knowledge resources, where subsumption relationships are defined as parentchild relationships when one concept is a subset of another concept. Keyword-based IR models could return imprecise and erroneous results due to their dependency on specific keywords. Concept-based representation approaches are intended to overcome this limitation by representing queries and documents with semantic concepts using biomedical resources such as UMLS. However previous work demonstrated that these approaches should be combined with term-based approaches for better performance [9]. In [5] authors tackle the issue of terminology mismatch by proposing a technique to represent queries with only medical concepts crucial to information need. In their method, they represent queries and documents using medical concepts that are directly related to symptoms, diagnosis, diagnosis test and treatment. One of the other challenges in medical IR is to retrieve those relevant documents that don't contain queries' concepts but contain concepts that are associated with them. For example a query may contain specific disease whereas the relevant document may contain symptoms of that disease. Our proposed approach attempts to address the presented challenges by modeling meaningful and expressive query context that represents the most related concepts to query concepts.
In this paper, we propose a novel IR approach that is able to tackle the presented challenges of clinical IR such as terminology mismatch and granularity mismatch and to capture those relevant documents not containing queries' keywords explicitly. The

.

801

proposed model also attempts to fulfill the objective of filling semantic gap between queries and documents. This model is capable of combining search by meaning and search by keyword and it uses a WordNet-based [6] semantic relatedness measure to calculate relatedness of pair of concepts. In this approach we develop query domain ontology for each query using the UMLS ontology. Concepts in the domain ontology are then weighted according to their relatedness to the original query concepts. Query context contains all the concepts in query domain ontology that have a related score above a specific threshold. The query context is used to enhance classical IR model and to calculate and assign a new score to documents by considering semantics. The key unique contributions in our paper concern (1) modeling query domain ontologies to extract representative concepts to information need and (2) modeling medical query context based on the modeled ontology and weight the concepts semantically in relation to the query according to a semantic-based concept weighting schema.
2. SEMANTIC-BASED APPROACH
Our approach attempts to enhance classical IR model by exploiting semantic relatedness to fulfill the objective of searching by meaning representation of content. To achieve this, we model a query context for each query, where each concept is weighted according to its relatedness to query concept(s). The query context is then exploited in the patient records query expansion and reranking for improving clinical retrieval performance.
A prerequisite step for modeling a representative query context is modeling query domain ontology. In order to find the most qualified concepts representing query context we model and develop query domain ontology for each query using UMLS Metathesaurus. To develop query domain ontology, first we map query keywords to UMLS concepts using MetaMap [1]. MetaMap is a program developed by National Library of Medicine (NLM), which maps biomedical texts to the UMLS Metathesaurus. Second we extract all the concepts that have semantic relationship with the query concept(s) in UMLS ontologies. The Metathesaurus contains synonymous and non-synonymous relationships between concepts from the same source vocabulary as well as concepts in different vocabularies. In general, the relationships connect closely related concepts, such as those that share some common property or are related by definition. Semantic relationships can be of different types including SY (synonym), RB (broader relationship), RN (narrower relationship), SIB (sibling) etc. The main motivation for including synonymous and non-synonymous relationships for modeling query concepts domain ontology is to capture all the concepts closely related to the query concept(s). Concepts extraction is capped at second level to avoid concepts that are not in a close relation with query concepts. This ontology is capable of addressing the issue of granularity mismatch by including the parent-child relationship, where the child concept is a subset of the parent concept. This ontology is also capable of tackling the problem of retrieving those relevant documents not containing queries' keywords explicitly but containing concepts semantically related to the queries, since it includes alternative names and views of the same concept as well as concepts that are closely related to the query concept(s).
Figure 1 demonstrates Hearing Impairment domain ontology. Medical query domain ontology is capable of expressing the close relationship of diseases (Hearing Impairment and Hearing Loss) disease and symptom (Hearing Impairment and Buzzing in Ear) and disease and treatment (Hearing Impairment and Hearing Aid). Therefore, according to this ontology if a document doesn't

contain hearing impairment concept but contains the associated treatment Hearing aid, our proposed model will retrieve it since it contains a concept that is in hearing impairment domain ontology. However related concepts receive a different weight from the original query concept(s).
The query context of each query consists of all the concepts in their modeled domain ontology. In order to identify the extent of a concept relevancy in relation with query concept we weight each concept in query domain ontology according to its semantic relatedness to the query concept. For the purpose of this study we use Gloss Vector semantic [8] relatedness measure. This choice relies on the fact that this measure relies on concepts definitions information rather than semantic network hierarchy; therefore it is able to calculate semantic relatedness of pair of concepts in different UMLS ontologies. This measure represents concepts by vectors of co- occurrences. Co- occurrences refer to words that commonly occur close to each other in a corpus of text. This measure calculates relatedness of pair of concepts by combining the structure and content of WordNet with co-occurrence information. The value of semantic relatedness indicates the relatedness of pair of concepts, the higher the value the higher the semantic relatedness. The semantic relatedness of pair of concepts is between 0 and 1, the value is 1 if two concepts are identical and 0 if they are not related.

Hearing   Imapiment(C1384666)  

Hearing   Dif7iculty(C1313969)  
Complete  Hearing   loss(C0581883)  
Deafness(C0011053)  
Sensation  of  Blocked   Ears(C0522348)  
Sensorineural  Hearing   Loss(C0018784)  
Partially  Hearing   Impaired(C0237648)  
Hearing  Problem   (C0260662)  
Hearing   Disability(C0848765)  

Encounter  due  to   Problems  with  
Hearing(C0438989)  
Deafness   symptom(C0439029)  
Hearing  loss,   Bilateral  (C0018775)  
Lipreading   (C0023844)  
Hearing   Aid(C0018768)  
Neural  Hearing   Loss(C0155550)  
Blocked   Ears(C0239221)  
Ear   Disease(C0013447)  
Disease  of  Inner   Ear(C0494559)  
Cogenitial   Deafness(C0339789)  
Hearing  Impaired   Persons(C0525064)  
Middle  Ear   Deafness(C0700045)  
Buzzing  in   Ear(C0235283)  

Figure 1. Hearing Impairment Domain ontology Therefore the query context is presented as:

! = !!,  !, !! , ... !",  !, !" , ( !!!,

(1)

(!, !!!)), ... !"#,  !, !"#

0 < ,  < 

.

!"#$%&(!, !)=

!! !!
!! . !!

(2)

Ci represents the query concept, Cin represents first level concepts and Cinj represents second level concepts, where i is the number of query concepts, n is the number of first level concepts and j is a number of second level concepts. rel represents the score calculated by measuring semantic relatedness of pair of concepts using Gloss Vector measure. rel(Ci,Cinj) represents semantic relatedness of related concepts in relation with the query concept; which is assigned to each concept as its weight. The relatedness of pair of concepts is measured by calculating the cosine of the angle

802

between v1 and v2 . v1 and v2 represent the context vector of cooccurrence data of C1 and C2 [8] .
We use the modeled query context to expand the original queries where concepts are presented using their preferred terms. The preferred term is the string preferred in UMLS Metathesaurus as the name of the concept. The query expansion process is performed by integrating the keyword-based query context CTXq in to the IR process, where we use DFR-based dependence model.
Since our model can only take advantage of query terms, it is necessary to refine initial retrieval results by taking concepts into consideration [11]. To re-rank the initial results according to query context, documents are semantically indexed using MetaMap [1] and represented by concept unique identifiers (CUI).
The re-ranking process is performed by integrating the CUI-based query context CTXq into the IR process. For each document dk retrieved, we combine the initial score Si based on DFR-based dependence model and the conceptual score Sc based on cosine measure between the conceptual representation of dk and CTXq . We combine Si and Sc using a tuning parameter  to balance the impact of the original score Si and the new score Sc.
 ! = 1 -  . ! , ! + . ! !, ! 0 <  < 1 (3)

The conceptual score is computed using cosine similarity measure between the initial retrieved results and concepts in the CUI-based query context obtained from modeled query domain ontology and weighted according to their semantic relatedness to query concept(s).

! !, ! = cos( ! , !)

(4)

3. EXPERIMENTAL SETTINGS
In our search experiments, we examine the impact of combining conceptual search using query context with the standard retrieval and compare it with the standard retrieval performance using the original keyword-based queries. The standard retrieval is based on DFR-based dependence model [7]. We use Terrier [7] for indexing and retrieval of term-based representation where we apply porter stemming and we remove general stop words. We also added specific stop words that are very frequent in medical domain such as patient, hospital, medical, etc as suggested by [2].

For query context modeling we conduct experiments to find the optimal value for  and weight threshold. Query context contains concepts with weight or semantic relatedness higher than or equal to the weight threshold. To tune the parameter , we vary it in [0 1] in equation 3. Figure 2 presents performance measures in different values of . According to Figure 2 we can confirm that the best value for parameter  is 0.2. In order to find the optimal threshold for weight restrictions, we conduct different runs to evaluate the impact of this parameter on retrieval performance. Figure 3 presents the impact of modeling query context based on weights equal to or higher than 0.7, 0.75, 0.8, 0.85 and 0.9. For each value we perform conceptual re-ranking of results obtained from query expansion and results confirm that the optimal weight threshold to model query context is 0.9.

The corpus that is used to develop and test our approach is provided within the context of the TREC [10] Medical Records Track 2011 challenge and is composed of query set, patients' records and relevance judgments. Each topic specifies a particular disease/condition set and/or a particular treatment/intervention set. The patients' records are a set of de-identified electronic medical

records. These documents are organized by visits and there are a total of 17,267 visits and 101'711 reports in which, each visit contains between 1 to 415 reports. Relevance judgments are binary and reflect whether a visit is relevant or not with respect to the query.

0.6   0.5   0.4   0.3   0.2   0.1  
0   0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.8   0.9  

map   R--prec   bpref   p5   p10  

Figure 2. Comparison of parameter , X-axis indicates parameter  and Y­axis indicates value of performance measures

0.6  

0.5  

0.4  

0.3  

0.2  

0.1  

0  

0.7  

0.75  

0.8  

0.85  

0.9  

p10   p5   Rprec   bpref  

Figure 3. Comparison of weight restriction, X-axis indicates parameter weight threshold and Y­axis indicates value of performance measures
4. EXPERIMENTAL RESULTS
Our evaluation objective is to evaluate the performance of combining conceptual search with standard retrieval and compare it to standard retrieval. Our hypothesis is that combining our method of conceptual search with classic term-based IR model improves the retrieval performance. Table 1 presents the retrieval performance of the runs using TREC's official measures: MAP, R-precision, bprerf, precision@5 (P@5) and precision@10 (P@10).
The baseline-Termbased run is based on DFR-based (Divergence from Randomness) dependence model, which incorporates term dependency in the DFR framework. This run is based on sequential dependency (SD) and is performed using term-based representation of documents and queries. The conceptual-DFR run is based on re-ranking the results that are obtained from query expansion using keyword-based query context. The query expansion is performed by integrating the keyword-based query context into DFR-based sequential dependence model where concepts are presented as keywords rather than CUIs. Results are then re-ranked based on the CUI-based query context using the conceptual representation of documents. In this run documents scores are calculated based on eq. 3. This run is based on =0.2 and weight threshold of 0.9, which were found to be effective in our experimental settings.
The results show that our proposed approach (conceptual-DFR) outperforms the baseline-Termbased run at all official measures especially at P@5. This suggests the effectiveness of medical query domain ontology for extracting associated diseases, treatments and symptoms. The positive results also indicate the

803

usefulness of combing semantic search with standard retrieval model.

We also evaluate the effectiveness of our approach after applying rocchio's query expansion mechanism. The DFR-QE run is based on DFR-based sequential dependence model using rocchio's query expansion mechanism from Terrier (default values of beta=0.4, 10 expansion documents and 40 expansion terms). This run is performed using term-based representation of queries and documents. The Conceptual-DFR-QE is based on re-ranking the results obtained from our proposed query expansion and applying rocchio's query expansion mechanism. The conceptual query expansion is performed by integrating the keyword-based query context into DFR-based sequential dependence model using keyword-based query context with =0.2 and weight threshold of 0.9. The conceptual re-ranking is performed using the conceptual representation of documents where documents' scores are calculated using eq. 3.

Table 1. Retrieval performance of runs

Runs Baseline-Termbased

MAP 0.3017

Rprec 0.3309

Bpref 0.426

P@5 0.5

P@10 0.4853

Conceptual-DFR

0.3232* 0.3623* 0.437 0.5706* 0.5294* (+7%) (+9.5%) (+2.5%) (+14%) (+9%)

DFR-QE

0.3332 (+10%)

0.3585 (+8%)

0.4539 (+6.5%)

0.5353 (+7%)

0.5 (+3%)

0.3986* 0.406* 0.5595* 0.6882* 0.5971* Conceptual-DFR-QE (+32%) (+22%) (+31%) (+37%) (+23)

Best-TREC

------

0.44

0.5523

------

0.6559

As Table 1 demonstrates, Terrier's rocchio query expansion mechanism (DFR-QE) improves the retrieval performance compare to baseline. Our proposed approach (Conceptual-DFRQE) improves the results significantly at all the performance measures compare to baseline. This run also improves the retrieval performance at all the performance measures compared to DFR-QE (MAP by 19.5%, R-prec by 13%, bpref by 23%, p@5 by 28% and p@10 by 19%). Results from conceptual runs foster the effectiveness of combining the conceptual search based on proposed query context modeling with DFR-based dependence IR model.
Table 1, also lists the best-submitted run in medical TREC 2011 challenge (Best-TREC). Participants of the track did not report MAP and P@5. Comparing our best run (conceptual-DFR-QE) with the best system reported at TREC 2011 (Best-TREC), we observe that our approach performs better than the best reported system at TREC 2011 in terms of bpref. However TREC 2011 best system relies on detection of negation and conditionals, which is not handled in our approach.
Overall, our proposed approach is promising. This is because the results generated from our approach significantly (statistical significance for our approach is denoted * over BaselineTermbased in Table 1) improve the performance compared to the term-based baseline run and term-based rocchio query expansion run at all the TREC's official performance measures especially at P@5.
5. CONCLUSIONS AND FUTURE WORK
In this paper we present a novel medical query context modeling based on query domain ontology that is modeled and developed from UMLS Metathesaurus. Query domain ontology represents

concepts related to query concept(s). Concepts in the query context are weighted according to their semantic relatedness to the query concept(s). The query context is exploited for re-ranking clinical search results obtained from classical IR model. We evaluate our proposed approach on the challenging ad-hoc retrieval task of TREC medical records track. Results show that our proposed approach improves the retrieval performance at all the TREC's official measures. The results suggest the effectiveness of combining conceptual search using query context modeling with standard term-based retrieval. Experimental results also indicate the capability of query domain ontology in expressing closely related concepts. For future work, we plan to do the retrieval process based on calculating the relatedness of query concepts with document's keywords using definition information and co-occurrence information.
6. ACKNOWLEDGMENTS
This research is supported by the research grant from the Natural Sciences & Engineering Research Council (NSERC) of Canada. We thank anonymous reviewers for their thorough review comments on this paper.
7. REFERENCES
[1] A.R. Aronson, F.M. Lang. An Overview of MetaMap: Historical Perspective and Recent Advances. JAMIA, 17(3): 229-236, 2010.
[2] J. Gobeil, A. Gaudinat, E. Pasche, D. Teodoro, D. Vishnyakova, P. Ruch. BiTeM group report for TREC Medical Records Track 2011. In TREC'11.
[3] X. Huang and Q. Hu. A Bayesian Learning Approach to Promoting Diversity in Ranking for Biomedical Information Retrieval. In SIGIR'09, pages 307-314, 2009.
[4] X. Huang et al. A Platform for Okapi-based Contextual Information Retrieval. In SIGIR'06, page 728, 2006.
[5] N. Limsopatham, C. Macdonald I. Ounis. A task-specific Query and Document Representation for Medical Records Search. In ECIR'13, pages 747-751, 2013.
[6] B.T. McInnes, T. Pedersen, S.V. Pakhomov. UMLSInterface and UMLS-Similarity: Open Source Software for Measuring Paths and Semantic Similarity. In AIMA'09.
[7] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, C. Lioma. Terrier: A High Performance and Scalable Information Retrieval Platform. In OSIR'06.
[8] T. Pederson, S.V. Pakhomov, S. Patwardhan, C. Chute. Measures of Semantic Similarity and Relatedness in the Biomedical Domain. JBI, 40(3): 288-299,2007.
[9] D. Trieschnigg, D. Hiemstra, F. de Jong, W. Kraaij. A CrossLingual Framework for Monolingual Biomedical Information Retrieval. In CIKM, pages 169-178, 2010.
[10] E. Voorhes, R. Tong. Overview of the TREC 2011 Medical Records Track. In TREC'11.
[11] Z. Ye, X. Huang, H. Lin. Incorporating Rich Features to Boost Information Retrieval Performance: A SVMregression Based Re-ranking Approach. Expert Syst Appl., 36(6): 7569-7574,2011.
[12] G. Zuccon, B. Koopman, A. Nguyen, D. Vickers, L. Butt. Exploiting Medical Hierarchies for Concept-based Information Retrieval. In ADCS, pages 111-114, 2012.

804

Flat vs. Hierarchical Phrase-Based Translation Models for Cross-Language Information Retrieval

Ferhan Ture1,2, Jimmy Lin3,2,1
1Dept. of Computer Science, 2Institute for Advanced Computer Studies, 3The iSchool University of Maryland, College Park
fture@cs.umd.edu, jimmylin@umd.edu

ABSTRACT
Although context-independent word-based approaches remain popular for cross-language information retrieval, many recent studies have shown that integrating insights from modern statistical machine translation systems can lead to substantial improvements in effectiveness. In this paper, we compare flat and hierarchical phrase-based translation models for query translation. Both approaches yield significantly better results than either a token-based or a one-best translation baseline on standard test collections. The choice of model manifests interesting tradeoffs in terms of effectiveness, efficiency, and model compactness.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation
Keywords: SCFG, query translation
1. INTRODUCTION
Despite the prevalence of context-independent word-based approaches for cross-language information retrieval (CLIR) derived from the IBM translation models [4], recent studies have shown that exploiting ideas from machine translation (MT) for context-sensitive query translation produces higher-quality results [17, 19, 24]. State-of-the-art MT systems take advantage of sophisticated models with "deeper" representations of translation units, e.g., phrase-based [13], syntax-based [25, 27], and even semantics-based [11] models. In particular, hierarchical phrase-based machine translation (PBMT) systems [5] provide a middle ground between efficient "flat" phrase-based models and expressive but slow syntax-based models. In terms of translation quality, efficiency, and practicality, flat and hierarchical PBMT systems have become very popular, partly due to successful open-source implementations.
This paper explores flat and hierarchical PBMT systems for query translation in CLIR. Previously, we have shown that integrating techniques from hierarchical models lead to
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM or the author must be honored. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

significant gains in effectiveness--however, it is unclear if such gains could have been achieved from "flat" representations. This question is interesting because it opens up a different region in the design space: flat representations are faster, more scalable, and exhibit less complexity--encoding a different tradeoff between efficiency and effectiveness.
There are two main contributions to this work: First, we test the robustness of query translation techniques introduced in earlier work [24] by comparing flat and hierarchical phrase-based translation models. In addition, we examine the effects of three different heuristics for handling one-tomany word alignments. We show that a combination-ofevidence approach consistently outperforms a strong tokenbased baseline as well as a one-best translation baseline for three different languages, Arabic (Ar), Chinese (Zh) and French (Fr), using either flat or hierarchical translation grammars. Second, we discuss differences between the two MT models and provide insights on the tradeoffs each represent. Experiments show that a hierarchical translation model yields higher effectiveness, which suggests that there is value in more sophisticated modeling of linguistic phenomena.
2. BACKGROUND AND RELATED WORK
Although word-by-word translation provides the starting point for query translation approaches to CLIR, there has been much work on using term co-occurrence statistics to select the most appropriate translations [10, 15, 1, 21]. Explicitly expressing term dependency relations has produced good results in monolingual retrieval [9, 18], but extending that idea to CLIR has not proven to be straightforward. Another thread of research has focused on translating multiword expressions in order to deal with ambiguity [2, 28].
Borrowing ideas from MT for IR dates back to at least Ponte and Croft's work on retrieval using language modeling [20]. That work was later extended to translation models for retrieval [3], followed by a series of successful adaptations to the cross-language case [26, 14, 8].
As MT systems have evolved away from the token-based translation approach, researchers have started exploring ways to integrate various components of modern MT systems for better CLIR effectiveness. Magdy et al. [17] showed that preprocessing text consistently for MT and IR systems is beneficial. Nikoulina et al. [19] built MT models tailored to query translation by tuning model weights with queries and reranking the top n translations to maximize effectiveness on a held-out query set. While improvements were more substantial using the latter method, another interesting finding was the low correlation between translation and

813

retrieval quality. This indicates that better translation may not necessarily help retrieval.

2.1 Context-Independent Baseline
As a baseline, we consider the technique presented by Darwish and Oard [6]. Given a source-language query s, we represent each token sj by its translations in the target language, weighted by the bilingual translation probability. These token-to-token translation probabilities, called P rtoken, are learned independently from a parallel bilingual corpus using automatic word alignment techniques [4]. In this approach, the score of document d, given sourcelanguage query s, is computed by the following equations:

Score(d|s) = Weight(tf(sj , d), df(sj))

(1)

j

tf(sj, d) = tf(ti, d)P rtoken(ti|sj)

(2)

ti

df(sj) = df(ti)P rtoken(ti|sj)

(3)

ti

In order to reduce noise from incorrect alignments, we impose a lower bound on the token translation probability, and also a cumulative probability threshold, so that translation alternatives of sj are added (in decreasing order of probability) until the cumulative probability has reached the threshold. Any weighting function can be used in conjunction with the tf and df values, and we chose the Okapi BM25 term weighting function (with parameters k1 = 1.2, b = 0.75).

2.2 Flat vs. Hierarchical Phrase-based MT
Machine translation can be divided into three steps: training the translation model, tuning parameters, and decoding. We will mostly focus on the first step, since that is where flat and hierarchical MT approaches differ the most.
The output of the first step is the translation model (called TM hereafter). For both flat and hierarchical variants, the TM consists of a set of rules (i.e., the translation grammar) in the following format:
 = 01. . . ||  = 01. . . || A || (  )
We call the sequence of i's the source side of the rule, and sequence of j's the target side of the rule. The above indicates that the source side translates into the target side with a likelihood of (  ).1 A contains token alignments in the format i-j, indicating that source token i is aligned to target token j .
A hierarchical model [5] differs from a flat model [13] in terms of rule expressivity: rules are allowed to contain one or more nonterminals, each acting as a variable that can be expanded into other expressions using the grammar, carried out in a recursive fashion. These grammars are called synchronous context-free grammars (SCFG), as each rule describes a context-free expansion on both sides.
Consider the following two rules from an SCFG:

R1. [X] leave in europe || cong´e de [X] en europe || 1-0 2-3 3-4 || 1
R2. maternal || maternit´e || 0-0 || 0.69

1The likelihood function is not a probability density function because it is not normalized.

In R1, the non-terminal variable [X] allows an arbitrarily long part of the sentence to be moved from the left of the sentence in English to the middle of the sentence in French, even though it generates a single token (i.e., maternal) using R2 in this particular example. As a result, an SCFG can capture distant dependencies in language that may not be realized in flat models.
Each sequence of rules that covers the entire input is called a derivation, D, and produces a translation candidate, t, which is scored by a linear combination of features. One can use many features to score a candidate, but two features are the most important: the product of rule likelihood values indicates how well the candidate preserves the original meaning, TM(t, D|s), whereas the language model score, LM(t), indicates how well-formed the translation is. Combining the two, the decoder searches for the best translation:

t(1) = arg max[ max TM(t, D|s)LM(t)]

(4)

t DD(s,t)

There is a tradeoff between using either flat or hierarchical grammars. The latter provides more expressivity in representing linguistic phenomena, but at the cost of slower decoding [16]. On the other hand, flat models are faster but less expressive. Also, due to the lack of variables, flat grammars contain more rules, resulting in a more verbose translation grammar.

3. QUERY TRANSLATION WITH MT
In our previous work [24], we described two ways to construct a context-sensitive term translation probability distribution using internal representations from an MT system. These distributions can then be used to retrieve ranked documents using equations (1)­(3).

3.1 Using the Translation Model

With appropriate data structures, it is possible to efficiently extract all rules in a TM (either flat or hierarchical) that apply to a given source query, s, called TMs. For each such applicable rule r, we identify each source token sj in r, ignoring any non-terminal symbols. From the token alignment information included in the rule structure, we can find all target tokens aligned to sj . For each such target token ti, the likelihood value of sj being translated as ti is increased by the likelihood score of r. At the end of the process, we have a list of possible translations and associated likelihood values for each source token that has appeared in any of the rules. We can then convert each list into a probability distribution, called P rPBMT for flat and P rSCFG for hierarchical grammars by normalizing the sum of likelihood scores:

P rSCFG/PBMT(ti|sj ) =

1 

(r)

(5)

rTMs

sj ti in r

where sj  ti represents an alignment between tokens sj and ti and  is the normalization factor.
When a source token sj is aligned to multiple target tokens in a rule, it is not obvious how to distribute the probability mass. In our previous implementation [24], each alignment was treated as an independent event with the same probability. We call this the one-to-one heuristic, and introduce two alternatives due to the following drawback: the target tokens aligned to sj are usually not independent. For example, the token brand is aligned to three tokens marque, de,

814

fabrique (En. brand, of, factory), which is an appropriate translation when put together. Even if de is discarded as a stopword, the one-to-one heuristic will learn the token pair (brand, fabrique) incorrectly. An alternative heuristic is to ignore these rules altogether, assuming that good translation pairs will appear in other rules, thus discarding these cases would not cause any harm (we call this the one-tonone technique). A third approach is to combine the target tokens into a multi-token expression. Thus, in the above example, we would learn the translation of brand as marque de fabrique, which is a useful mapping that we might not learn otherwise. We call the third technique one-to-many, and compare these three heuristics in our evaluation.

3.2 Using N-best Translations
Given t(1), the most probable translation of query s computed by equation (4), we can score a document d as follows:

Score(d|s) = Weight(tf(t(i1), d), df(t(i1)))

(6)

i

Since MT systems generate a set of candidate translations
in the process of computing equation (4), we can consider
the n most likely candidates. For each candidate translation t(k), and for each source token sj, we use token alignments to determine which tokens in t(k) are associated with sj . If there are multiple target tokens, we apply one of the three
methods introduced previously: one-to-none, one-to-one, or one-to-many. By the end of the process, we obtain a probability distribution of translations for each sj based on the n best query translations. If source token sj is aligned to (i.e., translated as) ti in the kth best translation, the value (t(k)|s) is added to its probability mass, producing the following for P rnbest (where  is the normalization factor):

P rnbest(ti|sj )

=

1 

n

(t(k)|s)

(7)

k=1

sj ti in t(k)

3.3 Evidence Combination
For P rtoken, translation probabilities are learned from all sentence pairs in a parallel corpus, whereas P rSCFG/PBMT only uses portions that apply to the source query, which reduces ambiguity in the probability distribution based on this context. P rnbest uses the same set of rules in addition to a language model to search for most probable translations. This process filters out some irrelevant translations at the cost of less diversity, even among the top 10 or 100 translations. Since the three approaches have complementary strengths, we can perform a linear interpolation of the three probability distributions:

P rc(ti|sj ; 1, 2) =1P rnbest(ti|sj ) + 2P rSCFG/PBMT(ti|sj )

+ (1 - 1 - 2)P rtoken(ti|sj )

(8)

Replacing any of these probability distributions introduced above for P rtoken in equations (1)­(3) yields the respective scoring formula.

4. EVALUATION
We performed experiments on three CLIR test collections: TREC 2002 En-Ar CLIR, NTCIR-8 En-Zh Advanced CrossLingual Information Access (ACLIA), and CLEF 2006 En-Fr CLIR, with sizes 383,872, 388,589 and 177,452 documents,

respectively. We used the title text of the 50 topics for the Arabic and French collections, and we treated the 73 wellformed questions in NTCIR-8 as queries.
For the flat and hierarchical translation models, we used Moses [12] and cdec [7], respectively. The training data consisted of Ar-En GALE 2010 evaluation (3.4m sentence pairs), Zh-En FBIS corpus (0.3m pairs), and Fr-En Europarl corpus v7 (2.2m pairs). A 3-gram language model was built for Arabic and Chinese using the target side of the parallel corpora. For French, we trained a 5-gram LM from the monolingual dataset provided for WMT-12. More details of the experimental setup can be found in [23].
Source code for replicating all the results presented in this paper is available in the open-source Ivory toolkit.2
4.1 Effectiveness
The baseline token-based model yields a Mean Average Precision (MAP) of 0.271 for Arabic, 0.150 for Chinese, and 0.262 for French. These numbers are competitive when compared to similar techniques applied to these collections. For each collection, we evaluated the three CLIR techniques (P rtoken, P rSCFG/PBMT, and P rnbest, with n  {1, 10}), exploring the effect of the different alignment heuristics as well as flat vs. hierarchical phrase-based translation models. Parameters of the interpolated model were learned by a grid search. Experimental results are summarized in Table 1.3
Based on a randomized significance test [22], the interpolated model outperforms (with 95% confidence, marked *) the token-based model for all runs except for Arabic with Moses, consistently with the one-to-many heuristic and in some cases with the two other heuristics. Furthermore, in five out of the six conditions, the interpolated model with the one-to-many heuristic is significantly better than the one-best MT approach (marked ). This confirms that combining different query translation approaches is beneficial, and is also robust with respect to the test collection, language, and underlying MT model. The one-to-many term mapping heuristic seems to be the most effective overall.
However, the two MT models display significant differences in the "grammar" column, as the hierarchical model significantly outperforms the flat model. This supports the argument that the former is better at representing translation alternatives since it is more expressive. Also as a result of this difference, the flat grammar is much larger than the hierarchical one, which leads to an order of magnitude increase in processing time for P rPBMT.4 These differences become especially important for the Arabic collection, where P rSCFG/PBMT performs much better than P r10-best, using either MT system. An additional benefit of using P rSCFG/PBMT is that we do not need to tune model parameters for translation, which is computationally intensive.
It is also interesting that the differences between the two MT models are insignificant for the 10-best approach, where the decoder finds similar translations in both cases. Therefore, it might be better to use flat representations for the 10-best approach for efficiency, since the end-to-end translation process is faster than hierarchical models.
2http://ivory.cc/ 3For the 1-best model, one-to-one and one-to-many perform very similarly, so we present only the former for space considerations. 4On the other hand, decoding with a flat grammar is substantially faster than decoding with hierarchical MT due to constraints imposed by language modeling.

815

Language Ar Zh Fr

MT
cdec Moses cdec Moses cdec Moses

token 0.271 0.150 0.262

grammar many one none 0.293 0.282 0.302 0.274 0.266 0.273 0.182 0.188 0.170 0.156 0.167 0.151 0.297 0.288 0.292 0.264 0.257 0.262

1-best one none 0.249 0.249 0.249 0.232 0.155 0.155 0.155 0.146 0.276 0.235 0.297 0.242

10-best many one none 0.255 0.249 0.248 0.264 0.254 0.249 0.159 0.159 0.159 0.169 0.163 0.163 0.307 0.304 0.295 0.289 0.300 0.282

interpolated

many one

none

0.293 0.282 0.302

0.280 0.274 0.276

0.192 0.193 0.182

0.183 0.177 0.188

0.318 0.314 0.315 0.307 0.301 0.300

Table 1: A summary of experimental results under different conditions, for all three CLIR tasks. Superscripts * and  indicate the result is significantly better than the token-based and one-best approaches, respectively.

5. CONCLUSIONS AND FUTURE WORK
In this paper, we extended an MT-based context-sensitive CLIR approach [24], comparing flat and hierarchical phrasebased translation models on three collections in three different languages. We make a number of interesting observations about the tradeoffs in incorporating machine translation techniques for query translation.
A combination-of-evidence approach was found to be robust and effective, but we have not examined how the interpolation model parameters can be learned using held-out data--this is the subject of ongoing work. Also, we are exploring ways of leveraging the translation of multi-token source-side expressions. Although we demonstrated the benefits of hierarchical grammars, we still do not explicitly take advantage of non-terminal information in the rules. It might be beneficial to perform a detailed error analysis to see what types of topics are improved with the use of SCFGs over flat grammars. Finally, we briefly discussed interesting tradeoffs between efficiency and effectiveness, but more detailed experiments are required to better understand different operating points and the overall design space.
6. ACKNOWLEDGMENTS
This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency, Contract No. HR0011-12-C-0015; NSF under awards IIS0916043 and IIS-1144034. Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect views of the sponsors. The second author is grateful to Esther and Kiri for their loving support and dedicates this work to Joshua and Jacob.
7. REFERENCES
[1] M. Adriani and C. Van Rijsbergen. Phrase identification in cross-language information retrieval. RIAO, 2000.
[2] L. Ballesteros and W. Croft. Phrasal translation and query expansion techniques for cross-language information retrieval. SIGIR Forum, 31:84­91, 1997.
[3] A. Berger and J. Lafferty. Information retrieval as statistical translation. SIGIR, 1999.
[4] P. Brown, V. Pietra, S. Pietra, and R. Mercer. The mathematics of statistical machine translation: parameter estimation. CL, 19(2):263­311, 1993.
[5] D. Chiang. Hierarchical phrase-based translation. CL, 33(2):201­228, 2007.
[6] K. Darwish and D. Oard. Probabilistic structured query methods. SIGIR, 2003.
[7] C. Dyer, J. Weese, H. Setiawan, A. Lopez, F. Ture, V. Eidelman, J. Ganitkevitch, P. Blunsom, and P. Resnik. cdec: a decoder, alignment, and learning framework for finite-state and context-free translation models. ACL Demos, 2010.
[8] M. Federico and N. Bertoldi. Statistical cross-language

information retrieval using n-best query translations. SIGIR, 2002.
[9] J. Gao, J.-Y. Nie, G. Wu, and G. Cao. Dependence language model for information retrieval. SIGIR, 2004.
[10] J. Gao, J.-Y. Nie, and M. Zhou. Statistical query translation models for cross-language information retrieval. TALIP, 5(4):323­359, 2006.
[11] B. Jones, J. Andreas, D. Bauer, K. Hermann, and K. Knight. Semantics-based machine translation with hyperedge replacement grammars. COLING, 2012.
[12] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. Moses: open source toolkit for statistical machine translation. ACL Demos, 2007.
[13] P. Koehn, F. Och, and D. Marcu. Statistical phrase-based translation. NAACL-HLT, 2003.
[14] W. Kraaij, J. Nie, and M. Simard. Embedding web-based statistical translation models in cross-language information retrieval. CL, 29(3):381­419, 2003.
[15] Y. Liu, R. Jin, and J. Chai. A maximum coherence model for dictionary-based cross-language information retrieval. SIGIR, 2005.
[16] A. Lopez. Statistical machine translation. ACM Computing Surveys, 40(3):8:1­8:49, 2008.
[17] W. Magdy and G. Jones. Should MT systems be used as black boxes in CLIR? ECIR, 2011.
[18] D. Metzler and W. Croft. A Markov random field model for term dependencies. SIGIR, 2005.
[19] V. Nikoulina, B. Kovachev, N. Lagos, and C. Monz. Adaptation of statistical machine translation model for cross-language information retrieval in a service context. EACL, 2012.
[20] J. Ponte and W. Croft. A language modeling approach to information retrieval. SIGIR, 1998.
[21] H.-C. Seo, S.-B. Kim, H.-C. Rim, and S.-H. Myaeng. Improving query translation in English-Korean cross-language information retrieval. IP&M, 41(3):507­522, 2005.
[22] M. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. CIKM, 2007.
[23] F. Ture. Searching to Translate and Translating to Search: When Information Retrieval Meets Machine Translation. PhD thesis, University of Maryland, College Park, 2013.
[24] F. Ture, J. Lin, and D. Oard. Looking inside the box: context-sensitive translation for cross-language information retrieval. SIGIR, 2012.
[25] D. Wu. A polynomial-time algorithm for statistical machine translation. ACL, 1996.
[26] J. Xu and R. Weischedel. Empirical studies on the impact of lexical resources on CLIR performance. IP&M, 41(3):475­487, 2005.
[27] K. Yamada and K. Knight. A syntax-based statistical translation model. ACL, 2001.
[28] W. Zhang, S. Liu, C. Yu, C. Sun, F. Liu, and W. Meng. Recognition and classification of noun phrases in queries for effective retrieval. CIKM, 2007.

816

Query Change as Relevance Feedback in Session Search

Sicong Zhang, Dongyi Guan, Hui Yang
Department of Computer Science Georgetown University
37th and O Street, NW, Washington, DC 20057 USA

{sz303, dg372}@georgetown.edu, huiyang@cs.georgetown.edu

ABSTRACT
Session search retrieves documents for an entire session. During a session, users often change queries to explore and investigate the information needs. In this paper, we propose to use query change as a new form of relevance feedback for better session search. Evaluation conducted over the TREC 2012 Session Track shows that query change is a highly effective form of feedback as compared with existing relevance feedback methods. The proposed method outperforms the state-of-the-art relevance feedback methods for the TREC 2012 Session Track by a significant improvement of >25%.

Categories and Subject Descriptors
H.3.3 [Information Systems ]: Information Storage and Retrieval--Information Search and Retrieval
Keywords
Relevance Feedback; Session Search; Query Change
1. INTRODUCTION
Session search retrieves documents for an entire session of queries. [3, 4]. It allows the user to constantly modify queries in order to find relevant documents. Session search involves many interactions between the search engine and the user. The challenge for session search is how to make use of these interactions and the user feedback to effectively improve search accuracy. In TREC (Text REtrieval Conference) 2012 Session tracks [6], the users (NIST assessors) clicked retrieved documents and interacted with a search engine to produce the queries and sessions. For each intermediate query, a retrieved document set containing the top 10 retrieval results ranked in decreasing relevance for the query are kept. The clicked data contains the documents clicked by users, their clicking orders, and dwell time. Figure 1 illustrates the interactions among the user and the search engine.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM or the author must be honored. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Figure 1: Session search. (TREC 2012 session 85)
Relevance feedback is a popular IR technique. By expanding queries with terms from relevant feedback documents, relevance feedback is able to generate better queries and uses them for better retrieval accuracy. Commonly used relevant feedback schemes include Rocchio with real user feedback [5], pseudo relevance feedback [1],and implicit relevant feedback [7]. Real user feedback is obtained from human assessors that indicate the relevance of a document retrieved for a query. Pseudo relevance feedback, also known as blind relevance feedback, that assumes that the top retrieved documents are relevant, and makes query expansion based on these pseudo relevant documents. Implicit relevance feedback is the form of feedback that is inferred from user behaviors, such as user clicks, clicking order, and dwell time.
In this paper, we propose to use query change as a new form of relevance feedback in session search. Our method utilizes editing changes between two adjacent queries, and the relationship between query change and retrieved documents for the earlier query to enhance session search. Our experiments demonstrate that the proposed approach outperforms other relevance feedback methods.
2. DEFINING QUERY CHANGE
We represent a search session S as a series of queries {q1, ..., qi, ..., qn}. For an individual query qi, we can write it as a combination of the common part and the changes between it and its previous query: qi = (qi  qi-1) + qi.
We define query change qi as the syntactic editing differ-

821

ences between two adjacent queries qi-1 and qi. Considering the directions of editing, query change qi can be further decomposed into two parts: positive q and negative q. They are written as "+q" and "-q" respectively. The positive q are new terms that the user adds to the previous query; that is, they appear in qi, but did not appear in qi-1. The negative q are terms that the user deletes from the previous query; that is, they appeared in qi - 1, but not appear in qi.
We thus decompose an adjacent query pair into:
+qi = qi qi-1

-qi = qi-1 qi

qtheme = qi (+qi) = qi-1 (-qi)
where +qi and -qi represent added terms and removed terms respectively, qtheme is the theme terms, and the notation of represents set-theoretic difference. Table 1 demonstrates a few example TREC 2012 Session queries and their query changes.
The theme terms (qtheme) appear in both qi-1 and qi. Generally it implies a strong preference for those terms from the user. For example, in Table 1 session 32, q1 = "bollywood legislation", q2 = "bollywood law". qtheme = "bollywood".
The added terms (+q) may indicate a specification or drifting between qi-1 and qi. In session 32, (+q2) = "law".
The removed terms (-q) may indicate a generalization or a drifting. In session 32, (-q2) = "legislation".

3. UTILIZING QUERY CHANGE
Besides queries, a TREC session also contains retrieved document sets D (set of Di) for each query qi, and clicked information C (set of Ci) for each query qi.
Based on observation of session search and user intension, we propose an important assumption that the previous search result Di-1 influences the current query change qi:

qi  Di-1.

In fact, this influence can be in quite a complex way. Figure 1 shows session 85 as an example, illustrating how the previous retrieved documents Di-1 influence the query changes.
Based on our definition of query change, we utilize different cases of query change in the calculation of relevance score between the current query qi and a document d.
Suppose P (t|d) is the original term weight for the retrieve model in our utilization, we increase and decrease term weights on top of it. In the following formulas, P (t|d) is calculated by the multinomial query generation language model with Dirichlet smoothing [9] while P (t|d) is calculated based on Maximum-Likelihood Estimation (MLE):

T F (t, d) + µP (t|C)

P (t|d) =

,

Length(d) + µ

where d is the document under evaluation, Length(d) is the length of the document, T F (t, d) is the term frequency of t in document d, P (t|C) calculates the probability that t appears in corpus C based on MLE. µ is set to 5000 in experiments.
We adjust the term weights for the three types of query changes as the following:
· Theme terms are the repeated common parts nearly appearing in the entire session. It implies their importance

Table 1: Examples of TREC 2012 Session queries.

Queries

Query Change

Session 32 Session 85

query 1 = bollywood legislation query 2 = bollywood law query 1 = glass blowing query 2 = glass blowing science query 3 = scientific glass blowing

+q2 = law -q2 = legislation
+q2 = science -q2 =  +q3 = scientif ic -q3 = science

to the session and to the user. We therefore propose to increase their term weights. It is worth noting that theme terms are common terms within a session which show a similar effect of stop words. However, they may not be common terms in the entire corpus. We propose to use a measure that is similar to inverse document frequency (idf ) to capture this characteristic. We employ the negation of the number of occurrences of t in Di-1, 1 - P (t|Di-1). The weight increase for a theme term t  qtheme is formulated as follows:

WT heme =

[1 - P (t|Di-1)] log P (t|d) (1)

tqtheme

· For the added terms that occurred in the previous search result Di-1, which are terms t  +q and t  Di-1, we deduct their term weights. This is because the term appear both in documents for previous query and in the current query, it will bring back repeated information from the previous query to the current query in some degree. In addition, t  +q shows these added terms are not theme terms. Therefore, it has a high probability to deviate from the recent focus of the current query. We thus deduct more weights to reduce redundant information. The weight deduction is proportional to t's term frequency in Di-1.

-WAdd,In = -

P (t|Di-1) log P (t|d)

t+q

(2)

tDi-1

· For the added terms that did not occur in the search result of previous query Di-1, which are terms t  +q and t / Di-1, we increase the term weights because they demonstrate the novel interests of the user for the current query qi. We propose to raise the term weights based on inverse document frequency in order not to increase their weights too much if they are common terms in the corpus.

WAdd,Out =

idf (t) log P (t|d)

t+q

(3)

t/Di-1

· For the terms that are from the previous query, which are terms t  -q. No matter t  Di-1 or t / Di-1, we should deduct their term weights. The reason is the following. If they appeared in Di-1, it means that the user observed them and disliked them. If they did not appear in Di-1, the user still dislikes the terms since they are not included in qi anyway. Just like terms that in added terms that appeared in previously retrieved documents (t  +q and t  Di-1), we deduct the term weight for the removed terms according to the following formula.

-WRemove = -

P (t|Di-1) log P (t|d) (4)

t-q

822

Table 2: Dataset statistics for TREC 2012 Session Track.
#topic =48 #query/session =3.03 #query = 297 #session =98 #session/topic =2.04 #docs =17,861

Table 3: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2012. A statistical significant improvement on nDCG@10 over the baseline is indicated with a  at p < 0.05.

Lemur PRF RF Di-1 Implicit Click Implicit SAT
QueryChg CLK
QueryChg SAT

nDCG@10 0.2622 0.2718 0.2122 0.2668 0.2655
0.3306
0.3300

%chg 0.00% 3.66% -19.07% 1.75% 1.26% 26.09% 25.86%

MAP 0.1342 0.1309 0.1137 0.1355 0.1335
0.1533
0.1535

%chg 0.00% -2.46% -15.28% 0.97% -0.52%
14.23%
14.38%

By considering all cases above, the relevance score between the current query qi and a document d can be represented as a linear combination of various term weight adjustments:

Score(qi, d) = log P (qi|d)+

(5)

+ WT heme - WAdd,In + WAdd,Out - WRemove

where d is the document under evaluation, log P (qi|d) is the original query-document relevance scoring function in log form, , , , and  are coefficients for each type of query changes. Empirically, we set the coefficients as  = 2.2,  = 1.8, = 0.07, and  = 0.4.

4. EXPERIMENTS
4.1 Search Accuracy Using the Last Query
We evaluate our algorithm on the TREC 2012 Session Track [6]. According to how much prior information is used, the Track is divided into four phases: RL1 (using only the last query), RL2 (using all queries in the session), RL3 (using all session queries and ranked lists of URLs and the corresponding web pages), RL4 (using all session queries, the ranked lists of URLs and the corresponding web pages, the clicked URLs, and the time that the user spent on the corresponding web pages). Table 2 shows the statistics about the TREC 2012 Session Track.
The corpus used in our evaluation is ClueWeb09 CatB.1 CatB consists of 50 million English pages from the Web collected during two months in 2009. We removed documents whose Waterloo's "GroupX" spam raining score [2] are less than 70.
We compare the following algorithms in this paper:
· Baseline (Lemur without relevance feedback) Using the original Lemur system (language modeling + Dirichlet smoothing) to retrieve for the last query qn.
· PRF (Pseudo Relevance Feedback). We utilize pseudo relevance feedback algorithm that developed in Lemur. We use the top 20 documents as pseudo relevant documents. The retrieval is for the last query qn.
· RF Di-1. Rocchio using the previously retrieved top documents proved by TREC. This method uses qn, qn-1, and Dn-1.
1http://lemurproject.org/clueweb09/

Table 4: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2012, after uniform aggregation. A statistical significant improvement on nDCG@10 over the baseline is indicated with a  at p < 0.05.

Lemur PRF RF Di-1 Implicit Click Implicit SAT QueryChg CLK QueryChg SAT

nDCG@10 0.3227 0.2986 0.2446 0.2916 0.2889 0.3258 0.3350

%chg 0.00% -7.46% -24.20% -9.64% -10.47% 0.96% 3.81%

MAP 0.1558 0.1413 0.1281 0.1449 0.1467 0.1532 0.1534

%chg 0.00% -9.31% -17.78% -7.00% -5.84% -1.67% -1.54%

· Implicit Click. Implicit relevance feedback based on clicked documents of the previous search query qi-1. This method uses qn, qn-1, Dn-1, Cn-1.
· Implicit SAT. Implicit relevance feedback based on SAT [8] clicked documents (the documents that the user clicked and stayed on for at least 30 seconds) from the previous query. This method uses qn, qn-1, Dn-1, Cn-1.
· QueryChg CLK. (Our algorithm) Relevance feedback using query change based on Eq. 5. This method uses qn, qn-1, Dn-1, Cn-1. Di-1 include the clicked documents and all snippets for the previous query.
· QueryChg SAT. (Our algorithm) Relevance feedback using query change based on Eq. 5. This method uses qn, qn-1, Dn-1, Cn-1. Di-1 are SAT clicks and all snippets for the previous query.
Table 3 shows the search accuracy for these seven runs. We employ the official TREC Session evaluation metrics, nDCG@10 and mean average precision (MAP), for measuring search accuracy. We can see that the proposed methods (QueryChg CLK, QueryChg SAT) improve the baseline by 26.09% and 25.86% respectively in nDCG@10. The improvements are statistically significant (one sided t-test, p =0.05). They also outperforms all other relevance feedback runs. Among other runs, PRF and implicit relevance feedback both improve over the baseline. RF Di-1, however, decreases nDCG@10 by 19.07% than the baseline. This decrease is expected. RF Di-1 makes query expansion based on Di-1, which increases the weights of old terms in Di-1. An ideal relevance feedback model, however, should assign a lower weight to these terms since they are no longer novel or no longer satisfying the current information need.

4.2 Search Accuracy Using All Queries

There are multiple queries in sessions search. Prior re-

search has demonstrated that using all queries can effec-

tively improve search accuracy for session search over just

using the last query [6]. This technique is called query aggre-

gation. We evaluate our algorithm with query aggregation

in this section.

Let Scoresession(qn, d) denote the overall relevance score

for a document d to the entire session, the aggregated ses-

sion relevance score can be written as: Scoresession(qn, d) =

n i=1

i

· Score(qi, d),

where

n

is

the

number

of

queries

in

a session, Score(qi, d) is the relevance score between d and

qi, and i is the query weight for qi. In this paper, we em-

ploy the uniform query aggregation by setting all queries are

equally weighted (i = 1) for all systems under evaluation.

Table 4 shows the search accuracy with uniform aggrega-

tion over all queries. Comparing Table 4 with Table 3, we

823

Table 5: nDCG@10 for different classes of sessions in TREC 2012 Session Track.

Lemur PRF RF Di-1 Implicit Click Implicit SAT QueryChg Click QueryChg SAT

Intellectual 0.2740 0.2814 0.2009 0.2742 0.2749 0.3746 0.3759

%chg 0.00% 2.70% -26.65% 0.10% 0.34% 36.73% 37.20%

Specific 0.2529 0.2721 0.1995 0.2508 0.2555 0.3041 0.3062

%chg 0.00% 7.60% -21.12% -0.83% 1.03% 20.23% 21.08%

Amorphous 0.2741 0.2713 0.2285 0.2873 0.2783 0.3646 0.3604

%chg 0.00% -1.02% -16.63% 4.81% 1.55% 33.03% 31.48%

Factual 0.2557 0.2664 0.2185 0.2627 0.2603 0.3062 0.3045

%chg 0.00% 4.20% -14.54% 2.74% 1.81% 19.77% 19.09%

Table 6: nDCG@10 for different classes of sessions in TREC 2012 Session Track. Uniform Aggregation

Lemur PRF RF Di-1 Implicit Click Implicit SAT QueryChg Click QueryChg SAT

Intellectual 0.3656 0.3634 0.2703 0.3235 0.3235 0.3575 0.3818

%chg 0.00% -0.60% -26.08% -11.51% -11.53% -2.22% 4.43%

Specific 0.2983 0.2654 0.2233 0.2743 0.2767 0.3089 0.3125

%chg 0.00% -11.05% -25.15% -8.07% -7.24% 3.55% 4.76%

Amorphous 0.3539 0.3412 0.2719 0.3138 0.3045 0.3474 0.3637

%chg 0.00% -3.58% -23.18% -11.33% -13.96% -1.84% 2.76%

Factual 0.2989 0.2626 0.2304 0.2739 0.2697 0.3082 0.3089

%chg 0.00% -12.12% -22.92% -8.36% -9.75% 3.11% 3.37%

observe that all systems improve their search accuracy when using query aggregation. The proposed QueryChg SAT run achieves an nDCG@10 of 0.3350, which is a 3.81% improvement over Lemur after uniform query aggregation, and a 27.76% improvement over Lemur without query aggregation. The Lemur run after query aggregation performs well (nDCG@10=0.32) as compared with without query aggregation (nDCG@10=0.26 in Table 3). However, the proposed query change runs (QueryChg Click, QueryChg SAT) do not benefit much from query aggregation. This may be because that uniform aggregation equally weights each query, which assumes query independence among the queries in a session; whereas the query change relevance feedback runs assume that previous query and current query are dependent. The difference in the assumptions between query change relevance feedback model and the uniform aggregation may be the reason that the former does not benefit much from the latter. Other aggregation methods may be able to improve the situation.
4.3 Results On Different Session Types
TREC 2012 sessions were created by considering two different dimensions: product type and goal quality. For product type, a session can be classified as searching for either factual or intellectual target. For search goal, a session can be classified as either specific or amorphous.
Both Table 5 and Table 6 show that the proposed method demonstrate difference effects on different session types. It achieves more improvement on Intellectual sessions (37.20%) and Amorphous sessions (31.48%) than on Factual sessions (19.09%) and Specific sessions (21.08%). This suggests that for more exploratory-style sessions, i.e., more difficult sessions, such as Intellectual and Amorphous sessions, our method is able to generate more performance gain. We believe that our method effectively captures query changes and well represents the dynamics in a search session.
5. CONCLUSION
Based on the idea that query change is an important form of feedback, this paper presents a novel relevance feedback model by utilizing query change. Experiments show that our approach is highly effective and outperforms other feedback models for the TREC 2012 Session Track. Moreover, the

proposed relevance feedback method demonstrates different effects over sessions with different types of search targets and goals. It achieves more improvement on the more difficult sessions, such as Intellectual and Amorphous sessions, over the baseline system which does not use relevance feedback. We believe that our method better captures the exploratory nature of a search session by treating query changes as effective user feedback.
6. ACKNOWLEDGMENTS
This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.
7. REFERENCES
[1] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting good expansion terms for pseudo-relevance feedback. In SIGIR '08, pages 243­250. ACM.
[2] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr., 14(5), Oct. 2011.
[3] D. Guan, H. Yang, and N. Goharian. Effective structured query formulation for session search. In TREC '12.
[4] J. Jiang, D. He, and S. Han. Pitt at trec 2012 session track. In TREC '12.
[5] T. Joachims. A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. In ICML '97.
[6] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2012 session track. In TREC'12.
[7] Y. Song and L.-w. He. Optimal rare query suggestion with implicit user feedback. In WWW '10.
[8] D. Sontag, K. Collins-Thompson, P. N. Bennett, R. W. White, S. Dumais, and B. Billerbeck. Probabilistic models for personalizing web search. In WSDM '12.
[9] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, Apr. 2004.

824

Boosting Novelty for Biomedical Information Retrieval through Probabilistic Latent Semantic Analysis

Xiangdong An, Jimmy Xiangji Huang
Information Retrieval and Knowledge Management Research Lab School of Information Technology
York University, Toronto, ON M3J 1P3, Canada
{xan, jhuang}@yorku.ca

ABSTRACT
In information retrieval, we are interested in the information that is not only relevant but also novel. In this paper, we study how to boost novelty for biomedical information retrieval through probabilistic latent semantic analysis. We conduct the study based on TREC Genomics Track data. In TREC Genomics Track, each topic is considered to have an arbitrary number of aspects, and the novelty of a piece of information retrieved, called a passage, is assessed based on the amount of new aspects it contains. In particular, the aspect performance of a ranked list is rewarded by the number of new aspects reached at each rank and penalized by the amount of irrelevant passages that are rated higher than the novel ones. Therefore, to improve aspect performance, we should reach as many aspects as possible and as early as possible. In this paper, we make a preliminary study on how probabilistic latent semantic analysis can help capture different aspects of a ranked list, and improve its performance by re-ranking. Experiments indicate that the proposed approach can greatly improve the aspect-level performance over baseline algorithm Okapi BM25.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Retrieval
General Terms
Algorithms, Experimentation
Keywords
Genomics IR, passage retrieval, aspect search
1. INTRODUCTION
Information retrieval (IR) in the context of biomedical databases is characterized by the frequent use of abundant acronyms, homonyms and synonyms. How to deal with the tremendous variants of the same term has been a challenging task in biomedical IR. The Genomics track of Text REtrieval Conference (TREC) provided a common platform to evaluate the methods and techniques proposed by various research groups for biomedical IR. In its last two years
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28 ­ August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

(2006 & 2007), the Genomics track focused on the passage retrieval for question answering, where a passage is a piece of continuous text ranging from a phrase up to a paragraph of a document. One of the performances concerned for passage retrieval was the aspect-based mean average precision (MAP) [8]. To evaluate the performance of a ranked list, in 2006, the judges of the competition first identified all relevant passages for each topic from all submissions, and then, based on the content of such relevant passages, assigned a set of Medical Subject Headings (MeSH) terms to each topic as their representative "aspects". In 2007, instead of MeSH terms, the judges picked and assigned terms from the pool of nominated passages deemed relevant to each topic as their "aspects". That is, the "aspects" of a topic in Genomics Track are represented by a set of terms. A passage for a topic is novel if it contains aspect terms assigned to the topic which has not appeared in the passages ranked higher. The novelty of a ranked list is rewarded by the amount of relevant aspects reached at each rank and penalized by the amount of irrelevant passages ranked higher than novel ones. The aspect-based MAP is an average reflection of novelty retrieval performance on all topics.
For the aspect-level evaluation, the search should reach as many relevant aspects as possible and rank their containing passages as high as possible. "Aspects" are assigned to each topic by the judges only after the submission by all groups, and such aspects are picked only from the nominated passages. At competition, nobody knows how many aspects there exist for each topic in the literature, and what they are. Therefore, "aspects" of each topic in this problem are latent, and it is also not an easy problem to figure out the aspects covered by a passage from its "bag-of-words" representation. However, it is well known that a topic model can represent a document as a mixture of latent aspects. That is, a topic model can convert a document from its "bag-of-words" space to its latent semantic space of a reduced dimensionality. In this paper, we study whether the latent semantic representation would help capture different "aspects" of a passage and further improve the performance of a ranked list by re-ranking. There exist a list of topic models such as Latent Semantic Analysis (LSA)[5], Probabilistic Latent Semantic Analysis (PLSA) [9], and Latent Dirichlet Allocation (LDA) [2]. In this preliminary study, we focus on PLSA. In the future, we would study the problem with both LSA and LDA included.
To the best of our knowledge, this is the first investigation about how well a topic model such as PLSA can help capture hidden aspects in novelty information retrieval. In the investigation, we also examine the hyperparameter settings for PLSA such as initial conditional probabilities and zero estimate smoothing in the context of our problem. Besides standard PLSA model [9], we also examine its variants, e.g. instead of word frequencies, tf-idf weighting is used.

829

2. RELATED WORK
In information retrieval, ranking based on pure relevance may not be sufficient when the potential relevant documents are huge and highly redundant with each other. In [3, 16, 14, 15, 18], different ways representing and optimizing novelty and diversity of the retrieved documents are studied. The objective is to find the documents that cover as many different aspects (subtopics) as possible while maintaining minimal redundancy. One problem with the novelty (diversity, aspect, subtopic)-based retrieval is how to evaluate the ranking quality. In [14], 3 metrics are introduced: the subtopic recall measures the percentage of subtopics covered as a function of rank, the subtopic precision measures the precision of the retrieved documents as a function of the minimal rank for a certain subtopic recall, and the weighted subtopic precision measures the precision with redundancy penalized based on ranking cost. In [4], a cumulative gain-based metric is proposed to measure the novelty and redundancy, which is also a function of rank.
Most existing methods [3, 16, 14, 15] improve novelty in IR by penalizing redundancy, but they seem not to work well in Genomics aspect search. In the 2006 TREC Genomics track, University of Wisconsin at Madison failed to promote novelty by penalizing redundancy based on a clustering-based approach [7]. In the 2007 TREC Genomics track, most teams simply submitted their relevant passage retrieval results for aspect evaluation such as National Library of Medicine (NLM) [6] and University of Illinois at Chicago [17]. In [10] and [12], a Bayesian learning approach is proposed to find potential aspects for different topics. In [13], a survival model approach is applied to biomedical search diversification with Wikipedia.
3. HIDDEN ASPECT-BASED RE-RANKING
It is well known that PLSA can help to reveal semantic relations between entities of interest in a principled way [9]. In this paper, we consider each retrieved passage di (1  i  N ) in D = {d1, ..., dN } as being generated under the influence of a number of hidden aspect factors Z = {z1, ..., zK } with words from a vocabulary W = {w1, ..., wM }. Therefore, all passages retrieved initially can be described as an N × M matrix T = ((c(di, wj))ij, where c(di, wj) is the number of times wj appears in passage di. Each row in D is then a frequency vector that corresponds to a passage. Assume given a hidden aspect factor z, a passage d is independent of the word w. Then by Bayes' rule, the joint probability P (d, w) can be obtained as follows:

P (d, w) = P (z)P (d|z)P (w|z).
zZ

To explain the observed frequencies in matrix T , we need to find P (z), P (d|z), and P (w|z) that maximize the following likelihood function:

L(D, W ) =

c(d, w)logP (d, w).

dD wW

It can be shown that the solution can be achieved by EM algorithm iteratively through the following two alternating steps.

1. By E-step, we calculate the posterior probabilities of the hidden aspect factors:

P (z|d, w)

=

P (z)P (d|z)P (w|z) P (z )P (d|z )P (w|z )

.

2. By M-step, we update parameters to maximize the complete

data likelihood:

P (w|z) =

dD

dD c(d, w)P (z|d, w) w W c(d, w )P (z|d, w

),

P (d|z) =

d

wW c(d, w)P (z|d, w) D wW c(d , w)P (z|d

,

w)

,

P (z) =

dD

wW dD

c(d,
wW

w)P c(d,

(z|d, w)

w)

.

After its convergence, we can calculate the probability of hidden aspect factor z given passage d by

P (z|d) =

P (d|z)P (z) zZ P (d|z)P (z)



P (d|z)P (z) = P (d, z).

Hence, we can summarize the aspect trend of each passage d by a normalized factor vector (P (zi|d))K i=1. By this way, we transform the passage representation from the "bag-of-words" space to
a lower latent semantic space. We expect this representation would
capture the aspect trend of each passage in a better way. All pas-
sages can then be clustered based on this vector representation or
simply based on their most probable hidden aspect factor

zd = argmax P (zi|d).
zi Z
With latter, we may sort all passages in each group based on the probability P (zd|d) in descending order. By either way, we can always re-rank retrieved passages by repetitively picking one passage from the top of each group until none is left.

4. EXPERIMENTAL RESULTS
We test our method on a set of runs obtained by the improved Okapi retrieval system [11] for TREC Genomics Track 2007 topics. The set of runs are acquired under different conditions as shown in Tables 1 to 4, where k1 and b are tuning constants of the weighting function BM25. Indexing on database could be paragraph-based (where each piece of indexed information is a paragraph from documents) or word-based (where each piece of indexed information has a limited number of words), and topic expansion is applied once based on unified medical language system (UMLS). To enhance the performance of these runs, feedback analysis is performed by the Okapi retrieval system. In feedback analysis, the system retrieves ten passages that are deemed most relevant for a particular topic, and forms a list of the most recurring words from those passages. Each topic is expanded by these words, and then relevant passages for the extended topic is retrieved. Each feedback term is assigned a weight by Okapi. In our experiments, feedback weight is set to 0.25.
To get their vector representation, we apply both Porter stemming and a stoplist with general stopwords to passages. After Porter stemming and stoplist application, around 4000 words are left for each topic. All passages nominated for each topic are then represented with these words weighted by tf-idf (Better performance is observed with tf-idf instead of frequency used in the standard PLSA model as described in Section 3). We try to use principal component analysis (PCA) to reduce vector dimensionality. It seems PCA is not very helpful in reducing vector dimensionality without hurting performance in this problem. It might be because of the sparsity of data, no obvious dimensions are much more important than others, and every word has some contribution in representing passages nominated for a topic.
Topic models like PLSA typically operate in extremely high dimensional spaces. As a consequence, the "curse of dimensionality" is lurking around the corner, and thus the hyperparameters (such as initial conditional probabilities and smoothing parameters) settings have the potential to significantly affect the results [1]. In the experiments, we find that we cannot start PLSA model with a

830

Table 1: Run1: k1=1.4, b=0.55, word-based indexing, no topic expansion, aspect-level MAP 0.1017.

# of aspects (K) 1

2

3

4

5

6

7

8

9

10

Rerank

0.1017 0.1124 0.1157 0.1430 0.1263 0.1295 0.1243 0.1355 0.1373 0.1279

Improvement 0.00% 10.48% 13.78% 40.67% 24.18% 27.38% 22.24% 33.25% 35.02% 25.75%

Table 2: Run2: k1=1.4, b=0.55, word-based indexing, with topic expansion, aspect-level MAP 0.0611.

# of aspects (K) 1

2

3

4

5

6

7

8

9

10

Rerank

0.0611 0.0639 0.0721 0.0779 0.0886 0.0627 0.0726 0.0739 0.0815 0.0852

Improvement 0.00% 4.50% 17.91% 27.45% 44.90% 2.62% 18.72% 20.92% 33.41% 39.32%

Table 3: Run3: k1=2.0, b=0.4, paragraph-based indexing, no topic expansion, aspect-level MAP 0.0596.

# of aspects (K) 1

2

3

4

5

6

7

8

9

10

Rerank

0.0596 0.0672 0.0650 0.0875 0.0774 0.0832 0.0726 0.0616 0.0660 0.0723

Improvement 0.00% 12.74% 9.10% 46.97% 30.05% 39.76% 21.83% 3.43% 10.88% 21.46%

Table 4: Run4: k1=2.0, b=0.4, word-based indexing, no topic expansion, aspect-level MAP 0.08237957.

# of aspects (K) 1

2

3

4

5

6

7

8

9

10

Rerank

0.0824 0.0886 0.0942 0.0919 0.0846 0.0888 0.0836 0.0930 0.0953 0.0902

Improvement 0.00% 7.54% 14.34% 11.56% 2.57% 7.83% 1.47% 12.85% 15.68% 9.45%

MAP MAP

uniform distribution for P (z), P (d|z), and P (w|z); otherwise, the convergence will happen immediately in the first iteration due to the sparsity of data. Instead, we start with a normalized random distribution for all these conditional probabilities (the results reported in this paper are the average of a few runs). Due to the large dimensionality, there are a lot of zero probabilities in each passage vector representation. Zero estimates could cause significant problems such as zeroing-out the impact of some other useful parameters in multiplication. Zero estimates could also cause computation problems such as "division by zero". In our experiments, we apply Laplace smoothing to avoid zero probability estimates. We add a small value 2-52 to all probabilities before normalization. In the future, more smoothing techniques would be studied.

0.15 0.145

Aspect-level performance comparison - run1

rerank original

0.14

0.135 0.13

0.125

0.12

0.115 0.11
0.105

0.1

1

2

3

4

5

6

7

8

9

10

Number of hidden factors

Figure 1: Performance improvement for run1.

In the experiment, we examine two ways of clustering passages in latent semantic space: one is centroid-based clustering with different distance functions (squared Euclidean, cosine, and cityblock) and the other is based on their most probable aspect factor. It is found that our problem is not so sensitive to either way of clustering, and for the former, not so much sensitive to the change of distance functions. We believe that this is also caused by the sparsity

of data. Our experiment results reported here are from centroidbased clustering with cityblock distance function. In the future, we would explore other clustering algorithms that might be more suitable to our problem such as hierarchical clustering and densitybased clustering.

0.09 0.085

Aspect-level performance comparison - run2

rerank original

0.08

0.075

0.07

0.065

0.06

1

2

3

4

5

6

7

8

9

10

Number of hidden factors

Figure 2: Performance improvement for run2.

In the experiments, we change the number of hidden aspects K from 1 to 10 continuously for all runs. When the number of hidden aspects is set to 1, there is no re-ranking and hence the performances are the same as the original runs. It turns out for all other 9 different hidden aspect numbers, all runs get positive performance improvements by re-ranking as shown in Tables 1 to 4. To illustrate the re-ranking performance graphically, we plot the data in Figures 1 to 4, respectively, where y-axis stands for the aspect-level performance MAP. It can be observed that on all 9 different number of hidden factors, the re-ranked results are all better than the original ones. Over all runs, the maximum improvement is 46.97% when K = 5 for run2, the minimum improvement is 1.47% when K = 7 for run4, and the average improvement is 20.06%. This is illustrated in Figure 5.
It should be noted that the hidden aspect factors in PLSA mod-

831

els are not necessarily the same as the aspects of Genomics Track. In PLSA models, the number of hidden aspect factors is a tuning variable, while the aspects of Genomics Track topics are constants once the corpus and topics are determined. The hidden aspect factors in PLSA models are statistically identified from data while the aspects of Genomics Track topics are assigned by the judges but not results of statistical analyses. Since PLSA models are good in semantic analysis and synonym and concept recognition [9], we use the hidden aspect factors identified by PLSA models to classify passages and then use this classification information to re-rank ranked lists in the hope that the hidden aspect factors do have some correlation with topic aspects in some way. Our experiment results highly support the hope.

0.09 0.085

Aspect-level performance comparison - run3

rerank original

0.08

0.075

MAP

0.07

0.065

0.06

0.055

1

2

3

4

5

6

7

8

9

10

Number of hidden factors

Figure 3: Performance improvement for run3.

0.096 0.094

Aspect-level performance comparison - run4

rerank original

0.092

0.09

MAP

0.088

0.086

0.084

0.082

1

2

3

4

5

6

7

8

9

10

Number of hidden factors

Figure 4: Performance improvement for run4.

5. CONCLUSIONS AND FUTURE WORK
In this paper, we conducted a preliminary study on using PLSA models to capture hidden aspects of retrieved passages. The hidden aspects caught are used to improve the performance of a ranked list by re-ranking. It turned out all runs on all 9 continuous hidden aspect numbers got positive improvements. This indicates PLSA models are very promising in finding diverse aspects in retrieved passages. By contrast, it was indicated [7] a clustering-based method always failed to improve the aspect performance over baseline algorithms.
In the future, more experiments will be conducted to further investigate the proposed method. We will extend the method to more runs, and will study whether there exist a range of hidden aspect numbers that can always be safely used in re-ranking to improve

performance. In addition, we will investigate how to set different hidden aspect numbers for different topics. We will also examine other topic models such as LDA and LSA on this matter.

0.5 0.45
0.4 0.35
0.3 0.25
0.2 0.15
0.1 0.05
1.47% 0
Min

46.97%

20.06%

Max

Average

Figure 5: Performance improvement summary.
6. ACKNOWLEDGMENT
This research is supported by the research grant from the Natural Sciences & Engineering Research Council (NSERC) of Canada. We thank anonymous reviewers for their thorough review comments on this paper.
7. REFERENCES
[1] A. Asuncion and et al. On smoothing and inference for topic models. In UAI'09, pages 27­34, 2009.
[2] D. M. Blei and et al. Latent dirichlet allocation. JMLR, 3(4-5):993­1022, 2003.
[3] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR'98, pages 335­336.
[4] C. Clarke and et al. Novelty and diversity in information retrieval evaluation. In SIGIR'08, pages 659­666.
[5] S. Deerwester and et al. Indexing by latent semantic analysis. JASIST, 41, 1990.
[6] D. Demner-Fushman and et al. Combining resources to find answers to biomedical questions. In TREC-2007, pages 205­214.
[7] A. B. Goldberg and et al. Ranking biomedical passages for relevance and diversity: University of Wisconsin, Madison at TREC genomics 2006. In TREC-2006, pages 129­136.
[8] W. Hersh, A. Cohen, and P. Roberts. TREC 2007 genomics track overview. In TREC-2007, pages 98­115.
[9] T. Hofmann. Probabilistic latent semantic analysis. In UAI'99, pages 289­296.
[10] Q. Hu and X. Huang. A reranking model for genomics aspect search. In SIGIR'08, pages 783­784.
[11] X. Huang and et al. A platform for okapi-based contextual information retrieval. In SIGIR'06, pages 728­728, 2006.
[12] X. Huang and Q. Hu. A bayesian learning approach to promoting diversity in ranking for biomedical informaiton retrieval. In SIGIR'09, pages 307­314.
[13] X. Yin and et al. Survival modeling approach to biomedical search result diversification using wikipedia. TKDE, 25(6):1201­1212, 2013.
[14] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR'03, pages 10­17.
[15] B. Zhang and et al. Improving web search results using affinity graph. In SIGIR'05, pages 504­511.
[16] Y. Zhang, J. Callan, and T. Minka. Novelty and redundancy detection in adaptive filtering. In SIGIR'02, pages 81­88.
[17] W. Zhou and C. Yu. TREC genomics track at UIC. In TREC-2007, pages 221­226.
[18] X. Zhu and et al. Improving diversity in ranking using absorbing random walks. In NAACL-HLT 2007, pages 97­104.

832

Task-Aware Query Recommendation

Henry Feild and James Allan
Center for Intelligent Information Retrieval School of Computer Science University of Massachusetts Amherst, MA 01003
{hfeild,allan}@cs.umass.edu

ABSTRACT
When generating query recommendations for a user, a natural approach is to try and leverage not only the user's most recently submitted query, or reference query, but also information about the current search context, such as the user's recent search interactions. We focus on two important classes of queries that make up search contexts: those that address the same information need as the reference query (on-task queries), and those that do not (off-task queries). We analyze the effects on query recommendation performance of using contexts consisting of only on-task queries, only off-task queries, and a mix of the two. Using TREC Session Track data for simulations, we demonstrate that ontask context is helpful on average but can be easily overwhelmed when off-task queries are interleaved--a common situation according to several analyses of commercial search logs. To minimize the impact of off-task queries on recommendation performance, we consider automatic methods of identifying such queries using a state of the art search task identification technique. Our experimental results show that automatic search task identification can eliminate the effect of off-task queries in a mixed context.
We also introduce a novel generalized model for generating recommendations over a search context. While we only consider query text in this study, the model can handle integration over arbitrary user search behavior, such as page visits, dwell times, and query abandonment. In addition, it can be used for other types of recommendation, including personalized web search.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--search process, query formulation
Keywords
Query recommendation, context-aware recommendation, search task identification
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Figure 1: A search context with interleaved tasks.
1. INTRODUCTION
Query recommendation is a common tool used by search engines to assist users in reformulating queries. When an information need, or task, requires multiple searches, the sequence of queries form a context around which new queries can be recommended. Figure 1 illustrates a series of queries issued by a user consisting of two tasks: 1) finding information about the history of black powder firearms and 2) preparing for the GMAT standardized test. Given this sequence, our goal is to generate a list of query suggestions with respect to the most recently submitted query, or reference query, which is "black powder inventor" in this example. Notice, however, that the user has interleaved the two tasks such that no two adjacent queries are part of the same task. If we use the entire context to generate recommendations, two of the queries will be off-task with respect to the reference query and three (including the reference query) will be on-task. This paper explores the effects that on- and offtask contexts have on query recommendation. While previous work has considered task-aware query recommendation over logged user data, we are not aware of any work that systematically explores the effects of on-task, off-task, and mixed contexts on recommendation performance.
Though the example in Figure 1 may seem an extreme case, consider that Lucchese et al. found 74% of web queries were part of multi-tasking search sessions [15] in a threemonth sample of AOL search logs; Jones and Klinkner observed that 17% of tasks were interleaved in a 3-day sample of Yahoo! web searches [10]; and Liao et al. [14] found 30% of sessions contained multiple tasks and 5% of sessions contained interleaved tasks in a sample of half a billion sessions extracted from Bing search logs. In addition, in a labeled sample of 503 AOL user sessions, we found 44% of search tasks consisted of two or more queries (see Figure 2), but there was only a 45% chance that any two adjacent queries

83

1.0

1.0

0.8

Likelihood of seeing n tasks

Likelihood of tasks of at least length x Likelihood of tasks of length x

0.6

0.8

10 tasks 9 tasks 8 tasks 7 tasks 6 tasks 5 tasks 4 tasks 3 tasks 2 tasks 1 task

0.6

Likelihood

0.4

0.4

0.2

0.2

0.0

0.0

2

4

6

8

10

Task length in number of queries (x)

Figure 2: Distribution of tasks lengths observed in a labeled sample of the AOL query log.

were part of the same task. Figure 3 shows the likelihood of seeing n tasks in any sequence of x queries, e.g., 10-query sequences typically consist of 3­7 search tasks. This means that a context consisting of the most recent n queries is very likely to consist of sub-contexts for several disjoint tasks, none of which may be a part of the same task as the reference query.
The goal of this paper is to better understand the effects of on-task, off-task, and mixed contexts on query recommendation quality. We also present and analyze several methods for handling mixed contexts. We address four questions concerning query recommendation:
RQ1. How does on-task context affect query recommendation performance?
RQ2. How does off-task context affect query recommendation performance?
RQ3. How does mixed context (on- and off-task queries) affect query recommendation performance?
RQ4. How do the following three methods affect query recommendation performance in a mixed context? (a.) using only the reference query, (b.) using the most recent m queries, or (c.) using the most recent m queries with same-task classification scores to weight the influence of each query.
To answer these questions, we perform a number of experiments using simulated search sequences derived from the TREC Session Track. For recommendation, we rely on random walks over a query flow graph formed from a subset of the 2006 AOL query log. We measure query recommendation performance by the quality of the results returned for a recommendation, focusing primarily on mean reciprocal rank (MRR). Our results show that on-task context is usually helpful, while off-task and mixed contexts are extremely harmful. However, automatic search task identification is a reliable way of detecting and discarding off-task queries.
There are four primary contributions of this paper: (1) an

2 3 4 5 6 7 8 9 10
Sequence length
Figure 3: The distribution of seeing n tasks in a sequence of x queries as observed in a labeled sample of the AOL query log.
analysis of task-aware query recommendation demonstrating the usefulness of on-task query context, (2) an analysis of the impact of automatic search task identification on taskaware recommendation, in which we show the state of the art works very well, (3) an open source data set for evaluating context-aware query recommendation, and (4) a generalized model of combining recommendations across a search context, regardless of the recommendation algorithm.
2. RELATED WORK
Huang, Chien, and Oyang [9] introduced a search logbased query recommendation algorithm that extracts suggestions from search sessions in a query log that appeared similar to the user's current session, thereby incorporating the surrounding search context. They found it outperformed methods that extract suggestions from retrieved documents in many aspects.
Filali et al. [8] presented a probabilistic model for generating rewrites based on an arbitrarily long user search history. Their model interpolates the same-task similarity of a rewrite candidate to the reference query with the average similarity of that candidate to all on-task queries from a user's history, weighted by each query's similarity to the reference query. They found that giving some weight to the history, but most weight to the reference query, did best on a set of manually and automatically labeled data.
Liao et al. [14] explored the effect of task-trails on three applications, including query recommendation. They compare two session-based, two task-based, and two single-query recommendation models and found they retrieve complementary sets of suggestions, though the task-based models provided the higher quality suggestions. To identify tasks, they used an SVM model using features similar to Jones and Klinkner [10], and the weighted connected components clustering method described by Lucchese et al. [15]. Unlike Liao et al., we focus on the effects of on- and off-task context in

84

various mixtures and evaluate on publicly accessible data. Their work is complementary to ours and our findings support theirs concerning the quality of recommendations using on-task recommendations.
Cao et al. [4] introduce a context-aware recommendation system that converts a series of user queries into concept sequences and builds a suffix tree of these from a large query log. To produce recommendations, a concept sequence is looked up in the suffix tree and the common next queries are given as suggestions.
Cao et al. [5] explored an efficient way to train a very large variable length Hidden Markov Model (vlHMM), which considers sequences of queries and clicks in order to produce query and URL recommendations as well as document reranking. The authors trained the vlHMM on a large commercial search log. However, they did not analyze the effects of on- and off-task contexts on recommendation and the vlHMM technique is tied to large search logs, limiting its portability.
Boldi et al. [2] presented a technique for building a query reformulation graph over user sessions, called a query flow graph. They considered two edge weighting techniques: one uses the threshold weight output by a same-task classifier and the other uses the observed likelihood of one query being submitted after the other. They looked at two applications of query flow graphs: (1) identifying sequences of queries that share a common search task and (2) generating query recommendations. The query suggestion component involves random walks and can be configured to consider the most recent n queries.
Several groups have investigated automatic methods for segmenting and clustering user search interactions into search tasks [2, 10, 13, 15, 18]. Most of these depend on machine learning algorithms--logistic regression, support vector machines, and decision trees--trained over large amounts of manually or automatically labeled user data to classify a pair of queries as belonging to the same task or not. We decided to use the Lucchese et al. method [15], which is a heuristic model that depends on lexical and semantic features of the pair of queries being classified. This method is the most recent and was shown to out-perform machine learned models on a sample of AOL data. The other methods are, however, equally applicable to the analysis we conduct.
3. QUERY RECOMMENDATION
We only consider one query recommendation algorithm based largely on the query flow graph (QFG) work of Boldi et al. [2] and the term-query graph (TQGraph) work of Bonchi et al. [3]. We use a query flow graph G in which the vertices V are queries from a query log L and the edges E represent reformulation probabilities. For any vertex v  V , the weights of all outgoing edges must sum to 1. A reformulation is defined as an ordered pair of queries (qi, qj) such that the pair occurs in a user search session in that order, though not necessarily adjacent. A session is defined to be the maximal sequence of queries and result clicks such that no more than t seconds separate any two adjacent events. The outgoing edges of v are normalized across all sessions and users in L.
While we do not require reformulations to be adjacent, Boldi et al. did. By considering all reformulations--adjacent and otherwise--within a session, we expand the coverage

of G beyond using adjacent query reformulations only but avoid incorporating as many off-task reformulations as when the scope is a user's entire search history. We assume that reformulations in which qi and qj are from different tasks, qj will be an infrequent follower of qi, and therefore statistically insignificant among qi's outgoing edges. In addition, Boldi et al. used a thresholded and normalized chaining probability for the edge weights, but we do not due to the sparseness of our data (the AOL query logs).
To generate recommendations, we rely on a slight adaptation of the query flow graph, called the term-query graph [3]. This adds a layer to the QFG that consists of all terms that occur in queries in the QFG, each of which points to the queries in which it occurs. Given a query q, we find recommendations by first producing random walk scores over all queries in Q for each term t  q.
To compute the random walk with restart for a given term t, we must first create a vector v of length |V | (i.e., with one element per node in Q). Each element corresponding to a query that contains t is set to 1 and all others are set to 0. This is our initialization vector. Next, we must select the probability, c, of restarting our random walk to one of the queries in our initialization vector. Given the adjacency matrix of G, A, and a vector u that is initially set to v, we then compute the following until convergence:

u = (1 - c)Au + cv

(1)

After convergence, the values in u are the random walk scores of each corresponding query q for t. We denote this as the term-level recommendation score rterm(q|t).
One issue with using the random walk score for a query is that it favors frequent queries. To address this, Boldi et al. used the geometric mean rterm of the random walk score rterm and its normalized score rterm. Given an initial query q, the scores for an arbitrary query q can be computed by:

rterm(q|q)

=

rterm(q|q) runiform (q  )

(2)

rterm(q|q) =

rterm(q|q) · rterm(q|q)

=

rterm(q|q)

(3)

runiform (q  )

where runiform(q) is the random walk score produced for q when the initialization vector v is uniform.
The final query recommendation vector is computed using a component-wise product of the random walk vectors for each term in the query. Specifically, for each query q  V , we compute the query-level recommendation score rquery(q|q) as follows:

rquery(q|q) = rterm(q|t)

(4)

tq

4. CONTEXT AWARE RECOMMENDATION
In this section, we introduce formal definitions of general and task-based contexts as well as the automatic search task identification algorithm used for the experiments.
4.1 Generalized context model
The recommendation models described above, and recommendation algorithms in general, that generate suggestions with respect to a single query can be easily extended to handle additional contextual information. The basic idea is

85

simple: when generating recommendations for a query, consider the search context consisting of the m most recently submitted queries from the user, weighting the influence of each according to some measure of importance. Many functions can be used to measure the importance of a context query. The two we consider in this paper are how far back in a user's search history a query occurs and whether the query appears to be related to a user's current task. However, others may include whether a context query was quickly abandoned or spell corrected, how many results the user visited, the time of day they were visited, and other behavioral aspects. In this section, we introduce a generalized model that makes it easier for us to talk about the various importance functions we are interested in and can be used with additional functions explored in future work.
Assume that we have a search context C that contains all the information about a user's search behavior related to a sequence of m queries, with the mth query, C[m], being the most recently submitted query. Also assume that we have a set of n importance functions, 1, 2, . . . , n and corresponding weights 1, 2, . . . , n that tell us how much weight to give to each of the importance functions. We will represent corresponding functions and weights as tuples in a set F = { 1, 1 2, 2 . . . , n, n }. We compute the context-aware recommendation score for a query suggestion q as follows:

m
rcontext(q|C, F ) = rquery(q|C[i])

 · (i, m, C)

i=1

, F

(5)

Each importance function , takes three parameters: i, m, C,

where i is the position of the query within context C for

which importance is being measured and m is the position

of the reference query. In this work, the reference query is

always the last query of C, but the model does not make

the assumption that this is always the case. The rcontext recommendation scoring function scores q with respect to

each query in the context (C[i]) and adds that score to the

overall score with some weight that is computed as the lin-

ear combination of the importance function values for that

query.

4.2 Decaying context model

One of the importance functions we consider in this paper is a decaying function, where queries earlier in a user's context are considered less important than more recent queries. As such, queries submitted more recently have a greater influence on recommendation scores. This has the intuitive interpretation that users are less interested in older queries, otherwise they would not have moved on to new queries.
Boldi et al. [2] discussed a decay weighting method for entries in the random walk initialization vector (v in Eq. 1). They proposed that each query in a search context receive a weight proportional to d, where d is the distance in query count from the current query. For example, the second most recent query would get a weight of 1, because it's one query away from the most recent query.
While the Boldi et al. method is specific to recommendations using random walks, we can transfer their exponential decay function to our model as follows:

decay(i, j, C) = j-i

(6)

rdecay(q|C) = rcontext(q|C, { 1.0, decay }) (7)

4.3 Task context model
While decaying the influence of queries earlier in a search context is a natural importance function, we are also interested in functions that incorporate the degree to which a query is on the same task as the reference query. It is reasonable to assume (an assumption we test) that queries from a search context that are part of the same task should be more helpful in the recommendation process than queries that are not.
As we have stated earlier, Lucchese et al. observed that 74% of web queries are part of multi-task search sessions [15] while Jones and Klinkner found that 17% of tasks are interleaved in web search [10]. Using a labeled sample of the AOL query log, we observed an exponential decrease in the likelihood that the previous m queries are part of the same task as m increases (see Figure 3). This suggests that using the m most recent queries as the the search context for generating recommendations will likely introduce off-topic information, causing recommendations that seem out of place. Therefore, it may be beneficial to identify which queries from that context share the same task as the reference query.
Formally, given a search context C with m queries, we define a task context T to be the maximal subset of queries in C that share a common task to C[m]:

T  C |  i  [1, m], C[i]  T  sametask(i, m, C) >  (8)
where sametask(i, m, C) is a function that outputs a prediction in the range [0,1] as to how likely C[i] and C[m] are to be part of the same search task given C and  is the decision threshold.
Once we have T , the natural question to pose is how do we use it? One method would be to treat T just as C and use it with the rdecay function, i.e., rdecay(q|T ). However, it may be that the off-task context is still useful, just not as useful as T . To support both of these points of view, we can use the following hard task recommendation scoring functions:

taskdecay(i, j, C) = taskdist(i,j,C)

(9)

hardtask(i, j, C) = taskdecay if sametask > ,

0

otherwise.

(10)

rhardtask(q|C) = rcontext(q|C, { , hardtask ,

1 - , decay })

(11)

where  can be used to give more or less weight to the task context and taskdist is the number of on-task queries between C[i] and C[j]. If we set  = 1, we only use the task context, whereas with  = 0, we ignore the task context altogether. If we use  = 0.5, we use some of the task information, but still allow the greater context to have a presence. Note that we have left off the parameters to decay and sametask in Eq. 16 for readability.
This approach may work well if one is comfortable with setting a hard threshold  on the output of the sametask function. If, however, we want to provide a mechanism by which we use the output of sametask as a confidence, we can use the following soft task recommendation scoring func-

86

tions:

softtask(i, j, C) = sametask · decay

(12)

rsofttask(q|C) = rcontext(q|C, { , softtask ,

1 - , decay })

(13)

Here,  smooths between using and not using the same-task scores to dampen the decay weights. As before, we have left off the parameters to sametask and decay in Eq. 12.
Two additional models we consider are both variations of what we call firm task recommendation, as they combine aspects of both the hard and soft task models. The first, called firmtask1, behaves similarly to the soft task model, except that the weight given to any queries with a same task score at or below the threshold  are weighted as 0. The second, called firmtask2, is identical to the hard task model, except that the task classification score is used in addition to the taskdecay weight. Mathematically, the firm task recommendation models are described by:

 sametask firmtask1(i, j, C) = 0 × decay

if sametask > , otherwise.

(14)

rfirmtask1(q|C) = rcontext(q|C, { , firmtask1 ,

1 - , decay })  sametask firmtask2(i, j, C) = 0 × taskdecay

(15) if sametask > ,
otherwise.

(16)

rfirmtask2(q|C) = rcontext(q|C, { , firmtask2 ,

1 - , decay })

(17)

Note that unlike the hard task model, the decay component of the firmtask1 model is affected by every query, not just those above the threshold.
For example, suppose we have a context C with five queries, q1, . . . , q5. Relative to the reference query, q5, suppose applying sametask to each query produces the sametask scores [0.4, 0.2, 0.1, 0.95, 1.0]. If we set  = 0.2, then T = [q1, q4, q5]. Using  = 0.8, notice in Figure 4 how the importance weight of each query in the context changes between using only the decay function (a.) and setting  = 1 for the task-aware recommendations (b.­e.). Note that when  = 0, the hard, firm, and soft task recommendation scores are equivalent (they all reduce to using the decay-only scoring function).
There are two primary differences between using hardand soft task recommendation. First, hard task recommendation does not penalize on-task queries that occur prior to a sequence of off-task queries, e.g. in Figure 4, we see that q1 is on-task and hardtask treats it as the first query in a sequence of three: n-1 = 0.82. Conversely, the soft task recommendation model treats q1 as the first in a sequence of five: m-1 = 0.84.
Second, soft task recommendation can only down-weight a query's importance weight, unlike-hard task recommendation, which we saw can significantly increase the weight of an on-task query further back in the context. At the same time, however, soft task recommendation only allows a query to

sametask = [.8, .2, .1, .9, 1.0]

a. rdecay

 [.4, .5, .6, .8, 1.0]

b. rsofttask  [.3, .1, .1, .7, 1.0]

c. rfirmtask1 = [.3, .0, .0, .7, 1.0]

d. rfirmtask2 = [.5, .0, .0, .7, 1.0]

e. rhardtask  [.6, .0, .0, .8, 1.0]

Figure 4: An example of the degree to which each query in a context contributes (right column) given its predicted same-task score (top row) for: (a.) decay only, (b.) soft task, (c.) firm task-1, (d.) firm task-2, and (e) hard task recommendation. We set  = 0.8 for all, and  = 1,  = 0.2 for b.­e.

be given a zero weight if its same-task score is zero. The two firm task models balance these aspects in different ways.

4.4 Automatic search task identification
In Section 3, we discussed how to use information about tasks for query recommendation, but we did not say how to generate the scores. We use the search task identification heuristic described by Luccehse et al. [15]. In deciding if two queries qi and qj are part of the same task, we calculate two similarity measures: a lexical and a semantic score, defined as follows.
The lexical scoring function slexical is the average of the Jaccard coefficient between term trigrams extracted from the two queries and one minus the Levenshtein edit distance of the two queries. The score is in the range [0,1]. Two queries that are lexically very similar--ones where a single term has been added, removed, or reordered, or queries that have been spell corrected--should have an slexical score close to one.
The semantic scoring function ssemantic is made of up two components. The first, swikipedia(qi, qj ), creates the vectors vi and vj consisting of the tf·idf scores of every Wikipedia document relative to qi and qj, respectively. The function then returns the cosine similarity between these two vectors. The second component, swiktionary(qi, qj ) is similarly computed, but over Wiktionary entries. We then set ssemantic(qi, qj ) = max swikipedia(qi, qj ), swiktionary(qi, qj ) . As with the lexical score, the range of the semantic score is [0,1].
The combined similarity score, s, is defined as follows:

s(qi, qj ) =  · slexical(qi, qj ) + (1 - ) · ssemantic(qi, qj )
We can define a same-task scoring function to use s directly, as follows:

sametask1(i, j, C) = s(C[i], C[j])

(18)

Alternatively, we can run one extra step: single-link clustering over the context C using s as the similarity measure. Clustering allows us to boost the similarity score between two queries that are only indirectly related. Similar to Liao et al. [14], our choice of clustering follows the results of Lucchese et al. [15], who describe a weighted connected components algorithm that is equivalent to single-link clustering with a cutoff of . After clustering, we use the notation KC [q] to represent the cluster or task associated with query q in context C; if two queries q, q  C are part of the same task, then KC [q] = KC [q], otherwise KC [q] = KC [q]. A

87

Figure 5: Examples of on-task/off-task segmentations using sametask1 and sametask2 scoring. The reference query, q5, sits in the bolded center node. Note that the edge (q1, q5) goes from 0.2 using sametask1 to 0.6 under sametask2 due to q1's strong similarity to q3, which has a similarity score of 0.6 with q5.

scoring function that uses task clustering is the following:

sametask2(i, j, C) =

max

s(C[i], C[k])

k  [1,|C|] :

k =i

KC [C[k]] =KC [C[j]]

(19)

Note that sametask2 will return the highest similarity between C[i] and any member of C[j]'s tasks, excluding C[i]. Figure 5 illustrates a case in which sametask2 improves over sametask1; note, however, that sametask2 can also be harmful when an off-task query is found to be similar to an on-task query.

5. EXPERIMENTAL SETUP
In this section, we describe the data, settings, and methodology used for the experiments.
5.1 Constructing a query flow graph
We extracted query reformulations from the 2006 AOL query log, which includes more than 10 million unique queries making up 21 million query instances submitted by 657,426 users between March­April 2006. Considering all ordered pairs from a 30-query sliding window across sessions with a maximum timeout of 26 minutes, we extracted 33,218,915 distinct query reformulations to construct a query flow graph (compared to 18,271,486 if we used only adjacent pairs), ignoring all dash ("-") queries, which correspond to queries that AOL scrubbed or randomly replaced. The inlink and outlink counts of the nodes in the graph both have a median of 2 and a mean of about 5. If we were to use only adjacent reformulations from the logs, the median would be 1 and the mean just under 2.
5.2 Task data
We used the 2010 and 2011 TREC Session Track [11, 12] data to generate task contexts. The 2010 track data contains 136 judged sessions, each with two queries (totaling 272 queries), covering three reformulation types: drift, specialization, and generalization relative to the first search. We ignore the reformulation type. The 2011 track data consists of 76 variable length sessions, 280 queries (average of 3.7 queries per session), and 62 judged topics. Several topics have multiple corresponding sessions. In total, we use all

212 judged sessions from both years. The relevance judgments in both cases are over the ClueWeb09 collection. Our goal is to provide recommendations to retrieve documents relevant to the last query in each session, thus we mark the last query as the reference query.
Each session constitutes a single task, and henceforth we refer to the sessions as tasks. Since the TREC data consists of single tasks, we need some way of simulating the case that multiple tasks are interleaved. We describe our approach for this next.
5.3 Experiments
To answer our four research questions, we use the following set of experiments. Throughout all of these experiments, the baseline is to use only the reference query for generating query recommendations.
Experiment 1. For RQ1, which seeks to understand the effect of including relevant context on recommendation performance, we use each task T from the TREC Session Track and recommend suggestions using the most recent m queries for m = [1, |T |]. If incorporating context is helpful, then we should see an improvement as m increases. Note that m = 1 is the case in which only the reference query is used.
Experiment 2. To address RQ2, which asks how offtask context affects recommendation performance, we modify the experiment described above to consider a context of m = [1, |T |] queries such that queries 2­|T | are off-task. To capture the randomness of off-task queries, we evaluate over R random samples of off-task contexts (each query is independently sampled from other tasks, excluding those with the same TREC Session Track topic) for each task T and each value of m > 1. If off-task context is harmful, we should see a worsening trend in performance as m increases.
Experiment 3. To address RQ3, which asks how query recommendation performance is affected by a context that is a mix of on- and off-task queries, we rely on a simulation of mixed contexts. As we saw in Figure 3, the probability that a sequence of m queries share the same task decreases exponentially as m increases, and so the mixed context assumed in RQ3 is realistic if not typical. We simulate mixed contexts by taking each task T of length n and considering a context window of length m = [1, n + R], where R is the number of off-task queries to add into the context. The last query in the context qm always corresponds to the last query qn in T . Queries q1, . . . , qm-1 consist of a mix of the queries from T and other tasks from the TREC Session Track. The queries from T will always appear in the same order, but not necessarily adjacent.
To incorporate noise, we initially set C = []. We select R off-task queries as follows: first, we randomly select an off-topic task, O, from the TREC Session Track and take the first R queries from that task. If |O| < R, we randomly selected an addition off-topic task and concatenate its first R-|O| queries to O. We continue the process until |O| = |R|. We now randomly interleave T and O, the only rule being that Tn--the reference query--must be the last query in C (an easy rule to adhere to by simply removing Tn before the randomized interleaving, and then concatenating it to the end). For a given value of R, we can perform many randomizations and graph the effect of using the most recent n + R queries to perform query recommendation.
Experiment 4. The final research question, RQ4, asks

88

how mixed contexts should be used in the query recommendation process. We have limited ourselves to consider three possibilities: (a.) using only the reference query (i.e., our baseline throughout these experiments), (b.) using the most recent n + R queries (i.e., the results from Experiment 3), or (c.) incorporating same-task classification scores. Experiment 4 concentrates on (c.) and analyzes the effect of incorporating same-task classification scores during the search context integration process. This is where we will compare the task-aware recommendation models described in the previous section.
5.4 Technical details
For the query recommendation using the TQGraph, we used a restart probability of c = 0.1, as was found to be optimal by Bonchi et al. [3]. Note that they refer to the restart value , where c = 1 - . To increase the speed of our recommendation, we only stored the 100,000 top scoring random walk results for each term. Bonchi et al. [3] found this to have no or very limited effects on performance when used with c = 0.1.
For task classification, we used the parameters found optimal by Lucchese et al. [15]:  = 0.2 (used during task clustering) and  = 0.5 (used to weight the semantic and lexical features). We also set  =  since  is used in much the same way in the task-aware recommendation models.
To evaluate recommendations, we retrieved documents from ClueWeb09 using the default query likelihood model implemented in Indri 5.3 [17].1 We removed spam by using the Fusion spam score dataset [6] at a 75th percentile, meaning we only kept the least spammy 25% of documents.2
6. RESULTS
In this section, we cover the results of each of the experiments described in Section 5.3. We then discuss the meaning of our findings as well as their broader implications.
6.1 Experimental results
In all experiments, we measured recommendation performance using the mean reciprocal rank (MRR) of ClueWeb09 document retrieved for the top scored recommendation averaged over the 212 TREC Session Track tasks. We found similar trends using normalized discounted cumulative gain (nDCG) and precision at 3, 5, and 10. There are several ways one can calculate relevance over the document sets retrieved for recommendations, such as count any document retrieved in the top ten for any of the context queries as nonrelevant (rather harsh), indifferently (resulting in duplicate documents), or by removing all such documents from the result lists of recommendations. We elected to go with the last as it is a reasonable behavior to expect from a context-aware system. We removed documents retrieved for any query in the context, not just those that are on-task. This is a very conservative evaluation and is reflected in the performance metrics.
Experiment 1. With this experiment, our aim was to quantify the effect of on-task query context on recommendation quality. Focusing on the top line with circles in Figure 6, the MRR of the top scored recommendation averaged over the 212 tasks performs better than using only the ref-
1http://www.lemurproject.org/indri/ 2http://plg.uwaterloo.ca/~gvcormac/clueweb09spam/

MRR 0.02 0.04 0.06 0.08 0.10

On-task context; =0.8 Reference query only Off-task context; =0.8

2

4

6

8

10

Context length (m)
Figure 6: The effect of adding on-task (blue circles) and off-task (red triangles) queries versus only the reference query on recommendation MRR (black squares). MRR is calculated on the top scoring recommendation.

Effect of on-task context per task

MRR difference MRR difference -1.0 0.0 1.0 -1.0 0.0 1.0

Effect of off-task context per task
Figure 7: The per session effect of on- and off-task context on the change in MRR of the top scoring recommendation. The y-axis shows the difference between the MRR of using context and using only the reference query. A higher value means context improved MRR. Note that 145 tasks were removed as neither on- nor off-task context had an effect. The bars are not necessarily aligned between the two plots and should not be compared.
erence query (middle line with squares). To generate these scores, we used the rdecay model with  = 0.8, as set by Boldi et al. [2] in their decay function. For each value of m, if a particular task T has fewer than m queries, the value at |T | is used. The MRR scores are low because for a large number of tasks, none of the methods provide any useful recommendations. We performed evaluations where such tasks were ignored and found that the MRR does indeed increase and the relationship between the methods plotted stays the same. However, in order to ensure comparability with future work, we elected to report on all tasks.
While performance is better on average in Figure 6, the top bar chart in Figure 7 breaks the performance down by the TREC search tasks and we can see that there are many tasks for which on-task context is very helpful, as well as several where it hurts. Note that some of the tasks are not displayed for readability.
Experiment 2. The goal of the second experiment was to ascertain the effect of off-task context on query recommendation. We generated 50 random off-task contexts for

89

each task and report the micro-average across all trials. The bottom line with triangles in Figure 6 shows that adding off-task queries under the rdecay model with  = 0.8 rapidly decreases recommendation performance for low values of m before more or less leveling off around m = 5 (it still decreases, but much slower). Its performance is well below that of the baseline of using only the reference query, making it clear that off task context is extremely detrimental.
Turning to the bottom plot in Figure 7, we see that offtask context has an almost entirely negative effect (there is an ever so slight increase in performance for the task represented by the far left bar). Interestingly, for the severely compromised tasks on the far right, the effect is not as negative as when on-task context hurts. We have not conducted a full analysis to understand this phenomena, but one possible cause is the averaging over 50 trials that takes place for the off-task contexts. We leave investigations into this for future work.
Experiment 3. With Experiment 3, we wanted to understand the effect of mixed contexts--consisting of both on- and off-task queries--on query recommendation performance. As explained earlier, the experiment explores the performance of tasks when R noisy queries are added to the entire set of on-task queries. The bottom line with triangles in Figure 8 shows just this, using rdecay with  = 0.8. The far left point, where R = 0, lines up with the far right point of the on-task line in Figure 6. We randomly generated 50 noisy contexts per task for each value of R. The solid line shows the micro-averaged MRR over all tasks' samples. The dotted lines on either side show the minimum and maximum values for the micro-average MRR on a set of 1,000 sub-samples (with replacement) of the original 50. As you can see, the bounds indicate relatively low variance of the micro-average across the 212 tasks. There are still certain tasks for which performance is very high or very low (that is, the bounds on the micro-average do not inform us of the variance among tasks).
An important observation from this experiment is that performance dips below that of the baseline when even a single off-task query is mixed in. This is quite startling when you consider that the chances of three queries (at R = 1, all contexts are of at least length three) in a row belonging to a single task are below 30% (see Figure 3) and that roughly 40% of tasks in the wild are of length three or more (see Figure 2). These results clearly show that blindly incorporating mixed context is a poor method of incorporating context.
Experiment 4. In the final experiment, we hoped to determine the effects of using recommendation models that consider the reference query only, the entire context, or the entire context, but in a task-aware manner. The first two were addressed in the previous experiments, where we learned that using the reference query is more effective than blindly using the entire context. Figure 8 shows the results of using the models we introduced in Section 4.3. We used the same randomizations as in Experiment 3 and likewise generated the minimum and maximum bounds around each model's performance line. For these experiments, sametask1 scores were used to produce same-task scores. We also performed the experiment using sametask2 and found it was comparable. We used  = 1 for all task-aware models; setting it to anything less resulted in an extreme degradation of performance.
There are several interesting observations. First, the firm-

MRR 0.02 0.04 0.06 0.08 0.10

Firm-task1 (=1, =0.8) Firm-task2 (=1, =0.8) Hard-task (=1, =0.8) Reference query only Soft-task (=1, =0.8) Decay (=0.8)

0

2

4

6

8

10

Noisy queries added (R)
Figure 8: The effect of adding off-task queries to a task context on MRR when same task classification is used and is not used versus only using the reference query (black squares). The sametask1 scoring method is used for all task-aware recommendation models. MRR is calculated on the top scoring recommendation.

task models performed best, though it is likely that the performance of the rfirmtask1 model (the top line with x's) would decrease with larger amounts of noise because the decay function depends on the length of the context, not the number of queries predicted to be on-task. Thus, for on-task queries occurring early on in very large contexts, the decay weight will effectively be 0. You may notice that this model also increases for a bit starting at R = 2. This is likely due to the decay function used: since every query in the context, and not just the on-task queries, count toward the distances between an on-task query and the reference query under rfirmtask1, on-task queries are actually down-weighted to a greater degree than in the rfirmtask2 and rhardtask models. The graph indicates that this change in weighting is helpful. This also suggests that setting  differently may improve the performance of the other models.
The rfirmtask2 model (diamonds) comes in at a close second and narrowly outperforms rhardtask (circles). All three models outperform the baselines--using only the reference query (squares) and rdecay over the entire context (triangles).
The rsofttask model, however, performs rather poorly. While it can offer some improvement over using just the reference query for a small amount of noise, once the noise level reaches four off-task queries, it is not longer viable. It does, however, outperform the decay model applied in a task-unaware manner.
Another interesting point is that the performance of the task-aware models is actually better at R = 0 than if the known tasks are used. The likely explanation is that the same-task scores prevent on-task queries that are quite different from the reference query from affecting the final recommendation. These kinds of queries may introduce more noise since their only recommendation overlap with the reference query may be generic queries, such as "google". This is not always the case, however. For example, one task consists of the queries ["alan greenspan", "longest serving Federal Reserve Chairman"]. The first query is detected to be

90

Rank Reference query RR Decay

RR Hard task ( = 1)

RR

1. google

history of films

black powder sabots

1.00

2. ebay

the history of european culture of drinking

black powder cannons

0.33

3. yahoo

the history of european alcohol drinking. . .

google

4. yahoo.com

the history of european alcohol drinking

marlin firearms

5. google.com

aircraft crash testing

black powder weapons for sale 0.08

Figure 9: The top 5 suggestions generated from three of the models for the randomly generated context shown in Figure 10. Reciprocal rank (RR) values of 0 are left blank.

No. Query context

sametask1

5. black powder inventor

1.00

4. wikipedia black powder

0.52

3. us geographic map

0.06

2. black powder ammunition

0.75

1. us political map

0.03

Figure 10: An example of a randomly generated mixed context along with the same-task scores. The top query (No. 5) is the reference query. The bolded queries are on-task.

off-task, however, it is imperative to generate decent recommendations since the reference query generates generic Federal Reserve-related queries and not ones focused on Alan Greenspan.
Overall, though, the same-task classification with a threshold  = 0.2 worked well. The same-task classification precision relative to the positive class was on average 80%. The precision relative to the negative class varied across each noise level, but was on average 99%. The average accuracy was 93%. The decent same-task classification is why the three models at the top of Figure 8 are so flat.
6.2 Discussion
The results of our experiments demonstrate not only the usefulness of on-task context, but also the extreme impact of off-task and mixed contexts. The results from Experiment 4 suggest that the appropriate model is one that balances a hard threshold to remove any influence from context queries predicted to be off-task, and to weight the importance of the remaining queries by both their distance to the reference query and by the confidence of the same-task classification. Based on the results, we recommend the rfirmtask2 model, since its performance will be consistent regardless of how far back in a user's history we go, unlike rfirmtask1.
We did not see any substantial effects from using task clustering, as Liao et al. [14] used. However, other task identification schemes may perform differently; after all, as we saw in Experiment 4, our task identification method actually caused slight improvements over using the true tasks.
To get a feel for the quality of the recommendations produced generally with the AOL query logs and specifically by different models, consider the randomly generated mixed context in Figure 10. The top five recommendations from three methods for this context are shown in Figure 9. Notice that using only the reference query produces popular queries, none of which are related to the context in the least. Meanwhile, blindly using the context produces suggestions that are swamped by the off-task queries. This is in stark contrast to using the hard task model, which suggests four decent looking suggestions, three of which have non-zeros reciprocal rank values.
Another observation from this example is the amount of

noise: very popular queries such as "google" appear in the suggestion lists. This is in part due to not finely tuning the various parameters of the underlying recommendation algorithm we used--something we avoided since the recommendation algorithm was not the focus of this work. It is likely also due to the age and scope of the AOL query log, which is not large compared to the commercial logs commonly used in query recommendation research. Nonetheless, the context-aware recommendation models we presented in Section 4 are compatible with any recommendation algorithm that supplies a sufficiently long, scored list of suggestions. We leave the investigation as to whether performance follows for future work.
Many of the tasks for which task-aware recommendation performed well involved specialization reformulations. Some examples include: heart rateslow fast heart rate, bobcat tractor bobcat tractor attachment, disneyland hotel disneyland hotel reviews, and elliptical trainer elliptical trainer benefits. One possible reason for this is that incorporating recommendations for a context query that is a subset of the reference query focuses the final recommendations on the most important concepts.
None of the experiments used notions of temporal sessions or complete user histories. We did this mainly because the mixed contexts were generated and not from actual user logs where context windows could be tied to temporal boundaries, e.g., two-day windows. We believe that by focusing on factors such as on- and off-task queries, we struck at the core questions in this space. We leave testing whether the results from these experiments port to real user data to future work, but we believe they will, especially given the results of related studies, such as those conducted by Liao et al. [14] and Filali et al. [8].
7. SUMMARY AND FUTURE WORK
In this work, we investigated four key research questions surrounding context-aware query recommendation and the effects of on- and off-task recommendation: (RQ1) How does on-task context affect recommendation quality relative to using only the most recently used query? (RQ2) How does off-task context affect recommendation quality? (RQ3) How does mixed context consisting of on- and off-task queries affect recommendation quality? and (RQ4) How do taskaware recommendation models affect performance given a mixed context?
We designed four sets of experiments that used random walks over a term-query graph of the 2006 AOL query logs for recommendations and the 2010­2011 TREC Session Track data for tasks. We evaluated all models using microaveraged mean reciprocal rank (MRR) over the 212 TREC tasks and used 50 trials when randomization was needed. The results of our experiments show that on-task context is, on average, very helpful, but can sometimes degrade per-

91

formance substantially. On the other hand, off-task context only degrades performance. We found that leveraging mixed contexts, when used without regard of the task corresponding to each of its constituent queries, reduced the quality of recommendations. Finally, we demonstrated the effectiveness of four task-aware models, which rely on a state-of-theart search task identification algorithm. Three of the four models out-performed using only the most recently submitted query when up to ten off-task queries were added. Results suggest that two of those models would maintain their accuracy at large levels of noise since their weighting schemes completely ignore any query classified as off-task.
In many ways these results are not surprising: incorporating relevant queries improves query accuracy just as relevance feedback improves document ranking. What is surprising, though, is the extraordinary sensitivity of these approaches to off-task queries. Even making a single mistake and including an off-task query can shift many approaches in the wrong direction, away from relevant documents. This work takes a methodical look at the relative impact of onand off-task queries and provides a deeper understanding of their inter-relationships than had been reported previously.
In addition, our findings rely on publicly accessible datasets, which eases others' efforts in validating and reproducing our results. Furthermore, it allows others to compare directly with our results if they so desire. The simulated contexts and corresponding recommendations used in our experiments are available on our website.3
There are many avenues of future work. One element to explore is the portability of our findings to real user search behavior over many more tasks. Another is to consider other recommendation algorithms, including ones not dependent on query logs [1] and ones that use other user behavior [7, 19]. We only considered the effectiveness of query recommendations to address the current task, but other measures of suggestion quality exists [16], such as attractiveness and diversity; exploring these other evaluations would provide a fuller picture of the value of task-aware recommendation. Other future directions include expanding the analysis to other applications, such as website or task recommendation [20].
8. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
References
[1] S. Bhatia, D. Majumdar, and P. Mitra. Query suggestions in the absence of query logs. In Proc. of SIGIR, pages 795­804, 2011.
[2] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and S. Vigna. The query-flow graph: model and applications. In Proc. of CIKM, pages 609­618, 2008.
[3] F. Bonchi, R. Perego, F. Silvestri, H. Vahabi, and R. Venturini. Efficient query recommendations in the long tail via center-piece subgraphs. In Proc. of SIGIR, pages 345­354, 2012.
3http://ciir.cs.umass.edu/downloads/ task-aware-query-recommendation

[4] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and H. Li. Context-aware query suggestion by mining clickthrough and session data. In Proceeding of KDD, pages 875­883, 2008.
[5] H. Cao, D. Jiang, J. Pei, E. Chen, and H. Li. Towards context-aware search by learning a very large variable length hidden Markov model from search logs. In Proc. of WWW, pages 191­200, 2009.
[6] G. Cormack, M. Smucker, and C. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information retrieval, 14(5):441­465, 2011.
[7] S. Cucerzan and R. W. White. Query suggestion based on user landing pages. In Proc. of SIGIR, pages 875­ 876, 2007.
[8] K. Filali, A. Nair, and C. Leggetter. Transitive historybased query disambiguation for query reformulation. In Proc. of SIGIR, pages 849­850, 2010.
[9] C. Huang, L. Chien, and Y. Oyang. Relevant term suggestion in interactive web search based on contextual information in query session logs. JASIST, 54(7):638­ 649, 2003.
[10] R. Jones and K. L. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. Proceedings of CIKM 2008, 2008.
[11] E. Kanoulas, P. Clough, B. Carterette, and M. Sanderson. Session track at TREC 2010. In Proc. of the SIGIR Workshop on the Simulation of Interaction, pages 13­14, 2010.
[12] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the TREC 2011 Session Track. In Proc. of TREC, 2011.
[13] A. Kotov, P. N. Bennett, R. W. White, S. T. Dumais, and J. Teevan. Modeling and analysis of cross-session search tasks. In Proc. of SIGIR, pages 5­14, 2011.
[14] Z. Liao, Y. Song, L.-w. He, and Y. Huang. Evaluating the effectiveness of search task trails. In Proc. of WWW, pages 489­498, 2012.
[15] C. Lucchese, S. Orlando, R. Perego, F. Silvestri, and G. Tolomei. Identifying task-based sessions in search engine query logs. In Proc. of WSDM, pages 277­286, 2011.
[16] Z. Ma, Y. Chen, R. Song, T. Sakai, J. Lu, and J. Wen. New assessment criteria for query suggestion. In Proc. of SIGIR, pages 1109­1110, 2012.
[17] D. Metzler and W. Croft. Combining the language model and inference network approaches to retrieval. Information processing & management, 40(5):735­750, 2004.
[18] F. Radlinski and T. Joachims. Query chains: learning to rank from implicit feedback. In Proc. of KDD, pages 239­248, 2005.
[19] Y. Song and L.-w. He. Optimal rare query suggestion with implicit user feedback. In Proc. of WWW, pages 901­910, 2010.
[20] G. Tolomei, S. Orlando, and F. Silvestri. Towards a task-based search and recommender systems. In ICDEW, pages 333­336, 2010.

92

Learning to Combine Representations for Medical Records Search

Nut Limsopatham1, Craig Macdonald2, Iadh Ounis2 nutli@dcs.gla.ac.uk1, firstname.lastname@glasgow.ac.uk2
School of Computing Science University of Glasgow, Glasgow, UK

ABSTRACT
The complexity of medical terminology raises challenges when searching medical records. For example, `cancer', `tumour', and `neoplasms', which are synonyms, may prevent a traditional search system from retrieving relevant records that contain only synonyms of the query terms. Prior works use bag-of-concepts approaches, to deal with this by representing medical terms sharing the same meanings using concepts from medical resources (e.g. MeSH). The relevance scores are then combined with a traditional bag-of-words representation, when inferring the relevance of medical records. Even though the existing approaches are effective, the predicted retrieval effectiveness of either the bag-of-words or bag-ofconcepts representation, which may be used to effectively model the score combination and hence improve retrieval performance, is not taken into account. In this paper, we propose a novel learning framework that models the importance of the bag-of-words and the bag-of-concepts representations, combining their scores on a per-query basis. Our proposed framework leverages retrieval performance predictors, such as the clarity score and AvIDF, calculated on both representations as learning features. We evaluate our proposed framework using the TREC Medical Records track's test collections. As our proposed framework can significantly outperform an existing approach that linearly merges the relevance scores, we conclude that retrieval performance predictors can be effectively leveraged when combining the relevance scores.
Categories and Subject Descriptors: H.3.3 [Information Search & Retrieval]: Search process
General Terms: Experimentation, Performance
Keywords: Medical Records Search; Regression; Controlled Vocabulary; Retrieval Performance Predictors
1. INTRODUCTION
Medical terminology, which can be complex, inconsistent, and ambiguous, poses an important challenge when searching in the medical domain [9, 10, 12, 15, 16]. For exam-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

ple, `heart disease' can be referred to as `coronary artery disease', `coronary heart disease', or `CHD'. This means that traditional search systems may not be able to retrieve medical documents relevant to a query, if those documents contain only synonyms of the query terms. To tackle this, prior works (e.g. [2, 8]) proposed bag-of-concepts (BoC) approaches to represent medical documents and queries using concepts from medical resources, such as MeSH1 and UMLS Metathesaurus2. Under these approaches, `heart disease', `coronary artery disease', `coronary heart disease', and `CHD', which share the similar meaning, are represented with the same concept. For instance, Aronson [2] deployed MetaMap [3] to identify medical concepts in medical records and queries and represented them in the form of the UMLS Concept Unique Identifiers (CUIs). Intuitively, such approaches should alleviate the terminology mismatch problem. However, empirical studies [15, 16] have shown that the BoC performance can be inconsistent, sometimes underperforming the traditional bag-of-words representation (BoW), since not all documents and queries could be effectively represented using medical concepts. For example, medical concepts may not be found in some queries. To cope with such a challenge, other works (e.g. [9, 15, 16]) combined the relevance scores of both BoW and BoC when inferring the relevance of a document. In particular, Srinivasan [15] proposed the so-called score combination approach that linearly combines the relevance scores from both BoW and BoC, when inferring the relevance of a document d towards a query Q, as follows [15]:

score(d, Q) = · scoreBoW (d, Q)

(1)

+ scoreBoC (d, Q)

where  is a parameter to emphasise the relevance score computed using BoW, which is set to 2.00 for all queries, as suggested in [9, 15].
In the context of medical records search, Limsopatham et al. [9] improved retrieval performance markedly by using the aforementioned score combination to merge the relevance scores from the BoW and their proposed task-specific representation (i.e. a BoC). They showed that combining the relevance scores from BoW and BoC is effective for searching in the medical domain. Importantly, these score combination approaches merge the relevance scores computed from both BoW and BoC representations by fixing a particular weight irrespective of the query.
We hypothesise that by learning a weight for BoW and BoC on a per-query basis, we can rank medical records more effectively. In this paper, we propose a novel learning
1http://www.ncbi.nlm.nih.gov/mesh 2http://www.nlm.nih.gov/research/umls/

833

framework to model the importance of BoW and BoC, when inferring the relevance of a medical record. Our proposed regression-based learning framework leverages retrieval performance predictors, such as the clarity score [5] and query scope [7], computed on both BoW and BoC as features, to learn an effective combination model on a per-query basis.
We evaluate our proposed framework in the context of the TREC 2011 [19] and 2012 [18] Medical Records track. Our results show that our learning framework is effective. Indeed, it significantly outperforms an existing strong score combination baseline.
The main contributions of this paper are threefold:
1. We show that some particular queries benefit more from a bag-of-words (BoW) representation, while the others profit from a bag-of-concepts (BoC) representation.
2. We propose a novel regression-based learning framework to model the importance of BoW and BoC using retrieval performance predictors, when inferring the relevance of medical records.
3. We thoroughly evaluate our proposed framework using standard collections provided by the TREC 2011 and 2012 Medical Records track.
The remainder of this paper is structured as follows. Section 2 introduces our novel regression framework that leverages retrieval performance predictors to learn an effective score combination model. Our experimental setup and results are presented in Sections 3 and 4. Finally, we provide concluding remarks in Section 5.
2. OUR PROPOSED FRAMEWORK
In this section, we describe our novel learning framework that models the combination of the relevance scores from the bag-of-words (BoW) and the bag-of-concepts (BoC) representations, for medical records search. The central idea is that queries may benefit differently from BoW and BoC; hence, we propose to learn a weight for BoW and BoC on a per-query basis. To do so, we use retrieval performance predictors as learning features to estimate the predicted retrieval effectiveness of each representation, when estimating the relevance scores of a medical record. In particular, we deploy a regression technique to learn the importance of the two representations when combining their relevance scores.
Our framework consists of four components:
1. A score combination model. 2. A procedure to estimate the model parameter for a
query. 3. A set of learning features to learn the model. 4. A regression procedure to infer the model using the
learning features.
In the remainder of this section, we describe each of these four components.
2.1 A Score Combination Model
To take advantage of both BoW and BoC, we follow [15] and combine the relevance scores of a medical record d towards a query Q as follows:

score(d, Q) =Q · scoreBoW (d, Q)

(2)

+ (1 - Q) · scoreBoC(d, Q)

where Q (0  Q  1) is a per-query parameter to estimate the importance of the relevance scores computed using

Table 1: List of learning features used to predict the

importance of the relevance scores from the bag-of-

words (BoW) and bag-of-concepts (BoC) represen-

tations.

ID Feature ­ Ratio (BoW/BoC)

1 Clarity Score [5]

2 SCQ [21]

3 MAXCQ [21]

4 NSCQ [21]

5 AvICTF [4]

6 AvIDF [4]

7 EnIDF [4]

8 Query Scope () [7]

9 AvPMI [4]

10 1 [7]

11 2 [7]

12 Query length [7]

the bag-of-words (BoW) and bag-of-concepts (BoC) representations. The higher the Q, the more the relevance score depends on BoW. Indeed, to generalise the model, we introduce a modification to Equation (1) of [15] with respect to the weighting between the relevance scores of BoW and BoC, so that our combination model can take into account the situation where only BoW (Q = 1) or BoC (Q = 0) is individually effective. In addition, when Q = 0.667, our model could produce the same list of medical records as Equation (1) with the recommended setting (i.e.  = 2.00), since the proportion of relevance scores from BoW and BoC computed by Equations (1) and (2) are equal.
2.2 Estimating the Combination Model
Next, in order to estimate an effective Q of the combination model, described in Section 2.1 (Equation (2)), on the training set, we identify the best Q that achieves the optimal retrieval effectiveness in terms of a particular retrieval measure (e.g. infNDCG) for each training query. Indeed, for each query, we sweep the Q parameter between 0 and 1 to find the best combination model in terms of the retrieval performance for that query. The identified effective Q parameter is used as the weight for the learning component of our framework to learn an effective combination model from the retrieval performance prediction features.
2.3 Learning Features
We next identify the features that we will use to choose the weight for an unseen query. These features should generalise across queries and correlate well with the Q that could result in the optimal retrieval performance. Table 1 lists our features. In particular, as previously discussed in Section 1, we propose to use existing retrieval performance predictors to estimate the retrieval performance of BoW and BoC. Hence, we use the ratio between the retrieval performance predictors computed on BoW and BoC, as the learning features. Specifically, the first set of features (Features 1-4), including the clarity score [5], SCQ [21], MaxSCQ [21] and NSCQ [21], consider the ambiguity of a query by measuring the coherence of the language used in each medical record. The more similar the query model is to the collection model, the better the retrieval performance would be expected. The next set of features (Features 5-8) measure the specificity of each query within a representation approach. Indeed, queries with explicit intents could result in a better performance than queries with general terms. The features include Average Inverse Collection Term Fre-

834

quency (AvICTF) [4], Average Inverse Document Frequency (AvIDF) [4], EnIDF [4], and the query scope () [7]. Next, Feature 9, the Average of the Pointwise Mutual Information over all query term pairs (AvPMI) [4], focuses on the relationship between query terms. The more co-occurrences among query terms, the better the chance that the relevant documents are being retrieved. Features 10-11 measure the distribution of informativeness among the query terms (i.e. 1 and 2 [7]), as a query with informative terms could attain an effective retrieval performance. Finally, Feature 12 is the number of non-stopword query terms, which could impact the normalisation methods of the probabilistic retrieval models, and hence affect retrieval performance [7].
2.4 Inferring the Combination Model using Regression Trees
We view the task of estimating the importance of different representation approaches as a supervised regression problem, where the objective is to predict a proper weight (Q) for each query, based on effective weights for similar training queries. By doing so, we would benefit from the fact that several retrieval performance predictors of the representation approaches can be used as learning features, when combining the relevance scores.
While any regression learners could be used here, we deploy the Gradient Boosted Regression Trees (GBRT) [17] (as implemented in the jforests package [6]3) to learn the combination model discussed in Section 2.1, as it has been shown to be effective in several search and regression tasks (e.g. [17, 20]). We use the root-mean-square error (RMSE) as the loss function when learning a combination model. Our proposed framework leverages retrieval performance predictors, introduced in Section 2.3, as learning features for the GBRT learner.
3. EXPERIMENTAL SETUP
In this section, we discuss our experimental setup when evaluating our proposed framework. In particular, Section 3.1 describes the used test collections and Section 3.2 discusses our ranking strategies.
3.1 Test Collection
We evaluate our framework using the 34 and 47 queries from the TREC 2011 and 2012 Medical Records track [18, 19], respectively. The task is to retrieve patient visits relevant to a given query. Indeed, a patient visit is identified by the medical records associated with a particular visit to a hospital by a patient. The collection contains about 102k medical records, which are associated with 17,265 patient visits [18, 19].
TREC deployed various measures to cope with the possible incompleteness of the gold-standard relevance judgements. In particular, bpref is used as the official measure for TREC 2011 [19], while infNDCG and infAP are used for TREC 2012 [18].
3.2 Ranking Approaches
We index the medical records using Terrier [14]. For the bag-of-words (BoW) representation, we apply Porter's English stemmer and remove stopwords. For the bag-ofconcepts (BoC) representation, we follow [9] and apply the so-called task-specific representation to represent medical records and queries using only medical concepts related to
3http://code.google.com/p/jforests/

the medical decision criteria (namely, symptom, diagnostic test, diagnosis, and treatment), as it has been shown to be effective for medical records search. In all experiments, the effective parameter-free DPH term weighting model [1] is used to rank medical records. To learn the combination model, when ranking medical records, we use the default setting of GBRT from the jforests package. We use a 5-fold cross validation across the 34 topics of TREC 2011 and 47 topics of TREC 2012, where each fold has separate training and test query sets. When training the combination model, we target the bpref and infNDCG retrieval measures for TREC 2011 and 2012 topics sets, respectively. Finally, to rank patient visits based on the relevance scores of their associated medical records, we use the expCombSUM voting technique [13], which gives more importance to the highly relevant medical records. Following [11], the number of medical records voting for the relevance of patient visits is limited to 5,000.
4. EXPERIMENTAL RESULTS
We evaluate the retrieval effectiveness of our proposed framework to learn an effective combination model of the bag-of-words (BoW) and the bag-of-concepts (BoC) representations using the retrieval performance predictors discussed in Section 2. Table 2 compares the retrieval performance of our framework on the TREC 2011 and 2012 Medical Records track test collection with three baselines, including a traditional bag-of-words representation (BoW), a task-specific representation [9] (BoC), and an existing score combination approach [15] (i.e. Equation (1)) with the suggested setting from [9, 15]. In addition, to evaluate the optimal potential effectiveness, the best retrieval performances that our proposed framework and the existing score combination could achieve are also reported (denoted oracle).
From Table 2, we observe the following. First, we see that for both TREC 2011 and TREC 2012 topics sets, both our proposed framework and the existing score combination approach markedly outperform the baselines where either of the representations are taken into account. This shows that combining the relevance scores from BoW and BoC is effective for medical records search. Next, for the TREC 2012 topics set, the retrieval performances of our framework (5fold) markedly outperform those of the score combination baseline ( = 2). In particular, in terms of the infNDCG retrieval performance, our framework (infNDCG 0.4723) significantly outperforms (paired t-test, p < 0.05) the existing score combination baseline (infNDCG 0.4557). For the infAP measure, our proposed framework performs markedly better than the score combination baseline (+6.5% improvement, from 0.1975 to 0.2133). In addition, our proposed framework (5-fold) also results in a markedly better retrieval effectiveness than the best possible setting of the score combination baseline (oracle). Indeed, in terms of infNDCG, our proposed framework significantly outperforms the score combination with the best setting for upto 3.21% (p < 0.05). For the infAP retrieval measure, our regression-based framework performs +4.50% better than the best setting of the score combination. However, for the TREC 2011 topics set, our framework (5-fold) could not outperform the score combination ( = 2) baseline (bpref 0.5078 vs. 0.5118). This is partially due to the fact that the TREC 2011 topics set contains only 34 queries; hence, with a small number of queries, when we conduct a 5-fold cross validation, the training and test sets could not generalise.
Finally, we discuss the optimal retrieval performance that our proposed framework could achieve to evaluate the po-

835

Table 2: The retrieval performances of different representation approaches on TREC 2011 and 2012 Medical

Records track test collections. Statistical significance (paired t-test) at p < 0.05, at p < 0.01, and at p < 0.001

over a baseline are denoted a, aa and aaa, respectively. a is 1, 2, 3, 4 or 5 to represent the bag-of-words

representation (BoW), the task-specific representation (BoC), the score combination ( = 2), our learning

framework (5-fold), or the score combination (oracle) baselines, respectively.

Approaches

2011 bpref

infNDCG

2012

infAP

Bag-of-words representation (BoW)

0.4871

0.4167

0.1703

Task-specific representation (BoC)

0.4929

0.4218

0.1920

Score Combination [15] ( = 2)

0.5118

0.455711

0.19751

Our learning framework (5-fold)

0.5078

0.472311,22,3

0.21331,2

Score Combination [15] (oracle)

0.5121

0.46041,2,4

0.20481

Our learning framework (oracle)

0.5796111,222,333,444,555 0.5130111,222,333,444,555 0.2381111,222,33,444,555

tential effectiveness of our framework, if more training data were available. As expected, we observe that, with the best setting, our framework (oracle) significantly (p < 0.01) outperforms all of the approaches discussed in this paper. This supports our hypothesis that some particular queries differently benefit from BoW and BoC. In particular, the retrieval performance of our framework with the best setting is upto +17.06% better than the 5-fold cross validation. Importantly, we find that the mean of the effective weights (Q with the best possible setting) across the two collections is 0.48459 (0  Q  1), while the standard deviation is 0.38085, which suggests that the effective weight should indeed vary across topics. For example, to attain an effective retrieval performance when a query contains multiple complex concepts (e.g. topic#106: patients who had positron emission tomography (PET), magnetic resonance imaging (MRI), or computed tomography (CT) for staging or monitoring of cancer),  in the combination model (Equation (2)) should be low, if all the concepts in the query can be effectively identified. From this, we conclude that there is no one combination of BoW and BoC that is effective for all queries. Hence, per-query prediction approaches, like the ones deployed here, have great potential to improve medical records search. However, there is still an open research area to explore effective features and learners to close the performance gap between the cross-validation and oracle regimes, even though by deploying the existing learner and features, our framework could in general markedly and significantly outperform the existing score combination approach [15].
5. CONCLUSIONS
We have tackled the challenge of dealing with the complex and ambiguous terminology in medical records search by modelling the combination of the relevance scores from both bag-of-words (BoW) and bag-of-concepts (BoC) representations. We have proposed a regression-trees-based learning framework that can effectively handle this combination using the Gradient Boosted Regression Trees to learn an effective combination model via retrieval performance predictors, such as the clarity score [5] and the query scope [7]. We have shown that our proposed framework is effective for the medical records search, as it could markedly and significantly outperform an effective score combination approach [15].
6. REFERENCES
[1] G. Amati, E. Ambrosi, M. Bianchi, C. Gaibisso, and G. Gambosi. FUB, IASI-CNR and University of Tor Vergata at TREC 2007 Blog Track, In TREC 2007.

[2] A. R. Aronson. Exploiting a Large Thesaurus for Information Retrieval. In RIAO 1994.
[3] A. R. Aronson and F. Lang. An Overview of MetaMap: historical perspective and recent advances. J. Am. Med. Inform. Assoc., 17(3), 2010.
[4] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services, 2(1), 2010.
[5] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting Query Performance. In SIGIR 2002.
[6] Y. Ganjisaffar, R. Caruana, and C. V. Lopes. Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models. In SIGIR 2011.
[7] B. He and I. Ounis. Query Performance Prediction. Inf. Syst., 31(7), 2006.
[8] W. Hersh, D. Hickam, R. Haynes, and K. McKibbon. A Performance and Failure Analysis of SAPHIRE with a MEDLINE Test Collection. J. Am. Med. Inform. Assoc., 1(1), 1994.
[9] N. Limsopatham, C. Macdonald, and I. Ounis. A Task-Specific Query and Document Representation for Medical Records Search. In ECIR 2013.
[10] N. Limsopatham, C. Macdonald, and I. Ounis. Inferring Conceptual Relationships to Improve Medical Records Search. In OAIR 2013.
[11] N. Limsopatham, C. Macdonald, I. Ounis, G. McDonald, M. Bouamrane. University of Glasgow at Medical Records track 2011: Experiments with Terrier. In TREC 2011.
[12] N. Limsopatham, R. L. T. Santos, C. Macdonald, and I. Ounis. Disambiguating Biomedical Acronyms using EMIM. In SIGIR 2011.
[13] C. Macdonald and I. Ounis. Voting for Candidates: adapting data fusion techniques for an expert search task. In CIKM 2006.
[14] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A High Performance and Scalable Information Retrieval Platform. In OSIR at SIGIR 2006.
[15] P. Srinivasan. Optimal Document-Indexing Vocabulary for MEDLINE. Inf. Process. Manage., 32(5), 1996.
[16] D. Trieschnigg, D. Hiemstra, F. de Jong, and W. Kraaij. A Cross-Lingual Framework for Monolingual Biomedical Information Retrieval. In CIKM 2010.
[17] S. Tyree, K. Q. Weinberger, K. Agrawal, and J. Paykin. Parallel Boosted Regression Trees for Web Search Ranking. In WWW 2011.
[18] E. Voorhees and W. Hersh. Overview of the TREC 2012 Medical Records Track. In TREC 2012.
[19] E. Voorhees and R. Tong. Overview of the TREC 2011 Medical Records Track. In TREC 2011.
[20] Y. Wang, B. Wei, J. Yan, Y. Hu, Z. H. Deng, and Z. Chen. A Novel Local Patch Framework for Fixing Supervised Learning Models. In CIKM 2012.
[21] Y. Zhao, F. Scholer, and Y. Tsegay. Effective Pre-retrieval Query Performance Prediction Using Similarity and Variability Evidence. In ECIR 2008.

836

The Cluster Hypothesis for Entity Oriented Search

Hadas Raviv, Oren Kurland
Faculty of Industrial Engineering and Management, Technion, Haifa 32000, Israel hadasrv@tx.technion.ac.il,kurland@ie.technion.ac.il

David Carmel
Yahoo! Research, Haifa 31905, Israel
david.carmel@ymail.com

ABSTRACT
In this work we study the cluster hypothesis for entity oriented search (EOS). Specifically, we show that the hypothesis can hold to a substantial extent for several entity similarity measures. We also demonstrate the retrieval effectiveness merits of using clusters of similar entities for EOS.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
Keywords: cluster hypothesis, entity oriented search
1. INTRODUCTION
The entity oriented search (EOS) task has attracted much research attention lately. The main goal is to rank entities in response to a query by their presumed relevance to the information need that the query expresses. For example, the goal in the TREC's expert search task was to rank employees in the enterprise by their expertise in a topic [6]. The goal in INEX entity ranking track was to retrieve entities that pertain to a topic in the English Wikipedia [7, 8, 9]. The goal in TREC's entity track was to rank Web entities with respect to a given entity by their relationships [2, 3, 4].
The EOS task is different than the standard ad-hoc document retrieval task as entities are somewhat more complex than (flat) documents. That is, entities are characterized by different properties such as name, type (e.g., place or person), and potentially, an associated document (e.g., a homepage or a Wikipedia page). Despite the fundamental difference between the two tasks, we set as a goal to study whether an important principle in ad-hoc document retrieval also holds for the EOS task; namely, the cluster hypothesis [22]. We present the first study of the cluster hypothesis for EOS, where the hypothesis is that "closely associated entities tend to be relevant to the same requests".
We use several inter-entity similarity measures to quantify the association between entities, which is a key point in the hypothesis. These measures are based on the entity type which is a highly important source of information [18, 14]. We then show that the cluster hypothesis, tested using
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Voorhees' nearest neighbor test [23], can hold to a substantial extent for EOS for several of the similarity measures.
Motivated by the findings about the cluster hypothesis, we explore the merits of using clusters of similar entities for entity ranking. We show that ranking entity clusters by the percentage of relevant entities that they contain can be used to produce extremely effective entity ranking. We also demonstrate the effectiveness of using cluster ranking techniques that are based on estimating the percentage of relevant entities in the clusters for entity ranking.
Our main contributions are three fold: (i) showing that for several inter-entity similarity measures the cluster hypothesis holds for EOS to a substantial extent as determined by the nearest neighbor test; (ii) demonstrating the considerable potential of using clusters of similar entities for EOS; and, (iii) showing that using simple cluster ranking methods can help to improve retrieval performance with respect to that of an effective initial search.
2. RELATED WORK
Any entity in the INEX entity ranking track, which we use for our experiments, has a Wikipedia page. Hence, some previously proposed estimates for the entity-query similarity are based on Wikipedia's category information [20, 1, 18, 14]. Similarly, we measure the similarity between two entities based on several measures of the similarity between the sets of categories of the two entity pages.
In Cao et al.'s work on expert search [5] the retrieval score assigned to an entity was smoothed with the retrieval score assigned to a cluster constructed from the entity. In contrast, we explore a retrieval paradigm that ranks entity clusters and transforms the ranking to entity ranking. Work on document retrieval showed that cluster-based (score) smoothing and cluster ranking are complementary [16]. We leave the exploration of this finding for the EOS task for future work.
There are several tests of the cluster hypothesis for document retrieval [13, 10, 23, 19]. We use Voorhees' nearestneighbor test for the EOS task as it is directly connected with the nearest neighbor clusters we use for ranking.
The findings we present for the EOS task echo those reported for document retrieval. Namely, the extent to which the cluster hypothesis holds [23], and the (potential) merits of using cluster ranking [12, 21, 15, 17].
3. THE CLUSTER HYPOTHESIS
Our first goal is to explore the extent to which the cluster hypothesis holds for EOS. To this end, we use the nearest

841

neighbor test [23]. Let L[qn] be the list of n entities that are the highest ranked by an initial search performed in response to query q. For each relevant entity in L[qn], we record the percentage of relevant entities among its K nearest neighbors in L[qn]. The nearest neighbors are determined using one of the inter-entity similarity measures specified in Section 5.1. The test result is the average of the recorded percentages over all relevant entities in L[qn], averaged over all test queries.
Some of the inter-entity similarity measures assign discrete values including 0. Hence, for some relevant entities there could be less than K neighbors as we do not consider neighbors with a 0 similarity value. In addition, a relevant entity might be assigned with more than K nearest neighbors due to ties in the similarity measure. That is, we keep collecting all entities having the same similarity value as that of the last one in the K neighbors list.

4. CLUSTER-BASED ENTITY RANKING

Our second goal is studying the potential merits of us-

ing entity clusters to induce entity ranking. We re-rank the

initial entity list L[qn] using a cluster-based paradigm which

is very common in work on document retrieval [17]. Let

Cl(L[qn]) be the set of clusters created from L[qn] using some

clustering method. The inter-entity similarity measures used

for creating clusters are those used for testing the cluster hy-

pothesis. (See Section 5.1 for further technical details.) The

clusters in Cl(L[qn]) are ranked by the presumed percentage

of relevant entities that they contain. Below we describe

two cluster ranking methods. Then, each cluster is replaced

with its constituent entities while omitting repeats. Within

cluster entity ranking is based on the initial entity retrieval

scores which were used to create the list L[qn].

The MeanScore cluster ranking method scores cluster

c by the mean retrieval score of its constituent entities:

1 |c|

ec Sinit(e; q); Sinit(e; q) is the initial retrieval score

of entity e; |c| is the number of entities in c.

When Sinit(e; q) is a rank equivalent estimate to that of

log(P r(q, e)) [18], the cluster score assigned by MeanScore

is rank equivalent to the geometric mean of the joint query-

entity probabilities' estimates in the cluster. Using a geometric-

mean-based representation for document clusters was shown

to be highly effective for ranking document clusters [17].

The regularized mean score method, RegMeanScore in

short, which is novel to this study, smoothes c's score:

. ec

Sinit (e;q)+

1 n

eL[qn] Sinit(e;q)

|c|+1

The cluster score is the

mean retrieval score of a cluster composed of c's entities

and an additional "pseudo" entity whose score is the mean

score in the initial list. This method helps to address, among

others, cluster-size bias issues.

5. EVALUATION
5.1 Experimental setup
We conducted experiments with the datasets of the INEX entity ranking track of 2007 [7], 2008 [8], and 2009 [9]. Table 1 provides a summary of the datasets. The tracks for 2007 and 2008 used the English Wikipedia dataset from 2006, while the 2009 track used the English Wikipedia from 2008. The set of test topics for 2007 is composed of 21 topics that

Data set
2007 2008 2009

Collection size
4.4 GB 4.4 GB 50.7 GB

# of documents
659, 388 659, 388 2, 666, 190

# of test topics
46 35 55

Table 1: INEX entity ranking datasets.

were derived from the ad hoc 2007 assessments, and additional 25 topics that were created by the participants specifically for the track. In 2008, 35 topics were created and used for testing. The topics used for testing in 2009 were 55 topics out of the 60 test topics used in 2007 and 2008.
We used Lucene (http://lucene.apache.org/core/) for experiments. The data was pre-processed using Lucene, including tokenization, stopword removal, and Porter stemming.
Inter-entity similarity measures. The inter-entity simi-
larity measures that we use utilize Wikipedia categories. Specifically, the categories associated with the Wikipedia page of the entity, henceforth referred to as its category set, serve as the entity type.
The Tree similarity between two entities e1 and e2 is exp(-d(e1, e2)) where d(e1, e2) is the minimum distance over Wikipedia's categories graph between a category in e1's category set and a category in e2's category set;  is a decay constant determined as in [18]. The SharedCat measure is the cosine similarity between the binary vectors representing two entities. An entity vector is defined over the categories space. An entry in the vector is 1 if the corresponding category is associated with the entity and 0 otherwise. Thus, SharedCat measures the (normalized) number of categories shared by the two entities [20]. The CE measure is based on measuring the language-model-based similarity between the documents associated with the category sets of two entities [14]. More specifically, each category is represented in this case by the text that results from concatenating all Wikipedia pages associated with the category. The similarity between the texts x and y that represent two categories is exp(-CE(p[x0](·)||p[yµ](·))); CE is the cross entropy measure; p[zµ](·) is the Dirichlet-smoothed unigram language model induced from z with the smoothing parameter µ (=1000). The CE similarity between two entities is defined as the maximal similarity, over all pairs of categories, one in the first entity's category set and the other in the second entity's category set, of the texts representing the categories. Finally, the ESA (Explicit Semantic Analysis) [11] similarity measure is the cosine between two vectors, each represents the category set of an entity. The vectors representing the category sets are defined over the entities space. The value of an entry in the vector is the number of the categories in the given category set that are associated with the corresponding entity. Using ESA to measure inter-entity similarity is novel to this study.
Three different initially retrieved entity lists, L[qn], are used for both the cluster hypothesis test and cluster-based ranking. The lists are created in response to the query using highly effective entity retrieval methods [18]. The first list, LDoc, is created by representing an entity with its Wikipedia document (page). The documents are ranked in response to the query using the standard language-model-based approach with Dirichlet-smoothed unigram language models and the cross entropy similarity measure. The second list,

842

Similarity measure Tree
SharedCat CE ESA

Initial list LDoc LDoc;T ype LDoc;T ype;N ame
LDoc LDoc;T ype LDoc;T ype;N ame
LDoc LDoc;T ype LDoc;T ype;N ame
LDoc LDoc;T ype LDoc;T ype;N ame

2007 30.0 29.8 32.7
35.7 33.5 37.9
33.4 34.5 37.5
34.3 33.7 37.3

2008 32.0 35.5 37.7
41.0 45.5 44.3
36.2 38.6 41.7
36.2 41.0 39.1

2009 42.7 44.9 44.9
45.4 52.2 52.7
46.0 50.3 49.7
46.2 49.5 49.0

Table 2: The cluster hypothesis test: the average percentage of relevant entities among the 5 nearest neighbors of a relevant entity.

LDoc;T ype, is created by scoring entities with an interpolation of two scores. The first is that used to create the list LDoc. The second is the similarity between the category set of the entity and the query target type (the set of categories that are relevant to the query, as defined by INEX topics). The Tree estimate described above is used for measuring similarity between the two category sets. The third list, LDoc;T ype;Name, is created by scoring an entity with an interpolation of the score used to create LDoc;T ype, and an estimate for the proximity-based association [18] between the query terms and the entity name (i.e., the title of its Wikipedia page) in the corpus. We employ the same traintest approach as in [18] to set the free-parameter values of the ranking methods used to create the initial lists. The number of entities in each initial list L[qn] is n = 50.
We use a simple nearest neighbor clustering method to cluster entities in the initial list L[qn]. Specifically, each entity in L[qn] and the K (= 5) entities in L[qn] that are the most similar to it, according to the inter-entity similarity measures described above, form a cluster. Using such small overlapping clusters was shown to be highly effective for cluster-based document retrieval [16, 15, 17]. We note that not all clusters necessarily contain K + 1 documents due to the reasons specified in Section 3. For consistency, we also use K = 5 in the cluster hypothesis test.
Following the INEX guidelines, the evaluation metric for INEX 2007 is mean average precision (MAP) while that for INEX 2008 and 2009 is infAP. We also report the precision of the top 5 entities (p@5). Statistically significant differences of retrieval performance are determined using the two tailed paired t-test with a 95% confidence level.
5.2 Experimental results
5.2.1 The cluster hypothesis
Table 2 presents the results of the nearest neighbor cluster hypothesis test that was described in Section 3. The test is performed on the different initially retrieved entity lists using the various inter-entity similarity measures. We see that the average percentage of relevant entities among the nearest neighbors of a relevant entity ranges between 30% and 53% across the various experimental settings. We also found out that, on average, the percentage of relevant entities in a list is often lower than 25% and can be as low as 10%. Thus, due to the relatively high percentage of relevant enti-

ties among the nearest neighbors of relevant entities, we can conclude that the cluster hypothesis holds to a substantial extent, according to the nearest neighbor test, with various inter-entity similarity measures.
Table 2 also shows that for most of the data sets and similarity measures the test results for the LDoc;T ype and LDoc;T ype;Name lists are higher than for LDoc. This finding is not surprising as LDoc;T ype and LDoc;T ype;Name were created using entity-query similarity measures that account for category information, while the similarity measure used to create LDoc does not use this information. The highest test results are obtained for the SharedCat similarity measure which, as noted above, measures the (normalized) number of shared categories between two entities.
5.2.2 Cluster-based entity ranking
Table 3 presents the results of employing cluster-based entity re-ranking, as described in Section 4, upon the three initial entity lists. The various inter-entity similarity measures are used for creating the clusters. 'Initial' refers to the initial ranking of a list. 'Oracle' is the ranking of entities that results from employing the cluster-based re-ranking paradigm described in Section 4; the clusters are ranked by the true percentage of relevant entities that they contain.
The high performance numbers for Oracle, which are substantially and statistically significantly better than those for Initial, attest to the existence of clusters that contain a very high percentage of relevant entities. More generally, these numbers attest to the incredible potential of employing effective cluster ranking methods to rank entities.
RegMeanScore, which outperforms MeanScore due to the regularization discussed in Section 4, is in quite a few cases more effective than Initial; specifically, using the Tree and SharedCat inter-entity similarity measures. While the improvements for LDoc are often statistically significant, this is not the case for LDoc;T ype and LDoc;T ype;Name. Naturally, the more effective the initial ranking (Initial), the more challenging the re-ranking task. Yet, the very high Oracle numbers for LDoc;T ype and LDoc;T ype;Name imply that effective cluster ranking methods can yield performance that is much better than that of the initial ranking. Finally, for both LDoc;T ype and LDoc;T ype;Name the best performance is in most cases attained by using RegMeanScore.
6. CONCLUSIONS AND FUTURE WORK
We showed that the cluster hypothesis can hold to a substantial extent for the entity oriented search (EOS) task with several inter-entity similarity measures. We also demonstrated the potential merits of using a cluster-based retrieval paradigm for EOS that relies on ranking entity clusters. Devising improved cluster ranking techniques is a future venue we intend to explore.
7. ACKNOWLEDGMENTS
We thank the reviewers for their comments. Part of the work reported here was done while David Carmel was at IBM. This paper is based on work that has been supported in part by the Israel Science Foundation under grant no. 433/12 and by Google's faculty research award. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsors.

843

8. REFERENCES
[1] K. Balog, M. Bron, and M. De Rijke. Query modeling for entity search based on terms, categories, and examples. ACM Trans. Inf. Syst., 29(4), 2011.
[2] K. Balog, A. P. de Vries, P. Serdyukov, P. Thomas, and T. Westerveld. Overview of the trec 2009 entity track. In Proceedings of TREC, 2009.
[3] K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the trec 2010 entity track. In Proceedings of TREC, 2010.
[4] K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the trec 2011 entity track. In Proceedings of TREC, 2011.
[5] Y. Cao, J. Liu, S. Bao, and H. Li. Research on expert search at enterprise track of TREC 2005. In Proceedings of TREC, volume 14, 2005.
[6] N. Craswell, A. P. de Vries, and I. Soboroff. Overview of the trec 2005 enterprise track. In Proceedings of TREC, 2005.
[7] A. P. de Vries, A.-M. Vercoustre, J. A. Thom, N. Craswell, and M. Lalmas. Overview of the INEX 2007 entity ranking track. In Proceedings of INEX, pages 245­251, 2007.
[8] G. Demartini, A. P. de Vries, T. Iofciu, and J. Zhu. Overview of the INEX 2008 entity ranking track. In Proceedings of INEX, pages 243­252, 2008.
[9] G. Demartini, T. Iofciu, and A. P. de Vries. Overview of the INEX 2009 entity ranking track. In Proceedings of INEX, pages 254­264, 2009.
[10] A. El-Hamdouchi and P. Willett. Techniques for the measurement of clustering tendency in document retrieval systems. Journal of Information Science, 13:361­365, 1987.
[11] E. Gabrilovich and S. Markovitch. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In Proceedings of IJCAI, pages 1606­1611, 2007.
[12] M. A. Hearst and J. O. Pedersen. Reexamining the cluster hypothesis: Scatter/Gather on retrieval results. In Proceedings of SIGIR, pages 76­84, 1996.
[13] N. Jardine and C. J. van Rijsbergen. The use of hierarchic clustering in information retrieval. Information Storage and Retrieval, 7(5):217­240, 1971.
[14] R. Kaptein and J. Kamps. Exploiting the category structure of wikipedia for entity ranking. Artificial Intelligence, 0:111 ­ 129, 2013.
[15] O. Kurland and C. Domshlak. A rank-aggregation approach to searching for optimal query-specific clusters. In Proceedings of SIGIR, pages 547­554, 2008.
[16] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. In Proceedings of SIGIR, pages 194­201, 2004.
[17] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In Proceedings of ECIR, pages 454­462, 2008.
[18] H. Raviv, D. Carmel, and O. Kurland. A ranking framework for entity oriented search using markov random fields. In Proceedings of the 1st Joint International Workshop on Entity-Oriented and Semantic Search, 2012.
[19] M. D. Smucker and J. Allan. A new measure of the cluster hypothesis. In Proceedings of ICTIR, pages 281­288, 2009.
[20] J. A. Thom, J. Pehcevski, and A.-M. Vercoustre. Use of wikipedia categories in entity ranking. CoRR, abs/0711.2917, 2007.
[21] A. Tombros, R. Villa, and C. Van Rijsbergen. The effectiveness of query-specific hierarchic clustering in information retrieval. Information Processing & management, 38(4):559­582, 2002.
[22] C. J. van Rijsbergen. Information Retrieval. Butterworths, second edition, 1979.
[23] E. M. Voorhees. The cluster hypothesis revisited. In Proceedings of SIGIR, pages 188­196, 1985.

Initial
Oracle MeanScore RegMeanScore
Oracle MeanScore RegMeanScore
Oracle MeanScore RegMeanScore
Oracle MeanScore RegMeanScore
Initial
Oracle MeanScore RegMeanScore
Oracle MeanScore RegMeanScore
Oracle MeanScore RegMeanScore
Oracle MeanScore RegMeanScore
Initial
Oracle MeanScore RegMeanScore
Oracle MeanScore RegMeanScore
Oracle MeanScore RegMeanScore
Oracle MeanScore RegMeanScore

2007

LDoc

2008

MAP p@5 infAP p@5

20.2 26.1 12.6 19.4

Tree

31.8i 51.3i 22.3i 49.7i

21.6 26.1 13.3 17.1

21.6 26.0 13.4 18.9

SharedCat

36.2i 60.0i 26.8i 66.3i

22.5 23.9 12.6 18.9

23.1i 27.8i 13.4i 20.6i

CE

32.3i 53.5i 23.3i 53.7i

22.6 27.4 13.5 20.6

22.0 26.5 13.5 20.6

33.7i

ESA 57.0i 26.0i

60.6i

20.9 22.6 12.8 14.9

21.9 23.9 12.9 14.9

2009 infAP p@5 19.1 35.3

25.5i 20.2i 20.2i

68.0i 36.7 36.7

30.5i 19.7 19.9

83.3i 37.1
38.5

28.0i 19.1 19.1

74.2i 36.7 36.7

28.8i 19.2 19.2

80.0i 35.6 35.3

LDoc;Type

2007

2008

MAP p@5 infAP p@5

30.8 37.4 28.2 44.0

Tree

37.7i 58.7i 32.5i 50.9i

31.6 40.0 27.7 37.7

31.6 40.0 28.1 40.6

SharedCat

43.8i 65.7i 38.3i 65.1i

30.8 36.1 28.7 42.3

31.1 37.0 28.9 42.3

CE

39.0i 60.0i 34.3i 58.3i

31.3 38.3 28.5 40.6

31.0 37.4 28.7 40.0

ESA

42.1i 64.3i 37.7i 66.9i

28.6 34.3 28.4 42.3

29.1 34.8 28.9 44.0

2009 infAP p@5 23.8 43.6

29.5i 23.6 23.4

70.2i 39.6 39.3

34.1i 23.2 23.6

87.6i 44.0 45.1

31.3i 23.7 23.7

75.6i 42.2 42.2

32.9i 22.8 22.7

83.6i 42.5 41.5

LDoc;Type;Name

2007

2008

MAP p@5 infAP p@5

33.3 40.4 35.4 46.9

Tree

39.6i 57.4i 42.3i 62.3i

34.1 43.5 35.7 42.3

34.0 42.6 35.7 43.4

SharedCat

47.4i 70.4i 47.8i 74.9i

32.9 38.7 33.5 42.9

33.1 39.1 34.8 44.0

CE

40.7i 59.1i 43.1i 63.4i

33.6 38.7 34.5 45.1

33.6 38.7 34.5 45.1

ESA

44.7i 66.5i 45.7i 70.3i

33.9 41.3 33.7 43.4

34.0 42.2 34.2 44.6

2009 infAP p@5 24.4 44.0

30.3i 24.7 24.6

72.0i 42.9 42.2

34.3i 24.6 24.9

87.i 3 41.5 42.9

31.6i 24.6 24.8

76.4i 43.3 43.6

32.9i 23.0 23.3

81.5i 35.3 35.6

Table 3: Retrieval performance. The best result in a column (excluding that of Oracle) per an initial list is boldfaced. 'i' marks statistically significant differences with Initial.

844

Estimating Query Representativeness for Query-Performance Prediction

Mor Sondak

Anna Shtok

Oren Kurland

mor@tx.technion.ac.il annabel@tx.technion.ac.il kurland@ie.technion.ac.il

Faculty of Industrial Engineering and Management, Technion Haifa 32000, Israel

ABSTRACT
The query-performance prediction (QPP) task is estimating retrieval effectiveness with no relevance judgments. We present a novel probabilistic framework for QPP that gives rise to an important aspect that was not addressed in previous work; namely, the extent to which the query effectively represents the information need for retrieval. Accordingly, we devise a few query-representativeness measures that utilize relevance language models. Experiments show that integrating the most effective measures with state-of-the-art predictors in our framework often yields prediction quality that significantly transcends that of using the predictors alone.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation
Keywords: query-performance prediction
1. INTRODUCTION
The task of estimating retrieval effectiveness in the absence of relevance judgments -- a.k.a. query-performance prediction (QPP) -- has attracted much research attention [2]. Interestingly, an important aspect of search effectiveness has been overlooked, or not explicitly modeled, in previously proposed prediction approaches; namely, the presumed extent to which the query effectively represents the underlying information need for retrieval.
Indeed, an information need can be represented by various queries which in turn might represent various information needs. Some of these queries might be more effective for retrieval over a given corpus than others for the information need at hand. Furthermore, relevance is determined with respect to the information need rather than with respect to the query. These basic observations underlie the development of the novel query-performance prediction framework that we present. A key component of the framework is the use of measures for the query representativeness of the information need. We propose several such measures that are based on using relevance language models [8].
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Empirical evaluation shows that integrating the most effective representativeness measures with state-of-the-art predictors in our framework yields prediction quality that often significantly transcends that of using these predictors alone.

2. RELATED WORK
Our query-performance prediction framework essentially generalizes a recently proposed framework [7], the basis of which was the estimation of the relevance of a result list to a query. Our framework relies on the basic definition of relevance with respect to the information need, and therefore accounts for the connection between the query and the information need. This connection was not (explicitly) addressed in previous work including [7]. For example, preretrieval predictors, which use only the query and corpusbased statistics, are mostly based on estimating the discriminative power of the query with respect to the corpus, but do not account for the query-information need connection.
Post-retrieval predictors analyze also the result list of topretrieved documents [2]. Our framework provides formal grounds to integrating pre-retrieval, post-retrieval, and queryrepresentativeness, which turn out to be three complementary aspects of the prediction task. Furthermore, we demonstrate the merits of integrating post-retrieval predictors with query representativeness measures in the framework.
The query representativeness measures that we devise utilize relevance language models [8]. Relevance models were used for other purposes in various predictors [3, 14, 5, 10]. We demonstrate the merits of integrating in our framework one such state-of-the-art predictor [14].

3. PREDICTION FRAMEWORK
Let q, d and D denote a query, a document, and a corpus of documents, respectively. The task we pursue is estimating the effectiveness of a retrieval performed over D in response to q when no relevance judgments are available [2] -- i.e., query performance prediction (QPP).
Let Iq be the information need that q represents. Since relevance is determined with respect to Iq rather than with respect to q, the QPP task amounts, in probabilistic terms, to answering the following question:
"What is the probability that the result list Dres, of the most highly ranked documents with respect to q, is relevant to Iq?"

Formally, the task is estimating

p(r|Iq, Dres)

=

p(Dres|Iq, r)p(r|Iq) , p(Dres |Iq )

(1)

853

where r is the relevance event and p(r|Iq, Dres) is the probability that the result list Dres satisfies Iq.
Estimating p(Dres|Iq, r) is the (implicit) basis of many post-retrieval prediction methods, if q serves for Iq, as recently observed [7]. The denominator, p(Dres|Iq), is the probability that the result list Dres is retrieved using some representation of Iq regardless of relevance. If q is used for Iq, then the probability of retrieving Dres depends on the properties of the retrieval method employed. Accordingly, the denominator in Equation 1 can serve as a normalizer across different retrieval methods [7]. However, standard QPP evaluation [2] is based on estimating the retrieval effectiveness of a fixed retrieval method across different queries. Thus, the denominator in Equation 1 need not be computed for such evaluation, if q serves for Iq [7].
The (novel) task we focus on is estimating the probability p(r|Iq) from Equation 1 that a relevance event happens for Iq. Obviously, the ability to satisfy Iq depends on the corpus D; e.g., if there are no documents in D that pertain to Iq then the estimate should be zero. Furthermore, the satisfaction of Iq also depends on the query q used to represent it. Thus, the estimate for p(r|Iq) can be approximated by:

p^(r|Iq )



p^(r|Iq, q, D)

=

p^(q|Iq, D, r)p^(r|Iq, D) , p^(q|Iq, D)

(2)

where p^(·) is an estimate for p(·). The estimate p^(q|Iq, D) for the probability that q is chosen
to represent Iq for retrieval over D can be used to account, for example, for personalization aspects. We leave this task for future work, and assume here a fixed user model, and accordingly, a fixed (across queries) p^(q|Iq, D).
If we use q for Iq in the estimate p^(r|Iq, D), we get the probabilistic basis for pre-retrieval prediction methods [6, 4]. These predictors implicitly estimate the probability for a relevance event using information induced from the query and the corpus, but not from the result list (Dres).
The task left for completing the instantiation of Equation 2, and as a result that of Equation 1, is devising p^(q|Iq, D, r) -- the estimate for the probability that q is the most likely query to effectively represent Iq for retrieval over D.

3.1 Estimating query representativeness

The only signal about the information need Iq is the (short)

query q. To induce a "richer" representation for Iq, we use

the generative theory for relevance [8]. Specifically, we con-

struct a (unigram) relevance language model R from doc-

uments in the corpus D. (Details are provided in Section

4.1.) Then, estimating q's representativeness amounts to es-

timating the probability p(q|R, D, r) of generating q by R.

Henceforth, we refer to such estimates as measures of q's

"representativeness", denoted X(q; R).

We assume, as in the original relevance model's formula-

tion [8], that q's terms ({qi}) are generated independently

by ity

R: p^(q|R, D, r) d=ef assigned to qi by R.

qTi op(pqri|eRve);ntp(tqhie|Rq)ueisryt-hleenpgrtohbbaibaisl-,

we use the geometric mean of the generation probabilities

which results in the GEO measure:

GEO(q; R) d=ef |q|

p(qi|R);

qi q

|q| is the number of terms in q.

We also consider the arithmetic mean of the generation probabilities, ARITH, as a representativeness measure:

ARIT H(q; R) d=ef

1 |q|

p^(qi|R).

qi q

For comparison purposes, we study the min and max aggregators of the generation probabilities:

M IN (q; R) d=ef min p(qi|R);
qi q

M AX(q; R) d=ef max p(qi|R).
qi q
Another measure that we consider is the weighted entropy of R, where q's terms are assigned with a unit weight and all other terms in the vocabulary are assigned a zero weight:
EN T (q; R) d=ef - p^(qi|R) log p^(qi|R).
qi q
The underlying assumption is that high entropy, which implies to a relatively uniform importance assigned to q's terms by R, is indicative of effective representation by q. Indeed, too little emphasis on some query aspects was identified as a major cause for retrieval failures [1].

4. EVALUATION
We next present an evaluation of our query-performance prediction (QPP) framework. We begin by describing the experimental setup in Section 4.1. In Section 4.2.1 we focus on using the query-representativeness measures. To that end, we use an oracle-based experiment where the relevance model is constructed only from relevant documents. In Section 4.2.2 we study the integration of the representativeness measures with post-retrieval predictors in our framework.

Collection
TREC12 TREC5 ROBUST
WT10G

Data
Disks 1,2 Disks 2,4 Disks 4,5-CR
WT10g

# of Docs
741,854 524,929 528,155
1,692,096

Topics
51-200 251-300 301-450, 601-700 451-550

Avg. query length
3.52 3.08 2.64
2.66

Table 1: TREC datasets used for experiments.

4.1 Experimental setup
Table 1 presents the TREC datasets used for experiments. TREC12, TREC5 and ROBUST are composed (mostly) of newswire documents, while WT10G is a noisy Web collection. Titles of TREC topics serve for queries. Documents and queries were stemmed with the Krovetz stemmer and stopwords (on the INQUERY list) were removed. The Indri toolkit (www.lemurproject.org) was used for experiments.
Following common practice [2], prediction quality is measured by the Pearson correlation between the true average precision (AP@1000) for the queries, as determined using the relevance judgments in the qrels files, and the values assigned to these queries by a predictor.
The query likelihood method [11] serves for the retrieval method, the effectiveness of which we predict. Document d's retrieval score is the log query likelihood: log qiq p(qi|d); p(qi|d) is the probability assigned to qi by a Dirichlet

854

smoothed unigram language model induced from d with the

smoothing parameter set to 1000 [13].

We use relevance model #1 (RM1) [8] in the query repre-

sentativeness measures: p(w|R) d=ef dS p(w|d)p(d|q); S is

a set of documents; p(w|d) is the maximum likelihood esti-

mate

of

term

w

with

respect

to

d;

p(d|q)

is

(i)

1 |S|

when

S

is

a set of relevant documents as is the case in Section 4.2.1;

and, (ii) d's normalized query likelihood:

p(q|d) dS p(q|d)

,

when

S is the set of all documents in the corpus that contain at

least one query term as is the case in Section 4.2.2. No term

clipping was employed for RM1.

4.2 Experimental results

4.2.1 The query-representativeness measures
The query-representativeness measures play an important role in our QPP framework, and are novel to this study. Thus, we first perform a controlled experiment to explore the potential extent to which these measures can attest to query performance. To that end, we let the measures use a relevance model of a (very) high quality. Specifically, RM1 is constructed from all relevant documents in the qrels files as described in Section 4.1. Table 2 presents the prediction quality of using the representativeness measures by themselves as query-performance predictors. As can be seen, the prediction quality numbers are in many cases quite high. All these numbers -- which are Pearson correlations -- are different than zero to a statistically significant degree according to the two-tailed t-test with a 95% confidence level.
We can also see in Table 2 that GEO is the most effective measure except for TREC5. ARITH and MIN are also quite effective, although often less than GEO. ENT is highly effective for TREC5 and WT10G but much less effective for TREC12 and ROBUST. The MAX measure is evidently less effective than the others, except for TREC5. All in all, we see that different statistics of the generation probabilities assigned by the relevance model to the query terms can serve as effective query representativeness measures for query-performance prediction.

GEO ARITH MIN MAX ENT

TREC12
0 588 .
0.457g 0.523g 0.216gn,a 0.251gn,a

TREC5 0.295 0.398 0.334 0.351 0.526x

ROBUST
0 376 . 0.274 0.328
0.153gn,a 0.222gx

WT10G
0 414 . 0.356 0.373
0.24g,a 0.375x

Table 2: Using the representativeness measures by themselves as query-performance predictors with RM1 constructed from relevant documents. Boldface: the best result in a column. 'g', 'a', 'n', 'x' and 'e' mark statistically significant differences in correlation [12] with GEO, ARITH, MIN, MAX, and ENT, respectively.

4.2.2 Integrating query-representativeness measures with post-retrieval predictors
Query-representativeness measures are one component of our QPP framework. Other important components are postretrieval and pre-retrieval prediction as described in Section 3. Since (i) the query representativeness measures constitute a novel contribution of this paper, (ii) the merits of the integration of post-retrieval and pre-retrieval prediction were

already demonstrated in previous work [7], and, (iii) postretrieval predictors often yield prediction quality that is substantially better than that of pre-retrieval predictors [2], we focus on the integration of the representativeness measures with the post-retrieval predictors in our framework. The integration is performed using Equations 1 and 2. In contrast to the case in Section 4.2.1, we use the standard practical QPP setting; that is, no relevance judgments are available. The relevance model used by the query-representativeness measures is constructed as described in Section 4.1 from all the documents in the corpus that contain at least one query term. Using only top-retrieved documents for constructing the relevance model resulted in inferior prediction quality.
Three state-of-the-art post-retrieval predictors, NQC [9], WIG [14] and QF [14], are used. As these predictors incorporate free parameters, we apply a train-test approach to set the values of the parameters. Since Pearson correlation is the evaluation metric for prediction quality, there should be as many queries as possible in both the train and test sets. Thus, each query set is randomly spit into two folds (train and test) of equal size. We use 40 such splits and report the average prediction quality over the test folds. For each split, we set the free-parameter values of each predictor by maximizing prediction quality over the train fold.
NQC and WIG analyze the retrieval scores of top-retrieved documents, the number of which is set to values in {5, 10, 50, 100, 500, 1000}. QF incorporates three parameters. The number of top-retrieved documents used to construct the relevance model (RM1) utilized by QF is selected from {5, 10, 25 , 50, 75, 100, 200, 500, 700, 1000} and the number of terms used by this RM1 is set to 100 following previous recommendations [10]. The cuttoff used by the overlap-based similarity measure in QF is set to values in {5, 10, 50, 100, 500, 1000}.
In Table 3 we present the average (over the 40 test folds) prediction quality of using the query-representativeness measures alone; using the post-retrieval predictors alone; and, integrating the representativeness measures with the postretrieval predictors in our framework. Although the queryrepresentativeness measures do not incorporate free parameters, we report their prediction quality when used alone using the same test splits. When the measures are integrated with the post-retrieval predictors, the free-parameters of the integration are those of the post-retrieval predictors. In this case, the parameters are tuned by optimizing the prediction quality of the integration over the train folds, as is the case when using the post-retrieval predictors alone. Differences of prediction quality (i.e., Pearson correlations) are tested for statistical significance using the two tailed paired t-test computed over the 40 splits with a 95% confidence level.1
We first see in Table 3 -- specifically, by referring to the underlined numbers -- that the best prediction quality for the majority of the corpora is attained by integrating a representativeness measure with a post-retrieval predictor.
Further exploration of Table 3 reveals the following. The GEO and ARITH measures are effective -- specifically, in comparison to the other representativeness measures which is reminiscent of the case in Table 2 -- both as stand-alone
1Note that the numbers in Table 2 are not comparable to those in Table 3. This is because the latter presents averages over the train-test splits while the former is based on using the all queries for the test set. Furthermore, as noted above, the relevance models used for the representativeness measures are constructed using different sets of documents.

855

GEO ARITH MIN MAX ENT
NQC
NQCGEO NQCARITH NQCMIN NQCMAX NQCENT
WIG WIGGEO WIGARITH WIGMIN WIGMAX WIGENT
QF QFGEO QFARITH QFMIN QFMAX QFENT

TREC12 0.642 0.635 0.583 0.465 0.277
0.666 0.705pq 0.713pq 0.663q 0.672q 0.598pq
0.665 0.688pq 0.689pq 0.645pq 0.604pq 0.462pq
0.673 0.723pq 0.711pq 0.692q 0.608pq 0.498pq

TREC5 0.380 0.435 0.272 0.396 0.381
0.289
0.303q 0.323q 0.272 0.303q 0.299q
0.250 0.371p 0.373pq 0.319pq 0.338pq 0.334pq
0.313 0.378p 0.429p 0.314q 0.438pq 0.393p

ROBUST 0.407 0.419 0.352 0.366 0.309
0.506 0.520pq 0.534pq 0.456pq 0.508q 0.491pq
0 514 .
0.467pq 0.480pq 0.416pq 0.447pq 0.409pq
0.500 0.518pq 0.528pq 0.471pq 0.504q 0.485pq

WT10G 0.317 0.287 0.305 0.210 0.256
0 422 .
0.411q 0.375pq 0.405q 0.309pq 0.421q
0 393 .
0.316p 0.285p 0.313p 0.240pq 0.333pq
0.267 0.372pq 0.353pq 0.361pq 0.272q 0.307pq

Table 3: Average prediction quality over the test folds of the query-representativeness measures, post-retrieval predictors, and their integration (marked with ). Boldface: the best result per corpus and a post-retrieval block; underline: the best result in a column. 'q' and 'p' mark statistically significant differences with using the query-representativeness measure alone and the post-retrieval predictor alone, respectively.

predictors and when integrated with the post-retrieval predictors. Indeed, integrating each of GEO and ARITH with a post-retrieval predictor yields prediction quality that transcends that of using the post-retrieval predictor alone in 9 out of the 12 relevant comparisons (three post-retrieval predictors and four corpora); many of these improvements are substantial and statistically significant.
These findings, as those presented above, attest to the merits of our QPP framework that integrates two different, and evidently complementary, aspects of prediction; namely, post-retrieval analysis of the result list and queryrepresentativeness estimation.2
In comparing the prediction quality numbers in Table 3 for the three post-retrieval predictors we make the following observation. For QF and WIG the integration with the queryrepresentativeness measures yields the highest and lowest number, respectively, of cases of improvement over using the post-retrieval predictor alone.
2It is not a surprise, therefore, that the post-retrieval predictors when used alone outperform in most cases the representativeness measures when used alone. This is because the post-retrieval predictors analyze the result list, while the representativeness measures do not. For TREC5, however, the reverse holds. Presumably, this is because there are only 50 queries for TREC5, while for all other corpora there are at least 100 queries. A relatively small query set makes it difficult to learn the free-parameter values of the post-retrieval predictors, while representativeness measures do not incorporate free parameters.

5. CONCLUSIONS AND FUTURE WORK
We presented a novel probabilistic framework for the queryperformance prediction task. The framework gives rise to an important aspect that was not addressed in previous work: the extent to which the query effectively represents the underlying information need for retrieval. We devised queryrepresentativeness measures using relevance language models. Empirical evaluation showed that integrating the most effective measures with state-of-the-art post-retrieval predictors in our framework often yields prediction quality that significantly transcends that of using the predictors alone.
Devising additional query-representativeness measures, and integrating pre-retrieval predictors with post-retrieval predictors and query-representativeness measures in our framework, are future venues to explore.
6. ACKNOWLEDGMENTS
We thank the reviewers for their comments. This work has been supported in part by the Israel Science Foundation under grant no. 433/12 and by a Google faculty research award. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsors.
7. REFERENCES
[1] C. Buckley. Why current IR engines fail. In Proceedings of SIGIR, pages 584­585, 2004. Poster.
[2] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan & Claypool Publishers, 2010.
[3] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proceedings of SIGIR, pages 299­306, 2002.
[4] C. Hauff, D. Hiemstra, and F. de Jong. A survey of pre-retrieval query performance predictors. In Proceedings of CIKM, pages 1419­1420, 2008.
[5] C. Hauff, V. Murdock, and R. Baeza-Yates. Improved query difficulty prediction for the web. In Proceedings of CIKM, pages 439­448, 2008.
[6] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In Proceedings of SPIRE, pages 43­54, 2004.
[7] O. Kurland, A. Shtok, S. Hummel, F. Raiber, D. Carmel, and O. Rom. Back to the roots: A probabilistic framework for query-performance prediction. In Proceedings of CIKM, pages 823­832, 2012.
[8] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proceedings of SIGIR, pages 120­127, 2001.
[9] A. Shtok, O. Kurland, and D. Carmel. Predicting query performance by query-drift estimation. In Proceedings of ICTIR, pages 305­312, 2009.
[10] A. Shtok, O. Kurland, and D. Carmel. Using statistical decision theory and relevance models for query-performance prediction. In Proccedings of SIGIR, pages 259­266, 2010.
[11] F. Song and W. B. Croft. A general language model for information retrieval (poster abstract). In Proceedings of SIGIR, pages 279­280, 1999.
[12] J. H. Steiger. Tests for comparing elements of a correlation matrix. Psychological Bulletin, 87(2):245­251, 1980.
[13] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR, pages 334­342, 2001.
[14] Y. Zhou and W. B. Croft. Query performance prediction in web search environments. In Proceedings of SIGIR, pages 543­550, 2007.

856

A Document Rating System for Preference Judgements
Maryam Bashir, Jesse Anderton, Jie Wu, Peter B. Golbus, Virgil Pavlu, Javed A. Aslam
College of Computer and Information Science, Northeastern University Boston, Massachusetts, USA
{maryam,jesse,evawujie,pgolbus,vip,jaa@ccs.neu.edu}

ABSTRACT
High quality relevance judgments are essential for the evaluation of information retrieval systems. Traditional methods of collecting relevance judgments are based on collecting binary or graded nominal judgments, but such judgments are limited by factors such as inter-assessor disagreement and the arbitrariness of grades. Previous research has shown that it is easier for assessors to make pairwise preference judgments. However, unless the preferences collected are largely transitive, it is not clear how to combine them in order to obtain document relevance scores. Another difficulty is that the number of pairs that need to be assessed is quadratic in the number of documents. In this work, we consider the problem of inferring document relevance scores from pairwise preference judgments by analogy to tournaments using the Elo rating system. We show how to combine a linear number of pairwise preference judgments from multiple assessors to compute relevance scores for every document.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval ]: Information Search and Retrieval
General Terms
Theory
Keywords
Evaluation, Preference Judgment
1. INTRODUCTION
Traditional methods of collecting relevance judgments make binary assumption about relevance i.e. a document is assumed to be either relevant or non-relevant to the information need of a user. This assumption turns relevance judgment into a classification problem. In the modern world,
We gratefully acknowledge support provided by NSF IIS1256172. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

search engines can easily retrieve thousands of documents at least somewhat relevant to the user's information need. Therefore it becomes necessary to assign a ranking to these documents based on their degree of relevance. This somewhat more continuous notion of relevance cannot be expressed through binary relevance judgments; researchers have developed two ways to express non-binary relevance judgments: either consider relevance as a relative notion such that one document is more or less relevant than another document, or consider relevance as a quantitative notion and create multiple grades of relevance. The first notion of relevance can be expressed as pairwise preference judgments; the second notion can be expressed as nominal graded relevance judgments, which appear far more prevalently in the literature.
Graded relevance has two significant shortcomings. First, the total number of grades must be defined in advance, and it is not clear how this choice effects the relative measurement of system performance. Second, graded judgments require assessors to choose between arbitrarily defined grades, a choice on which different assessors can easily disagree. The alternative, pairwise preference judgments, allows the assessor to make a binary decision, freeing him or her from the difficulty of deciding between multiple relevance grades. Another advantage of using preferences is that many popular learning-to-rank algorithms, e.g. RankBoost and RankNet, are naturally trained on preferences; thus a better training set can be obtained from direct preference judgments, as opposed to pairwise preferences inferred from nominal judgments.
Pairwise preference judgments have not been explored extensively in the literature. There have been several attempts to use preference judgments by inferring them from absolute judgments [4] and from click data [8]. Nie et al. [9] used preferences for relevance assessments and showed that labelling effort can be reduced by focussing on top ranked documents. Chen et al. [2] also used preferences but focused more on estimating worker quality. To the best of our knowledge, the only work where assessors were asked for direct pairwise preferences as well as absolute relevance judgments for the comparison of the two assessment approaches is by Carterette et al. [1]. The authors showed that rate of interassessor agreement is higher on preference judgments, and that assessors take longer to make absolute judgments than preference judgments.
If a simple routine is to be used to infer document relevance from pairwise preferences, it is essential that the preferences be transitive, so that we may sort documents by

909

preference and decide which and how many pairs to judge. Carterette et al., by collecting all O(n2) preference judgments found that the preferences they collected are transitive 99% of the time. However, the study used experts assessors. The critical property of transitivity might not hold when judgments are collected through the much noisier process of crowdsourcing.
In order to obtain document grades (or scores) from a smaller number of preference judgments, we draw an analogy to the tournament problem. In a typical tournament, pairs of players or teams compete in matches of one or more games. The desired outcome is a final ranking (or scoring) of each competitor. A common solution is to use the Elo rating system [3], in which players are assigned ratings which are updated iteratively each time the player competes in a match. Using the Elo rating system to combine preference judgments into document grades has the following benefits:
1. The judgments do not need to be transitive. We cannot simply sort the documents by preference since humans assessors can be intransitive in their assessments; especially when we are combining preference judgments from noisy assessments (e.g. through crowdsourcing). The Elo rating system produces a ranking of documents even if the preferences are not transitive.
2. We do not need a quadratic number of pairwise assessments for inferring the relevance of documents. The Elo rating system can be applied to any number of assessments. Indeed, it can infer highly reliable relevance scores using only a linear number of pairwise assessments.
3. For any pair of documents, the document scores produced using the Elo rating system can be used to compute the likelihood of one document is more relevant than the other. In this way we can predict all O(n2) preferences while only collecting O(n) judgments.
2. THE ELO RATING SYSTEM
The Elo rating system is a method for calculating the relative rating of players in two player games [3]. The system assigns each player a rating score, with a higher number indicating a better player. Each player's rating is updated after he or she has played a certain number of matches, increasing or decreasing in value depending on whether the player won or lost each match, and on the ratings of both players competing in each match--beating a highly rated player increases one's rating more than beating a player with a low rating, while losing to a player with a low rating decreases one's score more than losing to a player with a high rating. These scores are used in two ways: 1) players are ranked by their scores, and 2) the scores are used to compute the likelihood that one player will beat another. If the matches are selected intelligently, the stable ratings can be achieved after only O(n) matches played.
Given the two player's ratings before the match, denoted RA and RB, an expected match outcome is calculated for each player: EA and EB. The actual output of the match from the perspective of each player (since a win for player A is assumed to be a loss for player B) is denoted as SA and SB. The ratings are updated after each match, based on how the expected aligns with the actual outcome.

The Elo rating system can be applied directly to our problem by treating the documents as players, their scores as the ratings to be learned, and document-pairwise preference assessments as matches. All documents begin the "tournament" rated equally. After each document "plays" a match, we update its rating according to equation 2. Each match corresponds to a fixed number of assessors expressing a preference between the pair of documents. The actual outcome of the match for each document, S, is the number of assessors that preferred that document plus half the number of assessors who considered the documents to be "tied." After all the matches are played, we can rank the documents by their final score. This list can be thresholded to produce absolute relevance judgments. We can also use the scores directly to compute transitive preference judgments.

2.1 Math Details of the Elo Rating System

If, before a match, document A has a rating of RA and document B has a rating of RB, then the expected outcome of the match according to the Elo rating system is:

1

EA

=

1

+

10

RB

-RA F

;

1

EB

=

1

+

10

RA -RB F

(1)

where F is a rating disparity parameter used to control how quickly ratings can change.
If EA is greater than EB, then we expect document A to win the match. Once the match is played and we can observe SA and SB, the documents' Elo rating is updated as follows:

RA = RA + K(SA - EA); RB = RB + K(SB - EB) (2)
where K is a game importance parameter that can be varied so as to give some matches more weight than others.

2.1.1 Elo Rating with Variance

The Elo rating system assumes that the uncertainty about a player's skill rating does not change over time. Therefore, all skill rating updates are computed with the same variance, and any change in the uncertainty about the player's skills over time is not modeled. Glickman proposed to solve this problem by incorporating the variance over time in the player's skill rating [5]. Other researchers have used Glickman's system for the purpose of ranking documents based on clickthrough data [10]. Glickman presented the idea of modeling the belief about a player's skills as a Gaussian distribution whose mean corresponds to the player's rating. As a player plays more matches, the uncertainty about his her her skills is decreased, and this is reflected by a decrease in the variance of the player's associated Gaussian distribution. Rather than using equation 2, the mean rating (RA) and variance (2) of each document is updated using equation 3 and equation 4 as follows:

RA = RA + Kg(B2 )(SA - EA)

(3)

g(2) =

1

(4)

1

+

3q2 2 2

where,

1

EA

=

1

+

10-g(B2 )

RB -RA F

(5)

q

K=

1 A2

+

1 2

;

2 =

1 2

1

+

1 2

;

q = log 10 F

(6)

910

2 = q2

1

m j=1

nj

g(j2

)2

EA

(1

-

EA)

(7)

Throughout this work, we set F = 200. Each document is initialized with a mean of 100 and a variance of 10.

2.2 Selection of Preference Pairs
For our preliminary experiments, we select O(n) matches stochastically. Each document in the list will be compared against five other documents. We wish to sample pairs in such a way that we create a bias towards relevant documents. In this way, relevant documents will play more matches than non-relevant documents, giving them more opportunities to improve their ratings and move up the list. First, we calculate an initial relevance score for each document using BM25. This produces an initial ranking of the documents for each topic. We collected complete pairwise preferences between the top six documents. For each document below the top six, we select five documents from the set of documents with higher BM25 scores, uniformly at random. We collected four worker assessments for each preference pair which we selected for judgment. We sort all documents based on their Elo ratings after all O(n) matches have been played.

3. EXPERIMENTS
We will compare our methodology for collecting relevance grades from pairwise preferences to the results of the TREC 2012 Crowdsourcing track1. The goal of the track was to evaluate approaches to crowdsourcing high quality relevance judgments for text documents and images. Track participants were asked to provide new binary relevance grades, as well as probabilities of relevance, for 18,260 documents that had previously been judged with respect to ten topics selected randomly from the TREC 8 ad-hoc collection.
3.1 Crowdsourcing
We crowdsourced our preference judgments using Amazon Mechanical Turk (AMT)2. Each crowd worker was shown the interface presented in Figure 1. Workers were shown the title, description, and narrative fields of a TREC topic, and presented with two documents. Worker's were asked which document "does a better job of answering the questions at the top of the page." They were allowed to select either document, as well as the options "They're Equally Good" and "They're Equally Bad." Internally, these latter two options were treated equivalently as ties. Each task, known on AMT as a HIT, consisted of 20 preference pairs for the same topic, and had a time limit of 30 minutes. Workers were paid $0.15 for each approved HIT. The order in which the document pairs were displayed, as well as which document appeared on which side of the interface, was randomized.
3.1.1 Quality Control
The workers we employed have no particular training in assessing document relevance. Therefore, we need a means of verifying the quality of their work. We used trap questions, a document pair for which the "correct" preference is already known, in our study to ensure that workers are giving us reasonable results, and not just clicking randomly. We asked five graduate students studying information retrieval to create our trap questions by pairing documents which
1http://sites.google.com/site/treccrowd 2http://www.mturk.com

ELO progress after the first iteration though all pairs
9

percent of inverted pairs within top 200 documents

8

427

445

417

416

7

446

447

432

6

420

411

438

5

4

3

2

1

0

2

3

4

5

6

7

8

9

10

ELO iteration after the first one

Figure 2: Relationship of Number of Elo rating iterations to percent of pairs inverted, separately for each query.

they deemed highly relevant with documents they deemed highly non-relevant. We then inserted five of these trap questions, selected at random, into each HIT. As a result, each assignment consisted of five trap questions and fifteen "real" questions. Worker's submission were not accepted unless at least two of the five trap questions were answered correctly. Although, answering two of the five trap questions is not strict criteria but it makes sure that the worker's performance is not worse than random answers.
As another means of ensuring the quality of the collected judgments, we also employed Expectation Maximization (EM). In this context EM, is a means of estimating the "true" pairwise preferences from crowd workers as latent variables in a model of worker quality. For every pair of documents about which we collected judgments from workers, EM provides a probability that one document beats the other. EM has been shown to work well for aggregating labels from multiple crowd workers on AMT [7], and in particular with regarding to collecting relevance judgments [6].

3.2 Iterations of Elo Rating
In Elo rating system, the score of each document depends on the score of its opponent document in a match. The order in which matches are played has an impact on scores of documents. For example, if a document wins a match against a relevant document, and the relevant document has not played any match yet, then the score of the document would not increase significantly. If the relevant document has already played few matches and has raised its score, then wining a match against it would increase the score of a document to a large extent. Because of this, if we run only one iteration of Elo rating algorithm (through all pairs) then some document scores may not be reliable; we instead run several iterations of Elo rating algorithm so that scores of documents converge. Figure 2 shows the relationship of number of Elo rating iterations to percentage of pairs inverted, after the initial run through all pairs. Note that as we run more iterations, the percentage of pairs whose order is changed decreases.

3.3 Baseline
In order to measure the quality of our Elo-based system, we also implemented a naive system as a baseline. In our naive system, each document is given a score based on the percentage of its matches that it won and the number of matches it competed in. The score of a document A is calculated as:

score(A) =  winsA + (1 - ) matchesA

(8)

matchesA

matches

911

Figure 1: Preference pair selection interface

Topic ID

# Documents # Relevant in Collection Documents

Median Score of TREC Baseline Participant Runs

AUC Elo Without Vari- Elo ance

Elo+EM

411

2056

416

1235

417

2992

420

1136

427

1528

432

2503

438

1798

445

1404

446

2020

447

1588

27

0.86

42

0.85

75

0.75

33

0.71

50

0.73

28

0.71

173

0.78

62

0.83

162

0.82

16

0.76

0.809 0.919 0.848 0.808 0.864 0.544 0.725 0.750 0.700 0.935

0.811 0.940 0.897 0.834 0.871 0.536 0.731 0.748 0.716 0.995

0.857 0.944 0.887 0.823 0.882 0.637 0.708 0.790 0.720 0.859

0.862 0.939 0.914 0.853 0.907 0.558 0.774 0.843 0.865 1.000

All

18260

668

Not Reported

0.790

0.808

0.811 0.851

Table 1: Evaluation Results using AUC for Preference based Relevance Judgements. Elo+EM is statistically significantly better than Baseline, Elo is not significantly better than baseline.

where winsA is number of matches won by document A, matchesA is total number of matches played by a document A, and matches is total number of matches played. Since we did not have enough data to properly tune ,  is set to 0.5.
3.4 Results
Table 1 shows the Area Under the ROC Curve (AUC), one of the primary measures used in the TREC 2012 Crowdsourcing, of our Elo and Baseline systems, with and without EM, and the median scores of the 33 systems that participated in the Crowdsourcing track. For most topics, our Elo-based system outperforms both the Baseline naive system and the median TREC participant. When we also use EM, our results improve. The results using Elo+EM are significantly3 better than the simple baseline.
4. CONCLUSION AND FUTURE WORK
Preference judgments are easier for assessors to produce and are more useful for training learning-to-rank algorithms. However, their use has been limited due to the polynomial increase in the number of judgments that need to be collected. In this work, we have shown how the Elo rating system can be used to combine a linear number of preferences to obtain either an ordered list of documents or document relevance scores. The results of our experiments are encouraging and demonstrate the potential of our Elo-based system for inferring the relevance of documents from a linear number of pairwise preference judgments.
In future work, we plan to use active learning to intelligently select which pairs of documents to judge in an online manner. The pairwise preference judgments collected
3Statistical significance is determined using a two-tailed TTest and is measured at a significance level of 0.05.

in each phase of active learning will dictate which pairs are selected to be judged in the next phase.
5. REFERENCES
[1] B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais. Here or there. In ECIR, 2008.
[2] X. Chen, P. N. Bennett, K. Collins-Thompson, and E. Horvitz. Pairwise ranking aggregation in a crowdsourced setting. In Proceedings of WSDM. ACM, 2013.
[3] A. Elo and S. Sloan. The Rating of Chess Players, Past and Present. Arco Publishing, 1978.
[4] H. P. Frei and P. Schauble. Determining the effectiveness of retrieval algorithms. Inf. Process. Manage., 27(2-3), 1991.
[5] M. E. Glickman. Parameter estimation in large dynamic paired comparison experiments. In Applied Statistics, pages 48­377, 1999.
[6] M. Hosseini, I. J. Cox, N. Mili´c-Frayling, G. Kazai, and V. Vinay. On aggregating labels from multiple crowd workers to infer relevance of documents. In ECIR. Springer-Verlag, 2012.
[7] P. G. Ipeirotis, F. Provost, and J. Wang. Quality management on Amazon Mechanical Turk. In SIGKDD Workshop on Human Computation. ACM, 2010.
[8] T. Joachims. Optimizing search engines using clickthrough data. In SIGKDD. ACM, 2002.
[9] S. Niu, J. Guo, Y. Lan, and X. Cheng. Top-k learning to rank: labeling, ranking and evaluation. In Proceedings of SIGIR. ACM, 2012.
[10] F. Radlinski and T. Joachims. Active exploration for learning rankings from clickthrough data. In Proceedings of SIGKDD. ACM, 2007.

912

Composition of TF Normalizations: New Insights on Scoring Functions for Ad Hoc IR

François Rousseau*, Michalis Vazirgiannis*
*LIX, École Polytechnique, France Department of Informatics, AUEB, Greece Institut Mines-Télécom, Télécom ParisTech, France
rousseau@lix.polytechnique.fr, mvazirg@aueb.gr

ABSTRACT
Previous papers in ad hoc IR reported that scoring functions should satisfy a set of heuristic retrieval constraints, providing a mathematical justification for the normalizations historically applied to the term frequency (TF). In this paper, we propose a further level of abstraction, claiming that the successive normalizations are carried out through composition. Thus we introduce a principled framework that fully explains BM25 as a variant of TF-IDF with an inverse order of function composition. Our experiments over standard datasets indicate that the respective orders of composition chosen in the original papers for both TF-IDF and BM25 are the most effective ones. Moreover, since the order is different between the two models, they also demonstrated that the order is instrumental in the design of weighting models. In fact, while considering more complex scoring functions such as BM25+, we discovered a novel weighting model in terms of order of composition that consistently outperforms all the rest. Our contribution here is twofold: we provide a unifying mathematical framework for IR and a novel scoring function discovered using this framework.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms
Theory, Algorithms, Experimentation
Keywords
IR theory; scoring functions; TF normalizations; heuristic retrieval constraints; function composition
1. MOTIVATION
Fang et al. introduced in [3] a set of heuristic retrieval constraints that any scoring function used for ad hoc infor-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

mation retrieval (IR) should satisfy. In particular, these constraints involve term frequency, term discrimination, document length and the interactions between them. For instance, they stated that a scoring function should favor document matching more distinct query terms. It is one of the earliest works that formally defined the properties that both the TF and the IDF components of any weighting model should possess. It is a unifying theory in IR that applies to the vector space model (TF-IDF [13]), probabilistic (BM25 [10]), language modeling (Dirichlet prior [15]) and information-based (SPL [2]) approaches and the divergence from randomness framework (PL2 [1]).
The definition of these constraints contributed to the improvement of the overall effectiveness of most modern scoring functions. Constraints on the term frequency result in successive normalizations on the raw TF, each one satisfying one or more properties. In our work, we intended to go one step further and we propose the use of composition to explain how the normalizations are applied successively in the general TF×IDF weighting scheme. In section 2, we describe in details the mathematical framework we designed. In section 3, we present the experiments we conducted over standard datasets and the results obtained that indicate how important the order of composition is, along with a novel and effective weighting model, namely TFlp×IDF. Finally, in section 4, we conclude and mention future work.
2. MATHEMATICAL FRAMEWORK
In ad hoc IR, a scoring function associates a score to a term appearing both in a query and a document. This function consists of three components supposedly independent of one another: one at the query level (QF), one at the document level (TF) and one at the collection level (IDF). These components are aggregated through multiplication to obtain a final score for the term denoted hereinafter by TF×IDF. We omit voluntarily to mention QF in the name since it is a function of the term frequency in the query and it has usually a smaller impact on the score, in particular for Web queries that tend to be short. We make here a difference between the TF×IDF general weighting scheme and TF-IDF, the pivoted normalization weighting defined in [13]. Note that because we rank documents, these term scores will be aggregated through sum to obtain a document score but this is beyond the scope of the current paper.
2.1 A set of TF normalizations
Since the early work of Luhn [4], term frequency (TF) has been claimed to play an important role in information

917

retrieval and is at the center of all the weighting models. Intuitively, the more times a document contains a term of the query, the more relevant this document is for the query. Hence, it is commonly accepted that the scoring function must be an increasing function of the term frequency and the simplest TF component can be defined as follows:

T F (t, d) = tf (t, d)

(1)

where tf (t, d) is the term frequency of the term t in the document d. However, as the use of the raw term frequency proved to be non-optimal in ad hoc IR, the research community started normalizing it considering multiple criteria, mainly concavity and document length normalization. Later, these normalizations were explained as functions satisfying some heuristic retrieval constraints as aforementioned [3].

Concave normalization.
The marginal gain of seeing an additional occurrence of a term inside a document is not constant but rather decreasing. Indeed, the change in the score caused by increasing TF from 1 to 2 should be much larger than the one caused by increasing TF from 100 to 101. Mathematically, this corresponds to applying a concave function on the raw TF. We prefer the term concave like in [2] to sublinear like in [5] since the positive homogeneity property is rarely respected (and actually not welcomed) and the subadditivity one, even though desirable, not sufficient enough to ensure a decreasing marginal gain.
There are mainly two concave functions used in practice: the one in TF-IDF [13] and the one in BM25 [10] that we respectively called log-concavity (TFl) and k-concavity (TFk):

T Fl(t, d) = 1 + ln[1 + ln[tf (t, d)]]

(2)

T Fk(t, d)

=

(k1 + 1) · tf (t, d) k1 + tf (t, d)

(3)

where k1 is a constant set by default to 1.2 corresponding to the asymptotical maximal gain achievable by multiple occurrences compared to a single occurrence.

Document length normalization.
When collections consist of documents of varying lengths (like web pages for the Web), longer documents will ­ as a result of containing more terms ­ have higher TF values without necessary containing more information. For instance, a document twice as long and containing twice as more times a term should not get a score twice as large but rather a very similar score. As a consequence, it is commonly accepted that the scoring function should be an inverse function of the document length to compensate that effect. Early works in vector space model suggested to normalize the score by the norm of the vector, be it the L1 norm (document length), the L2 norm (Euclidian length) or the L norm (maximum TF value in the document) [11]. These norms still mask some subtleties about longer documents ­ since they contain more terms, they tend to score higher anyway. Instead, the research community has been using a more complex normalization function known as pivoted document length normalization and defined as follows:

tf (t, d)

T Fp(t, d)

=

1-

b

+

b

·

|d| avdl

(4)

where b  [0, 1] is the slope parameter, |d| the document

length and avdl the average document length across the collection of documents as defined in [12].

2.2 Composition of TF normalizations
Based on this set of properties and their associated functions, it seems natural to apply them to the raw TF successively by composing them. In the literature, the document length normalization has usually been applied to either the overall term score or the document score like one would normally normalize a vector. However, it was then hard to fully fit BM25 in the TF×IDF weighting scheme. With composition, it is just a matter of ordering the functions. We present here the two compositions behind TF-IDF and BM25, respectively TFpl (= TFp  TFl) and TFkp (= TFk  TFp):

1 + ln[1 + ln[tf (t, d)]]

T Fpl(t, d) =

1

-b+

b

·

|d| avdl

(5)

T Fkp(t, d)

=

(k1

+

1)

·

tf (t,d)

1-b+b·

|d| avdl

k + tf (t,d)

1

1-b+b·

|d| avdl

=

(k1 + 1) · tf (t, d)

k1

·

[1

-

b

+

b

·

|d| avdl

]

+

tf (t, d)

= (k1 + 1) · tf (t, d)

(6)

K + tf (t, d)

where

K

=

k1

·

(1

-

b

+

b

·

|d| avdl

)

as

defined

in

[10].

Note that under this form, it is not obvious that TFkp is

really a composition of two functions with the same proper-

ties as the one in TFpl. We think this is the main reason

why composition has never been considered before. By do-

ing so, we provide not only a way to fully explain BM25 as a

TF×IDF weighting scheme but also a way to easily consider

variants of a weighting model by simply changing the order

of composition. As we will see in section 3, this led us to a

new TF×IDF weighting scheme that outperforms BM25.

2.3 Inverse Document Frequency
While the higher the frequency of a term in a document is, the more salient this term is supposed to be, this is no longer true at the collection level. This is actually quite the inverse since these terms have a presumably lower discrimination power. Hence, the use of a function of the term specificity, namely the Inverse Document Frequency (IDF) as defined in [14] and expressed as follows:

N +1

IDF (t) = log

(7)

df (t)

where N is the number of documents in the collection and df (t) the document frequency of the term t.

2.4 TF-IDF versus BM25

By TF-IDF, we refer to the TF×IDF weighting model defined in [13], often called pivoted normalization weighting. The weighting model corresponds to TFpl×IDF:

1 + ln[1 + ln[tf (t, d)]]

N +1

T F -IDF (t, d) =

1

-

b

+

b

·

|d| avdl

× log

(8)

df (t)

By BM25, we refer to the scoring function defined in [10], often called Okapi weighting. It corresponds to TFkp×IDF when we omit QF (k-concavity of parameter k3 for tf (t, q)). Thus, it has an inverse order of composition between the

918

concavity and the document length normalization compared to TF-IDF. The within-document scoring function of BM25 is written as follows when using the IDF formula defined in subsection 2.3 to avoid negative values, following [3]:

BM 25(t, d) = (k1 + 1) · tf (t, d) × log N + 1 (9)

K + tf (t, d)

df (t)

2.5 Lower-bounding TF normalization

Through composition, we also allow additional constraints

for the TF component to be satisfied easily. Subadditiv-

ity is for instance a desirable property ­ if two documents

have the same total occurrences of all query terms, a higher

score should be given to the document covering more dis-

tinct query terms. Here, it just happens that TFl and TFk

already satisfy it as noted in [3].

Recently, Lv and Zhai introduced in [6] two new con-

straints to the work of Fang et al. to lower-bound the TF

component. In particular, there should be a sufficiently large

gap in the score between the presence and absence of a query

term even for very long documents where TFp tends to 0 and

a fortiori the overall score too. Mathematically, this corre-

sponds to be composing with a third function TF that is

always composed after TFp since it compensates the poten-

tial null limit introduced by TFp and it is defined as follows:

{

tf (t, d) +  if tf (t, d) > 0

T F(t, d) = 0

(10) otherwise

where  is the gap, set to 0.5 if TF is composed immediately after TFp and 1 if concavity is applied in-between. These are the two values defined in the original papers [6, 7] and we just interpreted their context of use in terms of order of composition. We did not change nor tune the values.
The weighting models Piv+ and BM25+ defined in [6] correspond respectively to TFpl×IDF and TFkp×IDF while BM25L defined in [7] to TFkp×IDF. We clearly see that the only difference between BM25+ and BM25L is the order of composition: this is one of the advantages of our framework ­ easily represent and compute multiple variants of a same general weighting model. In the experiments, we considered all the possible orders of composition between TFk or TFl, TFp and TF with the condition that TFp always precedes TF as explained before.
For instance, we will consider a novel model TFlp×IDF defined as follows:

tf (t, d)

T Flp ×IDF (t, d) = 1+ln[1+ln[ 1-b+b·

|d| +]]
avdl

(11)

where b is set to 0.20 and  to 0.5.

3. EXPERIMENTS
Following our mathematical framework that relies on composition, we wondered why the order of composition was different between two widely used scoring functions ­ TF-IDF and BM25. In the original papers [11, 9], there was no mention of the difference in the order and this motivated us to investigate the matter. Our initial thought was that using an inverse order of composition in BM25 could improve it or vice-versa for TF-IDF. As a consequence, we tried exhaustively the combinations among TFk, TFl and TFp and report the results. Thereafter, as mentioned in subsection 2.5, we followed the same procedure considering a third function to compose with: TF. Indeed, we wanted to explore

the extensions considered by the research community [6, 7] in terms of composition. This led us to a novel weighting model that outperforms them (see subsection 3.4).
3.1 Datasets and evaluation
We used two TREC collections to carry out our experiments: Disks 4&5 (minus the Congressional Record) and WT10G. Disks 4&5 contains 528,155 news releases while WT10G consists of 1,692,096 crawled pages from a snapshot of the Web in 1997. For each collection, we used a set of TREC topics (title only to mimic Web queries) and their associated relevance judgments: 301-450 and 601-700 for Disks 4&5 (TREC 2004 Robust Track) and 451-550 for WT10G (TREC9-10 Web Tracks).
We evaluated the scoring functions in terms of Mean Average Precision (MAP) and Precision at 10 (P@10) considering only the top-ranked 1000 documents for each run. Our goal is to compare weighting models that use the same functions but with a different order of composition and select the best ones on both metrics. For example, in Table 1, TFpl×IDF is compared with TFlp×IDF and TFkp×IDF with TFpk×IDF. The statistical significance of improvement was assessed using the Student's paired t-test considering p-values less than 0.01 to reject the null hypothesis.
3.2 Platform and models
We have been using Terrier version 3.5 [8] to index, retrieve and evaluate over the TREC collections. For both datasets, the preprocessing steps involved Terrier's built-in stopword removal and Porter's stemming. We did not tune the slope parameter b of the pivoted document length normalization on each dataset. We set it to the default value suggested in the original papers: 0.20 when used with logconcavity [13] and 0.75 when used with k-concavity [10].
3.3 Results for TF-IDF versus BM25
We report in Table 1 the results we obtained on the aforementioned datasets when considering concavity and pivoted document length normalization. To the best of our knowledge, experiments regarding the same functions (TFk, TFl and TFp) with a different order of composition have never been reported before. They indeed show that the original order chosen for both TF-IDF and BM25 is the most effective one: TFpl×IDF outperforms TFlp×IDF and TFkp×IDF outperforms TFpk×IDF. But since the order is different between the two, this also indicates that the order does matter depending on which function is chosen for each property.
For these two models (TF-IDF and BM25), the use of a different concave function to meet the exact same constraints requires the pivoted document length normalization to be applied before or after the function. The impact is even more significant on the Web dataset (WT10G) that corresponds the most to contemporary collections of documents.
3.4 Results for lower-bounding normalization
In Table 2, we considered in addition the lower-bounding normalization function TF defined in subsection 2.5. The best-performing weighting model on both datasets is a novel one ­ TFlp×IDF ­ and it even outperforms BM25+ and BM25L (significantly using the t-test and p < 0.01). This model has never been considered before in the literature to the best of our knowledge. In fact, the results from Table 1 establish that TFl should apparently be applied before TFp

919

Table 1: TF-IDF vs. BM25: an inverse order of

composition; bold indicates significant performances

Weighting model

TREC 2004 Robust TREC9-10 Web MAP P@10 MAP P@10

IDF

0.1396 0.2040 0.0539 0.0729

TF

0.0480 0.0867 0.0376 0.0833

TFp [b=0.20] TFp [b=0.75] TFl TFk

0.0596 0.0640 0.1591 0.1768

0.1193 0.1289 0.3141 0.3269

0.0531 0.0473 0.1329 0.1522

0.1021 0.1000 0.2063 0.2104

TFpk TFlp TFpl TFkp

0.0767 0.1645 0.1797 0.2045

0.1932 0.3651 0.3647 0.3863

0.0465 0.0622 0.1260 0.1702

0.0604 0.1854 0.1875 0.2208

TFpk ×IDF TFlp×IDF TFpl×IDF [TF-IDF] TFkp×IDF [BM25]

0.1034 0.1939 0.2132 0.2368

0.2293 0.3964 0.4064 0.4161

0.0507 0.0750 0.1430 0.1870

0.0833 0.2125 0.2271 0.2479

like in TF-IDF. With lower-bounding normalization, it no longer holds. The formula for TFlp×IDF was given in equation 11. Without the use of our formal framework and composition, it would have been harder to detect and test these variants that can outperform state-of-the-art scoring functions when the order of composition is chosen carefully.

Table 2: TFlp×IDF vs. BM25+ and BM25L; bold indicates significant performances.

Weighting model

TREC 2004 Robust TREC9-10 Web MAP P@10 MAP P@10

TFpk TFlp TFlp TFpl TFkp TFkp TFpk ×IDF TFlp×IDF TFlp×IDF TFpl×IDF [Piv+] TFkp×IDF [BM25L] TFkp×IDF [BM25+]

0.1056 0.1807 0.2130 0.2002 0.2155 0.2165 0.1466 0.2096 0.2495 0.2368 0.2472 0.2466

0.2349 0.3751 0.4064 0.3876 0.3936 0.3956 0.2723 0.4048 0.4305 0.4157 0.4217 0.4145

0.0556 0.0668 0.1907 0.1436 0.1806 0.1835 0.0715 0.0806 0.2084 0.1643 0.2000 0.2026

0.0771 0.2021 0.2625 0.2021 0.2292 0.2354 0.1000 0.2292 0.2771 0.2438 0.2563 0.2521

4. CONCLUSIONS AND FUTURE WORK
Scoring function design is a cornerstone issue in information retrieval. In this short paper, we intended to provide new insights on scoring functions for ad hoc IR. In particular, we proposed a unifying mathematical framework that explains how weighting models articulate around a set of heuristic retrieval constraints introduced in related work.
Using composition to combine the successive normalizations historically applied to the term frequency, we were able to fully explain BM25 as a TF×IDF weighting scheme with just an inverse order of composition between the concavity and the document length normalization compared to TF-IDF. Besides, the framework also allowed us to discover and report a novel weighting model ­ TFlp×IDF ­ that consistently and significantly outperformed BM25 and its extensions on two standard datasets in MAP and P@10.
Future work might involve the design of novel retrieval constraints and their compositions with existing ones. We

are confident that refining the mathematical properties behind scoring functions will continue to improve the effectiveness of these models in ad hoc IR.
5. ACKNOWLEDGMENTS
We thank the anonymous reviewers for their useful feedbacks. This material is based upon work supported by the French DIGITEO Chair grant LEVETONE.
6. REFERENCES
[1] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Transactions on Information Systems, 20(4):357­389, Oct. 2002.
[2] S. Clinchant and E. Gaussier. Information-based models for ad hoc IR. In Proceedings of SIGIR'10, pages 234­241, 2010.
[3] H. Fang, T. Tao, and C. Zhai. A formal study of information retrieval heuristics. In Proceedings of SIGIR'04, pages 49­56, 2004.
[4] H. P. Luhn. A statistical approach to mechanized encoding and searching of literary information. IBM Journal of Research and Development, 1(4):309­317, Oct. 1957.
[5] Y. Lv and C. Zhai. Adaptive term frequency normalization for BM25. In Proceedings of CIKM'11, pages 1985­1988, 2011.
[6] Y. Lv and C. Zhai. Lower-bounding term frequency normalization. In Proceedings of CIKM'11, pages 7­16, 2011.
[7] Y. Lv and C. Zhai. When documents are very long, BM25 fails! In Proceedings of SIGIR'11, pages 1103­1104, 2011.
[8] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A high performance and scalable information retrieval platform. In Proceedings of SIGIR'06, 2006.
[9] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of SIGIR'94, pages 232­241, 1994.
[10] S. E. Robertson, S. Walker, K. Spärck Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In Proceedings of the TREC-3, pages 109­126, 1994.
[11] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513­523, 1988.
[12] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In Proceedings of SIGIR'96, pages 21­29, 1996.
[13] A. Singhal, J. Choi, D. Hindle, D. Lewis, and F. Pereira. AT&T at TREC-7. In Proceedings of TREC-7, pages 239­252, 1999.
[14] K. Spärck Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):11­20, 1972.
[15] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR'01, pages 334­342, 2001.

920

The Impact of Intent Selection on Diversified Search Evaluation

Tetsuya Sakai
Microsoft Research Asia,
P.R.C.
tetsuyasakai@acm.org

Zhicheng Dou

Charles L. A. Clarke

Microsoft Research Asia,

University of Waterloo,

P.R.C.

Canada

zhichdou@microsoft.com claclark@plg.uwaterloo.ca

ABSTRACT
To construct a diversified search test collection, a set of possible subtopics (or intents) needs to be determined for each topic, in one way or another, and per-intent relevance assessments need to be obtained. In the TREC Web Track Diversity Task, subtopics are manually developed at NIST, based on results of automatic click log analysis; in the NTCIR INTENT Task, intents are determined by manually clustering "subtopics strings" returned by participating systems. In this study, we address the following research question: Does the choice of intents for a test collection affect relative performances of diversified search systems? To this end, we use the TREC 2012 Web Track Diversity Task data and the NTCIR-10 INTENT-2 Task data, which share a set of 50 topics but have different intent sets. Our initial results suggest that the choice of intents may affect relative performances, and that this choice may be far more important than how many intents are selected for each topic.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Experimentation
Keywords
diversity, evaluation, intents, subtopics, test collections
1. INTRODUCTION
Given an ambiguous or underspecified query, diversified search aims to cover different possible search intents with a single search engine result page, by balancing relevance and diversity. TREC1 started a Diversity Task in the Web Track2 in 2009, while NTCIR3 started a related task called INTENT4 in 2011. Unlike traditional retrieval evaluation where pooled documents are assessed in terms
1http://trec.nist.gov/ 2http://plg.uwaterloo.ca/~trecweb/ 3http://research.nii.ac.jp/ntcir/ 4http://research.microsoft.com/INTENT/
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2034-4/13/07 ...$15.00.

quit smoking (TREC topicID=182; NTCIR topicID=0432)

(a) TREC "subtopics"
1. What are the ways you can quit smoking?
2. What are the benefits of quitting
smoking?
3. Can you quit smoking using the cold turkey method?
4. How can hypnosis help someone
quitting smoking?

(b) NTCIR "intents" 1. effects (0.15) 2. ways (0.15) 3. benefit (0.14) 4. reasons (0.14) 5. products (0.14)
6. public resource (0.10) 7. aids (0.10)
8. people (0.08)

(c) NTCIR "subtopic strings"
women weight gain quit smoking;what happens when you quit smoking;...
what to do to quit smoking;ways to quit smoking cigarettes;...
quit smoking health benefits;quit smoking health;...
why quit smoking;reasons to quit smoking
wellbutrin quit smoking;using guided imagery to quit smoking;...
women quit smoking forums;tobacco products scientific advisory committee;... Stop Smoking Aids;quit smoking tobacco
advertising;... the people behind quitsmoking;the man who
quit smoking;...

Figure 1: TREC subtopics vs. NTCIR intents and subtopic strings.
of relevance with respect to each topic, diversity evaluation requires a set of subtopics (or intents) for each topic, and pooled documents are assessed with respect to each intent. In the TREC Diversity Task, subtopics are manually developed at NIST, based on results of automatic click log analysis [4]; in the NTCIR INTENT Task, intents are determined by manually clustering "subtopics strings" returned by participating systems [6, 7]. However, it is difficult to say exactly what the most appropriate intents are for a given topic for the purpose of evaluating diversified search.
One may be tempted to hypothesise that an effective diversified search system should be effective regardless of the particular choice of intents used to evaluate it. Thus, in this study, we address the following research question: Does the choice of intents for a test collection affect relative performances of diversified search systems? To this end, we use the TREC 2012 Web Track Diversity Task data and the NTCIR-10 INTENT-2 Task data, which share a set of 50 topics but have different intent sets [6, 7]. Figure 1 provides an actual example from the 50 topics we used in our experiments: for this topic ("quit smoking"), four subtopics were obtained for TREC at NIST as shown in Figure 1(a), by following the aforementioned click-based methodology; whereas, at NTCIR, subtopic strings were returned by the participating system of the INTENT2 Subtopic Mining Subtask as shown in Figure 1(c), which were then manually clustered and filtered to form a set of eight intents as shown in Figure 1(b)5. As (a) and (b) were obtained completely independently using different methods, they obviously differ, although they may partially overlap as indicated by the dotted lines in the figure. For example, the first TREC subtopic for this topic is "What are the ways you can quit smoking?" which is probably similar to the second NTCIR intent "(quit smoking) ways."
5As shown in the figure, the INTENT task also estimates the probability of each intent given the query based on assessor voting. However, following the practice at TREC, we assume that the probability distribution is uniform across all intents throughout this study.

921

Table 1: Test collection and pseudo-qrels statistics. Eight

TREC intents with no relevant documents have been removed.

In Part (b), statistics for the truncated pseudo-qrels are shown

in parentheses.

(a) TREC 2012

(b) English INTENT-2

Diversity and pseudo-qrels derived

topics

50 (provided from TREC to NTCIR)

intents/topic

3.7 all: 7.8; matched: 7.7 (3.7)

subtopic strings/topic

­

82.7

pooled non-junk docs/topic

303.9

­

unique relevant/topic 4-relevant/topic 3-relevant/topic 2-relevant/topic 1-relevant/topic

111.2 49.7
2.6 23.5 111.6

287.0 (263.8) ­
11.8 (8.8) 289.7 (182.7) 881.7 (435.3)

foreach topic t do { foreach NTCIR intent i for t do { foreach TREC pooled document d for t do { matchcount = 0; foreach reduced subtopic s for i do { if d contains s by exact match then matchcount + +; } relevancelevel (d) = max(0, trunc(log(matchount ) + 1)); //the function trunc takes the integer part of the argument. }
}
Figure 2: Algorithm for automatically generating pseudo-qrels.

This intent was devised at NTCIR based on a cluster of subtopic strings obtained from participating systems, including "what to do quit smoking" and "ways to quit smoking cigarettes" amongst others. Hereafter, we shall also refer to TREC subtopics as "intents" to avoid confusion.
To address the above research question, we replace the TREC intents with the NTCIR intents and then re-evaluate the runs submitted to the TREC 2012 diversity task. Unfortunately, while the NTCIR-10 INTENT-2 Task had Document Ranking (i.e., diversified search) subtasks for Chinese and Japanese, it only had a Subtopic Mining subtask for English [6, 7], and therefore the English intents from NTCIR lack document relevance assessments. While it would be ideal to actually conduct relevance assessments of the TREC pooled documents with respect to each NTCIR intent, we explore a cheaper alternative in this paper, namely, to automatically construct pseudo-qrels by simply matching the TREC pooled documents against the NTCIR subtopic strings6. While the lack of true relevance assessments for the NTCIR intents is a limitation of this study, our pseudo-qrels do provide partial answers to our research question: our initial results suggest that the choice of intents may in fact affect relative performances, and that this choice may be more important than how many intents are selected for each topic.
2. EXPERIMENTAL SETTING
2.1 Data
Table 1(a) shows some statistics of the TREC 2012 Web Diversity topics, after having removed eight intents from the original qrels.diversity file as they did not have any relevant documents. At TREC, there were six relevance levels: 4 ("navigational"), 3 ("key"), 2 ("highly relevant"), 1 ("relevant"), 0 ("nonrelevant") and -2 ("junk") [4]. The table refers to the first four as 4-, 3-, 2- and 1-relevant, respectively; we treat the other two
6In TREC parlance, qrels means relevance assessments. When qrels are obtained automatically without involving manual relevance assessments, they are often referred to as pseudo-qrels [9].

as nonrelevant. Also, as shown in the table, we had 303.9 pooled non-junk documents per topic on average, which we obtained from qrels.diversity7.
To re-evaluate the TREC 2012 diversity runs after replacing the TREC intents with the NTCIR intents, we created pseudo-qrels as follows: The original NTCIR intents have an average of 7.8 intents and 82.7 subtopic strings per topic (recall Figure 1(b) and (c)). To obtain pseudo-relevant documents from the TREC pools for each NTCIR intents, we first removed the topic string from each NTCIR subtopic string automatically: for example, "women weight gain quit smoking" in Figure 1 was turned into "women weight gain." We call the resultant strings reduced subtopics. We then used the simple algorithm shown in Figure 2 to obtain pseudo-relevant documents for each NTCIR intent. Note that each pooled document is tested whether it matches with any of the reduced subtopic, under the assumption that pooled documents already contain the actual topic string (e.g., "quit smoking") or some related term. The algorithm also determines the relevance level of each document based on the number of matches with reduced subtopics: the actual number of matches (matchcount ) varied from 0 to 19; the (natural) log-based function in the algorithm maps them to 0-3. A total of 28 pooled documents were removed during the process, as they have been detected as containing a virus.
Table 1(b) shows some statistics of the NTCIR-10 INTENT-2 intents and the pseudo-qrels we derived from them. Note that, of the 303.9 TREC pooled documents per topic, as many as 293.6 matched with at least one reduced subtopic and are treated as relevant. This strongly suggests that our pseudo-qrels contain a lot of false matches. Moreover, because of this problem, note that the average number of intents per topic in the pseudo-qrels is 7.7, which is easily twice as large as the corresponding number for TREC, namely, 3.7. Thus, if the evaluation outcome with the pseudo-qrels is different from that with the true qrels, this may be because either (i) the two intent sets contain different intents; or (ii) the two intent sets differ in size (the NTCIR intents sets are larger so may require systems to diversify more aggresively); or both.
In order to separate the above two effects, we also created another version of pseudo-qrels, called truncated pseudo-qrels (or simply "truncated" for short). This was done by cutting down "less popular" intents from the original pseudo-qrels to ensure that the TREC and NTCIR intent sets are equal in size for each topic. For the example shown in Figure 1, although the original pseudo-qrels has eight intents, the truncated pseudo-qrels has only the first four intents with the highest intent probabilities. The statistics for the truncated pseudo-qrels are shown in parentheses in Table 1(b).
2.2 Evaluation Metrics and Analysis Methods
We primarily consider four diversity evaluation metrics: D-nDCG, D -nDCG [8], -nDCG and ERR-IA [3]. D-nDCG is a version of normalised Cumulative Discounted Gain (nDCG) [5] which combines per-intent graded relevance and intent probabilities to compute the gain value of each document. D -nDCG is a simple average of D-nDCG and intent recall (I-rec), a.k.a. subtopic recall [11]. D -nDCG summarises a graph that plots D-nDCG (i.e. overall relevance) against I-rec (pure diversity). -nDCG is a version of nDCG which defines graded relevance as the number of intents covered by a document, and discounts the value of a retrieved relevant document for each intent based on relevant documents already seen. This property is known as diminishing return [2]. ERR-IA first
7At TREC 2012, a common pool was created across the ad hoc task and the diversity task for each topic. Hence, the pooled documents obtained from qrels.diversity are identical to those from qrels.adhoc.

922

computes an Expected Reciprocal Rank value for each intent and then combines them across intents. It also possesses the diminishing return property. Both -nDCG and ERR-IA may be expressed in terms of a common framework, differing primarily in the discounts they apply for document rank [3].
The NTCIR INTENT task uses I-rec, D-nDCG and D -nDCG as the primary metrics for ranking the runs. Following the task's practice, we compute the values using NTCIREVAL8, by using the relevance levels as the gain values. However, it should be noted that I-rec is not a good stand-alone metric for our purpose: although we measure performance at document cutoffs of 10 and 20 (denoted by "@10" and "@20"), recall that the average number of intents per topic with the true qrels is only 3.7: thus it should be fairly easy for systems to cover most intents, especially with 20 documents. Furthermore, I-rec does not work well with our pseudo-qrels: because of the aforementioned false match problem, I-rec is heavily overestimated for all of the TREC runs when the pseudo-qrels are used. Nevertheless, we include the results with I-rec for separating the effects of diversity and relevance in diversified search evaluation [8].
The TREC Web Track Diversity Task uses -nDCG and ERR-IA along with some other metrics. Following the practice at TREC, we computed these metrics using ndeval9. It should be noted that, while NTCIREVAL utilises the per-intent graded relevance data to compute D( )-nDCG, ndeval reduces the data to per-intent binary relevance data before computing -nDCG and ERR-IA. Thus the computation of relevance levels in Figure 2 does not affect these two metrics.
In order to compare the relative performances of the TREC 2012 diversity runs before and after replacing the original TREC intents with the NTCIR ones, we compare the run rankings in terms Kendall's  , and its variant called ap [10]. These measures count the number of pairwise system swaps; ap is more sensitive to the swaps near the top ranks than  is. However, what is perhaps more important is whether replacing the intent sets affects statistical significance testing, which is often used for forming research conclusions in the IR community. We therefore conduct a randomised version of the two-sided Tukey's Honestly Significantly Different (HSD) test [1] at  = .05 for the entire set of runs before and after replacing the intent sets. Given the entire set of runs, this kind of test is more appropriate than those that test one run pair at a time while ignoring the others. We then compare the two sets of significantly different run pairs. For example, is a significantly different run pair obtained according to the TREC intents still significantly different according to the NTCIR intents with its pseudo-qrels?
3. RESULTS AND DISCUSSIONS
Table 2 shows the  and ap between rankings produced by two different metrics based on the true qrels, to show how the diversity metrics behave differently. Table 3 is more important for our purpose: for each metric, the  and ap for the ranking with the true qrels and that with the pseudo-qrels are shown. It can be observed that the rankings with the pseudo-qrels (i.e., those based on the NTCIR intents) are quite different from those with the true qrels (i.e., those based on the TREC intents). This is true even for the truncated pseudo-qrels, as shown in Part (b) of the table, which suggests that the discrepancies between TREC and NTCIR may arise not from how many intents are used but from the actual choice of intents.
8http://research.nii.ac.jp/ntcir/tools/ ntcireval-en.html 9http://trec.nist.gov/data/web2012.html

Table 2:  /ap between two metrics using true qrels (20 TREC

2012 diversity runs).

@10

D-nDCG D -nDCG -nDCG ERR-IA

I-rec

.347/.323 .642/.493 .547/.451 .568/.496

D-nDCG

- .705/.780 .695/.719 .674/.700

D -nDCG

-

- .779/.769 .695/.706

-nDCG

-

-

- .895/.895

@20

D-nDCG D -nDCG -nDCG ERR-IA

I-rec

.179/.319 .705/.661 .495/.537 .453/.522

D-nDCG

- .474/.580 .621/.656 .600/.637

D -nDCG

-

- .705/.745 .579/.635

-nDCG

-

-

- .853/.855

Table 3:  /ap between rankings by the same metric using true

and (truncated) pseudo-qrels (20 TREC 2012 diversity runs).

@10

@20

(a) true

I-rec

.632/.392 .568/.302

vs.

D-nDCG .653/.698 .684/.692

pseudo

D -nDCG .611/.651 .632/.652 -nDCG .674/.673 .716/.704

ERR-IA .589/.611 .589/.614

(b) true

I-rec

.579/.317 .526/.271

vs.

D-nDCG .621/.660 .653/.666

truncated D -nDCG .684/.682 .695/.668 -nDCG .663/.668 .716/.706

ERR-IA .579/.614 .579/.616

1 0.9 0.8 0.7 0.6 0.5 0.4
0.3 (a) I-rec@20

0.8 0.7 0.6 0.5 0.4 0.3 0.2
0.1 (b) D-nDCG@20

true pseudo truncated

uogTrA44xu uogTrB44xu utw2012c1 uogTrA44xl UDInfoDivSt utw2012lda utw2012sc1 srchvrs12c00 srchvrs12c10 UDInfoDivC2 DFalah120D DFalah121D UDInfoDivC1 ICTNET12DVR3 ICTNET12DVR2 ICTNET12DVR1
lcm4res manualSTA
autoSTB autoSTA uogTrA44xu uogTrB44xu DFalah120D DFalah121D uogTrA44xl lcm4res srchvrs12c10 srchvrs12c00 autoSTA autoSTB utw2012c1 manualSTA utw2012lda utw2012sc1 UDInfoDivSt ICTNET12DVR1 ICTNET12DVR2 ICTNET12DVR3 UDInfoDivC1 UDInfoDivC2

0.9 0.8 0.7 0.6 0.5 0.4 0.3
(c) D#-nDCG@20
0.2

0.9 0.8 0.7 0.6 0.5 0.4
0.3 (d) ERR-IA@20
0.2

uogTrA44xu uogTrB44xu uogTrA44xl DFalah120D DFalah121D utw2012c1 srchvrs12c00 srchvrs12c10 utw2012lda utw2012sc1 UDInfoDivSt
lcm4res UDInfoDivC2 UDInfoDivC1 ICTNET12DVR3 ICTNET12DVR2 ICTNET12DVR1
manualSTA autoSTB autoSTA
uogTrA44xu uogTrB44xu DFalah121D DFalah120D utw2012c1 utw2012lda uogTrA44xl utw2012sc1 srchvrs12c00 srchvrs12c10 ICTNET12DVR1
autoSTA ICTNET12DVR3 ICTNET12DVR2
lcm4res autoSTB manualSTA UDInfoDivSt UDInfoDivC1 UDInfoDivC2

Figure 3: Run rankings: true vs. pseudo vs. truncated. The x axis represents runs sorted by a metric with true relevance data from TREC.
Figure 3 visualises the "@20" column of Table 3 for selected metrics. Recall that I-rec is a pure diversity metric; that D-nDCG is an overall relevance metric; and that D -nDCG and ERR-IA consider both aspects. It can be observed that I-rec with pseudo-qrels is almost completely useless for ranking runs. On the other hand, D-nDCG with pseudo-qrels does better: for example, the top two runs in terms of D-nDCG with the true qrels (uogTrA44xu and uogTrB44xu) are still the top two in terms of D-nDCG with the (truncated) pseudo-qrels. The same two runs are also top performers in terms of D -nDCG as well, regardless of the qrels being used. As for ERR-IA, while the same two runs are the top performer in terms of the true qrels, the second run uogTrB44xu is ranked third with the (truncated) pseudo-qrels. To sum up, while our pseudoqrels cannot properly estimate systems's intent recall, the top run at TREC, namely, uogTrA44xu, is still the top run when evaluated with D( )-nDCG and ERR-IA based on the NTCIR intents and the pseudo-qrels. However, the overall rankings do differ when the TREC intents are replaced with those from NTCIR. Again, since the graphs for the original and truncated pseudo-qrels behave very

923

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0
151 156 161 166 171 176 181 186 191 196
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0
151 156 161 166 171 176 181 186 191 196

(a) D#-nDCG @20
true pseudo truncated
(b) ERR-IA @20
true pseudo truncated

Figure 4: Per-topic performance values for lcm4res.

similarly, the discrepancies between TREC and NTCIR are probably due to the choice of intents.
In Figure 3, as indicated by the arrows, Run lcm4res is heavily overestimated with D( )-nDCG and ERR-IA based on the pseudoqrels. Figure 4 provides a per-topic diagnosis for this run with D nDCG and ERR-IA, which reveals that the pseudo-qrels overestimate the run's performance for almost all topics. Perhaps the worst case is Topic 170 ("scooters"), for which there are three TREC intents and eight NTCIR intents: even though the true D -nDCG and ERR-IA values are zero (as indicated by the arrows), the corresponding values with the pseudo-qrels are .8698 and .8750 (.9378 and 1 when truncated). As we have not conducted relevance assessments, we cannot rule out the possibility that this run actually retrieved many documents that are relevant to the NTCIR intents yet nonrelevant to the TREC intents for this topic. However, the overall trend across the topics strongly suggests that our pseudoqrels do not provide accurate estimates of intent recall. We leave the analysis of the TREC runs using true relevance assessments for the NTCIR intents for future work.
We now discuss the effect of replacing the TREC intent sets with the NTCIR ones on statistical significance testing: Table 4 summarises the results. Note that, if the significance test results with true and pseudo-qrels are identical, the number of significantly different pairs in the TR, PS and TRPS will be the same, and that the TR-PS and PS-TR will contain zeroes. Such is not the case. That is, conclusions drawn from an experiment based on the original TREC intents and those drawn from one based on the intents derived from NTCIR can be quite different. For example, in Table 4(b), ERR-IA@20 obtains 31 significantly different run pairs with the true qrels and 18 significantly different run pairs with the pseudo-qrels; but only 9 pairs overlap. Again, truncating the pseudo-qrels (Table 4(c)(d)) does not seem to solve any problems, which again suggests that the choice of intents do matter for the purpose of comparing diversified search systems.
4. CONCLUSIONS AND FUTURE WORK
We addressed the following research question: Does the choice of intents for a test collection affect relative performances of diversified search systems? To this end, we used the TREC 2012 Web Track Diversity Task data and the NTCIR-10 INTENT-2 Task data, which share a set of 50 topics but have different intent sets. Our initial results suggest that the choice of intents may in fact affect relative performances, and that this choice may be more important than how many intents are selected for each topic.
One limitation of the present work is that we used automaticallygenerated pseudo-qrels for the NTCIR intents instead of conducting relevance assessments of TREC pooled documents for the NT-

Table 4: Significance test concordances and discordances be-

tween true qrels and pseudo-qrels (190 TREC 2012 diver-

sity run pairs; randomised two-sided Tukey's HSD test at

 = .05). TR (PS): significant differences obtained with true

qrels (pseudo-qrels); TR-PS (PS-TR): pairs significant with

true qrels (pseudo-qrels) but not significant with pseudo-qrels

(true qrels); TRPS: pairs significant with both true qrels and

pseudo-qrels.

TR PS TR-PS TRPS PS-TR

(a) true

I-rec

12 21

6

6

15

vs.

D-nDCG 51 56

9

42

14

pseudo D -nDCG 25 29

9

16

13

@10

-nDCG 26 24

16

10

14

ERR-IA 29 19

20

9

10

(b) true

I-rec

9 11

7

2

9

vs.

D-nDCG 60 60

15

45

15

pseudo D -nDCG 35 40

9

26

14

@20

-nDCG 26 24

15

11

13

ERR-IA 31 18

22

9

9

(c) true

I-rec

12 11

8

4

7

vs.

D-nDCG 51 47

16

35

12

truncated D -nDCG 25 14

11

14

0

@10

-nDCG 26 9

24

2

7

ERR-IA 29 9

25

4

5

(d) true

I-rec

9 28

3

6

22

vs.

D-nDCG 60 55

20

40

15

truncated D -nDCG 35 35

12

23

12

@20

-nDCG 26 13

24

2

11

ERR-IA 31 10

27

4

6

CIR intents. In particular, we found that the pseudo-qrels estimate intent recall very poorly. On the other hand, we have also found that the official top performer at the TREC 2012 diversity task is still the top performer even after the intent sets have been replaced with the ones from NTCIR. In order to obtain a more clear answer to our research question, we hope to come back to it with true relevance assessments for the NTCIR intents.
5. REFERENCES
[1] B. Carterette. Multiple testing in statistical analysis of systems-based information retrieval experiments. ACM TOIS, 30(1), 2012.
[2] O. Chapelle, S. Ji, C. Liao, E. Velipasaoglu, L. Lai, and S.-L. Wu. Intent-based diversification of web search results: Metrics and algorithms. Information Retrieval, 14(6):572­592, 2011.
[3] C. L. A. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A comparative analysis of cascade measures for novelty and diversity. In Proceedings of ACM WSDM 2011, pages 75­84, 2011.
[4] C. L. A. Clarke, N. Craswell, and E. M. Voorhees. Overview of the TREC 2012 web track. In Proceedings of TREC 2012, 2013.
[5] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM TOIS, 20(4):422­446, 2002.
[6] T. Sakai, Z. Dou, T. Yamamoto, Y. Liu, M. Zhang, M. P. Kato, R. Song, and M. Iwata. Overview of the NTCIR-10 INTENT-2 task. In Proceedings of NTCIR-10, 2013.
[7] T. Sakai, Z. Dou, T. Yamamoto, Y. Liu, M. Zhang, M. P. Kato, R. Song, and M. Iwata. Summary of the NTCIR-10 INTENT-2 task: Subtopic mining and search result diversification. In Proceedings of ACM SIGIR 2013, 2013.
[8] T. Sakai and R. Song. Evaluating diversified search results using per-intent graded relevance. In Proceedings of ACM SIGIR 2011, pages 1043­1042, 2011.
[9] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In Proceedings of ACM SIGIR 2001, pages 66­73, 2001.
[10] E. Yilmaz, J. Aslam, and S. Robertson. A new rank correlation coefficient for information retrieval. In Proceedings of ACM SIGIR 2008, pages 587­594, 2008.
[11] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. In Proceedings of ACM SIGIR 2003, pages 10­17, 2003.

924

A Comparison of the Optimality of Statistical Significance Tests for Information Retrieval Evaluation

Julián Urbano jurbano@inf.uc3m.es

Mónica Marrero mmarrero@inf.uc3m.es

Diego Martín dmartin@dit.upm.es

University Carlos III of Madrid Department of Computer Science
Leganés, Spain

Technical University of Madrid Department of Telematics Engineering
Madrid, Spain

ABSTRACT
Previous research has suggested the permutation test as the theoretically optimal statistical significance test for IR evaluation, and advocated for the discontinuation of the Wilcoxon and sign tests. We present a large-scale study comprising nearly 60 million system comparisons showing that in practice the bootstrap, t-test and Wilcoxon test outperform the permutation test under different optimality criteria. We also show that actual error rates seem to be lower than the theoretically expected 5%, further confirming that we may actually be underestimating significance.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation.
Keywords
Evaluation, Statistical significance, Randomization, Permutation, Bootstrap, Wilcoxon test, Student's t-test, Sign test.
1. INTRODUCTION
An Information Retrieval (IR) researcher is often faced with the question of which of two IR systems, A and B, performs better. She conducts an experiment with a test collection, and chooses an effectiveness measure such as Average Precision or nDCG. Based on the effectiveness difference she concludes that, for instance, system A is better. But we know there is inherent noise in the evaluation for a wealth of reasons concerning document collections, topic sets, relevance assessors, etc. Therefore the researcher needs the conclusion to be reliable, that is, the observed difference unlikely to have happened just by random chance. She employs a statistical significance test to compute this probability (the p-value). If p   (the significance level, usually  = 0.05 or  = 0.01) the difference is considered statistically significant (A B). In practice this means that she can be confident that the difference measured with a similar test collection
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

will be (at least) as large as currently observed. If p >  the difference is not significant (A B), and she can not be confident that the observed difference is indeed real.
Unfortunately, there has been a debate regarding statistical significance testing in IR evaluation. Classical tests such as the paired t-test, the Wilcoxon test and the sign test make different assumptions about the distributions, and effectiveness scores from IR evaluations are known to violate these assumptions. The bootstrap test is an alternative that makes fewer assumptions and has other advantages over classical tests, and the permutation or randomization test is an even less stringent test in terms of assumptions that theoretically provides exact p-values. Because IR evaluations violate most of the assumptions, it is very important to know how robust these tests are in practice and which one is optimal.
Previous work [4, 5] compared these five tests with TREC Ad Hoc data, reaching the following conclusions: a) the bootstrap, t-test and permutation test largely agree with each other, so there is hardly any practical difference in using one or another; b) the permutation test should be the test of choice, though the t-test seems suitable as well; the bootstrap test shows a bias towards small p-values; c) the Wilcoxon and sign tests are unreliable and should be discontinued for IR evaluation. However, all these conclusions were based on the assumption that the permutation test is optimal. For example, authors showed that the Wilcoxon and sign tests fail to detect significance when the permutation test does and vice versa. That is, they are unreliable according to the permutation test.
But we may follow different criteria to chose an optimal test. We may want the test to be powerful, that is, to produce significant results as often as possible. Additionally, we may want it to be safe and yield low error rates so that it is unlikely that we draw wrong conclusions. But power and safety are inversely related; different tests show different relations depending on the significance level. The lower  the lower the power, because we need p   for the result to be significant. Error rates are expected to be at the nominal  level, so the higher the significance level the higher the expected error rate. The test is exact if we can trust that the actual error rate is as dictated by the significance level. If it is below it means we are being too conservative and we are missing significant results; if it is above it means we are deeming as significant results that probably are not.
This paper presents a large-scale empirical study that compares all five tests according to these optimality criteria, providing significance and error rates at various significance levels for 50-topic sets. Our main findings are:

925

· In practice the bootstrap test is optimal in terms of power, the t-test is optimal in terms of safety, and the Wilcoxon test is optimal in terms of exactness.
· For all tests the actual error rate seems to be lower than the nominal 0.05 level, meaning that we are actually being too conservative.
· In practice the permutation test is not found to be optimal under any criterion.
2. DATA AND METHODS
To compare the five statistical significance tests at hand, we employed data from the TREC 2004 Robust Track. A total of 249 topics were used, 100 of which were originally developed in the TREC 7 and 8 Ad Hoc tracks (50 and 50). A total of 110 runs were submitted by 14 different groups. This dataset is unusually large both in terms of topics and runs, given that TREC tracks usually employ 50 topics. The subset with the 100 Ad Hoc topics is especially interesting: all 100 topics were developed and judged by the same assessors for the most part, and they were developed using the same methodology and pooling protocol with roughly the same number of runs contributing to the pools [6]. Additionally, all three tracks used disks 4 and 5 as document collection. Therefore, we can consider these two sets of 50 topics as two different samples drawn from the same universe of topics.
We randomly split these 100 topics into two disjoint subsets of 50 topics each: T and T . For each of these two subsets we evaluated all 110 runs as per Average Precision. This provides us with 5,995 system pairwise comparisons with T and another 5,995 with T . We ran all five statistical significance tests between each of these system pairs1. This gives us a total of 5,995 pairs of p-values per test, which can be regarded as the two p-values observed with two different test collections for any two systems. We performed 1,000 random trials of this experiment, so we have a total of 5,995,000 system pairwise comparisons and the corresponding 5,995,000 with another topic subset. Thus, this paper reports results on nearly 12 million p-values for each of the five tests, for a grand total of nearly 60 million p-values. To our knowledge, this is to date the largest study of this type.
Given an arbitrary topic set split, the 5,995 pairs of pvalues provided by a test can be used to study its optimality. Consider a researcher that used topic subset T and ran a test to compute a p-value; under the significance level  he draws a conclusion. What can he expect with a different topic subset T ?. One of these situations can occur:
· Non-significance. The result with T is A B. We can really expect any result with T ; there is a lack of statistical power in the experiment.
· Success. The result with both T and T is A B. Both experiments show evidence of one system outperforming the other.
· Lack of power. The difference is A B with T but it is A B with T . There is evidence of a lack of power in the second experiment.
· Minor error. The result with T is A B, but with T it is A  B. The second experiment shows some evidence of a wrong conclusion in the first one.
· Major error. The result with T is A B, but with T it is A  B. The two experiments conflict.
1As in [4, 5], we calculated 100,000 samples in the permutation and bootstrap tests.

A powerful test minimizes the non-significance rate, a safe test minimizes the minor and major error rates, and an exact test maintains the global error rate at the nominal  level.
3. RESULTS
For every statistical significance test we computed the non-significance, success, lack of power and error rates at 32 significance levels   {0.0001, ..., 0.0009, 0.001, ..., 0.009, ..., 0.1, ..., 0.5}. Tables 1 and 2 report the results for a selection of significance levels, and Figures 1 and 2 plot detailed views in the arguably most interesting [0.001, 0.1] range. Please note that all plots are log-scaled.
Non-significance rate. The bootstrap test consistently produces smaller p-values, and it is therefore the most powerful of all tests across significance levels. Next are the permutation test for  < 0.01 and the Wilcoxon test for the usual   0.01. The t-test is consistently less powerful, though the difference is as small as roughly 1% fewer significant results at the usual  = 0.05. The sign test is by far the least powerful of all five. Its stair-like behavior is explained by its resolution: p-values depend only on the sign of the score differences, not on the magnitude (see Figure 5 in [4]).
Success rate. The bootstrap and Wilcoxon tests are the most successful overall. For small significance levels   0.001 the bootstrap test shows the highest success rate, but for the more usual levels 0.001 <   0.05 the Wilcoxon test performs better. Next are again the permutation test and the t-test, with very similar success rates about 0.3% lower than the Wilcoxon and bootstrap tests at the usual  levels. The sign test is clearly the worst of all.
Lack of power rate. Most of the unsuccessful comparisons are due to a lack of power with the second topic subset T . Relative results are comparable to results above: the bootstrap test dominates at small significance levels and the Wilcoxon tests dominates at the usual levels, again followed by the permutation test and the t-test.
Minor error rate. Except for rare occasions where the sign test's step-like behavior results in the smallest minor error rate, the t-test is generally the safest of all five across significance levels. The permutation test follows next with rates about 0.03% higher. The bootstrap test is consistently outperformed by the t-test and the permutation test; it yields 0.13% more minor errors at  = 0.05. The Wilcoxon test performs even better than the permutation test for low significance levels, but it performs worse at the usual levels. As mentioned, the sign test wiggles between the other tests.
Major error rate. Similarly the t-test consistently performs best in terms of major errors, followed by the permutation and bootstrap tests. It is noticeable that for small significance levels neither of these three tests show any major error at all. For instance, at  = 0.005 the t-test provides as many as 3,006,441 (50.2%) significant comparisons, and yet none of them results in a major error with the second topic subset. The Wilcoxon test outperforms the permutation test sporadically, but it performs worse overall. In general though, it is important to the bear in mind the magnitudes of the major error rates. For instance, at  = 0.05 the t-test produced 1,082 major errors and the bootstrap test produced 1,523. While the difference may seem small compared to the total of significants (0.0277% vs 0.0383%), this is actually a large +41% relative increase. The sign test is clearly the worst of all, having an extremely large major error rate at small significance levels.

926

Non-significance rate
 t-test perm. boot. Wilcox. sign .0001 .67698 .65006 .6402 .67189 .72367 .0005 .61184 .59202 .58186 .60471 .6782 .001 .5807 .56367 .5532 .5722 .63438 .005 .49842 .48755 .47647 .48911 .58347 .01 .45752 .44937 .43847 .4485 .53308 .05 .34779 .34539 .33613 .34215 .42762 .1 .29264 .29235 .28412 .28725 .37308 .5 .12398 .12581 .12153 .11957 .14934

Success rate
t-test perm. boot. Wilcox. sign
.78749 .79451 .79757 .78859 .73691 .80788 .8107 .8123 .80765 .75479 .81328 .81491 .81547 .8147 .76847 .82051 .82145 .82365 .82598 .78018 .82777 .82893 .83233 .83338 .79225 .85579 .85565 .85856 .85935 .81999 .86905 .86899 .87086 .86941 .83013 .8836 .88369 .8836 .88429 .85641

Lack of power rate
t-test perm. boot. Wilcox. sign
.21222 .20503 .20191 .21107 .26264 .19138 .18827 .18653 .19146 .24431 .18556 .18359 .18285 .18392 .22998 .1764 .17503 .17243 .17039 .2169 .16753 .16595 .16205 .16115 .20276 .13157 .1314 .12743 .12624 .16709 .1107 .11072 .10736 .10805 .15031 .05175 .05232 .05088 .04985 .07511

Table 1: Non-significance rates over total of pairs (lower is better), success rates over total of significants (higher is better), and lack of power rates over total of significants (lower is better). Best per  in bold face.

0.65

Non-significance rate
t-test permutation bootstrap Wilcoxon sign

Success rate

Lack of power rate

0.14 0.16 0.18 0.20 0.22

Lacks of power / Total significants

0.80 0.82 0.84 0.86

Successes / Total significants

0.35 0.4 0.45 0.5 0.55

Non-significants / Total

0.78

0.12

0.3

0.76

.001

.005 .01

.05

.1

.001

.005 .01

.05

.1

.001

.005 .01

.05

.1

Significance level 

Significance level 

Significance level 

Figure 1: Non-significance rates over total of pairs (lower is better), success rates over total of significants (higher is better), and lack of power rates over total of significants (lower is better).

Minor error rate
 t-test perm. boot. Wilcox. sign .0001 .00029 .00046 .00051 .00034 .00045 .0005 .00074 .00104 .00117 .00089 .00089 .001 .00116 .00149 .00168 .00138 .00155 .005 .00309 .00352 .00392 .00362 .00282 .01 .00469 .00511 .0056 .00546 .00484 .05 .01236 .01264 .01363 .014 .01251 .1 .01903 .01906 .02027 .02123 .01862 .5 .03403 .03409 .03389 .03645 .03518

Major error rate

t-test perm. boot. Wilcox. sign

0

0

0

5.08e-7 6.04e-7

0

0

0

4.22e-7 5.18e-7

0

0

0

3.9e-7 4.56e-7

0

0

6.37e-7 1.96e-6 .0001

.00001 .00001 .00002 .00001 .00016

.00028 .0003 .00038 .0004 .00041

.00122 .00123 .00152 .00131 .00095

.03062 .0299 .03163 .02941 .0333

Global error rate
t-test perm. boot. Wilcox. sign
.00029 .00046 .00051 .00034 .00045 .00074 .00104 .00117 .00089 .00089 .00116 .00149 .00168 .00138 .00155 .00309 .00352 .00392 .00362 .00292 .0047 .00512 .00562 .00547 .00499 .01264 .01294 .01402 .01441 .01292 .02025 .02029 .02178 .02254 .01956 .06465 .06399 .06552 .06586 .06849

Table 2: Minor error rates over total of significants (lower is better), major error rates over total of significants (lower is better), and global error rates over total of significants (errors =  is better). Best per  in bold face.

0.020

Minor error rate
t-test permutation bootstrap Wilcoxon sign
y=x

5e-04

Major error rate

2e-02 5e-02

Global error rate

0.010

2e-03 5e-03

Minor and Major errors / Total significants

5e-05

Major errors / Total significants

0.005

Minor errors / Total significants

5e-06

0.002

5e-04

5e-07

0.001

.001

.005 .01

.05

.1

.001

.005 .01

.05

.1

.0001 .0005.001

.005 .01

.05 .1

.5

Significance level 

Significance level 

Significance level 

Figure 2: Minor error rates over total of significants (lower is better), major error rates over total of significants (lower is better), and global error rates over total of significants (errors =  is better).

927

Global error rate. Aggregating minor and major errors we have a global error rate that can be used as an overall indicator of test safety and exactness. Given the relative size of minor and major error rates, the trends are here nearly the same as fwith minor errors, but for the sake of completeness we plot the full range of significance levels. The t-test approximates best the nominal error rate for low significance levels, but the Wilcoxon test does better for the usual levels and best overall. Surprisingly the permutation test does not seem to be the most exact at any significance level.
4. DISCUSSION
Zobel [7] compared the t-test, Wilcoxon test and ANOVA at  = 0.05, though with only one random split in 25-25 topics. He found lower error rates with the t-test than with the Wilcoxon test, and generally lower than the nominal 0.05 level. Given that the latter showed higher power and has more relaxed assumptions, he recommended it over the t-test. Sanderson and Zobel [3] ran a larger study also with splits of up to 25-25 topics. They found that the sign test has higher error rates than the Wilcoxon test, which has itself higher error rates than the t-test. They also suggested that the actual error rate is below the nominal 0.05 level when using 50 topic sets. Voorhees [6] also observed error rates below the nominal 0.05 level for the t-test, but more unstable effectiveness measures resulted in higher rates. Cormack and Lynam [1] used 124-124 topic splits and various significance levels. They found the Wilcoxon test more powerful than the t-test and sign test; and the t-test safer than the Wilcoxon and sign test. Sakai [2] proposed the bootstrap method for IR evaluation, but did not compare it with other tests.
Smucker et al. [4] compared the same five tests we study in this paper, arguing that the t-test, permutation and bootstrap tests largely agree with each other. Nonetheless, they report RMS Errors among their p-values of roughly 0.01, which is a large 20% for p-values of 0.05. Based on the argument that the permutation test is theoretically exact, they concluded that the Wilcoxon and sign tests are unreliable, suggesting that they should be discontinued for IR evaluation. They find the bootstrap test to be overly powerful, and given the appealing theoretical optimality of the permutation test they propose its use over the others, though the t-test admittedly performed very similarly. In a later paper [5] they found that the tests tended to disagree with smaller topic sets, though the t-test still showed acceptable agreement with the permutation test, again assumed to be optimal. The bootstrap test tended again to produce smaller p-values, so authors recommend caution if using it.
In this paper we ran a large-scale study to revisit these issues under different optimality criteria. In terms of safety, the t-test produced the smallest error rates across significance levels, followed by the Wilcoxon test for low levels and the permutation test for usual levels. In general, all tests yielded error rates higher than expected for low significance levels, but much lower for the usual levels. This suggests that we are being too conservative when assessing statistical significance at  = 0.05; we expect 5% of our significant results to be wrong, but in practice only about 1.3% do indeed seem wrong. We must note though that this global error rate, as the sum of minor and major errors, is just an approximation of the true Type I error rates [1].
Table 3 shows the agreement of the five tests with themselves: p-values with topic subset T compared to those with

t-test perm. boot. Wilcox. sign p .0001 .03603 .04348 .04475 .03514 .0556 .0001< p .0005 .10124 .11635 .11923 .09976 .13014 .0005< p .001 .13059 .12999 .1623 .14516 .14619 .001< p .005 .16716 .17044 .20032 .17841 .18024 .005< p .01 .20724 .21624 .2387 .21454 .21737 .01< p .05 .25275 .26685 .29801 .25779 .26114 .05< p .1 .29734 .31101 .33996 .30015 .30344 .1< p .5 .31624 .31855 .33816 .31804 .31802
Table 3: RMS Error of all five tests with themselves (lower is better). Best per bin in bold face.
subset T . The Wilcoxon test turns out to be the most stable of all for very small p-values, and generally more so than the permutation test. The t-test is the most stable overall. Indeed, if we compute the difference between the actual and nominal error rates we find that the Wilcoxon test is the one that best tracks the significance level and therefore seems to be the most exact (RMSE 0.1146), followed by the bootstrap, t-test, sign and permutation tests (RMSEs 0.1148, 0.1153, 0.1153 and 0.1155). This is particularly interesting for the bootstrap test: it provides the most significant results and the actual error rate is still lower than expected.
In summary, a researcher that wants to maximize the number of significant results may use the more powerful bootstrap test and still be safe in the usual scenario. Researchers that want to maximize safety may use the t-test, and researchers that want to be able to trust the significance level may proceed with the Wilcoxon test. For large meta-analysis studies we encourage the use of the t-test and Wilcoxon test because they are far less computationally expensive and show near-optimal behavior. Unlike previous work concluded, our results suggest that in practice the permutation test is not optimal under any criterion. Further analysis with varied test collections and effectiveness measures should be conducted to clarify this matter, besides devising methods to better approximate what actual Type I error rates we have in IR evaluation. We further support the argument of discontinuing the sign test.
5. REFERENCES
[1] G. V. Cormack and T. R. Lynam. Validity and Power of t-test for Comparing MAP and GMAP. In ACM SIGIR, pages 753­754, 2007.
[2] T. Sakai. Evaluating Evaluation Metrics Based on the Bootstrap. In ACM SIGIR, pages 525­532, 2006.
[3] M. Sanderson and J. Zobel. Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability. In ACM SIGIR, pages 162­169, 2005.
[4] M. D. Smucker, J. Allan, and B. Carterette. A Comparison of Statistical Significance Tests for Information Retrieval Evaluation. In ACM CIKM, pages 623­632, 2007.
[5] M. D. Smucker, J. Allan, and B. Carterette. Agreement Among Statistical Significance Tests for Information Retrieval Evaluation at Varying Sample Sizes. In ACM SIGIR, pages 630­631, 2009.
[6] E. M. Voorhees. Topic Set Size Redux. In ACM SIGIR, pages 806­807, 2009.
[7] J. Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In ACM SIGIR, pages 307­314, 1998.

928

Assessor Disagreement and Text Classifier Accuracy

William Webber
College of Information Studies University of Maryland
United States of America wew@umd.edu

Jeremy Pickens
Catalyst Repository Systems Denver, CO
United States of America jpickens@catalystsecure.com

ABSTRACT
Text classifiers are frequently used for high-yield retrieval from large corpora, such as in e-discovery. The classifier is trained by annotating example documents for relevance. These examples may, however, be assessed by people other than those whose conception of relevance is authoritative. In this paper, we examine the impact that disagreement between actual and authoritative assessor has upon classifier effectiveness, when evaluated against the authoritative conception. We find that using alternative assessors leads to a significant decrease in binary classification quality, though less so ranking quality. A ranking consumer would have to go on average 25% deeper in the ranking produced by alternative-assessor training to achieve the same yield as for authoritative-assessor training.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and software--performance evaluation.
General Terms
Evaluation
Keywords
Text classification, evaluation, assessor disagreement
1. INTRODUCTION
Text classification based upon machine learning is a useful tool for text retrieval tasks on corpora with many relevant documents, where high recall is required, and where the searcher is willing to devote significant effort to the task. One such environment is that of e-discovery--the retrieval of responsive documents in civil law-- and classification technologies have been widely deployed there.
To learn a relevance model, a machine learner is provided with example documents, annotated by a human assessor. The assessor making the relevance judgments may not, however, be the person whose conception of relevance is authoritative. In e-discovery, for instance, senior lawyers commonly delegate assessment to junior lawyers or contract paralegals, due to time and cost constraints.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Human assessors frequently disagree on document relevance [8], which questions the use of non-authoritative assessors to train text classifiers. How reliable are classifiers trained by non-authoritative assessors when evaluated by the authoritative conception of relevance? Does the classifier compensate for the disagreement between assessors, or does it amplify it?
2. PREVIOUS WORK
In an experiment reported by Voorhees [8], TREC AdHoc documents were assessed by two alternative assessors, and high levels of assessor disagreement were observed. Based on simulation experiments, Carterette and Soboroff [2] find that overly-conservative assessors (those who find fewer documents relevant) distort retrieval effectiveness evaluation less than liberal ones do.
In the e-discovery domain, Grossman and Cormack [5] compare non-authoritative assessors with automated techniques guided by authoritative feedback, finding the latter to be at least as reliable as the former when evaluated against the authoritative conception of relevance. Webber [9] analyses assessor agreement levels on the same dataset, finding considerable variability in assessor reliability.
Brodley and Friedl [1] present methods for automatically identifying mislabeled training data by using ensemble classifiers to detect outliers. Ramakrishnan et al. [7] similarly use a Bayesian network to detect outliers in textual data. Such methods do not work, however, if the annotator is consistently incorrect.
3. MATERIALS AND METHODS
We distinguish two assessors: the training assessor, who makes the annotations of the training examples; and the testing assessor, whose conception of relevance the output classifier is intended to represent. Where training and test assessor are the same, we refer to the task as self-classification. Where the assessors are different, we refer to the task as cross-classification.
3.1 Metrics
We use F1 score--the harmonic mean of precision and recall-- as our measure of effectiveness for binary classification. To measure ranking quality, we calculate the maximum F1 score achievable across all possible cutoff points in the ranking (termed hypothetical F1 by Cormack et al. [4]). Area under the ROC curve gives similar trends to those reported here. Significance testing is by paired two-tailed t tests.
3.2 Dataset
Our dataset is taken from the TREC 4 AdHoc track. In that year, the organizers arranged for selected documents to be triplyassessed, first by the author of the TREC topic, and then by two additional assessors, who were authors of other TREC topics [8].

929

1.0

1.0

F1 (cross-classification)

Max-F1 (cross-classification)

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0
0.0 0.2 0.4 0.6 0.8 1.0
Max-F1 (self-classification)
Figure 1: (Ranking) Maximum-F1 score for cross classification versus self-classification, with the original assessor as target assessor. An origin-anchored regression line is drawn.
We treat the original assessor as the authoritative, testing assessor, and separately treat each additional assessor as a training assessor for cross-classification.
We restrict our document set to the Associated Press (AP) subcollection, in order to avoid certain biases in the original (nonrandom) selection of documents for multiple assessment. We include only those 39 (of the original 49) topics for which all three assessors found at least 8 AP documents relevant. The mean number of relevant documents per topic is 73 (standard deviation 60), and of irrelevant documents 191 (sd 31). The mean F1 between the original and alternative assessors is 0.63 (sd 0.21).
3.3 Classifier
We use LibSVM as our classifier [3], with a linear kernel and default settings. Features are term TF*IDF scores, using length normalization, the Lovins stemmer, case folding, and stop word removal. Inverse document frequency was calculated only on AP documents multiply-assessed for at least one topic.
As the dataset is small, classification is approximated by classifying each tenth of the collection using a model trained on the other nine-tenths. The tested tenths are then amalgamated to form a single, margin-based ranking. Holdout experiments showed a mean Kendall's  of 0.88 between document rankings produced by different fold models, indicating high stability between models.
LibSVM optimizes its binary classification for accuracy, but this proved to give poor results for the F1 measure. Instead, we create binary rankings by fitting probabilities using the method of Platt [6], then choosing the cutoff point that optimizes F1 in expectation.
4. RESULTS
4.1 Self- versus cross-classification
We begin by comparing the ranking effectiveness of the classifier trained by the authoritative assessor (self-classification) with that trained by an alternative assessor (cross-classification). Figure 1 compares the max-F1 scores achieved by the two approaches.

Max-F1 (liberal assessor)

0.0 0.0 0.2 0.4 0.6 0.8 1.0 F1 (self-classification)
Figure 2: (Binary) F1 score for cross classification versus selfclassification.
1.0
0.8
0.6
0.4
0.2
0.0 0.0 0.2 0.4 0.6 0.8 1.0 Max-F1 (conservative assessor)
Figure 3: Cross-classification effectiveness of conservative versus liberal alternative assessor, with original assessor as target, as measured by maximum-F1 score.
Mean max-F1 is 0.738 for self- and 0.637 for cross-classification; the difference is highly significant (p < 0.0001). Cross-classification leads to an average max-F1 score 14% than self-classification.
Next we consider binary classification, as shown in Figure 2. Mean binary F1 is 0.629 for self- and 0.456 for cross-classification, again a highly significant difference. Cross-classification leads to a 28% lower F1 score than self-classification, a greater fall than for max-F1. Cross-classification seems to harm selection of a binary cutoff even more than it does ranking of the documents.
4.2 Comparing different assessor types
An interesting question is whether, given an assessor disagrees

930

Max-F1 (union of assessors) Maximum F1

1.0
0.8
0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Max-F1 (intersection of assessors)
Figure 4: Cross-classification effectiveness of the union of alternative assessors' relevant documents versus the intersection, measured using maximum-F1 score.
with the authoritative conception, it is better that the assessors tends to assign more documents as relevant (the assessor is liberal), or fewer (the assessor is conservative). We explore this question by denoting the alternative assessor with the lower prevalence for each topic the conservative assessor, and the assessor with the higher prevalence the liberal assessor. Figure 3 compares the max-F1 scores on the rankings produced via cross-classification using conservative versus liberal assessors. Mean max-F1 is 0.629 for the conservative assessors, 0.646 for the liberal ones. The difference, however, is not significant (p > 0.1).
A related question is how to combine multiple assessments, where available, when creating training data. Should the union of the documents found relevant by either assessor be marked relevant in the training data, or the intersection (that is, only documents both assessors find relevant)? Figure 4 compares two alternatives: marking as relevant documents found relevant by either assessor (union), versus only those found relevant by both (intersection). The intersection of the assessors gives a mean max-F1 of 0.623, the union one of 0.657, with the difference being statistically significant (p < 0.05). It seems on balance better to give more, if noisier, examples of relevant documents than fewer, if cleaner, examples. (Only retaining examples on which both assessors agreed was also tried; the mean max-F1 score is intermediate between that for the intersection and that for the union.)
4.3 Random disagreement
The previous sections have examined the absolute loss of effectiveness from using non-authoritative assessors to train the classifier. Is this loss greater or less than one would expected, given inter-assessor agreement? One way of answering this is to compare cross-classification effectiveness of the actual alternative assessor, with that of other randomly simulated alternative assessors having the same agreement level. We do this by starting with the original assessments and the false positive and false negative counts, FP and FN, of the alternative assessor (we arbitrarily choose the first alternative assessor for this experiment). We then random select FP of the originally irrelevant documents and mark them relevant, and

1.0

0.8 0.6 0.4

_x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x

0.2 _

Actual

0.0

Mean Random

0

10

20

30

40

Topics (ordered by median random Maximum F1)

Figure 5: Actual cross-classification effectiveness versus range of effectivenesses of randomly-degraded cross-classification, measured using maximum-F1 score. The mean and 95% intervals on the random cross-classifications are shown. Topics are sorted by mean random cross-classifier effectiveness. There are 81 random simulations of alternative assessors for each topic.

FN of the originally relevant documents, and mark them irrelevant, creating a simulated alternative assessor training set. We then train a cross-classifier on this simulated set, and compare its effectiveness with the actual alternative assessor.
Figure 5 compares simulated cross-classification effectiveness (across 81 simulations per topic) with that of the actual alternative assessor, measured using max-F1. The mean of the actual max-F1 scores is 0.633, that of the median random 0.615; the difference is statistically significant (p < 0.05). On average, the actual alternative assessor gives slightly better ranking quality than inter-assessor agreement would predict, though the difference is small. There is considerable variability between topics (or assessors): actual is outside the empirical 95% interval for 7 of the 39 topics (above for 3, below for 4).
4.4 User effort
Differences in effectiveness have been expressed in previous sections in terms of the system evaluation metrics of F1 and max-F1. These results can be difficult to interpret in terms of the actual cost to the user of poorer performance. One way of measuring this cost is how much further down the ranking one must go in order to achieve a certain level--say 75%--of recall. In e-discovery, productions are often finalized by manually reviewing the ranking from the top down to the point where it is estimated that a certain threshold of recall (and 75% is one such threshold1) has been achieved, so depth to achieve 75% recall is a reasonable measure of one component of expense in e-discovery.
Figure 6 compares the proportion of the ranking that must be processed to achieve 75% recall for cross-classification with that
1See, for instance, Global Aerospace Inc., et al., v. Landow Aviation, L.P., et al., No. CL 61040 (Va. Cir. Ct. Apr. 9, 2012) ("Memorandum in support of motion for protective order approving the use of predictive coding").

931

Cross-classification

1.0
0.8
0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Self-classification
Figure 6: Proportion of ranking that must be processed in order to achieve 75% recall, under cross-classification and selfclassification.
for self-classification. In the median case, using cross-classification requires that 24% more of the ranking must be processed than using self-classification, but around one in eight cases, processing must go to twice the depth or more.
5. CONCLUSION
In this paper, we have examined the loss of effectiveness that occurs when a text classifier is trained using annotations made by an assessor other than the authoritative assessor, whose conception of relevance is to be used to evaluate the classifier's effectiveness. We have found that using a non-authoritative assessor leads to a significant decrease in classifier reliability, of around 14% for ranking quality measured using maximum F1, and twice that for binary classification measured using F1 score. In terms of user effort, this means that around 24% more of the ranking must be processed to achieve recall of 75%. The liberality or conservativeness of the assessor does not make a significant difference to cross-classification reliability, though where multiple assessments are available, it seems slightly better to take the union of their relevance sets rather than their intersection as training data. Cross-classification leads to slightly better average performance than might be expected given the degree of inter-assessor disagreement (as measured via a random simulation experiment). However, for all of these findings, there is considerable variability between tasks and between assessors.
Considerable future work remains to be done. Though the training sets employed here have been sufficient to achieve creditable accuracy (mean F1 of 0.629) on the low-yield ad-hoc tasks, larger training sets are used in many text-classification tasks, such as ediscovery, where a few thousand training examples are more common. Larger training sets may contain more redundancy, reducing the impact of assessor disagreement; though to the extent that disagreement is systematic rather than random, the reduction may be slight. Similarly, the relative desirability of liberal or conservative assessors, or of the union or intersection of multiple assessment sets, will likely be affected by the amount of training data. We have

explored the question of user cost in terms of additional processing of the output ranking; another dimension of cost that a larger experimental training set would allow us to explore is the additional number of annotations required under cross-classification to achieve the same effectiveness as self-classification, and also what the (near-) maximum effectiveness achievable with both assessor types is. We intend to explore this question using TREC Legal Track data.
Finally, the evaluation metrics used in this paper include F1 and user effort. However, user effort does not always have uniform cost. One of the primary motivations for this work is that nonauthoritative assessors (e.g. junior attorneys in an e-discovery matter) have a lower hourly cost than authoritative assessors (e.g. senior attorneys). One of the next phases of this research is integrating economic cost models with retrieval effectiveness metrics, to paint an overall picture of the cost of using non-authoritative, less accurate assessors.
Acknowledgments
This material is based in part upon work supported by the National Science Foundation under Grant No. 1065250. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
References
[1] Carla E. Brodley and Mark A. Friedl. Identifying mislabeled training data. Journal of Artificial Intelligence Research, 11:131­167, 1999.
[2] Ben Carterette and Ian Soboroff. The effect of assessor errors on IR system evaluation. In Proc. 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 539­546, Geneva, Switzerland, July 2010.
[3] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1­27:27, 2011. Software available at http://www. csie.ntu.edu.tw/~cjlin/libsvm.
[4] Gordon V. Cormack, Maura R. Grossman, Bruce Hedin, and Douglas W. Oard. Overview of the TREC 2010 legal track. In Ellen Voorhees and Lori P. Buckland, editors, Proc. 19th Text REtrieval Conference, pages 1:2:1­45, Gaithersburg, Maryland, USA, November 2010.
[5] Maura R. Grossman and Gordon V. Cormack. Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review. Richmond Journal of Law and Technology, 17 (3):11:1­48, 2011.
[6] John C. Platt. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In Alexander J. Smola, Peter Bartlett, Bernhard Schölkopf, and Dale Schuurmans, editors, Advances in Large Margin Classifiers, pages 61­74. MIT Press, 1999.
[7] Ganesh Ramakrishnan, Krishna Prasad Chitrapura, Raghu Krishnapuram, and Pushpak Bhattarcharyy. A model for handling approximate, noisy or incomplete labeling in text classification. In Proc. 22nd International Conference on Machine Learning, pages 681­688, Bonn, Germany, August 2005.
[8] Ellen Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5):697­716, September 2000.
[9] William Webber. Re-examining the effectiveness of manual review. In Proc. SIGIR Information Retrieval for E-Discovery Workshop, pages 2:1­8, Beijing, China, July 2011.

932

Extracting Query Facets from Search Results

Weize Kong and James Allan
Center for Intelligent Information Retrieval School of Computer Science
University of Massachusetts Amherst Amherst, MA 01003
{wkong, allan}@cs.umass.edu

ABSTRACT
Web search queries are often ambiguous or multi-faceted, which makes a simple ranked list of results inadequate. To assist information finding for such faceted queries, we explore a technique that explicitly represents interesting facets of a query using groups of semantically related terms extracted from search results. As an example, for the query "baggage allowance", these groups might be different airlines, different flight types (domestic, international), or different travel classes (first, business, economy). We name these groups query facets and the terms in these groups facet terms. We develop a supervised approach based on a graphical model to recognize query facets from the noisy candidates found. The graphical model learns how likely a candidate term is to be a facet term as well as how likely two terms are to be grouped together in a query facet, and captures the dependencies between the two factors. We propose two algorithms for approximate inference on the graphical model since exact inference is intractable. Our evaluation combines recall and precision of the facet terms with the grouping quality. Experimental results on a sample of web queries show that the supervised method significantly outperforms existing approaches, which are mostly unsupervised, suggesting that query facet extraction can be effectively learned.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Clustering, Query formulation
General Terms
Algorithms, Experimentation
Keywords
Query Facet, Semantic Class Extraction, Multi-faceted Query
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1. INTRODUCTION
Web search queries are often ambiguous or multi-faceted [27]. Current popular approaches try to diversify the result list to account for different search intents or query subtopics [24]. A weakness of this approach is that the query subtopics are hidden from the user, leaving him or her to guess at how the results are organized.
In this work, we attempt to extract query facets from web search results to assist information finding for these queries. We define a query facet as a set of coordinate terms ­ i.e., terms that share a semantic relationship by being grouped under a more general hypernym ("is a" relationship). For example, for the query mars landing, three possible query facets are shown in Table 1.
Table 1: Example query facets Query: mars landing 1. Curiosity, Opportunity, Spirit 2. USA, UK, Soviet Union 3. video, pictures, news Query: baggage allowance 1. Delta, Jetblue, AA, Continental, ... 2. domestic, international 3. first class, business class, economy class 4. weight, size, quantity Query: mr bean 1. comics, movies, tv, books 2. the curse of mr bean, mr bean goes to town, ... 3. rowan atkinson, richard wilson, jean rochefort, ... 4. mr bean, irma gobb, rupert, hubert, ...
The first query facet, {Curiosity, Opportunity, Spirit}, includes different Mars rovers. The second query facet, {USA, UK, Soviet Union}, includes countries relevant to Mars landings. These are both facets where the terms are instances of the same semantic class. Somewhat differently, the last facet, {video, pictures, news}, includes labels for different query subtopics. These labels can be viewed as instances of a special semantic class, the subtopics of the query mars landing.
Query facets can be used to help improve search experience in many ways. Like in faceted search [6], query facets can help users to navigate through different topics of the search results by applying multiple filters. Using the examples in Table 1, for query baggage allowance, a user can select Delta, international, business class, quantity from each of its facets, to find pages discussing the number of bags allowed on Delta's international business class flights. Query facets

93

can also be used as query suggestions or clarification questions to help users specify search intent. For example, for the query mars landing, a system might suggest the query facet {video, pictures, news} or generate clarification questions like "Which Mars rover are you looking for? a) Curiosity, b) Opportunity, c) Spirit". Query facets are also useful for exploratory search since they succinctly summarize interesting facts for the issued query. For example, facets for the query mr bean list episodes titles, characters and casts for the Mr. Bean television series.
In this paper we develop a supervised method based on a graphical model for query facet extraction. The graphical model learns how likely it is that a term should be selected and how likely it is that two terms should be grouped together in a query facet. Further, the model captures the dependencies between the two factors. We propose two algorithms for approximate inference on the graphical model since exact inference is intractable. Also, we design an evaluation metric for query facet extraction, which combines recall and precision of the facet term, with the grouping quality.
The rest of this paper is organized as follows. We discuss related work in Section 2, and then present the problem formulation in Section 3. Section 4 describes the general framework we use for query facet extraction. Section 5 describes our graphical model based approach in detail. Section 6 briefly describes two alternate approaches that we use as baselines. We describe the dataset as well as the metrics we used for evaluation in Section 7, and report experimental results in Section 8. Finally, we conclude the work in Section 9.
2. RELATED WORK
Related work of query facet extraction can be divided into the following topics.
2.1 Search Results Diversification
Search result diversification has been studied as a method of tackling ambiguous or multi-faceted queries while a ranked list of documents remains the primary output feature of Web search engine today[24]. It tries to diversify the ranked list to account for different search intents or query subtopics. A weakness of search result diversification is that the query subtopics are hidden from the user, leaving him or her to guess at how the results are organized. Query facet extraction addresses this problem by explicitly presenting different facets of a queries using groups of coordinate terms.
2.2 Search Results Clustering/Organization
Search results clustering is a technique that tries to organize search results by grouping them into, usually labeled, clusters by query subtopics [4]. It offers a complementary view to the flat ranked list of search results. Most previous work exploited different textual features extracted from the input texts and applied different clustering algorithms with them. Instead of organizing search results in groups, there is also some work [14, 15, 16] that summarizes search results or a collection of documents in a topic hierarchy. For example, Lawrie et al. [14, 15] used a probabilistic model for creating topical hierarchies, in which a graph is constructed based on conditional probabilities of words, and the topic words are found by approximately maximizing the predictive power and coverage of the vocabulary. Our work is different from

these work in that our target is to extract different facets of a query from search results, instead of organizing the search results.
2.3 Query Subtopic/Aspect Mining
To address multi-faceted queries, much previous work studied mining query subtopics (or aspects). A query subtopic is often defined as a distinct information need relevant to the original query. It can be represented as a set of terms that together describe the distinct information need [29, 31, 5] or as a single keyword that succinctly describes the topic [28]. Different resources have been used for mining query subtopics, including query logs [30, 11, 32, 29, 31, 33], document corpus [2] and anchor texts [5].
A query subtopic is different from a query facet in that the terms in a query subtopic are not restricted to be coordinate terms, or have peer relationships. Query facets, however, organize terms by grouping "sibling" terms together. For example, {news, cnn, latest news, mars curiosity news} is a valid query subtopic for the query mars landing, which describes the search intent of Mars landing news, but it is not a valid query facet, given our definition, since the terms in it are not coordinate terms. A valid query facet that describes Mars landing news could be {cnn, abc, fox }, which includes different news channels. In a recent work [7], Dou et al. developed a system to extract query facets from web search results and showed the potential of doing so. However, the unsupervised method they proposed is far from optimal, and it does not improve by having human labels available. Also, to the best of our knowledge, their evaluation can be problematic in some cases, which will be discussed in Section 7.2.3.
2.4 Semantic Class Mining
Semantic class mining can be used to help query facet extraction. Class attribute extraction [17, 18] aims to extract attributes for a target semantic class usually specified by as a set of representative instances. For example, given a semantic class country, together with some instances like USA, UK, China, some class attributes can be capital city, president, population. Those extracted class attributes can be used as query subtopics. However, class attribute extraction targets semantic classes, not general search queries.
Semantic class extraction aims to automatically mine semantic classes represented as their class instances from certain data corpus. Existing approaches can be roughly divided into two categories: distributional similarity and pattern-based [25]. The distributional similarity approach is based on the distributional hypothesis [8], that terms occurring in analogous contexts tend to be similar. Different types contexts has been studied for this problem, including syntactic context [20] and lexical context [21, 1, 19]. The patternbased approach applied textual patterns [9, 22], HTML patterns [26] or both [34, 25] to extract instances of a semantic class from some corpus. The raw semantic class extracted can be noisy. To address this problem, Zhang et al. [34] used topic modeling to refine the extracted semantic classes. Their assumption is that, like documents in the conventional setting, raw semantic classes are generated by a mixture of hidden semantic class. In this paper, we apply pattern-based semantic class extraction on the top-ranked Web documents to extract candidates for finding query facets.

94

2.5 Faceted Search
Faceted search is a technique for accessing information organized according to a faceted classification system, allowing users to digest, analyze and navigate through multidimensional data. It is widely used in e-commerce and digital libraries [6]. Faceted search is similar to query facet extraction in that both of them use sets of coordinate terms to represent different facets of a query. However, most existing works for faceted search are build on as specific domain or predefined categories [7], while query facet extraction does not restrict queries in a specific domain, like products, people, etc.

3. PROBLEM FORMULATION
Query facet extraction is the problem of finding query facets for a given query q from available resources, such as web search results. A query facet F = {t} is a set of coordinate terms, terms that are part of a semantic set, which we call facet terms. These facet terms can be instances of a semantic class, for example Curiosity, Opportunity, Spirit are all Mars rovers. They can be labels for query subtopics, such as video, pictures, news for the query mars landing. We use F = {F } to denote the set of query facets. TF = {t|t  F, F  F } is the set of all the facets terms that appear in F.
Query facets can be extracted from a variety of different resources, such as a query log, anchor text, taxonomy and social folksonomy. In this work, we only focus on extracting query facets from the top k web search results D = {D1, D2, . . . , Dk}. We intend to explore the use of other information sources for this problem in future work.

4. GENERAL FRAMEWORK
In this section, we describe the general framework we use for extracting query facet from web search results. Given a query q, we retrieve the top k search results, D, as input to our system. Then query facets F are extracted by, first, extracting candidates from search results D and then finding query facets from the candidates.
4.1 Extracting candidate lists
Similar to Dou et al. [7], we use pattern-based semantic class extraction approach [25] to extract lists of coordinate terms from search results as candidates for query facets. In pattern-based semantic class extraction, patterns are applied on the corpus to discover specific relationships between terms. For example, the pattern "NP such as NP, NP, ..., and NP " can be used to extract coordinate terms and their hypernyms from text. Besides lexical patterns, HTML patterns are often used on HTML documents to extract coordinate terms from some HTML structures, like <UL>, <SELECT> and <TABLE>.

Table 2: Semantic class extraction patterns

Type Pattern

Lexical item, {,item}, (and|or) {other} item

<select><option>item </option>...</select>

HTML

<ul><li>item </li>...</ul> <ol><li>item </li>...</ol>

<table><tr><td>item <td>...</table>

We use both of the two types of patterns, summarized in

Table 2. In the table, all items in each pattern are extracted as a candidate list. For example, from the text sentence "... Mars rovers such as Curiosity, Opportunity and Spirit", according to the lexical pattern, we will extract a candidate list {Curiosity, Opportunity, Spirit}. For the lexical pattern, we also restrict those items to be siblings in the parse tree of that sentence. We use the PCFG parser [12] implemented in Stanford CoreNLP1 for parsing documents. For HTML tables, following Dou et al. [7], lists from each column and each row are extracted.
After extracting the lists from the top ranked results D, we further process them as follows. First, all the list items are normalized by converting text to lowercase and removing non-alphanumeric characters. Then, we remove stopwords and duplicate items in each lists. Finally, we discard all lists that contain fewer than two item or more than 200 items. After this process, we have a set of candidate lists L = {L}, where each list L = {t} is a set of list items.
4.2 Finding query facets from candidate lists
The candidate lists extracted are usually noisy [34], and could be non-relevant to the issued query, therefore they cannot be used directly as query facets. Table 3 shows four candidate lists extracted for the query mars landing. L1 contains list items that are relevant to mars landing, but they are not coordinate terms. L2 is a valid query facet, but it is incomplete ­ another Mars rover Spirit appears in L3. L3 is extracted from the sentence, "It is bigger than the 400-pound Mars Exploration rovers, Spirit and Opportunity, which landed in 2004 ". The list item "the 400 pound mars exploration rovers" is an extraction error.
Table 3: Four candidate lists for query mars landing L1: curiosity rover, mars, nasa, space L2: curiosity, opportunity L3: the 400 pound mars exploration rovers, spirit, opportunity L4: politics, religion, science technology, sports, ...
Since the candidate lists are frequently noisy, we need an effective way to find query facets from extracted candidate lists. More formally, given a set of candidate lists L = {l}, the task is to find a set of query facets F , where TF  TL. Similar to TF , TL = {t|t  L, L  L} is the set of all list items in L. To address this problem, we develop a graphical model, which learns how likely a list item is a facet term, how likely two list items should be grouped in a query facet, and capture the dependencies between the two factors.
5. A GRAPHICAL MODEL FOR FINDING QUERY FACETS
In this section, we describe the directed graphical model we use to find query facts form noisy candidate lists. A directed graphical model (or Bayesian network) is a graphical model that compactly represents a probability distribution over a set of variables [23]. It consists of two parts: 1) a directed acyclic graph in which each vertex represents a variable, and 2) a set of conditional probability distributions that describe the conditional probabilities of each vertex given its parents in the graph.
We treat the task of finding query facets from candidate lists as a labeling problem, in which we are trying to predict
1http://nlp.stanford.edu/software/corenlp.shtml

95

1) whether a list item is a facet term, and 2) whether a pair of list items is in one query facet. Then, we used a directed graphical model to exploit the dependences that exist between those labels. Similar to conditional random fields [13], we directly model the conditional probability P (y|x), where y is the label we are trying to predict and x is the observed data ­ list items and item pairs. Thus, it avoids modeling the dependencies among the input variables x, and can handle a rich set of features. For our graph model, exact maximum a posteriori inference is intractable; therefore, we approximate the results using two algorithms.
5.1 The Graphical Model

5.1.1 Graph
First we define all the variables in our graphical model. Let Y = {yi}, where yi = 1{ti  TF } is a label indicating whether a list item ti is a facet term. Here 1{·} is an indicator function which takes on a value of 1 if its argument is true, and 0 otherwise. pi,j denotes the list items pair (ti, tj ), and PL = {pi,j |pi,j = (ti, tj ), ti, tj  TL, ti = tj } denotes all the items pairs in TL. Let Z = {zi,j}, where zi,j = 1{F  F , ti  F  tj  F } is a label indicates whether the corresponding item pair pi,j should be grouped together in a query facet. The vertices in our graphical model are V = TL  PL  Y  Z. Note that the list items TL, and item pairs PL are always observed.
As shown in Figure 1, there are three types of edges in the graph: 1) edges from each list item ti to its corresponding labels yi; 2) edges that point to each item pair label zi,j from the two corresponding list items yi and yj; 3) edges from each item pair pi,j to its corresponding label zi,j.

p

p

1,2

1,i

p 1,j

p 2,i

p ... 2,j

...

...

...

...

z

z

z

z

z ...

1,2

1,i

1,j

2,i

2,j

p ... i,j
z ... i,j

y1

y ... 2

yi

...

yj ...

t1

t 2 ...

t i ...

t j ...

Figure 1: A graphical model for candidate list data

5.1.2 Conditional Probability Distribution

We use logistic-based conditional probability distributions (CPDs) for variable yi and zi,j, defined as in Equation 1 and Equation 2.

1

P (yi = 1|ti) = 1 + exp{- k kfk(ti)}

(1)

P (zi,j

=

1|pi,j , yi, yj )

=

yiyj 1 + exp{- k µkgk(pi,j )}

(2)

fk and gk are features that characterize a list item and a item pair respectively.  and µ are the weights associated with fk and gk respectively. Compared to a conventional logistic function, Equation 2 has an extra term, yiyj, in the numerator. When yi = 0 or yj = 0, we have P (zi,j = 1|pi,j, yi, yj) = 0. This means when either of the two list

items is not a facet term, the two items can never appear in a query facet together. When both of the ti and tj are facet terms, P (zi,j = 1|pi,j, yi, yj) becomes a conventional logistic function, which models the probability of ti and tj being grouped together in a query facet, given the condition that both ti and tj are facet term.
The joint conditional probability for the graphical model is calculated as

P (Y, Z|TL, PL) =

P (yi|ti)

P (zi,j |pi,j , yi, yj ) (3)

yi Y

zi,j Z

where the CPDs are defined in Equation 1 and Equation 2.

5.1.3 Parameter Estimation
The training set for the graphical model can be denoted as {TL, PL, Y , Z}, where Y , Z are the ground truth labels for the list items TL and item pairs PL. The conditional probability of the training set can be calculated according to Equation 4.

P (, µ) =

P (Y , Z|TL, PL)

(4)

TL ,PL

The log-likelihood l(, µ), can be calculated as follows,

l(, µ) = lt() + lp(µ)

(5)

lt() =

log P (yi|ti) -

k 2k 22

(6)

TL yiY

lp(µ) =

log P (zi,j |pi,j , yi, yj ) -

k µ2k 22

(7)

TL zi,j Z

l(, µ) is separated into two parts, lt() and lp(µ). The last terms of Equation 6 and Equation 7 are served as regularizers which penalize large values of , µ.  and  are regularization parameters that control the strength of penalty. Notice that, in the train set, for those item pairs pi,j with any of its list item not being a facet term, their labels zi,j = 0. According to Equation 2, for those item pairs, log P (zi,j|pi,j, yi, yj) = 0, which makes no contribution to lp(µ), and thus lp(µ) can be simplified as

lp(µ) =

log P (zi,j |pi,j , yi, yj ) -

k µ2k 22

(8)

TL zi,j Z

where Z is a subset of Z, which contains only the labels for item pairs with both of its list items being facet terms.
We can see that Equations 6 and 8 are exactly the same as log-likelihoods for two separated logistic regressions. In fact, Equation 6 learns a logistic regression model for whether a list item is a facet term, and Equation 8 learns a logistic regression model for whether two facet terms should be grouped together. The parameter  and µ can be learned by maximizing the log-likelihood using gradient descent, exactly same as in logistic regression.

5.2 Inference
When given a new labeling task, we could perform maximum a posteriori inference - compute the most likely labels Y , Z by maximizing the joint conditional probability P (Y, Z|TL, PL). After that, the query facet set F can be easily induced from the labeling Y , Z. (Collect list items with yi = 1 as facet terms, and group any two of them into

96

a query facet if the corresponding zi,j = 1.) Note that the graphical model we designed does not enforce the labeling to produce strict partitioning for facet terms. For example, when Z1,2 = 1, Z2,3 = 1, we may have Z1,3 = 0. Therefore, an optimal labeling results may induce an overlapping clustering. To simplify the problem, we add the strict partitioning constraint that each facet term belongs to exactly one query facet. Also, to directly produce the query facets, instead of inducing them after predicting labels, we rephrase the optimization problem as follows. First, we use the following notations for log-likelihoods,

st(ti) = log P (yi = 1|ti) st(ti) = log (1 - P (yi = 1|ti)) sp(ti, tj ) = log P (pi,j = 1|pi,j , yi = 1, yj = 1) sp(ti, tj ) = log (1 - P (pi,j = 1|pi,j , yi = 1, yj = 1))
Using the notations above, the log-likelihood l(F) for a particular query facet set F formed from L can be written as

l(F ) = lt(F ) + lp(F )

lt(F ) =

st(ti) +

st(ti)

tTF

tTF

lp(F ) =

sp(ti, tj ) +

sp(ti, tj ) (9)

F F ti,tj F

F,F tiF, F tj F

In the right hand side of Equation 9, the first term is the intra-facet score, which sums up sp(·, ·) for all the item pairs in each query facet. The second term is the inter-facet score, which sums up the sp(·, ·) for each item pair that appears in different query facets. Then the optimization target becomes F = arg maxFF l(F ), where F is the set of all possible query facet sets that can be generated from L with the strict partitioning constraint.
This optimization problem is NP-hard, which can be proved by a reduction from the Multiway Cut problem [3]. Therefore, we propose two algorithms, QF-I and QF-J, to approximate the results.

5.2.1 QF-I
QF-I approximates the results by predicting whether a list item is a facet term and whether two list items should be grouped in a query facet independently, which is accomplished two phases. In the first phase, QF-I selects a set of list items as facet terms according to P (yi|ti). In this way, the algorithm predicts whether a list item ti is a facet term independently, ignoring the dependences between yi and its connected variables in Z. In our implementation, we simply select list items ti with P (ti) > wmin as facet terms. (For convenience, we use P (ti) to denote P (yi = 1|ti).) In the second phase, the algorithm clusters the facet terms TF selected in the first phase into query facets, according to P (ti, tj). (P (ti, tj) is used to denote P (zi,j = 1|pi,j, yi = 1, yj = 1)). Many clustering algorithm can be applied here, using P (ti, tj) as the distance measure. For our implementation, we use a cluster algorithm based on WQT [7], because it considers the importance of nodes while clustering. We use P (ti) as the measure for facet term importance, and dt(ti, tj) = 1 - P (ti, tj) as the distance measure for facet terms. The distance between a cluster and a facet term is computed using complete linkage distance, df (F, t) = maxt F d(t, t ), and the diameter of a

cluster can be calculated as dia(F ) = maxti,tjF dt(ti, tj ). The algorithm is summarized in Algorithm 1. It processes the facet terms in decreasing order of P (t). For each facet term remaining in the pool, it builds a cluster by iteratively including the facet term that is closest to the cluster, until the diameter of the cluster surpasses the threshold dmax.
Algorithm 1 WQT for clustering facet term used in QF-I
Input: TF , P (t), df (F, t), dia(F ), dmax Output: F = {F } 1: Tpool  F 2: repeat 3: t  arg maxtTpool P (t) 4: F  {t} 5: iteratively include facet term t  Tpool that is closest
to F , according to df (F, t ), until the diameter of the cluster, dia(F ), surpasses the threshold dmax. 6: F  F  {F }, Tpool  Tpool - F 7: until Tpool is empty 8: return F
5.2.2 QF-J
QF-I finds query facets based on the graphical model by performing inference of yi and zi,j independently. The second algorithm, QF-J, instead tries to perform joint inference by approximately maximizing our target l(F) with respect to yi and zi,j iteratively. The algorithm first guesses a set of list items as facet terms. Then it clusters those facet terms by approximately maximizing lp(F ), using a greedy approach. After clustering, the algorithm checks whether each facet term "fits" in its cluster, and removes those that do not fit. Using the remaining facet terms, the algorithm repeats the process (clustering and removing outliers) until convergence.
QF-J is outlined in Algorithm 2. The input to the algorithm are the candidate list item set TL, and the loglikelihoods l(F ), lp(F ). In the first step, we select top n list items according to st(t) as the initial facet terms, because it is less sensitive to the absolute value of the log-likelihood. In our experiment, n is set to 1000 to make sure most of the correct facet terms are included. Then, the algorithm improves l(F) by iteratively performing functions Cluster and RemoveOutliers. Cluster performs clustering over a given set of facet terms. In step 10 to 12, it puts each facet terms into a query facet by greedily choosing the best facet, or creates a singleton for the list item, according to the resulting log-likelihood, lp(F ). We choose to process these list items in decreasing order of st(t), because it is more likely to form a good query facet in the beginning by doing so. RemoveOutliers removes facet terms according to the joint log-likelihood l(F). In step 20 to 22, it checks each facet term to see if it fits in the facet, and removes outliers. F is the set of facet terms the algorithm selected when processing each facet F .
5.2.3 Ranking Query Facets
The output of QF-I and QF-J is a query facet set F. To produce ranking results, we defined a score for a query facet as scoreF (F ) = tF P (t), and rank the query facets according to this scoring, in order to present more facet terms in the top. Facet terms within a query facet are ranked according to scoret(t) = P (t).

97

Algorithm 2 QF-J

Input: TL = {t}, l, lp

Output: F = {F }

1: TF  top n list items from TL according to st(·)

2: repeat

3: F  Cluster(TF , lp)

4: TF  RemoveOutliers(F , l)

5: until converge

6: return F

7:

8: function cluster(TF , lp)

9: F  

10: for each t  TF in decreasing order of st(t) do

11:

Choose to put t into the best facet in F or add

t as a singleton into F, whichever that has the highest

resulting lp(F ).

12: end for

13: return F

14: end function

15:

16: function RemoveOutliers(F, l)

17: TF  all facet terms in F

18: for each F  F do

19:

F =

20:

for each t  F in decreasing order of st(·) do

21:

choose to add t into F or not, whichever has

the highest resulting l({F })

22:

if not, TF  F - {t}

23:

end for

24: end for

25: return TF

26: end function

27: return F

5.3 Features

There are two types of features used in our graphical

model, summarized in Table 4.

Item features, fk(t) in the graphical model, character-

ize a single list item. To capture the relevance of item t

to the query, we use some TF/IDF-based features extracted

from the top k search results, D. For example, snippetDF

is the number of snippets in top k search results that con-

tain item t. snippetDF and other frequency-based features

are normalized using log(f requency + 1). To capture how

likely item t is to be an instance of a semantic class, we

use features extracted from candidate lists. For example,

listTF is the frequency of t in the candidate lists extracted

from D. Some list items occur frequently in candidate lists

across different queries, such as home, contact us and pri-

vacy policy. They are treated as stopwords, and removed

from the candidate lists. We also use listIDF to cope with

this problem. listIDF is the IDF of a list item in a general

collection of candidate lists we extracted (see Section 7.1).

It

is

calculated

as

listIDF (t)

=

log

N -Nt+0.5 Nt +0.5

,

where

N

is

the total number of lists in the collection, Nt is the number

of lists contain t. The same form is used for clueIDF, IDF

in ClueWeb092 collection.

Item Pair Features, g(pi,j) in the graphical model, are

used to capture how likely a pair of list items should be

grouped into a query facet, given that the two list item both

2http://lemurproject.org/clueweb09

Table 4: Two types of features

Item Features for list item t

length

Number of words in t

clueIDF

IDF of t in ClueWeb09 collection

TF

Term frequency of t in D

DF

Document frequency of t in D

wDF

Weighted DF. Each document count weighted by 1/ docRank

SF

Site frequency. Number of unique

websites in D that contain t

titleTF

TF of t for the titles of D

titleDF

DF of t for the titles of D

titleSF

SF of t for the titles of D

snippetTF

TF of t for the snippets of D

snippetDF

DF of t for the snippets of D

snippetSF

SF of t for the snippets of D

listTF

Frequency of t in candidate lists

extracted from D

listDF

Number of documents that contain t

in their candidate lists

listSF

Number of unique websites that

contain t in their candidate lists

listIDF

IDF of t in a general candidate list

collection

TF.clueIDF

TF × clueIDF

listTF.listIDF listTF × lisIDF

Item Pair Features for item pair pi,j = (ti, tj)

lengthDiff listCooccur

Length difference, |len(ti) - len(tj)| Number of candidate lists extracted

from D, in which ti, tj co-occur textContextSim Similarity between text contexts

listContextSim Similarity between list contexts

are facet terms. This can be measured by context similarity [25]. For textContextSim, we use window size 25, and represent text context as a vector of TF weights. Cosine similarity is used as the similarity measure. Similarly, we use the candidate lists that contain the list item as its list context, and calculate listContextSim in the same way as textContextSim.
6. OTHER APPROACHES
In this section, we describe two alternative approaches for finding query facets from candidate lists. They are used as baselines in our experiments.
6.1 QDMinder
Dou et al. [7] developed QDMiner/QDM for query facet extraction, which appears to be the first work that addressed the problem of query facet extraction. To solve the problem of finding query facets from the noisy candidate lists extracted, they used an unsupervised clustering approach. It first scores each candidate list by combining some TF/IDFbased scores. The candidate lists are then clustered with bias toward important candidate lists, using a variation of the Quality Threshold clustering algorithm [10]. After clustering, clusters are ranked and list items in each clusters are ranked/selected based on some heuristics. Finally, the top k clusters are returned as results. This unsupervised approach does not gain by having human labels available. Also, by clustering lists, they lose the flexibility of breaking a candidate list into different query facets.

98

6.2 Topic modeling
In semantic class extraction, Zhang et al. [34] proposed to use topic models to find high-quality semantic classes from a large collection of extracted candidate lists. Their assumption is, like documents in the conventional setting, candidate lists are generated by a mixture of hidden topics, which are the query facets in our case. pLSA and LDA are used in their experiments. We find this approach can be directly used for finding query facets from candidate lists. The major change we need to make is that: in semantic class extraction, topic modeling is applied globally on the candidate lists (or a sample of them) from the entire corpus; in query facet extraction, we apply topic modeling only on the top k search results D, assuming the coordinate terms in D are relevant to the query. Then, the topics are returned as query facets, by using the top n list items in each topic (according to the list item's probability in the topic). Though this topic modeling approach is more theoretically motivated, it does not have the flexibility of adding different features to capture different aspects such as query relevance.

7. EVALUATION
7.1 Data
Queries. We constructed a pool of 232 queries from different sources, including random samples from a query log, TREC 2009 Web Track queries 3, example queries appearing in related publications [32, 29] and queries generated by our annotators. Annotators were asked to select queries that they are familiar with from the pool for annotating. Overall, we collect annotations for 100 queries (see Table 5).

Table 5: Query statistics

Source #queries #queries

collected annotated

query log

100

30

related publications

20

10

TREC 2009 Web Track

50

20

annotators generated

62

40

sum

232

100

Search results. For each query, we acquire the top 100 search results from a commercial Web search engine. A few search results are skipped due to crawl errors, or if they are not HTML Web pages. For the 232-query set, we crawled 22,909 Web pages, which are used for extracting feature listIDF described in Section 5.3. For the 100 annotated queries, the average number of crawled Web pages is 98.7, the minimum is 79, both the maximum and the median are 100.
Query facet annotations. We asked human annotators to construct query facets as ground truth. For each query, we first constructed a pool of terms by aggregating facet terms in the top 10 query facets generated by different models, including two runs from QDM, one run from each of pLSA and LDA using top 10 list items in each query facets, and one run for our graphical model based approach. Then, annotators were asked to group terms in the pool into query facets for each query they selected. Finally, the annotator was asked to give a rating for each constructed query facet,
3http://trec.nist.gov/data/web/09/wt09.topics.queriesonly

regarding how useful and important the query facet is. The rating scale of good=2/fair=1 is used. Annotation statistics are given in Table 6. There are 50 query facets pooled per query, with 224.8 distinct facet terms per query.

Table 6: Annotation statistics fair good pooled

#terms per query 26.6 55.8 224.8

#facets per query 3.1 4.8 50.0

#terms per facet 8.6 11.6

8.8

7.2 Evaluation Metrics
Query facet extraction can be evaluated from different aspects. We use standard clustering and classification evaluation metrics, as well as metrics designed for this particular task to combine different evaluation aspects.
Notation: we use "" to distinguish between system generated results and human labeled results, which we used as ground truth. For example, F denotes the system generated query facet set, and F  denotes the human labeled query facet set. For convenience, we use T to denote TF in this section, omitting subscript F . T  denotes all the facet terms in human labeled query facet set. We use rF  to denote the rating score for a human labeled facet F .

7.2.1 Effectiveness in finding facet terms

One aspect of query facet extraction evaluation is how well a system finds facet terms. This can be evaluated using standard classification metrics as follows,

·

facet

term

precision:

P (T, T ) =

|T T | |T |

·

facet

term

recall:

R(T, T ) =

|T T | |T |

·

facet

term

F1:

F T (T, T ) =

2|T T | |T |+|T |

where facet term F1 is denoted as FT (the T stands for facet term) to avoid confusion with a query facet F and clustering F1 defined below. These metrics do not take clustering quality into account.

7.2.2 Clustering quality
To evaluate how well a system groups facet terms correctly, similar to Dou et al. [7], we use several existing cluster metrics, namely, Purity, NMI/Normalized Mutual Information and F1 for clustering. To avoid confusion with facet term F1, FT, we call F1 for facet term clustering facet clustering F1, and denote it as FP (with P standing for term pair ).
In our task, we usually have T = T . The facet terms in the system generated and human labeled clustering results might be different: the system might fail to include some human identified facet terms, or it might mistakenly include some "incorrect" facet terms. These standard clustering metrics cannot handle these cases properly. To solve this problem, we adjust F as if only facet terms in T  were clustered by the system, since we are only interested in how well the "correct" facet terms are clustered from these metrics. The adjusting is done by removing "incorrect" facet terms (t  T - T ) from F , and adding each missing facet term (t  T  - T ) to F as singletons. By this adjusting, we do not take into account the effectiveness of finding correct facet terms.

99

7.2.3 Overall quality

To evaluate the overall quality of query facet extraction,

Dou et al. [7] proposed variations of nDCG (Normalized Dis-

counted Cumulative Gain), namely fp-nDCG and rp-nDCG.

It first maps each system generated facet F to a human la-

beled facet F  that covers the maximum number of terms

in F . Then, it assigns the rating rF  to F , and evalu-

ates F as a ranked list of query facets using nDCG. The

discounted gains are weighted by precision and/or recall of

facet terms in F , against its mapped human labeled facet

F .

For

fp-nDCG,

only

precision

are

used

as

weight,

|F

 F |F |

|

.

For rp-nDCG, precision and recall are multiplied as weight,

. |F F |2
|F ||F |

However, to the best of our understanding, this

metric can be problematic in some cases. When two facets

F1 and F2 are mapped to a same human labeled facet F ,

only the first facet F1 is credited and F2 is simply ignored, even if it is more appropriate to map F2 to F  (e.g. F2 is exactly same as F , while F1 contain only one facet term in F ).

The quality of query facet extraction is intrinsically multi-

faceted. Different applications might have different empha-

sis in the three factors mentioned above - precision of facet

terms, recall of facet terms and clustering quality of facet

terms. We propose a metric P RF, to combine the three

factors together, using weighted harmonic mean. Let p = P (T, T ), r = R(T, T ), f = F P (F , F ), then P RF, can

be expressed as follows,

P RF, (F , F )

=

(2 + 2 + 1)prf 2rf + 2pf + pr

(10)

where  and  are used to adjust the emphasis between the
three factors. When  =  = 1, we omit the subscript part
for simplicity, i.e. P RF  P RF1,1. While P RF, has the flexibility to adjust emphasis be-
tween the three factors, it does not take into account the dif-
ferent ratings associated with query facets. To incorporate ratings, we use a weighted version of P (T, T ), R(T, T ) and F P (F , F ) in P RF,. We call the new metric wP RF,. The weighted facet term precision, recall and FT are defined
as follows

· weighted facet term precision: wP (T, T ) =

tT T  w(t) tT w(t)

· weighted facet term recall: wR(T, T ) =

tT T  w(t) tT  w(t)

·

weighted facet term F1:

wF T (T, T ) =

2wP (T,T )wR(T,T ) wP (T,T )+wR(T,T )

where w(t) is the weight for facet term t, and assigned as

follows

w(t) =

rF  if t  T  1 otherwise

Similarly, wF P (F , F ) is computed by weighting its pairwise precision and recall in the same fashion as the weighted facet term precision and recall above. Instead of w(t), we need weight for a pair of facet terms w(t1, t2) in this calculation. We assign weight for facet term pair w(t1, t2) using their sum, w(t1) + w(t2).

8. EXPERIMENT RESULTS

8.1 Experiment settings
We compare effectiveness of the five models, QDM, pLSA, LDA and QF-I, QF-J, on the 100-query data set. All the

models take the same candidate lists extracted/cleaned (see Section 4.1) as input. We perform 10-fold cross validation for training/testing and parameter tuning in all experiments and for all models (if applicable). When training the graphical model, we standardize features by removing the mean and scaling to unit variance. We set both of the two regularizers  and  in Equation 5 to be 1. There are too many negative instances (yi = 0, zi,j = 0) in the training data, so we stratify samples by labels with the ratio of positive:negative to be 1:3. For QDM, we tune the two parameters used in the clustering algorithm Diamax (the diameter threshold for a cluster) and Wmin (the weight threshold for a valid cluster), as well as two parameters used for selecting facet terms in each facet (St|F > |Sites(F )| and St|F > ). For pLSA and LDA, we tune the number of facet terms in a query facet. For QF-I, we tune the weight threshold for facet terms, wmin, and the diameter threshold, dmax. For QF-J, there are no parameter need to be tuned. We returned top 10 query facets from all the five models in all evaluation.
8.2 Finding Facet terms
To evaluate effectiveness in finding facet terms, we tune all the models on wFT, which combines both precision and recall, and takes into account facet term weighting.

Table 7: Facet term precision, recall and F1 tuned

on wFT

Model P

wP

R

wR FT wFT

pLSA 0.284 0.385 0.562 0.561 0.351 0.430

LDA 0.292 0.394 0.595 0.593 0.364 0.446

QDM 0.407 0.523 0.378 0.388 0.360 0.420

QF-I 0.347 0.458 0.644 0.652 0.427 0.514

QF-J 0.426 0.534 0.525 0.526 0.449 0.511

Table 8: Average number of facet terms in output

per query for different models

Model #terms/query

pLSA

153.1

LDA

154.8

QDM

68.0

QF-I

153.7

QF-J

97.8

Table 7 shows facet term precision, recall and F1 and their weighted version described in Section 7.2. QF-I and QF-J perform relatively well for both precision and recall. Their improvements over the other three models shown are all significant (p < 0.05, using paired t-test), except the improvements of QF-J over QDM for P and wP. The two topic model based approaches, pLSA and LDA, have relatively high recall and low precision. Contrarily, QDM has high precision and low recall. This difference can be explain by Table 8, which gives the number of facet terms output per query from each models. QDM only outputs 68 facet terms per query, while pLSA and LDA both output over twice that number. One possible reason for the low precision of pLSA and LDA is that they select facet terms solely according to term probabilities in the learned topics (query facets in our case) and do not explicitly incorporate query relevance. We find most of their facet terms are frequently-occurring list items, which are not necessary relevant to the query.

100

While the number of facet terms QF-I outputs is similar to pLSA and LDA, QF-I obtain much higher precision and recall, likely due to the rich set of features used. Table 9 shows the five most important item features according to the absolute values of learned weights. Not surprisingly, list TF/IDF features which are used to capture the likelihood of being a coordinate term have relatively high weights, as well as some features that are used to capture query relevance, e.g. T F.clueIDF .

Table 9: Top 5 item features, ranked by absolute

weights

Feature

Weight

listTF.listIDF 2.6424

listSF

2.1374

wDF

-1.0754

TF.clueIDF 1.0115

SF

0.6873

From Table 7, we also find the the weighted metrics are usually consistent with their corresponding unweighted metric. One exception is that QF-J performs better than QF-I in FT, but it does slightly worse than QF-J in wFT. This is likely to be caused by the high recall for QF-I, which may include more highly rated facet terms.
8.3 Clustering Facet terms
Table 10 shows clustering performance of the five models, which are tuned on wFP. The improvements of QF-I and QF-J over the other three models shown are all significant (p < 0.05, using paired t-test). pLSA and LDA do not perform well in clustering, which could be caused by data sparsity. There are on average 5159 candidate lists per query, but only 3.9 items per list.

Table 10: Facet clustering performance tuned on wFP
Model Purity NMI FP wFP
pLSA 0.793 0.524 0.230 0.229 LDA 0.773 0.511 0.227 0.226 QDM 0.871 0.565 0.367 0.380 QF-I 0.843 0.606 0.408 0.410 QF-J 0.922 0.631 0.352 0.346

Table 11: Weights learned for item pair features

Feature

Weight

listContextSim 1.4944

textContextSim 0.7186

listCooccur

0.0817

lengthDiff

0.0563

The better performance in clustering for QF-I and QF-J can be explained by their incorporating factors other than list item co-occurrence information. In Table 11, we list the weights learned for item pair features. Besides one item co-occurrence related feature, listContextSim, we also find that textContextSim has a relatively high weight. textContextSim is used to capture the similarity of the two list items using their surrounding text, so it can help to group two facet terms together even if they might not co-occur a lot

in candidate lists. As an example, for the query baggage allowance, we find different airlines do not co-occur a lot in candidate lists, (e.g. delta and jetblue only co-occur twice), but they tend to have high textContextSim (e.g. textContextSim(delta, jetblue) = 0.81), and are therefore grouped together by QF-I and QF-J.
8.4 Overall Evaluation
To compare overall effectiveness of the five models, we tune all the models on wPRF, and the results are reported in Table 12.

Table 12: Overall performance tuned on wPRF Model wP wR wFP wPRF pLSA 0.353 0.630 0.229 0.309 LDA 0.358 0.670 0.225 0.311 QDM 0.523 0.388 0.253 0.319 QF-I 0.450 0.667 0.399 0.444 QF-J 0.534 0.526 0.346 0.417
Unweighted metrics are very similar to their corresponding weighted metrics in terms of conclusions, and are omitted due to space limitation. Results here are consistent with the results that were tuned on wFT or wFP. pLSA and LDA have high recall, but low precision and FP. QDM has relatively high precision, but low recall and FP. It has on average 68 facet terms per query as output, and fails to improve the overall effectiveness when including more facet terms in its output. QF-I and QF-J are among the best two models according to both PRF and wPRF.
Since wPRF does not account for facet ranking effectiveness, we also report fp-NDCG and rp-NDCG tuned on themselves in Table 13. QF-J gives the best performance for both fp-NDCG and rp-NDCG. The improvements of QF-I and QF-J over the other three models shown in the Table 12 and 13 are all significant (p < 0.05, using paired t-test), except the improvements of QF-J over QDM for wP and QF-I over QDM for fp-nDCG.

Table 13: fp-nDCG and rp-nDCG tuned on them-

selves

Model fp-nDCG rp-nDCG

pLSA 0.250

0.071

LDA 0.238

0.063

QDM 0.257

0.093

QF-I 0.290

0.157

QF-J 0.336

0.193

9. CONCLUSIONS
In this paper, we studied the problem of extracting query facets from search results. We developed a supervised method based on a graphical model to recognize query facets from the noisy facet candidate lists extracted from the top ranked search results. We proposed two algorithms for approximate inference on the graphical model. We designed a new evaluation metric for this task to combine recall and precision of facet terms with grouping quality. Experimental results showed that the supervised method significantly outperforms other unsupervised methods, suggesting that query facet extraction can be effectively learned.

101

10. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval and in part under subcontract #19-000208 from SRI International, prime contractor to DARPA contract #HR0011-12-C-0016. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
11. REFERENCES
[1] E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pa¸sca, and A. Soroa. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of NAACL-HLT'09, pages 19­27, 2009.
[2] J. Allan and H. Raghavan. Using part-of-speech patterns to reduce query ambiguity. In Proceedings of SIGIR'02, pages 307­314, 2002.
[3] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In MACHINE LEARNING, pages 238­247, 2002.
[4] C. Carpineto, S. Osin´ski, G. Romano, and D. Weiss. A survey of web clustering engines. ACM Comput. Surv., 41(3):17:1­17:38, July 2009.
[5] V. Dang, X. Xue, and W. B. Croft. Inferring query aspects from reformulations using clustering. In Proceedings of CIKM '11, pages 2117­2120, 2011.
[6] D. Dash, J. Rao, N. Megiddo, A. Ailamaki, and G. Lohman. Dynamic faceted search for discovery-driven analysis. In Proceedings of CIKM '08, pages 3­12, 2008.
[7] Z. Dou, S. Hu, Y. Luo, R. Song, and J.-R. Wen. Finding dimensions for queries. In Proceedings of CIKM '11, pages 1311­1320, 2011.
[8] Z. Harris. Distributional structure. The Philosophy of Linguistics, 1985.
[9] M. A. Hearst. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING '92, pages 539­545, 1992.
[10] L. Heyer, S. Kruglyak, and S. Yooseph. Exploring expression data: identification and analysis of coexpressed genes. Genome research, 9(11):1106­1115, 1999.
[11] Y. Hu, Y. Qian, H. Li, D. Jiang, J. Pei, and Q. Zheng. Mining query subtopics from search log data. In Proceedings of SIGIR '12, pages 305­314, 2012.
[12] D. Klein and C. D. Manning. Accurate unlexicalized parsing. In Proceedings of ACL '03, pages 423­430, 2003.
[13] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML '01, pages 282­289, 2001.
[14] D. Lawrie, W. B. Croft, and A. Rosenberg. Finding topic words for hierarchical summarization. In Proceedings of SIGIR '01, pages 349­357, 2001.
[15] D. J. Lawrie and W. B. Croft. Generating hierarchical summaries for web searches. In Proceedings of SIGIR '03, pages 457­458, 2003.
[16] C. G. Nevill-manning, I. H. Witten, and G. W. Paynter. Lexically-generated subject hierarchies for

browsing large collections. International Journal on Digital Libraries, 2:111­123, 1999. [17] M. Pa¸sca. Organizing and searching the world wide web of facts ­ step two: harnessing the wisdom of the crowds. In Proceedings of WWW '07, pages 101­110, 2007. [18] M. Pa¸sca and E. Alfonseca. Web-derived resources for web information retrieval: from conceptual hierarchies to attribute hierarchies. In Proceedings of SIGIR '09, pages 596­603, 2009. [19] P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu, and V. Vyas. Web-scale distributional similarity and entity set expansion. In Proceedings of EMNLP '09, pages 938­947, 2009. [20] P. Pantel and D. Lin. Discovering word senses from text. In Proceedings of KDD '02, pages 613­619, 2002. [21] P. Pantel, D. Ravichandran, and E. Hovy. Towards terascale knowledge acquisition. In Proceedings of COLING '04, 2004. [22] M. Pasca. Acquisition of categorized named entities for web search. In Proceedings of CIKM '04, pages 137­145, 2004. [23] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. 1988. [24] T. Sakai and R. Song. Evaluating diversified search results using per-intent graded relevance. In Proceedings of SIGIR '11, pages 1043­1052. ACM, 2011. [25] S. Shi, H. Zhang, X. Yuan, and J.-R. Wen. Corpus-based semantic class mining: distributional vs. pattern-based approaches. In Proceedings of COLING '10, pages 993­1001, 2010. [26] K. Shinzato and K. Torisawa. Acquisition of categorized named entities for web search. In Proceedings of RANLP '05. [27] C. Silverstein, H. Marais, M. Henzinger, and M. Moricz. Analysis of a very large web search engine query log. SIGIR Forum, 33(1):6­12, Sept. 1999. [28] R. Song, M. Zhang, T. Sakai, M. Kato, Y. Liu, M. Sugimoto, Q. Wang, and N. Orii. Overview of the ntcir-9 intent task. In Proceedings of NTCIR-9 Workshop Meeting, pages 82­105, 2011. [29] X. Wang, D. Chakrabarti, and K. Punera. Mining broad latent query aspects from search sessions. In Proceedings of KDD '09, pages 867­876, 2009. [30] X. Wang and C. Zhai. Learn from web search logs to organize search results. In Proceedings of SIGIR '07, pages 87­94, 2007. [31] F. Wu, J. Madhavan, and A. Halevy. Identifying aspects for web-search queries. J. Artif. Int. Res., 40(1):677­700, Jan. 2011. [32] X. Xue and X. Yin. Topic modeling for named entity queries. In Proceedings of CIKM '11, pages 2009­2012, New York, NY, USA, 2011. ACM. [33] X. Yin and S. Shah. Building taxonomy of web search intents for name entity queries. In Proceedings of WWW '10, pages 1001­1010, 2010. [34] H. Zhang, M. Zhu, S. Shi, and J.-R. Wen. Employing topic models for pattern-based semantic class discovery. In Proceedings of ACL '09, pages 459­467, 2009.

102

Relating Retrievability, Performance and Length
Colin Wilkie and Leif Azzopardi
School of Computing Science, University of Glasgow
Glasgow, United Kingdom
{Colin.Wilkie,Leif.Azzopardi}@glasgow.ac.uk

ABSTRACT
Retrievability provides a different way to evaluate an Information Retrieval (IR) system as it focuses on how easily documents can be found. It is intrinsically related to retrieval performance because a document needs to be retrieved before it can be judged relevant. In this paper, we undertake an empirical investigation into the relationship between the retrievability of documents, the retrieval bias imposed by a retrieval system, and the retrieval performance, across different amounts of document length normalization. To this end, two standard IR models are used on three TREC test collections to show that there is a useful and practical link between retrievability and performance. Our findings show that minimizing the bias across the document collection leads to good performance (though not the best performance possible). We also show that past a certain amount of document length normalization the retrieval bias increases, and the retrieval performance significantly and rapidly decreases. These findings suggest that the relationship between retrievability and effectiveness may offer a way to automatically tune systems.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Performance Evaluation
Terms Theory, Experimentation Keywords Retrievability, Simulation
1. INTRODUCTION
Traditionally IR systems are evaluated in terms of efficiency and performance [13]. However IR systems can also be evaluated in terms of retrievability [3], which assesses how often documents are retrieved (as opposed to the speed of retrieval and the quality of retrieval respectively). Retrievability is fundamental to IR because it precedes relevancy [3]. In this paper, we investigate the relationship between retrievability and retrieval performance in the context of ad-hoc topic retrieval for two standard best match retrieval models. To this end, we shall first formally define retrievability be-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

fore discussing how it relates to performance. Next, we perform an empirical analysis where we hypothesize that lower retrievability bias will lead to better retrieval performance.

2. BACKGROUND
In [3], retrievability was defined as a measure that provides an indication of how easily a document could be retrieved under a particular configuration of an IR system. Put formally the retrievability r(d) of a document d with respect to an IR system is:

r(d)  f (kdq, {c, g})

(1)

qQ

where q is a query in a very large query set Q, kdq is the rank at which d is retrieved given q, and f (kdq, c) is an access function which denotes how retrievable the document d is for the query q given the rank cutoff c. This implies that retrievability of a document is obtained by summing over all possible queries Q. The more queries that retrieve the document, the more retrievable the document is.
A simple measure of retrievability is a cumulative-based approach, which employs an access function f (kdq, c), such that if d is retrieved in the top c documents given q, then f (kdq, c) = 1 else f (kdq, c) = 0. Essentially, this measure provides an intuitive value for each document as it is simply the number of times that the document is retrieved in the top c documents. A gravity based approach instead weights the rank at which the document is retrieved, as follows: f (kdq, g) = 1/kdqg. This introduces a discount g, such that documents lower down the list are assumed to be less retrievable. Retrievability Bias: To quantify the retrievability bias across the collection, the Gini coefficient [8] is often used [3, 5, 6] as it measures the inequality within a population. For example, if all documents were equally retrievable according to r(d), then the Gini coefficient would be zero (denoting equality), but if all but one document had r(d) = 0 then the Gini coefficient would be one (denoting total inequality). Usually, most documents have some level of retrievability and the Gini coefficient is somewhere between zero and one. Essentially, the Gini coefficient provides an indication of the level of inequality between documents given how easily they can be retrieved using a particular retrieval system and configuration. Uses: Retrievability - and the theory of - has been used in numerous contexts, for example, in the creation of reverted indexes that improve the efficiency and performance of retrieval systems by capitalizing on knowing what terms

937

within a document makes that document more retrievable [10]. Retrievability has also been used to study search engine and retrieval system bias on the web [2] and within patent collections [4] to improve the efficiency of systems when pruning [14]. It has also been related to navigability when tagging information to improve how easily users browsing through the collection could find documents [12]. While these show that retrievability is useful in a number of respects, here, we are interested in how retrievability relates to performance. There have been a few works exploring this research direction in different ways [3, 1, 5, 6]. Relating Retrievability and Performance: In [1], Azzopardi discusses the relationship with respect to the definition of retrievability and claim that a purely random IR system would ensure equal retrievability (resulting in a Gini = 0). However, the author argues that this would also result in very poor retrieval performance. Conversely, if an oracle IR system retrieved only the set of known relevant documents, and only these documents, then there would be a very high inequality in terms of retrievability across the collection. They suggest that neither extreme is desirable and instead suggest that there is likely to be a trade-off between retrievability and retrieval performance. In [3], it is acknowledged that some level of bias is necessary because a retrieval system must try and discriminate relevant from non-relevant. The preliminary study conducted in [1] investigating this relationship on two small TREC test collections showed that as retrieval performance increased, the retrievability bias tended to decrease: suggesting a positive and useful relationship.
In [5], Bashir and Rauber studied the effect of Pseudo Relevance Feedback (PRF) on performance and retrievability. They found that standard Query Expansion methods, while increasing performance, also increased the retrievability bias. To combat the increase in bias, they devised a method of PRF that used clustering; this resulted in a reduction in bias, as well as an increase in performance over other QE techniques. When employing their PRF technique to patent retrieval, they showed that the decrease in bias led to improved recall for prior art search [6]. This later work suggested that there may be a positive correlation between recall and retrievability. In [7], they compared a number of retrieval models in terms of retrievability bias and performance as a way to rank different retrieval systems. They found that there was a positive correlation between the bias and performance for standard best match retrieval models. It is important to note that since retrievability can be estimated without recourse to relevance judgements it provides an attractive alternative for evaluation.
3. EXPERIMENTAL METHOD
Previous work suggests that the relationship between retrievability bias and retrieval performance is much more complicated than previously thought and the relationship is somewhat unclear. In this paper, we shall perform a more detailed analysis across larger TREC collections and explicitly plot the relationship between the retrievability bias and retrieval performance in order to form a deeper understanding of how they relate to each other.
To this end, three TREC test collections were used AP, Aquaint (AQ) and DotGov (DG) along with their corresponding ad-hoc retrieval topics (see Table 1 for details). These collections had stop words removed and were Porter

stemmed. For the purposes of this study, we will focus on the effect of document length normalization within two standard retrieval models (Okapi BM25 and DFRs PL2). This is due to the fact that retrieval models will often favour longer documents which introduces an undesirable retrieval bias that if accounted for, has been shown to improve performance [11]. For each of these retrieval models we shall investigate how retrievability bias and performance changes as we manipulate the normalization parameter b and c respectively. In our experiments for BM25's b parameter we used values between 0 and 1 with steps of 0.11, and for PL2's c parameter we used values 0.1, 1, . . . 10, 100 with steps of 1 between 1 and 10.
For each collection, model and parameter value, we recorded the retrieval performance (Mean Average Precision (MAP) and Precision @ 10 (P@10)) and calculated the retrievability bias using the approach employed in prior work [3, 1, 5, 7]. This was performed as follows; we extracted all terms that co-occurred more than once with each other from the collection and used these as bigram queries (Table 1 shows the number of queries per collection issued). These queries were issued to the retrieval system given its parametrization and the results were used to compute retrievability measures. We computed both cumulative scores with cut-offs of 5, 10, 20, 50, 70, 100, and gravity based scores with cut-off of 100 and  (where  is the discount factor) values of 0.5, 1, 1.5 and 2. For brevity, we only report on a subset of these. These were used to compute the corresponding Gini coefficient for each measure, model, normalization parameter and collection.

Collections Docs
Collection Type Trec Topics
# Bigrams/Queries

AP 164,597
News 1-200 81964

AQ 1,033,461
News 303-369 273245

DG 1,247,753
Web 551 - 600
337275

Table 1: Collection Statistics

4. RESULTS
During the course of our analysis we examined the following research questions:
How does the retrievability bias change across length normalization parameters? In Figure 1, the left hand plots show the relationship between the retrievability bias (denoted by the Gini coefficient for different retrievability measures) and the parameter settings for BM25 and PL2. Immediately we can see that as the parameter setting is manipulated, the retrievability bias changes. For BM25, the minimum bias was when b=0.7 on AP and AQ, and b=0.9 on DG, whereas on PL2 the minimum bias was was between c=1 and c=3 (see Table 2). This was regardless of the Gini measure used i.e. the cumulative and gravity based measures resulted in the same minimum. Subsequently, for the remainder of this paper, we will use the gravity based retrievability measure when g = 1. What was different between Gini measures was the magnitude of the bias. For example, when the cut-off is low then the bias observed was higher than when the cutoff was increased. This finding is consistent with prior work [3]. When the parameter value of the retrieval was increased or decreased, then we observed that retrieval biased increased, sometimes quite dramatically. This suggests that either longer or shorter documents
1When b = 0, this is equivalent to BM15 and when b = 1 this is equivalent to BM11

938

Gini Coefficient

1 0.95
0.9 0.85
0.8 0.75
0.7 0.65
0.6 0.55
0.5 0
1 0.95
0.9 0.85
0.8 0.75
0.7 0.65
0

Aquaint BM25 gini vs b

C10 C100 G0.5 G1.0 G1.5

0.2

0.4

0.6

0.8

BM25 b Parameter Values

.Gov BM25 gini vs b

C10 C100 G0.5 G1.0 G1.5

0.2

0.4

0.6

0.8

BM25 b Parameter Values

Gini Coefficient

1 0.95
0.9 0.85
0.8 0.75
0.7 0.65

1

0

0.98

0.96

0.94

Gini Coefficient

0.92

0.9

0.88

0.86

0.84

0.82

0.8

1

0

Aquaint PL2 gini vs. c
C10 C100 G0.5 G1.0 G1.5

2

4

6

8

10

PL2 c Parameter Values

.Gov PL2 gini vs. c

2

4

6

8

PL2 c Parameter Values

C10 C100 G0.5 G1.0 G1.5
10

Average R(d)

Average R(d)

Aquaint BM25 (G1.0) R(d) vs. Length

10

0.1

9

0.7

8 0.9

7

6

5

4

3

2

1

0

0

500

1000

1500

Average Length in Bucket

.Gov BM25 (G1.0) R(d) vs. Length

20

0.1

18

0.7

16 0.9

14

12

10

8

6

4

2

0

0

5000

10000

15000

Average Length in Bucket

Average R(d)

Average R(d)

Aquaint PL2 (G1.0) R(d) vs. Length

10

1

9

3

8 10

7

6

5

4

3

2

1

0

0

500

1000

1500

Average Length in Bucket

.Gov PL2 (G1.0) R(d) vs. Length

20

1

18

3

16

10

14

12

10

8

6

4

2

0

0

5000

10000

Average Length in Bucket

15000

Gini Coefficient

Figure 1: Left: Retrieval Bias versus b/c. Right: Retrievability versus Document Length

Collections AP
Aquaint .Gov

Model
BM25 PL2
BM25 PL2
BM25 PL2

Minimum Gini (G1.0) b/c Gini MAP P10
0.7 0.643 0.232 0.354 3 0.701 0.235 0.374
0.7 0.701 0.162 0.316 2 0.746 0.169 0.331
0.9 0.749 0.167 0.222 1 0.872 0.182 0.220

Maximum Map (G1.0) b/c Gini MAP P10
0.3 0.708 0.239* 0.378* 3 0.701 0.235 0.374
0.3 0.785 0.185* 0.390* 6 0.774 0.178 0.374*
0.6 0.707 0.179 0.216 2 0.883 0.184 0.222

Min. Gini (G1.0) Corr. Gini-MAP Gini-P10

-0.075 -0.978*

0.184 -0.925

0.186 -0.844*

0.451 -0.523

-0.646 -0.887

-0.775 -0.916*

Table 2: Minimum Gini/Maximum MAP and corresponding parameter settings/values.  represent statistical significance. Far right columns show the Pearson's Correlation between Retrievability Bias and performance.

were being favoured depending on the setting of b and c. In BM25, as b tends to 1, longer documents are penalized, while when b tends to 0, longer documents are favoured. For PL2, as c tends to 0 then longer documents are penalized, and as c tends to infinity, longer documents are favoured.
How does the retrievability of documents change across length normalization parameters? To examine this intuition, we sorted the documents in each collection by length and placed them into buckets. We then computed the mean length within each bucket, and the mean retrievability r(d) given the documents in the bucket. The plots on the right in Figure 1 show how the retrievability changes when the length normalization parameter changes on BM25/PL2 and AQ/DG for a subset of parameters with a retrievability measure of g = 1. The plots clearly indicate that when the retrieval bias is minimized (for example, when b = 0.7 on AQ) the retrievability across length is about as equal/fair as it gets across the collection. This shows that the measures of retrievability bias (i.e. the Gini coefficients) capture the bias towards longer and shorter documents as the document length parameter is manipulated.
What is the relationship between Performance and Retrievability? In Figure 2, we plotted the Gini coefficient against MAP for each collection using the results from BM25. Similar plots can be found in Figure 3 for other retrieval models and other performance measures. On these plots, a large diamond has been added to the lines to indicate the length normalization parameter that favours longer documents (ie. b = 0 and c = 100). As the parameter value is increased/decreased for BM25/PL2 the retrieval model

setting favours shorter documents (as shown in the plots in Figure 1).
The plots provide a number of key insights into the relationship between retrievability bias and performance. Firstly, the relationship is non-linear with a trend that suggests that minimizing bias leads to better performance. If we consider the relationship in Figure 2 for DotGov, we can see that a reduction in bias leads to successive improvements in terms of performance for MAP (and P@10). With this collection, the effect is the most pronounced and minimizing retrieval bias does tend to the best retrieval performance. Once too much length normalization has occurred, such that shorter documents are overly favoured, then the retrievability bias increases and performance begins to decrease.
When we examine the relationship for AP and AQ, we see that as shorter documents become more favoured, a trade-off between bias and performance quickly develops, but again once the minimum bias point has been reached, the kick back (i.e. loss in performance and increase in bias) is much more pronounced. These findings suggest that as document length normalization is applied in order to penalize longer documents, going past the point of minimum bias will degrade performance and retrievability.
Table 2 shows the performance of the retrieval models when bias is minimized and when performance is maximized. Included in the table are the results of t-tests conducted to determine whether there was a statistical difference in performance between the two settings (assuming p < 0.05,  indicates a significant difference). For AP and AQ, we found there was a significant difference for BM25 (both MAP and

939

P@10), however the differences were less pronounced for PL2 and on DotGov. These findings suggest that minimizing bias may not lead to the best performance. It does however, tend to give reasonably good retrieval performance that on many occasions, is not significantly different from the best possible retrieval performance. We speculate that the mis-match between may be due to the length bias with the TREC relevance pools as discovered in [9]. Thus, in the absence of relevance judgements tuning a retrieval system such that it minimizes retrieval bias is likely to be a good starting point.

AP, Aquaint & .Gov BM25 (G1.0) MAP vs. gini

1
AP

0.95

Aquaint

.Gov
0.9

0.85

Gini

0.8

0.75

0.7

0.65

0.05

0.1

0.15

0.2

0.25

MAP

Figure 2: MAP vs. Gini for all collections on BM25.
5. SUMMARY
In this paper we have analysed the relationship between retrievability bias and performance in the context of adhoc retrieval across length normalization parameters for two standard IR models. Empirically, we have shown that the relationship is much more complex that previously thought and in fact non-linear. Nonetheless, we have shown that reducing the bias within the collection leads to reasonably good performance, and crucially, if too much document length normalization is performed then this will invariably result in a degradation of performance and an increase in bias. This is a useful finding suggesting that retrievability could be used to tune retrieval systems without recourse to relevance judgements. Future work will be directed to studying the relationship between retrievability and performance (in terms of MAP and P@10) in more detail across other collections and across different parameter settings (i.e. query parameters, smoothing parameters) and with different retrieval models (language models, query expansion, link/click evidence, etc).
Acknowledgments This work is supported by the EPSRC Project, Models and Measures of Findability (EP/K000330/1).

6. REFERENCES
[1] L. Azzopardi and R. Bache. On the relationship between effectiveness and accessibility. In Proc. of the 33rd international ACM SIGIR, pages 889­890, 2010.
[2] L. Azzopardi and C. Owens. Search engine predilection towards news media providers. In Proc. of the 32nd ACM SIGIR, pages 774­775, 2009.

Gini

Gini

AP, Aquaint & .Gov BM25 (G1.0) MAP vs. gini

1 AP

0.95

Aquaint

.Gov 0.9

0.85

0.8

0.75

0.7

0.65

0.05

0.1

0.15

0.2

0.25

MAP

AP, Aquaint & .Gov BM25 (G1.0) P@10 vs. gini

1 AP

0.95

Aquaint

.Gov 0.9

0.85

0.8

0.75

0.7

0.65

0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 P@10

Gini

Gini

AP, Aquaint & .Gov PL2 (G1.0) MAP vs. gini

1 AP

0.95

Aquaint

.Gov

0.9

0.85

0.8

0.75

0.7

0.65

0.05

0.1

0.15

0.2

0.25

MAP

AP, Aquaint & .Gov PL2 (G1.0) P@10 vs. gini

1 AP

0.95

Aquaint

.Gov

0.9

0.85

0.8

0.75

0.7

0.65 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 P@10

Figure 3: MAP (Top plots) and P@10 (Bottom plots) vs. Gini for all collections on BM25 (right plots) and PL2 (left plots).
[3] L. Azzopardi and V. Vinay. Retrievability: An evaluation measure for higher order information access tasks. In Proc. of the 17th ACM CIKM, pages 561­570, 2008.
[4] R. Bache. Measuring and improving access to the corpus. In Current Challenges in Patent Information Retrieval, volume 29 of The Information Retrieval Series, pages 147­165. 2011.
[5] S. Bashir and A. Rauber. Improving retrievability of patents with cluster-based pseudo-relevance feedback documents selection. In Proc. of the 18th ACM CIKM, pages 1863­1866, 2009.
[6] S. Bashir and A. Rauber. Improving retrievability of patents in prior-art search. In Proc. of the 32nd ECIR, pages 457­470, 2010.
[7] S. Bashir and A. Rauber. Improving retrievability and recall by automatic corpus partitioning Transactions on large-scale data- and knowledge-centered systems ii. chapter I, pages 122­140. 2010.
[8] J. Gastwirth. The estimation of the lorenz curve and gini index. The Review of Economics and Statistics, 54:306­316, 1972.
[9] D. E. Losada, L. Azzopardi, and M. Baillie. Revisiting the relationship between document length and relevance. In Proc. of the 17th ACM CIKM'08, pages 419­428, 2008.
[10] J. Pickens, M. Cooper, and G. Golovchinsky. Reverted indexing for feedback and expansion. In Proc. of the 19th ACM CIKM, pages 1049­1058, 2010.
[11] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In Proc. of the 19th ACM SIGIR, pages 21­29, 1996.
[12] A. Singla and I. Weber. Tagging and navigability. In Proc. of the 19th WWW, pages 1185­1186, 2010.
[13] C. J. van Rijsbergen. Information Retrieval. 1979. [14] L. Zheng and I. J. Cox. Document-oriented pruning of
the inverted index in information retrieval systems. In Proc. of the 2009 WIANA, pages 697­702, 2009.

940

Cumulative Citation Recommendation: Classification vs. Ranking

Krisztian Balog
University of Stavanger
krisztian.balog@uis.no

Heri Ramampiaro
NTNU Trondheim
heri.ramampiaro@idi.ntnu.no

ABSTRACT
Cumulative citation recommendation refers to the task of filtering a time-ordered corpus for documents that are highly relevant to a predefined set of entities. This task has been introduced at the TREC Knowledge Base Acceleration track in 2012, where two main families of approaches emerged: classification and ranking. In this paper we perform an experimental comparison of these two strategies using supervised learning with a rich feature set. Our main finding is that ranking outperforms classification on all evaluation settings and metrics. Our analysis also reveals that a ranking-based approach has more potential for future improvements.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software
Keywords
Knowledge base acceleration, cumulative citation recommendation, information filtering
1. INTRODUCTION
Knowledge bases, such as Wikipedia, are increasingly being utilised in various information access contexts. With the exponential growth of the amount of information being produced, a continuously increasing effort is demanded from editors and content managers who are responsible for the maintenance and update of these knowledge bases. To partly address this challenge, the Text REtrieval Conference (TREC) has launched a Knowledge Base Acceleration (KBA) track in 2012 with the ultimate goal to develop systems that can aid humans expand knowledge bases by automatically recommending edits based on incoming content streams [10]. In its first year, the track focused on a single problem and introduced the cumulative citation recommendation (CCR) task: given a textual stream consisting of news and social media content and a target entity from a knowledge base (Wikipedia), generate a score for each document based on how pertinent it is to the input entity.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

At TREC, two main families of approaches emerged: classification and ranking. While ranking methods were more popular, no empirical evidence has been provided yet to support that ranking is to be preferred over classification for CCR. Based on participants' system descriptions, we found a single case where the choice was made deliberately after consideration. Efron et al. [8] argue that ranking provides a relevance score, as opposed to a binary decision, which "would allow a Wikipedia editor to browse documents in decreasing order of predicted relevance, while also admitting a simple thresholding if she wished to see only those documents judged to be either `relevant,' `central' or both." This, however, has to do with the consumption of the results. A classifier's confidence values can easily be turned into relevance scores [3­5].
Our goal with this study is to compare classification and ranking approaches for CCR. To ensure a fair assessment, we employ supervised learning for both groups of methods and utilise a rich feature set we developed in prior work [3]. For classification, we use various multi-step models from existing work that have been shown to deliver state-of-the-art performance [3]. For ranking we test pointwise, pairwise, and listwise learning-to-rank approaches. We find a pointwise ranking approach, Random Forests, to perform best and that it improves substantially on best known results so far. Our analysis also reveals that this approach has more potential for future improvements than its classification-based counterpart.
2. RELATED WORK
Knowledge base population (KBP) refers to efforts made to expand knowledge bases by reading arbitrary text provided, e.g., as Web documents, and extracting meaningful information from it [6, 9]. The Text Analysis Conference (TAC) introduced a dedicated Knowledge Base Population track in 2009 [13]. A core component of KBP that bears immediate relevance to our task is entity linking: given an entity and a document containing a mention of the entity, identify and link the corresponding node in the knowledge base [17, 18]. The CCR task naturally has an entity identification element to it; entity disambiguation, however, is tackled implicitly, as part of centrality detection [3, 10].
The CCR task also shares similarities with both information filtering [12] and topic detection and tracking (TDT) [1]. One key difference lies in the end user task that is being modelled. Another key difference is that CCR has no novelty requirement. Finally, CCR attempts to make fine-grained distinctions between relevant and central documents.
3. TASK AND DATA DESCRIPTION
Recognising the need for intelligent systems that can help reduce efforts associated with the maintenance of large-scale knowledge bases, TREC has launched a Knowledge Base Acceleration (KBA) track in 2012. The track has introduced the cumulative citation

941

recommendation (CCR) task: filter a time-ordered corpus for documents that are highly relevant to a predefined set of entities [10].
Data collection. A document collection, called KBA Stream Corpus 20121, has been developed specifically for this track. It covers the time period from October 2011 to April 2012, and is composed of three types of documents: (1) news, i.e., global public news wires; (2) social, i.e., blogs and forums; and (3) linking, i.e., content from URLs shortened at bitly.com. The raw collection amounts to 8.7TB. It also comes in a "cleansed" version, 1.2TB (270GB compressed), where body text is extracted after HTML boilerplate removal. For social documents the content is further separated into title, body, and anchor fields (for news and linking only the body field is available). We work with the cleansed version, but we do not make use of the provided named entity annotations. Each stream document (or document for short) is timestamped and is uniquely identified by a stream_id.
Topics. The topic set consists of 29 entities (27 persons and 2 organisations), referred to as target entities and are uniquely identified by a urlname. These are described by semi-structured articles in a knowledge base, specifically, Wikipedia. Target entities were chosen such that they receive a moderate number of mentions in the stream corpus: between once per day and once per week.
Training and testing data. TREC KBA provides training annotation data, i.e., assessor judgements, for corpus documents from the October to December 2011 period. Documents from the January to April 2012 period are used for testing. We follow this setup, i.e., we only use pre-2012 documents for training.
Annotations are provided along two dimensions: contain mention and relevance. The annotation matrix is shown in Figure 1. Rows denote whether the document mentions the target entity explicitly (top) or not (bottom). Columns indicate the level of relevance, which is judged on a 4-point scale: garbage (G): not relevant (e.g., spam); neutral (N): not relevant (nothing can be learned about the target entity); relevant (R): relates indirectly to the target entity (e.g., mentions topics or events that are likely to have an impact on the entity); central (C): relates directly to the target entity (e.g., the entity is a central figure in the mentioned topics or events). The aim for systems performing the CCR task is to replicate the central judgment, that is, to propose documents that a human would want to cite in the Wikipedia article of the target entity.
Note that, in theory, a document can be relevant, even if it does not mention the target directly. In practice, however, centrality never happens without an explicit mention of the entity in the document [10]. Therefore, we are focusing only on documents with explicit mentions, i.e., the top row in Figure 1.
Evaluation Methodology. Systems performing the CCR task are required to process the documents in chronological order (in hourly batches) and assign a score in the range of (0, 1000] for each document that is deemed citation-worthy for a given target entity. Evaluation against the ground truth (i.e., manual annotations) is performed as follows. A cutoff value is varied from 0 to 1000 (in steps of 50) and documents with a score above the cutoff are considered being identified as relevant (positive cases) by the system. Consequently, documents below the cutoff are treated as irrelevant (belong to the negative class). Precision, recall, and F-score (F1) are computed as a function of the relevance cutoff. In addition, as with general information filtering, the notion of Scale Utility (SU) is used to evaluate the ability for a system to accept relevant and reject non-relevant documents from a document stream [19].
Following the official TREC KBA evaluation methodology, we consider two experimental settings: (i) treating only central documents as positives and non-centrals as negatives (denoted as C)
1http://trec-kba.org/kba-stream-corpus-2012.shtml

non-relevant

relevant

garbage neutral relevant central

GN R C

yes

contain mention

no

Figure 1: Document annotation matrix from the TREC 2012 KBA track. The goal of the CCR task is to identify central documents, i.e., the ones in the top right corner.
and (ii) accepting both relevant and central documents as correct (denoted as R+C). Further, as suggested in [3], we present two alternative ways of determining confidence cutoffs: (i) using a single cutoff value that maximises F1/SU across all entities and (ii) setting the cutoff values on a per-entity basis so that F1/SU is maximised for each individual entity. All scores are macro averages.
4. APPROACHES
In this section we discuss two main approaches to solving the CCR task: classification and ranking. Note that at this point our main focus in on the overall strategy; we are unconcerned about the particular choice of classification/ranking algorithm and of the features used. For both classification and ranking we employ supervised learning and use the same feature set (presented in Section 5).
Identifying entity mentions. Common to both approaches is an initial step that provides a filter to identify whether or not a document contains a mention of the target entity. We base this detection on strict string matching, using known name variants of the entity extracted from DBpedia, an approach that is shown to achieve high recall while keeping a low false positive rate [3]. Note that central documents always contain an explicit mention of the entity (cf. Section 3). Therefore, we expect this initial filtering to have a precision-enhancing effect, potentially, at the expense of recall. This filtering step also allows us to avoid having to compute a potentially large set of features for all possible document-entity pairs.
4.1 Classification
In [3] two multi-step classification approaches are presented. For both, the first step is concerned with the identification of entity mentions in documents (which may be viewed as a binary classification task). It is followed by one or two subsequent binary classification steps (making it two or three in total, respectively).
Under the 2-step approach documents are classified as central or not in a single step. Document-entity pairs labeled as garbage (G) or neutral (N) are used as negative examples and central (C) ones are used as positive examples. Note that instances labeled as relevant (R) are ignored (so as not to "soften the distinction between the two classes that we are trying to separate" [3]). Negative predictions are mapped to the (0, 500] range and the positive (central) predictions to the (500, 1000] range, using the classifier's confidence estimations.
The 3-step approach first attempts to separate garbage or neutral documents from the relevant or central ones (GN vs. RC). A second classifier is applied to documents that fall into the latter category to distinguish them further into relevant and central classes (R vs. C). The same set of features is used for both steps, but the model is trained on the respective classes. Final document scores are also determined in two steps. Documents that are classified as negative in the first step are mapped to the (0, 500] range. In the second step, documents classified as negative are mapped

942

to the (500, 750] range, while documents classified as positive are mapped to the (750, 1000] interval. As before, the actual score is determined based on the classifier's confidence estimates.
TREC 2012 KBA track. In [14] CCR is approached as a classical text classification task; a linear SVM model in trained using unigram and entity name features. Both [4] and [5] apply a Random Forest classifier on top of documents that have been filtered to contain mention of the target entity. In [4] a single classification step is used; this corresponds to our 2-step approach. In [5] documents are first classified into G+N vs. R+C classes, then, documents assigned to the latter class are further separated into R and C, using a second classifier; this is equivalent to our 3-step approach.
4.2 Ranking
We can also approach CCR as a learning-to-rank (LTR) problem: estimate a numerical score for document-entity pairs. The target score directly corresponds to the target class: 0 for garbage and neutral, 1 for relevant, and 2 for central documents. When training the models, we can directly target the different evaluation scenarios. In the "Central" setting we exclude documents being labeled relevant from the training set. For the second setting, "Relevant + Central," we include relevant documents too, for training. Unlike with classification, we do not need a different approach for the two cases, this can be left to the learning algorithm. Also, mapping the estimated target scores to (0, 1000] is straightforward.
In principle, any LTR algorithm may be used. However, since our target scores can only take a handful of possible values, we would expect pointwise and pairwise methods to work best. Listwise algorithms might have difficulties as target scores are not continuous functions w.r.t. to the ranking model's parameters.
TREC 2012 KBA track. A variety of scoring methods has been tried, including language models [2], Markov Random Fields [7], standard Lucene scoring [20], Jaccard similarity [15], and custom ranking functions based on entity co-occurrences [11, 16]. Using merely the name of the target entity as a query is a very sparse representation; therefore, query expansion is often employed to enrich the "entity profile" query with other name variants and contextual information (terms and/or related entities) from Wikipedia [7, 8, 15, 16, 20] or from the document stream [7, 8, 15].
5. FEATURES
The choice of features has a direct impact on the accuracy of machine learning algorithms. A variety of features has been proposed at TREC 2012 KBA with the aim of going beyond the term space, capturing phenomena specific to the CCR task. We wish to emphasise that we recognise the importance of feature engineering for this task and it is worthy of a study on its own account. In this paper, however, our focus is on the comparison of classification vs. ranking approaches and not on the features themselves. Therefore, we use the features proposed in [3], a total of 68, and resort to a high-level overview of them. We would also like to note that the features are only computed for document-entity pairs where the entity is mentioned in the document.
Document features. Surface level features that are based solely on the characteristics of the document and are independent of the target entity: the length of various document fields, such as body, title, and anchor text; the source type (news, social, or linking); whether the document is in English.
Entity features. This group consists of a single feature: the number of entities that are known to be related to the target entity (i.e., already recorded as related in the knowledge base).
Document-entity features. One group of features characterises the occurrences of the target entity in the document: the number of oc-

Table 1: CCR results using (i) a single cutoff value for all entities (columns 2­5) and (ii) using the best cutoff value for each entity (columns 6­10). Best scores are typeset boldface.

Method

Single cutoff

C

R+C

Per-entity cutoff

C

R+C

F1 SU F1 SU F1 SU F1 SU

Classification

2-step J48 2-step RF 3-step J48 3-step RF

.360 .263 .352 .342 .335 .300 .351 .347

.649 .630 .668 .657 .685 .673 .691 .673

.394 .292 .412 .427 .379 .328 .395 .423

.708 .710 .715 .736 .703 .697 .710 .721

Ranking

Random Forests .390 .369 .722 .718 .463 .480 .776 .790

RankBoost

.339 .356 .697 .691 .405 .452 .745 .766

LambdaMART .354 .351 .646 .624 .410 .463 .673 .701

TREC bests

HLTCOE [14] .359 .402 .492 .555 .416 .481 .508 .576

UDel [16]

.355 .331 .597 .591 .365 .419 .597 .613

currences in different document fields; first and last positions in the document body; the "spread" of the entity's mentions across the document body. These are computed using both strict and loose name matching. The second subset of features focuses on other entities that are known to be related to the target: counts of related entity mentions in various document fields (body, title, and anchor text). The last batch of features measures the textual similarity between the stream document and the target entity's article in the knowledge base (that is, the entity's Wikipedia page): Jaccard similarity, cosine similarity with TF-IDF term weighting, and the Kullback-Leibler divergence.
Temporal features. Two sources are used to capture if something is "happening" around the target entity at a given point in time. First, based on Wikipedia page view statistics: average hourly page views; page views volume in the past h hours, both as an absolute value and relative to the normal volume; whether there is a burst in the past h hours (where h is 1, 2, 3, 6, 12, 24). Second, the same set of features, but based on the volume of documents in the stream that mention the target entity: average hourly page views; absolute and relative volumes; whether there is a burst detected.
6. EXPERIMENTAL EVALUATION
For classification we employ two decision tree classifiers, as in [3]: J48 and Random Forest (RF).2 We use the implementations of the Weka machine learning toolkit3 with default parameter settings. For ranking we experimented with different learning-to-rank methods that are currently available in RankLib.4 Because of space limitations, we only report on the top performing method from each class: Random Forests for pointwise, RankBoost for pairwise, and LambdaMART for listwise. Table 1 displays the results. Runs and detailed evaluation results are made available at http://bit.ly/16RraPB.
We find that all classification methods deliver similar performance. With a single exception, RF always outperforms J48. Both
2We also experimented with SVM and Naive Bayes in prior work, but the performance of those were far below that of decision trees. 3http://www.cs.waikato.ac.nz/~ml/weka/ 4http://people.cs.umass.edu/~vdang/ranklib.html

943

F-score difference

F-score difference

0.5
0.4
0.3
0.2
0.1
0
-0.1
-0.2
0.5
0.4
0.3
0.2
0.1
0
-0.1
-0.2
Figure 2: Random Forests ranking vs. Random Forest 3-step classification. (Top) best overall cutoff value; (Bottom) best cutoff value per topic.
the 2-step and 3-step approaches gain approximately equal benefits when moving from single to per-entity cutoffs.
The best ranking method, Random Forests, is a pointwise one; this is in line with our expectations. The pairwise method (RankBoost) outperforms the listwise approach (LambdaMART) when both central and relevant are accepted; for central-only there is no clear winner. We observe the same trends when moving from single to per-entity cutoffs as with classification approaches. In overall, we find that the best ranking method outperforms all classification approaches, while the other two deliver competitive performance.
For reference, we also report results on the two top performing official TREC submissions. The HLTCOE approach has very high SU scores for C; in the single cutoff case it cannot be matched. However, on all the other metrics and settings, the Random Forests ranking method outperforms the best TREC approaches and does so by a considerable margin (ranging from 8.6% up to 52.7%).
7. TOPIC-LEVEL ANALYSIS
Average results might hide interesting differences on the level of individual topics; we continue with a topic-level analysis in this section. To remain focused, we use a single representative for each family of approaches: Random Forest 3-step for classification and Random Forests for ranking. Further, we limit ourselves to the "Central" evaluation setting (that is, only central documents are accepted as relevant) and use F1 as our evaluation measure.
Since evaluation measures are computed as a function of the relevance cutoff, we consider two settings. First, we use the best overall cutoff value; this corresponds to column 2 in Table 1. In Figure 2 (Top) we plot topics in decreasing order of F1 score differences. Positive values mean that the ranking approach performs better, while negative values indicate the advantage of the classification approach on that particular topic. We find that ranking performs better on 14 topics, the difference is negligible on 13 (< 0.05), and classification wins only on 2.
Next, we perform the same comparison, but selecting the best cutoff value for each individual topic; this corresponds to column 6 in Table 1. Figure 2 (Bottom) displays the results. We can see that the ranking approach benefits a lot more from the cutoff optimisation, not just in terms of absolute score, but also on the level of individual topics. This suggests that the ranking approach holds more promise for additional improvements that might be achieved by optimising w.r.t. the cutoff parameter.

8. CONCLUSIONS
In this work, we have carried out a comparative study on two families of approaches to support the cumulative citation recommendation (CCR) task for knowledge base acceleration (KBA). Specifically, we have compared classification methods against ranking methods with respect to their ability to identify central documents from a content stream that would imply modifications to a given target entity in a knowledge base. Our results have shown that ranking approaches are a better fit for this task. Our conjecture is that this has to do with the particular evaluation methodology employed by the TREC KBA track; ranking methods directly emit results in the desired format, while classification methods need to resort to additional mechanisms that can translate their output to a ranking. Further gains might be achieved by optimising against the relevance cutoff parameter; we have found that a ranking-based approach has more room for improvements in this respect.
References
[1] J. Allan. Topic detection and tracking: event-based information organization, volume 12. Springer, 2002.
[2] S. Araujo, G. Gebremeskel, J. He, C. Bosscarino, and A. de Vries. CWI at TREC 2012, KBA track and session track. In TREC '12, 2013.
[3] K. Balog, N. Takhirov, H. Ramampiaro, and K. Nørvåg. Multi-step classification approaches to cumulative citation recommendation. In Proc. of OAIR'13, pages 121­128, 2013.
[4] R. Berendsen, E. Meij, D. Odijk, M. de Rijke, and W. Weerkamp. The University of Amsterdam at TREC 2012. In TREC '12, 2013.
[5] L. Bonnefoy, V. Bouvier, and P. Bellot. LSIS/LIA at TREC 2012 knowledge base acceleration. In TREC '12, 2013.
[6] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. H. Jr., and T. M. Mitchell. Toward an architecture for never-ending language learning. In Proc. of AAAI'10, 2010.
[7] J. Dalton and L. Dietz. Bi-directional linkability from Wikipedia to documents and back again: UMass at TREC 2012 knowledge base acceleration track. In TREC '12, 2013.
[8] M. Efron, J. Deisner, P. Organisciak, G. Sherman, and A. Lucic. The University of IllinoisâA Z´ Graduate School of Library and Information Science at TREC 2012. In TREC '12, 2013.
[9] O. Etzioni, M. Banko, S. Soderland, and D. S. Weld. Open information extraction from the web. Commun. ACM, 51(12):68­74, 2008.
[10] J. R. Frank, M. Kleiman-Weiner, D. A. Roberts, F. Niu, C. Zhang, C. Ré, and I. Soboroff. Building an entity-centric stream filtering test collection for TREC 2012. In TREC '12, 2013.
[11] O. Gross, A. Doucet, and H. Toivonen. Term association analysis for named entity filtering. In TREC '12, 2013.
[12] U. Hanani, B. Shapira, and P. Shoval. Information filtering: Overview of issues, research and systems. User Modeling and User-Adapted Interaction, 11(3):203­259, 2001.
[13] H. Ji and R. Grishman. Knowledge base population: successful approaches and challenges. In Proc. of ACL HLT'11, 2011.
[14] B. Kjersten and P. McNamee. The HLTCOE approach to the TREC 2012 KBA track. In TREC '12, 2013.
[15] Y. Li, Z. Wang, B. Yu, Y. Zhang, R. Luo, W. Xu, G. Chen, and J. Guo. PRIS at TREC2012 KBA track. In TREC '12, 2013.
[16] X. Liu and H. Fang. Entity profile based approach in automatic knowledge finding. In TREC '12, 2013.
[17] R. Mihalcea and A. Csomai. Wikify!: linking documents to encyclopedic knowledge. In Proc. of CIKM'07, pages 233­242, 2007.
[18] D. N. Milne and I. H. Witten. Learning to link with Wikipedia. In Proc. of CIKM'08, pages 509­518, 2008.
[19] S. E. Robertson and I. Soboroff. The TREC 2002 filtering track report. In TREC'02, 2003.
[20] C. Tompkins, Z. Witter, and S. G. Small. SAWUS Siena's automatic Wikipedia update system. In TREC '12, 2013.

944

Effectiveness/Efficiency Tradeoffs for Candidate Generation in Multi-Stage Retrieval Architectures

Nima Asadi1,2, Jimmy Lin3,2,1
1Dept. of Computer Science, 2Institute for Advanced Computer Studies, 3The iSchool University of Maryland, College Park
nima@cs.umd.edu, jimmylin@umd.edu

ABSTRACT
This paper examines a multi-stage retrieval architecture consisting of a candidate generation stage, a feature extraction stage, and a reranking stage using machine-learned models. Given a fixed set of features and a learning-to-rank model, we explore effectiveness/efficiency tradeoffs with three candidate generation approaches: postings intersection with SvS, conjunctive query evaluation with Wand, and disjunctive query evaluation with Wand. We find no significant differences in end-to-end effectiveness as measured by NDCG between conjunctive and disjunctive Wand, but conjunctive query evaluation is substantially faster. Postings intersection with SvS, while fast, yields substantially lower end-to-end effectiveness, suggesting that document and term frequencies remain important in the initial ranking stage. These findings show that conjunctive Wand is the best overall candidate generation strategy of those we examined.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation
Keywords: query evaluation; postings intersection
1. INTRODUCTION
One possible architecture for web retrieval breaks document ranking into three stages: candidate generation, feature extraction, and document reranking. There are two main reasons for this multi-stage design. First, there is a general consensus that learning to rank provides the best solution to document ranking [13, 12]. As it is difficult to apply machine-learned models over the entire collection, in practice a candidate list of potentially-relevant documents is first generated. Thus, learning to rank is actually a reranking problem (hence the first and third stages). Second, separating candidate generation from feature extraction has the advantage of providing better control over cost/quality tradeoffs. For example, term proximity features are significantly more costly to compute than unigram features; therefore, by
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

decoupling the first two stages in the architecture, systems can exploit "cheap" features to generate candidates quickly and only compute "expensive" term proximity features when necessary--thus decreasing overall query evaluation latency.
Given a fixed set of features and a learning-to-rank model, we explore effectiveness/efficiency tradeoffs in the candidate generation stage, comparing postings intersection with SvS, conjunctive query evaluation with Wand, and disjunctive query evaluation with Wand. Previous work has suggested that conjunctive query evaluation yields early precision results that are at least as good as disjunctive query evaluation, but is much faster. We experimentally confirm this observation, and additionally show that postings intersection (with results sorted by spam scores) yields substantially worse output within the same framework.
The contribution of this work is an empirical evaluation of common candidate generation algorithms in a multi-stage retrieval architecture. By fixing the feature generation and reranking stages, we are able to isolate the end-to-end effectiveness and efficiency implications of different algorithms.
2. BACKGROUND AND RELATED WORK
We begin with a more precise specification of our threestage architecture, illustrated in Figure 1. The input to the candidate generation stage is a query Q and the output is a list of k document ids {d1, d2, ...dk}. In principle, this can be considered a sorted list, but that detail is unimportant here. These document ids serve as input to the feature extraction stage, which returns a list of k feature vectors {f1, f2, ...fk}, each corresponding to a candidate document. These serve as input to the third document reranking stage, which typically applies a machine-learned model to produce a final ranking.
This multi-stage retrieval architecture has recently been explored by many researchers. Some have examined the entire pipeline; for example, Macdonald et al. [14] assessed the impact of variables such as the number of candidate documents and the objective metric to use when training the learning-to-rank model (however, they did not explore effectiveness/efficiency tradeoffs with candidate generation). Others have specifically looked at candidate generation, e.g., postings intersection on multi-core architectures [17], dynamic pruning [18], and approximate techniques [3]. There has been some work focused on the feature extraction stage, exploring how to best represent positional information; two studies have independently come to the conclusion that it is advantageous to store document positions in a document vector representation distinct from the inverted index [1, 2]. Finally, most work on learning to rank [13, 12] assumes such

997

Relevance

d1

d2

d3

Q

Candidate Generation

d4 d5 d6

Feature Extraction

d7

...

dk

f1

dr1

f2

dr2

f3

dr3

f4 f5 f6

Document Reranking

dr4 dr5 dr6

f7

dr7

...

...

fk

drk

Candidates

Features

Final Results

Figure 1: Illustration of a multi-stage retrieval architecture with distinct candidate generation, feature extraction, and document reranking stages.

an architecture, because the input to the machine-learned model is a set of feature vectors. Occasionally, this fact is made explicit: for example, Cambazoglu et al. [6] experimented with reranking 200 candidate documents to produce the final ranked list of 20 results.
There are two general approaches to candidate generation: conjunctive and disjunctive query processing. In the first, only documents that contain all query terms are considered, whereas in the second, any document with at least one query term is potentially retrievable. One popular algorithm is Wand [4], which can operate in either conjunctive or disjunctive mode with respect to a particular scoring model (e.g., BM25). Wand uses a pivot-based pointer movement strategy to avoid needlessly evaluating documents that cannot be in the final top k ranking. An alternative approach to candidate generation involves simple postings intersection, without reference to a particular scoring model. Culpepper and Moffat [8] demonstrated SvS to be the best algorithm for accomplishing this. Note that since SvS works by iteratively intersecting the current result set with the next shortest postings list, it must exhaustively compute the intersection set. In this paper we explore both Wand and SvS for candidate generation.
It is a well-known fact that disjunctive query processing is much slower than conjunctive processing. However, it is unclear what impact output quality at the candidate generation stage has on end-to-end effectiveness. Although Broder et al. [4] found conjunctive processing to yield higher early precision, the results were not in the context of learningto-rank experiments. We hypothesize that end-to-end effectiveness is relatively insensitive to candidate generation quality, due to an emphasis on early precision in most web search tasks today--we simply need to make sure there are enough relevant documents for the machine-learned model to identify. Thus, there is an interesting possibility that conjunctive query processing might lead to both good and fast results. Our paper explores this hypothesis.

3. EXPERIMENTAL SETUP
3.1 Candidate Generation Algorithms
We examined four approaches to candidate generation, outlined below. For each, we varied the number of candidates generated k, where k  {100, 250, 500, 1000}.
SvSSpam. Since SvS must compute the entire intersection set, to obtain k documents, we sort the intersection results by a static prior and return the top k.1
1One caveat worth mentioning: the common trick to renumber docids based on the static prior doesn't help much here since we need to compute the complete intersection set regardless; it would save the final sort by score but time spent on that is negligible.

SvSBM25. In this approach, we first compute the full intersection set, and then in a second pass we compute BM25 scores for every document in the intersection set. After the second pass, the top k documents are returned.
WandCon. We evaluate the query in conjunctive mode and return the top k documents based on BM25.
WandDis. We evaluate the query in disjunctive mode and return the top k documents based on BM25.
All algorithms were implemented in C and an equal amount of time was spent optimizing each to ensure a fair comparison. All conditions used the same (non-positional) inverted index (even though SvSSpam does not need access to tf's) and for SvSBM25 we modified the accumulators to hold the tf's extracted from the postings for the second-pass scoring. We assumed that all index structures are retained in memory, so query evaluation never involves hitting disk. Experiments were performed on a server running Red Hat Linux, with dual Intel Xeon "Westmere" quad-core processors (E5620 2.4GHz) and 128GB RAM.
3.2 Test Collections and Metrics
We performed experiments on the ClueWeb09 collection, a best-first web crawl from early 2009. Our experiments used only the first English segment, which has 50 million documents (247GB compressed). The Waterloo spam scores [7] were used as the static priors for SvS reranking.
For evaluation, we used three different sets of queries: first, the TREC 2005 terabyte track "efficiency" queries (50,000 queries total).2 Since there are no relevance judgments for these queries, they were used solely for efficiency experiments. Second, a set of 100,000 queries sampled randomly from the AOL query log [16]. Our sample retains the query length distribution of the original dataset. Similar to the TREC 2005 terabyte track queries, we used these queries only to evaluate efficiency.
Finally, we used the TREC web track topics from 2009­ 2011, 150 in total. These topics comprise a complete test collection in that we have relevance judgments, but there are too few queries for meaningful efficiency experiments. We performed five fold cross validation, using three folds for training, one for validation, and one for testing.
We collected two figures of merit: the first was end-to-end effectiveness in terms of NDCG. For efficiency, we measured query latency of the candidate generation stage, defined as the elapsed time between the moment a query is presented to the system and the time when top k candidates are retrieved and forwarded to the feature extraction stage. To capture variance, we repeated runs five times.
3.3 Features and Ranking Model
We used a standard suite of features very similar to those described in previous work [15, 18]. They consist of basic information retrieval scores (e.g., language modeling and BM25 scores), term proximity features (exact phrases, ordered windows, unordered windows), query-independent features (e.g., PageRank, content quality score [7], etc.). Relevant features were computed across multiple fields: the entire document, anchor text, as well as title fields. In total, there are 91 features.3
2http://www-nlpir.nist.gov/projects/terabyte/ 3The complete test collection, including feature descriptions, can be found at https://github.com/lintool/ClueWeb09-TREC-LTR.

998

Dis. Con. Spam Number of documents

Model
BM25 Linear -MART
BM25 Linear -MART
BM25 Linear -MART

NDCG@1 100 250 500 1000
0.06 0.09 0.09 0.11 0.06 0.08 0.10 0.14 0.05 0.09 0.10 0.15
0.25 0.25 0.25 0.25 0.26 0.25 0.25 0.24 0.26 0.25 0.24 0.25
0.25 0.25 0.25 0.25 0.26 0.25 0.25 0.23 0.26 0.25 0.25 0.24

100
0.05 0.05 0.05
0.22 0.25 0.28
0.22 0.25 0.28

NDCG@5 250 500
0.06 0.08 0.06 0.08 0.07 0.09
0.22 0.22 0.24 0.25 0.27 0.25
0.22 0.22 0.24 0.24 0.27 0.26

1000
0.10 0.10 0.11
0.22 0.23 0.25
0.22 0.23 0.25

100
0.04 0.03 0.03
0.21 0.25 0.26
0.21 0.25 0.26

NDCG@20 250 500
0.05 0.06 0.05 0.07 0.05 0.06
0.21 0.21 0.24 0.23 0.25 0.24
0.21 0.21 0.24 0.23 0.25 0.24

1000
0.08 0.08 0.08
0.21 0.23 0.23
0.21 0.23 0.23

100
0.027 0.027 0.027
0.230 0.248 0.250
0.230 0.248 0.250

NDCG 250 500

0.044 0.045 0.045

0.060 0.060 0.060

0.280 0.304 0.297 0.317 0.300 0.318

0.281 0.308 0.298 0.320 0.301 0.321

1000
0.077 0.079 0.079
0.327 0.340 0.340
0.331 0.344 0.344

Table 1: NDCG at different cutoffs using different candidate generation algorithms: SvSSpam (Spam), SvSBM25
and WANDCon (Con.), and WANDDis (Dis.). Vertical blocks show each metric, and columns show the number of candidate documents retrieved.  indicates statistical significance vs. BM25 using the t-test (p<0.05).

For the third stage reranking model, we used two different learning-to-rank techniques. In the first, we trained a linear model using the greedy feature selection approach [15]. The model is iteratively constructed by adding features, one at a time, according to a greedy selection criterion. During each iteration, the feature that provides the biggest gain in effectiveness (as measured by NDCG [11]) after being added to the existing model is selected. This yields a sequence of one-dimensional optimizations that can easily be solved using line search techniques. The algorithm stops when the difference in NDCG between successive iterations drops below a given threshold (10-4). This training procedure is simple, fast, and fairly effective.
Separately, we learned a LambdaMART model [5] using the open-source jforests implementation4 [10]. To tune parameters, we used grid search as suggested by the authors and selected the parameter setting that results in the largest gain in NDCG on the validation set: max number of leaves (9), feature and data sub-sampling (0.3), minimum observations per leaf (0.75), and the learning rate (0.05).
4. RESULTS
Table 1 summarizes NDCG at different rank cutoffs and no cutoff for all combinations of candidate generation and reranking models. The first horizontal block of the table labeled "Spam" refers to SvSSpam, "Con." indicates SvSBM25 or WandCon (as both algorithms produce the same results), and "Dis." shows the use of WandDis. Within each block of the table, "BM25" indicates reranking candidates with BM25, which serves as the baseline (note this only alters the postings intersection candidate results; in the other cases, this is equivalent to a reranker that does nothing); "Linear" is the linear model learned using Metzler's greedy-feature selection method; and "-MART" is LambdaMART. Each column shows the number of candidate documents retrieved.
Candidates generated using SvSSpam yield very low endto-end effectiveness. This shows that combining postings intersection and with a static prior does not provide a sufficiently discriminative signal to include relevant documents in the top k, where k is small relative to the size of the collection. The quality of the static prior is not to blame here, as the Waterloo spam scores have been demonstrated to be very helpful in reranking [7]. Of course, as we increase the size of k, the results of SvSSpam will approach that of the other candidate generation algorithms, but at the cost of more time spent performing feature extraction. It is clear
4http://code.google.com/p/jforests/

300 No judgments

Non-relevant

250

Relevant

200

150

100

50

0

1000 500 250 100
1000 500 250 100

Linear

K

-MART

Figure 2: Number of rel, non-rel, and unjudged documents in the top 20 for different candidate sizes using WANDDis (averaged across folds).

that both term frequencies and document frequencies cannot be ignored in candidate generation.
On the other hand, WandCon (and SvSBM25, which produces exactly the same results) yields NDCG scores that are statistically indistinguishable from those produced by WandDis (with no cutoff, WandDis yields slightly higher scores, but the differences are not statistically significant). In other words, with BM25 scoring, conjunctive candidate generation and disjunctive candidate generation are equally good from the end-to-end effectiveness perspective. This finding is consistent with the results of Broder et al. [4] (but in a learning-to-rank context).
Increasing k improves NDCG at all rank cutoffs for candidates obtained by SvSSpam. We observe the same trend for NDCG without cutoff in all settings. This is expected since larger k translates into higher recall at the candidate generation stage, i.e., more relevant documents are available to the reranker. However, with conjunctive and disjunctive query processing we see a decline in NDCG at rank cutoffs 5 and 20 as we increase the number of candidate documents; this trend is consistent for both the linear model and LambdaMART. To examine this effect in a bit more detail, we computed the number of relevant, non-relevant, and unjudged documents in the top 20 obtained by reranking WandDis. Figure 2 shows these statistics for different values of k. We see that as k increases, more unjudged documents find their way to the top 20, while both the number of relevant and non-relevant documents decreases. Thus, this trend appears to be an artifact of the test collection and not a property of the candidate generation algorithms.

999

Time (ms) Time (ms)

140

SvS (Spam) WAND-Conjunctive

SvS (BM25) WAND-Disjunctive

120

100

80

60

40

20

400 SvS (Spam) WAND-Conjunctive

350

SvS (BM25) WAND-Disjunctive

300

250

200

150

100

50

0100

250

500

1000

0100

250

500

1000

Number of Candidates

Number of Candidates

(a) Terabyte

(b) AOL

Figure 3: Average candidate generation per-query latency across five trials.

Figure 3 shows per-query latency of the candidate generation stage for different values of k. On average, SvSSpam is the fastest since the ranking function is query independent: score computation boils down to table lookups of spam scores. We see that SvSBM25 is on par with WandCon for larger k values. SvSBM25 retrieves the full intersection set and computes scores for every document in the set regardless of k; therefore, increasing k has no impact on latency. On the other hand, WandCon slows with increasing k (but is faster with a small k). Conjunctive query evaluation with Wand can only terminate when a postings list is fully consumed. However, this termination is mostly independent of k; the only factor affected by k is the set of heap operations performed during query evaluation. Finally, WandDis is not only the slowest overall, but latency grows with larger values of k faster than with conjunctive evaluation.
5. CONCLUSIONS
Our experiments show that conjunctive Wand is the best candidate generation strategy of those examined: in terms of end-to-end effectiveness, it is statistically distinguishable from disjunctive query evaluation using Wand, but much faster in query evaluation. Note that the "block-max" optimization to Wand proposed by Ding and Suel [9] does not affect this conclusion--although the optimization increases disjunctive query evaluation speed, it remains slower than conjunctive processing. In addition, we also show that postings intersection with static priors yields very poor end-toend effectiveness, at least with the size of the candidate sets we examined. This suggests that postings intersection, while an interesting algorithmic challenge since it represents the more general problem of intersecting two lists of sorted integers, is not particularly useful by itself if one's goal is to build effective and efficient search systems.
6. ACKNOWLEDGMENTS
This work has been supported by NSF under awards IIS0916043, IIS-1144034, and IIS-1218043. Any opinions, findings, or conclusions are the authors' and do not necessarily reflect those of the sponsor. The first author's deepest gratitude goes to Katherine, for her invaluable encouragement and wholehearted support. The second author is grateful to Esther and Kiri for their loving support and dedicates this work to Joshua and Jacob.

7. REFERENCES
[1] D. Arroyuelo, S. Gonz´alez, M. Marin, M. Oyarzu´n, and T. Suel. To index or not to index: Time-space trade-offs in search engines with positional ranking functions. SIGIR, 2012.
[2] N. Asadi and J. Lin. Document vector representations for feature extraction in multi-stage document ranking. IRJ, in press, 2012.
[3] N. Asadi and J. Lin. Fast candidate generation for two-phase document ranking: Postings list intersection with Bloom filters. CIKM, 2012.
[4] A. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. CIKM, 2003.
[5] C. Burges. From RankNet to LambdaRank to LambdaMART: An overview. Technical Report MSR-TR-2010-82, Microsoft Research, 2010.
[6] B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt. Early exit optimizations for additive machine learned ranking systems. WSDM, 2010.
[7] G. Cormack, M. Smucker, and C. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. arXiv:1004.5168v1, 2010.
[8] J. Culpepper and A. Moffat. Efficient set intersection for inverted indexing. TOIS, 29(1), 2010.
[9] S. Ding and T. Suel. Faster top-k document retrieval using block-max indexes. SIGIR, 2011.
[10] Y. Ganjisaffar, R. Caruana, and C. Lopes. Bagging gradient-boosted trees for high precision, low variance ranking models. SIGIR, 2011.
[11] K. J¨arvelin and J. Kek¨al¨ainen. Cumulative gain-based evaluation of IR techniques. TOIS, 20(4):422­446, 2002.
[12] H. Li. Learning to Rank for Information Retrieval and Natural Language Processing. Morgan & Claypool, 2011.
[13] T.-Y. Liu. Learning to rank for information retrieval. FnTIR, 3(3):225­331, 2009.
[14] C. Macdonald, R. Santos, and I. Ounis. The whens and hows of learning to rank for web search. IRJ, in press, 2012.
[15] D. Metzler. Automatic feature selection in the Markov random field model for information retrieval. CIKM, 2007.
[16] G. Pass, A. Chowdhury, and C. Torgeson. A picture of search. InfoScale, 2006.
[17] S. Tatikonda, B. Cambazoglu, and F. Junqueira. Posting list intersection on multicore architectures. SIGIR, 2011.
[18] N. Tonellotto, C. Macdonald, and I. Ounis. Efficient and effective retrieval using selective pruning. WSDM, 2013.

1000


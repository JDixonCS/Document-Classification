ChatNoir: A Search Engine for the ClueWeb09 Corpus

Martin Potthast

Matthias Hagen

Benno Stein

Jan Graﬂegger

Maximilian Michel

Martin Tippmann

Clement Welsch

Bauhaus-Universit‰t Weimar 99423 Weimar, Germany
<first name>.<last name>@uni-weimar.de

ABSTRACT
We present the ChatNoir search engine which indexes the entire English part of the ClueWeb09 corpus. Besides Carnegie Mellon's Indri system, ChatNoir is the second publicly available search engine for this corpus. It implements the classic BM25F information retrieval model including PageRank and spam likelihood. The search engine is scalable and returns the first results within three seconds, which is significantly faster than Indri. A convenient API allows for implementing reproducible experiments based on retrieving documents from the ClueWeb09 corpus. The search engine has successfully accomplished a load test involving 100 000 queries.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Search process
General Terms: Experimentation
Keywords: search engine, TREC, ClueWeb09
1. INTRODUCTION
Many of the current TREC tracks and TREC style retrieval performance experiments are based on the ClueWeb09 corpus--a collection of 1 billion web pages crawled and provided by the Carnegie Mellon University. As indexing and searching such a large corpus requires a decent amount of hardware probably not available to all researchers interested in TREC style experiments or TREC participation, a public search engine has been provided with the release of the corpus.1 So far, this is the only publicly available search engine for the ClueWeb09 and it has been used in many TREC runs over the last years. However, the engine is rather slow (answer time of about 10 seconds or more) and only offers the Indri retrieval model (language modeling combined with an inference network). As an alternative, we provide a faster public search engine--ChatNoir.
2. CHATNOIR SEARCH
The ChatNoir search engine is based on the classic BM25F retrieval model [4] including the anchor text list provided by the University of Twente [3], the PageRank list provided by the Carnegie Mellon University,2 and the spam rank list provided by the University of Waterloo [1]. ChatNoir also incorporates an approximate proximity feature with variable width buckets as described by Elsayed et al. [2]. The text body of each document is divided into 64 buckets such that neighboring buckets have a half-bucket overlap. For each keyword, not the exact position is stored in a 1-gram index but occurrence in the individual buckets is indicated via a
1 lemurproject.org/clueweb09.php/index.php#Services 2 boston.lti.cs.cmu.edu/clueweb09/wiki/tiki-index.php?page=PageRank
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

bit flag. Hence, for each document and each occurring keyword, a 64-bit vector is used in ChatNoir's approximate proximity feature.
The web interface of ChatNoir is similar to that of commercial search engines (snippets, phrasal search, etc.). As for query processing, non-phrasal queries are handled by a 1-gram index of the ClueWeb09 built with Hadoop. Phrasal queries are handled by a 2gram index and a 3-gram exact position index. For phrase queries containing only 2-grams, the 2-gram index suffices. For phrase queries with longer phrases, the 2-gram index is used to identify documents that contain all 2-grams of a longer phrase while merging the postlists with the 3-gram positional index finally identifies the documents that contain the exact searched phrase. To ensure fast answer times, long queries with more than 2 keywords or phrases are treated in a divide-and-conquer manner. The long query is split into sub-queries for which a parallel retrieval is conducted. The parallel results are then merged into just one list.
The ChatNoir engine runs on a cluster of 10 standard quad-core PCs and 2 eight-core servers. It comes with a web interface and a developer API at chatnoir.webis.de. This is the first public alternative to Carnegie Mellon's Indri search for reproducible experiments on the ClueWeb09 without the need of an own cluster for indexing/searching. A load test with 100 000 unique queries from a commercial search engine log showed the robustness and scalability of ChatNoir. The first ten results are typically shown within three seconds compared to more than ten seconds for an Indri search.
3. CONCLUSION AND OUTLOOK
With our ChatNoir search engine we provide the second public API for reproducibly searching the ClueWeb09 corpus in TREC style experiments. ChatNoir's much faster BM25F based search offers an alternative to the available Indri retrieval model.
A first use case we are currently handling with ChatNoir is the international PAN competition for automatic plagiarism detection. Therefore, paid human individuals write short reports on TREC topics plagiarizing parts from ClueWeb09 documents found via ChatNoir searches. The competition participants are supposed to find the most likely plagiarism sources also using ChatNoir.
4. REFERENCES
[1] Cormack, Smucker, and Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr. 14(5):441≠465, 2011.
[2] Elsayed, Lin, and Metzler. When close enough is good enough: approximate positional indexes for efficient ranked retrieval. CIKM 2011, pp. 1993≠1996.
[3] Hiemstra and Hauff. MIREX: MapReduce information retrieval experiments. Tech. Report TR-CTIT-10-15, Universiteit Twente, 2010.
[4] Robertson, Zaragoza, and Taylor. Simple BM25 extension to multiple weighted fields. CIKM 2004, pp. 42≠49.

1004

A Hybrid Model for Ad-hoc Information Retrieval

Zheng Ye, Jimmy Xiangji Huang
Information Retrieval and Knowledge Management Research Lab
School of Information Technology York University Toronto, Canada
{yezheng, jhuang}@yorku.ca

Jun Miao
Information Retrieval and Knowledge Management Research Lab
Department of Computer Science & Engineering York University Toronto, Canada
jun@cse.yorku.ca

ABSTRACT
Many information retrieval (IR) techniques have been proposed to improve the performance, and some combinations of these techniques has been demonstrated to be effective. However, how to effectively combine them is largely unexplored. It is possible that a method reduces the positive influence of the other one even if both of them are effective separately. In this paper, we propose a new hybrid model which can simply and flexibly combine components of three different IR techniques under a uniform framework. Extensive experiments on the TREC standard collections indicate that our proposed model can outperform the best TREC systems consistently in the ad-hoc retrieval. It shows that the combination strategy in our proposed model is very effective. Meanwhile, this method is also re-useable for other researchers to test whether their new methods are additive to the current technologies.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models, Relevance feedback
General Terms
Algorithms, Performance, Experimentation
Keywords
Hybrid Model, Rocchio's Relevance Feedback
1. INTRODUCTION
In the past thirty years, researchers make great progress in the Information Retrieval (IR) area. A plenty of new technologies, e.g., stemming, query expansion and smoothing methods, have been introduced and help to obtain better performance in retrieving relevant documents. Some attempts to combine these technologies show that the strategy of combination is very important because one technology can counteract the affects of others. However, how to make an effective combination is still largely unexplored, especially under a unified framework.
In this paper, we propose a hybrid model to incorporate three different retrieval techniques that have proven to be effective for the ad-hoc retrieval on the TREC collections.
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

We analyze the best TREC systems for ad-hoc retrieval, and extend the Rocchio's feedback method by incorporating three kinds of IR techniques, which are proximity, feedback document quality estimation and query performance prediction techniques, under the pseudo relevance feedback (PRF) framework to boost the overall performance. We mainly focus on how to refine the representation of the query under the PRF framework in order to avoid the drawbacks of traditional PRF methods. Experimental results on various TREC datasets show that our hybrid model consistently obtains better results over the best TREC systems. Because our proposed model is component-based, it is very flexible to import different techniques in the future. Meanwhile, the hybrid model can help researchers to test whether their methods are additive to improve the overall performance of ad-hoc retrieval which was mentioned in [1].

2. A HYBRID RETRIEVAL MODEL
Rocchio's algorithm [4] is a classic framework for implementing (pseudo) relevance feedback via improving the query representation. When the negative feedback documents are ignored, the traditional Rocchio's model is as follows:

r

Q1 =   Q0 +  

|R|

(1)

rR

where Q0 and Q1 represent the original and first iteration query vectors, R is the set of (pseudo) relevance documents, and r is the expansion term weight vector.
Although the Rocchio's model has been introduced for many years, it is still effective in obtaining relevant documents. According to [7], "BM25 term weighting coupled with Rocchio feedback remains a strong baseline which is at least as competitive as any language modeling approach for many tasks". Meanwhile, it is very flexible to adapt additional components. However, the traditional Rocchio's model can still be reformed to be better. First, the query term proximity information which has proven to be useful is not considered. Second, Rocchio's algorithm views terms from different feedback documents equally. Intuitively, a candidate expansion term in a document with better quality is more likely to be relevant to the query topic. Third, the interpolation parameter  is always fixed across a group of queries. In fact, for a well expressed query, the candidate feedback documents are always more reliable for relevance feedback. In this paper, we use a regression model to predict this interpolation parameter. In order to alleviate influence of these problems, we extend Rocchio's algorithm which re-

1025

Table 1: Direct comparison with the best MAP results in each TREC year. In the Hybrid (Fixed) method, proximity and

feedback document quality are utilized. In the Hybrid (Regression) method, all the three techniques are adapted. A "*"

indicates a statistically significant improvement when a component technique is added in our algorithm.

Method

TREC1 TREC2 TREC3 TREC6 TREC7 TREC8 TREC2004 TREC2005

BM25 BM25+Prox Hybrid-Fixed Hybrid-Reg

0.2292 0.2461 0.2938
0.3012

0.2058 0.2111 0.2913
0.2971

0.2787 0.2929 0.3811 0.3912

0.2397 0.2507 0.2763 0.2886

0.1819 0.1936 0.2576
0.2611

0.2471 0.2582 0.2909 0.3103

0.2672 0.3148 0.3375 0.3431

0.3403 0.3661 0.4083 0.4193

BEST TREC 0.2062 0.2475 0.3231 0.2876 0.2614 0.3063

0.3052

0.4056

TREC2006
0.2965 0.3459 0.3944
0.3921
0.3737

fines the query representation as follows.

Q1 =   (  Q0 + (1 - )  Qp) + (1 - ) 

r  q(dr) (2) |R|

rR

where  controls how much we rely on the query term proximity information [5],  controls how much we rely on the original query, Qp is an n-gram of original query terms and q(dr) is the quality score of document d.
As we can see from Equation 2, our proposed algorithm is very flexible and can evaluate different techniques. In this paper, we adopt the co-occurrence interpretation of term proximity to compute Qp, where the proximity among query terms is represented by the n-gram frequencies and BM25 is used as the weighting model [2].
Full dependencies of query terms are taken into account. For the document quality factor q(dr), we simple use the scores from the first-pass retrieval for approximation as in [6]. For the prediction to , we use the same features as in [3] to train the regression model. The difference is that we do it within Rocchio's framework.

3. EXPERIMENTS AND ANALYSIS
We conduct experiments on three representative test collections: disk1&2, disk4&5, and GOV2, which are used in different TREC years. We present the results for each TREC year such that we can directly compare our results with the best TREC systems. Detailed information about the TREC datasets and the evaluation criteria, please refer to http://trec.nist.gov. For the preprocessing of the collections, we use the Porter Stemmer and a stopword list. In addition, we only use the title part of the topics to retrieve.
In our experiments, we first empirically evaluate different combinations of our implemented component techniques, then evaluate how these techniques perform when they are integrated in our hybrid model. We set the parameters to fixed values in a parsimonious way such that each component technique gets considerable improvements on most collections. In other words, there is still room for improvement if the parameters are tuned on a collection-by-collection basis. Particularly, for the basic retrieval model, we use the Okapi BM25 model, and set b in BM25 to 0.3. When only the query term proximity technique is added, denoted as "BM25+Prox", we set  to 0.3. In addition, when query expansion and document quality estimation techniques are added, denoted as "Hybrid-Fixed", we empirically set , |R| to 0.5 and 30. However, when the query performance prediction technique is used, denoted as "Hybrid-Reg",  is not fixed. But it is obtained from a regression model that is trained as in [3]. When evaluating our hybrid model for a particular TREC year, the queries in the remainder TREC years on the same collection are used as training data.
From Table 1, we can see that our hybrid model with different component techniques can significantly outperforms

the basic retrieval model, which reconfirms the effectiveness of these techniques. In addition, when all these component techniques are used in our hybrid model, the retrieval performance can be further improved. It indicates that performance gains from these two component techniques can be added up in our proposed hybrid model. However, when we use a regression component to predict , the performance gain is not very obvious compared with other components. We conjecture the main reason is as follows: when the feedback document set is more reliable for relevance feedback, the regression component is less useful.
When compared with the best TREC systems, our proposed model obviously outperforms the best TREC systems on most collections . It is of note that the results in our paper are obtained in a uniform setting across all collections while the best TREC results were from different participants independently. We believe the significant improvement is mainly from our successful integration of different IR techniques in the proposed model. In addition, according to Armstrong et al.'s survey, very few published results are better than the best TREC systems, mostly below medium systems. Our proposed model is promising, which provides a good avenue for future IR research, especially for evaluating the overall performance of a system (not a particular component of a system).
4. CONCLUSIONS
In this paper, we propose a hybrid model which can successfully integrate three effective techniques in a uniform model. Extensive experiments show that our approach obviously outperforms the best TREC systems in most cases. In the future, we will investigate more effective techniques in IR and incorporate them into our framework to conduct this research in details.
5. ACKNOWLEDGMENTS
This research is supported by the research grant from the Natural Sciences & Engineering Research Council (NSERC) of Canada and the Early Researcher Award/ Premier's Research Excellence Award, Zhejiang Provincial Natural Science Foundation, Q12F020016.
6. REFERENCES
[1] T. G. Armstrong, A. Moffat, W. Webber, and J. Zobel. Improvements that don't add up: ad-hoc retrieval results since 1998. In CIKM, pages 601≠610, 2009.
[2] B. He, J. X. Huang, and X. Zhou. Modeling term proximity for probabilistic information retrieval models. Inf. Sci., 181(14):3017≠3031, 2011.
[3] Y. Lv and C. Zhai. Adaptive relevance feedback in information retrieval. In CIKM, pages 255≠264, 2009.
[4] J. Rocchio. Relevance feedback in information retrieval, pages 313≠323. Prentice-Hall Englewood Cliffs, 1971.
[5] C. J. van Rijsbergen. A theoretical basis for the use of co-occurence data in information retrieval. Journal of Documentation, 1977.
[6] Z. Ye, B. He, X. Huang, and H. Lin. Revisiting rocchio's relevance feedback algorithm for probabilistic models. In AAIRS, pages 151≠161, 2010.
[7] C. Zhai. Statistical language models for information retrieval a critical review. Found. Trends Inf. Retr., 2:137≠213, 2008.

1026

Active Query Selection for Learning Rankers

Mustafa Bilgic
Illinois Institute of Technology, Chicago, IL
mbilgic@iit.edu

ABSTRACT
Methods that reduce the amount of labeled data needed for training have focused more on selecting which documents to label than on which queries should be labeled. One exception to this [4] uses expected loss optimization (ELO) to estimate which queries should be selected but is limited to rankers that predict absolute graded relevance. In this work, we demonstrate how to easily adapt ELO to work with any ranker and show that estimating expected loss in DCG is more robust than NDCG even when the final performance measure is NDCG.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Search and Retrieval--active learning
Keywords
Active learning, query selection

1. INTRODUCTION
Research in information retrieval evaluation has examined how to construct minimal test collections [2], and the balance between the number of queries judged and the depth of judging [3]. With respect to training rankers, most work has focused on document selection [1] or balancing number of queries with depth of documents judged using random query selection [5]. In this paper, we focus on selecting queries in order to most rapidly increase ranker retrieval performance.
In particular, we focus on the application of expected loss optimization (ELO) to query selection. Long et al. [4] used ELO to select queries for training but relied on having a ranker that estimated absolute graded relevance. We generalize this to work with any ranker ≠ many of which induce a ranking but not absolute labels. To generalize to any ranker, we introduce a calibration phase over validation data.
In addition, although in theory ELO can be used with any performance measure for active learning, we show using DCG loss (as done in [4]) leads to better performance whether DCG or NDCG is used as the final evaluation of ranker performance. We provide evidence this is because ELO using DCG loss tends toward queries that have both more relevant examples and many degrees of relevance.

2. APPROACH

ELO suggests that, given a set of candidate queries C, one pick the query q  C for labeling where the expected loss is the greatest. Mathematically, we have:

max
qC

EP

(Y

|Xq

,D)

max M((Xq), y) - M(R(Xq), y)


(1)

Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

Paul N. Bennett
Microsoft Research, Redmond, WA
pauben@microsoft.com
where D is the given training data, and P (Y | Xq, D) is a distribution over graded relevance labels Y for the documents, Xq, to be ranked for the query q. M(r, y) is a retrieval performance measure such as DCG that can evaluate the quality of a ranking, r, for a set of documents given a particular labeling of the documents, y. (Xq) is simply a permutation of the documents and R(Xq) denotes the current ranking of the documents. For most retrieval performance measures, the inner max on the left-hand side of the difference is easily found by sorting from highest relevance to the lowest.
In order to estimate the label distribution P (Y | Xq, D) Long et al. [4] relied on training an ensemble of models to predict absolute graded relevance. We generalize ELO to work with any ranker by mapping the current ranking model to a distribution over graded labels. To do so, we introduce a calibration phase where a classification model is trained over the labels of the top k documents according to the ranker in the validation data.1 During active learning, the classification model is used to estimate the P (Y | xq, D) for each document xq  Xq. The quantity in Eq. 1 is then estimated through sampling of the labels from this distribution.
3. EXPERIMENTS
Like most active learning evaluation settings, we start with some labeled data D that is randomly chosen, train the models on D, pick a number of queries to be labeled from the candidate set C, add those to D, and repeat this process for a number of iterations. The performance of the active learning strategy in augmenting D is judged at each iteration by evaluating the current induced rankers on held-out test data. We perform 20 iterations of labeling and based on the findings reported in [5] we label 15 documents per query (or the maximum available). We repeat this process five times, each time starting with a different set of labeled data and report averages. We report both DCG@10 and NDCG@10 (one can use DCG for selection but NDCG for evaluation and so forth).
We use the publicly available Yahoo! LETOR challenge dataset that has 3 splits: we treat the train split as the candidate data C, utilize the validation split for tuning the rankers' parameters and training the calibration models, and use the test split for evaluation. We experiment with two rankers: one that does not produce absolute graded relevance (SVMRank) and one that does (Additive Regression (AR)). SVMRank labels 750 query-document pairs per iteration where AR labels only 150. The difference is due to AR having a steeper learning curve.
We examine four query/document selection methods: (1) select queries and documents randomly (rndQ-rndD); (2) se-
1We use the same validation data that is used for model parameter search ≠ this ensures our method does not require any additional labeled data.

1033

DCG@10 NDCG@10

Active Learning Experiments - DCG@10
13.8

Active Learning Experiments - NDCG@10
0.72

13.6
0.71 13.4

13.2 13.0 12.8 12.6 12.4 12.2
0

5

10

15

Number of active training batches

0.70

svm-rndQ-rndD

ar-rndQ-rndD

0.69

svm-rndQ-topD

ar-rndQ-topD

0.68

svm-dcqELOq

ar-dcqELOq

0.67

20

svm-ndcgELOq

0

ar-ndcgELOq

5

10

15

20

Number of active training batches

Figure 1: Comparison of query-selection strategies.

lect queries randomly and select the top documents according to the current ranker (rndQ-topD); (3) select queries according to ELO with DCG@10 as the selection measure and the top documents according to the current ranker (dcgELOq); (4) same as 3 but with NDCG@10 instead (ndcgELOq).

4. RESULTS AND DISCUSSION
Figure 1 shows the results for SVMRank (solid) and AR (dashed) when the evaluation measure is DCG@10 (left) and NDCG@10 (right).2 Error bars are standard error about the mean over the five trials. As reported elsewhere [4], selecting the top documents performs as well or better than selecting documents randomly. Note that regardless of whether evaluating by DCG or NDCG, using NDCG for selection (ndcgELOq) leads to the worst performance. In contrast, using DCG for selection leads to the best performance across both rankers and both evaluation measures. Finally, we note that the differences between methods are less significant when evaluated by NDCG than DCG. This suggests that while the learners are more effective ≠ finding more relevant results per query ≠ they are contributing to the marginal relevance for each query according to NDCG. The perceived impact on user utility will likely depend on the scenario and the degree to which the task is recall-oriented.

Rating Distribution - SVMRank
60%

50%

40%

Percentage

30%

20%

10%

0%

0

1

2

3

4

Ratings

rndQ-rndD rndQ-topD dcgELOq ndcgELOq

Figure 2: Rating distribution of selected queries.
Figure 2 displays the rating distribution of the training data collected at the last step of active learning as a per-
2SVMRank and AR are displayed together for space and to emphasize the similarity in trends. We are interested in comparisons within each and not across the two.

centage for the 15,000 labeled instances for SVMRank (the distribution trends for AR were nearly identical). Note that ndcgELOq selects far more irrelevant items than the other methods. This may seem surprising since NDCG selection is the same as DCG but normalized by the max estimate on the left-hand side of Eq. 1. However, for a poorly performing query with a single relevant document, NDCG's max will be 1 but current performance will be near zero. Thus, the selection method often selects queries with very few relevant documents. In contrast, dcgELOq not only obtains the largest percent of documents at the relevant side (labels 3,4) and fewest on the irrelevant side (label 0), it selects queries where a variety of relevance grades exist. This is consistent with the literature that biasing toward relevant documents is not sufficient in itself [1] ≠ one also needs a variety of relevance grades present.
5. SUMMARY
We presented a method that generalizes the applicability of ELO for query selection to any ranker. Our method also has the benefit of being less of a computational burden than training ensembles at each step prior to labeling. We also demonstrated that whether one cares about DCG or NDCG for performance, using DCG provides a more stable query selection method. This is because the nature of NDCG as a ratio pushes the selection toward queries that often have few relevant documents. In contrast, using DCG in the selection mechanism promotes queries that have more relevant documents, and the expected loss component ensures that there will be a variety of relevance grades ≠ since current performance is far below the max. These insights may be useful in developing new query selection methods.
6. REFERENCES
[1] J. Aslam, E. Kanoulas, V. Pavlu, S. Savev, and E. Yilmaz. Document selection methodologies for efficient and effective learning-to-rank. In SIGIR '09.
[2] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In SIGIR '06.
[3] B. Carterette, V. Pavlu, E. Kanoulas, J. Aslam, and J. Allan. If i had a million queries. In ECIR '09.
[4] B. Long, O. Chapelle, Y. Zhang, Y. Chang, Z. Zheng, and B. Tseng. Active learning for ranking through expected loss optimization. In SIGIR '10.
[5] E. Yilmaz and S. Robertson. Deep versus shallow judgments in learning to rank. In SIGIR '09.

1034

Clarity Re-Visited

Shay Hummel, Anna Shtok, Fiana Raiber, Oren Kurland
Faculty of Industrial Engineering and Management, Technion, Haifa 32000, Israel {hummels,annabel,fiana}@tx.technion.ac.il,
kurland@ie.technion.ac.il

David Carmel
IBM Research Lab, Haifa 31905, Israel carmel@il.ibm.com

ABSTRACT
We present a novel interpretation of Clarity [5], a widely used query performance predictor. While Clarity is commonly described as a measure of the "distance" between the language model of the top-retrieved documents and that of the collection, we show that it actually quantifies an additional property of the result list, namely, its diversity. This analysis, along with empirical evaluation, helps to explain the low prediction quality of Clarity for large-scale Web collections.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation
Keywords: query-performance prediction, Clarity
1. INTRODUCTION
Many query performance predictors were proposed over the years [2]. Clarity [5] is a well known, commonly used, state-of-the-art predictor that measures the "coherence" of the top-retrieved documents with respect to the collection. Specifically, the more distinguishable the language used in the retrieved documents from the general language used in the collection, the better the retrieval is assumed to be1. Clarity was shown to be highly effective for most TREC benchmarks [6]. However, low prediction quality is observed when using Clarity for large scale, noisy, Web corpora [1].
We present a novel formal analysis of Clarity that sheds some light on its underlying components and the properties of the result list of top-retrieved documents that it quantifies. While Clarity is commonly described as a measure of the "distance" between a language model induced from the result list and that induced from the collection, we show that Clarity actually quantifies an additional property of the result list, namely, its diversity. Our empirical analysis shows that the diversity of the result list has a negative correlation with retrieval performance for older TREC benchmarks and a positive correlation for the new ClueWeb collection. These findings, along with the formal analysis, help to explain the poor prediction quality of Clarity over ClueWeb.
1There are are several variants of clarity, among which is a pre-retrieval method (SCS [7]) that considers only the query and the corpus and not the retrieved documents.
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

In addition, the novel interpretation we present suggests new integration approaches of Clarity's building blocks.

2. INSIDE CLARITY

Let q and D denote a query and a corpus of documents, respectively. In what follows we use p(w|x) to denote the probability assigned to term w by a unigram (smoothed) language model induced from x.
The query likelihood (QL) retrieval method scores doc-
ument d by log p(q|d) d=ef log qiq p(qi|d), where qi is a term in q. Let Dq[k] denote the result list of the k highest ranked documents. The assumption behind Clarity is that the higher the divergence of a model of Dq[k] from that of the corpus, the higher the effectiveness of Dq[k] is, and thereby, the better the quality of the QL-based retrieval. The KL divergence between a relevance language model, R, induced from Dq[k], and a language model induced from D, is used to quantify this divergence. R is a weighted linear mixture of the models of documents in Dq[k] [5]. Accordingly, the Clarity of q is defined as:

Clarity(q) d=ef KL p(∑|R) p(∑|D) =

p(w|R)

log

p(w|R) p(w|D)

.

w

As is the case for any two probability distributions, we can write the KL divergence as follows:

KL p(∑|R) p(∑|D) = CE p(∑|R) p(∑|D) -H (p(∑|R)) ;

CE is the cross entropy between R and D,
CE p(∑|R) p(∑|D) d=ef - p(w|R) log p(w|D);
w
H is the entropy of the relevance model,
H (p(∑|R)) d=ef - p(w|R) log p(w|R).
w
Under this decomposition, Clarity integrates two measures (building blocks). The first is the "distance" of R from the corpus, as measured by the cross entropy. The more distant R from D, the higher the cross entropy is. We use CDistance (for "corpus distance") to refer to the cross entropy between R and D.
The second measure used by Clarity is the entropy of R. High entropy means that R assigns relatively low weights (i.e., probabilities) to a large number of terms; thereby, Dq[k] is highly diverse. In contrast, low entropy means that only

1039

a few terms are highly weighted, hence Dq[k] is more focused.

setup that affects the estimation of the corpus language

We use LDiversity (for "list diversity") to refer to R's en-

model. Evidently, spam removal did not improve prediction

tropy. Next we study the prediction quality of each of these

quality over ClueWeb.

building blocks and compare it with that of Clarity which

Second, LDiversity and retrieval performance have posi-

amounts to their equal-weight linear interpolation:

tive correlation for "ClueWebs", yet negative correlation is

Clarity(q) = CDistance(q) - LDiversity(q). (1)

observed for "SmallScales". We presume that list coherence can attest to improved retrieval effectiveness, as is implied

3. EXPERIMENTS

by the findings for "SmallScales", which are mainly composed of unambiguous (coherent) queries. On the other

We conducted experiments using the following TREC bench- hand, list diversity might correspond to improved retrieval

marks (disks and topics are indicated in the parentheses):

performance, as is implied by the findings for "ClueWebs",

TREC4 (disks 2-3; 201-250), TREC5 (disks 2,4; 251-300),

for ambiguous queries, if it attests to coverage of various

WT10G (WT10G; 451-550), Robust (disks 4-5 - {CR}; 301-

query aspects. It was found that for Clue09 topical diver-

450, 601-700), GOV2 (GOV2; 701-850), and the ClueWeb

sity and retrieval performance are strongly correlated [3].

collection (category B). Two sets of topics were used for

Following the observations made above, we can explain

ClueWeb: Clue09 (1-50) and Clue10 (51-100). We applied

the low effectiveness of Clarity for "ClueWebs"; Clarity, as

Porter stemming and stopword removal upon queries and

presented in Equation 1, is the subtraction of prediction

documents using the Lemur/Indri toolkit.

values assigned by two predictors which are both positively

Previous work hypothesized that the low prediction qual-

correlated with retrieval effectiveness. Usually, two predic-

ity of Clarity over Web collections is due to the large amount

tors that are positively correlated with retrieval performance

of noise (e.g. spam) [6]. To address spam effects in ClueWeb,

are integrated by multiplication or summation [2]. Thus,

we filtered out the spammiest documents from the result lists

the subtractive integration of CDistance and LDiversity,

(those assigned a spam score below 50 by Waterloo's clas-

as implemented by Clarity, yields low quality prediction over

sifier [4]) and retained the original ranking for the residual

"ClueWebs".

corpus. Thus, we get two additional experimental setups for

ClueWeb: Clue09+SpRM and Clue10+SpRM.

4. SUMMARY

We use the predictors to predict the performance of the QL retrieval method specified above; unigram Dirichlet-smoothed document language models are used with the smoothing parameter set to 1000. We study three predictors: CDistance, LDiversity, and Clarity which interpolates the two (see Equation 1). Following the common practice to evaluating prediction quality [2], we report Pearson's correlation between the values assigned by the predictor and retrieval effectiveness measured by average precision computed using TREC's relevance judgments. The size of the result list, k, was set to 500 following previous recommendations [6]. The relevance models were clipped to use only the 100 highest

We showed that Clarity amounts to an equal weight interpolation of two predictors; one measures the "distance" of the result list from the collection, while the second measures the list's "diversity". We used this formal finding to help explain the low prediction quality of Clarity over ClueWeb, in contrast to its high effectiveness over other TREC benchmarks. Preliminary results of using non-equal weights for the interpolation mentioned above, and independently optimizing free-parameter values for each predictor, attest to the merits of these future directions. (Actual numbers are omitted due to space considerations.)

weighted terms. The language models of documents used to construct the relevance model were not smoothed.

Acknowledgments. We thank the reviewers for their com-
ments. This paper is based upon work supported in part

by the Israel Science Foundation under grant no. 557/09,

by IBM's SUR award, by an IBM Ph.D. Fellowship and by Miriam and Aaron Gutwirth Memorial Fellowship. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsors.

Figure 1: Comparing the prediction quality of Clarity and its building blocks.
Figure 1 presents our main results. To simplify the presentation, we refer to the ClueWeb benchmarks as "ClueWebs" (on the left of the graph) and to all the other benchmarks as "SmallScales" (on the right). The differences of the patterns observed for "ClueWebs" and "SmallScales" are as follows. First, CDistance is not very effective over "ClueWebs" in comparison to "SmallScales". We attribute this finding to the low quality of the collection statistics in a noisy Web

5. REFERENCES
[1] N. Balasubramanian, G. Kumaran, and V. R. Carvalho. Predicting query performance on the web. In Proceedings of SIGIR, pages 785≠786, 2010.
[2] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis lectures on information concepts, retrieval, and services. Morgan & Claypool, 2010.
[3] C. L. A. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 Web track. In Proceedings of TREC, 2009.
[4] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. CoRR, abs/1004.5168, 2010.
[5] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proceedings of SIGIR, pages 299≠306, 2002.
[6] C. Hauff, V. Murdock, and R. Baeza-Yates. Improved query difficulty prediction for the web. In Proceedings of CIKM, pages 439≠448, 2008.
[7] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In Proceedings of SPIRE, pages 43≠54, 2004.

1040

Time Drives Interaction: Simulating Sessions in Diverse Searching Environments
Feza Baskaya, Heikki Keskustalo, Kalervo J‰rvelin
School of Information Sciences FI-33014
University of Tampere, Finland
{ Feza.Baskaya, Heikki.Keskustalo, Kalervo.Jarvelin }@uta.fi

ABSTRACT
Real life information retrieval takes place in sessions, where users search by iterating between various cognitive, perceptual and motor subtasks through an interactive interface. The sessions may follow diverse strategies, which, together with the interface characteristics, affect user effort (cost), experience and session effectiveness. In this paper we propose a pragmatic evaluation approach based on scenarios with explicit subtask costs. We study the limits of effectiveness of diverse interactive searching strategies in two searching environments (the scenarios) under overall cost constraints. This is based on a comprehensive simulation of 20 million sessions in each scenario. We analyze the effectiveness of the session strategies over time, and the properties of the most and the least effective sessions in each case. Furthermore, we will also contrast the proposed evaluation approach with the traditional one, rank based evaluation, and show how the latter may hide essential factors that affect users' performance and satisfaction - and gives even counter-intuitive results.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search process
Keywords
Session-based evaluation, simulation, time-based evaluation
1. INTRODUCTION
Interaction through search interface and environment greatly affects the user behavior, user experience, and user performance.
Many earlier studies have extended the traditional Cranfield view of IR and discussed various aspects of interactive searching (see, e.g., [4], [5], [6], [13], [21]), user interaction, and query modification (see, e.g., [3], [10], [14], [28]).
During interaction the user selects between subtasks, e.g., whether to scan the result or launch a new query instead, and how to construct the query. Such selections obviously affect session gains. However, different subtasks also have costs, e.g., they take time. This is important because real life IR often takes place under (time)
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12-16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08...$10.00.

constraints. In particular, keeping the overall session cost reasonable may be essential for end users.
The costs of subtasks may vary for many reasons between searching environments. For example, regarding the query side, small devices and touch screens are inconvenient for typing [11]. Recently, novel kinds of searching devices, including personal phone-based mobile devices, have become increasingly popular.
In order to minimize the overall session costs, a mobile phone user might e.g., avoid typing and prefer result scanning. Low input costs might change the situation from the user's point of view, leading to longer queries. Therefore, if we assume two users having identical needs and identical cost constraints regarding the overall session time, it is possible that different devices render different subtask combinations optimal in searching.
Traditional IR evaluation focuses on the quality of the ranked output. In this view, the costs of posing queries are non-problematic, even uninteresting. In this paper we will utilize simple scenarios to bring time factors into the research setting. Scenarios formalize and quantify the gains and costs of interactive sessions. We construct two cases ≠ a personal desktop computer (PC) and a smart phone (SP) case, with subtask costs derived from the literature. We will simulate session interaction involving multiple queries based on prototypical but empirically grounded query modification strategies using a test collection. We then explore the effectiveness of searching via the exhaustive set of querying-scanning combinations possible, and evaluate the effectiveness of both scenarios in terms of Cumulated Gain (CG) [16] under time constraint (overall session time). We use non-normalized metrics, because normalized metrics may yield misleading results, especially if time is taken into account.
Early papers on IR evaluation had a comprehensive approach to interactive IR evaluation. Cleverdon et al. [8] pointed out, among others, physical and intellectual user effort as an important factor in IR evaluation. Salton [24] identified user effort measures in the context of IR evaluation. More recently Su [30] gave a comparison of 20 different evaluation measures for interactive IR, including actual cost of search, several utility measures, and worth of search results vs. time expended. The interactive aspect of IR requires attention because previous studies have repeatedly shown that discrepancy exists between interactive and non-interactive evaluation results. Hersh et al. [12] showed that a weighting scheme giving maximum improvement over the baseline in non-interactive batch evaluation failed to surpass others when real users performed a simulated task. Turpin and Hersh [31] observed that a system superior over the baseline in batch evaluation, measured by mean

105

average precision, was not superior in an interactive situation. Turpin and Scholer [32] found no significant relationship between the search engine effectiveness measured by mean average precision and real user success in a precision-oriented task. Smith and Kantor [25] observed that users of degraded systems were as successful as those using non-degraded systems. They suggested that users achieved this by altering their behavior.
Dunlop [9] proposed "time-to-view" graphs, which incorporate user interface and system as well as the time component into the same framework for evaluation of system effectiveness. However he did not analyze time constraints, query modification strategies and different devices.
Smucker [26] brought time factors into the traditional Cranfield setting by augmenting it with the use of the GOMS [7] model (acronym for Goals, Operators, Methods, and Selections). He suggests a user model for IR where the search process is seen as a sequence of actions (e.g., typing; clicking; evaluating a summary; waiting for the results to load) with associated times and probabilities (e.g., whether the simulated user will click on a relevant summary). He used the model in a simulated study to demonstrate the impact of changes in the IR system interface (e.g., when the speed and accuracy of the summary evaluation is varied) on user performance (the number of relevant documents read within a given time frame). While his experiment was limited to single query situations, the approach can be extended to multiple query scenarios, e.g., for computing the costs of specific query reformulations.
Azzopardi [2] addressed the cost aspect by treating interactive IR as an economical problem and studied the trade-off between querying and browsing while maintaining a given level of normalized CG (NCG) [15] in sessions. His analysis focused on querying ≠ scanning depth combinations for various formal retrieval methods that deliver a given level of NCG.
Our approach in the present paper differs from earlier studies. Our study is based on the simulation of multiple-query sessions generated with various query modification and scanning strategies in different searching environments.
In the next section we start by discussing session generation with costs, and present the research questions. In Section 3 we describe the research setting. In Section 4 we will run an experiment in a test collection based on scenarios and discuss the results. We close the paper by discussing the significance of our approach in the last section.
2. CONSTRUCTION OF SESSIONS
A use case is "a relatively informal description of system's behavior and usage, intended to capture the functional requirements of the system by describing the interaction between the outside actors and the system, to reach the goal of the primary actor" [19]. We utilize simplified use cases, which we call scenarios, to present an alternative way to look at the effectiveness of IR approaches based on the user viewpoint. The next subsections will first explain the session generation formally, and then explain the specific query modification (QM) and scanning strategies utilized in the scenarios.
2.1 Session generation
For session simulation, we first formally generate all possible sessions under constraints. We will represent sessions as sequences of actions with costs. For example the tuple <(a1,c1), (a2,c2), ..., (an,cn)> is a session of n actions and each pair (ai,ci) in the session

representation represents an action ai and its cost ci. The elementary action types are:
∑ Initial query, represented as (`iq', ic), where `iq' is the action label and ic (R) the cost in seconds.
∑ Query reformulation (`q', qc), where `q' is the action label and qc (R) the cost in seconds.
∑ Document snippet scan (`s', sc), where `s' is the action label and sc (R) the cost in seconds.
∑ Next page request (`n', nc), where `n' is the action label and nc (R) the cost in seconds.
The constraints are:
∑ MaxSLen, maximum session length in terms of elementary actions, here 50 actions.
∑ MaxSCost, maximum session cost (seconds), here 60, 90 or 120 seconds.
∑ A session always begins with an initial query. ∑ All queries (initial and reformulation) are followed by at least
one snippet scan. ∑ The longest scan sequence we consider is a scan of 10 snippets
(i.e. one typical result page).
In effect, the shortest possible session therefore is initial action IA = <(iq, ic),(s, sc)>, consisting of an initial query followed by the scan of one snippet (with costs). To generate longer sessions, we define the set NA for the possible subsequent actions:
NA = {<(q, qc), (s, sc)>, <(s, sc)>, <(n,nc), (s,sc)>}
Note here that the next actions are tuples of one or two elementary actions; a scan may appear individually, while a reformulation / next page requires a scan to follow. Sessions are generated by concatenating next actions to the initial action. Concatenation of two tuples S1 = <e1, e2 ,..., en> and S2 = <f1, f2 ,..., fm> is denoted by < S1, S2 > = <e1, e2 ,..., en, f1, f2 ,..., fm>. This operation generalizes over a set of session tuples Si, denoted as:
◊i=1...n Si = <<... <<S1, S2>, S3>, ...>, Sn>.
The cost of a session S is, informally, the sum of its action costs. More formally, we derive this cost by the function s-cost as follows:
 s-cost(S)= (a,c)  S c
[N.B. we extend the definition of the set membership operator from sets to tuple components in an obvious way.] For example, the cost of the session S1 = <(`iq', ic),(`s', sc), (`q', qc),(`s', sc)> is scost(S1) = ic+sc+qc+sc.
The condition of maximum scan length of n in a session S is enforced by the Boolean predicate max-scan(S, n). It yields 'true' for a given session S if S does not contain a subsequence of scan actions <(`s', sc)1, (`s', sc)2, ..., (`s', sc)n>, otherwise `false' (formal definition here omitted for brevity).
To generate sessions, we first generate all sessions up to the max number of actions MaxSLen. This session set is MLS:
 ◊ MLS = i=1...MaxSLen{<IA, j=1...i acj > | acj  NA}
We then select the subset of sessions fulfilling the time constraint MaxSCost and the scan length constraint as follows. All sessions in MLS with maximal cost MaxSCost (or less) form the set MCS:
MCS = {S  MLS | s-cost(S)  MaxSCost  max-scan(S, 11)}

106

Note that this approach does not define the query contents or modifications in sessions. However, it keeps them within constraints and guarantees that the last action is a document snippet scan. In our experiments, we excluded the next page action from NA due to the max scan length constraint of 10. The next two sub-sections explain and justify the query modification and scanning strategies used in the experiment.
2.2 Query Modification Strategies
We will simulate interactive search sessions as querying-scanning iterations having a goal, a procedure to reach the goal, and constraints regarding the procedure. We define the goal in terms of maximizing CG during the session under the constraint on the overall session time available. The procedure is defined in terms of QM and scanning strategies.
The previous section did not define any particular QM strategies. We assume that a set of individual words {w1, w2, w3, w4, w5} is available for each particular topic, and QM strategies determine how elements from this set are combined to form queries (either the initial query, or one of the subsequent queries). In other words, given a set of individual search words for the topic, the QM strategy defines how to form a sequence of queries.
Five QM strategies (S1 ≠ S5) were used in the experiment. These prototypical strategies are based on term level changes which have grounding in the observed real life behavior and are justified by literature (see [1], [20], [33]):
∑ S1: an initial one-word query (w1) is followed by repeatedly varying the search word : Q1: w1 -> Q2: w2 ->Q3: w3 ->Q4: w4 ->Q5: w5
∑ S2: an initial two-word query (w1 w2) is followed by queries formed by repeatedly varying the second word : Q1: w1 w2 -> Q2: w1 w3 -> Q3: w1 w4 -> Q4: w1 w5
∑ S3: an initial three-word query (w1 w2 w3) is followed by queries formed by repeatedly varying the third word : Q1: w1 w2 w3 -> Q2: w1 w2 w4 -> Q3: w1 w2 w5
∑ S4: an initial one-word query (w1) is followed by adding one word to each subsequent query : Q1: w1 -> Q2: w1 w2 -> Q3:w1 w2 w3 -> Q4:w1 w2 w3 w4 -> ...
∑ S5: an initial two-word query (w1 w2) is followed by adding one word to each subsequent query : Q1: w1 w2 -> Q2: w1 w2 w3 -> Q3: w1 w2 w3 w4 -> ...
This means that the sessions consist of at most 3 to 5 queries; this reflects real life behavior [22]. Generally speaking, constructing a query entails a cost due to the cognitive user load plus the edit costs. We will return to the cost factors in Section 2.4.
2.3 Scanning Strategies
The user may simply scan one or more documents after each query before formulating the next query candidate or ending the session. After a single query Qi a sequence of one or more document snippets may be scanned:
Q1->s11->s12->s13->...
The cost of this session manifests as:
qc1 + sc11 + sc12 + sc13 + ...

When a set of queries is available for one topic, the user can scan varying numbers of document snippets after any particular query, leading to a vast number of possible querying-scanning sessions, e.g.,
Q1->s11->Q2->s21->Q3->s31-> ... or
Q1->s11->s12->Q2->s21->... or
Q1->s11->s12->s13->Q2->s21-> s22->Q3->s31->... etc.
In real life a session typically continues until the user has found what he was looking for, at least partially, and/or when he runs out of time or queries. The scanning lengths may fluctuate for many reasons. In this paper we study the properties of optimal and less optimal interactive behaviors in sessions below the given overall time constraint. Therefore we produced all possible sessions as follows. For all five QM strategies we formed all possible combinations of scanning lengths exhaustively (from 1 to 10 documents) using a sequence of all possible queries available per topic (cf. equation MCS in Section 2.1). We focus on the top documents because only few top documents may be inspected by the user in real life [14], [23], and only these may matter for the user [1]. As we had 5 words for each topic, sessions had at most 5 queries, controlled by the QM strategy and time constraint. As the query words were ordered by quality (see 3.1), the query words were used in that particular order, not permuted.
2.4 Cost Factors
There is a cost involved with the subtasks of formulating the query and scanning. We assume that the absolute cost is partially determined by the scenario. Empirical studies show that it takes significantly more time to enter queries by using a small smart phone keypad than by using an ordinary keyboard [17]. To study the significance of subtask costs under overall session cost constraint we define two scenarios, i.e., a Desktop PC scenario (PC) and a Smart phone scenario (SP). These scenarios have different subtask costs. This is justified because the properties of the devices partially determine the subtask costs [17].
Obviously, also forming queries under different QM strategies S1 ≠ S5 have very different relative costs. All queries in strategies S1, S2 and S3 have a fixed query length in sessions (one, two or three words, correspondingly) while in strategies S4 and S5 the queries grow longer. In real life the typing speed is affected by, e.g., the experience and knowledge of the person, the size of the keyboard, the layout of the keyboard (e.g., nine-key multi-tap vs. qwerty keyboard) [17], [18], and whether predictive text feed is available and used. We used literature to derive the cost values in scenarios PC and SP regarding the initial query cost and the subsequent query cost (Table 1).The query costs in S1 ≠ S5 in the Desktop PC case are based on the typing costs of 3.0 seconds per word. The corresponding Smart Phone costs are based on [17]. The authors performed a large-scale log analysis of cell phone usage and observed that an average smart phone query length was 2.56 words and the average query-entry time was 39.8 seconds (average typing cost of 15.5 seconds per word). We assume in our simulations that the cost of adding one word to a query (that is, S4 and S5) or replacing one word at the end of the previous query (that is, S1, S2, S3) is a constant, i.e., either 3.0 or 15.5 seconds depending on the scenario.

107

Table 1. Average subtask costs (in seconds) of five QM strategies (S1-S5) for two scenarios: (i) initial query cost, (ii) subsequent query cost, and (iii) the cost of scanning one document snippet

Scenario 1: Desktop PC

QM strategy

S1

S2

S3

S4

S5

Initial query

3.0

6.0

9.0

3.0

6.0

Subsequent query

3.0

3.0

3.0

3.0

3.0

Snip. scanning cost 3.0

3.0

3.0

3.0

3.0

Scenario 2: Smart Phone

QM Strategy

S1

S2

S3

S4

S5

Initial query

15.5 31.0

46.5

15.5

31.0

Subsequent query

15.5 15.5

15.5

15.5

15.5

Snip. scanning cost 3.0

3.0

3.0

3.0

3.0

To check whether these costs are reasonable we also performed a small-scale experiment where four test persons typed the initial and subsequent queries according to strategies S1-S5 using two types of interfaces (Desktop PC and Smart Phone) for three test topics. The experiment corroborated that the query time estimates were reasonable.
The document snippet scanning costs in real life are affected by the motor and perceptual costs plus the cognitive load related to the task. In this study we assume that the document snippet scanning cost is constant in both scenarios and across the searching strategies S1 ≠ S5 (see Table 1). In the SP case we defined a scanning cost of three seconds per snippet. We justify this by an observation by Kamvar and Baluja [17] that the average cell phone user used 30 seconds to scan the search results before selecting one, after receiving 10 search results. For the snippet scanning costs in the Desktop PC case we decided to use the same value. Obviously, our methodology is well-suited to experiment with different costs. The overall cost constraint of a session was defined as 60, 90, or 120 seconds. In the simulations all subtasks (querying and scanning) had to be performed within this time constraint. We excluded the eventual thinking time in producing query words.
2.5 Research questions
We set forth the following research questions:
1. How effective are the five QM strategies (S1 to S5) in terms of CG when we compare the Desktop PC and the Smart Phone scenarios under overall time constraint?
2. What are the characteristics of the best and the worst sessions achieved in terms of average scan length, and average number of queries?
3. How stable are the observed trends when the overall time constraint changes? Can we recommend QM strategies based on the scenario - what to do, and what not to do, assuming a specific time constraint?
4. What is proper evaluation methodology when time is part of the evaluation setting?

3. RESEARCH SETTING
3.1 Test Collection and Search Engine
We used a subset of the TREC 7-8 document collection with 41 topics for the experiment. The documents have graded relevance assessments on a four-point scale with respect to the topics. [27]
The present authors obtained query words for session generation for the test topics from [20] where the authors used real test persons to suggest keywords of various lengths for queries on the 41 topics. The test persons were asked to directly propose good search words from topic descriptions (descriptions and narratives) in a structured way. Among others, they produced query versions of various lengths: (i) one word, (ii) two words, and (iii) three or more words. These were collected per topic as ordered word lists of 5 words for each topic. During the query formulation experiment the test persons did not interact with a real retrieval system. While this may have affected negatively the quality of queries, Keskustalo and colleagues [20] suggest that the test persons were able to construct the query words in a descending order of effectiveness.
Retrieval system Lemur with language modeling and two-stage smoothing options was used in the experiment.

3.2 Session Data
For each topic we utilized a minimum of 1 query and a maximum of 5 queries in each session. A minimum of 1 document snippet and a maximum of 10 document snippets were scanned per query.
In Table 2, the number of possible scanning paths is given for consecutive queries. If the session comprises at most 2 queries, first there are 10 possible paths after the first query, and for every path there are 10 possible paths after the second query. So the combinations of these at most two queries sum up to 10+10*10 =110 possible paths. In our experiment design, users can pose up to 5 queries depending on session strategy; this presents altogether 111,110 possible paths, which are taken into consideration.
Table 2. Number of possible sessions per number of queries, when at most 10 documents can be scanned after each query

Queries Possible sessions

1 2 10 100

3 1000

4 10,000

5 100,000

 111,110

We ran all 41 topics * 5 QM strategies * Q queries, Q  {3, 4, 5} depending on the strategy, and collected their results. Then we generated all 111K possible sessions from the query results, pruned the ones exceeding the time constraint in each scenario, and by using the recall base (qrels), evaluated the CG of the scanned snippets for each session. For example, for the session Q1->s11-> s12>s13->Q2->s21->s22->Q3->s31, the CG is calculated on the basis of the snippet sequence s11, s12, s13, s21, s22, s31. Altogether about 45 million sessions (41 topics * 5 QM strategies* 111,110 possible scanning sessions * 2 scenarios) were evaluated. As the collection has graded relevance assessments, CG was incremented by 3 points for the highly relevant documents, 2 points for the fairly relevant documents and 1 point for the marginal ones. Whenever a duplicate was retrieved by a subsequent query in a session, its gain was nullified. Finally, we ranked all sessions within a topic and a strategy by their CG scores. In this data set per topic, strategy and time constraint, each session is represented by its tuple of actions (see 2.1) and its gain.

108

3.3 Data Analysis
The action tuples allow the analysis of the number of queries and the length of each scan in a session. The ranked order of sessions allows identification of the best and the worst session across topics, strategy, scenario, and time constraint. We analyze the sets of 10 best sessions, and 10 worst sessions per topic as averages instead the single best or worst session. This approach smoothes minor random variations in human behavior and thus the set of top (bottom) 10 sessions provide more reliable measurements compared to the single best/worst session when we explore their properties under varying conditions. Since the present study does not aim to prove one retrieval method better than another, we report the findings without tests on significance of statistical differences.
4. EXPERIMENTS
4.1 Results for the 60 Seconds Time Frame
First we discuss the CG results under the two scenarios, PC and SP. We present the best case and worst case results regarding all querying-scanning sessions based on the five QM strategies: S1 (sequence of individual words); S2 (two-words; last word varied); S3 (three-words; last word varied); S4 (incremental extension starting from one word); and S5 (incremental extension starting from two words). Table 3 gives the averaged CG values, the number of queries and scans per query for 10 best and 10 worst cases for every QM strategy for the 60 second time constraint, which are utilized in the following figures in this section.

Table 3. Averaged CG, number of Queries (#q) and Scans per Query (s/q) for scenarios PC and SP, for 5 strategies for the 10 best (b) and 10 worst (w) sessions, time constraint 60 seconds

Time Environment (60 s) best/worst

b

PC

avg.

w

CG

b

SP

w

b

PC

avg.

w

#q

b

SP

w

b

PC

avg.

w

s/q

b

SP

w

Query Modification Strategies

S1 S2

S3

S4

S5

4.9 8.3 8.5 7.9 8.1

1.2 5.4 7.1 4.7 6.0

2.8 3.6 2.3 4.4 3.7

1.2 2.8 2.3 2.0 2.9

2.7 2.6 2.5 4.2 3.0

5.0 4.0 3.0 5.0 4.0

1.9 1.5 1.0 2.0 1.5

2.7 1.7 1.0 2.6 1.7

6.4 6.3 6.2 3.8 5.3

3.0 3.8 5.0 3.0 3.8

4.7 3.6 2.5 4.0 3.6

1.6 2.5 2.5 1.7 2.5

Table 4 and Table 5 are equivalent to Table 3 but for the time constraints 90 and 120 seconds, respectively. Figure 1 shows the CG of the best (worst) sessions for each strategy in both scenarios under the overall cost constraint of 60 seconds. Note that all sessions require 60 seconds or less if no further action fits in (the absolutely worst imaginable session without any time requirement, in terms of the CG, would naturally consist of the initial action (IA) only). In other words, regarding the worst results, we report CG for the worst possible 60 second performance.

When the best sessions of the PC and SP cases are compared in Figure 1, the PC case performs at a considerably higher level (average CG is above 8 in three strategies) than the SP case (average CG is below 5 in all strategies).
Fig 1. Cumulated Gain under cost constraint of 60 seconds. Second, when the best and the worst cases are compared within the scenarios, not surprisingly, the best case results are typically clearly better than the worst case results except in SP case for S3. In the latter case both the best and the worst session may not contain more than one query because of high query entry cost. Third, among the best cases for PC the strategies S2 and S3 are almost equally good. For the SP case, the strategy S2 (varying the second word), S4 (extending from one word), and S5 (extending from two words) lead to much higher gain than S1 and S3. An interesting trade-off in the SP scenario can be observed when the scanning length is considered. In the best case the gain reached increases from S1 to S2. However, the average scanning length decreases (Fig. 2). In other words, a better result is achieved using the longer queries although a smaller number of documents are scanned on the average; the ranking is simply better.
Fig 2. Average number of scanned document snippets per query under cost constraint of 60 seconds.

109

Table 4. Averaged CG, number of Queries (#q) and Scans per Query (s/q) for scenarios PC and SP, for 5 strategies for the 10 best (b) and 10 worst (w) sessions, time constraint 90 seconds

Time (90 s)
avg. CG
avg. #q
avg. s/q

Environment

Query Modification Strategies

best/worst S1 S2

S3

S4

S5

b 5.8 10.5 10.4 10.4 10.5

PC w 1.9 7.6

10.3

7.6

8.7

b 5.0 7.3 6.0 7.1 7.0

SP w 0.9 2.5

3.0

2.5

2.6

b 3.8 3.5 3.0 5.0 4.0

PC w 5.0 4.0

3.0

5.0

4.0

b 2.0 2.0 1.9 2.5 2.0

SP w 4.1 3.4

2.6

4.1

3.4

b 6.9 7.3 8.3 5.0 6.3

PC w 5.0 6.3

8.3

5.0

6.3

b 8.8 6.8 4.7 6.3 6.8

SP w 1.6 1.4

1.7

1.4

1.4

4.2 Results for the 90 Seconds Time Frame
Figure 3 shows the CG results when the sessions take 90 seconds. In this case, the observations comply with the 60 second case. Difference between S3's best and worst CG values is closing in the PC scenario; this is because of lacking further scanning options, there is now enough time to scan almost all 10 documents for each query. S3 strategy has a maximum of 3 queries to execute before the 5 keywords run out. This in turn confines the possible scanning space. It is also conspicuous that the difference between best and worst CG values in SP case is much larger than in PC case.

Fig 3. Cumulated Gain under cost constraint of 90 seconds.
When scanning in the best sessions of the PC and SP cases is compared (Fig. 4), we notice that even though the scans per query values for SP case are higher than or similar to the PC case, the CG values are always poorer (Fig. 3). This is due to the smaller number of posed queries in SP case than in PC case. This follows from the trade-off between query vs. scan costs.

Fig 4. Average number of scanned document snippets per query under cost constraint of 90 seconds.

Interestingly, the difference between the best and the worst sessions both in terms of gain and average scan length remains great in SP case, but fades away in PC case. In the latter, 90 seconds allows the searcher to launch almost all queries and scan the best results in all cases. When the results are compared between different strategies, the strategy S4 with on average 5 scans in PC case and approximately 6 scans in SP case (Fig. 4) produce similar CG values as the other QM strategies (Fig. 3). Again, larger queries yield better rankings. On the other hand, S3 in SP case has less than 5 scans per query, and still achieves slightly better CG results than S1 strategy.

Table 5. Averaged CG, number of Queries (#q) and Scans per Query (s/q) for scenarios PC and SP, for 5 strategies for the 10 best (b) and 10 worst (w) sessions, time constraint 120 seconds

Time Environment Query Modification Strategies

(120 s) best/worst S1 S2

S3

S4

S5

PC b 6.4 11.1 11.4 11.7 11.5

avg.

w 3.4 10.5 11.4 10.0 10.9

CG

SP b 5.6 9.1

9.2

9.1

8.9

w 1.1 5.1 6.7 4.5 5.7

b 4.8 4.0 3.0 5.0 4.0

PC

avg.

w 5.0 4.0 3.0 5.0 4.0

#q

b 3.0 2.9 2.0 3.0 2.9

SP

w 5.0 4.0 3.0 5.0 4.0

b 7.3 8.8 10.0 7.0 8.8

PC

avg.

w 7.0 8.8 10.0 7.0 8.8

s/q

SP b 7.6 6.6

8.8

7.6

6.5

w 2.8 3.5 4.7 2.8 3.5

4.3 Results for the 120 Seconds Time Frame
Figure 5 shows the CG values under the cost constraint of 120 seconds. In the PC case, the gaps between the best and worst CG values are diminishing. This can be explained so that every strategy except S1and S4 has enough time to pose all the queries and employ much scanning. According to the experiment design, worst cases must also use up the allocated time, and this results in that there is enough time to launch all queries and scan the results. When the best sessions of the PC and SP cases are compared, we notice that there are no large differences. Again, in Figure 6 we can see as many scans per query (S/Q) for S1 and S4 in the SP case as in the

110

PC case for best sessions. Besides all the strategies for PC case show the same S/Q for 10 best and 10 worst sessions. Although in SP case S/Q values diverge from each other, Figure 6 exhibits similar patterns as Figure 4. From Figure 5 one can conclude that, if there is enough time for searching, one should use at least two word queries for good results. If the queries are of lower quality like S1, then scanning matters. In short, the more you scan, the more you get.
Fig 5. Cumulated Gain under cost constraint of 120 seconds.
Fig 6. Average number of scanned document snippets per query under cost constraint of 120 seconds.
5. DISCUSSION
We had three empirical and one methodological research question. The three empirical ones were about effectiveness of different QM strategies under time constraints, characteristics of the best and the worst QM sessions, and the stability of the observed trends. The methodological one was about proper evaluation of sessions under time constraints. We will consider each of the questions below. Strategy effectiveness. Given a stringent time frame in the PC scenario, the user cannot use the entire vocabulary (all queries) and perform exhaustive scanning for all queries. Short queries (strategy S1) are clearly inferior regarding session effectiveness. It seems reasonable to invest on two to three word queries (S2, S3) because the evidence thereby added for ranking significantly improves the quality of the results. This can also be seen in strategies S4 and S5, when they have enough time to advance beyond the first query. When more time is allocated to searching, the weaker strategies catch up because there is more time for scanning the results and the weaker ranking effectiveness is not that critical.

In the SP scenario the rules of the game change a bit. In a stringent time frame there is no time for tedious query input, and one must compromise toward short scanning of weaker quality rankings. The more effective strategies cannot be applied at all due to high query input cost. Again, when more time is allocated, weaker strategies catch up. In the longest sessions of S2-S5, the gap between the best vs. worst sessions begins to close.
Session characteristics. In the PC scenario, under stringent time constraints, the best sessions involved less queries and longer scans than the worst sessions (Table 3). However, as the time allocation grows, the differences disappear. Between the best strategies in the PC case, both the number of queries and the average scan lengths increase as time allocation grows (Tables 3-5). Correspondingly, in the worst sessions, the number of queries does not change as time grows, but the scan lengths grow. This is because the worst sessions consume all possible queries even under the shortest time frame. Similarity with best sessions grows.
In the SP scenario, under stringent time constraints, the best sessions also involved less queries and longer scans than the worst sessions (Table 3). As the time allocation grows, the differences remain, probably due to shortage of time even in the longer sessions. Between the best strategies in the SP case, both the number of queries and the average scan lengths increase as time allocation grows, the latter dramatically between 60 and 90 seconds (Tables 34). Correspondingly, in the worst sessions, the number of queries grows along time, but the scan lengths remain low. The worst behavior here means investing the effort in query input. Also here there were interesting differences in scan lengths between queries in sessions.
All in all, if time allows, two to three first query words that one identifies, followed by a longer scan, seem to provide reasonable performance, no matter what the strategy among S2-S5 is.
Effect of time. With limited time allowance, it seems important to make a good compromise between providing evidence for ranking (longer queries) and scanning the search results. The compromise depends on the overall cost levels related to the stringency of the time frame and on the relations between cost types. This depends on the searching device. Expensive input favors scanning at length, cheap input favors better queries. The more time is available the less it matters how one searches ≠ there will be time to identify the relevant documents.
Evaluation methodology. Typical IR evaluation metrics are based on the quality of ranking alone. In session-based evaluation they must be applied with great care because they may be insufficient or even misleading. They may be partially insensitive to the user's experience and observed costs and benefits. This is particularly critical, when user's costs (time expenditure) are taken into account and the metric employs normalization, i.e. scaling the measurements to a predefined range such as [0, 1]. For example, the popular NDCG metric [15] and its non-discounted counterpart NCG should be avoided in any comparisons between searching environments, and between strategies within a given searching environment when input costs are taken into account. This is because the ideal gain vector used for normalization is read to vastly different lengths between strategies or environments. For example, consider Figure 7, which plots NCG over time for strategy S2 in the two scenarios.

111

Fig 7. NCG vs. time comparison of PC and SP for S2 (41 Topics).
Due to normalization (division by the ideal cumulated gain vector) the SP scenario seems to have better performance in the time frame from 40 to 135 seconds. This is due to (a) ranking being somewhat effective, and (b) the number of documents seen in each session: in the PC case the user sees 15 to 35 documents, but in the SP case only 5 to 20 documents in the indicated time frame. Figure 8 plots CG with the corresponding data and makes the difference clear.

Fig 9. Traditional View, CGs over ranks for 41 topics, scenarios PC and SP for strategies S1 (allowing 5 queries) and S3 (allowing only 3 queries).
In Figure 9, both scenarios PC and SP have the same observed effectiveness, because the evaluation focuses on the gain (CG) over the result ranks, no matter how long it takes to retrieve the documents. The two strategies S1 and S3 differ in effectiveness, S3 providing far better effectiveness than S1. However, when time is taken into account (Fig. 10), the scenarios and strategies differ greatly from each other. Up to 60 seconds, S3 in the SP case is the worst strategy and this is entirely due to the high input cost of the long query. With enough time (180 sec.), S3 in SP catches up S3 in PC case. Also, PC and SP do not much differ for S1 due to the relatively low input cost and weak result quality. Comparing Figures 9 and 10, it is easy to see that time drives interaction and profoundly affects both user experience and effectiveness in sessions in different scenarios.

Fig 8. CG over time for S2 in scenarios PC and SP (41 Topics).
Similar pitfalls also plague the most classic metric, MAP. Consider the following two rankings observed for a given topic in two scenarios and/or strategies under the same time constraint (say, one minute; queries omitted and binary relevance for simplicity):
r1: 0 0 0 0 0 0 0 0 1 1
r2: 1 0 0 0 0
Further, assume that there are three relevant documents for the topic. The MAP for the ranking r1 is (1/9 + 2/10 + 0)/3 = 0.103 and for r2 (1 + 0 + 0)/3 = 0.333. Arguably, r2 is the better ranking, but if both require one minute, what is the user's opinion? The first session collected twice as many relevant documents.
Even within the un-normalized metric, such as CG, incorporating time in session-based evaluation has profound effects. Consider Figures 9 and 10. The former gives traditional cumulated gain over ranks for strategies S1 and S3 for the 41 topics. The latter gives CG over time in the two scenarios.

Fig 10. Time based View, CGs over time for 41 topics, scenarios PC and SP for strategies S1 and S3.
Limitations. In our study we did not take into account the time, which users spend for pondering about possible query words. One might argue that the more words one needs to identify, the harder (and slower) per word it comes. However, the thinking time is the same between sessions using the same number of words. In addition, this could be taken into account by revising subsequent query costs (Table 1). We have chosen to short-cut here in order to avoid too much complexity at this stage. Furthermore, we do not consider the time users spend in examining documents. This may depend on the device used. This can be seen as an artificial limitation. Tackling it would, however, complicate analysis, and this is therefore left for later study. We did not simulate user's learning during a session. Admittedly, learning from snippets and seen documents take place.

112

This is not impossible to simulate but some challenges remain to be solved.
We employed in the evaluation relatively limited query vocabularies, simple bag-of-word queries, and relatively short time frames. The query vocabularies and structure are justified by query length statistics in many search environments [14], [29], and the time frames by our simulation capabilities. However, the time frames are for effective search time in sessions, excluding thinking and document examination time. While the query vocabularies are short, they are human-generated for this collection, and therefore more realistic than words mined, e.g., from known relevant documents (in qrels).
We did not cover all the imaginable complex sessions. However we employed idealized and literature-based sessions, which shed the light on the peculiar evaluation problems beyond the traditional rank-based evaluation. This is a step forward while we are not suggesting that anyone follows a single strategy consistently in real life.
Our initial results are promising. First, the scenario, and to a large extent the device itself, dictate what kind of interactive behavior can be successful. Because real users do have limited resources and they use various devices having different properties, our methodology has unquestionable user relevance and potential pragmatic value for the industry. Measuring the effectiveness of systems from the pragmatic point of view may increase the validity of the results achieved. This may lead to greater user satisfaction. Secondly, our experimental results suggest that strict time constraints determine some session strategies as the best strategies as they maximize CG. The strengths of our approach are:
∑ The QM strategies S1-S5 have an empirical real life grounding
∑ The query vocabularies were generated by real test persons, and only thereafter used in automatic simulation
∑ We were able to evaluate over 20M sessions in each scenario; this is clearly intractable both physically, intellectually and economically with human test persons.
We have only taken the first steps. In future, we will study the dimensions of variation related to users, systems, information sources and sessions to construct more fine-grained scenarios explicating hypotheses about user goals, learning, and behaviors to validate evaluation measures used. [19]
6. CONCLUSIONS
In this study, we have shown the necessity of a pragmatic evaluation approach based on scenarios with explicit subtask costs under an overall time constraint. Effectiveness of various query modification and scanning strategies for two scenarios, namely, PC and SP is analyzed. Furthermore, the characteristics of the best and the worst interactive search sessions are examined. Expensive input favors scanning at length, cheap input favors better queries. The more time is available the less it matters how one searches ≠ there will be time to identify the relevant documents. We have shown that the effort required by searching devices and the overall search time allocation drive interaction and profoundly affect both user experience and effectiveness in sessions in different scenarios. Moreover, we have also pointed out the inapt use of all normalized rank-based measures. Thus, we hope we could instigate new evaluation metrics for time-based comparisons.
7. ACKNOWLEDGMENT
This research was funded by Academy of Finland grant number 133021.

8. REFERENCES
[1] Azzopardi, L. 2007. Position Paper: Towards Evaluating the User Experience of Interactive Information Access Systems. In SIGIR'07 Web Information-Seeking and Interaction Workshop, 5 p.
[2] Azzopardi, L. 2011. The economics of interactive information retrieval. In Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 15-24.
[3] Bates, M. J. 1979. Information search tactics. Journal of the American Society for Information Science, 30(4), 205-214.
[4] Bates, M. J. 1989. The Design of Browsing and Berrypicking Techniques for the Online Search Interface. Online Review, 13(5), 407-424.
[5] Beaulieu, M. 2000. Interaction in Information Searching and Retrieval. Journal of Documentation, 56(4), 431-439.
[6] Belkin, N. L. 1980. Anomalous States of Knowledge as a Basis for Information Retrieval. Canadian Journal of Information and Library Science, 5, 133-143.
[7] Card, S. K., Moran, T. P., and Newell, A. 1983. The Psychology of Human-Computer Interaction. L. Erlbaum Assoc. Inc., Hillsdale, NJ, USA.
[8] Cleverdon, C.W., Mills, L., and Keen, M. 1966. Factors determining the performance of indexing systems, vol. 1-design. In Aslib Cranfield Research Project, Cranfield.
[9] Dunlop, M. D. 1997. "Time Relevance and Interaction Modeling for Information Retrieval". In Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Philadelphia,206-213.
[10] Fidel, R. 1985. Moves in online searching. Online Review, 9 (1), 62-74.
[11] Hearst, M. A. 2011. "Natural" Search User Interfaces. Communications of the ACM, vol. 54, 60-67.
[12] Hersh, W., Turpin, A., Price, S., Chan, B., Kraemer, D., Sacherek, L., and Olson, D. 2000. Do Batch and user Evaluations Give the Same Results? In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 17-24.
[13] Ingwersen, P. and J‰rvelin, K. 2005. The Turn: Integration of Information Seeking and Retrieval in Context. Heidelberg, Springer.
[14] Jansen, M. B. M., Spink, A., and Saracevic, T. 2000. Real Life, Real Users, and Real Needs: A Study and Analysis of User Queries on the Web. Information Processing & Management, 36(2), 207-227.
[15] J‰rvelin, K. and Kek‰l‰inen, J. 2002. Cumulated Gain-Based Evaluation of IR Techniques. ACM Transactions on Information Systems, 20(4), 422-446.
[16] J‰rvelin, K. and Kek‰l‰inen, J. 2000. IR evaluation methods for retrieving highly relevant documents. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, 41-48.
[17] Kamvar, M. and Baluja, S. 2007. Deciphering Trends in Mobile Search. Computer, 40(8), 58-62.
[18] Karat, C-M., Halverson, C., Horn, D., and Karat, J. 1999. Patterns of entry and correction in large vocabulary continuous

113

speech recognition systems. In ACM Conference on Human Factors in Computing Systems, 568-575.
[19] Karlgren, J., J‰rvelin, A., Eriksson, G, and Hansen, P. 2011. Use cases as a component of information access evaluation. In DESIRE'11 workshop, October 28, 2011, Glasgow, Scotland, UK.
[20] Keskustalo, H., J‰rvelin, K., Pirkola, A., Sharma, T. and Lykke, M. 2009. Test Collection-Based IR Evaluation Needs Extension Toward Sessions ≠ A Case of Extremely Short Queries. In Proceedings of the 5th Asia Information Retrieval Symposium (AIRS'09), 63-74.
[21] Kuhlthau, C. C. 1991. Inside the Search Process. Journal of the American Society for Information Science, 42(5), 361-371.
[22] Price, S.L., Nielsen, M.L., Delcambre, L.M.L., and Vedsted, P. 2007. Semantic Components Enhance Retrieval of Domainspecific Documents. In Proceedings of the 16th ACM CIKM, 429-438.
[23] Ruthven, I. 2008. Interactive Information Retrieval. In Annual Review of Information Science and Technology, vol. 42, 2008. 43-91.
[24] Salton, G. 1970. Evaluation Problems in Interactive Information Retrieval. Information Storage and Retrieval, 6, 29-44.
[25] Smith, C. L. and Kantor, P. B. 2008. User Adaptation: Good Results from Poor Systems. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 147-154.
[26] Smucker, M. D. 2009. Towards Timed Predictions of Human Performance for Interactive Information Retrieval Evaluation. In Third Workshop on Human-Computer Interaction and In-

formation Retrieval (HCIR'09), October 23, 2009, Washington DC, USA.
[27] Sormunen, E. 2002. Liberal Relevance Criteria of TREC ≠ Counting on Negligible Documents? In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Tampere, 324-330.
[28] Spink, A. 1997. Study of Interactive Feedback during Mediated Information Retrieval. Journal of the American Society for Information Science, 48(5), 382-394.
[29] Stenmark, D. 2008. Identifying Clusters of User Behavior in Intranet Search Engine Log Files. Journal of the American Society for Information Science, 59(14), 2232-2243.
[30] Su, L.T. 1992. Evaluations Measures for Interactive Information Retrieval. Information Processing & Management 28(4), 503-516.
[31] Turpin, A. and Hersh, W. 2001. Why Batch and User Evaluations Do Not Give the Same Results. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 225-231.
[32] Turpin, A. and Scholer, F. 2006. User Performance versus Precision Measures for Simple Search Tasks. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 11-18.
[33] Vakkari, P. 2000. Cognition and changes of search terms and tactics during task performance. In Proceedings of RIAO 2000 Conference, Paris: C.I.D., 894-907.

114

Effect of Dynamic Pruning Safety on Learning to Rank Effectiveness

Craig Macdonald1, Nicola Tonellotto2, Iadh Ounis1
1 University of Glasgow, Glasgow, G12 8QQ, UK 2 National Research Council of Italy, Via G. Moruzzi 1, 56124 Pisa, Italy
{craig.macdonald, iadh.ounis}@glasgow.ac.uk1, {nicola.tonellotto}@isti.cnr.it2

ABSTRACT
A dynamic pruning strategy, such as Wand, enhances retrieval efficiency without degrading effectiveness to a given rank K, known as safe-to-rank-K. However, it is also possible for Wand to obtain more efficient but unsafe retrieval without actually significantly degrading effectiveness. On the other hand, in a modern search engine setting, dynamic pruning strategies can be used to efficiently obtain the set of documents to be re-ranked by the application of a learned model in a learning to rank setting. No work has examined the impact of safeness on the effectiveness of the learned model. In this work, we investigate the impact of Wand safeness through experiments using 150 TREC Web track topics. We find that unsafe Wand is biased towards documents with lower docids, thereby impacting effectiveness.
Categories & Subject Descriptors: H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval
General Terms: Performance, Experimentation
Keywords: Dynamic Pruning, Learning to Rank
1. INTRODUCTION
A search engine deploying learning to rank techniques reranks the top K documents retrieved by a standard weighting model, known as the sample [3], as shown in Figure 1. To improve the efficiency of such a deployment, a dynamic pruning strategy such as Wand [1] could easily be used, which omits the scoring of documents that cannot reach the top K retrieved set. In doing so, Wand is safe-to-rank-K, which we denote as safe for short.
Wand follows a Document-at-a-time retrieval strategy (DAAT), whereby the posting lists for all constituent terms of a query are processed in parallel, allowing immediate decisions as to whether a document has scored high enough to make the current top K retrieved set. In particular, Wand repeatedly calculates a pivot term, by comparing the upper bounds (t) of each query term t to the current score of the K-th ranked document, known as the threshold  . The next document containing the pivot term is called the pivot document, which will be the next document to be fully scored. If the scored document exceeds the threshold  , then the current K-th ranked document is expelled from the retrieved set, the new document inserted, and  updated. As the scoring for a query continues, the threshold  rises, such that more documents can be omitted from scoring.
However, Broder et al. [1] showed that Wand can be made more efficient by relaxing the safeness guarantee. This is
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

Figure 1: Retrieval phases of a search engine.
achieved by artificially increasing  by a factor F  1. F = 1 guarantees safe retrieval, while for F > 1, increased efficiency can be achieved without much degradation in effectiveness. However, to the best of our knowledge, no previous work in the literature has addressed how such unsafe document rankings affect retrieval performance within a modern learning to rank setting. This paper provides a first study into the effect of safeness within a learning to rank setting, while providing explanations for the observed inherent bias in unsafe pruning that can improve effectiveness in some settings. Indeed, in contrast to a safe setting, unsafe Wand is dependent on the ordering of the collection, suggesting that further research into addressing the bias is needed.
2. DATA & METHODS
Our experiments use the ClueWeb09 (cat. B) collection, which comprises 50 million English Web documents, and is aimed to represent the first tier index of a commercial search engine. We use the 150 corresponding topics and relevance assessments from the TREC Web tracks 2009-2011.
We index this collection using the Terrier information retrieval platform1, with stemming and stopword removal. Following the three phase architecture of Figure 1, the top K = 1000 documents are ranked by Wand using the DPH Divergence from Randomness weighting model. We use a total of 33 standard query-dependent features (e.g. term weighting models, proximity features) and query-independent document features (e.g. link analysis, URL length, content quality). To re-rank the documents in the sample, we use the LambdaMART learning to rank technique [2, 4], which represents a state-of-the-art learning to rank technique, as per its recent performance in the 2011 Yahoo! learning to rank challenge. In particular, the 150 TREC topics are randomly split into three sets, namely training, validation and test. In the following, we experiment with the effectiveness of samples and LambdaMART models for various F values, while comparing and contrasting their retrieval effectiveness, in terms of NDCG@20 and relevant documents retrieved.
1http://terrier.org

1051

0.3

0.25

NDCG@20

0.2

0.15

0.1

0.05

WAND Sample WAND+LambdaMART

0

1 2 3 4 5 6 7 8 9 10

F
Figure 2: NDCG@20 for the WAND sample, and LambdaMART applied on the WAND sample.

LambdaMART NDCG@20 Sample NDCG@20 Relevant Retrieved in Sample Mean docid of Relevant Retrieved
Mean docid of docs present in sample A not present in sample B

F =1 0.2718 0.2242 1931 26.7M
F = 1 (A)  F = 1.75 (B)

F = 1.75 0.3055* 0.2242
1849 25.6M
F = 1.75 (A)  F = 1 (B)

35.8M

11.2M

Table 1: Analysis of safe and unsafe samples and learned models, as well as comparative statistics.

3. RESULTS & ANALYSIS
Figure 2 shows the NDCG@20 effectiveness of both the Wand sample, and LambdaMART applied on the sample document rankings from Wand, as F is varied. Note that as the learned model obtained by LambdaMART may be sensitive to a given F setting, a different model is learned for each F value. From Figure 2, we observe that the effectiveness to rank 20 of the Wand sample is unchanged for 1  F  3, mirroring the original observations of Broder et al. [1]. However, the LambdaMART performance is much less stable for different F values - indeed, the overall NDCG@20 trend is downward for larger F . This is explained in that the learned model ranks documents from deep in the sample, and hence is affected by degradations in the number of relevant documents retrieved in unsafe samples. Indeed, on analysing F = 1, we find that of the top 20 documents ranked by LambdaMART, some were found as deep as rank 935 in the input sample ranking, while the mean rank in the sample of LambdaMART's top 20 documents was 89.
However, for some small F values in Figure 2 (1 < F < 2), a learned model obtained from an unsafe sample could improve over the NDCG@20 of the learned model obtained from the safe F = 1 sample. To analyse this unexpected characteristic, in Table 1 we compare and contrast four settings: F = 1 and F = 1.75, with and without the application of LambdaMART. Indeed, F = 1.75 is an interesting setting as while it is unsafe, it does not degrade NDCG@20 of the sample ranking obtained from Wand, but significantly improves the effectiveness of LambdaMART, according to a paired t-test (p < 0.01). Moreover, F = 1.75 is an efficient setting (we find that it reduces the mean response time of 1000 queries from a query log by 19% compared to F = 1, while larger F values do not cause further time reductions).
Next, comparing the sample rankings obtained from Wand for F = 1 and F = 1.75, we note a decrease of 82 relevant documents retrieved across the 50 test queries. Moreover, we examined the docids (in the range 0..50M for ClueWeb09 cat. B) of the documents selected in the safe and unsafe samples. We found that, on average, the safe sample retrieved documents from later in the posting lists (i.e. higher docids) than the unsafe sample (mean docids: 24.3M vs 21.9M). This observation is mirrored in the documents retrieved in one sample and not the other: the mean docid of documents

Figure 3: Distribution of relevant documents across the docid range, and for the safe & unsafe samples.
in the safe sample that are missing from the unsafe sample is 35M, while unsafe sample documents missing from safe have a mean docid of 11.2M. Finally, Figure 3 presents the distribution of relevant docids, as well as those retrieved in the safe and unsafe samples. This shows that while there is no docid bias for relevant documents, unsafe Wand is more biased towards low docids than safe Wand. Indeed, the mean docid of the relevant documents retrieved in the safe sample is higher than those found in the unsafe sample (26.7M vs 25.6M in Table 1), explaining the change in retrieval effectiveness. Overall, this shows that aggressive, unsafe pruning by Wand can change the selected documents in a biased manner that is not present in safe pruning.
This behaviour of Wand is explained as follows: by artificially increasing the threshold  by the factor F , the threshold for unsafe Wand causes more documents to be prevented from entering the top K. Early in the traversal of the posting lists, when the  is lower, documents can still enter into the retrieved set. However as  gets higher, more pruning occurs, even for documents that would have made the retrieved set for F = 1. This explains unsafe Wand's comparative preference for lower docid documents.
4. CONCLUSIONS
We contrasted the effectiveness of safe vs. unsafe rankings from Wand, and its impact on the effectiveness of a learning to rank technique, using ClueWeb09 cat. B and 150 TREC Web track topics. We found that while unsafe retrieval effectiveness has little impact on the top ranked documents directly retrieved by Wand, it does impact deeper down, which can be to the detriment of a learned model applied on that sample. Some unsafe settings were even found to benefit the learned model. Through further analysis, we found that unsafe retrieval has an inherent bias towards documents with lower docids in the applied index ordering.
The observations in this paper can give rise to several further research lines. In particular, static collection orderings may be devised that counteract unsafe Wand's preference for lower docids. On the other hand, it may be possible to devise different manners of changing the threshold for increasing Wand's efficiency in a less biased manner.
Acknowledgements
Craig Macdonald and Iadh Ounis acknowledge the support of EC-funded project SMART (FP7-287583).
5. REFERENCES
[1] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In Proc. of CIKM 2006, 426≠434.
[2] Y. Ganjisaffar, R. Caruana, and C. Lopes. Bagging gradient-boosted trees for high precision, low variance ranking models. In Proc. of SIGIR 2011, 85≠94.
[3] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in IR, 3(3):225≠331, 2009.
[4] Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao. Ranking, boosting, and model adaptation. Technical Report MSR-TR-2008-109, Microsoft, 2008.

1052

Effect of Written Instructions on Assessor Agreement

William Webber
College of Information Studies University of Maryland
United States of America wew@umd.edu

Bryan Toth and Marjorie Desamito 
Eleanor Roosevelt High School Greenbelt, Maryland
United States of America bryan.n.toth@gmail.com magicaura2000@yahoo.com

ABSTRACT
Assessors frequently disagree on the topical relevance of documents. How much of this disagreement is due to ambiguity in assessment instructions? We have two assessors assess TREC Legal Track documents for relevance, some to a general topic description, others to detailed assessment guidelines. We find that detailed guidelines lead to no significant increase in agreement amongst assessors or between assessors and the official qrels.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and software--performance evaluation.
Keywords
Retrieval experiment, evaluation, e-discovery
General Terms
Measurement, performance, experimentation
1. INTRODUCTION
Assessors frequently disagree on the relevance of a document to a topic. Voorhees [2000] finds that TREC adhoc assessors have mutual F1 scores of around 0.6, while Roitblat et al. [2010] report mutual F1 as low as 0.35 for professional e-discovery reviewers. Such low agreement is of serious practical concern in e-discovery, where large-scale, delegated manual review is still widely used. Possible causes of disagreement include assessor error and ambiguity in instructions. We examine whether detailed relevance guidelines increase agreement amongst assessors and with the guideline author, and find no significant increase in either form of agreement
2. METHODS AND MATERIALS
We measure inter-assessor agreement by Cohen's , for which 1 is perfect and 0 is random agreement [Cohen, 1960]. Unassessable documents (too long or misrendered) are ignored. A trial experiment of 75 documents per treatment, on Topic 301 from the TREC 2010 Legal Track, indicated that a sample size of 215 documents per treatment, with even proportions relevant and irrelevant, was required to achieve 80% power for a true  delta of 0.23, being the difference in agreement with official assessments between first and second tercile assessors in the TREC 2009 Legal Track. Work performed while interns at the University of Maryland.
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

Message type

Messages

> 1 relevant document

5

Relevant appealed

170

" unappealed

38

Irrelevant appealed

58

" unappealed returned

32

" " unreturned

16

Total

319

Documents

rel irrel unass

13 12

0

170 82

4

38 7

0

0 78

1

0 44

1

0 17

1

221 240

7

Table 1: Number and types of messages and documents sampled from Topic 204 for re-assessment. A message is classed as "relevant" if it contains a single relevant document (body or attachment). Counts of relevant, irrelevant, and unassessable documents are using the official, post-appeal assessments.

Topic 204 from the interactive task of the TREC 2009 Legal Track [Hedin et al., 2009] was used for the full experiment. The corpus is the EDRM Enron emails. Whole messages were sampled, but each email body and attachment was separately assessed. A stratified sample was taken, as described in Table 1. The strata were divided evenly and randomly into two batches. Each batch was assessed in document id order, with the parts of a message being assessed sequentially, as in TREC.
At TREC, a senior lawyer called the topic authority develops the topic, writes the detailed guidelines, and adjudicates appeals against first-round assessments. The appeals process for this topic was thorough [Webber, 2011], and the majority of sampled documents were appealed; we regard the assessments as an accurate representation of the topic authority's conception of relevance. We measure agreement for each batch between the two experimental assessors and the official, post-appeal assessments.
The latter two authors of this paper acted as experimental assessors. Each assessor assessed all documents in each batch. For the first batch, assessors were given the 42-word topic statement to guide their assessments; for the second, they received the 5-page detailed relevance guidelines. A third pass was then made, in which the two assessors jointly reviewed both batches, in light of the detailed guidelines, and tried to agree on a conception of relevance.
3. EXPERIMENTAL RESULTS
Table 2 shows the results of our experiments. The provision of detailed assessment guidelines (Batch 2) did not improve agreement, significantly or otherwise, over topic-only instructions (Batch 1), either amongst assessors or with the official assessments, in either the full or the trial experiment. Message-level analysis (in

1053

Batch
1 2 Jnt-1 Jnt-2

Full experiment

AvB AvO BvO

0.519 0.528 0 .992
0 .950

0.454ab
0.555 0.677a 0.665b

0.710 0.637 0.686 0.674

Trial experiment

AvB AvO BvO

0.229 0.275
- -

0.557 0.439
- -

0.417 0.294
- -

Table 2: Cohen's  values between official and two experimental assessors, for full and trial experiments, on single-assessed Batch 1 (with topic statement only), single-assessed Batch 2 (with detailed guidelines), and (for full experiment only) jointassessed Batches 1 and 2 (with topic guidelines and consultation between assessors). Columnar value pairs significantly different at  = 0.05 (excepting inter-experimenter joint review) are marked by superscripts.

Assessors
A v. B A v. Official B v. Official

Confidence interval
[-0.155, 0.173] [-0.061, 0.263] [-0.211, 0.065]

Table 3: Two-tailed 95% normal-approximation confidence intervals on the true change in  between Batch 1 and Batch 2 amongst different assessor pairs, for the full experiment.

which a message is relevant if any part of it is relevant) gives similar results. Inter-assessor  values are high for the full experiment's joint assessment, since assessors reached agreement on all save a handful of documents (1 for Batch 1, and 5 for Batch 2). Assessor A's agreement with the official assessments increases significantly under joint review, but this may be due to Assessor A's assessments moving closer to Assessor B's; Assessor A's self-agreement on Batch 1 is 0.399 post-consultation, whereas Assessor B's is 0.739.
Table 3 gives 95% confidence intervals on the true change in  values with the addition of assessor guidelines. A substantial improvement is still plausible in agreement between Assessor A and the official assessments, but not for Assessor B and official, nor for inter-assessor agreement.
Agreement between the original TREC assessors and the authoritative assessment on the documents examined in our experiment is 0.102 for Batch 1 and 0.024 for Batch 2, much lower than for our experimental assessors; however, this is a biased comparison, since sampling was heavily weighted towards appealed documents. Over the 7,289 documents sampled for assessment at TREC, though, the original assessors achieved a  of 0.320, still well below that of the experimental assessors. The relatively high reliabilty of the assessor is reflected in their high mutual F1 scores (Table 4).
Qualitatively, the experimental assessors described the full experiment topic description by itself as being clear, and the detailed guidelines as being very clear and easy to relate to the documents. As can be seen in Table 2, agreement for this topic was generally higher than for the trial experiment.
4. DISCUSSION
Our initial, seemingly common-sense, hypothesis was that more detailed instructions would raise agreement between assessors and the authoritative conception of relevance, and therefore amongst assessors themselves. The results of this experiment have failed to confirm this hypothesis, or even to show a general trend in this direction. The only significant improvement occurred when Asses-

Batch
1 2

AvB
0.679 0.769

AvO
0.648 0.791

BvO
0.828 0.823

Table 4: Assessor mutual F1 scores for the full experiment.

sor A consulted with Assessor B, but that may be attributable to the former's assessments moving closer to the latter's. Indeed, confidence intervals indicate that a substantial increase in agreement is not plausible, except possibly between one assessor and the official view. We can conclude that, for this topic and these assessors, the provision of more detailed assessment guidelines did not lead to any marked increase in assessor reliability.
It is also notable that our experimental assessors, who were high school students with no legal training, appear to have produced assessments much more in line with the authoritative conception of relevance than the original TREC assessors, who were legally trained, professional document reviewers.
Our findings are not reassuring for the widespread practice of using delegated manual review in e-discovery. If assessors do no better with detailed guidelines than with a general outline of the topic, then there is an irreducible loss of signal in transmitting the relevance conception of an authoritative reviewer into the minds of other assessors. E-discovery practice is moving towards the use of statistical classification tools [Grossman and Cormack, 2011]; it may well be that the lawyer overseeing a case is better able to convey their conception of relevance by personally training a machine classifier, than by instructing delegated human reviewers.
Acknowledgments.
Venkat Rangan of Symantec eDiscovery provided the TIFF images of Enron documents used in the TREC 2009 Legal Track assessments. Maura Grossman and Gord Cormack advised on the choice of TREC topics. This material is based upon work supported by the National Science Foundation under Grant No. 1065250. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
References
Jacob Cohen. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37≠46, 1960.
Maura R. Grossman and Gordon V. Cormack. Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review. Richmond Journal of Law and Technology, 17(3):11:1≠ 48, 2011.
Bruce Hedin, Stephen Tomlinson, Jason R. Baron, and Douglas W. Oard. Overview of the TREC 2009 legal track. In Ellen Voorhees and Lori P. Buckland, editors, Proc. 18th Text REtrieval Conference, pages 1:4:1≠ 40, Gaithersburg, Maryland, USA, November 2009. NIST Special Publication 500-278.
Herbert L. Roitblat, Anne Kershaw, and Patrick Oot. Document categorization in legal electronic discovery: computer classification vs. manual review. Journal of the American Society for Information Science and Technology, 61(1):70≠80, 2010.
Ellen Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5): 697≠716, September 2000.
William Webber. Re-examining the effectiveness of manual review. In Proc. SIGIR Information Retrieval for E-Discovery Workshop, pages 2:1≠8, Beijing, China, July 2011.

1054

Exploiting Term Dependence while Handling Negation in Medical Search

Nut Limsopatham1, Craig Macdonald2, Richard McCreadie2, Iadh Ounis2 nutli@dcs.gla.ac.uk1, firstname.lastname@glasgow.ac.uk2
School of Computing Science University of Glasgow, Glasgow, UK

ABSTRACT
In medical records, negative qualifiers, e.g. no or without, are commonly used by health practitioners to identify the absence of a medical condition. Without considering whether the term occurs in a negative or positive context, the sole presence of a query term in a medical record is insufficient to imply that the record is relevant to the query. In this paper, we show how to effectively handle such negation within a medical records information retrieval system. In particular, we propose a term representation that tackles negated language in medical records, which is further extended by considering the dependence of negated query terms. We evaluate our negation handling technique within the search task provided by the TREC Medical Records 2011 track. Our results, which show a significant improvement upon a system that does not consider negated context within records, attest the importance of handling negation.
Categories and Subject Descriptors: H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval
General Terms: Experimentation, Performance
Keywords: Medical Retrieval, Negation, Term Dependence
1. INTRODUCTION
Search in the medical domain is notable for its extensive use of negated language within medical records [2]. However, recently Koopman et al. [2] asserted that the use of term frequency in document weighting models such as BM25 automatically alleviates the problems of negated language. Notably, they argue that terms in negative contexts typically appear once per medical record while, in contrast, terms indicating the presence of a medical condition (positive terms) appear frequently. In this paper, we argue that a dedicated negation handling approach will outperform traditional retrieval models for medical domain queries. Additionally, prior works have focused almost exclusively on negated language in queries rather than documents (e.g. boolean retrieval models or vector negation [7]). For example, for the query `NOT chest pain AND shortness of breath', the boolean model will find documents not containing `chest pain', thereby not considering any negation occurring within the records. Moreover, most words indicating negation (e.g. no, not) are stopwords, which are not typically indexed [1].
To cope with negation in medical search, we propose a two-step process: a term representation approach, NegFlag, to facilitate the handling of negative context in medical records; and a novel term dependence approach to demote
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

Original record
Negation detection
Removing stopwords NegFlag representation

Patient reports palpitations but does not have fever Patient reports palpitations but does not have fever Patient reports palpitations fever Patient reports palpitations n0fever

Table 1: The NegFlag process for a medical record - italicised terms occur in a negative context.

medical records containing query terms in a negated context. We evaluate our proposed approach using the TREC 2011 Medical Records track [6] test collection. As our results exhibit significant improvement over a baseline where negation is not handled, we conclude that negated language should be explicitly handled for effective medial records search.

2. THE NEGFLAG APPROACH
Our negated term representation approach, NegFlag, modifies the indexing process to distinguish between positive and negative context terms in medical records, which are identified using the NegEx algorithm [1]. Identified negated terms are replaced with special negated versions of those terms. Table 1 shows how an example sentence is processed using NegFlag, such that the term `fever' is replaced with its negated version, `n0fever'.
Hence, given the query `find patient with fever', without NegFlag, a retrieval model might erroneously rank the original record in Table 1 highly, because it contains all of the query terms. However, after NegFlag processing, as `fever' has become `n0fever', the record would not score as highly.
However, while NegFlag only helps to retrieve records containing query terms with positive contexts, it does not prevent records with the negated occurrences from being retrieved. For example, the NegFlag processed example in Table 1 might still be retrieved for the query `find patient with fever and palpitations', when the patient is known not to have a fever. To alleviate this, in the next section, we propose the use of term dependence to demote medical records containing query terms within negative contexts.

3. TERM DEPENDENCE FOR NEGATION
Term dependence (e.g. Markov Random Fields [4]) has been used to improve effectiveness by scoring higher documents containing many occurrences of pairs of query terms in close proximity. In contrast, we propose to use term dependence to demote records containing the negated form of neighbouring terms occurring in the queries. For example, given a query `chest pain', documents containing the pair of terms `n0chest n0pain' should be demoted. To this effect, we score medical record r for a query Q, taking negation into account, as follows:

1065

score (r, Q) = X score(r, t) - X score(r, t1, t2 ) (1)

tQ

t1,t2 Q

There are two components in Equation (1), namely the

positive scoring of positive query terms, and the negative

term dependence score for the negated forms of the query

terms. score(r, t) is the score assigned to a query term t in medical record r using any term weighting model, Q is the

set of negated forms of the positive query terms in Q, and t1, t2 is a pair of negated terms in Q. Two types of term

dependence are possible [4, 5]: for full dependence (FD),

t1, t2 is the set that contains unordered pairs of neigh-

bouring terms; for sequential dependence (SD), t1, t2 is the set that contains ordered pairs of neighbouring terms.

For score(r, t1, t2 ), we use the binomial randomness model pBiL [5] from the Divergence from Randomness (DFR) frame-

work to score the occurrences of a pair of terms within

window size tokens in a medical record r.

4. EXPERIMENTAL RESULTS
We evaluate our negated term representation and the term dependence approaches using the 34 topics from the TREC 2011 Medical Records track [6]. In this track, the task is to identify relevant patient visits for each topic, where a visit contains all of the medical records associated with that patients' admission to the hospital. For indexing and retrieval, we use Terrier1, applying Porter's English stemmer and removing stopwords. The parameter-free DFR DPH term weighting model is used to rank medical records. The expCombSUM voting technique [3] is then used to rank visits based on the scores of their associated medical records. The number of voting medical records is limited to 5,000 as this was found to be effective in preliminary experiments. We hypothesise that negation should be explicitly handled, hence we compare our approach with a baseline where negation is not explicitly handled, as suggested in [2].
Figure 1 shows the bpref retrieval performance comparing NegFlag, as well as the SD and FD variants of term dependence with NegFlag, and the baseline where negation is not handled. From Figure 1, we observe that our approach employing either only NegFlag or both NegFlag and term dependence outperforms the baseline (bpref 0.4871), by up to 4%. Indeed, NegFlag alone markedly improves the retrieval performance over the baseline, while term dependence for window size = 3 using either SD or FD results in statistically significant improvements over the baseline (paired t-test, p < 0.05).
For SD, small window sizes are more effective, but performance is generally stable across different window sizes, suggesting that the presence of negated ordered pairs anywhere in a medical record is sufficient to ascertain if it should be demoted. For FD, window size > 3 degrades performance compared to NegFlag, but still outperforms the baseline.
Next, we further evaluate the effectiveness of our approach after applying a query expansion (QE) technique (namely DFR Bo1 from Terrier). Table 2 shows the retrieval performances (in terms of bpref, precision@10, and R-precision) of NegFlag, with SD and FD term dependence window size = 3 (identified best settings in Figure 1), as well as without term dependence, compared to a baseline applying only QE. The performances of the top 3 best systems at TREC 2011 are also reported. We observe that the QE baseline is outperformed by NegFlag for all the measures, however,
1http://terrier.org

bpref

0.505 0.5
0.495 0.49

Baseline NegFlag Only NegFlag with SD NegFlag with FD

0.485

5 10 15 20 25 30 35 40 45 50 window_size

Figure 1: bpref performances of the baseline and our approach, while varying window size.

Approach
Baseline NegFlag NegFlag with SD window size = 3 NegFlag with FD window size = 3
CengageM11R3 SCAIMED7 UTDHLTCIR

QE Bo1 Bo1
Bo1
Bo1
N/A N/A N/A

bpref 0.5264 0.5436 0.5420
0.5433
0.5520 0.5520 0.5450

P@10 0.6147 0.6324 0.6324
0.6235
0.6560 0.6030 0.6030

R-prec 0.4290 0.4351 0.4337
0.4332
0.4400 0.4250 0.4220

Table 2: Performances of NegFlag and negative term dependence with QE, and the top 3 best systems from TREC 2011.

the combination of term dependence and query expansion when using NegFlag remains challenging. In addition, comparing with the best systems reported at TREC 2011, we find that NegFlag performs better than the second ranked group in terms of precision@10 and R-precision, while for bpref, it is comparable with the third ranked group. Importantly, this is despite our approach not requiring any of the domain-specific ontologies that are exploited in those systems. Overall, our results are very promising, particularly if term dependence and query expansion can be successfully combined in future work.
5. CONCLUSIONS
We have proposed a novel approach to handle negation in medical search using our NegFlag term representation and a novel use of term dependence to demote documents containing the query terms in a negative context. Our approach is shown to be effective on the Medical Records track test collection, across a range of window sizes. Moreover, it verifies our stance that negation should be explicitly handled in medical search. Our proposed approach could also work with queries that contain negative contexts; however, as the available topics in the TREC 2011 test collection do not contain any negated terms, we leave this evaluation for future work.
6. REFERENCES
[1] W. Chapman, W. Bridewell, P. Hanbury, G. Cooper, B. Buchanan. A simple algorithm for identifying negated findings and diseases in discharge summaries. J. of Biomedical Informatics, 5:301≠310, 2001.
[2] B. Koopman, P. Bruza, L. Sitbon, M. Lawley. Analysis of the effect of negation on information retrieval of medical data. In ADCS'10.
[3] C. Macdonald, I. Ounis. Voting for candidates: adapting data fusion techniques for an expert search task. In CIKM'06
[4] D. Metzler, W. B. Croft. A Markov random field model for term dependencies. In SIGIR'05.
[5] J. Peng, C. Macdonald, B. He, V. Plachouras, I. Ounis. Incorporating term dependency in the DFR framework. In SIGIR'07.
[6] E. Voorhees, R. Tong. Overview of the TREC 2011 Medical Records Track. In TREC'11.
[7] D. Widdows. Orthogonal negation in vector spaces for modelling word-meanings and document retrieval. In ACL'03.

1066

Fixed versus Dynamic Co-Occurrence Windows in TextRank Term Weights for Information Retrieval

Wei Lu
School of Information Management
Wuhan University, China
reedwhu@gmail.com

Qikai Cheng

Christina Lioma

School of Information

Computer Science

Management

University of Copenhagen,

Wuhan University, China

Denmark

chengqikai0806@gmail.com c.lioma@diku.dk

ABSTRACT
TextRank is a variant of PageRank typically used in graphs that represent documents, and where vertices denote terms and edges denote relations between terms. Quite often the relation between terms is simple term co-occurrence within a fixed window of k terms. The output of TextRank when applied iteratively is a score for each vertex, i.e. a term weight, that can be used for information retrieval (IR) just like conventional term frequency based term weights.
So far, when computing TextRank term weights over cooccurrence graphs, the window of term co-occurrence is always fixed. This work departs from this, and considers dynamically adjusted windows of term co-occurrence that follow the document structure on a sentence- and paragraphlevel. The resulting TextRank term weights are used in a ranking function that re-ranks 1000 initially returned search results in order to improve the precision of the ranking. Experiments with two IR collections show that adjusting the vicinity of term co-occurrence when computing TextRank term weights can lead to gains in early precision.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords
TextRank, term co-occurrence
1. INTRODUCTION
Associative networks have long been used to represent units of text and their interconnecting relations [5]. The symbolic structures that emerge from these representations correspond to graphs, where text constituents are represented as vertices and their interconnecting relations as edges. Graph ranking algorithms, such as the TextRank [5, 6] variant of PageRank, have been used successfully in keyword extraction [6], classification [3] and information retrieval [2] to compute term weights from graphs of individual documents, where vertices represent the document's terms, and edges represent term co-occurrence within a fixed window. Using these computations iteratively, the weight of a term can be estimated with respect to the terms that fall in its vicinity and their respective term weights. An underlying
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

assumption in these approaches is that the vicinity of term co-occurrence is fixed for all terms. To our knowledge, there is no theoretical or intuitive basis for this assumption.
Fixed-window term co-occurrence may not be optimal for TextRank term weights. Lexical affinities may span across more words in longer sentences than they do in shorter sentences. Hence, adjusting the co-occurrence window according to the discourse span of the text might be a better choice. Based on this intuition, in this work we look at the effect of dynamically adjusted windows of term co-occurrence upon their resultant TextRank term weights. We experiment with co-occurrence windows that follow the document structure on two levels of granularity: sentences and paragraphs. For each of these, we compute term weights using TextRank, and use them for retrieval using the ranking model of [2], i.e. linearly combined with inverse document frequency (idf). Experiments using these TextRank term weights for re-ranking the top 1000 search results show that sentence-based cooccurrence can outperform fixed-window co-occurrence in terms of early precision.

2. CO-OCCURRENCE WINDOWS

2.1 Methodology
We experiment with two datasets: Reuters RCV1 from TREC 2002 (2.5GB, 50 title-only queries) and INEX 2005 (764MB, 40 content-only queries). We build a separate graph for each document: terms are represented as vertices (initially unweighted), and term co-occurrence within a window is represented as an undirected edge linking the vertices of the co-occurring terms. We use TextRank [6] to compute iteratively the score of each vertex vi:

s(vi)

=

(1

-

)

+



◊

jV

(vi )

S(vj ) |V (vj)|

(1)

where s(vi) is the TextRank score of vertex vi, V (∑) denotes the set of vertices connecting with a vertex, | ∑ | marks cardinality, and 0    1 is a damping factor that integrates into the computation the probability of jumping randomly from one vertex to another. We iterate the formula 200 times, using the default  = 0.85 [6]. The final score of each vertex represents a term weight where the higher the number of different words that a given word co-occurs with, and the higher their weight, the higher the weight of this word. It has been shown that a nonlinear correlation exists between such TextRank term weights and term frequency based term weights [5].

1079

We use these term weights to compute the score of a document for a query (s(d, q)) according to [2]:

s(d, q) = log idfi ◊ log s(i)

(2)

iq

where i is a query term, and s(i) is the corresponding TextRank score for vertex vi. No document length normalisation is used. We use Porter's stemmer for the documents and queries.
To compare fixed versus dynamically adjusted windows of term co-occurrence, we use a baseline where the window of term co-occurrence is fixed to the best values reported in the IR literature (albeit for other datasets)1[2]: k =5 & 6. We compare this baseline against term co-occurrence that is dynamically adjusted to the length of each (a) sentence and (b) paragraph2, separately. The sentence/paragraph term statistics are displayed in Table 1. We evaluate this comparison in a re-ranking scenario, where the task is to re-rank an initially retrieved set of 1000 documents. For the INEX collection (where relevance assessments apply to document sections) we consider a document relevant if any of its containing sections is assessed relevant.

2.2 Findings
Table 2 shows different metrics of retrieval performance when using fixed versus sentence- and paragraph-length windows of term co-occurrence. We see that results vary3. For average precision (NDCG) fixed co-occurrence is best for RCV1, and sentence-based co-occurrence is best for INEX. The reverse happens for precision in the top 10 retrieved documents (P@10): fixed co-occurrence is best for INEX, and sentence-based co-occurrence is best for RCV1. The only consistent trend is in the precision of the single top retrieved document (MRR), which benefits more from dynamically adjusted co-occurrence consistently for both collections. This finding is novel, considering the earlier position of [6] that the larger the window of co-occurrence, the lower the precision. This finding indicates that larger window sizes may lead to gains in precision, if however they are not fixed but rather dynamically adjusted to text units like sentences.
Finally, sentences appear to be an overall better boundary of term co-occurrence than paragraphs, with the exception of NDCG for INEX where paragraph-based co-occurrence slightly outperforms sentence-based co-occurrence (and they both outperform fixed co-occurrence). This could be due to the fact that INEX paragraphs are relatively short and focused content-wise [4].

3. CONCLUSION
We modelled individual documents as separate graphs where vertices represent terms, and co-occurrence relations among terms represent edges. Using the TextRank model of Mihalcea et al. [5, 6] we computed vertex weights corresponding to term weights, which we used for retrieval using
1In non-IR literature, optimal fixed values are: k =2,4 for classification [3] and k =2 for keyword extraction [6], however these values consistently underperform for IR [1, 2]. 2We treat these elements as paragraphs: p (for RCV1) and ilrj, ip1, ip2, ip3, ip4, ip5, item-none, p, p1, p2, p3, Bib, Bm, St (for INEX). 3Results were not stat. significant when the t-test was used.

min length max length min tokens max tokens average tokens

sent (RCV1) 1
1731 1
250 19.87

para (RCV1) 1
31696 1
4662 20.35

sent (INEX) 1
7920 1
2447 15.73

para (INEX) 1
111136 1
17379 58.51

Table 1: Sentence (sent) and paragraph (para) statistics per retrieval dataset.

Re-ranking top 1000 retrieved documents

co-occurrence

RCV1

window

NDCG MRR P@10 NDCG

fixed

5 terms 6 terms

0.5238 0.6736 0.4300 0.5541 0.5025 0.6559 0.4280 0.5540

dynamic

sentence paragraph

0.5119 0.5178

0.6811 0.4340 0.5543 0.6574 0.4160 0.5545

INEX MRR 0.6865 0.6966 0.7021 0.6975

P@10 0.4750 0.4714 0.4743 0.4714

Table 2: Retrieval performance with TextRank term weights using fixed vs. dynamic co-occurrence windows, on two datasets. Bold font marks best scores.

the ranking of Blanco et al. [1, 2]. Unlike all these existing approaches where term co-occurrence is fixed to a window of k terms at all times, we reasoned that term co-occurrence should be varied according to sentence or paragraph length. Our motivation was that meaningful term relations may span across more words in longer sentences than they do in shorter sentences, hence fixing term co-occurrence may not be optimal for all terms.
Preliminary experiments in a re-ranking scenario with two retrieval datasets showed that sentence-based co-occurrence can lead to early precision gains over fixed term co-occurrence at 5 and 6 terms, which are optimal values in the IR literature. More experiments with larger datasets and fullranking (as opposed to re-ranking) documents are needed to investigate the optimal term co-occurrence vicinity. This small-scale work contributes a novel comparison between fixed versus dynamically adjusted co-occurrence windows for TextRank term weights, and the initial finding that sentencebased co-occurrence can improve early precision.
Acknowledgments. Work partially funded by DANIDA (grant no. 10-087721) and the National Natural Science Foundation of China (grant no. 71173164).
4. REFERENCES
[1] R. Blanco and C. Lioma. Random walk term weighting for information retrieval. In W. Kraaij, A. P. de Vries, C. L. A. Clarke, N. Fuhr, and N. Kando, editors, SIGIR, pages 829≠830. ACM, 2007.
[2] R. Blanco and C. Lioma. Graph-based term weighting for information retrieval. Inf. Retr., 15(1):54≠92, 2012.
[3] S. Hassan, R. Mihalcea, and C. Banea. Random walk term weighting for improved text classification. Int. J. Semantic Computing, 1(4):421≠439, 2007.
[4] S. Malik, G. Kazai, M. Lalmas, and N. Fuhr. Overview of inex 2005. In N. Fuhr, M. Lalmas, S. Malik, and G. Kazai, editors, INEX, volume 3977 of Lecture Notes in Computer Science, pages 1≠15. Springer, 2005.
[5] R. Mihalcea and D. Radev. Graph-Based Natural Language Processing and Information Retrieval. Cambridge University Press, 2011.
[6] R. Mihalcea and P. Tarau. Textrank: Bringing order into text. In EMNLP, pages 404≠411. ACL, 2004.

1080

Impact of Assessor Disagreement on Ranking Performance

Pavel Metrikov Virgil Pavlu Javed A. Aslam
College of Computer and Information Science Northeastern University, Boston, MA, USA
{metpavel, vip, jaa}@ccs.neu.edu

ABSTRACT
We consider the impact of inter-assessor disagreement on the maximum performance that a ranker can hope to achieve. We demonstrate that even if a ranker were to achieve perfect performance with respect to a given assessor, when evaluated with respect to a different assessor, the measured performance of the ranker decreases significantly. This decrease in performance may largely account for observed limits on the performance of learning-to-rank algorithms.
Categories and Subject Descriptors: H. Information Systems; H.3 Information Storage and Retrieval; H.3.3 Information Search and Retrieval:Retrieval models
General Terms: Experimentation, Measurement, Theory
Keywords: Inter-assessor Disagreement, Learning-to-Rank, Evaluation
1. INTRODUCTION
In both Machine Learning and Information Retrieval, it is well known that limitations in the performance of ranking algorithms can result from several sources, such as insufficient training data, inherent limitations of the learning/ranking algorithm, poor instance features, and label errors. In this paper we focus on performance limitations due solely to label "errors" which arise due to inter-assessor disagreement.
Consider a training assessor A that provides labels for training data and a testing assessor B that provides labels for testing data. Even if a ranker can produce a perfect list as judged by A, its performance will be suboptimal with respect to B, given inevitable inter-assessor disagreement. In effect, no ranking algorithm can simultaneously satisfy two or more disagreeing assessors (or users). Thus, there are inherent limitations in the performance of ranking algorithms, independent of the quality of the learning/ranking algorithm, the availability of sufficient training data, the quality of extracted instance features, and so on.
We model inter-assessor disagreement with a confusion matrix C, where cij corresponds to the (conditional) probability that a document labeled i by testing assessor B will be labeled j by training assessor A, for some given set of label grades such as {0, 1, 2, 3, 4}. Given such a model of interassessor disagreement, we ask the question, "What is the
9This work supported by NSF grant IIS-1017903.
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

expected performance of a ranked list optimized for training assessor A but evaluated with respect to testing assessor B?" We approach this question in two ways, via simulation and closed form approximation. In the former case, we use the confusion matrix C to probabilistically generate training labels A from testing labels B, optimally rank documents according to A, and evaluate with respect to B. In the latter case, we analytically derive a closed-form approximation to this limiting performance, as measured by nDCG.
Given a confusion matrix C modeling inter-assessor disagreement, one can apply our results to any learning-to-rank dataset. The limiting nDCG values obtained correspond to reasonable upper bounds on the nDCG performance of any learning-to-rank algorithm, even one given unlimited training data and perfect features. Considering the performance of existing algorithms on these datasets, and comparing with the upper bounds we derive, one can argue that learning-torank is approaching reasonable limits on achievable performance.
2. ASSESSOR DISAGREEMENT MODEL
Much research in the IR community has focused on addressing the problem of system evaluation in the context of missing, incomplete, or incorrect document judgments [1]. Soboroff and Carterette [4] provide an in-depth analysis of the effect of assessor disagreement on the Million Query Track evaluation techniques. Both assessors and users often disagree on the degree of document relevance to a given query, and we model such disagreement with a confusion matrix C as described above. On data sets with multiple assessments per query-document pair, such as the TREC Enterprise Track [2], these confusion matrices can be directly estimated from data, and they can be obtained from user studies as well [3, 5].
For any ranked list returned by a system, the expected limiting nDCG due to assessor disagreement can be formulated as a function of (1) the disagreement model C and (2) the number of assessed documents and their distribution over the label classes. One way to compute this expected nDCG is numerical simulation: For every document d having testing label id in the ranked list, we randomly draw an alternative label jd with the probability cij; we then sort the ranked list in decreasing order of {jd} and evaluate its nDCG performance with respect to labels {id}. This simulation is repeated multiple times and the results averaged to obtain an accurate estimate of the expected limiting nDCG.
In our first experiment, we test whether the inter-assessor confusion matrix C alone can be used to estimate the limiting nDCG value. We do so by considering data sets that have multiple judgments per query-document pair, such as were collected in the TREC Enterprise Track where each

1091

Real NDCG Bound Real NDCG Bound

Real vs. Simulated NDCG Bounds (Enterprise C)

Real vs. Simulated NDCG Bounds (MSR C)

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

SG

0.6

SG

BG

BG

0.5

GS

0.5

GS

BS

BS

0.4

GB

0.4

GB

SB

SB

0.3

0.3

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Simulated NDCG Bound

Simulated NDCG Bound

Figure 1: Applying C ENT model (left) and C MSR model (right) to TREC enterprise data. X-axis is the simulated nDCG upper bound, while Y-axis is the actual nDCG assessor disagreement measured between 2 TREC assessors; pairs of assessor type ("Gold-Silver" as GS) are indicated by colors.

topic was judged by three assessors: a gold assessor G (expert on task and topic), a silver assessor S (expert at task but not at topic), and a bronze assessor B (expert at neither). Each G, S, and B set of assessments can take on the role of training or testing assessor, as described above, giving rise to six possible combinations: GS, GB, SG, SB, BG, BS. For each such combination, such as GS, the optimal ranked list can be computed with respect to G and evaluated with respect to S, resulting in a real suboptimal nDCG. The GS confusion matrix can also be computed from the data given and the simulation described above performed, yielding an estimated limiting nDCG. These actual and estimated limiting nDCG values can then be compared.
Using the TREC Enterprise data, Figure 1 compares the estimated limiting nDCG obtained through simulation with a confusion matrix (x-axis) with the real suboptimal nDCG (y-axis) obtained from different assessors. The left plot uses a confusion matrix CENT obtained from the TREC Enterprise data itself, as described above, while the right plot uses a confusion matrix CMSR obtained from a user study conducted by Microsoft Research [3]. Note that the more accurate confusion matrix yields better simulated results, as expected, and that the confusion matrix alone can be used to accurately estimate limiting nDCG values in most cases.
Given that a confusion matrix alone can be used, via simulation, to estimate limiting nDCG performance, we next consider other data sets and their associated real or estimated confusion matrices. Yandex [5] conducted user studies to obtain confusion matrices specific to Russian search (CY anR) and to Ukrainian search (CY anU ), and these were shown to improve learning-to-rank performance if the learner was given such models as input. Table 1 presents the estimated limiting nDCG values when applying three different confusion matrices to two learning-to-rank data sets. For comparison, the last column in the table presents the actual best known performance of a learning algorithm on these data sets. Consider the difference between the estimated limiting nDCG bounds (middle three columns) and known ranking performance (last column): If CMSR is a good model of assessor disagreement for these data sets, then the known learning-to-rank performance is reasonably close to the limiting bound, and little improvement is possible. On the other hand, if CYanU is a better model of inter-assessor disagreement, then learning algorithms have room for improvement.

3. CLOSED FORM APPROXIMATION
Let L = {0, 1, 2, 3, 4} be the set of relevance grades, nk the number of documents with reference label k  L, n the total

Collection MSLR30K(SIM) MSLR30K(CFA)
Yahoo(SIM) Yahoo(CFA)

C MSR 0.780 0.794 0.861 0.887

C YanR 0.867 0.869 0.920 0.919

C YanU 0.900 0.898 0.944 0.938

LearnToRank 0.741
0.801

Table 1: nDCG upper bounds derived from disagreement models C applied to popular learning-torank data sets. SIM rows are simulated values; the CFA rows are closed form approx. Last column is best known learning-to-rank performance.

number of documents in the rank-list, and Prank(i, r) the probability that the rank of a given document with reference label i is r, as ordered by the alternative labels j. One can then show that the expected nDCG as measured by reference labels is

E [nDC G]

=

h

I

1 dealDC

G

∑PiL

ni

∑

gain(i)

∑

Pn
r=1

i Prank (i,r)
discount(r)

where

Prank(i, r)

=

P
jL

h cij

∑

"Pr-1
h=0

Pn-h-1
s=r-1-h

ij (h,s) "i
s+1

and ij(h, s) is the probability that other s documents have

the same alternative label j, and other h documents have

alternative label higher than j, given that a particular doc-

ument with reference label i has alternative label j. Com-

puting ij(h, s) straightforwardly is inefficient for even moderately long rank-lists, with a running time of O(n2|L|).

We instead employ a closed form approximation (CFA)

based on approximating , a sum-product of binomial con-

ditional distributions, with a Gaussian joint distribution of

two variables (h + s, s). This approximation becomes more

accurate as rank-lists get longer. For a fixed i and j we have

`h+s¥
s



N ij (µ,

),

µ

=

`µh+s
µs

¥,

and



=

` h2+s cov h+s,s
cov s,h+s s2

¥

where

(1)

aij

=

P
kj

cik ,

(2)

µh+s

=

-aij

+

P
kL

nk

∑

akj ,

(3)

µs

=

-cij

+

P
kL

nk

∑

ckj ,

and

h2+s

=

-aij

∑

(1

-

aij )

+

P
kL

nk

∑

akj

∑

(1

-

akj )

s2

=

-cij

∑

(1

-

cij )

+

P
kL

nk

∑

ckj

∑

(1

-

ckj )

cov s,h+s

=

-cij

∑

(1

-

aij )

+

P
kL

nk

∑

ckj

∑

(1

-

akj ).

We can approximate E[nDCG] in O(n2) time given that the "spread" of the Gaussian grows as O( n) per component. The CFA rows of Table 1 show closed form approximations for comparison with simulated nDCG upper bounds.

4. CONCLUSION

We present a simple probabilistic model of assessor disagreement and results which indicate that the performance of learning-to-rank algorithms may be approaching inherent limits imposed by such disagreement.

5. REFERENCES
[1] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P.
de Vries, and E. Yilmaz. Relevance assessment: Are
judges exchangeable and does it matter? SIGIR, 2008.

[2] P. Bailey, A. P. De Vries, N. Craswell, and I. Soboroff. Overview of the TREC-2007 Enterprise Track.

[3] B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais. Here or there: Preference judgments for relevance. In ECIR, 2008.

[4] B. Carterette and I. Soboroff. The effect of assessor error on ir system evaluation. In SIGIR, 2010.

[5] A. Gulin, I. Kuralenok, and D. Pavlov. Winning the transfer learning track of Yahoo!'s learning to rank challenge with YetiRank. Journal of Machine Learning Research, 2011.

1092

Incorporating Statistical Topic Information in Relevance Feedback


Karla Caballero
UC, Santa Cruz Santa Cruz CA, USA
karla@soe.ucsc.edu

Ram Akella
UC, Santa Cruz Santa Cruz CA, USA
akella@soe.ucsc.edu

ABSTRACT
Most of the relevance feedback algorithms only use document terms as feedback (local features) in order to update the query and re-rank the documents to show to the user. This approach is limited by the terms of those documents without any global context. We propose to use statistical topic modeling techniques in relevance feedback to incorporate a better estimate of context by including global information about the document. This is particularly helpful for difficult queries where learning the context from the interactions with the user is crucial. We propose to use the topic mixture information obtained to characterize the documents and learn their topics. Then, we rank documents incorporating positive and negative feedback by fitting a latent distribution for each class of documents online and combining all the features using Bayesian Logistic Regression. We show results using the OHSUMED dataset for 3 different variants and obtain higher performance, up to 12.5% in Mean Average Precision (MAP).
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Relevance Feedback, Document Filtering
General Terms
Algorithms, Experimentation
Keywords
Relevance Feedback,Topic Models, Language Models
1. INTRODUCTION
Relevance feedback has been studied extensively in Information Retrieval as a form of incorporating feedback from the user to refine the results retrieved. The authors in [5] concluded that negative feedback is also valuable to improve the ranking. However, the need to capture broader context in difficult queries is still a challenge. The authors in [2] have showed that including global features and using clusters can improve the retrieval performance significantly. Thus, statistical topic modeling provides a robust and automatic method to incorporate context to the user feedback.
Main contact.
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

Previous approaches have used statistical topic models to represent documents according to their latent topic content and use this representation in information retrieval [1, 4]. Authors in [4], use topics as a form of smoothing the Language Model used in retrieval. However, this approach does not address the incorporation of relevance feedback. Recent work from [1] explores the use of topics as a form to perform the query expansion for relevance feedback. However, this action might make the query noisier because the top topic terms might not contribute to a better discrimination of the relevant documents. In addition, these terms might not be distinctive across different topics.
We propose to include the topic information as feedback using the document topic mixture instead of the document word mixture. We first estimate the topic mixture for each document in the corpus using LDA and save it as meta data. Given an initial query we use a standard retrieval engine, Language Models for this case, to show the first set of documents to the user and obtain relevance judgments. Then, we assume that topic mixtures for feedback documents are observed. We then define two latent Dirichlet distributions: one for relevant documents and another for nonrelevant documents. We fit these distributions iteratively, by finding a sufficient statistic and maximizing the likelihood of observing this statistic. To score the documents, we use Bayesian Logistic Regression. This function results in a very efficient scoring function, and incorporates the benefits of active learning. Under this model, we incorporate positive and negative feedback, and context based on topics in the interaction without changing the query. We also provide efficient updates of the latent distributions based on topics.

2. METHODOLOGY

In this section we describe how we incorporate the topic

mixture of feedback documents as a global measure in con-

trast to query expansion. To achieve this, we estimate the

topic mixtures of the documents, i, for K topics in the cor-

pus using LDA off-line. Given a initial ranking, the user

provides relevance feedback which is used to fit the latent

relevant/non-relevant distributions of topic mixtures. Thus,

we assume two Dirichlet distributions: one for the relevant

set of documents R, and one for the non-relevant documents RØ . Therefore, we have:

P (i|R)



Dirichlet(R)

=

(

K k=1

k,R

)

K k=1

(k,R)

K
k,R -1 i,k k=1

for R and RØ . Then, we calculate the log-probability of the document being generated by those distributions:

1093

K

K

K

LogP (i|) = log ( k)- log (k)+ (k-1) log(i,k)

k=1

k=1

k=1

We denote these scores as P Ri and P RØi respectively. To

update the latent distributions of the relevant R and non-

relevant topics RØ, we use the topic content from the docu-

ments labeled by the user as relevant, R, and non-relevant,

RØ , after each interaction. The Dirichlet distribution guar-

antees a unique maximum when the Maximum Likelihood

(ML) is estimated for . Moreover, a sufficient statistics,

SS, can be estimated to update this distribution as more

observations are available. We can update SS efficiently

without keeping previous document feedback. The initial value of the sufficient statistic SSk(0,R) for the relevant topic k and its update from the interaction j is described by:

S Sk(0,R)

=

1 NR(0)

log i,k
iR0

S Sk(j,R)

=

NR(j-1) NR(j)

S

Sk(j,R-1)

+

1 NR(j)

log i,k
iRj

where NR(j) = NR(j-1) + |Rj |, and |Rj | is the total number of relevant documents at the j-th interaction. Given SSR(j) and SSR(Øj), we use the method proposed in [3] to calculate the ML estimator for (Rj) and (RØj). In addition to these distributions, we use the topic-based Language Model PT W for document i as follows:
K
PT W,i(w|i, ^) = P (w|z = k, ^k)P (z = k|i)
k=1

where ^ are the word mixture for the topics obtained from

LDA. Thus, the score ST W,i for query Q with terms q is defined as:

ST W,i =

PT W,i(w = q|i, ^)

qQ
To combine the scores from the latent relevant/non-relevant

topic mixtures and the topic-based Language Model, we use

the Bayesian Logistic Regression approach [5]. Let yi = {+1, -1} be the relevant/non-relevant label for document,

we have the score function:

P (yi|, di)

=

1

+

1 exp(-T diyi)

where di is the feature vector scores: P Ri, P RØi, ST W,i.  is a parameter vector assumed to be normally distributed and updated in a Bayesian form. Here, the distribution of  from the j-th iteration is taken as prior distribution for the next iteration. To approximate the posterior distribution we
use the Laplace approximation as discussed in [5].

3. RESULTS
We test our method using the OHSUMED dataset which consists of 196, 000 medical abstracts and 3, 506 relevance labels for 63 queries from the Document Filtering Track from TREC 4. As suggested in the track, we assume unobserved labels as non-relevant. We fit the LDA model using K = 50 topics, which is the number of topics with highest performance based on Empirical Likelihood. To test the impact of topic information, we use standard Language Model (LM) with Dirichlet smoothing described in [4] as baseline. This score is used with Bayesian Logistic Regression. We test 3 variants of the model and the baseline: LM as baseline;

Table 1: Results of Topic feedback using 50 topics

in the OHSUMED dataset

Method

P@10 MAP DiscGain

LM

0.3968 0.4286 0.5660

LM +ST W

0.4206 0.4557

LM +ST W +P R

0.2968 0.3307

LM +ST W +P R+P RØ 0.4698 0.5141

0.6315 0.5590 0.6580

LM+ST W ; LM+ST W +P R; LM+ST W +P R+P RØ. We calculate the initial ranking using LM and asked for feedback
until we have at least one relevant and one non-relevant documents. We use 10 feedback documents and estimate precision at 10 (P@10), Mean Average Precision (MAP), and Discounted Gain (DiscGain). There are two relevance level labels available in the dataset, {1, 2}, that are assumed equally, {+1} for P@10 and MAP. However for DiscGain, we use both labels in the evaluation.
Table 1 shows the results for the variants tested. We observe that the LM+ST W performs better than the baseline. This score is similar to the LDA-based retrieval proposed in [4] but the value of the linear combination parameters  is fitted based on the feedback as opposed to a corpus-wide parameter. When we incorporate only the score from the relevant distribution of topics P R, the performance decreases. However, when the score for the non-relevant distribution P RØ is incorporated, the performance is the highest. This shows the value of negative feedback reported previously in [5]. We notice that, the combination of P R and P RØ is equivalent to the log of the likelihood ratio test (probabilistic ranking principle) weighted by . This also explains why both scores should be included in the model.
We observe that the combination of the four scores improves the general performance by: 11.6% P@10, 12.8%
MAP, and 4.6% DiscGain respect topic-based language model (LM+ST W ). This demonstrates, the power of statistical topic modeling in relevance feedback.

4. CONCLUSION AND FUTURE WORK
We have presented a method to incorporate statistical topic information in relevance feedback without changing the query. Results show that including the mixture of topics in relevance feedback improves the performance by pruning the search space, and adding context to the query. As future work, we plan to incorporate a policy to decide when to update the parameters of the relevant and non-relevant topic distribution optimally.

5. ACKNOWLEDGMENTS
This work is partially funded by CONACYT grant 207751 and SAP Gift Support
6. REFERENCES
[1] D. Andrzejewski and D. Buttler. Latent topic feedback for information retrieval. In Proceedings of the 17th ACM SIGKDD conference, KDD '11, pages 600≠608, 2011.
[2] K. S. Lee, W. B. Croft, and J. Allan. A cluster-based resampling method for pseudo-relevance feedback. In Proceedings of the SIGIR conference, pages 235≠242, 2008.
[3] T. Minka. Estimating a dirichlet distribution. Technical report, 2003.
[4] X. Wei and W. B. Croft. Lda-based document models for ad-hoc retrieval. In Proceedings of the 29th ACM SIGIR conference, SIGIR '06, pages 178≠185, 2006.
[5] Z. Xu and R. Akella. A bayesian logistic regression model for active relevance feedback. In Proceedings of the 31st ACM SIGIR conference, SIGIR '08, pages 227≠234, 2008.

1094

Inferring Missing Relevance Judgments from Crowd Workers via Probabilistic Matrix Factorization

Hyun Joon Jung
Dept. of Electrical and Computer Engineering University of Texas at Austin
hyunJoon@utexas.edu

Matthew Lease
School of Information Science University of Texas at Austin
ml@ischool.utexas.edu

ABSTRACT
In crowdsourced relevance judging, each crowd worker typically judges only a small number of examples, yielding a sparse and imbalanced set of judgments in which relatively few workers influence output consensus labels, particularly with simple consensus methods like majority voting. We show how probabilistic matrix factorization, a standard approach in collaborative filtering, can be used to infer missing worker judgments such that all workers influence output labels. Given complete worker judgments inferred by PMF, we evaluate impact in unsupervised and supervised scenarios. In the supervised case, we consider both weighted voting and worker selection strategies based on worker accuracy. Experiments on crowd judgments from the 2010 TREC Relevance Feedback Track show promise of the PMF approach merits further investigation and analysis.
Categories and Subject Descriptors
I.2.6 [Artificial Intelligence]: Learning
General Terms
Algorithms, Design, Experimentation, Performance
Keywords
Crowdsourcing, label aggregation, matrix Factorization
1. INTRODUCTION
Crowdsourced relevance judging offers potential to reduce time, cost, and effort of relevance judging [1] and benefit from greater diversity of crowd judges. However, quality of judgments from non-workers continues to be a concern, motivating continuing work in quality assurance methods based on statistical label aggregation methods or greater attention to human factors. A common approach is to collect multiple, redundant judgments from workers and aggregate them via methods like majority voting (MV) or expectation maximization (EM) to produce consensus labels [4].
Because each crowd worker typically judges only a small number of examples, collected judgments are typically sparse and imbalanced, with relatively few workers influencing output consensus labels. MV is completely susceptible to this problem. EM addresses this indirectly: while only workers
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

Figure 1: Crowdsourcing workers judgments (Left) are copied to a sparse worker-task matrix (Middle). Missing judgments are inferred via PMF (Right).

labeling an example vote on it, global worker judgments are used to infer class priors and worker confusion matrices.
We propose to tackle this issue directly by adopting a collaborative filtering approach, which routinely deals with the issue of each user rating only a small number of items (e.g., movies, books, etc.) vs. the complete set. In particular, we employ probabilistic matrix factorization (PMF), which induces a latent feature vector for each worker and example [6] in order to infers unobserved worker judgments for all examples. Figure 1 depicts our approach graphically.
We are not familiar with any prior work investigating PMF, or collaborative filtering approaches more generally, toward crowdsourcing quality assurance. Related prior work has investigated other ways to infer bias corrected labels in place of raw labels [4], as well as inference of missing labels by estimating a unique classifier for each worker [3].
Probabilistic Matrix Factorization (PMF). Suppose we have M tasks (examples to be labeled), N workers, and a label matrix R in which Rij indicates the label of worker i for task j. Let U  RDM and V  RDN be latent feature matrices for workers and tasks, with column vectors Ui and Vj representing D-dimensional worker-specific and taskspecific latent feature vectors, respectively. The conditional probability distribution over the observed labels R  RNM is given by Equation 1. Indicator Iij equals 1 iff worker i labeled task j. We place zero-mean spherical Gaussian priors on worker and task feature vectors (Equations 2 and 3).

NM

p(R|U, V, 2) =

[N (Rij |UiT Vj , 2)]Iij

(1)

i=1 j=1

N

p(U |U2 ) = [N (Ui|0, U2 I)]

(2)

i=1

M

p(V |V2 ) = [N (Vj |0, V2 I)]

(3)

j=1

1095

Method 1 2 3 4 5 6 7

Supervised No No No Yes Yes Yes Yes

Worker Labels raw (sparse) raw (sparse) PMF (complete) raw (sparse) raw (sparse) raw (sparse) PMF (complete)

Label Aggregation MV EM MV WV
Filtering(=0.67) WV & Filtering(=0.67) WV & Filtering(=0.7)

ACC 0.603 0.644 0.643 0.642 0.752 0.750 0.673

Rank 4 3 3 3 1 1 2

RMSE 0.63 0.596 0.598 0.598 0.498 0.500 0.571

Rank 4 3 3 3 1 1 2

SPE 0.332 0.418 0.440 0.900 0.838 0.848 0.542

Rank 6 4 5 1 2 2 3

Table 1: Results of PMF-based inference of missing worker labels. For the unsupervised case, majority voting (MV) with PMF (Method 3) is compared to MV and EM approaches using input (sparse) worker labels (Methods 1-2). With supervision, we compare weighted voting (WV) and/or filtering with and without PMF. Ranks shown indicate statistically significant differences at p <= 0.05 using a two-tailed paired t-test.

To estimate model parameters, we maximize the log-posterior over task and worker features with fixed hyper-parameters. Maximizing the posterior with respect to U and V is equivalent to minimizing squared error with L2 regularization:

1N 2

M

Iij (Rij

-

UiT

Vj )2

+

U 2

N

Ui

2 F

+

V 2

M

Vj

2 F

i=1 j=1

i=1

i=1

where U = U /, V = V /, and

2 F

denotes

the

Frobe-

nius Norm. We use gradient descent to find a local mini-

mum of the objective for U and V . Finally, we infer missing

worker judgments in the worker-task matrix R by taking

the scalar product of U and V. Note that as in [4], we also

replace actual labels with bias-corrected inferred labels.

Label Aggregation. Given the complete set of inferred

worker relevance judgments in matrix R, we next aggregate

worker judgments to induce consensus labels. We consider

both unsupervised supervised scenarios. In the former, we

consider majority voting with raw (sparse) labels (Method

1), expectation maximization with raw labels (Method 2),

and PMF-based MV (Method 3). In the supervised case, we

measure each worker's accuracy based on expert judgments,

with labels of anti-correlated workers flipped such that ac-

curacy is always  50%. We use supervision in two distinct

ways: weighted voting (WV) and worker filtering, in which

only workers with accuracy   participate in voting.

2. EVALUATION
Experiments are performed on crowd judgments collected in the 2010 TREC Relevance Feedback Track [2] from Amazon Mechanical Turk. 762 crowd workers judged 19033 querydocument tasks (examples), and 89624 judgments were collected. Our worker-task matrix thus has 762 columns (workers) and 19,033 rows (tasks); only 89,624 out of 14,503,146 labels (0.6%) are observed, so data is extremely sparse. 3,275 expert relevance judgments by NIST are partitioned into training (2,275) and test (1,000) sets. The test set is evenlybalanced between relevant and non-relevant classes.
Parameters. For dimensionality of task and worker latent feature vectors, we consider D  10, 30, 50 and select D = 30 based on cross-validation on the entire set of labels (unsupervised). We similarly tune regularization parameter   {0.001, 0.01, 0.1, 0.5} and select  = 0.1. We tune the worker filtering threshold   [0.6, 0.99] by cross-validation on the training set using a linear sweep with step-size 0.01.
Metrics and Results. Table 1 reports accuracy (ACC), RMSE, and specificity achieved by each method.
Unsupervised Methods. Method 2 of PMF with ma-

jority voting (MV) outperforms the MV baseline (Method 1) and performs equivalently to EM (Method 2).
Supervised vs. Unsupervised Methods. While supervised methods tend to dominate, unsupervised EM and PMF both match performance of the supervised weighted voting (WV) method without filtering or PMF (Method 4).
Supervised Methods. Worker filtering is clearly seen to provide the greatest benefit, and surprisingly performs better without PMF than with PMF (Methods 6 vs. 7). When filtering is used, use of WV is not seen to further improve performance (Methods 5 vs. 6). We do see PMF-based modeling outperform non-PMF modeling when worker filtering is not employed (Methods 7 vs. 4).
3. CONCLUSION
While unsupervised consensus labeling accuracy with PMF only matched EM performance, PMF is advantageous in that once complete worker judgments are inferred, they might be used for a variety of other purposes, such as better routing or recommending appropriate tasks to workers.
Intuitively, an accurate worker's empirical label distribution should resemble the actual class prior. This suggests an alternative, more weakly supervised scenario to consider in which class priors are known while example labels are not. In the unsupervised case, we might instead simply examine the distribution of empirical priors for each worker and detect outliers [5]. In future work, we plan to investigate these ideas further in combination with those described here.
4. REFERENCES
[1] O. Alonso, D. Rose, and B. Stewart. Crowdsourcing for relevance evaluation. SIGIR Forum, 42(2):9≠15, 2008.
[2] C. Buckley, M. Lease, and M. D. Smucker. Overview of the TREC 2010 Relevance Feedback Track (Notebook). In Proc. of the 19th Text Retrieval Conference, 2010.
[3] S. Chen, J. Zhang, G. Chen, and C. Zhang. What if the irresponsible teachers are dominating? a method of training on samples and clustering on teachers. In 24th AAAI Conference, pages 419≠424, 2010.
[4] P. Ipeirotis, F. Provost, and J. Wang. Quality management on amazon mechanical turk. In Proceedings of the ACM SIGKDD workshop on human computation, pages 64≠67. ACM, 2010.
[5] H. J. Jung and M. Lease. Improving Consensus Accuracy via Z-score and Weighted Voting. In AAAI Workshop on Human Computation (HComp), 2011.
[6] R. Salakhutdinov and et al. Probabilistic matrix factorization. In NIPS 2008, volume 20, January 2008.

1096

Looking Inside the Box: Context-Sensitive Translation for Cross-Language Information Retrieval

Ferhan Ture1, Jimmy Lin2,3, Douglas W. Oard2,3
1Dept. of Computer Science, 2College of Information Studies, 3UMIACS University of Maryland
fture@cs.umd.edu, jimmylin@umd.edu, oard@umd.edu

ABSTRACT
Cross-language information retrieval (CLIR) today is dominated by techniques that use token-to-token mappings from bilingual dictionaries. Yet, state-of-the-art statistical translation models (e.g., using Synchronous Context-Free Grammars) are far richer, capturing multi-term phrases, term dependencies, and contextual constraints on translation choice. We present a novel CLIR framework that is able to reach inside the translation "black box" and exploit these sources of evidence. Experiments on the TREC-5/6 EnglishChinese test collection show this approach to be promising.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords: machine translation, context
1. INTRODUCTION
Query translation approaches for cross-language information retrieval (CLIR) can be pursued either by applying a machine translation (MT) system or by using a token-to-token bilingual mapping. These approaches have complementary strengths: MT makes good use of context but at the cost of producing only one-best results, while token-to-token mappings can produce n-best token translations but without leveraging available contextual clues. This has led to a small cottage industry of what we might refer to as "context recovery" in which postprocessing techniques are used to select or reweight translation alternatives, usually based on evidence from term co-occurrence in a comparable collection.
We argue that this false choice results from thinking of MT systems as black boxes [5]. Inside an MT system we find not alternative translations for individual tokens, but rather for entire sentences. In state-of-the-art MT systems, these alternative "readings" are based on Synchronous Context-Free Grammars (SCFG) and pieced together from units of varying size with complex hierarchical dependencies. Reducing these to context independent token translation probabilities discards potentially useful contextual constraints. An elegant solution, which we explore in this work, is to perform translation in context using a full SCFG MT system and then to reconstruct context-sensitive n-best token translation probabilities by tokenizing each reading and accumulating translation likelihood evidence, which can then be renormalized as estimates of probabilities. This technique is now routinely used in speech retrieval [7], but we are not aware of its prior use for CLIR.
These context-sensitive token translation probabilities can then be used in the same way as context-independent probabilities. We
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

use a technique based on mapping term statistics before computing term weights [8, 2] to establish a strong context-independent baseline. Experiments on the TREC-5/6 English-Chinese CLIR task show that our new approach yields promising (although not statistically significant) improvements over that baseline.

2. APPROACH
We consider the technique presented by Darwish and Oard [2] as the baseline. Given a source-language query s, we represent s in the target language as a probabilistic structured query, where weights are derived from word-to-word bilingual translation probabilities that are learned automatically from parallel text ("bitext"):

# tokens

Score(D|s) =

bm25(tf(sj , D), df(sj ))

(1)

j=1

tf(sj, D) =

tf(ti, D)P rbitext(ti|sj ) (2)

ti,P rbitext(ti|sj )>L

df(sj) =

df(ti)P rbitext(ti|sj )

(3)

ti,P rbitext(ti|sj )>L

where L is a lower bound on conditional probability. We also impose a cumulative probability threshold, C, so that translation alternatives are added (starting from most probable ones) until the cumulative probability has reached C. We use the Okapi BM25 term weighting function, although in principle any other weighting function can be substituted.
Compared against this baseline, we show how we can take advantage of a full MT decoder to better estimate translation probabilities. A decoder uses a translation model (TM) and language model (LM) to find all possible derivations of a source text, and the corresponding translations in the target language, along with associated derivation scores. We use cdec [3], a state-of-the-art MT system which provides fast "decoding" using Hiero-style synchronous grammars [1] for representing the translation model in a way that can model distant dependencies within a sentence.
As a point of comparison, we might use only the best translation:

m

Score(D|s) = bm25(TF(t(i1), D), DF(t(i1)))

(4)

j=1

where t(1) is the most probable translation of s, computed by:

t(1) = arg max (t|s) = arg max TM(s, t)LM(t) (5)

t

t

where is the likelihood function, a mapping learned by the decoder, which scores each derivation using the TM and LM.
Decoders produce a set of candidate sentence translations in the process of computing equation (5), so we can generalize our model

1105

to consider the n candidates with highest likelihood, for some n > 1. In this case, the score of document D would be a weighted average of scores with respect to each candidate translation:

N

Score(D|f ) = Score(D|e(k))P rcdec(e(k)|f )

(6)

k=1

where P rcdec is the normalized likelihood value. In order to compute tf and df statistics for tokens, we start by
tokenizing each candidate sentence translation. For each token sj of source query s, we use word alignments in the grammar rules to determine which target tokens it is associated with. By doing this,
we are constructing a probability distribution of possible translations of sj based on the n query translations. Specifically, if source token sj is aligned to (i.e., translated as) ti in the kth best translation, it receives a weight equal to P rcdec(t(k)|s).1 As a result, we can map tf and df statistics by replacing P rbitext with P rnbest (see below,  is the normalization factor) in Equations 2 and 3.

P rnbest(ti|sj )

=

1 

N

k=1

ti

P rcdec(t(k)|s) (7)

sj aligned to ti in t(k)

This new probability distribution (i.e., P rnbest) is based only on the n translations that the decoder scores highest for the source query. Therefore, the distribution is informed by the query context and its derivation by the translation model. From this we would expect the distribution to be better biased in favor of appropriate translations, but perhaps at the cost of some reduction in variety due to overfitting. Finally, we can combine the two probability estimates to mitigate overfitting using simple linear interpolation: P rc(sj ) = P rnbest(sj ) + (1 - )P rbitext(sj ).

3. EVALUATION
We evaluated our system on the TREC-5/6 CLIR task, using a corpus of 164,778 Chinese documents and titles of the 54 English topics as queries. The evaluation metric is Mean Average Precision (MAP). The English-to-Chinese translation model was trained using the FBIS parallel text collection, which contains 1.6 million parallel sentences. The Chinese collection was tokenized using the Stanford segmenter for Chinese, the Porter stemmer was used for English, and alignment was performed using GIZA++ [6]. A SCFG was extracted from these alignments using a suffix array [4]. A Chinese token 3-gram model serves as the LM.
Results are summarized in Figure 1. At the left edge of the graph, at  = 0, we have the approach in equation (2) with contextindependent translation probabilities (call this A).2 At the right edge of the graph, at  = 1.0, we rely exclusively on context-sensitive probabilities (call this B). Effectiveness peaks at  = 0.78 (call this C).3 For reference, the horizontal line represents simply taking the one-best translation from the MT system (call this D). We also tried one-best context-independent translation for each token, and the MAP score was 0.2431. A randomization test shows that this is significantly below A, B, C and D (p < 0.05). On the other hand, A, B, C, and D are statistically indistinguishable on this test collection. Yet, results are still promising: when C is compared to A and D, the p-value is approximately 0.15 and 0.18, respectively. Also, a
1Since a source term may be aligned to multiple target terms in the same query translation, we still need to normalize the final weights. 2We selected C = 0.95 and L = 0.005 for baseline model parameters after manually trying a range of values. 3We selected N = 10 for conditions B and C, but N = 5 also yields similar results, peaking at a MAP score of 0.3387.

0.35
0.34
D=0.3306 0.33

C=0.3404 B

MAP

0.32
A 1-best, context-sensitive 10-best, linear mixture
0.31 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Figure 1: Evaluation on TREC-5/6 English-Chinese CLIR task.
topic-specific analysis shows that C yields better average precision than D on 36 of the 54 topics (which actually is significant by a two-tailed sign test at p < 0.05).
4. CONCLUSIONS AND FUTURE WORK
In this work, we have introduced an approach that combines the representational advantage of probabilistic structured queries with the richness of the internal representation of a translation model. We introduced a novel way to learn term translation probabilities from the top scoring "readings" of alternative query translations, as generated by the decoder. We evaluated our approach on the English-Chinese CLIR task of TREC-5/6: although we did not observe significant improvements, we feel that this approach is nevertheless promising. In future work we plan to try this approach for document translation (where we would expect greater benefit from context, although with higher computational cost, at least in experimental settings). Replications on test collections with larger numbers of topics, and with a greater variety of query and topic languages, can also be expected to yield additional insights.
Acknowledgements. This work was supported in part by DARPA BOLT under contract HR0011-12-C-0015. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the view of DARPA.
5. REFERENCES
[1] D. Chiang. Hierarchical phrase-based translation. Computational Linguistics, 33:201≠228, 2007.
[2] K. Darwish and D. W. Oard. Probabilistic structured query methods. In SIGIR, 2003.
[3] C. Dyer, J. Weese, H. Setiawan, A. Lopez, F. Ture, V. Eidelman, J. Ganitkevitch, P. Blunsom, and P. Resnik. cdec: a decoder, alignment, and learning framework for finite-state and context-free translation models. In ACL Demos, 2010.
[4] A. Lopez. Hierarchical phrase-based translation with suffix arrays. In EMNLP, 2007.
[5] W. Magdy and G. Jones. Should MT systems be used as black boxes in CLIR? In ECIR, 2011.
[6] F. Och and H. Ney. A systematic comparison of various statistical alignment models. CL, 29(1):19≠51, 2003.
[7] J. Olsson and D. Oard. Combining LVCSR and vocabulary-independent ranked utterance retrieval for robust speech search. In SIGIR, 2009.
[8] A. Pirkola. The effects of query structure and dictionary-setups in dictionary-based cross-language information retrieval. In SIGIR, 1998.

1106

On Building a Reusable Twitter Corpus

Richard McCreadie1, Ian Soboroff2, Jimmy Lin3, Craig Macdonald1, Iadh Ounis1, Dean McCullough2
1 University of Glasgow, Glasgow G12 8QQ, UK 2 National Institute of Science and Technology, MD 20899, USA
3 University of Maryland, MD 20742, USA
{richard.mccreadie, craig.macdonald, iadh.ounis}@glasgow.ac.uk1, {ian.soboroff, dean.mccullough}@nist.gov2, jimmylin@umd.edu3

ABSTRACT
The Twitter real-time information network is the subject of research for information retrieval tasks such as real-time search. However, so far, reproducible experimentation on Twitter data has been impeded by restrictions imposed by the Twitter terms of service. In this paper, we detail a new methodology for building and distributing Twitter corpora, developed through collaboration between the Text REtrieval Conference (TREC) and Twitter. In particular, we detail how the first publicly available Twitter corpus ≠ referred to as Tweets2011 ≠ was distributed via lists of tweet identifiers and dedicated tweet crawling software. Furthermore, we analyse whether this distribution approach remains robust over time, as tweets in the corpus are removed by users. Tweets2011 was successfully used by 58 participating groups for the TREC 2011 Microblog track, and our results attest to the robustness of the crawling methodology over time.
Categories and Subject Descriptors: H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval
General Terms: Experimentation, Performance
Keywords: Twitter, Corpus Creation, Reproducibility
1. INTRODUCTION
Twitter is a communications platform on which users can send short, 140-character messages, called "tweets", to their "followers" (other users who subscribe to those messages). Conversely, users can receive tweets from people they follow via a number of mechanisms, including web clients, mobile clients, and SMS. As of Spring 2011, Twitter has over 140 million active users worldwide, who collectively post over 340 million tweets per day. Twitter is an active research area in the information retrieval (IR) field [1]. However, previously it has not been possible to build and distribute reusable tweet corpora due to restrictions placed upon researchers by Twitter's terms of service.1 Indeed, this has resulted in two prior unsuccessful attempts to share Twitter data by Stanford and Edinburgh universities.
The Text REtrieval Conference (TREC) is a workshop series that aims to improve the state of the art in information access task effectiveness through building sharable test collections. Beginning in 2011, TREC ran the Microblog track
1twitter.com/tos
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

Figure 1: Illustration of tweet crawling.
that investigated tweet search and ranking [2]. In line with the TREC aims and in collaboration with Twitter, a new methodology was developed to build and distribute a publicly available Twitter dataset, known as Tweets2011. In this paper, we describe this new methodology, whereby a corpus like Tweets2011, is distributed as a set of tweet identifiers and a tweet crawling tool for downloading the identified tweets. However, since researchers separately download the tweets themselves at different times, and the set of tweets available is not static (as tweets can be deleted), the exact composition of the corpus will vary depending upon when it is downloaded. We also examine how robust the distribution methodology is over time, by comparing crawls of Tweets2011 made at different points in time. Our results show that over the time period tested, the changes in the corpus over time had no noticeable effect on the systems that participated in the TREC 2011 Microblog track.
2. DISTRIBUTION METHODOLOGY
The Twitter terms of service forbids third parties from data redistribution, which means that researchers that have gathered tweets cannot share them. To overcome these constraints, a compromise had to be reached. In particular, a collection would not consist of the tweets themselves, but rather (username, tweet id) pairs and associated software for reconstructing the tweets.
The Twitter REST API provides flexible access to any available tweet; nearly all common Twitter capabilities can be programmatically accessed, e.g., posting new tweets, retweeting, following a user, searching, etc. The API is generally available to the public, although by default it is rate limited; the most common unauthenticated connection places a limit of 150 requests per hour. This restriction makes it impractical to gather large number of tweets for offline processing. Historically, Twitter has lifted the API request limit for some clients based on a particular IP address or an authentication token, but this capability is no longer offered.

1113

Figure 2: Tweets2011 tweet distribution over time.
The software for reconstructing tweets is an asynchronous HTTP fetcher that downloads each tweet individually. For researchers that had access to the REST API without rate limit restrictions, that can be used to download each tweet in JavaScript Object Notation (JSON) format. However, for researchers without this access, the fetcher instead crawls raw HTML pages from the twitter.com site and reconstructs the tweets in JSON format, as shown in Figure 1.
3. TWEETS2011 AND TREC 2011
We used the above methodology to distribute the Tweets2011 collection to the participants of the TREC 2011 Microblog track, which is now available to everyone.2 To create Tweets2011, we identified a common set of tweets (username, tweet id pairs), for distribution. In particular, we created a sample from tweets posted during the period from January 23rd to February 8th, 2011 (inclusive). It was important to sample, as even for users who were not rate limited, downloading billions or more tweets in a sequential manner would not be practical. Instead, some spam removal was performed and then approximately 1% of the remaining tweets were sampled for the corpus, resulting in a set of approximately 16 million tweets. The distribution of these tweets over the two week period is shown in Figure 2. The chosen time period includes the Egyptian revolution as well as the US Superbowl, and a spike of tweeting activity on February 6th is easily observed. Moreover, to ensure the corpus was representative of the multi-lingual tweet retrieval environment, no language filtering was performed.
For the evaluation of TREC 2011 participating systems, 49 topics were created. From the pool of 50,324 tweets formed from the participants runs for these topics, 2,965 were judged relevant.
4. COLLECTION DEGRADATION
Tweets2011 is unique in the history of information retrieval test collections in that it will degrade over time. Twitter users can delete their tweets, or mark their accounts as private, and after that point, these tweets will not be part of the collection. This is an experimental challenge because system effectiveness may be affected by missing tweets even if those tweets are not relevant, for example due to altered collection statistics. If two systems are compared on different versions of the collection, we worry that those comparisons may not be valid.
Figure 3 illustrates the effect as we have been able to observe it in the short lifetime of the collection so far. When someone downloads the collection in HTML format, which was the case for nearly all participants, each tweet has an HTTP status code associated with it. HTTP 404 indicates a deleted tweet, 403 indicates a protected tweet, and 302 indicates a retweet. Status 301 represents a new report code from the Twitter API and we are not entirely certain what
2trec.nist.gov/data/tweets/

Figure 3: HTTP statuses of participant crawls of the Tweets2011 collection over time.
it indicates; preliminary investigations suggest that it occurs for users that have changed their screen name since the tweets were originally sampled, but the frequency seems much too high for that to be the only explanation. For the 4th of November 2011 crawl, 3,424,155 (21.2%) of the original 16,141,809 tweets were unavailable. Note that for some status codes, the number of missing tweets is not monotonically increasing. This behavior can result from crawler (download) errors, as well as when users mark their accounts as private and subsequently open them again.
We measured the effect of this collection decay by recrawling the collection after all participants had done so, and removing 200 tweets that were pooled and judged for the 49 topics, but which were subsequently deleted or protected. We then computed the precision at rank 30 of all participating runs, and compared the ordering of runs by P@30 to the official results using Kendall's  . The correlation was 0.99, indicating that missing tweets seemed to affect all participating systems equally if at all.
Further decay in the collection may render it unusable at some point in the future. We do not recommend comparing evaluation scores done after TREC to the official TREC results, because of decay concerns. Rather, experimenters should compare multiple systems (or versions of the same system) that all use the same crawl, or closely contemporaneous crawls. This is not a concern for the official TREC results since participants all crawled the collection during a short window of time.
5. CONCLUSIONS
We detailed a new methodology for building and distributing tweet corpora, developed through collaboration between the Text REtrieval Conference (TREC) and Twitter. In particular, we described how tweet corpora can distributed via lists of tweet identifiers in tandem with tweet crawling software. This distribution approach will work for any collection of tweets. We detailed the first corpus to be distributed in this manner, Tweets2011. We recognized that the corpus may change over time as tweets are deleted, and through an analysis of different versions of the Tweets2011 corpus downloaded by TREC participants, we showed that the distribution method, at least to date, is robust to changes in the underlying corpus.
6. REFERENCES
[1] M. Busch, K. Gade, B. Larson, P. Lok, S. Luckenbill and J. Lin. Earlybird: Real-Time Search at Twitter. In Proc. of ICDE'12.
[2] I. Ounis, C. Macdonald, J. Lin and I. Soboroff. Overview of the TREC-2011 Microblog Track In Proc. of TREC'11.

1114

On Judgments Obtained from a Commercial Search Engine

Emine Yilmaz, Gabriella Kazai
Microsoft Research Cambridge, UK
{eminey,v-gabkaz }@microsoft.com

Nick Craswell, S.M.M. Tahaghoghi
Microsoft, Bellevue, WA, USA
{nickcr,saied.tahaghoghi}@microsoft.com

ABSTRACT
In information retrieval, relevance judgments play an important role as they are used for evaluating the quality of retrieval systems. Numerous papers have been published using judgments obtained from a commercial search engine by researchers in industry. As typically no information is provided about the quality of these judgments, their reliability for evaluating retrieval systems remains questionable. In this paper, we analyze the reliability of such judgments for evaluating the quality of retrieval systems by comparing them to judgments by NIST judges at TREC.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.3[Information Search and Retrieval]
General Terms
Experimentation, Human Factors, Measurement
Keywords
Crowdsourcing, Evaluation, Test Collection
1. INTRODUCTION
In information retrieval (IR), test collections are typically used to evaluate and optimize the performance of IR systems. The quality of a test collection can impact the conclusions of the evaluation, where the quality of the relevance judgments is a key factor. For example, evaluation outcomes are shown to be affected by using different judge populations [1] and different judging guidelines [3]. On the other hand, using different judges from the same population of NIST judges employed by TREC has been shown to lead to relatively stable conclusions as to which retrieval algorithm beats another [4].
In recent years, several papers using judgments obtained from a commercial search engine have been published [2]. Most of these papers use such judgments (which are typically not publicly available) to validate the superiority of their proposed methods over existing algorithms. Since judges employed by commercial search engine companies are likely to come from different populations than NIST judges and are likely to be subjected to different training and judging
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

procedures, we may reason that judgments from a commercial search engine are likely to lead to different conclusions than judgments from NIST judges.
We analyze whether judgments obtained from a commercial search engine are reliable, in terms of leading to the same evaluation conclusions as when using NIST judgements.
2. EXPERIMENTAL RESULTS
We use the test collection from the TREC Web Track Adhoc tasks from 2009 and 2010. This dataset consists of nearly 50K NIST relevance labels, roughly 25K in each year, for 50 topics in each year. We took these 100 topics and using the topic titles as queries we scraped the top 10 search results from Google and Bing for each query. This gave us a total of 1603 unique query-URL pairs for the 100 topics.
We constructed three different collections by obtaining judgments from three judge groups : judges from (1) the TREC Web track ad-hoc task (NIST), (2) a commercial search engine (ProWeb), and (3) crowdsourcing (Crowd). ProWeb judges were experienced and highly trained judges, employed by the search engine company, while crowd workers, recruited via Clickworker, received no prior training on relevance assessing.
The NIST judgments differ across the two years. In 2009, three relevance levels were used (Highly Relevant, Relevant, and Not Relevant), while in 2010, five grades of relevance were used (Navigational, Key, Relevant, Non-relevant, and Junk). The ProWeb and Crowd judgments were obtained using a simple interface that asked judges to rate a search result's usefulness to a query using a five point scale that can be viewed as a variation of the 2010 Web Track scale (Ideal, Highly Relevant, Relevant, Somewhat Relevant, Nonrelevant). Unlike the ProWeb and Crowd judges, the NIST judges were given descriptions (topic narrative) about what information need is associated with a particular query.
Using the different sets of judgments and the NDCG measure, we evaluate the effectiveness of the runs submitted to the TREC 2009 and 2010 Web Track ad-hoc task. Since we only have labels for a subset of the retrieved documents, we remove unjudged documents from the runs. To avoid variance due to having different documents labeled across the different judge groups, we only consider documents that were judged by all three groups.
To remove the inconsistency across different judge groups due to the different levels of relevance scales used, we converted all the judgments to the Web Track 2009 scale using the following mapping:Navigational (or Ideal ) judgments to Highly Relevant, Key and Relevant (or Highly Relevant and

1115

1.0

1.0

1.0

Kendall's tau = 0.896955

Kendall's tau = 0.860887

Kendall's tau = 0.928039

0.8

0.8

0.8

0.6

NDCG Crowd

0.6

NDCG Crowd

0.6

NDCG ProWeb

0.4

0.4

0.4

0.2

0.2

0.2

0.0

0.0

0.0

1.0

0.0

0.2

0.4

0.6

0.8

1.0

NDCG NIST

Kendall's tau = 0.639488

1.0

0.0

0.2

0.4

0.6

0.8

1.0

NDCG NIST

Kendall's tau = 0.538047

1.0

0.0

0.2

0.4

0.6

0.8

1.0

NDCG ProWeb

Kendall's tau = 0.783693

0.8

0.8

0.8

0.6

NDCG Crowd

0.6

NDCG Crowd

0.6

NDCG ProWeb

0.4

0.4

0.4

0.2

0.2

0.2

0.0

0.0

0.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

NDCG NIST

NDCG NIST

NDCG ProWeb

Figure 1: Comparisons of evaluation results for runs submitted to TREC 2009 (top) and TREC 2010 (bottom)

using three different sets of judges and their judgements

Relevant) to Relevant, and Non-relevant and Junk (or Somewhat Relevant and Non-relevant) to Not Relevant. We have also experimented with various other mappings but all supported the same conclusions of this poster.
Figure 1 shows the obtained scatter plots: TREC 2009 results in the top row and TREC 2010 in the bottom row. For each year, we plot the results obtained from evaluating the runs using the NIST vs ProWeb (left plot), NIST vs. Crowd (middle plot) and Crowd vs. ProWeb (right plot) judgments. In each plot, we also include the Kendall's  correlation between the resulting system rankings, obtained using the two respective sets of judgments.
We see that for TREC 2009, evaluations using the NIST, ProWeb and Crowd judgments mostly agree with each other (Kendall's  of 0.86-0.93). This suggests that the judgments obtained from a commercial search engine are more or less consistent with NIST: using them will not cause major differences in the evaluation. Crowd judgments may be somewhat noisier, but still lead to stable evaluations. The differences in the three plots could be caused by the consistency in the number and description of relevance grades between the Crowd and ProWeb judges as compared to the NIST judges.
On the other hand, for TREC 2010, evaluations using the ProWeb and NIST judgments are quite different (Kendall's  = 0.639). At first glance, one might think that ProWeb judgments are not reliable at evaluating the systems and the high agreement on the TREC 2009 data is due to chance. However, if we compare the agreements between the evaluations using the Crowd and ProWeb judgments, we also see low agreements. This suggests that the low correlation is specific to this particular TREC. As the figure suggests, systems that were submitted to this particular TREC have very similar performance. Hence, the Kendall's  statistic may be affected by these systems. Furthermore, when we

considered the cases where NIST judgments highly disagree with the Crowd and ProWeb judgments, we found that there are quite a few documents that got the best rating by the Crowd and ProWeb judges but were labeled as non-relevant by the NIST judges. When we analyzed the disagreement cases, we realized that these differences could be caused by the specific topic description that was given to the NIST judges, which limited the possible intents associated with a query. Since the Crowd and ProWeb judges were not given such topic descriptions, they would have considered all possible intents for a query when assigning labels to the documents.
Overall, our conclusion is that even though judgments from a commercial search engine could lead to slightly different conclusions than NIST judges in some settings, evaluations using the two judge groups seem mostly consistent.
3. REFERENCES
[1] Peter Bailey, Nick Craswell, Ian Soboroff, Paul Thomas, Arjen P. de Vries, and Emine Yilmaz. Relevance assessment: are judges exchangeable and does it matter. In Proc. of ACM SIGIR Conference, pages 667≠674. ACM, 2008.
[2] Christopher J. C. Burges, Robert Ragno, and Quoc Viet Le. Learning to rank with nonsmooth cost functions. In NIPS, pages 193≠200. MIT Press, 2006.
[3] Charles L.A. Clarke, Nick Craswell, Ian Soboroff, and Azin Ashkan. A comparative analysis of cascade measures for novelty and diversity. In Proc. of ACM WSDM Conference, pages 75≠84. ACM, 2011.
[4] Ellen M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. In Proc. of ACM SIGIR Conference, pages 315≠323. ACM, 1998.

1116

On Real-time Ad-hoc Retrieval Evaluation

Stephen E. Robertson
Microsoft Research 7 JJ Thomson Avenue Cambridge CB3 0FB, UK
stephenerobertson@hotmail.co.uk

Evangelos Kanoulas 
Information School University of Sheffield
Sheffield, UK
ekanoulas@gmail.com

ABSTRACT
Lab-based evaluations typically assess the quality of a retrieval system with respect to its ability to retrieve documents that are relevant to the information need of an end user. In a real-time search task however users not only wish to retrieve the most relevant items but the most recent as well. The current evaluation framework is not adequate to assess the ability of a system to retrieve both recent and relevant items, and the one proposed in the recent TREC Microblog Track has certain flaws that quickly became apparent to the organizers. In this poster, we redefine the experiment for a real-time ad-hoc search task, by setting new submission requirements for the submitted systems/runs, proposing metrics to be used in evaluating the submissions, and suggesting a pooling strategy to be used to gather relevance judgments towards the computation of the described metrics. The proposed task can indeed assess the quality of a retrieval system with regard to retrieving both relevant and timely information.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance Evaluation
General Terms
Experimentation, Measurement
Keywords
evaluation, measures, pooling, real-time retrieval, microblog
1. INTRODUCTION
Lab-based evaluations typically assess the quality of a retrieval system with respect to its ability to retrieve documents that are relevant to the information need of an end user. There are retrieval tasks however for which relevance is only one of the dimensions on which we want to evaluate retrieval systems. In particular, we consider a real-time ad-hoc retrieval task, where the user wishes to retrieve information that is not only relevant, but also recent ≠ the
We gratefully acknowledge the support provided by the European Commission grant FP7-PEOPLE-2009-IIF-254562.
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

focus of the recent TREC Microblog Track [1]. The corpus for the task was composed of about 16 million tweets with time-stamps. Information needs were represented by queries at specific time-stamps. Systems were expected to respond to each query with the most recent relevant tweets, prior to the query time.
Evaluating a system over a real-time ad-hoc task clearly requires a measure that accounts both for relevance and recency. Instead of inventing a new measure the TREC Microblog Track chose to use traditional set-based evaluation measures on a time-ordered ranked list of the submitted by participants ranked lists of tweets/runs. Each participant was asked to submit a set of up to 1000 tweets, submitted tweets were then ordered by reverse time, the top 30 were pooled, handed to NIST assessors to be assessed with respect to relevance ("interestingness"), and each system was then evaluated by precision@30 over these 30 time-ordered tweets.
What became immediately apparent was that such an experiment could not properly measure the effectiveness of a system to provide both timely and relevant tweets. A system could achieve optimal performance by returning just 30 relevant tweets, ignoring the time dimension. Hence, the optimal strategy for each run was to return a set of the top-30 tweets ordered by a probability (score) of relevance, even if these were not the most recent relevant tweets. An intuitive explanation of the flaw in this evaluation experiment is that runs were not evaluated against a single global target set of tweets (the most relevant, recent tweets) and thus the evaluation measure was only aware of the relevant and non-relevant tweets returned by each individual system separately. The aim of this poster is to correct this flaw by redefining a real-time ad-hoc task experiment, in a form suitable for the Microblog track, in which systems are evaluated for their ability to return both relevant and timely information.
2. REAL-TIME AD-HOC TASK
In what follows we define the following elements of the task:
∑ general description of the task
∑ submission requirements
∑ metric(s)
∑ pooling/assessment strategy
As in the Microblog Track [1] we assume that there is an effective timeline for items, and queries are time-specific.

1119

To simplify the evaluation of runs we assume that judgments are binary, i.e. items are either relevant or not relevant to a query. We briefly discuss graded relevance in Section 3 that follows.
General description of the task: The aim for each system is to retrieve the 30 most recent relevant items.
Submission requirements: Participants will need to submit (a) a set of not more than 30 items; (b) a ranked list of 1000 top previous items by relevance only. Evaluation will be based on the set only (see Metrics below), but the ranked list will be used in constructing the assessment pools (see pooling/assessment strategy below).
Metric(s): For each topic, we define a single Target Set, consisting of the 30 most recent relevant items known. Basic metrics will be recall and precision in relation to the target set. Further metrics might include e.g. F1. Defining a single target set informs the metrics of the optimal behaviour of a retrieval system, which is to retrieve the most recent 30 relevant items, allows the observation of suboptimal behaviour of submitted systems when the items they return in the submitted set are either irrelevant or too old, and makes scores comparable across all submitted runs.
Pooling/assessment strategy: The pooling process is iterative rather than one-off ≠ involving further additions to pools after initial items have been judged. The goal of the pooling process is to identify the items in the target set, that is the 30 relevant recent items. The iterative process follows:
1. Pool all sets of the 30 items submitted, together with the top 1 ranked item from all the ranked lists. Judging all sets avoids having holes in the sets of the 30 items over which runs are evaluated and thus enforces fairness across all submissions.
2. Obtain judgments for the current pool.
3. Identify the 30 most recent relevant items so far in the pool and set a time window from the time of the 30th most recent to the time of the query. This time window allows pruning of the space of items to be judged. Any item with a time stamp out of this time frame, i.e. older than the 30th most recent relevant item, does not need to be judged for relevance.
4. Take the next item from each result list which falls within the window, add it to the pool, judge unjudged documents. If additional relevant documents are found, iterate.
As mentioned this method guarantees to evaluate all submissions fairly according to the metric, and also provides good material for participants to do diagnostic work on their mixture of relevance and time-like criteria for retrieval. For future use, it identifies what is likely to be a good approximation to the true target set, and also labels for relevance a reasonable number of older items, again helpful for diagnostic work.
3. DISCUSSION
So far we have only considered binary relevance assessments; often multiple grades are used. In the Microblog

Track tweets were judged as non-relevant, relevant, or vital (highly relevant) [1]. A simple way to account for graded relevance in the proposed framework is to define the target set to contain not only the 30 most recent relevant items but also all previous vital items known. This is a rather crude balance between the importance of relevance and time but it allows the measurement process to remain quite simple.
Note that the current set up does not prohibit a future definition of a measure that combines relevance and time through some gain function for instance. Under such a development, the submission requirements, as described here, should remain similar with participants submitting an auxiliary list of items ranked by relevance only to allow the construction of a target set/ranking. Further, pruning of the judgment space during pooling should also be possible in a way similar to the one described in the proposal above.
Further it should be also possible to consider giving extra weight to the retrieval of all vital items, but that complicates the metrics somewhat and a discussion on this is out of the scope of the current poster. An extension of set measures to graded relevance can be found in Robertson et al. [2].
Another point regarding the proposed experiment is that one could continue adding items to the pool even if no relevant documents are found for a small number of iterations, so that any recent relevant items that appear low in the ranked list by all systems gets a chance to be pooled. The number of iterations that would be required is a point of further investigation on the implementation of the proposed experiment and out of the scope of this poster.
Last, note also that set retrieval typically requires ranking and thresholding; thresholding is known to be hard. However, for the task as defined, participants will be able to get away with thresholding at fixed rank (20 or 25 or 30) without disastrously damaging effectiveness. There will be room for more sophisticated thresholding methods to be used.
4. CONCLUSION
In this poster we proposed a experiment to test the effectiveness of retrieval systems towards returning both relevant and timely information. We discussed a rather simple case, where relevance is binary and the only measures considered are set measures. Extensions of the proposed experiment for graded relevance, ranking measures are possible and follow the general principles of the proposed framework with somewhat more complex details on the implementation side. The proposed framework resolves the flaw of the current evaluation framework used by the TREC Microblog Track.
5. REFERENCES
[1] Craig Macdonald, Iadh Ounis, Jimmy Lin, Abdur Choudhury, and Ian Soboroff. TREC Microblog Track. https://sites.google.com/site/microblogtrack/, 2011.
[2] Stephen E. Robertson, Evangelos Kanoulas, and Emine Yilmaz. Extending average precision to graded relevance judgments. In SIGIR '10: Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 603≠610, New York, NY, USA, 2010. ACM.

1120

Rewarding Term Location Information to Enhance Probabilistic Information Retrieval

Jiashu Zhao1, Jimmy Xiangji Huang2, Shicheng Wu2
Information Retrieval and Knowledge Management Research Lab 1Department of Computer Science & Engineering, 2School of Information Technology
York University, Toronto, Canada
jessie@cse.yorku.ca, {jhuang, scwu}@yorku.ca

ABSTRACT
We investigate the effect of rewarding terms according to their locations in documents for probabilistic information retrieval. The intuition behind our approach is that a large amount of authors would summarize their ideas in some particular parts of documents. In this paper, we focus on the beginning part of documents. Several shape functions are defined to simulate the influence of term location information. We propose a Reward Term Retrieval model that combines the reward terms' information with BM25 to enhance probabilistic information retrieval performance.
Categories and Subject Descriptors
H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval
Keywords
BM25-RT, Influence Shape Function
1. INTRODUCTION
Using document semantic information is usually regarded as an effective approach for improving Information Retrieval (IR) performance. In this paper, we propose to reward terms according to their location information in documents.
Human beings do not treat each word equally when reading a document. People pay more attention on certain parts of the document. This is due to the fact that many authors would summarize their ideas in some particular locations, e.g. the beginning and/or the end. It particularly fits articles that have abstract and introduction at the beginning and conclude the summary in the end. In this paper, we focus on the situation that the beginning of the documents should be rewarded during retrieval.
Many researchers have studied using semantic information in some particular domains in IR. For example, Ontology Web Language and Information Retrieval (OWLIR) system [4] is designed for Semantic Web documents, using several techniques such as information extraction, annotation and inference. Wang and Li [5] investigated semantically annotated Wikipedia XML corpus to improve retrieval performance. Zhao and Callan [6] utilized query structure expansion to enhance structured retrieval in a question answering (QA) application. Gao and Toutanova [1] learned semantic models from large amounts of query-title pairs con-
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

Influence

1

Cosine

0.8

Linear

Parabolic

0.6

0.4

0.2

0

0

20

40

60

80

100

Position in Document

Figure 1: Influence Shape Functions

structed from clickthrough data. Compared with the above approaches, our approach does not require any domain-specific knowledge.
For structured data, different fields could be given different weights. Robertson and Zaragoza [3] proposed an extension of BM25 to weight multiple fields separately, such as title text, body text, and anchor text. However, each part of a document is treated equally for unstructured documents. Compared to their approach, our approach rewards terms smoothly according to their locations in documents, and doesn't incorporate any field information.
The remainder of this paper is organized as follows. In Section 2, we propose several Influence Shape functions and extend BM25 by rewarding terms according to their locations in documents. Section 3 presents the experimental results and parameter sensitivity in ISF function. Section 4 concludes the findings in this paper and discusses possible future directions.
2. OUR APPROACH
We firstly build the Influence Shape Functions (ISF) that could represent the term influence at the locations closer to the beginning part, and then integrate the influence into BM25 by adding an extra part to term frequency. Under our assumption, when a term occurs closer to the beginning part in the document, it would be more likely to reveal the meaning of the document. Therefore, an ISF should have the highest value at the first position and gradually decreases. We build three functions with different gradients which satisfy this property.

p ISFP arabola(p, D) = (  ∑ |D|

- 1)2,

when

p <  ∑ |D|

p

ISFLinear(p, D) = 1 -

,  ∑ |D|

when

p <  ∑ |D|

1137

∑p ISFCosine(p, D) = cos( 2 ∑  ∑ |D| ), when p <  ∑ |D|

where p is the position that a term occurs, |D| is the length

of the document, and  is a parameter controlling the per-

centage of the document that should be considered as the

beginning part. It ranges from 0 to 1. When  = 0, p is

always greater than  ∑ |D|. Therefore the value of each ISF

is always 0. It becomes a special case that ISFs do not affect

the retrieval process. The above ISFs are normalized to 0

to 1, and their shapes are shown in Figure 1. In this figure,

the document length |D| is 100 and  is 0.8.

In [3], the field information is integrated into BM25 by us-

ing weighted term frequency on each field. Here we enhance

the within-document term frequency of a query term by the

ISFs to reward query terms occurring closer to the begin-

ning of the document. The within-document query term

frequency is linearly combined with the accumulation of a

term's ISF value.

tf (i)

tfRT (i, D) = tf (i, D) + ISF (pj, D)

(1)

j=1
where ISF could adopt any of the above defined shape func-

tion. The only parameter that is affected is k1, which controls the non-linear tf function.

k1RT (i) = k1 ∑

DIndex tfRT (i, D) DIndex tf (i, D)

(2)

The weighting function of BM25 with Rewarded Terms (BM25RT) is as follows.

wRT (qi, D)

=

(k1RT + 1)  tfRT K + tfRT

 (k3 + 1)  qtf k3 + qtf

log

N

- n + 0.5 n + 0.5

where N is the number of documents in the collection, n is the number of documents containing qi, tfRT is withindocument term frequency, qtf is within-query term frequency, dl is the length of the document, avdl is the average document length, nq is the number of query terms, k3 is tuning constant defaulted to be 8, K equals to k1RT  ((1 - b) + b  dl/avdl). In this paper, we set b = 0.75 and k1 = 1.2, which are regarded as default parameter settings in many BM25 applications [2].

3. EXPERIMENTAL RESULTS
We evaluate the proposed approach on three data sets: WT2G (topic 401-450), TREC8 (topic 401-450), and Blog06 (topic 851-950). The WT2G collection is a 2G size crawl of Web documents. The TREC8 contains 528,155 newswire articles from various sources, such as Financial Times (FT), the Federal Register (FR) etc., which are usually considered as high-quality text data with little noise. The Blog06 collection includes 100,649 blog feeds collected over an 11 week period from December 2005 to February 2006. For all test collections used, each term is stemmed using Porter's English stemmer, and standard English stopwords are removed. We use the MAP and P@10 as measurements in our experiments.
Table 1 shows the result of BM25-RT on the above three data sets using Cosine ISF, Linear ISF, and Parabolic ISF. We compare BM25-RT with BM25, since BM25-RT doesn't incorporate any field or annotation information. Significant improvement is observed on all the data sets.
Figure 2 shows how the parameter  in ISF affects the retrieval performance. We have discussed in Section 2 that

BM25
Cosine Linear Parabola

WT2G

MAP

P@10

0.2694

0.4400

0.2812* 0.2814* 0.2787

0.4500 0.4440 0.4460

TREC8

MAP

P@10

0.2410

0.4460

0.2467* 0.2460* 0.2460*

0.4580 0.4580 0.4560

Blog06

MAP

P@10

0.2966

0.6033

0.2968 0.2874 0.2982*

0.6173* 0.6200* 0.6180*

Table 1: MAP and P@10 for BM25-RT with different ISFs (* indicates significant improvements compared to BM25)

MAP P@10

0.282 0.28

Cosine Linear Parabola

0.278

0.276 0.274

0.272

0.27

0.268

0.266

0

0.2 0.4 0.6 0.8

1



(a) MAP

0.455 0.45
0.445 0.44
0.435 0.43
0.425 0.42 0

Cosine Linear Parabola

0.2 0.4 0.6 0.8

1



(b) P@10

Figure 2: Parameter Sensitivity of  on WT2G for BM25-RT with Cosine ISF, Linear ISF, and Parabola ISF.

BM25-RT becomes BM25 when  = 0. When  is increasing, it means BM25-RT takes into more proportion of document into account. We can see that MAP and P@10 of BM25-RT increase at first when  increments from 0. It means the term rewarding technique boosts BM25-RT's performance. As  keeps incrementing, both MAP and P@10 decrease after reaching optimal values.

4. CONCLUSIONS AND FUTURE WORK
In this paper, we propose a BM25-RT model which tries to reward terms according to their location information during retrieval. A term occurrence is regarded to be more important when it is closer to the beginning of the document. In the future, we plan to discuss more possible document distributions, for instance, to reward the endings of documents.

5. ACKNOWLEDGEMENTS
This research is supported in part by the research grant from the Natural Sciences & Engineering Resarch Council (NSREC) of Canada and the Early Researcher/Premier's Research Excellence Award.

6. REFERENCES
[1] J. Gao, K. Toutanova, and W. Yih. Clickthrough-based latent semantic models for web search. In SIGIR'11, pages 675≠684, New York, NY, USA, 2011. ACM.
[2] S. Robertson, S. Walker, and M. Beaulieu. Okapi at TREC-7: automatic ad hoc, filtering, vlc and interactive track. In TREC'99, pages 253≠264, 1999.
[3] S. Robertson, H. Zaragoza, and M. Taylor. Simple BM25 extension to multiple weighted fields. In CIKM'04, pages 42≠49, New York, NY, USA, 2004. ACM.
[4] U. Shah, T. Finin, A. Joshi, R. S. Cost, and J. Matfield. Information retrieval on the semantic web. In CIKM'02, pages 461≠468, New York, NY, USA, 2002. ACM.
[5] Q. Wang, Q. Li, S. Wang, and X. Du. Exploiting semantic tags in xml retrieval. In INEX'09, pages 133≠144, Berlin, 2010.
[6] L. Zhao and J. Callan. Effective and efficient structured retrieval. In CIKM'09, pages 1573≠1576, New York, NY, USA, 2009. ACM.

1138

Scheduling Queries Across Replicas

Ana Freire1, Craig Macdonald2, Nicola Tonellotto3, Iadh Ounis2, Fidel Cacheda1
1 University of A CoruÒa, Campus de ElviÒa s/n, 15017 A CoruÒa, Spain 2 University of Glasgow, G12 8QQ Glasgow, UK
3 National Research Council of Italy, Via G. Moruzzi 1, 56124 Pisa, Italy
{ana.freire, fidel.cacheda}@udc.es1, {craig.macdonald, iadh.ounis}@glasgow.ac.uk2, {nicola.tonellotto}@isti.cnr.it3

ABSTRACT
For increased efficiency, an information retrieval system can split its index into multiple shards, and then replicate these shards across many query servers. For each new query, an appropriate replica for each shard must be selected, such that the query is answered as quickly as possible. Typically, the replica with the lowest number of queued queries is selected. However, not every query takes the same time to execute, particularly if a dynamic pruning strategy is applied by each query server. Hence, the replica's queue length is an inaccurate indicator of the workload of a replica, and can result in inefficient usage of the replicas. In this work, we propose that improved replica selection can be obtained by using query efficiency prediction to measure the expected workload of a replica. Experiments are conducted using 2.2k queries, over various numbers of shards and replicas for the large GOV2 collection. Our results show that query waiting and completion times can be markedly reduced, showing that accurate response time predictions can improve scheduling accuracy and attesting the benefit of the proposed scheduling algorithm.
Categories & Subject Descriptors: H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval
General Terms: Performance, Experimentation
Keywords: Query Scheduling, Simulation
1. INTRODUCTION
A distributed information retrieval (IR) system consists of several query servers, each of them storing the index shard for a subset of the documents in the corpus. New queries arrive at the broker, which routes them to each shard, before collating and merging the results for presentation to the user. The efficiency of each query server can be improved by deploying a dynamic pruning strategy, such as Wand [1], which aims to avoid the scoring of postings for documents that cannot make the top K retrieved set.
While multiple shards increase efficiency compared to a monolithic ("single shard") retrieval system, the throughput of a distributed retrieval system can be further enhanced by replicating shards, so that one of multiple query servers can provide the results for a single shard [6]. The problem tackled in this work is how a broker should select (schedule) the most suitable replica of a given shard in order to reduce the queue waiting time. For example, the replica with the minimum number of queued queries can be selected.
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

However, the response time for different queries can vary widely, particularly if dynamic pruning is employed [5]. Hence, the accurate choice of replica is made more difficult, as the number of queries queued by a given query server does not accurately predict the processing backlog of the server. A recently proposed technique for query efficiency prediction [5] offers a plausible manner to estimate the workload of a replica. Hence, we hypothesise that query efficiency prediction [4] can permit accurate query scheduling in a distributed/replicated IR system. Indeed, to the best of our knowledge, this work contributes the first study to applying query efficiency predictors for scheduling in a distributed/replicated IR system. Using a simulated distributed/ replicated search environment, based on actual query response times, we experiment to determine how different scheduling algorithms can be deployed for replica selection. Our results show that by using query efficiency prediction, we can improve the selection of replicated query servers, and hence the average query completion times are reduced. For instance, using predicted response time to select between 4 replicas of a 2 shard index results in a 42% reduction in mean completion time compared to selecting replicas by considering only the length of their queues.
2. SIMULATING REPLICATION
In comparing different scheduling algorithms, we experiment with a various numbers of shards and replicas. To facilitate such experiments without exhaustive hardware resources, we build a simulation framework that supports different distributed settings. Indeed, Cacheda et al. [2] showed that a simulation framework could accurately model the efficiency of a real distributed IR system, including the network delays, the queue waiting and processing time for queries and the time for merging the results. Following this, we implement a simulation framework1, extended to encapsulate the presence of multiple shards each with multiple replicas as well as several scheduling algorithms for selecting replicas. The constants for network delays follow [2], in order to achieve a proven realistic simulation environment.
The baseline scheduling algorithms implemented for selecting replicas are: Random (the replica is chosen randomly); Round Robin (modulo the number of replicas, if replica i was selected for this query, replica i + 1 is used for the next query); Queue Length (the replica with the fewest queries waiting to be processed is selected). In addition to these baselines, we propose Predicted, where the replica with the current shortest queue in terms of predicted response times is selected. We use query efficiency predictions [5]
1Built using JavaSim: http://javasim.codehaus.org/

1139

Replicas

Random ACT AWT

Round ACT

Robin AWT

Queue ACT

Length AWT

Prediction ACT AWT

Oracle ACT AWT

2 Shards

2 9617 9382 10061 9826 8897 8662 613 362 610 359

4 902 667 409 174 434 199 253 3 253 3

8 410 175 263 28 428 193 250 0 250 0

5 Shards

2 375 237 241 103 247 109 158 4 159 5

4 265 126 155 16 231 93 154 0 154 0

8 192 54 140 2 231 93 154 0 154 0

10 Shards

2 168 69 120 22 145 47 114 1 114 1

4 139 41 101 3 144 46 114 0 114 0

8 123 25 98 0 144 46 114 0 114 0

Table 1: ACTs and AWTs (in milliseconds) for different settings and scheduling algorithms.

for estimating the response time of a query. Moreover, as the predicted response time is dependent on statistics of the query terms on that index shard, our framework accounts for the time to calculate the prediction at the selected replica and transmit it back to the broker, such that the expected workload of the replica can be updated.
Finally, as the selection of replicas is based on predicted response times, we additionally implement an Oracle scheduling algorithm, which knows the actual response time of a query before it is executed, but still accounts for the calculating the predicted response time. In this way, Oracle represents a best-case scenario for Predicted scheduling.

3. EXPERIMENTAL SETUP
We hypothesise that using predicted response times can increase overall efficiency compared to other scheduling algorithms. To address this hypothesis, we conduct experiments by indexing TREC GOV2 corpus using Terrier2, applying Porter's English stemmer and removing standard stopwords. We experiment with three different index configurations: 2, 5 and 10 shards. For retrieval on each query server, we use a set of 2200 queries of the TREC 2005 Terabyte track Efficiency task. We sample real arrival times of a set of queries from an Excite query log and assign them to our TREC queries (query arrival rates vary from 20 to 180 per second). We use the Wand dynamic pruning strategy [1] to retrieve K = 1000 documents, scored by the DPH Divergence from Randomness document weighting model. Timings are made using an Intel Xeon 2.66GHz.
To obtain the response time predictions, we follow Tonellotto et al. [5], by calculating various term-level statistics, such as the IDF, maximum score, number of postings, number of postings with scores > 95% maximum score. These are then aggregated across terms by sum, max, min, mean, median, stddev, variance and range functions, to form a total of 113 features (14 statistics * 8 aggregations + query length). Predicted response times are obtained by gradient boosted regression trees [3], trained on a separate subset of 2500 Efficiency task queries. Finally, to compare the five scheduling algorithms, we use two measures: average waiting time (AWT) and average completion time (ACT) over all the queries, in milliseconds (ms). Note that the average completion time is inclusive of the average waiting time.
4. EXPERIMENTAL RESULTS
From Table 1, we note that increasing both the numbers of shards and the number of replicas reduces both ACTs and
2http://terrier.org

AWTs. Indeed, in general, 2 shards with only 2 replicas is insufficient for a low completion time for this query workload, as queries can spend 8 seconds waiting for an available query server. For 5 or more shards, more than 4 replicas is sufficient for eliminating any contention for query servers (i.e. AWTs close to 0).
Next, comparing the scheduling algorithms, we note that Random obtains the highest ACTs and AWTs, because it can choose replicas that are busy, whist other replicas for that shard are idle. Queue Length is superior to Round Robin under high contention (i.e. 2 shards, 2 replicas). In other settings, Round Robin appears to better balance load than Queue Length. However, across different numbers of shards and replicas, Prediction always achieves the smallest AWT. For instance, with 4 replicas of the 2 shard index, Prediction can reduce AWT to 3ms, compared to 199ms for Queue Length and 174ms for Round Robin. Under settings with very little contention (e.g. 10 shards, 4 or 8 replicas), Round Robin has slightly lower ACTs than Prediction and even Oracle, due to the expense of predicting the response time (typically 6-40ms, depending on query length). Finally, Prediction obtains ACTs and AWTs that are almost identical to the best-case Oracle algorithm, based on actual response times. Overall, we find that using predicted response times to select the suitable replica for each query results in improved efficiency.
5. CONCLUSIONS
We proposed that using the predicted response time (obtained using query efficiency prediction) could enhance replica selection within a distributed/replicated IR system, compared to other scheduling algorithms. Indeed, experiments using the GOV2 corpus showed that the proposed Prediction algorithm could attain marked reductions in the query waiting times, across different number of shards and replicas. In future work we will investigate if query response times within a shard are correlated, and hence if the number of replicas required for a given shard can be predicted in advance.
6. ACKNOWLEDGEMENTS
Ana Freire acknowledges the support from the Spanish Government (Project TIN2009-14203). Craig Macdonald and Iadh Ounis acknowledge the support of EC-funded project SMART (FP7-287583).
7. REFERENCES
[1] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In Proc. CIKM 2003.
[2] F. Cacheda, V. Carneiro, V. Plachouras, and I. Ounis. Performance analysis of distributed information retrieval architectures using an improved network simulation model. Information Processing and Management, 43:204≠224, 2007.
[3] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189≠1232, 2000.
[4] C. Macdonald, N. Tonellotto, and I. Ounis. Learning to Predict Response Times for Online Query Scheduling. In Proc. SIGIR 2012.
[5] N. Tonellotto, C. Macdonald, and I. Ounis. Query efficiency prediction for dynamic pruning. In Proc. LSDS-IR 2011.
[6] F. Cacheda, V. Carneiro, V. Plachouras and I. Ounis. Performance Comparison of Clustered and Replicated Information Retrieval Systems. In Proc. ECIR 2007.

1140

Re-Examining Search Result Snippet Examination Time for Relevance Estimation

Dmitry Lagun
Emory University
dlagun@emory.edu

Eugene Agichtein
Emory University
eugene@mathcs.emory.edu

ABSTRACT
Previous studies of web search result examination have provided valuable insights in understanding and modelling searcher behavior. Yet, recent work (e.g., [3]) has been developed based on the assumption that the time a searcher spends examining a particular result abstract or snippet, correlates with result relevance. While this idea is intuitively attractive, to the best of our knowledge it has not been empirically tested. This poster investigates this hypothesis empirically, in a controlled setting, using eye tracking equipment to compare search result examination time with result relevance. Interestingly, while we replicate previous findings showing examination time to be indicative of whole-page relevance, we find that viewing time of individual results alone is a poor indicator of either absolute result relevance or even of pairwise preferences. Our results should not be taken as negating the usefulness of modeling searcher examination behavior, but rather to emphasize that snippet examination time is not in itself a good indicator of relevance.
Categories and Subject Descriptors
H.4 [Informational storage and retrieval]: evaluation, search process.
Keywords
Web search snippet examination; web search behavior; search evaluation.
1. MOTIVATION AND OVERVIEW
As terabytes of search log data are being generated daily, correct interpretation of patterns in these data becomes ever more challenging. Mining of click data has become a fertile area for many applications, including result relevance estimation, automatic query suggestion, and many others. One interesting application of using search logs to infer result examination behavior on Search Result Pages (SERP) was introduced by He et al. [3]. Another successful application of examination data is identification of relevant sub-parts of documents, which can be used for query expansion or term re-weighting (see Buscher et al. [1] for more details). Yet, while result examination data is likely to provide rich contextual information for machine learning algorithms for accurate user modeling and interpretation of past clicks, to our knowledge, the examination behavior on the SERP itself as a predictor of relevance has not been empirically evaluated in prior research [3].
This poster reports preliminary results on analyzing whether
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

viewing time of a search result snippet can be directly used to estimate its relevance label. Using eye tracking equipment, we confirm that previously suggested measures of viewing behavior (e.g., the distribution of viewing time across all result ranks) indeed correlate with overall result quality. However, we also find that examination time alone is unlikely to be a good indicator of relevance of individual results, primarily due to severe position bias in the examination behavior. Our results are consistent with and complement the findings of Guan and Cutrell [2], where users were observed to spend more time finding relevant results placed in the bottom of the result list, as opposed to the original result ordering. Next, we describe the details of eye tracking user study that we conducted, after what we present our findings on correlating result viewing time and relevance, both for the whole set of results and for individual search results.
2. USER STUDY
Our study used 25 benchmark search tasks selected from the Web Track of the TREC 2009 competition. The goal for each task (the description) was provided to the participants. For example, the goal of the query "toilet" was stated as: "Find information on buying, installing, and repairing toilets". For each task, the query keywords were submitted to the Google search engine, and the Search Engine Result Pages (SERPs), as well as all the result URLs linked from each SERP, were cached. We did not remove non-organic results such as ads, embedded image results, news or local results, etc. - thereby recreating a realistic search experience for the subjects. Paid annotators rated every search result for relevance, with at least five ratings per document, on a three-point scale ("Bad", "Partially Relevant" and "Perfect"). We retained the majority opinion as the final result relevance label. For the rest of the details about user study we refer to [5].
3. RESULTS AND DISCUSSION
While our main goal is to investigate whether snippet examination time can be used as a proxy for result relevance, we first confirm that our data are in correspondence with prior work on studying user examination behavior on the search result page.
Figure 6(a) of [5] reports the fraction of the examination time of the snippets, broken down by the result rank. Specifically, the values were computed for each subject and query (that is, the viewing time for each result abstract by a subject was divided by the total viewing time of the corresponding SERP) for an individual query, and then averaged across all queries. The decaying shape of the viewing time distribution is similar to that reported in prior work (e.g., by Joachims et al. [4]). The fact that web search users tend

1141

Pooled (all) Pairwise - All Pairwise - Above Pairwise - Adjacent

Viewing time (rel.) 0.157 0.194 0.119 0.007

Viewing time (abs.) 0.107 0.148 0.063 0.003

Table 1: Pearson correlation between individual result examination time and relevance labels.

to spend more time reading top results irrespective to the result relevance is well known and typically referred as position bias (sometimes called presentation bias). However, when considering viewing time for individual queries, the effect of the position bias is smaller. In fact, for some of the queries in our dataset, the viewing time does not peak at rank one, and actually can be used to estimate the whole page relevance, as we discuss below. Viewing time vs. individual result relevance: we now investigate whether variation in snippet examination time, as measured per result or by entire distribution, could be attributed to result relevance. Figure 1 reports the box plot of relative viewing time, broken down by relevance labels, where the boxes correspond to 25th and 75th percentiles, and the median is marked with a red line. As Figure 1 reports, there is a substantial overlap in values of viewing time for different relevance levels. Statistical tests on differences of result viewing time between Bad and Partially Relevant groups, and Bad and Perfect groups fail to reject the null hypothesis, leading us to conclude that snippet examination time alone can not be used to infer individual result relevance. To verify our findings, we pool viewing time and relevance labels from all queries, and compute Pearson correlation between them. Overall, we find that there is only a weak correlation (0.157) between snippet viewing time and relevance. In order to mitigate the effects of position bias, we also compared relative result preferences based on relevance vs. based on the difference in examination time. Specifically, we compared the differences of snippet viewing time per result to the differences in relevance levels, in three ways: Pairwise-All, pairwise preferences derived based on all (usually 10) results on the SERP; Pairwise-Above, pairwise preferences between each result and only the results ranked above it; Pairwise-Adjacent, pairwise preferences between adjacent results only. Surprisingly, as reported in Table 1, we find that even pairwise preferences do not exhibit a stronger correlation with viewing time, except for PairwiseAll, where pairwise preferences have a higher correlation with viewing time compared to using the absolute document relevance. Additionally, we re-calculated these correlation coefficients using absolute viewing time, instead of the relative viewing time. Finally, we repeated our computations and considered only the results viewed at least once, without finding significantly higher correlation levels. We omit details on these experiments due to space constraints.
Viewing time distribution vs. whole page relevance: while viewing time on individual results is a poor predictor of relevance, we also investigated whether we can relate viewing behavior on the SERP as a whole, with result relevance of the whole result set. To quantify overall page quality we calculated standard information retrieval metrics, namlely: Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG) and Expected Reciprocal Rank (ERR). Intuitively, when the result ranking is poor, the users are expected to spend more time reading

Figure 1: Viewing time on search results broken down by relevance label.
MAP NDCG ERR Views@1 0.452 0.249 0.102 Mean Rank 0.162 0.041 -0.042
Table 2: Pearson correlation between viewing time and whole page relevance.
result abstracts at lower ranks. To validate our intuition we compute correlation between relative viewing time at rank one (Views@1) and mean rank of viewing time distribution (analog of distribution mean). Note that the relative Views@1 is linearly depended on viewing time at lower ranks (due to normalization constraint), hence when more attention is spent on lower ranks, the value of Views@1 decreases. Table 2 reports Person correlation between Views@1 and mean rank and whole page quality represented by one of the three metrics above. We find that Views@1 has substantial correlation of 0.452 with the MAP metric, while exhibiting smaller, but non-negligible correlation with NDCG (0.249) and to a smaller degree with ERR (0.102). Summary: to our knowledge, we performed the first empirical examination of the result examination hypothesis, disproving the intuition that longer snippet examination times indicate higher relevance of individual results. Surprisingly, this was the case both for absolute relevance levels, and for pairwise result preferences, under commonly observed strategies of result examination (i.e., the "Skip-Above" and "Skip-Next" strategies in reference [4]). However, we confirmed that the distribution of the viewing times on the SERP does correlate with ranking quality of the SERP as a whole, suggesting a more promising direction for further research. In our future work we plan to conduct more thorough analysis on result examination behavior and associated relevance that would account for consistent biases in result perception such as abstract length and judgeability.
4. REFERENCES
[1] G. Buscher, A. Dengel, R. Biedert, and L. V. Elst. Attentive documents: Eye tracking as implicit feedback for information retrieval and beyond. ACM Trans. Interact. Intell. Syst., 1(2):9:1≠9:30, 2012.
[2] Z. Guan and E. Cutrell. An eye tracking study of the effect of target rank on web search. In Proc. of CHI, pages 417≠420, 2007.
[3] Y. He and K. Wang. Inferring search behaviors using partially observable markov model with duration (pomd). In Proc. of WSDM, pages 415≠424, 2011.
[4] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM TOIS, 25(2):7, 2007.
[5] D. Lagun and E. Agichtein. Viewser: enabling large-scale remote user studies of web search examination and interaction. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information, pages 365≠374. ACM, 2011.

1142

Evaluating Aggregated Search Pages

Ke Zhou
University of Glasgow Glasgow, United Kingdom
zhouke@dcs.gla.ac.uk

Ronan Cummins

Mounia Lalmas

National University of Ireland

Yahoo! Labs

Galway, Ireland

Barcelona, Spain

ronan.cummins@nuigalway.ie mounia@acm.org

Joemon M. Jose
University of Glasgow Glasgow, United Kingdom
joemon.jose@glasgow.ac.uk

ABSTRACT
Aggregating search results from a variety of heterogeneous sources or verticals such as news, image and video into a single interface is a popular paradigm in web search. Although various approaches exist for selecting relevant verticals or optimising the aggregated search result page, evaluating the quality of an aggregated page is an open question. This paper proposes a general framework for evaluating the quality of aggregated search pages. We evaluate our approach by collecting annotated user preferences over a set of aggregated search pages for 56 topics and 12 verticals. We empirically demonstrate the fidelity of metrics instantiated from our proposed framework by showing that they strongly agree with the annotated user preferences of pairs of simulated aggregated pages. Furthermore, we show that our metrics agree with the majority user preference more often than the current diversity-based information retrieval metrics. Finally, we demonstrate the flexibility of our framework by showing that personalised historical preference data can improve the performance of our proposed metrics.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Retrieval
General Terms
Measurement, Experimentation
Keywords
aggregated search, evaluation, performance metric, diversity
1. INTRODUCTION
With the emergence of various vertical search engines dedicated to certain media types and genres, such as news, image, video, it is becoming popular to present results from
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

a set of specific verticals dispersed throughout the standard "general web" results, for example by adding image results to the ten blue links for the query "pictures of flowers". This new search paradigm is often known as aggregated search [4]. The three main challenges that arise in realising such systems are vertical selection (VS), item selection (IS), and result presentation (RP). Vertical selection deals with deciding which verticals are implicitly intended by a query. Item selection deals with selecting a subset of items from each vertical to present on the aggregated page. Result presentation deals with organising and embedding the various types of items on the result page. The most common presentation strategy for aggregated search is to merge the results into one ranked list of so-called blocks, and is now the `de facto' standard in many search engines.
Although various approaches exist for selecting relevant verticals [4, 5] and for optimising aggregated search pages [2, 17], evaluating the quality of aggregated search pages is still a challenge. Consider the query "yoga poses" which suggests that a visual element in the result page would be useful to many users. Furthermore consider that 75% of users who issue this query would prefer "image" results, 60% would prefer "video" results, and 10% would prefer "news" results, to "general web" results. Figure 1 shows three possible aggregated search pages1 (A, B, and C) for the sample query. It is clearly difficult to objectively ascertain the aggregated search page that represents a more effective returned set, as there are a variety of compounding factors that could affect a user preference. A user may prefer a page because of his/her preference towards a specific vertical (vertical preference). In such a case, a user may prefer page A because it contains more images. A user who prefers a result set with more items that are topically relevant might prefer page C, whereas a user who prefers more relevant items towards the top of the page (presentation preference) might prefer page B. Furthermore, a user who desires a more diverse returned set (vertical diversity) may prefer page C. Any combination of those factors can influence the perceived quality and user preference of the pages.
In this paper, we propose a general framework for instantiating metrics that can evaluate the quality of aggregated search pages in terms of both reward and effort. Specifically, we develop an approach that uses both topical-relevance and vertical-orientation information to derive the utility of any
1R and N represent a Relevant or Non-relevant result respectively.

115

Figure 1: Three Examplar Aggregated Search Pages for the Query "yoga poses".

given aggregated search page. Our approach is flexible and takes into account any combination of items retrieved, any combination of verticals selected, and the positions of those results on the presented page.
This paper makes several contributions: (i) we propose a framework for the evaluation of aggregated search pages that capture both effort and reward in a formal way; (ii) we outline a novel approach for simulating aggregated search pages and collect a large set of user preferences over page pairs; (iii) we demonstrate the effectiveness of the metrics derived from our framework by comparing them with several existing IR metrics; and (iv) we show that our metrics can be personalised for each user and, therefore, can be further improved using training data.
Related work is reviewed in Section 2. In Section 3, we formally outline the problem of aggregated search evaluation and list the assumptions made in this work. In Section 4, we propose a general framework from which we derive a number of metrics. A method for collecting the page preferences of users is outlined in Section 5. Subsequently in Section 6, these are used to evaluate the performance of our metrics against baseline ones. We also show how the performance of our metrics can be improved using training data. Conclusions and future work are discussed in Section 7.
2. RELATED WORK
Various works evaluating one component of an aggregated search system in isolation exist. Vertical selection in aggregated search has been studied in [4, 5, 15, 26]. Much of this research aims to measure the quality of the set of selected verticals, compared with an annotated set obtained by collecting manual labels from assessors [4, 5, 26] or derived from user interaction data [15]. The annotation can be binary [4, 5] or graded [26]. The quality of a particular vertical selection approach is mostly evaluated with standard measures of precision and recall using the binary annotated set. Our work also evaluates this key component by utilising graded vertical-orientation information derived from a multi-assessor preferred vertical annotation set [26], as this allows for a more refined evaluation scheme.
Recent attempts to evaluate the utility of the whole aggregated search page [3, 17] consider the three key components of aggregated search (VS, IS, RP) together. Our work takes a similar holistic approach and proposes a general evaluation framework for measuring aggregated search page

quality. For example, [17] evaluate the utility of a page based on a user engagement metric (CTR). This evaluation framework requires large-scale user interaction data, which may not always be available. In addition, it is not feasible to collect user interaction data for all possible page combinations. Others [6] evaluate the utility of the page by asking annotators to make assessments based on a number of criteria (e.g. relevance, diversity). Although this work is a comprehensive way to evaluate aggregated pages, it remains costly to gather assessments for all possible aggregated pages.
The most similar work [3] to ours collects preferences on block pairs from users and measures the page quality by calculating the distance between the page in question and the ideal (reference) page; the shorter the distance, the better the page. One advantage is that any possible combination of vertical blocks that form an aggregated page can be tested, from a block-oriented point of view (without regard to item selection). However, when the results retrieved for a vertical (block) change, the assessments previously gathered may not be reusable, as the preference will undoubtedly change accordingly. As our work follows the Cranfield paradigm, once the assessments (both item topical-relevance and verticalorientation) are gathered, it can be applied to evaluate any possible aggregated search page (any combination of vertical selection, item selection and result presentation). Therefore, our work leads to a more robust, inexpensive, and reusable approach for evaluating aggregated search pages.
Topical diversity is an important topic. Various diversityaware IR metrics have been proposed [8, 10, 18], capturing the importance of each subtopic, the degree to which an item represents the subtopic, and the topical-relevance of the item. Diversity-based metrics can promote returned sets that are both topically relevant and diverse. A simplistic way of adapting these metrics to aggregated search is to treat subtopics as verticals and subtopic importance as verticalorientation. In this way, all existing diversity-based IR metrics can be adapted to evaluate aggregated search. Although in principle suitable to evaluate aggregated search, diversitybased metrics are not appropriate for use with block-based pages where user behaviour is different; for instance user browsing behavior within a block containing images may be different to that within a block containing "general web" results. Furthermore, the various types of items (text, image, etc.) that need to be accounted for in an aggregated search scenario are not explicitly modelled in diversity-based met-

116

rics. For example, the effort in reading a piece of text is greater than the effort in viewing a picture. Our framework is better adapted to the task of aggregated search, and models all key components simultaneously.
Others [20] have proposed an aggregated search metric that captures both vertical diversity and topical diversity. It can be noted that the framework developed in this paper can also be extended to incorporate topical diversity, but due to space limitations, we will leave this as future work.
3. PROBLEM FORMULATION
We introduce some formal notation and outline some of the main assumptions used throughout this work.
3.1 Aggregated Page Composition
An aggregated search page P is composed of a set of blocks {B1, B2, ...Bn}, where each block Bi consists of a set of items {Ii1, Ii2, ...Iim}. An item can be a "general web" page or a vertical result. Only snippets of each item appear on the aggregated search page. We make several assumptions2 about the page P : (i) results are presented into blocks from top to bottom, and within each block, items are shown either from left to right (Image, Video) or from top to bottom (News, Recipe); (ii) each block Bi consists of items originating from only one vertical; (iii) only one block of each type is placed on a page (with the exception of "general web" blocks); and (iv) a block consists of one `general web' item or k vertical items. This is different to previous work [3, 17] where vertical block could be embedded into only three positions of "general web" results (top of the page, middle of the page, bottom of the page). We relax this assumption and allow a vertical block to be slotted between any two "general web" blocks on the page.
3.2 Relevance and Orientation
Our objective is to develop metrics that measure the quality of any possible aggregated search page. The metrics must work regardless of the selected verticals, the items retrieved from each vertical, and where the vertical results are positioned on the page. To achieve this, we assume that the following two types of relevance assessments are available:
∑ The topical-relevance of each item, which is an assessment indicating whether a given item Iij within block Bi is topically relevant to a topic q3. This is denoted qrel(Iij |q).
∑ The user specific vertical-orientation [26], which is a value between zero and one indicating the fraction of users that prefer a page to contain items from the vertical Vi rather than "general web" results for a topic q. This is denoted orient(Vi|W, q).
The two relevance assessments are assumed to be made independently. The concept of vertical-orientation [26] reflects the perceived usefulness of a vertical from the user perspective prior to viewing vertical results and without regard to the quality of the vertical results. The vertical-orientation assessment is obtained by comparing each vertical in turn to
2These assumptions are made in accordance with existing aggregated search systems. 3In this work, we assume that topical-relevance assessments are binary.

the reference "general web" vertical, by asking users whether items from this vertical are likely to improve the quality of a standard web page. Consequently, the vertical orientation of the Web (orient(W |W, q)) is deemed to be 0.5, as we can imagine that a user would randomly select a page when presented with two similar "general web" pages. The topical-relevance assessment of each item contributes to the measurement of relevance for each retrieved result. This type of assessment can be made using similar pooling techniques [13] to those used in TREC.
With these two assessment types, we assume that a user obtain the highest reward by reading the most topically relevant item, originating from the most highly oriented vertical, first. With this assumption, only the vertical (or verticals) with a higher orientation than the "general web" (orient(V |W, q) > 0.5) should be presented on the aggregated search page; all other verticals should be suppressed.
3.3 User Interaction Behaviour
We make some assumptions about how users interact with an aggregated search page P :
∑ The user examines each page one block at a time. When the user reads page P , a block Bi on the page P has a certain probability of being examined. This probability denoted Exam(Bi) is estimated depending on the type of browsing model assumed.
∑ After the user decides to examine a block Bi, we assume a static user browsing behavior within the block; the user reads all the items Ii1 to Iim within that block.
Given that our metrics are based on average user and that there is usually only a limited number of items per block, this simple within block user browsing model is appropriate.
4. EVALUATION FRAMEWORK
We aim to develop metrics that evaluate an aggregated search page similarly to how a user might. Given two pages P1 and P2, we wish to measure their effectiveness in satisfying a user information need using a utility function U til(P ). If a user prefers P1 over P2 for a given query, the utility measure should lead to U til(P1) > U til(P2).
Following [11], the utility of a page is determined by reward and effort. A page with a high utility should satisfy the average user information need with relatively little effort. We define the utility metric U til(P ) of the page P based on all blocks {B1, B2, ...Bn} on the page. When a user reads page P , a block Bi on the page P has a certain probability Exam(Bi) of being examined. This probability might depend on the position of the block presented, the snippet type of the items (image, text) within the block, or the satisfaction level after reading previous blocks B1 to Bi-1. The probability Exam(Bi) can be estimated depending on the type of browsing model assumed.
After the user decides to read block Bi, he/she will be rewarded with some gain G(Bi) coming from reading all the items Ii1 to Iim within that block. Here we assume that the topical-relevance of the item snippet is a good indication of the relevance of the item itself. Therefore, by reading all the items within the block Bi, the user will also have spent some effort E(Bi) in reading this block. Therefore, based on our assumptions, we define the utility of the page U til(P ) as

117

the expected gain of reading a page divided by the expected effort spent:

U til(P ) =

|P | i=1

E

xam(Bi)

∑

G(Bi

)

|P | j=1

E

xam(Bj

)

∑

E

(Bj

)

(1)

where |P | is the number of blocks on page P . To ensure suitable normalisation over a set of queries, we define a normalized utility score nU til(P ), similar to nDCG [12]. We normalise the score of the utility of page P by that of the ideal page Pideal:

nU til(P )

=

U til(P ) U til(Pideal)

(2)

Until now, we have defined a general evaluation framework for any aggregated search page that considers both reward and effort simultaneously. Consequently, for two pages P1 and P2, we can say P1 is better than the other when nU til(P1) > nU til(P2). In the following sections, we instantiate the gain G(Bi), the effort E(Bi), and the examination probability Exam(Bi) of the blocks. We then outline how to normalise the U til(P ) metrics by constructing an ideal page. Finally, we incorporate a simple personalisation parameter that captures the degree to which a user prefers vertical diversity on an aggregate search page.

4.1 Gain of Reading a Block
Given a block Bi containing a set of items (Ii1, Ii2, ... Iim) originating from vertical Vj, we would expect that if the vertical is highly oriented given the query, the user will achieve a higher gain. We denote this block orientation as Orient, which is related to the task of vertical selection. Furthermore, we would expect that the more topically relevant items a block contains, the higher the gain for the user. We denote the topical-relevance of the block as Topic. Before combining these two factors, we define the gain relating to the vertical-orientation of the block Bi:

Orient(Bi, ) = g(orient(Vj|W, q), )

(3)

where orient(Vj|W, q) is a value between 0 and 1. The function g() is used so that the relative gain of the vertical can be altered using a tuning parameter . The orient(Vj|W, q) value is defined as the fraction of users that would prefer the vertical Vj to be added to the "general web" results W . As the "general web" is the pivot to which verticals are added, if orient(Vj|W, q) > 0.5, then adding the vertical should be rewarded. If orient(Vj|W, q) < 0.5, the gain of the block should be less than the "general web" results (i.e. 0.5). Therefore, we use a pivot at the 0.5 value through which g() must pass. The following function satisfies these criteria:

g(x, )

=

1 1 + -log10(x/(1-x))

(4)

A graph of the function g(x, ) is shown in Figure 2. This function controls how much the gain increases as the verticalorientation level increases. When  is small (1 <  < 10), we obtain a more steep curve; highly oriented verticals are more rewarded, and conversely, low orientated verticals are more penalised. When  equals to 10, the reward is exactly the same as the vertical orientation orient(Vj|W, q).
Now we define the gain relating to topical-relevance of the

Gain for Viewing Vertical

Reward for Vertical Selection

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

g(x,1)

0.2 0.1

g(x,2) g(x,10) g(x,1000)

0

0

0.2 0.4 0.6 0.8

1

x = orient(Vi|q)

Figure 2: Function g() for Controlling Reward on Orientation with Various Parameter .

items within Bi:

|Bi |

T opic(Bi) = qrel(Iik|q)

(5)

k=1

|Bi| is the number of items within block Bi and qrel(Iik|q) is the binary relevance assessment of the item Iik. In short, we use the sum of the binary relevance judgments of the items as the topical-relevance gain of all the items within the block Bi.
Now that we have defined the gain of a block in terms of both vertical-orientation and topical-relevance, we combine these in a suitable manner. Specifically, we combine the gain based on the above two criteria:

G(Bi) = Orient(Bi, ) ∑ T opic(Bi)

(6)

where  is the tuning parameter as described above. We combine these two factors in an independent manner as both vertical-orientation and topical-relevance are related to the quality of the block. Either a low oriented block (low Orient(Bi)) or a topically irrelevant item (low T opic(Bi)) would result in an unsatisfied user.

4.2 Effort of Reading a Block
We now consider the effort E(Bi) spent in examining a block Bi. Based on the assumed block-based user browsing behavior, the effort of examining a block is defined as the accumulative effort of reading all the items within it:

|Bi |

E(Bi) = E(Iik)

(7)

k=1

where |Bi| is the number of items within block Bi, E(Iik) is the effort spent in reading the item Iik.
Several factors may affect the effort spent in examining an item E(Iik): the media type of the snippet (text, image) or the size of the snippet (text length). We assume that there are only three categories of item snippet ("image", "text" and "video"). Furthermore, we assume that "image", "text" and "video" have a standard size. Based on [23], the time taken to assess the relevance of an image is estimated 2.34 seconds, while the time taken to assess a text snippet is 7.02 seconds. We extrapolate that a video takes twice as much

118

Table 1: Effort of Reading each Category.
Snippet Category "image" "text" "video"

Effort

1

3

6

time to assess as a text4 (14 seconds). Therefore, the relative effort taken to examine each snippet type is shown in Table 1 and is used as the unit of effort. These settings are not optimal and have been chosen heuristically after a review of the literature. Identifying more optimal settings is outside the scope of this work.
4.3 Examination Probability for a Block
We concentrate on defining the user browsing model for examining a block Exam(Bi) on a page. Several models exist [8, 9, 16] that aim to predict the probability with which a user will examine an item. Position models [16] use only the position of the item in a result set. The cascade model [8] uses the relevance of the items previously examined, the intuition being that a sufficiently satisfied user will not continue to examine extra items. Motivated by the fact that users tend to be attracted by vertical results and the visual attention on them will increase the examination probability of other nearby web results, the attention model [9] aims to capture the visual attractiveness of the page. We do not propose a new user browsing model for aggregated search. Rather, we adopt these different models and incorporate them into our framework, namely the position examination models DCG [12] and RBP [16], the cascade model ERR [8], and the attention model ATT [9].
To adapt ERR to block examination, we assume that the satisfaction of viewing previous blocks is defined as the average gain of viewing each item within the block. For ATT, dist is the distance between the item under consideration and the closest vertical that has the attention bias (image and video). As we do not have access to query logs to accurately estimate the attention bias parameter , instead of assuming that  is a position-specific parameter, we assume that  is a global variable that is constant for all positions. In addition, there will be attention-bias only when results from image or video verticals are presented on the page. The standard  is obtained by exploring the optimal setting in a development set.
4.4 Normalisation Using the Ideal Page
A summary of the non-normalised utility metrics that can be instantiated in our framework are listed in Table 2. We have a suite of metrics that reward pages that contain highly oriented verticals, contain topically-relevant items, promote topically-relevant blocks earlier on the page, for less effort. The utility metrics must be normalised by the ideal aggregated page. To obtain the latter, we require a brute-force approach that calculates the metric score for all pages, and then selects the page with the maximal score as the ideal page (arg max(U til(P ))P ). This approach is not viable, given the number of possible combinations of various components of aggregated search. Therefore, we use a greedy algorithm to select a subset of aggregated pages from all the pages that exist, and only select the optimal page from this set. The idea is to use a simple metric for each com-
4We assume that users need to open and view the video item to assess its topical-relevance.

ponent, and only select the pages that perform optimally for all those components. This is described in Section 5.2, where the simulation of aggregated page pairs is discussed.
4.5 Personalised Utility Metrics
Previous research [26] has shown that different users have different preferences with regard to the type of vertical. A vertical with low orientation to a query for the average user may still be beneficial to users that prefer a very diverse information space. Therefore, we define a personalised vertical diversity preference factor to capture this scenario. We achieve this by linearly combining the normalised utility of the page with the vertical recall. This introduces a personalised preference parameter i:
I U til(P, i) = (1 - i) ∑ nU til(P ) + i ∑ vRecall(P ) (8)
where i is a parameter between 0 and 1 for user i, and controls the trade-off between vertical diversity and the quality of the aggregated search page. vRecall(P ) represents the fraction of all verticals that are presented on page P . The larger i is, the more the user prefers a page with items originating from different verticals (high vertical diversity).
5. COLLECTING PAIRWISE PREFERENCE
ASSESSMENTS
To validate the fidelity of our metrics (how they agree with actual user preferences of aggregated search pages), we collected a set of pairwise preference assessments over aggregated page pairs. We first present the data and material used for this purpose. We then simulate a set of aggregated search pages that vary in different levels of quality for each topic. Afterwards, we select a set of page pairs (two simulated pages) for each topic. Finally, we collect preference assessments for the page pairs for all topics. We outline some statistics and analysis of the assessments gathered.
5.1 Data
We use an aggregated search test collection [25] created by reusing the existing web collection ClueWeb09. This test collection consists of a number of verticals (listed in Table 3), each populated by items of that vertical type, a set of topics (320) expressing information needs relating to one or more verticals, and assessments indicating the topical-relevance of the items and the perceived user-oriented usefulness of their associated verticals to each of the topics. The verticals are created either by classifying items in the web collections into different genres (e.g. Blog, News) or by adding items from other multimedia collection (e.g. Image, Video). The topics and topical-relevance assessments of items that vary in genres are obtained by reusing assessments developed in TREC evaluation tasks (TREC Web Track and MillionQuery Track). The vertical-orientation information of each topic [26] is obtained by only providing the vertical names (with a description of their characteristics) and asking a set of assessors to make pairwise preference assessments, comparing each vertical in turn to the reference "general web" vertical ("is adding results from this vertical likely to improve the quality of the ten blue links?").
We select a subset of topics from which to collect assessments. We ensure that this subset of topics still conforms to the real-world distribution of aggregated search covering a wide range of needs with different highly oriented verticals. Therefore, we selected 56 topics detailed in Table 4.

119

Table 2: Summarisation of Utility Metrics for Aggregated Search.

Metric

Examination Model Exam(k)

Parameter

Utility

ASDCG

1 log(k+1)



ASRBP

 k-1

, 

U til(P ) =

|P | i=1

Orient(Bi )∑Exam(i)

|P | j=1

E(Bj

)∑Exam(j)

ASERR ASAT T

k-1 j=1

(1-

G(Bj |Bj |

)

)

k

[(1 -

1 log(k+1)

)

∑

dist

+

1 log(k+1)

]

∑



 , 

Table 4: Distribution of Number of Selected Topics Assigned to Various Highly Oriented Verticals.

Verticals

Image Video Recipe News Book Blog Ans Shop Disc Schol Wiki Web- Total Qrys

only

Topic Num 4

3

3

4

3

3

3

3

5

3

12

10

56

Table 3: Verticals Used in this Paper.

Vertical

Document

Type

Image Video Recipe News Books Blog Answer Shopping Discussion
Scholar Reference/Wiki General web

online images online videos
recipe page news articles book review page blog articles answers to questions product shopping page discussion thread from forums research technical report encyclopedic entries standard web pages

media genre

5.2 Simulating Aggregated Search Pages
For each topic, we simulate a set of aggregated search pages. As indicated in Section 3, we assume that a page consists of ten "general web" blocks (one "general web" page is a block) and up to three vertical blocks dispersed throughout those ten blocks (where each vertical block consists of a fixed number of three items). Recall that there are three key components of an aggregated search system that can be varied: (i) Vertical Selection (VS); (ii) Item Selection (IS); and (iii) Result Presentation (RP). We generate pages by simulating an aggregated search system in which the three components vary in quality.
The assessments for vertical-orientation were created by gathering annotations across several users. For the process of varying VS, for a given vertical Vi and query q, we consider the vertical to have a high vertical orientation if orient(Vi|W, q) is greater than 0.75 5. We simulate four different vertical selection strategies, namely Perfect, ReDDE, CORI, Bad. Perfect selects all the highly oriented verticals, while Bad randomly selects the maximum number (three) of lowly oriented verticals. ReDDE and CORI rank the verticals according to the ReDDE [21] and CORI [7] resource selection approaches, and select the top K ranked verticals.
5We select the threshold as 0.75 as 75% assessors majority preference is a suitable percentage whereby the assessments are neither too noisy (50%) or stringent (100%). Furthermore, it creates a vertical intent distribution across the topics that realistically conforms to the real-world [4].

For IS we simulate three potentially different levels of relevance. These are Perfect, BM25, and TF. Perfect selects all items in the vertical that are topically relevant. BM25 and TF select the top three ranked items from the rankings provided by the BM25 and a simple TF (term frequency) weighting respectively, with the PageRank score as a prior for both BM25 and TF.
For RP, we simulate three different result presentation approaches: Perfect, Random and Bad. Perfect places the vertical blocks on the page so that gain could potentially be maximised, i.e. all the relevant items are placed before nonrelevant items. However, if these items are part of a vertical, we position the highest orientated vertical first. Random randomly disperses the vertical blocks on the page while maintaining the position of the "general web" blocks. Bad reverses the perfectly presented page.
By varying the quality of each of the three key components, we can vary the quality of the result pages created by an aggregated search system in a more controlled way. For each topic, we can create 36 (4 ◊ 3 ◊ 3) pages6. In addition, the snippet of each item is automatically generated by the Lemur Toolkit and the presentation style conforms with typical search page presentation (presenting the vertical name in front of vertical results). Using this approach we can create a near ideal aggregated page for a query by using Perfect VS, Perfect IS, and Perfect RP. This is a greedy approach to the problem and is used as our method of normalisation for nU til.
5.3 Constructing and Selecting Page Pairs
We now describe the selection of page pairs so that they can be presented to a user for judgment. One way to achieve this is to randomly sample two aggregated search pages, and collect a sufficient set of user preference judgments. However, following [3], we attempt a broad categorisation of the aggregated search pages into "bins" according to page quality, i.e. H (High), M (Middle) and L (Low). We can then provide a more in depth analysis of the performance of the metrics over different regions of the page space.
Although we do not know the quality of all the pages, we can roughly estimate the page quality using the quality of
6Certain combinations of VS, IS, and RP do not create unique simulated pages.

120

the components that created the page. We estimate this by assuming that the three components contribute equal importance to the quality of the page. We then evaluate each component respectively using a suitable metric. The quality score of the page is determined by linearly combining the metric score for each component. This is a coarse approach of determining the quality of the page. We use the F-measure (VS), Mean Precision (IS), and Kendall-tau correlation (RP). We then rank all the pages according to the three linearly combined metrics and evenly categorise the pages in the ranking into "H", "M" and "L" bin respectively.
We now have a method of comprehensively analysing how various metrics perform over the whole page space by selecting pages from these pre-assigned bins. Specifically, we have six bin pairs, H-H, H-M, H-L, M-M, M-L, L-L, which uniformly represent all the entire page space for the queries (albeit in coarse intervals). For each pair of bins, we randomly select 8 page pairs from it. Consequently, we select in total 48 (6 ◊ 8) page pairs for each topic.
5.4 Collecting Pairwise Preference Assessments
Our preference assessment data is collected over the Amazon Mechanical Turk crowd-sourcing platform, where each worker was compensated $0.01 for each assessment made. A page pair was presented with the topic (title and description) shown in the upper position of the assessment page. This was followed by a pair of aggregated pages shown sideby-side. The assessor was provided with three options when making the assessments: "left page is better", "right page is better" and "both are bad". The latter option captures the scenario where a user is confused due to the poor page quality7. For each page pair, we collect four assessments (from four different assessors). The total number of assessments made during this preference collection process was 10752 (56 ◊ 48 ◊ 4). Following [19], a quality control was ensured by including 500 "trap" HITs. Each "trap" HIT consists of a triplet (q, i, j) where either page i or j was taken from a query other than q. We interpreted an assessor preferring the set of extraneous results as evidence of malicious or careless assessments and assessors who failed more than two trap HITs were discarded.
5.5 Analysis of Assessments
Of the 203 assessors who contributed HITs, 39 had their assessments removed from the assessment pool due to failing more than 2 trap HITs. For the remaining 164/203, participation followed a power law distribution where about 12% (20/164) of the assessors completed about 60% (6522/10752) of our HITs. We also found out that assessors rarely select the "both are bad" options provided as only 7% (684/10752) of the assessments are of this option.
We want to answer the following question: RQ1 Do users agree with each other when assessing aggregated search pairs? Therefore, we measured annotator agreement of preferences of aggregated page pairs using Fleiss' Kappa [24] (denoted by KF ), which corrects for agreement due to chance. Fleiss' Kappa is convenient because it ignores the identity of the assessor-pair, and is designed to measure agreement over instances labeled by different (even disjoint) sets of assessors. The results are shown in Table 5.
7The option "Both are good" is not included because this information can be potentially obtained by investigating interassessor agreement for definite preferences.

Table 5: Statistics of User Preference Assessment Agreement over Various Quality Bins.

bins
all H-H H-M H-L M-M M-L L-L

4/4
2231 347 427 461 287 394 315

3/4
8051 1396 1354 1318 1424 1327 1332

Kappa agreement
0.241 0.238 0.283 0.317 0.192 0.261 0.210

We observe that assessor agreement on presentation-pairs was KF = 0.241, which is considered fair agreement [24]. This result is similar to previous research [3, 26], which reaffirm that evaluating aggregated search is not an easy task, and that various users have their own assumptions about what a good page is. Of all 10752 aggregated page-pairs, 8051 (74.8%) had a majority preference of at least 3/4 and only 2231 (20.7%) had a perfect 4/4 majority preference. It is perhaps not surprising that assessor agreement is not high as agreement on page-pairs requires that assessors make similar assumptions about the cost of different types of errors. Furthermore, the low inter-assessor agreement may be explained by the fact that users make different assumptions regarding the importance of each aggregated search component (VS, IS, RP). Alternatively, it may be that assessors have a hard time distinguishing between good presentations. Following previous research [3], given this low level of inter-assessor agreement, rather than focusing on the metrics agreement with each individual preference, we focus on their agreement with the majority preference (3/4 or greater, and 4/4) in the evaluation.
6. EVALUATION
We investigate the fidelity8 [22] of the proposed metrics. We leave an investigation on the reliability of the metric (discriminative power [18]) for future work. We aim to answer the following questions:
1. RQ2 With standard parameter settings, are the standard diversity metrics suitable for aggregated search, and do our proposed metrics accurately predict user preferences for aggregated search pages?
2. RQ3 Can we learn personalised parameters from historical data and, subsequently, provide a higher agreement with the user preferences?
To demonstrate the fidelity of our four metrics (ASDCG, ASRBP , ASERR and ASAT T ), we compare them with existing IR metrics. We utilise both user-oriented IR metrics capturing topical-relevance (nDCG [12], P @10), and diversityaware metrics (-nDCG [10], D-nDCG [18], D#-nDCG [18], IA-nDCG [1]) which we adapt to incorporate vertical diversity. We select the latter as they are the most prevalent user-oriented IR metrics. Their adaptation is as follows: (i) we replace subtopic importance with orient(V |W, q); (ii) we substitute the user model for ranks to the one that applies to blocks; and finally (iii) we normalise according to the ideal aggregated search page.
8The extent to which an evaluation metric measures what it is intended to measure.

121

To measure the performance of the metrics, we calculate the percentage of agreement (percentage of those pairs for which the metric agrees with the majority preference of users). The larger the percentage of agreement, the more accurately the metric can predict the user preference of any aggregated search page pairs, and the higher the metric fidelity. A two-tailed t-test (significant at the p < 0.01 level, denoted by or ) is used to show which metric correlates more significantly with the user preferences9.
6.1 Standard Parameter Settings
To answer RQ2, we carry out a set of experiments where we employ the prevalent standard parameter settings for the metrics used in IR experiments. We utilise the standard log discount function for all DCG related metrics (ASDCG, nDCG, -nDCG, D-nDCG and D#-nDCG). We set the  parameter in -nDCG to 0.5 and  to 0.5 for D#-nDCG. For our proposed metrics, we set  = 10 (a linearly increasing vertical-orientation function) and i = 0.0 (no personalised vertical diversity preference) as the standard parameters. For the user persistence parameter in ASRBP , we set  = 0.8 as this value best correlates with the user browsing behavior from a real-world query-log data [16]. These standard settings instantiate a simple metric (e.g. ASDCG) similar to existing topical diversity-aware metrics that incorporate subtopic importance probability (D-nDCG). The standard  of ASAT T is obtained by exploring the optimal setting in a development set that contained 500 preference page pairs that contain visually attractive results (results coming from Image and Video).
Our evaluation, the fidelity of the metrics, thus focuses on the agreement (of each metric) with the user preferences over the set of aggregated search results. As we have already categorised page pairs into various quality "bins" (H-H, HM, H-L, M-M, M-L, L-L), we report the experimental results over different bin pairs, in order to understand each metric performance over the whole evaluation space. Our experiments have two parts: (i) when fixing the assumed user browsing model (e.g. DCG), we compare the performance of our proposed metrics with existing IR metrics; (ii) under the proposed framework, we compare user models to investigate which ones make more accurate prediction of the user preferences on aggregated page pairs.
6.1.1 Comparison of Metrics
We present results for a majority preference of 3/4 or greater, or 4/4, in Table 6. The significance is calculated in comparison with one of the proposed metrics, ASDCG. Our metrics have higher agreement with user preferences for the H-M, H-L and M-L bins compared to the less discriminative bins (H-H, M-M, or L-L). In addition, for page pairs with higher majority user agreement (4/4 instead of 3/4), our metrics tend to make more accurate prediction of the user preferences. After closer examination, we observe that the metrics agreement with the majority user preference is higher on pairs where there is greater consensus between assessors. This is similar to reported in [3].
We also observe that overall the proposed aggregated search metrics (ASDCG) work better than existing IR metrics (nDCG
9We also used the sign test [3]. For all page pairs with majority of preference, our proposed metrics performed significantly better than random. Since we are interested in comparing metrics, we do not report the sign test outcomes.

and P @10). They have a significantly better performance across almost the entire metric space. This is not surprising given that the proposed metrics incorporate aspects unique to aggregated search (vertical-orientation), which can affect user preferences. Indeed, when the page quality is expected to be high, traditional IR metrics that do not consider vertical-orientation perform worse than the proposed metrics. But it is worth noting that nDCG performs significantly better than other metrics on L-L page pairs. This might be because as the returned verticals are of low orientation, and for these types of page pairs, simply measuring topical relevance of items might correlate more with the user browsing behavior than considering the additional vertical orientation; when assessing two low-quality pages, the user is trying to find more topically relevant items, without regard to the orientation of the vertical.
For the diversity-aware metric, -nDCG performs significantly worse than the proposed metrics. This is because nDCG implicitly penalises the within vertical redundancy of items. This evaluation strategy is not appropriate when presenting results from the same vertical in a block. A close examination shows that this degraded performance is due to the over-penalisation for items within each vertical. Although recent research [14] has suggested that  may be tuned on a per query basis to either promote or discount extra items from the same sub-topic (vertical), we leave this for future work. In addition, instead of fully utilising the graded orient(V |W, q) information, -nDCG treats relevant verticals in a binary sense, another reason that may cause the degraded performance.
The other existing diversity-aware metric D-nDCG performs comparably well. This is not surprising as when employed with standard parameter setting, D-nDCG is most similar to the proposed aggregated search metrics (ASDCG). The major difference is that ASDCG captures the effort of examining result snippets of different types. D#-nDCG performs significantly worse than D-nDCG over the entire simulated page space used for evaluation in the context of aggregated search. This proves that simply promoting vertical diversity without considering vertical-orientation can degrade the evaluation performance. In addition, as we will see later, because of the various users vertical diversity preference, personalised vertical diversity can be a better strategy for the evaluation of aggregated search. Finally, IA-nDCG also performs considerably worse than ASDCG. A close examination suggests that this is due to the over-rewarding of the vertical results in a page.
When we assume a uniform effort distribution of the resulting snippets, which can be of various types, the metric performances decrease from 67.3% to 65.6%. However, this decrease is not statistically significant. This might be due to the small number of topics promoting image or video vertical results. Estimation of the efforts associated with reading snippet of various types on a large-scale dataset is needed.
6.1.2 Comparison of User Models
For the proposed metrics with various user models (ASDCG, ASRBP , ASERR and ASAT T ), their agreements with the users majority preference (3/4 or greater) are shown in Table 710. We observe that the metric agreements are com-
10The results of metric agreement with 4/4 users majority preference is similar and is, therefore, not included due to space limitations.

122

Table 6: Metric Agreements with Various User's Majority Preference: Proposed Metric vs. Baseline Metrics.

majority preference 3/4 or greater

bins
all H-H H-M H-L M-M M-L L-L

ASDCG 67.3% 61.4% 74.3% 78.0% 64.7% 72.4% 51.3%

D-nDC G
65.9% 60.4% 72.3% 78.4% 62.7% 68.1% 52.6%

D#-nDC G
62.9% 57.2% 68.8% 76.3% 64.2% 67.1% 53.2%

I A-nDC G
64.3% 57.0% 71.1% 75.8% 64.8% 65.8% 53.1%

-nDC G
62.4% 54.0% 60.5% 73.3% 64.9% 70.2% 51.7%

nDC G
60.1% 53.3% 63.1% 67.9% 61.1% 67.3% 54.7%

P @10
53.9% 49.5% 61.2% 58.3% 51.2% 55.1% 47.3%

all

71.1% 69.4%

64.8%

67.7%

63.1%

60.9% 54.1%

H-H 68.2% 65.4%

56.3%

62.1%

53.1%

52.4% 52.3%

H-M 76.3% 76.0%

70.1%

78.2%

62.0%

64.8% 58.1%

4/4

H-L 77.6% 78.9%

76.9%

78.3%

74.1%

65.9% 56.7%

M-M 67.3% 65.1%

65.1%

63.7%

63.4%

62.5% 49.4%

M-L 75.2% 72.4%

66.5%

68.4%

72.0%

68.3% 57.2%

L-L 61.1% 57.8%

51.3%

54.5%

51.9%

52.6% 52.3%

paratively similar; although, overall, the metrics based on position-based user models (ASDCG and ASRBP ) perform consistently better than the adapted cascade model metric ASERR or the attention-based model ASAT T .
We further see that comparatively ASERR performs better on H-M and H-L bins and worse on others. The degraded performance might be due to the fact that only binary topical-relevance assessments (of items) are available and the metric largely rewards the top relevant results. This also partly explains why ASERR performs particularly well between high quality pages (highly oriented and relevant results are presented at the top of the page) and low quality pages. It is most likely that instead of considering the entire page, most assessors looked only at the early results of the page when assessing.
However, surprisingly, by incorporating attention bias (of visually attractive vertical results) into the position-based model, the performance of the metric ASAT T degrades, compared with ASDCG. This might be due to the inaccurate estimation of the attention bias  from our small-scale experiments. After closer examination, it may be that assessors have a considerable preference bias on pages that contain visually attractive results (image, video) [3]. Therefore, the preference assessment between pages containing image and video verticals may be noisier, which could result in a natural bias for those types. Further experiments are needed to explain and understand this bias and its effect.
In comparing ASDCG and ASRBP , although it is observed that ASRBP performs slightly better for page pairs consisting of pages with high quality agreements, the result is not significant. As the only difference between ASDCG and ASRBP is the position-based discounting factor (the user browsing model), the slight improvement is caused by the different user model. This user browsing modelling factor is examined in more detail later.
6.1.3 Summary
Although the results of our proposed metrics are promising when compared with existing IR metrics, the results should be treated with caution as the agreement is not substantial (the best performance is 67.7% from our proposed metric ASRBP ). After a close examination of the user preferences, compared with the metric prediction, the reasons for this include: (i) the vertical-orientation annotations [26] may not fully agree with the real user preference of verticals (they are noisy estimations); and (ii) although three

Table 7: Proposed Metric Agreements with 3/4

User Majority Preferences: Comparison of User Ex-

amination Models.

bins/metrics all

ASDCG 67.3%

ASRBP 67.7%

ASERR 63.8%

ASAT T 66.9%

H-H H-M

61.4% 74.3%

62.1% 75.4%

53.1% 78.2%

60.5% 72.1%

H-L M-M M-L

78.0% 64.7% 72.4%

80.3% 65.2% 71.9%

79.1% 56.7% 64.9%

77.4% 66.3% 70.0%

L-L

51.3% 48.8% 54.1% 54.5%

key components of aggregated search are captured, we have only used simple default values for some of the parameters. This motivates further experiments that aim to learn personalisation parameters from historical data.
6.2 Learning for Metrics
We can improve the performance of our metrics by learning suitable parameter settings using training data, thus addressing the research question RQ3. We only use ASRBP as an example. We recall that ASRBP has three parameters:  that controls the degree to which vertical orientation is rewarded;  that controls the user browsing behavior in terms of user persistence; and i that controls the degree to which a user prefers a diverse aggregated page.
Training is done in two stages. First, we learn suitable values for  and  independently of i. We categorised the user preference data into five sets and use five-fold cross validation for training and testing. We set i = 0.0 (users do not prefer vertical-based diverse results unless the vertical provides better results) and iterate through different settings of values:  (from 1 to 100) and  (from 0.5 to 1.0). The optimal combination is obtained with  = 7.0 and  = 0.85 indicating that users generally favour results that contain highly-oriented verticals, and that users do not have a persistent browsing behaviour (they care more about the results returned in a high position in the page). The corresponding results are shown in Table 8. The performance of the metric is improved over the standard parameter settings from 67.7% to 72.6%. This improvement is due to the better estimation of two parameters  and  concerned with two main aspects of aggregated search, vertical selection and result presentation. By learning from historical data, ASRBP (and other metrics) can better capture these two aspects. Second, we fix the optimal settings for  and  and learn personalised

123

Table 8: Learned ASRBP Metric Agreements with User's Majority Preferences for All Page Pairs.

Parameter Agreement

Standard Optimal  and  Optimal ,  and i

67.7%

72.6%

75.9%

user preference parameters for diversity (i). Although not optimal, this is sufficient to analyse the "personalisable" parameter independently of others.
As we need sufficient data for learning the parameter, we only test this over the top twenty "head" assessors who made most of the assessments. Like with previous setting, for each assessor, we separate assessor data into five sets and use five-fold cross validation to train and test. For the overall performance, we average the performance for all those assessors. The results are also shown in Table 8. The optimal setting for i varies from 0.15 to 0.4 among different assessors whereas the average optimal setting for those assessors is 0.23. Similar to [26], this demonstrates that each user has his/her own understanding and preference over the diversity of the results. We can report that by using this personalised parameter, the prediction of the metric agreement with the majority of user preference is improved significantly, from 72.6% to 75.9%. This effectively illustrates that aggregated search can be improved if we have a better understanding of each user preference over the diversity of results. This is particularly useful for systems that can gather personalised interaction data for their users.

7. CONCLUSIONS AND FUTURE WORK
We introduced a general evaluation framework that captures several traits unique to aggregated search. We instantiated a suite of metrics for evaluating aggregated search pages from this framework. We presented a methodology to collect user preferences over aggregated pages, which allowed us to measure various aspects of our proposed metrics. We did this by simulating aggregated search pages of different quality for a range of topics. The approach allowed us to analyse different parts of the aggregated page pair space. Furthermore, we showed that the proposed metrics correlate well with the majority user preferences and that traditional IR metrics are not well suited to the task. In addition, while some diversity-based metrics can be adapted to measure the preference between page pair, they are not ideal. By instantiating several non-tuned versions of metrics from our framework, we showed that these metrics are at least comparable to diversity-based IR metrics. We also showed that our metrics have the ability to tune their behaviour for pages for which personalised preference data is available.
Future work will involve extending and setting several parameters of the metrics so that they more closely correlate with user preferences for the sets of page pairs. In particular, when query-log data is available, we can further extend the framework by proposing new user browsing models for aggregated search and investigate their values. We will also devise new approaches to utilise the available implicit user feedback data to better estimate the parameters. Another interesting challenge will be to compare the effectiveness and weakness of existing diversity-aware metrics for evaluating aggregated search, and study the ability of our framework
to generalise them.

Acknowledgments This work was supported partially by both the EU LiMoSINe project (288024) and by the Irish Research Council's INSPIRE (FP7 COFUND) programme. Any opinions, findings, and recommendations expressed in this paper are the authors' and do not necessarily reflect those of the sponsors.
8. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. WSDM, 2009.
[2] J. Arguello, F. Diaz, and J. Callan. Learning to aggregate vertical results into web search results. CIKM, 2011.
[3] J. Arguello, F. Diaz, J. Callan, and B. Carterette. A methodology for evaluating aggregated search results. ECIR, 2011.
[4] J. Arguello, F. Diaz, J. Callan, and J.-F. Crespo. Sources of evidence for vertical selection. SIGIR, 2009.
[5] J. Arguello, F. Diaz, and J.-F. Paiement. Vertical selection in the presence of unlabeled verticals. In SIGIR, 2010.
[6] P. Bailey, N. Craswell, R. W. White, L. Chen, A. Satyanarayana, and S. M. M. Tahaghoghi. Evaluating whole-page relevance. SIGIR, 2010.
[7] J. P. Callan, Z. Lu, and W. B. Croft. Searching distributed collections with inference networks. SIGIR, 1995.
[8] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. CIKM, 2009.
[9] D. Chen, W. Chen, H. Wang, Z. Chen, and Q. Yang. Beyond ten blue links: enabling user click modeling in federated web search. WSDM, 2012.
[10] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu®ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. SIGIR, 2008.
[11] G. Dupret. User Models to Compare and Evaluate Web IR Metrics. In SIGIR 2009 Workshop on The Future of IR Evaluation, 2009.
[12] K. J®arvelin and J. Kek®al®ainen. Cumulated gain-based evaluation of ir techniques. TOIS, 2002.
[13] K. S. Jones and C. J. Rijsbergen. Report on the need for and the provision of an `ideal' information retrieval test collection. British Library Research and Development Report No. 5266, 1975.
[14] T. Leelanupab, G. Zuccon, and J. M. Jose. A query-basis approach to parametrizing novelty-biased cumulative gain. In ICTIR, 2011.
[15] X. Li, Y.-Y. Wang, and A. Acero. Learning query intent from regularized click graphs. In SIGIR, 2008.
[16] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. TOIS, 2008.
[17] A. K. Ponnuswami, K. Pattabiraman, Q. Wu, R. Gilad-Bachrach, and T. Kanungo. On composition of a federated web search result page: using online users to provide pairwise preference for heterogeneous verticals. WSDM, 2011.
[18] T. Sakai and R. Song. Evaluating diversified search results using per-intent graded relevance. SIGIR, 2011.
[19] M. Sanderson, M. L. Paramita, P. Clough, and E. Kanoulas. Do user preferences and evaluation measures line up? SIGIR, 2010.
[20] R. L. T. Santos, C. Macdonald, and I. Ounis. Aggregated search result diversification. ICTIR, 2011.
[21] L. Si and J. Callan. Relevant document distribution estimation method for resource selection. SIGIR 2003.
[22] E. M. Voorhees. Overview of the trec 2003 question answering track. In TREC, 2003.
[23] X.-B. Xue, Z.-H. Zhou, and Z. M. Zhang. Improving web search using image snippets. ACM Trans. Internet Technol., 8:21, 2008.
[24] J. Fleiss. Measuring nominal scale agreement among many raters. Psychological Bulletin., 76(5), 1971.
[25] K. Zhou, R. Cummins, M. Lalmas, and J. M. Jose. Evaluating large-scale distributed vertical search. In LSDS-IR workshop in CIKM, 2011.
[26] K. Zhou, R. Cummins, M. Halvey, M. Lalmas and J. M. Jose. Assessing and Predicting Vertical Intent for Web Queries. In ECIR, 2012.

124

Time to Judge Relevance as an Indicator of Assessor Error

Mark D. Smucker
Department of Management Sciences University of Waterloo
mark.smucker@uwaterloo.ca

Chandra Prakash Jethani
Yahoo! Inc. Sunnyvale, CA, USA
jethani@yahoo-inc.com

ABSTRACT
When human assessors judge documents for their relevance to a search topic, it is possible for errors in judging to occur. As part of the analysis of the data collected from a 48 participant user study, we have discovered that when the participants made relevance judgments, the average participant spent more time to make errorful judgments than to make correct judgments. Thus, in relevance assessing scenarios similar to our user study, it may be possible to use the time taken to judge a document as an indicator of assessor error. Such an indicator could be used to identify documents that are candidates for adjudication or reassessment.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords: Relevance judging, assessor error detection
1. INTRODUCTION
We have two motivations for wanting to better understand assessor behavior and assessor error. First, knowledge of assessor behavior should help IR researchers in the construction of more accurate relevance assessing systems. Second, while traditional retrieval metrics are relatively insensitive to assessor error [6], newer metrics that sample documents for judging, can be dramatically affected by assessor error [1, 7]. A single sampled document can represent thousands of documents in the final estimation, and a single document falsely judged to be relevant can greatly inflate the estimated number of relevant documents. Being able to predict which documents might have incorrect judgments could be useful for improving assessor consistency by asking assessors to judge certain documents a second time [1].
We have previously reported on relevance judging errors made by the 48 participants who took part in our user study [5], but we have not previously reported on what behaviors correlate with errors.
In this paper, we present results for a relevance assessing scenario where there was not an incentive for users to work faster. In this scenario, where the assessor is not paid per judgment, but is paid for the time spent, we have found an interesting result: users take more time to make mistakes than they do to make correct relevance judgments.
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

2. MATERIALS AND METHODS
The user study presented participants with fixed ranked lists of documents. Given a ranked list, participants were instructed to search for and save relevant documents. The user interface was web-based and consisted of two web pages. The first page showed 10 document summaries in a format similar to a web search engine's results page. Clicking on a summary took the participant to a page that displayed the full document and allowed the participant to save the document as relevant. We did not require relevance judgments to be made. We treat viewing a full a document and not saving it as a decision to judge the document as non-relevant. Participants could view the next 10 results by clicking on a link at the bottom of the document summaries page.
The study used 8 topics from the 2005 TREC Robust track. The documents came from the AQUAINT newswire collection. Each participant searched ranked lists with a uniform precision of 0.6 or 0.3. We use the NIST relevance judgments as truth.
Participants worked on 4 search topics for 10 minutes each. The study used a fully balanced design. Participants were instructed to work as fast as possible while making as few mistakes as possible. We use data from the second phase of the study. All participants had already completed phase 1 of the study and gained experience with relevance judging before participating in the second phase. The result lists contained near-duplicate documents. We use only the first judgment on a set of near-duplicates.
We measured the time the participants spent on the full document page before saving the document as relevant or leaving the page. It is well-known that large variations in behavior are caused by the search topic and the user. To allow comparisons of times across users and topics, we standardized a participant's times on a topic to have a mean of zero and a standard deviation of 1.
We average a participant's times on a topic, and then average across topics for that participant to produce a participant average. We finally average across participants to produce average participant times.
3. RESULTS AND DISCUSSION
When a participant judges a non-relevant document to be relevant, a false positive error is made. Likewise, when a relevant document is judged to be non-relevant, a false negative error is made. Table 1 and Figure 1 show the main results. These results are based on 3614 judgments (2706 correct judgments, 511 false negative errors, 397 false positive errors).

1153

Average Participant Time to Judge Doc. Relevance Correct False Positive p-value False Negative p-value

Standardized Times

-0.05 ± 0.01 0.26 ± 0.07 < 0.001 0.13 ± 0.08

0.02

Raw Times in Seconds

28 ± 2

39 ± 4

0.03

33 ± 3

0.16

Table 1: Average participant times to judge document relevance for both standardized times and raw times in seconds. Shown with the average is the standard error of the mean. We used a two-sided, Student's t-test to measure the statistical significance of the difference between the time to make correct judgments and each of the error conditions: false positive and false negative errors. For the standardized times, both types of errors took a statistically significant longer time (p < 0.05).

Average Participant Standardized Time to Judge -1.0 -0.5 0.0 0.5 1.0 1.5

False Positive

Correct

False Negative

Figure 1: Average participant standardized times to judge document relevance for false positive, correct, and false negative judgments.

We found that on average, participants took significantly longer to judge documents when they made an error compared to when they were correct in their judgment. A priori, one might guess that participants would make errors by not taking enough time, but in contrast, we find here that time appears to indicate difficulty in making a correct judgment.
We hypothesize that the study participants make judgment errors according to the following process. First the participants identify a topically relevant document based on the document's summary and then click on the summary to view the full document. Participants then study the document to determine if it is relevant or not to the specifics of the search topic. The more difficult it is to determine the document's relevance, the longer the participant takes. At some point, the participant decides to save the document as relevant or abandon the document without saving it. False negatives result from not finding the relevant material in the document, and false positives are the result of the final decision being a guess.
There are many other factors that can cause a user to take longer to judge a document. For example, we have shown that the longer a document, the longer it takes to judge its relevance [3]. Is it just that mistakes are being made on longer documents? To investigate this, we modeled the probability that a judgment was an error using logistic regression. For the model, we considered time, document length measured in words, and the rank of the document in the results list. We found that when both time and document length were used together in the model, document length was not a useful predictor of errors. On the other hand, when we included the interaction between time and document length, this interaction was helpful. In other

words, if the time to judge is too long given the length of the document, this is an indicator of error. We also found that ranks above about 50 were predictive of making errors. Reaching a rank over 50 in 10 minutes is likely indicative of a participant who is not carefully judging documents. For ranks less than 50, rank was not predictive of errors.
The relevance assessing setup may have a significant effect on assessor behavior. In preliminary analyses of separate studies where assessors had an incentive to work faster [4] and relevant documents were highly prevalent (90% relevant) [2], we have seen false negatives be long errors and false positives be short errors. In this paper, both types of errors were long errors. In future work we intend to analyze these studies in more detail by using this paper's method of standardizing times.
4. CONCLUSION
We found in a relevance assessing scenario where the assessor is trusted and paid by time spent, and not by number of judgments made, that all things equal, the longer an assessor takes to make a relevance judging decision, the more likely a mistake will be made. We hypothesize that time to judge a document relative to other documents, gives an indication of the difficulty of judging the document.
5. ACKNOWLEDGMENTS
David Hu wrote the software to compute the sets of nearduplicate documents. This work was supported in part by NSERC, in part by Amazon, and in part by the University of Waterloo.
6. REFERENCES
[1] B. Carterette and I. Soboroff. The effect of assessor error on IR system evaluation. In SIGIR, pp. 539≠546, 2010.
[2] C. Jethani. Effect of prevalence on relevance assessing behavior. Master's thesis, University of Waterloo, 2011.
[3] C. Jethani and M. D. Smucker. Modeling the time to judge document relevance. In Proceedings of the SIGIR'10 Workshop on the Simulation of Interaction, 2010.
[4] M. D. Smucker and C. Jethani. The crowd vs. the lab: A comparison of crowd-sourced and university laboratory participant behavior. In Proceedings of the SIGIR 2011 Workshop on Crowdsourcing for Information Retrieval, 2011.
[5] M. D. Smucker and C. Jethani. Measuring assessor accuracy: a comparison of NIST assessors and user study participants. In SIGIR, pp. 1231≠1232, 2011.
[6] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. IPM, 36:697≠716, September 2000.
[7] W. Webber, D. W. Oard, F. Scholer, and B. Hedin. Assessor error in stratified evaluation. In CIKM, pp. 539≠548, 2010.

1154

Unsupervised Linear Score Normalization Revisited

Ilya Markov
University of Lugano Lugano, Switzerland
ilya.markov@usi.ch

Avi Arampatzis
Democritus University of Thrace
Xanthi, Greece
avi@ee.duth.gr

Fabio Crestani
University of Lugano Lugano, Switzerland
fabio.crestani@usi.ch

ABSTRACT
We give a fresh look into score normalization for merging result-lists, isolating the problem from other components. We focus on three of the simplest, practical, and widelyused linear methods which do not require any training data, i.e. MinMax, Sum, and Z-Score. We provide theoretical arguments on why and when the methods work, and evaluate them experimentally. We find that MinMax is the most robust under many circumstances, and that Sum is-- in contrast to previous literature--the worst. Based on the insights gained, we propose another three simple methods which work as good or better than the baselines.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords
Score Normalization, Distributed Retrieval
1. INTRODUCTION
Merging ranked-lists produced by several engines requires two steps: score normalization and combination. In distributed setups with disjoint collections, the combination step becomes trivial since each document is assigned a single score from one of the participating engines. This makes such setups ideal in isolating the normalization problem. Distributed setups usually include another step before normalization, i.e. resource selection (RS). While useful in improving efficiency, RS has been also shown to improve effectiveness significantly. We argue that this is due to the far-fromperfect quality of state-of-the-art normalization methods: in an ideal normalization, e.g. scores are normalized to probabilities of relevance, any kind of RS will hurt effectiveness by excluding sources with relevant documents; in such an ideal situation, the more systems one combines, the better the effectiveness. Consequently, the theoretical ceiling of effectiveness can only be achieved with an ideal normalization without RS, and distributed setups with disjoint collections are best for experimenting with normalization [1].
Previously proposed normalizations vary from linear to non-linear functions which may require or not training data for their estimation. While there is a rich literature on the subject, most experiments reported do not isolate the prob-
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

lem as we described above [3, 4, 5]. We give a fresh look into the simplest normalization methods: linear functions which do not require any training or search engine cooperation. Being the most practical, they are also the fastest and generally have been proved to be robust and effective. Beyond an empirical comparison, we also give theoretical justifications on their implicit assumptions. To our knowledge, these assumptions have never been made explicit or they are `lost' in the large volume of the related literature. Based on the gained insights, we propose three alternative linear normalization methods.
In the experiments we use the gov2.1000 and gov2.30 splits of the TREC GOV2 dataset [2]. In gov2.1000 the largest 1000 hosts of the GOV2 dataset are treated as 1000 sources, and the number of sources that contain relevant documents is usually much less than 1000. In gov2.30 the collections of gov2.1000 are clustered into 30 sources; here, relevant documents appear in most of the sources. We use ten retrieval functions implemented by the Terrier toolkit 1, namely, BM25, tf-idf (Terrier and Lemur versions), language modeling (original and with Dirichlet smoothing), and a number of DFRbased functions (BM25, BB2, IFB2, InL2 and PL2). Retrieval functions are randomly assigned to sources. Topics 701-850 are used as queries. Operationally, engines return truncated rankings for efficiency reasons, thus, we only consider either the top 10 or 1000 results. Our results are summarized in Tables 1 and 2, which we will refer to throughout the paper. Statistical significance is measured with a paired t-test.  shows the significantly best baseline (MinMax, Sum or Z-Score) and  marks the modification that is significantly better than any baseline, both at the 0.05 level.
2. LINEAR SCORE NORMALIZATION
The MinMax method [3] scales the output score range per engine to [0, 1]. We argue that the most important assumption behind MinMax is that each source contains at least 1 relevant document, and that this document will mostlikely get ranked 1st. By assigning the same highest score to all 1st documents, they are ranked before any other in the merged list. Since it is also assumed that these documents are most-likely relevant, a high early precision is achieved which pushes higher other evaluation measures sensitive to early rank positions, e.g. MAP. Thus, MinMax owes its success to getting right the early ranks in the merged list. However, due to fact that the 1st document of each engine is assigned the same highest score, MinMax produces a roundrobin effect in merging which may impact effectiveness neg-
1http://terrier.org

1161

atively as the number of engines increases. Indeed, MinMax shows high performance when run on 30 sources that contain sufficient relevant documents, i.e. the above assumption is satisfied. On the contrary, when run on 1000 sources, where the assumption is weakened by the sparseness of relevant documents and additionally the round-robin effect is more prominent, MinMax performance degrades considerably.
The Sum method [5] is similar to MinMax, but without using a fixed highest score eliminating the undesirable round-robin effect: the minimum score is shifted to 0 while the sum of all scores per ranked-list is scaled to 1: s = s-min, snorm = s / i si. The intuition given in [4] is: under the assumption of exponentially-distributed scores, the normalization is equivalent to setting the means of score distributions of sources to be equal to each other. However, many studies on modeling scores show that the aforementioned assumption of exponentially-distributed scores does not hold in practice, especially for top-ranked documents. Our experimental results show that the Sum method almost always has the worst performance. This contradicts the results of the original work [5], however, the authors in the last-mentioned study used a meta-search setup and evaluated normalization and combination methods together--a setup that does not isolate the normalization problem.
In [5] also the Z-Score method is proposed, which normalizes each score to its number of standard deviations that it is away from the mean score. However, as previously noted in [1], this method rather assumes a bell-like distribution of document scores (e.g. a Normal), where the mean would be a meaningful `neutral' score. However, in practice score distributions are highly skewed and clearly violate this assumption. Still if only the top documents are considered from each result list, they are likely to be relevant, and therefore the distribution of their scores may be close to that of relevant documents, i.e. likely a Normal. Indeed, our results show that when only the top-10 documents are retrieved from each source, Z-Score usually shows higher performance than when applied on the top-1000 documents.
So far we have seen that methods that assume particular score distribution shapes, such as Sum and Z-Score, are worse, and that MinMax which does not make such assumptions is better. In other words, the sum or the mean of scores do not seem to be a good statistics for normalization, and that we should be looking into the direction of MinMax for developing a better normalization. We also know that using a fixed highest score should be preferably avoided so as to eliminate the round-robin effect, as well as, to be able to down-weigh sources of no relevance (haunting the main assumption of MinMax). Another problem rarely mentioned that all three methods above have is that they are greatly affected by the minimum score seen, or else, the chosen truncation point of the rankings (e.g. the minimum score of top-10 documents is usually very different from the minimum score of top-1000).
We will first try to deal with the minimum-score problem. The lowest theoretical score of many scoring functions is 0. Thus, making the assumption that the most non-relevant documents usually score at 0, the minimum in MinMax function should also be set to 0. This way we obtain the Max method: snorm = s/max. Our results show that Max, although simpler, almost always has the same or higher performance than that of the MinMax method.
Let us now try to deal with non-relevant sources which vio-

MinMax Sum Z-Score Max MMStdv UV

MAP 0.0700 0.0171 0.0367 0.0722 0.0632 0.0182

gov2.30

gov2.1000

p@10 p@100 MAP p@10 p@100

0.1953 0.1674 0.0027 0.0161 0.0140

0.0134 0.0292 0.0004 0.0013 0.0019

0.0819 0.0935 0.0014 0.0013 0.0054

0.1953 0.1734 0.0023 0.0161 0.0140

0.1738 0.1348 0.0038 0.0114 0.0088

0.1248 0.0676 0.0006 0.0013 0.0009

Table 1: Top 1000 documents are retrieved.

MinMax Sum Z-Score
Max MMStdv UV

gov2.30 MAP p@10 0.0372 0.1966 0.0355 0.1785 0.0353 0.1839 0.0397 0.1953 0.0366 0.1604 0.0381 0.1980

gov2.1000 p@100 MAP p@10 p@100 0.1437 0.0027 0.0161 0.0140 0.1401 0.0029 0.0013 0.0140 0.1413 0.0031 0.0007 0.0162 0.1656 0.0028 0.0161 0.0140 0.1432 0.0025 0.0054 0.0139 0.1498 0.0049 0.0228 0.0225

Table 2: Top 10 documents are retrieved.

late the main assumption of MinMax. Scoring functions aim

at assigning scores in a way that relevant documents have

very different scores from non-relevant documents. There-

fore, if the standard deviation  of scores in a ranked-list

is high, the list is likely to contain both relevant and non-

relevant documents. If  is low, the list is likely to contain

either only relevant or only non-relevant documents. On

one hand, long ranked-lists (e.g. 1000 documents) are likely

to contain many non-relevant documents and, therefore, we

prefer those that have high standard deviation (i.e. they also

contain many relevant documents). In this case, the follow-

ing linear modification makes sense:

snorm

=



s-min max-min

.

We call it MM-Stdv. On the other hand, short ranked lists

(e.g. 10 documents) may contain only relevant documents

and, therefore, we might prefer the lists with low standard

deviation. Unit-variance linear modification (UV) is similar

to Z-Score and scale-invariant but does not shift the mean

to 0: snorm = score/. Our results support both formulas

depending on the situation.

3. CONCLUSIONS
We isolated the normalization problem and found that MinMax is the most robust method under many circumstances, Z-Score and Sum may perform well when some conditions are met, and Sum is worse than previous literature suggested. Furthermore, we gave theoretical insights on why and when these methods work or fail, and proposed three new methods that work as good or better than the baselines.

4. REFERENCES
[1] A. Arampatzis and J. Kamps. A signal-to-noise approach to score normalization. In Proceeding of the ACM CIKM, pages 797≠806, 2009.
[2] J. Arguello, J. Callan, and F. Diaz. Classification-based resource selection. In Proceedings of the ACM CIKM, pages 1277≠1286. ACM, 2009.
[3] J. H. Lee. Analyses of multiple evidence combination. In Proceedings of the ACM SIGIR, pages 267≠276. ACM, 1997.
[4] R. Manmatha and H. Sever. A formal approach to score normalization for meta-search. In Proceedings of the HLT, pages 98≠103. MKP Inc., 2002.
[5] M. Montague and J. A. Aslam. Relevance score normalization for metasearch. In Proceedings of the ACM CIKM, pages 427≠433. ACM, 2001.

1162

Using PageRank to Infer User Preferences
Praveen Chandar and Ben Carterette {pcr,carteret}@udel.edu
Department of Computer and Information Sciences University of Delaware
Newark, DE, USA 19716

ABSTRACT
Recently, researchers have shown interest in the use of preference judgments for evaluation in IR literature. Although preference judgments have several advantages over absolute judgment, one of the major disadvantages is that the number of judgments needed increases polynomially as the number of documents in the pool increases. We propose a novel method using PageRank to minimize the number of judgments required to evaluate systems using preference judgments. We test the proposed hypotheses using the TREC 2004 to 2006 Terabyte dataset to show that it is possible to reduce the evaluation cost considerably. Further, we study the susceptibility of the methods due to assessor errors.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]
Keywords: preference judgments, PageRank
1. INTRODUCTION
The idea of pairwise preference judgments is relatively new in IR literature. In the case of preference judgments, an assessor looks at a pair of documents and expresses a preference for one over the other instead of assigning a relevance label to a document. Comparison studies between absolute and preference judgments show that preference judgments have various advantages such as reducing the complexity of the task and increasing inter-assessor agreement [3]. Preference judgments tend to help assessors make finer distinctions between documents [5]. Although it is possible to use graded absolute judgments, it is difficult to determine the specifics of the grades, and the burden on assessors is likely to increase with an increase in the number of grades.
A major drawback of using pairwise preference judgments is that as the number of documents in the pool increases, the number of judgments increases polynomially. If there are n documents in the pool, then n(n-1)/2 preference judgments would be necessary to ensure that all pairs of documents are judged. This enormous increase in the number of pairwise judgments not only increases evaluation cost but also gives rise to user fatigue and boredom, possibly leading to poor quality of judgments.
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

In this work, we describe a novel technique that could be used to reduce the number of pairwise judgments. Firstly, we show that pairwise judgments for each query can be represented in the form of a directed graph on which various graph algorithms can be applied. We study the behavior of PageRank and show that the number of pairwise judgments could be reduced considerably. Further, we show that the PageRank method, although susceptible to assessor error, does significantly better than the majority vote approach that is considered as the baseline.
2. PREFERENCE FRAMEWORK
In this section we describe a novel technique to collect preference judgments at a reduced cost. The technique consists of two steps: graphical representation of pairwise judgments and scoring of each node in the graph. Scoring of nodes can be done using various graph algorithms such as PageRank, HITS, etc.
We describe a typical user interface for collecting preference judgments as follows: the assessor would be shown two documents and a statement of an information need (a topic); the assessor would have to pick the most preferred document using the prefer left or prefer right buttons. Additionally, both documents could be judged not relevant to indicate that all other documents should be preferred to them. The criteria for preference is task dependent and is often explained to the assessor in the form of guidelines.
The collected pairwise judgments are then represented in the form of a directed graph. Each unique document is represented as a vertex in the graph and the edges between vertices corresponds to the pairwise judgments. While there are several ways to do this conversion, we describe a way which worked for us. Let's say there are three judgments a user can make for a pair of documents (DocA, DocB): prefer DocA, both non relevant, and prefer DocB. When prefer DocA is selected for a pair, this translates to a directed link from DocB to DocA, similarly a link from DocA to DocB when DocB is preferred and there exists no link between DocA and DocB in the case of both non relevant.
Now that the pairwise judgments have been represented in the form of a graph, there are several graph algorithms such as PageRank, HITS, etc. that could be used to assign a score. The score represents the relevance of a document and documents are ranked based on these scores. In this work we compare the performance of PageRank with the a baseline majority vote method.

1167

PageRank The PageRank algorithm proposed by Brin and Page [2] is a graph algorithm that assigns a numerical weighting to each vertex in a graph, with the purpose of measuring its relative importance within the graph. We compute the page rank scores for each node in the preference graph and hypothesize that the page rank scores correlate to the degree of relevance of the document. Majority Vote A common technique used to produce a ranking from pairwise judgments is to sort documents by the number of times the document was preferred. The count of the number of preferences can be obtained from the preference graph by computing the in-degree for each node. We use this method as our baseline.
Both algorithms produce the same ranking when all pairwise judgments are used, i.e. if n(n - 1)/2 pairwise judgments are used to judge n documents. But their behavior changes if some pairwise judgments are removed from the sample of n(n - 1)/2 pairwise judgments. In order to study this behavior, we simulate preference judgments from graded judgments and evaluate the performance of rankings produced by randomly sampling 1%, 5%, 10%, ..., 100% of n(n - 1)/2 pairwise judgments. We use the nDCG measure proposed by Jarvelin and Kekalainen [4] to evaluate the performance of each ranking against original graded judgments.
3. IMPLEMENTATION AND RESULTS
Our experiments are conducted on data simulated using the TREC 2004 to 2006 Terabyte dataset. The TREC Terabyte consists of a total of 150 topics with relevance graded judgments on a three-point scale. The corpus is a collection of Web data crawled from websites in the GOV domain during early 2004. The absolute graded judgments are converted to preference judgments by generating all possible pairs for the documents in the qrels file for each topic; the document with a higher grade is preferred in each pair. Aslam et al. [1] presented a meta-search approach known as meta-AP which is a function of the document's position in a set of ranked lists, with higher rankings contributing more to the metaAP score. We employ metaAP scores to resolve ties when the grades of both the documents in a pair are equal (document with higher meta score is chosen). Document pairs containing two non-relevant documents are considered as both non-relevant (no link in preference graph).
Figure 1 shows the nDCG scores at rank 20 and 1000 for the PageRank method compared with the majority votes method. The figure includes a run each for various sample sizes of the pairwise judgments and three Down Sample runs represented as DS. The Down Samples were generated by pairing each document in the pool of n documents with another document k times, i.e. each document in the pool of n documents is paired with another document for DS1 and two other documents for DS2 and so on. Note that there would be n pairwise judgments in DS1 and 2n pairwise judgments in DS2. The nDCG computed from PageRank-based judgments is much higher than that computed from majority vote. Clearly, the PageRank method requires fewer number of pairs of judgments compared to the majority votes method on average and the results suggest that it is possible to collect preference judgments by judging just 5% of the pairs with minimal loss of performance.
Figure 2 shows the performance of majority votes and PageRank methods with error. We added error for each run by sampling 1%, 5% and 10% of the pairs and chang-

nDCG@20 0.5 0.6 0.7 0.8 0.9 1.0 DS1 DS2 DS3 1 5 10 20 30 40 50 60 70 80 90 100
nDCG@1000 0.5 0.6 0.7 0.8 0.9 1.0 DS1 DS2 DS3 1 5 10 20 30 40 50 60 70 80 90 100

    





 

     



 

















 



















 PageRank  MajorityVote

 PageRank  MajorityVote

Rankings

Rankings

Figure 1: nDCG@20 and nDCG@1000 scores for the PageRank and majority votes method for various sample sizes. DS denotes Down Sample.

ing the judgments. The judgments were changed randomly either by deleting the edge or changing the direction of the edge. The PageRank method dominates the majority vote method when more preferences are given (5% and greater), but majority vote tends to do better for smaller samples and when there are a constant number of preference judgments for each document.

nDCG@20 0.2 0.4 0.6 0.8 1.0 DS1 DS2 DS3 1 5 10 20 30 40 50 60 70 80 90 100
nDCG@20 0.2 0.4 0.6 0.8 1.0 DS1 DS2 DS3 1 5 10 20 30 40 50 60 70 80 90 100

  

 


 


 


 


 


 


 

 


  

 



 





 





 pageRank No Error  pageRank 1% Error  pageRank 5% Error  pageRank 10% Error

 





























 



 majorityVotes No Error  majorityVotes 1% Error  majorityVotes 5% Error  majorityVotes 10% Error

Rankings

Rankings

Figure 2: nDCG@20 scores for the PageRank and majority votes method for various sample sizes with assessor errors. DS denotes Down Sample.

4. CONCLUSION AND FUTURE WORK
We have presented a novel method to reduce evaluation cost using PageRank. While it is common to use majority vote to score and obtain a ranking from preferences, we have shown that using PageRank might be cost effective. Further, the PageRank method outperforms the baseline while assessor errors are added. Future directions for our work includes studying various other graph methods, picking documents intelligently for pairwise judging.
5. REFERENCES
[1] J. A. Aslam, V. Pavlu, and E. Yilmaz. Measure-based metasearch. In Proceedings of SIGIR, SIGIR '05, pages 571≠572, NY, USA, 2005. ACM.
[2] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In Proceedings of WWW, pages 107≠117, 1998.
[3] B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais. Here or there: preference judgments for relevance. In Proceedings of the ECIR, pages 16≠27, 2008.
[4] K. J®arvelin and J. Kek®al®ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., pages 422≠446, October 2002.
[5] M. E. Rorvig. The simple scalability of documents. JASIS, 41(8):590≠598, 1990.

1168

Utilizing Inter-Document Similarities in Federated Search

Savva Khalaman

Oren Kurland

savvakh@tx.technion.ac.il kurland@ie.technion.ac.il

Faculty of Industrial Engineering and Management Technion -- Israel Institute of Technology Haifa 32000, Israel

ABSTRACT
We demonstrate the merits of using inter-document similarities for federated search. Specifically, we study a resultsmerging method that utilizes information induced from clusters of similar documents created across the lists retrieved from the collections. The method significantly outperforms state-of-the-art results merging approaches.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation
Keywords: inter-document similarities, federated search
1. INTRODUCTION
Federated search is the task of retrieving documents from multiple (possibly non-overlapping) collections in response to a query [1]. The task is typically composed of three steps: attaining resource (collection) description, selecting resources (collections), and merging the results retrieved from the selected collections [1]. We focus on the resultsmerging step; specifically, we study the merits of using information induced from inter-document similarities.
While there is much work on utilizing inter-document similarities for the single-corpus retrieval setting, there is little work along that venue for federated retrieval. For example, clustering was used to transform a single-collection retrieval setting into that of multiple collections [8]. Clusters of sampled documents were used for performing query expansion in federated search [5]; yet, inter-document similarities were not used for results merging. Furthermore, it was shown that among the clusters created across the lists retrieved from different collections there are some that contain a high percentage of relevant documents [3]; still, a results merging method exploiting these clusters was not proposed
The only work, to the best of our knowledge, that uses inter-document-similarities for (direct) results merging is based on scoring a document by its similarity with other documents in the retrieved lists [6]. We show that the method we study substantially outperforms this approach.
The method we present for merging results in federated search is adapted from recent work on fusing lists that were retrieved from the same collection [4]. In contrast to the non-overlapping collections setting we explore here, the retrieved lists in this work [4] were (partially) overlapping and
Copyright is held by the author/owner(s). SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.

the merging (fusion) methods used to assign initial scores to documents exploited this overlap. The adapted method that we study integrates retrieval scores assigned by a state-ofthe-art results-merging approach (e.g., CORI [1] and SSL [7]) with information induced from clusters created from similar documents across the retrieved lists. Specifically, a document can provide relevance-status support to documents in the same list or in other lists that it is similar to. The resultant retrieval performance is substantially better than that of using only the initial retrieval scores assigned by the state-of-the-art merging approach.

2. RESULTS-MERGING METHOD
Suppose that some resource (collection) selection method
was applied in response to query q [1]. We assume that docu-
ment lists were retrieved from the selected (non-overlapping)
collections and merged by some previously proposed resultsmerging algorithm [1, 7]. Let Di[nn]it denote the list of the n documents most highly ranked by the merging algorithm
that assigns the (initial) score Finit(d; q) to document d. Our goal is re-ranking Di[nn]it to improve ranking effec-
tiveness. (Documents not in Di[nn]it remain at their original ranks.) To that end, we study the utilization of inter-
document similarities by adapting a method proposed in
work on fusing lists retrieved from the same corpus [4]. Let Cl be the set of document clusters created from Di[nn]it using some clustering algorithm; c will denote a cluster. Then, the score assigned to document d ( Di[nn]it) by the Clust method is:

FClust(d; q) d=ef (1 X
P

- ) P
d
F (c; q)

Finit(d; q)

+

Di[nn]it

Finit(d P

;

q)

P

diPc Sim(di,

d)

;

cCl c Cl F (c ; q) d Di[nn]it dic Sim(di, d )

F (c; q)

d=ef

Q
di c

Finit(di; q);

as

all

the

clusters

in

Cl

contain

the same number of documents (see details below), there is

no cluster-size bias incurred; Sim(∑, ∑) is the inter-document

similarity measure used to create Cl;  is a free parameter.

Thus, d is highly ranked if (i) its (normalized) initial score

is high; and, (ii) it is similar to documents in clusters c

that contain documents that were initially highly ranked.

In other words, similar documents provide relevance-status

support to each other via the clusters to which they belong. Note that if  = 0, then cluster-based information is not used and FClust(d; q) is d's normalized initial score.

1169

Uni Rel NRel Rep KM

CORI SSL CORI SSL CORI SSL CORI SSL

initial CRSC Clust initial CRSC Clust
initial CRSC Clust initial CRSC Clust
initial CRSC Clust initial CRSC Clust
initial CRSC Clust initial CRSC Clust

CORI SSL

initial CRSC Clust initial CRSC Clust

Queries: 51-100 p@5 p@10 MAP

.460
.524 .536i .432 .492
.504

.420 .060 .474i .062 .498i .064i
.410 .060
.458 .062 .490i .064i

.424 .384 .059

.428 .476 .412

.402 .061 .442i .063ic .384 .095

.412 .404 .096 .480ic .454ic .099ic

.452 .446 .064

.476 .476 .066

.552ic .516ic .069ic .464 .448 .065

.488 .462 .067

.536i .516ic .070ic

.428 .408 .060

.416 .418 .060

.492c .474ic .064ic .444 .408 .061

.448 .434 .060

.476 .470i .064ic

Queries: 201-250

p@5 p@10 MAP

.468 .402 .135

.484 .396 .138

.496 .460ic .147i .480 .412 .152

.512 .412 .150 .532 .478ic .165ic

Queries: 101-150 p@5 p@10 MAP

.396 .376 .480ic .412 .400 .492c
.432 .456 .496 .440 .452 .492

.380 .054
.372 .052 .454ic .059ic .396 .055 .346 .052 .468ic .060ic
.370 .064 .442 .072i .466i .073i
.376 .095 .436 .103i .474i .105i

.416 .472 .500i .396
.456 .504i

.394 .414
.450 .398 .412
.452

.054
.057 .059ic .055
.058 .060i

.428 .432
.496 .452 .396
.508c

.388 .050
.424 .052 .474i .056ic .408 .052
.418 .052 .484ic .058ic

Table 1: Results. Boldface: the best result per testbed, evaluation measure, and initial merging method; 'i' and 'c': statistically significant differences with initial and CRSC, respectively.

3. EVALUATION
We conducted experiments with testbeds that are commonly used in work on federated search [1, 2, 5, 8, 7]: (i) Uni: Trec123-100col-bysource (Uniform), (ii) KM: Trec4kmeans (K-means), (iii) Rep: Trec123-2ldb-60col (Representative), (iv) Rel: Trec123-AP-WSJ-60col (Relevant), and (v) NRel: Trec123-FR-DOE-81col (non-relevant). Titles of the TREC topics 51-150 served for queries for all testbeds except for KM where the description fields of TREC topics 201-250 were used. Tokenization, Porter stemming, and stopword removal were applied using the Lemur toolkit (www.lemurproject.org), which was used for experiments. To acquire resource (collection) description, we adopt the query-based sampling method from [2] which was also used in [1, 7, 5]. Following common practice [1, 7], the 10 highest ranked collections are selected in the resource-selection phase using CORI's resource selection method. As in previous report [7], 1000 documents are retrieved from each selected collection using the INQUERY search engine. Then, the retrieved lists are merged using either CORI's merging method [1] or the single-model SSL merging approach [7]. The initial score, Finit(d; q), assigned to d by these methods is used in our Clust method.
To cluster Di[nn]it, we use a simple nearest-neighbors-based clustering approach [4]. For each d  Di[nn]it we create a cluster that is composed of d and the  - 1 ( = 5) documents d in Di[nn]it (d = d) that yield the highest Sim(d, d ) d=ef

"

"

exp -D(p[d0](∑)||p[dµ](∑)) ; p[xµ] is the Dirichlet-smoothed un-

igram language model induced from x with the smooth-

ing parameter  (= 1000); D is the KL divergence; the

term-counts statistics used for smoothing language models

is based on the query-based sampling mentioned above.

The initial ranking induced by the CORI and SSL results-

merging methods serves for a baseline. Additional reference

comparison that we use is the Cross Rank Similarity Com-

parison scoring

(CRSC) approach [6] that

P

d with

d

(=d)Di[nn]it

P d

(=d

re-ranks Di[nn]it
Sim(d ,d)
)Di[nn]it Sim(d ,d

here by ) . Our

Clust method incorporates two free parameters: n ( {10, 30,

50, 100}), the number of documents in Di[nn]it, and  ( {0, 0.1, . . . , 1}), the interpolation parameter; CRSC only depends

on n. We set the free-parameter values for each method us-

ing leave-one-out cross validation performed over the queries

per testbad; MAP@1000 serves as the optimization criterion

in the learning phase. In addition to MAP, we also present

p@5 and p@10 performance numbers. Statistically signif-

icant differences in performance are determined using the

two-tailed paired t-test at a 95% confidence level.

Results and Conclusions. We see in Table 1 that Clust
always outperforms the initial ranking that was induced by a state-of-the-art results-merging method; often, the improvements are statistically significantly. This finding attests to the merits of integrating the initial results-merging score with information induced from clusters of similar documents. Further exploration reveals that   {0, 1} often yields optimal performance. This shows that the integration just mentioned yields performance that is better than that of using each of the two integrated components alone. We also see that Clust consistently outperforms CRSC, which does not utilize the initial results-merging scores, nor uses a cluster-based approach.
Acknowledgments We thank the reviewers for their comments. This paper is based upon work supported in part by the Israel Science Foundation under grant no. 557/09. Any opinions, findings and conclusions or recommendations expressed here are the authors' and do not necessarily reflect those of the sponsors.

4. REFERENCES
[1] J. Callan. Distributed information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 5, pages 127≠150. Kluwer Academic Publishers, 2000.
[2] J. Callan and M. Connell. Query-based sampling of text databases. ACM Transactions on Information Systems, 19(2):97≠130, 2001.
[3] F. Crestani and S. Wu. Testing the cluster hypothesis in distributed information retrieval. Information Processing and Management, 42(5):1137≠1150, 2006.
[4] A. Khudyak Kozorovitsky and O. Kurland. Cluster-based fusion of retrieved lists. In Proceedings of SIGIR, pages 893≠902, 2011.
[5] M. Shokouhi, L. Azzopardi, and P. Thomas. Effective query expansion for federated search. In Proceedings of SIGIR, pages 427≠434, 2009.
[6] X. M. Shou and M. Sanderson. Experiments on data fusion using headline information. In Proceedings of SIGIR, pages 413≠414, 2002.
[7] L. Si and J. Callan. A semisupervised learning method to merge search engine results. ACM Transactions on Information Systems, 21(4):457≠491, October 2003.
[8] J. Xu and W. B. Croft. Cluster-based language models for distributed retrieval. In Proceedings of SIGIR, 1999.

1170

Personalization of Search Results Using Interaction Behaviors in Search Sessions

Chang Liu
School of Communication and Information, Rutgers University
4 Huntington Street,
New Brunswick, NJ 08901, USA
changl@eden.rutgers.edu

Nicholas J. Belkin
School of Communication and Information, Rutgers University
4 Huntington Street,
New Brunswick, NJ 08901, USA
belkin@rutgers.edu

Michael J. Cole
School of Communication and Information, Rutgers University
4 Huntington Street,
New Brunswick, NJ 08901, USA
m.cole@rutgers.edu

ABSTRACT
Personalization of search results offers the potential for significant improvement in information retrieval performance. User interactions with the system and documents during informationseeking sessions provide a wealth of information about user preferences and their task goals. In this paper, we propose methods for analyzing and modeling user search behavior in search sessions to predict document usefulness and then using information to personalize search results. We generate prediction models of document usefulness from behavior data collected in a controlled lab experiment with 32 participants, each completing uncontrolled searching for 4 tasks in the Web. The generated models are then tested with another data set of user search sessions in radically different search tasks and constrains. The documents predicted useful and not useful by the models are used to modify the queries in each search session using a standard relevance feedback technique. The results show that application of the models led to consistently improved performance over a baseline that did not take account of user interaction information. These findings have implications for designing systems for personalized search and improving user search experience.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ≠ relevance feedback.
General Terms
Measurement, Human Factor
Keywords
Implicit feedback, search behaviors, document usefulness, task type, personalization
1. INTRODUCTION
User search interactions are complex, but they are also coherent over segments of a search session, for example when evaluating results from a query. Overall, one expects the actions of a user to reflect their task goal and the process they carry out to achieve that goal. It is reasonable, then, to think that evidence of the user's search interests and task goal may be available in observations of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08...$15.00.

their behaviors during the search session. In this study, we investigate the effectiveness of using the observation of people's behaviors during information-seeking sessions for improving and personalizing their interactions with information. We generated document usefulness prediction models from a laboratory experiment of the influence of different types of tasks on task search session behaviors, including a general model and several specific models tailored to different types of tasks. Then these prediction models were applied to TREC 2011 Session Track data to evaluate their retrieval performance.
Our study makes several contributions to the literature. First, we examined user behavioral measures as predictors of document usefulness and built prediction models. Second, we demonstrate the prediction models perform well on an entirely different dataset of user interactions. Third, we found that the prediction accuracy of document usefulness and some of users' search behaviors are positively related to the consistent increase in retrieval performance by the personalization model.
2. RELATED WORK
2.1 Sources for Implicit Relevance Feedback
Users of information retrieval systems can provide explicit information about their interests and task intent via relevance feedback. However, users may not be willing to provide explicit relevance feedback, so research has concentrated on techniques to gain relevance feedback information via implicit processes. These implicit relevance feedback (IRF) techniques observe user search behaviors unobtrusively and infer the relevance/usefulness of documents and other information objects from the user interactions. Evidence for IRF comes from various measures of user search behaviors while interacting with individual content pages, search result pages, and other behaviors during a search session [11] [9].
Document usefulness for IRF may be learned from behaviors such as dwell time on each content page, the number of clicks and scrolling on each content page, number of visits to each content page, and further usage of content pages [9]. User behaviors on search result pages have been found to be good indicators of user document preferences [9]. These behaviors include the time on a search result page before the first click, total time on a search result page, click-through and click order of each content page on the search result page, and so on [1] [7]. There is also evidence that the nature of query reformulation during a search episode can be predictive of document usefulness, and perhaps of task type [13]. In the research to date, these sources for IRF have typically been investigated individually, with little work integrating multiple user behaviors over all types of Web pages to predict document usefulness.

205

2.2 Implicit Relevance Feedback
Correlation of individual measures of user behavior with users' document preferences have been found to vary for different task types [10] [16] and in different search stages [14]. White and Kelly [16] analyzed retrieval performance after implementing an IRF system based on dwell time on content pages when task type information was considered and when the user information was considered. They found that dwell time combined with knowledge of task information improved retrieval performance. White, Ruthven and Jose [18] examined the effect of search stages on the utility of implicit relevance feedback (IRF) and explicit relevance feedback (ERF) in two separate systems. Their results showed that in the IRF system, IRF is used more in the middle of the search than at the beginning or end, whereas in ERF system, ERF is used more towards the end. Liu and Belkin [14] examined the interaction between decision time on content pages, search stage, and document usefulness, and found knowledge of search stage could help improve the interpretation of decision time as IRF, as could knowledge of task and topic. This work suggests that a high performance search personalization model should predict document usefulness taking account of the users' search context, including task type, user task and topic knowledge, search stage, and so on. Generally, the performance of a personalization algorithm should be improved if it incorporates the ability to predict aspects of the user's task and knowledge.
3. USER EXPERIMENT
3.1 Experimental Design
For our initial study, we recruited 32 participants from a domain relevant to our work/search tasks. They were informed in advance that they would receive $20 for participation. To ensure they treated their assigned tasks seriously, they were told that the top 25% who saved the best set of pages for all four tasks, as judged by an external expert, would receive an additional $20.
The participants were undergraduates at Rutgers university majoring in journalism and were between 18 and 27 years old (26 female, and 6 male). Most were native English speakers (78%) with the balance indicating a high proficiency in English. Participants had an average search experience of 8.85 years using a range of browsers (IE, Firefox, Safari, Chrome, and others). Generally, participants rated their search experience high with more Web search experience as compared to online library catalog search. They were generally positive about their average success when conducting online searches.
Each participant was given a tutorial as a warm-up task and then performed four Web search tasks (described in section 3.2). A pre-search questionnaire elicited their prior familiarity with each task, and their knowledge of the task topic. Participants were asked to search using IE 6.0 on our lab computer, were free to go anywhere on the Web to search for information, and were asked to continue the search until they had gathered enough information to accomplish the task. There was a time limit of 20 minutes for each task.
For each task, participants were asked to save content pages that were useful for accomplishing the assignment, and could delete saved pages that were later found to be not useful. When participants decided they had found and saved enough information objects for purposes of the task, they were then asked to evaluate the usefulness of the information objects they saved, or saved and then deleted, through replaying the search using a screen capture program. An online questionnaire was then administered to ask

about their searching experience, including their subjective evaluation of their performance, and reasons for that evaluation. The order of the four tasks was systematically rotated for each participant following a Latin Square design. After completing four different tasks, an exit questionnaire was administered asking about their overall search experience.
3.2 Search Tasks
The four work tasks and associated search tasks that we identified are presented below. These tasks follow the normal scenario practice as proposed by Borlund [4], and are couched in journalism terms; that is, journalists are typically given an assignment, and an associated task to complete. Each task was constructed using the faceted task classification scheme proposed by Li & Belkin [12], (modified slightly by us) in order to vary tasks systematically by facet values. In the task descriptions, below we identify the facet values for each (indicated in italics). These are also specified in Table 1.
Background Information Collection (BIC)
Your assignment: You are a journalist for the New York Times, working with several others on a story about "whether and how changes in US visa laws after 9/11 have reduced enrollment of international students at universities in the US". You are supposed to gather background information on the topic, specifically, to find what has already been written on this topic. Your task: Please find and save all the stories and related materials that have already been published in the last two years in the Times on this topic, and also in five other important newspapers.
The BIC task is a Mixed Product because identifying "important" newspapers is intellectual, but finding topical documents is factual. It is Document Level because whole stories are judged. It has the Specific Goal of finding documents on a well-defined topic, but Unnamed because the search targets are not specifically identified (compare with CPE, below).
Interview Preparation (INT)
Your assignment: Your assignment editor asks you to write a news story about "whether state budget cuts in New Jersey are affecting financial aid for college and university students. Your Task: Please find the names of two people with appropriate expertise that you are going to interview for this story and save just the pages or sources that describe their expertise and how to contact them.
INT is a Mixed Product, because defining expertise is intellectual, and contact information is a fact. It is at the Document Level, because expertise is determined by a whole page. The Goal Quality is Mixed, because determining expertise is amorphous but contact information is specific. It is Unnamed because the search targets are not specifically identified in the task.
Copy Editing (CPE)
Your assignment: You are a copy editor at a newspaper and you have only 20 minutes to check the accuracy of the three underlined statements in the excerpt of a piece of news story below.
Topic: New South Korean President Lee Myung-bak takes office
Body: Lee Myung-bak is the 10th man to serve as South Korea's president and the first to come from a business background. He won a landslide victory in last December's election. He pledged to make the economy his top priority during the campaign. Lee promised to achieve 7% annual economic growth, double the country's per capita income to US$4,000 over a decade and lift

206

the country to one of the topic seven economies in the world. Lee, 66, also called for a stronger alliance with top ally Washington and implored North Korea to forgo its nuclear ambitions and open up to the outside world, promising a better future for the impoverished nation. Lee said he would launch massive investment and aid projects in the North to increase its per capita income to US$3,000 within a decade "once North Korea abandons its nuclear program and chooses the path to openness."
Your Task: Please find and save an authoritative page that either confirms or disconfirms each statement.
CPE is a Factual Product, because facts have to be identified. It is at the Segment Level, because items within a document need to be found. It has the Specific Goal of confirming facts, and it is Named because the search targets are specified.
Advance Obituary (OBI)
Your assignment: Many newspapers commonly write obituaries of important people years in advance, before they die, and in this assignment, you are asked to write an advance obituary for a famous person. Your task: Please collect and save all the information you will need to write an advance obituary of the artist Trevor Malcolm Weeks.
OBI is a Factual Product, because facts about the person are needed. It is at the Document Level because entire documents need to be examined. It is Unnamed because the search targets are not specifically identified in the task. The Goal Quality is Amorphous because "all the information" is undefined.
Table 1 Variable facet values for the search tasks

Task Naming

Product

Level

Goal (quality)

BIC Unnamed CPE Named INT Unnamed OBI Unnamed

Mixed Factual Mixed Factual

Document Segment Document Document

Specific Specific Mixed Amorphous

3.3 Data Collection and Behavioral Measures
During the search, all of the participants' interactions with the computer system were logged during the searches on the client side: eye-tracking; mouse movement capture; clicks and URL requests; scrolling; keystrokes; queries and other behaviors. This logging required multiple systems, including Tobii eyetracking software, UsaProxy, the Morae screen-capture program [3], as well as our own software. From these logs, we extracted a variety of measures of the users' search behaviors on search result pages (SERPs) and content pages throughout the session, and during query intervals. A query interval is defined as the interval between two successive queries issued in one search session; and the last query interval is the period after issuing the last query till the completion of the task. The behavioral variables used in the work reported here are listed in Table 2. We classified these behaviors into two groups: behavioral measures on the clicked documents and behavioral measures during the query intervals.

Table 2 Behavioral measures considered for learning predictive models

Behavioral measures

Definition

Behavioral measures on clicked documents

dwell time

the dwell time on the document, from the point the page is opened till the point the
page is closed

number.of. mouseclick

the number of mouse clicks, including Left button down, Right button down, and
scrolling, while the page is opened

number.of. keystrokes

the number of keystrokes while the page is opened

visit_id

the number of times a content page has been visited during one search session

Behavioral measures during query intervals

time_to_first_click

the time before first click on pages after issuing a query

content_mean

the average dwell time on content pages during the query interval

content_sum

the total dwell time on content pages during the query interval

content_count

the total number of content pages visited during the query interval

serp_mean

the average dwell time on SERPs during the query interval

serp_sum

the total dwell time on SERPs during the query interval

serp_count

the total number of SERPs examined during the query interval

prop_content

the proportion of time on content pages of the total dwell time during the interval

interval_time

the total time in each query interval

diff_content

the difference between the dwell time on a content page and the average dwell time
on all content pages during the query interval

4. GENERATION OF PREDICTION MODELS
4.1 Building the Models
We used binary recursive partitioning analysis [5] to identify the most important predictors of useful pages. Recursive partitioning is a stochastic learning technique for non-parametric classification problems. It grows decision trees by examining all independent variables and splits a node using the variable that best distinguishes the remaining data in the collection. The tree is grown until all of the data has been assigned to a node. Recursive partitioning has an intuitive interpretation as a collection of rules to classify the dataset. An example (illustrative only) might be "The document is useful if dwell time > 20 seconds and the time to the first click < 34 seconds and there are fewer than 5 SERPs in the query interval". This method has not been used much in identifying predictors of document usefulness or user interests.

207

Recursive partitioning may be superior to logistic regression when the goal is to correctly classify members rather than optimize overall accuracy. Recursive partitioning can take account of the possible interactions between predictor variables natively in the algorithm.
We used recursive partitioning analysis to generate prediction models of document usefulness based on the potentially important behavioral variables listed in Table 2. Decision trees were generated using these predictors, and we identified the cutoff points based on the observed distribution of the variables in the dataset. To better learn the behavioral measures, we made a training collection that balanced the number of saved pages and not-saved pages by sampling the larger not-saved pages pool. In this way, ten balanced training sets were constructed, each sharing the same saved pages pool. The recursive partitioning method was applied to each sample to generate the prediction model. We then compared the variables and cutoff values in these generated models to identify the repeated variables and cutoff values for the decision-tree based prediction model.
4.2 General Prediction Models
We first built the general prediction model when considering all tasks in the experiment, without taking account of the task type. In this model, three behavioral measures were identified as the important factors: visit_id, dwell time and time to first click.

with the total number of saved pages, and then generated the prediction models using recursive partitioning. Because the CPE task and OBI task in our user experiment differ the most from one another as task types, we present the specific prediction models for CPE (Figure 2) and OBI (Figure 3).
These tree models show that dwell time on documents is still the most important predictor. However, the cutoff point for each task is very different, i.e. 30.65 seconds in CPE and 17.5 seconds in OBI. This result indicates that appropriate cutoff points are needed for different types of tasks, as White and Kelly [16] suggested. As for the general model, we suggest the median as the cutoff point for each task. The CPE model also identifies visit time of the page, number of mouse clicks on the page, and the proportion of time spent on content pages during the query interval (prop_content) as important variables. Among them, the number of mouse clicks was negatively related to the saved documents, and the prop_content was positively related to the saved documents. The OBI model looks very similar to the general model, except that it specifies a range for the variable time to first click, which is close to the first quarter to the median value for that variable in the dataset.

Figure 1 General prediction model
Among these three variables, the dwell time on documents is the most important variable in the general prediction model. Examining the distribution of dwell time in the dataset, we found that the model cutoff (17.45 seconds) is the third quarter of the dwell time in all the data. At the same time, we found users saved one quarter of all the visited pages in the assigned task, so it is reasonable to infer that the cutoff point for the dwell time variable depends upon the proportion of saved documents among all visited documents. If the relative frequency of saved documents is not clear in a new dataset, the median dwell time can be adopted as used in White and Kelly [17]. Not many documents were visited more than once by users in one search session, so the model specifies that if a page has been visited more than once, even though the dwell time is shorter than the median, the page is predicted to be useful. Finally, the cutoff time for time to first click is close to the first quarter of the distribution in the dataset. We will apply these cutoff points in our test dataset to evaluate the prediction model.
4.3 Specific Prediction Models
We also conducted 10 random sampling and recursive partitioning analyses on each of the individual tasks to see if tasks of different types would lead to different combinations of important predictors and prediction models. Following the procedure in section 4.1, for each task, we made balanced pools of saved and non-saved pages

Figure 2. Specific prediction model for CPE
Figure 3. Specific prediction model for OBI
5. IMPLEMENTATION OF PREDICTION MODELS
In order to evaluate the prediction models (both the general model and the specific models) generated from the user experiment, we applied our models to the data collected for the TREC 2011 Session Track. The Session Track [8] aims to provide test collections and evaluation measures for studying information retrieval based on previous user interactions during a search session, rather than one-time queries. The track used the ClueWeb09 collection (English only) 1 , and the topics were 1 lemurproject.org/clueweb09

208

defined in the usual TREC ad hoc sense, with a title, description, and narrative. Sessions of real user interactions were recorded, including users' queries, clicks on search result pages, and associated time stamps. There were search sessions for 76 topics in total.

5.1 Task Classification

Before implementing the prediction models, we first classified the tasks in the TREC 2011 Session Track using our modification of the Li & Belkin [12] task classification scheme. The 76 sessions were classified into 4 types of tasks according to the task facets. From Table 3 we can see that all of the 76 tasks required searching for factual information, so the product for them is Factual. In addition, 66 of the 76 sessions can be classified as searching for information on the Segment level, with Specific goal, with search targets Named (SSN). There are very few examples of three other types of tasks (i.e. SSU, DSN, DAN) in the data set. When we tried to match these task types with the task classification in our user experiment (Table 1), we found that the SSN type of task was mostly similar to CPE, and DSN and DAN types of tasks were very similar to OBI, so we used these specific models for those tasks. We applied the General model for the SSU task type.

Table 3 Task classification of the search tasks in TREC 2011 Session Track

Task type

SSN

SSU

DSN

DAN

Level

Segment Segment Document Document

Goal(quality)

Specific

Specific Specific Amorphous

Naming

Named Unnamed Named

Named

Product Model applied number of tasks

Factual CPE 66

Factual
General model
4

Factual OBI 4

Factual OBI 2

5.2 Prediction Models on the TREC Dataset
The general model for the TREC dataset was based on the general model we built from the user experiment and the threshold was determined based on the targeted dataset.
The general model (decision tree) is:
if (visit_id > 1),{then it is a useful page}; else if (dwell time > 28.55 seconds), {then it is a useful page}; else if (6.33 seconds < time-to-first-click <14.55 seconds), {then it is a useful page}; else {non-useful pages}.

The implementation of specific models in the TREC dataset is based on our task classification (Table 3). The CPE specific model is used for SSN type of task, the OBI specific model is used for DSN and DAN types of task, and the other task types use the general model. Since the interaction log for the TREC 2011 Session Track was collected on the server side, and did not include any user interactions on the documents (i.e. content pages), the variable number of mouse clicks was not included in the specific models..

The specific model (decision tree) is:
if (visit_id > 1), then useful pages if (task type = SSN)
{if (dwell time >=28.84 #Median dwell time for this type of task)
{then useful pages } else if (prop_content>=0.6 #Median for this type of task)
{then useful pages } else { then non-useful pages }}
else if (task type = SSU) {if (dwell time >=17 #Median dwell time for this
type of task) {then useful pages }
else if (task type = DSN or DAN, {if (dwell time >=32.82 #Median dwell time for
these types of task) {then useful pages}
else if (time_to_first_click > 6.33 & time_to_first_click < 14.55)
{then useful pages } else {non-useful pages }}}
5.3 Relevance Feedback Technique
For evaluation of the various prediction models in this paper, we use only one of the three types of query modification specified in the TREC 2011 Session Track, which uses the maximum amount of behavioral data obtained during the search session [8]. Since the Session Track's basic concept was to compare the search results of the final query in a search session using a baseline system with no behavioral data, to the results of a final query using such data, we used the results of our document usefulness prediction models to modify the last query but one (last -1) in the session, using both positive and (positive + negative) relevance feedback (RF). The TREC 2011 Session Track database, Clueweb09 English only [8] was the searched collection, and was the source for documents and terms for RF.
From the prediction of document usefulness for each of our models, we generated a pool of "relevant" documents that occurred during each search session, and calculated the term frequency for each term in the pool, and in the equivalent corpus of non-useful documents. The observed term frequency was then discounted by the prior of the expectation of appearance in a random document in the English language using the Brown corpus [15].
With respect to the number of useful and non-useful terms for query expansion, we used the approach described in Belkin et al. [2], in which a negative RF system was implemented. The number of suggested feedback terms was determined by the formula:
5n + 5, where n = number of judged documents to a maximum of 25 suggested terms.
The query was parsed as a weighted sum, using the default Lemur2 weighting for RF term addition for positive terms, and adding the negative terms under the InQuery "NOT" operator, with 0.6 weight.
2 lemurproject.org

209

Two relevance feedback methods were implemented:
Positive relevance feedback only: In the runs with positive RF only, the predicted "useful" documents were used to calculate the term frequency, the top 25 terms were selected to expand the last -1 query in the session.
Both positive and negative relevance feedback: In the runs with both positive and negative RF, the predicted "useful" documents were used to calculate the term frequency for "useful" terms and the top 15 terms were selected to be "useful" terms; the predicted "non-useful" documents were used to calculate the term frequency for "non-useful" terms and the top 10 terms were selected to be "non-useful" terms. We then combined the last-1 queries with the 15 "useful" terms (with positive weight 1.0), and the 10 "nonuseful" terms (with negative weight 0.6) using the Indri query language.
Therefore, we have four prediction models of document usefulness generated and evaluated in this study:
∑ General model with Positive RF only (GP) ∑ General model with Positive and Negative RF (GPN) ∑ Specific model with Positive RF only (SP) ∑ Specific model with Positive and Negative RF (SPN)
5.4 Baseline model
Our baseline model used Pseudo Relevance Feedback on the last queries issued by the users in each session. The default parameters in the Indri Retrieval System were used as follows:
Parameters for Pseudo Relevance Feedback:
int fbDocs = _param.get( "fbDocs" , 10 );
int fbTerms = _param.get( "fbTerms" , 10 );
double fbOrigWt = _param.get( "fbOrigWeight", 0.5 );
double mu = _param.get( "fbMu", 0 );
We compare the retrieval performance of our personalized models with the baseline system, to evaluate whether the personalized models based on users' interactions in the search session improve results over pseudo relevance feedback that does not take account of users' interaction behaviors.
6. EVALUATION OF PREDICTION MODELS
6.1 Prediction Accuracy of Document Usefulness
We evaluated the prediction accuracy of document relevance against the TREC assessors' relevance judgments [8]. The relevance judgments ranged from -2 to 3 (-2 stands for spam document, 0 stands for not relevant, 1 for relevant, 2 for highly relevant, and 3 means the document is a key to the information need). Since we used binary relevance ratings in our prediction models, we grouped TREC assessors' judgments into two groups: ratings that were -2 or 0 were grouped as "not-relevant", and ratings of 1 to 3 were grouped as "relevant".
Table 4 and Table 5 show the prediction accuracy of the general model and the specific models on the document relevance. They demonstrate that the general model achieved better prediction performance than the specific model for the TREC 2011 Session Track data.

Table 4 Prediction accuracy of the general model

TREC assessment

not- relevant relevant

total

General notmodel relevant

29 (53%)

45(Type I 74 error)

predicted relevant

26 (Type II 83 (65%) 109 error)

total

55

128

183

Overall accuracy

61%

Table 5 Prediction accuracy of the specific model

TREC assessment

not- relevant relevant total

Specific model predicted

not-

24 (44%)

relevant

55 (Type I 69 error)

relevant 31(Type II 73 (57%) 102 error)

total

55

128

183

Overall accuracy

53%

6.2 Retrieval Performance Comparison
6.2.1 Overall performance
We first calculated the mean of the retrieval performance over 76 sessions by each of the models (Table 6). Our results demonstrate that all of the four models improve over the baseline, on almost all the measures provided by TREC 2011 Session Track [8].

Table 6 Overall retrieval performance

ERR3

Baseline GPN GP SPN SP

0.20

0.28 0.29 0.27 0.28

ERR@10 nERR4

0.19

0.28 0.29 0.26 0.28

0.29

0.44 0.45 0.41 0.42

nERR@10

0.27

0.43 0.45 0.40 0.42

nDCG5

0.30

0.24 0.25 0.24 0.25

nDCG@10

0.20

0.34 0.35 0.32 0.33

AP6

0.09

0.09 0.09 0.08 0.09

GAP7

0.08

0.09 0.09 0.09 0.09

6.2.2 Retrieval performance by sessions
Expected Reciprocal Rank (ERR) [6] was chosen for the following analysis because the user model underlying the ERR measure matches the user model underlying our prediction models. When examining results by search sessions, we found, as usual,
3 ERR: Expected Reciprocal Rank; 4 nERR: ERR normalized by the maximum ERR per query; 5 nDCG: normalized discounted cumulative gain; 6 AP: Average Precision 7 GAP: Graded Average Precision;

210

that there is great variety in evaluation results for any single technique between sessions. For example, the GP model had better retrieval performance on ERR in 42 sessions, but had worse performance on ERR in 32 sessions.

Table 7 Retrieval performance change over baseline on ERR

All sessions

Run Measures

GP GPN SP SPN

Mean ERR

0.29 0.28 0.28 0.27

Percent improvement over baseline (ERR=0.20)

44% 38%

38% 32%

Number of sessions that improve the baseline

40 40

36 36

Number of session that decrease from the baseline

29 32 32 34

Number of session that did not change the baseline

7

4

8 6

6.2.3 The relationship between prediction accuracy and retrieval performance
Among the 76 sessions, there were 12 sessions in which users did not click on any documents. We first compare the retrieval performance and the change over baseline between the sessions that did not contain any clicked documents (N=12) with the sessions that contained at least one clicked document (N=64).
Table 8 Comparison between two types of sessions

Models

Sessions that do not contain clicked documents (N=12)
Mean (SD) of retrieval performance evaluated using ERR measure

GPN

0.08 (0.15)

GPN change over baseline -0.07 (0.3)

GP

0.15 (0.28)

GP change over baseline 0.01 (0.13)

SPN

0.08 (0.15)

SPN change over baseline -0.07 (0.3)

SP

0.15 (0.28)

SP change over baseline 0.01 (0.13)

Sessions that contain clicked documents (N=64)

MannWhitney test

Mean (SD) of p retrieval performance evaluated using ERR measure

0.32 (0.25) <0.05

0.11 (0.31)

<0.05

0.32 (0.25) <0.05

0.11 (0.31)

<0.05

0.31 (0.27) <0.05

0.09 (0.32)

<0.05

0.31 (0.27) <0.05

0.09 (0.32)

0.095

Table 8 shows that the retrieval performance of our four models and the improvement of our models over baseline (except SP) is significantly better in the sessions that contain clicked documents than in sessions that did not contain any clicked documents. This result shows that our models need clicked document data to improve search results.
Since we are using users' interactions during sessions to build the prediction models of document usefulness, in the following analysis, we present results for sessions that contain at least one clicked document (N=64) to evaluate the performance of our prediction models. We then examine the relationship between prediction accuracy and retrieval performance, by three types of change (decrease, no change, and increase) in retrieval performance compared with the baseline.
We compare the amount of user interaction in each session, and the prediction accuracy of document usefulness by the general model among the three groups (Table 9). The amount of user interactions is defined as the sum of the number of queries, the number of search result pages, and the number of content pages visited. Our results show that the prediction accuracy of document usefulness in the sessions for which use of the general model increased the retrieval performance over the baseline was as high as 75%. In contrast, the prediction accuracy was only 51% in the sessions that where personalization decreased the retrieval performance. The Kruskal Wallis test revealed that the differences in the overall accuracy was significant (p<.05). Neither the number of clicked documents nor the number of interactions was significantly different among three groups.
Table 9 Comparison of behaviors and prediction in the general model

Sessions that decrease from the baseline

Sessions that did not change the baseline

Sessions that increase from the baseline

KruskalWallis Test

Mean

Mean

Mean

p

Number of 2.87

2.33

2.84

0.82

clicked

documents

in the

session

Number of 12.57

9.67

interaction

s in the

session

12.39

0.70

Overall 51%

53%

75%

<.05

accuracy

of the

general

model

We also compared the amount of user interaction in each session, and the prediction accuracy of document usefulness by the specific model among the three groups (Table 10). As with the general model, we found that the sessions that increased retrieval performance over the baseline by the specific model had significantly higher prediction accuracy of document usefulness (64%) with the specific model as compared to the sessions that decreased the retrieval performance (34%). Again, neither the

211

number of clicked documents nor the number of interactions was significantly different among three groups.
Table 10 Comparison of behaviors and prediction in the specific model

Sessions that decrease from the baseline

Sessions that did not change the baseline

Sessions that increase from the baseline

KruskalWallis Test

Mean

Mean

Mean

p

Number of

clicked documents

2.4

2.4

3.21

0.70

in the

session

Number of

interaction 11.44

9.8

s in the

session

13.35

0.45

Overall

accuracy of the

0.34

0.41

0.64

.009

general

model

In general, when applying our prediction models on the TREC Session Track 2011 dataset, our results indicate that the better the prediction accuracy, the greater the improvement of retrieval performance over the baseline.

6.2.3.1 When to personalize?
Figure 4 and Figure 5 show the ERR values for each search session for the baseline and for each of our personalization methods. It is clear there are specific search sessions which will benefit from our personalization methods, and others for which such personalization is either not worth the effort, or harmful.

Figure 4 When to personalize using the general model (sessions ordered according to increasing positive difference between GPN and baseline)

Figure 5 When to personalize using the specific model (sessions ordered according to increasing positive difference between SPN and baseline)

The question is when should one apply personalized models? (cf. [16]) In practice we need to identify these sessions from the observable user interactions during the search session. To accomplish this we focus on interaction behaviors that are associated with sessions where our models were particularly good and particularly bad. Specifically, we look at the behaviors in sessions where our models improved retrieval performance by greater than 0.20 on ERR and for sessions where the models did worse than-0.05 on ERR. We compare the behavioral variables in the search sessions between the two extremes in retrieval performance defined by the GP model in Table 11. For the sessions in which retrieval performance benefited a lot from the personalization model, users spent significantly more dwell time on each content page (51.25 seconds) than in the other group of sessions where the performance was decreased (34.61 seconds). They also spent a relatively greater proportion of task time on reading content pages (43%) than in the other group of sessions (29%). These results, in particular the former, suggest that it is possible to identify search sessions which would benefit from personalization relatively early in the course of the search session.

Table 11 Comparison of behavioral measures between two groups defined by GP model

Behavioral measures

Decrease Benefit d group group (N=13) (N=24)

MannWhitne y test

Mean

Mean

p

Total time on task (seconds) 354.27 317.35

1

Total time on content pages 102.78 141.71

.19

(seconds)

Total time on SERPs

223.03 141.73

.18

Mean dwell time on each 34.61

51.25

<.05

content page (seconds)

Mean dwell time on each 37.43

24.00

.31

SERPs (seconds)

Number of query

4.23

3.96

.37

Time to first click (seconds) 33.47

22.44

.19

Query interval time 86.97

86.64

.52

(seconds)

Proportion of time spent on 29%

43%

<.05

content pages in the session

212

7. DISCUSSION
In this study, we generated document usefulness prediction models based on a laboratory experiment of the influence of different types of tasks on task session behaviors. We constructed a general prediction model and several specific prediction models tailored to different types of tasks. Recursive partitioning analysis was used to generate decision tree models based on users' interaction behaviors during search sessions. Our models identified three main behavioral measures as important predictors of document usefulness: dwell time on document, the number of times a page has been visited in the session (visit_id), and the time before first click after issuing a query (time to first click). Besides the positive relationship between dwell time and document usefulness, we found that, in all but one case, the visit_id and time_to_first_click to be positively related to document usefulness. The specific models for different types of tasks show major differences by task type for both the predictors and the rules. For example, in the specific prediction model for CPE, number of mouse clicks on the document was negatively related to document usefulness, and the proportion of time spent on content pages during query interval (prop_content) was positively related to document usefulness. However, neither of these two measures and associated rules was useful in the specific model for OBI. Furthermore, although all models included dwell time as a significant factor, cut-off points for dwell time to predict usefulness differed between the different tasks, and from that of the general model.
A primary focus of this study was to evaluate whether the prediction models generated from our user experiment could improve retrieval performance, so we applied our models to the TREC 2011 Session Track data. The comparison of the prediction by the different models against TREC assessors' relevance judgments showed the general model had higher accuracy than the specific models. One reason for this is that there is a great imbalance in task type in the TREC 2011 Session Track data. The vast majority of the tasks were of a task type similar to the CPE task in our experiment. Our specific model for CPE contains number of mouse clicks on the documents as one of the predictors, but such data was not available in the server-side logs in the TREC 2011 Session Track. The task type imbalance also meant that not enough data was available (10 sessions in total) for three of the four types of tasks (SSU, DSN, DAN). Consequently, one can expect the specific models will fail when they try to adapt to the individual task types. In addition, other facets of the task types may present in the TREC Session Track that differ from those in the user experiment in our lab. Such facets could influence users' behaviors in different ways. For example, users were given 20 minutes for searching in our experiment, whereas in the crowdsourced search sessions in the TREC Session Track, users were given only 5 minutes for searching. Users were likely to have searched with an awareness of time-pressure which might not have been the case during our experiment.
With respect to retrieval performance, all of our models improved a great deal from the baseline which used pseudo relevance feedback on the last queries users issued in each session. Our general model had somewhat better retrieval performance than the specific model on both of the two relevance feedback methods. The models with positive relevance feedback performed only slightly better than the models with both positive and negative relevance feedback. This suggests that using negative IRF may not be worth the effort, although perhaps the limited size of the data set for the TREC Session Track mitigates this conclusion. In addition, we found that the better the prediction accuracy of

document usefulness, the more likely that the model could improve retrieval performance.
Teevan, Dumais & Horvitz [16] examined users' clicks and explicit judgments for the same queries and found they differed greatly from one another. They proposed that the potential for personalization could be defined by the gap between how well search engines could perform if they were to tailor results to the individual, and how well they currently perform by returning results designed to satisfy everyone. In our study, we also addressed the question of when to apply personalization models by presenting the retrieval difference between our models and the baseline. We found our prediction models consistently improved retrieval performance over the baseline in some sessions, but consistently decreased the performance of the baseline line in some other sessions. We compared some users' behavioral measures during these sessions, and found significant differences between the two extreme groups. In particular, users spent significantly more dwell time on each content page and a greater proportion of task time on content pages for the sessions where the personalization models we applied were beneficial. Dwell time on content pages has been proven to be positively related to document relevance, and one explanation for our results is that users were visiting more useful documents in the sessions where the models performed well and that query expansion based on the useful documents helped users to articulate their information needs. Therefore, IRF based on the predicted useful documents achieved better results than using the top n returned documents, as in pseudo relevance feedback. Further work is needed to investigate when the system should apply personalization models to improve retrieval performance.
Because the data that we used for our personalization models should be applicable to any general retrieval system, one might expect that our levels of improvement would be applicable to techniques with much higher baseline performance, resulting in higher absolute performance levels. It is also the case that our usefulness prediction models were used as input to quite standard, and rather simple relevance feedback techniques. More sophisticated use of the models could result in better overall performance improvement.
8. CONCLUSION
In this study, we used searchers' interaction behavioral measures derived from a laboratory experiment to build prediction models of document usefulness. We then evaluated the prediction accuracy and retrieval performance of these models by applying them to the TREC 2011 Session Track data set, where users search interactions were generated in radically different search sessions. We found several behavioral measures, e.g. dwell time, visit time, time to first click, proportion of time on content pages, and others could be potential predictors of document usefulness. We also found different combinations of variables and rules of prediction models based on these measures for different types of tasks. The results demonstrate that the prediction models we generated from our user experiment improve the performance over a baseline that did not take account of user interaction information in search sessions with quite different characteristics than those from which the prediction models were developed. The positive "transfer" effect leads us to believe the models we have developed may be used for personalization of retrieval in a variety of searching circumstances, and that we could expect even greater performance benefit when richer, client-side data that our prediction models depend upon are applied.

213

ACKNOWLEDGMENTS
The research that led to this work was funded by the IMLS, under grant number LG-06-07-0105-07. We thank all of the members of the Personalization of the Digital Library Experience (PoODLE) research team at Rutgers University, without whose efforts this work could not have been accomplished. Thanks to Si Sun for assistance in TREC 2011 Session Track. Thanks to Jingjing Liu for validating task classification as the third coder. Thanks to David Pane at CMU, who helped us greatly and generously in performing the Indri runs.
REFERENCES
[1] Agichtein, E., Brill, E., Dumais, S., & Ragno, R. (2006). Learning user interaction models for predicting web search result preferences. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. (pp. 3-10). Seattle, Washington, USA.
[2] Belkin, N. J., Carballo, J. P., Cool, C., Lin, S., Park, S. Y., Rieh, S. Y., et al. (1998). Rutgers' TREC-6 interactive track experience. Proceedings of the Sixth Text REtrieval Conference, 597-610.
[3] Bierig, R., Cole., M.J., & Gwizdka, J. (2009). A user centered experiment and logging framework for interactive information retrieval. In N.J. Belkin, R. Bierig, G. Buscher, L. van Elst, J. Gwizdka, J. Jose, et al. (Eds), CEUR Workshop Proceedings: 512. Proceedings of the SIGIR 2009 Workshop on Understanding the User: Logging and interpreting user interactions in information search and retrieval, UIIR'2009 (pp. 8-11). Aachen, Germany: CEUR Workshop Proceedings.
[4] Borlund, P. (2003). The IIR evaluation model: A framework for evaluation of interactive information retrieval systems. Information Research, 8(3), paper no. 152. Retrieved from http://informationr.net/ir/8-3/paper152.html.
[5] Breiman, L. (1984). Classification and Regression Trees. Boca Raton: Chapman & Hall/CRC.
[6] Chapelle, O., Metzler, D., Zhang, Y., and Grinspan, P. (2009). Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM), 2009, 621-630.
[7] Fox, S., Karnawat, K., Mydland, M., Dumais, S., & White, T. (2005). Evaluating implicit measures to improve web search. ACM Transactions on Information Systems, 23(2), 147-168.
[8] Kanoulas, E., Hall, M. Clough, P. Carterette, B., & Sanderson, M. (2012) Overview of the TREC Session Track. In: The Twentieth Text REtrieval Conference Proceedings (TREC 2011). Gaithersburg, MD: National Institute of Standards and Technology. Retrieved on 20 May 2012 at http://trec.nist.gov/pubs/trec20/t20.proceedings.html.

[9] Kelly, D. (2005). Implicit feedback: Using behavior to infer relevance. In A. Spink and C. Cole (Eds.) New Directions in Cognitive Information Retrieval (pp.169-186). Netherlands: Springer Publishing.
[10] Kelly, D. & Belkin, N.J. (2004). Display time as implicit feedback: Understanding task effects. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR '04). ACM, New York, NY, USA, 377-384. DOI= http://doi.acm.org/10.1145/1008992.1009057
[11] Kelly, D. & Teevan, J. (2003). Implicit feedback for inferring user preference: A bibliography. SIGIR Forum, 37(2), 18-28.
[12] Li, Y. & Belkin, N.J. (2008). A faceted approach to conceptualizing tasks in information seeking. Inf. Process. Manage. 44, 6 (November 2008), 1822-1837. DOI=10.1016/j.ipm.2008.07.005 http://dx.doi.org/10.1016/j.ipm.2008.07.005
[13] Liu, C., Gwizdka, J., & Liu, J. (2010). Helping identify when users find useful documents: examination of query reformulation intervals. In Proceeding of the third symposium on Information interaction in context (IIiX '10). ACM, New York, NY, USA, 215-224. DOI= http://doi.acm.org/10.1145/1840784.1840816
[14] Liu, J. & Belkin, N.J. (2010). Personalizing information retrieval for multi-session tasks: The roles of task stage and task type. In Proceedings of the 33rd Annual International ACM SIGIR Conference on Research & Development on Information Retrieval (SIGIR '10). Geneva, Switzland, July 19-23, 2010.
[15] Loper, E. and Bird, S. (2002). NLTK: The Natural Language Toolkit. Proceedings of the ACL02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics, Volume 1, July, 2002 Association for Computational Linguistics.
[16] Teevan, J., Dumais, S.T., and Horvitz, E. (2010). Potential for personalization. ACM Trans. Comput.-Hum. Interact. 17, 1, 1-31.
[17] White, R. W. & Kelly, D. (2006). A study on the effects of personalization and task information on implicit feedback performance. In Proceedings of the 15th ACM international conference on Information and knowledge management (pp. 297-306). Arlington, Virginia, USA.
[18] White, R.W., Ruthven, I., and Jose, J.M. (2005). A study of factors affecting the utility of implicit relevance feedback. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR '05). ACM, New York, NY, USA, 35-42.

214

User Evaluation of Query Quality

Wan-Ching Wu
SILS University of North Carolina Chapel Hill, NC, 27599 USA

Diane Kelly
SILS University of North Carolina Chapel Hill, NC, 27599 USA

Kun Huang
School of Management Beijing Normal University Beijing, China, 100875

wanchinw@email.unc.edu

dianek@email.unc.edu

huangkun@bnu.edu.cn

ABSTRACT
Although a great deal of research has been conducted about automatic techniques for determining query quality, there have been relatively few studies about how people judge query quality. This study investigated this topic through a laboratory experiment with 40 subjects. Subjects were shown eight information problems (five fact-finding and three exploratory) and asked to evaluate queries for these problems according to several quality attributes. Subjects then evaluated search engine results pages (SERPs) for each query, which were manipulated to exhibit different levels of performance. Following this, subjects reevaluated the queries, were interviewed about their evaluation approaches and repeated the rating procedure for two information problems. Results showed that for fact-finding information problems, longer queries received higher ratings (both initial and post-SERP), and that post-SERP query ratings were more affected by the proportion of relevant documents viewed to all documents viewed rather than the ranks of the relevant documents. For exploratory information problems, subjects' ratings were highly correlated with the number of relevant documents in the SERP as well as the proportion of relevant documents viewed. Subjects adopted several approaches when evaluating query quality, which led to different quality ratings. Finally, during the reliability check subjects' initial evaluations were fairly stable, but their post-SERP evaluations significantly increased.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Information Search and Retrieval - query formulation, search process.
General Terms
Experimentation, Human Factors
Keywords
Query quality, query recommendation, query evaluation
1. INTRODUCTION
Query performance prediction (QPP) is the task of estimating the expected quality of search results for a query in the absence of relevance feedback [4, 8]. The basic goal is to predict when a query will perform poorly so that some intervention can occur before results are returned. For example, additional information
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08...$10.00.

might be elicited from the user or term expansion might be used to enhance the query. QPP approaches are classified into two types: pre-retrieval and post-retrieval [4, 8]. Pre-retrieval approaches estimate query performance based on features of the query while post-retrieval approaches consider the results retrieved by the query. Pre-retrieval approaches are further subdivided into those that exploit the linguistic structure of the query, including the morphological, syntactical and semantic properties of the query, and those that use term statistics, including specificity, similarity, coherency and relatedness. Post-retrieval approaches include measures such as clarity and robustness, and score analysis.
Although a great deal of research has been conducted about QPP, there have been relatively few studies about the relationship among QPPs and users' evaluations of query difficulty. Hauff et al. [10] note "while most QPP methods have been motivated and developed based on how a user might rate a query, these intuitions have never been empirically validated" (pg. 980). To address this limitation, Hauff et al. [9, 10] compared the query performance ratings made by humans with performance scores estimated by a suite of QPP methods. Results showed that user ratings and QPPs were mostly uncorrelated, suggesting that QPP methods are not representative of how users evaluate query quality. Lioma et al. [12] found that users could not reliably identify pre-determined query difficulty ratings associated with a set of 420 queries, but were able to identify some features that would make a query difficult for a search system.
While these previous studies provide some insight about the relationship among QPPs and users' evaluations of query difficulty, they do not reveal insight about how people actually judge query quality. In one of the studies reported by Hauff et al. [10], assessors were provided with queries and information need descriptions and asked to judge the queries based on what they expected the results to be if they submitted the queries to a Web search engine. Assessors made their judgments using a 5-point scale, where 1=poor quality query and 5=high quality query. The researchers did not report assessors' experiences using this scale to evaluate query quality, although it was noted that their ratings varied considerably. Lioma et al. [12] asked assessors to rate queries using three categories (easy, medium, hard). In both studies, assessors evaluated queries without inspecting results. Neither study probed people about how they judged query quality.
People rate a variety of objects in daily life (e.g., movies, restaurants, books), but it is unlikely that many people have rated queries. How would people approach this task? What factors would they consider when evaluating query quality? How would they make decisions about which numeric ratings to assign to which queries? In this paper we explore these questions. We are not concerned with the relationship between QPPs and people's evaluations of query quality, but instead seek to address more fundamental questions about how people make evaluations of query quality. Specifically, our research questions are (RQ1)

215

How do people make judgments about query quality? (RQ2) How are people's judgments related to features of the query, information problem and search results? (RQ3) How reliable are people's judgments?
Understanding how people evaluate query quality is important for several reasons. A better understanding of how people evaluate query quality might provide data on which to model future QPPs, or it might help researchers better understand the differences between automatic QPPs and human QPP. Although QPPs leverage collection-based statistics and should not necessarily be correlated with users' query evaluations, understanding how people view query quality might be helpful in modeling computer-human interaction regarding QPP. Understanding how people evaluate query quality is also important in the context of another popular IR technique: query suggestion. Traditionally, query suggestions are presented to users without any information about their potential goodness or quality. However, it might be useful to allow users to provide query recommendations to others, that is, query suggestions that have rating information associated with them in a manner similar to that provided by recommendation services such as Amazon and Netflix. It might also be useful for an information system to elicit query ratings from users so that it can make more personalized recommendations in the future. While suggestion and recommendation are often used as synonyms in the literature, in this study we distinguish between these two terms to demarcate a difference between the provision of unrated and rated suggestions, respectively. We pose one additional question to better understand what people think of the idea of query recommendation: (RQ4) What are people's perceptions of query recommendations? This is a natural question to ask in the context of an experiment that focuses on how people evaluate query quality, since this would presumably underlie query recommendation.
2. BACKGROUND
This study is related to three major areas of research: query performance predictors (QPPs), query suggestion, and online recommendations. Because this paper is not focused on automatic QPPs, this research is not reviewed (see [4] for an overview).
In a series of studies, Hauff and colleagues [9, 10] explored the relationship between QPPs and users' evaluations of query quality. This work was motivated by the observation that underlying most QPP methods are assumptions about how users evaluate query quality, despite a lack of research about how users make decisions about queries and suggestions. In a series of studies, Hauff and colleagues found few strong correlations between assessors' query quality evaluations and QPPs. The researchers first collected pre-retrieval ratings from 18 assessors for a set of 50 topics. Assessors were shown information need descriptions and queries and asked to indicate their expectations of the quality of the search results.
Hauff et al. [9, 10] found great variability is assessors' query ratings, and few direct correlations between these ratings and a set of QPP pre-retrieval measures. Hauff et al. also compared preretrieval and post-retrieval QPP measures with assessors' pre- and post-retrieval query quality ratings of a set of query suggestions. While assessors could distinguish between high and low quality suggestions, their ratings were uncorrelated with the automatic QPP measures. Across all experiments, the QPP measure that was most correlated with assessors' ratings was the pre-retrieval predictor SumSCQ which assigns higher quality scores to more specific queries. Typically queries that contain more terms are

associated with a higher SumSCQ; this suggests that people's preretrieval query quality judgments might be influenced by query length. Similarly, Lioma et al [12] found that people were fairly good at identifying features that were correlated with query difficulty, including queries that were "too vague" or "too short," but were unable to reliably assess query difficulty using a rating scale of easy, medium and hard.
In addition to these studies, two studies have examined the relationship between QPPs and user performance [18, 19]. While these studies focused on different research questions than the ones on which we focus, their results showed that user performance was unrelated to several QPP measures. Turpin and Hersh [18] found that the clarity scores of queries created by their users were much lower on average than those reported in system-based QPP evaluations where queries were automatically created. Turpin and Hersh further found that there was no correlation between the clarity of a user's query (a post-retrieval QPP) and the user's actual performance on the search task. Zhao and Scholer [19] examined nine pre-retrieval QPPs and found that they were not useful predictors of when users experienced search difficulties.
A portion of Hauff et al.'s [10] work was a secondary analysis of data collected in a study investigating the extent to which users could be induced to take bad query suggestions because they believed many others had taken the suggestions [11]. Kelly et al. [11] did not find an effect for "query popularity," but did find that users selected significantly more high quality than low quality query suggestions. Although this research was not conducted in the context of QPP, it does provide some indirect evidence that people make predictive judgments about query quality when selecting among a set of suggestions and that these judgments are often accurate. Like QPP research, query suggestion research is an area that might benefit from an increased understanding of how people evaluate query quality.
Increasing amounts of research have been published about query suggestion within the last ten years [15] and query suggestion is now a common feature of many information search systems. When users issue queries to most major commercial search services, they receive search results and query suggestions. However, they do not receive information about how the query suggestions were selected. While these query suggestions are likely identified through a combination of techniques based on aggregated query log data, details about the usage and frequency of various queries are not displayed to users. In social and collaborative search systems, query suggestions come from fellow users who belong to a community or specialized group with a common set of information needs [1, 16, 17]. These suggestions are also free-standing in the sense that users only know that they have been suggested or used, but nothing about their potential quality or usefulness. Thus, it seems that an important opportunity might exist in allowing users to associate ratings with query suggestions. In recent work, Baraglia et al. [2] proposed the idea of machine-rated query suggestions where queries would be rated with a 10-point scale (10=positive, 5=neutral and 1=negative), but their research was focused on algorithms for selecting and rating suggestions, rather than user ratings or interpretations of ratings.
In this paper we distinguish between query suggestions and query recommendations. The key difference between the two is the presence (recommendation) or absence (suggestion) of rating information. While suggestions can be generated by both systems and humans, ratings are, in most cases, associated with humans. The basic act of associating ratings with items is one with which most people are familiar. However, rating behavior, in general, is

216

not a well-understood topic, even in the recommender system literature [7]. A recent call for a special issue of ACM Transactions on Interactive Intelligent Systems (TiiS) states that little research has focused on the decision-making processes of users. Instead, research has focused on algorithms for identifying recommendations and eliciting and modeling users' preferences1.
Rating behavior has been studied more extensively in the consumer behavior literature. This research has primarily focused on consumer bias, cultural differences, brand loyalty and receiver experience and expertise in relation to commercial products [c.f., 6, 14]. A typical way these studies conceptualize objects that are rated, and people's subsequent use of ratings for making decisions about these objects, is by classifying them into search and experience goods [13]. Search goods are goods that are characterized by product attributes for which full information can be acquired before purchase. Search goods (e.g., dishwasher) are typically evaluated using objective criteria (e.g., capacity, noise level). Institutional-based ratings (e.g., Consumer Reports) often guide purchasing behavior. Experience goods (e.g., wine, movies, and books) are goods whose ratings are dominated by subjective attributes. For these types of goods, user ratings (instead of institutional ratings) often guide purchasing behavior.
We submit that query recommendations have more in common with experience goods. As with most experience goods, users need to experience the item before rating it; we are not suggesting that people blindly rate other people's queries, but rather rate their own queries after they have finished searching. It is an open question as to how people would approach this task and if they would even find recommendations useful. Furthermore, previous research has found the stability of people's ratings of experience goods somewhat brittle and influenced by the ratings of others [20]; thus, another open question concerns the stability of people's ratings.
3. METHOD
A laboratory experiment was conducted using a classic pretest/post-test design. The pre-test allowed us to observe how subjects would rate query quality before viewing search results (pre-retrieval), while the post-test allowed us to observe if and how these evaluations changed as a result of viewing search results (post-retrieval). We manipulated search result quality to determine if specific performance levels could be mapped to specific query quality scores.
Subjects were instructed that they would be shown other people's information problems and queries and then provide evaluations of query quality. Subjects completed three major steps: (1) initial query evaluation where they were shown an information problem description and query and asked to evaluate the query according to several attributes; (2) search result evaluation where they were shown a list of 10 search results and asked to evaluate these results; and (3) post-SERP query evaluation where they were once again presented with the information problem description and query and asked to re-evaluate the query. These steps were completed for eight information problems. After the evaluation, subjects were interviewed about the strategies they used to evaluate queries and their opinions about the usefulness of query recommendations. Following this, subjects were asked to repeat the rating procedure for two information problems/queries they
1 http://tiis.acm.org/pdf/human-decision-making-and-recommdersystems.pdf

had previously seen. This functioned as a reliability check to test the stability of subjects' query quality evaluations.

3.1 Information Problems and Queries
Eight information problems were used in this study. Five of the information problems were fact-finding information problems (FF) and were based on the navigational tasks used in [3]. Three were exploratory information problems (EX) and were created based on topics used in the TREC Aquaint collection (although results were from the Web, not the Aquaint corpus). Each information problem was associated with one query. Query length for FF information problems ranged from one to five words, while for EX information problems, it remained fixed at three words.

Subjects completed the FF and EX problems in blocks (all the FF problems were in one block and all the EX problems were in another). Subjects were randomly assigned to receive either the FF block or EX block first. Within each block, information problems were rotated using a Latin-square. Examples of information problems and queries are shown in Table 1. The full set of queries is shown in Table 2. The full set of information problems and queries can be viewed online at (http://ils.unc.edu/sigir2012queryrating/).

Table 1. Examples of information problems and queries.

Information Problem Description Query

Bob is interested in researching the history of motorcycles. To start his research, he decides to find out when each major motorcycle company was founded. Specifically, he wants to determine the year in which Harley Davidson motorcycles was founded. Janet is planning a boat convention for her company which will be held next year in Las Vegas and needs to select a hotel. She has heard positive things about the Bellagio hotel, but first wants to find out how many guest rooms are available.

harley davidson
bellagio hotel las vegas rooms

Task Type FF
FF

Carol is planning to fly to Amsterdam airport body EX next month and would like to learn scanners more about the body scanners that are being used in many airports as part of routine security procedures. Specifically, she is interested in gathering a range of information and opinions about these scanners and any privacy or health issues related to their use.

Table 2. Information problem IDs, queries and length.

Information Problem IDs FF1 FF2 FF3 FF4 FF5 EX1 EX2 EX3

Query
billiards harley davidson bank savings rates statue of liberty spikes bellagio hotel las vegas rooms black bear attacks modern day piracy airport body scanners

Length
1 2 3 4 5 3 3 3

217

3.2 Initial Query Evaluation
During the initial query evaluation, subjects were presented with an information problem and query along with four questions about query quality (Table 3). The query quality questions assessed the subject's beliefs about the representativeness of the query, the likelihood it would retrieve useful results, the extent to which the subject would recommend the query to others, and the subject's star-rating of the query. The first three items were measured using a 5≠point scale and the star-rating scale ranged from 1 star to 5 stars. This latter type of rating was selected because of its ubiquity in online computing environments.
Table 3. Initial query evaluation items.

Item How well do you think the query represents the information problem? How useful do you expect the search to be? How likely would you be to recommend this query to someone else searching for the same information problem? Please select the star rating that best reflects your opinion of the potential quality of the query.

Scale
1=not at all; 3=somewhat; 5=very well
1=not at all; 3=somewhat; 5=very useful
1=not at all; 3=somewhat; 5=very likely

3.2 Search Result Evaluation
After subjects completed the initial query evaluation, they were directed to a search engine results page (SERP) containing a list of ten search results. We collected 10 search results for each information problem using the Google search engine. Search results were carefully screened until the relevance of each search result was agreed upon by two authors of the study. On the SERP, each result was represented by a title, snippet, and URL (Figure 1). To enhance the realism of the search result environment, the SERP was deliberately designed to mimic Google's SERP. Subjects were debriefed at the end of the experiment about this manipulation. We modified snippets so that they did not contain answers to the information problems. The information problem was visible at the bottom of the page (omitted from screen shot to conserve space).

Figure 1. Artificial SERP constructed for study.
When presenting search results of FF information problems, subjects were randomly assigned to a high or a low condition, measured by Reciprocal Rank (RR) ≠ the inverse of the rank of the first relevant search result in a list. For each FF information problem, only one relevant search result appeared in the search

results list. For subjects assigned to the high condition, the relevant search result appeared at positions 1 through 5 for FF1FF5 and for subjects in the low condition the relevant search result appeared at positions 6 through 10. The performance conditions can be viewed in Table 4. Each FF information problem was associated with two performance conditions.

Table 4. Performance conditions for FF problems.

Information Reciprocal Rank Reciprocal Rank

Problem

(High Condition) (Low Condition)

FF1

1.000

0.167

FF2

0.500

0.143

FF3

0.330

0.125

FF4

0.250

0.111

FF5

0.200

0.100

For each EX information problem, subjects were randomly

assigned to one of three performance conditions: low, medium, or

high. The conditions varied in terms of both number of relevant

results presented and the positions of the relevant results, resulting

in variations in Normalized Discounted Cumulated Gain (nDCG)

from 0.31 to 1. The three conditions for any single EX

information problem only differed in terms of where relevant

search results were ranked, but different EX information problems

differed in the number of relevant results shown on a result list.

The performance conditions are shown in Table 5.

Table 5. Performance conditions for EX problems.

Information Problem EX1
EX2
EX3

Number Relevant
2
4
6

Performance Condition
Low Medium High Low Medium High Low Medium High

nDCG
0.31 0.51 1.00 0.42 0.59 0.80 0.65 0.73 0.91

For each search result viewed, subjects were asked two questions: (1) Does the webpage contain the exact information needed? (Yes, No) and (2) Do you think the webpage was useful to the person who typed the query? (1=not at all; 3=somewhat; 5=very useful). Only when both questions were answered could they return to the SERP. Subjects were not required to examine all 10 results and could examine the results in any order. Once they felt they had gathered enough information from the result list to evaluate the query, they proceeded to the post-SERP query evaluation.
3.3 Post-SERP Query Evaluation
During the post-SERP query evaluation, subjects were shown the information problem and query again, and asked the following: (1) How likely would you be to recommend this query to someone else searching for the same information problem? (1=not at all; 3=somewhat; 5=very likely) and (2) Based on the documents you've examined on the search result list, please select the star rating that best reflects your opinion of the actual quality of the query (subjects were presented with the 5-star rating widget). Both of these questions were from the initial query evaluation and functioned as post-test questions. While the first question was identical to one of the initial query evaluation questions, the second contained slight word changes to indicate that subjects should consider their experiences evaluating search results.

218

3.4 Exit Interview
After subjects completed the evaluations, semi-structured interviews were conducted to obtain qualitative data about the strategies subjects took to evaluate queries and their perceptions of the usefulness of query recommendations. Interviews were tape-recorded and the interview scheme was composed of four parts: questions regarding how subjects judged the quality of queries on their own, questions regarding how search results affected subjects' judgments of query quality, questions regarding evaluation strategies used for different information problem types and questions regarding subjects' opinions of query recommendations in the context of online searching.
3.5 Reliability Check
After the interviews, subjects were asked to re-evaluate the last information problem they were given from the first task block and the first information problem from the second task block. For each problem, subjects repeated the initial query evaluation, search results evaluation and post-SERP query evaluation. Subjects did not have access to their previous ratings.
3.6 Subjects
Subjects were recruited through the staff mailing list at our university. Forty-one subjects participated. From the interview session it was found that one subject misunderstood the study instructions and was therefore excluded from analysis.
Before starting the query evaluations, subjects completed a Demographic Questionnaire. The vast majority of respondents were females (70.73%), 26.83% were males, and 2.44% did not answer this question. Their ages ranged from 18-66 years old (M=39.6, SD=14.0). With respect to the status of participants, 25 (62.5%) were university staff; nine (22.5%) had both student and staff status; two (5%) were students and three (7.5%) were neither university staff nor student, and the status of one subject was unknown. For subjects who were university staff or full-time professionals (37, 92.5%), their occupations ranged from research assistant, project manager/director, programmers/analyst, administrative assistant or manager, financial counselor, business manager, lecturer, editor, teacher, to masseuse.
Subjects' search experience was measured with the Search SelfEfficacy Scale [5]. The Search Self-Efficacy Scale is a 14-item scale used to characterize search expertise. Subjects indicate their confidence in completing a series of activities using a 10 point scale where 1=totally unconfident and 10=totally confident. Subjects scored an average of 7.38 (SD=1.42) on the Search SelfEfficacy Scale, indicating moderate to high search experience. Because we slightly modified the wording of some items from the original scale to make them more contemporary, Cronbach's alpha was computed using subjects' responses to these items. This was found to be 0.947, demonstrating high reliability.
4. RESULTS
The research questions we addressed were (RQ1) How do people make judgments about query quality? (RQ2) How are people's judgments related to features of the query, information problem and search results? (RQ3) How reliable are people's judgments? And (RQ4) What are people's perceptions of query recommendations?
4.1 Initial Query Evaluation
In this section, we investigate how subjects' initial evaluations varied according to information problem type and query length

(RQ2). Pearson product-moment correlation coefficients were first computed to assess the relationships among the four initial query evaluation items. Results showed that there was a high correlation among subjects' responses to the items (Table 6).
Table 6. Correlations among initial query evaluation items.

Rep Useful Rec Rating

Representativeness -

.869** .865** .843**

Useful

.869**

-

.866** .859**

Recommendation .865** .866**

-

.885**

Rating

.843** .859** .885**

-

**Correlation is significant at the 0.01 level (2-tailed).

Figures 2 and 3 show subjects' initial query quality ratings for the fact-finding (FF) and exploratory (EX) information problems, respectively. The two figures show that while the initial query evaluations across the four items were virtually identical for different EX information problems, these values increased for FF information problems as query length increased from one word to four words. The figures also show that subjects were generally more conservative with their recommendation and star-ratings judgments. It was found that for FF information problems there was a significant difference in subjects' judgments of representativeness (F(4,195)=23.21, p<.001), usefulness (F(4,195)=22.34, p<.001), recommendation (F(4,195)=30.11, p<.001) and star-rating (F(4,195)=22.41, p<.001). Post-hoc tests indicate that the ratings given to FF1 were significantly lower than those given to FF2, FF3, FF4 and FF5; that the ratings given to FF2 were significantly lower than those given to FF4 and FF5; and the ratings given to FF3 were significantly lower than those given to FF4 and FF5. There were no significant differences in subjects' ratings of FF4 and FF5. For EX information problems, there were no significant differences for any of the initial query ratings (representativeness: F(2, 78)=.27, p=.762; usefulness: F(2, 78)=.10, p=.907; recommendation: F(2,78)=.16, p=.852; starrating: F(2, 78)=.04, p=.959)

Figure 2. Initial evaluations of FF information problem queries (error bars represent +/-1 standard deviation).

219

Figure 3. Initial evaluations of EX information problem queries (error bars represent +/-1 standard deviation).
Since all EX information problems were represented by threeword queries, EX information problems were compared to FF3 to examine whether there was a difference in how people rated queries according to information problem type. It was found that overall subjects rated the three-term EX queries higher on all four initial query evaluation items than the three-term FF query. Subjects found EX three-term queries to be more representative (EX: M=3.41, SD=.65; FF3: M=2.93, SD=.92) and the searches to be more useful (EX: M=3.35, SD=.64; FF3: M=2.95, SD=.81); subjects were more likely to recommend three-term EX queries to other people (EX: M=3.06, SD=.76; FF3: M=2.63, SD=1.13) and also assigned more stars to these queries (EX: M=3.03, SD=.70; FF3: M=2.50, SD=.99). Paired-samples t-tests were conducted to compare the means between EX and FF3 on all four items and the results show that three-term queries for EX information problems were rated as significantly more representative (t(39)=-2.65, p=.012), more useful (t(39)=-2.53, p=.015) and received significantly more stars than FF3 (t(39)=-2.85, p=.007).
4.2 Search Result Evaluation
RQ2 also asked about how people's judgments are affected by the quality of the search results. Before we examine this, we first examine the extent to which our performance manipulations worked. During the search result evaluation stage a total of 1479 result clicks were made, which represented 46.22% of the total search results shown to subjects. Figure 4 shows the percentage of times subjects' explicit relevance judgments corresponded with the judgments made by the researchers and the click rate for each information problem. Click rate is defined as the percentage of web results clicked from a ten-result SERP. If a web result prejudged by the researchers as relevant was clicked and also judged by a subject as relevant, it was counted as an instance of correspondence. Note that we are not measuring inter-rater reliability (as is common in the IR literature), but rather whether our experimental manipulations worked. This is why we use the term correspondence and report percent agreement. The figure shows that all of the information problems had an 80% or above correspondence rate except for FF3, which had only a 65% correspondence rate. FF3 also received the highest click rate among a range of click rates from 0.40 to 0.57. Even though the information problem had a well-defined goal (locate a bank with a particular savings rate), subjects might have had a difficult time processing the results and understanding if results were relevant.
Figures 5 and 6 compare subjects' experienced performance (EP) (according to their clicks and explicit relevance judgments) with the manipulated performance, or the performance intended by the researchers (IP). EP for FF information problems, measured by

RR, was computed by taking the inverse of the rank of the first clicked result. The greatest disagreement occurred for FF3, which was expected given the results in Figure 4. EP for EX information problems was measured by nDCG of the ten returned results; unclicked results were scored as irrelevant. Despite the disagreement between EP and IP for EX information problems, subjects experienced the intended relative performance conditions except for EX3. For this topic, there were 6 relevant documents in the list and for the high condition this meant that most of these were concentrated near the top of the list. It is likely that subjects stopped reviewing the list once they found a few relevant documents, which would lead to lower EP nDCG scores for high performance conditions. Overall, the mismatch between IP and EP for EX information problems is likely related to the varying number of documents opened by subjects and a discrepancy between their judgments and the researchers' judgments.
Figure 4. Search results click rate and correspondence rate.
! Figure 5. Comparison of intended performance (IP) and experienced performance (EP) for FF information problems.
! Figure 6. Comparison of intended performance (IP) and experienced performance (EP) for EX information problems.

220

Results also provide some initial insight into RQ1: How do people make judgments about query quality? In considering subjects' evaluation approaches for each information problem type (Figure 7), we see that for FF information problems most subjects either took an exhaustive approach, evaluating all 10 (22%) search results or a selective approach only evaluating 1-3 (50%) results. For EX information problems, we see greater diversity in the number of results evaluated. Sixty-one percent of subjects evaluated 1-4 results, while about 18% evaluated all 10 results. In Section 4.4, we examine the relationship between evaluation approach (exhaustive vs. selective) and subjects' post-SERP query evaluations in more depth.

p=0.46; FF3: t(38)=-0.67, p=0.51; FF4: t(38)=-1.80, p=.08; t(38)=-0.32, p=.75).

Figure 7. Distribution of number of clicked results according to information problem type.
The order in which search results were clicked was analyzed to investigate whether there was a relationship between click order, rank of result and subjects' relevance judgments. Click order was positively correlated with rank of search result (r=.56, p<.001), and subjects tended to click results ranked higher first. Click order was negatively correlated with perceived usefulness of a search result (r= -.46, p<.01); the search results subjects clicked earlier in time were evaluated as more useful than the ones clicked later.
4.3 Post-SERP Evaluation
After investigating subjects' interactions with the search results, we can now investigate how perceived quality of the search results impacted subjects' ratings (RQ2). During the post-SERP evaluation, subjects were asked again how likely they would be to recommend a query and how many stars they would assign it. The star-ratings assigned and likelihood to recommend the query were positively correlated (r=.88, p<.001). Thus, in subsequent analysis, we only include results related to one of these measures.
4.3.1 FF Information Problem Post-SERP Ratings
Figure 8 shows the relationship between subjects' initial query quality ratings and their post-SERP ratings for the FF information problems. The post-SERP ratings are further divided according to intended performance level (recall that FF information problems were grouped into high and low performance sets). Queries associated with higher performance levels consistently received higher post-SERP ratings than those associated with lower levels. Subjects in the low performance group evaluated an average of 4 documents, while those in the high performance group evaluated an average of 7 documents. Many subjects in the low performance group likely gave-up without finding a relevant document; this might explain the lower post-SERP ratings of this group. Independent-samples t-tests were applied to compare the means in post-SERP ratings between the high (n=21) and low performance groups (n=19) for each FF information problem. None of pairs was significant (FF1: t(38)=-1.96, p=0.056; FF2: t(38)=-0.76,

Figure 8. Comparisons of initial and post-SERP ratings for FF information problems according to performance.
The post-SERP ratings were higher than the initial ratings for all information problems except FF4 and FF5. Paired-samples t-tests were conducted to compare initial ratings to post-SERP ratings. Results showed that the only times that initial and post-SERP ratings were significantly different from each other were when FF1 and FF3 information problems were coupled with the high performance levels: FF1-H (t(20)=-5.45, p<.001); FF3-H (t(20)=2.72, p=.013) and when FF4 was coupled with the low performance level (t(18)=2.79, p=.012). During the search result evaluations, subjects were allowed to view any number of search results and in any order. To better understand the relationship between subjects' evaluation behaviors and their query quality ratings, we examine interactive precision, or the ratio of number of documents marked relevant to number of documents evaluated (Figure 9). In general, post-SERP ratings increased as interactive precision went up; the two variables were significantly correlated (r(198)=.57, p<.001).
Figure 9. Relationship between interactive precision and postSERP ratings for FF information problems (data labels represent number of information problems).
Multiple linear regression analysis was used to model the postSERP ratings given to FF information problems. Since in most cases, the initial and post-SERP ratings did not differ significantly from each other, query length was considered to be a predictor along with number of relevant search results experienced, and interactive precision. A stepwise method was used and two models were derived. The first model showed that interactive precision was the single best predictor in post-SERP ratings,  = .57, t(198) = 9.72, p < .001. Interactive precision also explained a significant proportion of variance in post-SERP rating (adjusted R2=.32, F (1, 197) = 94.49, p<.001). In the second model both

221

interactive precision and length significantly predicted post-SERP ratings (interactive precision:  = .58, t(198) = 10.00, p < .001; length:  = .16, t(197) = 2.75, p = .001); they explained 34.1% of variance in post-SERP rating (F (2, 197) = 52.58, p<.001).
4.3.2 EX Information Problems Post-SERP Ratings
While in FF information problems the descriptive statistics show that higher post-SERP ratings followed high performance levels, the trend was not observed for EX information problems. When post-ratings were grouped individually by interactive precision, post-SERP ratings significantly increased with interactive precision (Figure 10) (r(118)=.61, p<.001).

found one or two relevant results, the direction of the differences were not as predictable. When subjects identified more than three relevant search results, in most cases, subjects' post-SERP ratings were higher than their initial ratings.

Figure 10. Relationship between interactive precision and post-SERP ratings for EX information problems (data labels
represent number of information problems). Initial ratings and post-SERP ratings of query quality for EX information problems are displayed in Figure 11. Paired-samples t-tests found that the initial and post-SERP ratings were only significantly different for the medium performance level in EX3 (t(13)=-3.24, p=.006). At first glance it might seem unusual that the difference between the initial and post-SERP ratings were not also significant for the low group for EX3. Paired-sample t-tests were conducted, so there was likely a difference between the initial ratings of those who received the low and medium performance levels for this problem.
Figure 11. Comparisons of initial and post-SERP ratings for EX information problems by information problem and
performance level (data labels represent number of subjects). Figure 12 shows the differences in subjects' initial ratings and their post-SERP ratings as number of relevant search results experienced increased. Overall, subjects were more consistent in the direction of rating adjustment when they encountered few relevant search results. In cases where subjects identified no relevant documents at all, in most information problems the postSERP ratings decreased (12 out of 14). In cases where subjects

Figure 12. Relationship between number of relevant search results experienced and difference in initial and post-SERP
ratings for EX information problems.
A regression analysis was also conducted for EX information problems to better understand which factors most influenced subjects' ratings. Interactive precision, number of relevant search results found, and experienced performance were entered into a stepwise regression. Results showed that interactive precision was the best predictor of post-SERP ratings,  = .61, t(118) = 8.44, p<.001; it also explained a significant proportion of variance in post-SERP ratings (adjusted R2=.371, F(1, 118)=71.25, p<.001).
4.4 Query Quality Evaluation Approaches
RQ1 asked how people make judgments of query quality. In Section 4.2, it was shown that subjects seemed to take different strategies when evaluating search results. In order to examine whether different approaches had any influence on post-SERP ratings, we classified and compared subjects' evaluation approaches. For FF information problems, we classified subjects' evaluation approaches into two types: Selective and Exhaustive. Subjects using the Selective approach only examined one search result, while those using an Exhaustive approach examined all ten search results. For EX information problems subjects took more varied approaches resulting in three groups: Selective, Persistent and Exhaustive. The Selective approach represented cases where subjects examined 1-3 results, the Persistent, 4 to 7 results, and the Exhaustive, 8 to 10 results. These evaluation approaches were examined in the context of subjects' post-SERP ratings.
Results show that for FF information problems, those using the Exhaustive evaluation approach gave lower ratings to queries (M=2.14, SD=1.04, n=43) than those using the Selective approach (M=3.26, SD=1.27, n=39). An independent-samples t-test indicated this difference was significant (t(1,80)=4.38, p<.001). For EX information problems, those using the Exhaustive approach assigned the highest star-ratings (M=3.17, SD=0.96, n=24), followed by the Persistent (M=3.07, SD=1.04, n=44) and the Selective (M=3.12, SD=1.34, n=52), but the differences were not significant (F(2, 117)=.06, p=0.95).
In addition to examining subjects' evaluation behaviors, we asked subjects during the exit interview how they initially evaluated queries and then re-evaluated them after viewing search results (RQ1). With respect to the criteria subjects used to assign the initial query ratings, most stated that the specificity of the query was the most important factor. Some mentioned that they

222

compared queries to what they would put in a search box; if they could think of better expressions they would rate the queries worse, and vice versa. Subjects took more varied approaches to assigning post-SERP ratings. Some mentioned they based their ratings on the number of relevant results they had found, some relied on the positions of relevant search results, and others considered both factors. Yet rarely did subjects differentiate between approaches taken to evaluate queries for FF and EX information problems. Although some indicated they spent more time on EX information problems because they wished to gather more background information on the topic, many of them said that for FF problems they kept looking for other search results even when they had successfully identified the answer. The motivation for the latter came from the fear that they could miss something, the answer could be wrong, they had personal interests in some topics, or simply out of habit.
We also asked subjects how they felt about using star-ratings during real-world searching (RQ4). When asked whether they would consider using a system which provides rated query suggestions, many said yes because it would save time and benefit people who are not experienced with searching. Some said it depended on who rated the query suggestions and how well they could formulate queries by themselves. People who said they would not use such systems expressed doubt because systems or other people could not predict their information needs, or even if they could some subjects believed star-ratings were very subjective and they would rather have descriptions about why a query could be useful. Subjects constantly associated query starratings with hotel and restaurant stars and other product reviews and ratings such as those that appear in Netflix and Amazon. Their familiarity with these common usages resulted in a general agreement on the concept: the more stars there were, the higher the quality. The majority of the subjects believed 5-star query suggestions would lead to the most relevant results, some mentioned that they would expect these queries to also help them find information the quickest. Subjects said they would interpret 3-star queries as those that lead to information that was buried in the search results and would take some time to discover. Finally, subjects said they would interpret 1-star queries as useless and unhelpful; most said they would not use 1-star queries.
4.5 Reliability Check
To examine the consistency of subjects' evaluations (RQ3), subjects were asked to repeat the evaluation procedures for the last information problem they were given from the first task block and the first information problem from the second task block; thus, subjects reevaluated one FF and one EX information problem. Note that they did not have access to their previous ratings in this stage. Paired-samples t-tests were conducted to examine the differences in subjects' evaluations of FF and EX information problems (Table 7). Results demonstrated that subjects' initial ratings in the reliability check did not significantly differ from those in the main study session for both FF and EX information problems, but subjects' post-SERP ratings in the reliability check were significantly higher than those in the main study session for both information problem types. We note that subjects clicked on fewer search results during the reliability check (MFF=3.05, MEX=3.60) than the study session (MFF=4.15, MEX=4.68) and that the distributions of the clicked results were significantly different (FF:Z=-6.50, p<.001; EX:Z=-6.51, p<.001).

Table 7. Relationship between subjects' ratings during the main study and reliability check (*p<.05; **p<.01).

Main Study
Reliability Check

FF: Initial Rating
2.60 (1.32) 2.80 (1.10)

FF: PostSERP Rating 2.70 (1.49)
3.10 (1.34)

EX: Initial Rating 3.10 (1.13)
3.30 (1.04)

EX: PostSERP Rating 2.80 (1.29)
3.23 (1.19)

t-statistic

-1.11

-2.58**

-1.31

-2.21*

5. DISCUSSION
Our study conceptualized query evaluation as a two-stage process: initial evaluation and post-SERP evaluation, a distinction drawn from pre-retrieval and post-retrieval QPP approaches. Queries were first assessed based on impressions of query strings, and further refined after SERPs were examined. We found that by first impression, longer queries led to higher query quality for FF information problems; this was supported by the concept of "specificity" solicited from subjects during the interviews. This result also aligns with Hauff et al. [10] who found that assessors' query quality ratings were moderately correlated with the preretrieval predictor SumSCQ which assigns higher quality scores to more specific queries and with Lioma et al. [12] whose subjects identified vague and short queries as problematic for systems.
We found that subjects rated queries associated with EX information problems higher in quality compared with FF queries of the same length. Subjects' post-SERP evaluations of queries for EX information problems were also higher than for FF problems. These results might be explained by the vagueness of EX information problems as mentioned by subjects; subjects probably did not pose as strict evaluation standards on open information needs than on more defined information needs. It is also likely that subjects viewed themselves as more open to ideas when searching for EX information problems. This suggests that perhaps query recommendations would have the greatest potential for these types of information problems.
When examining how result quality affected query quality, it appeared that interactive precision, the number of relevant search results found to the number of documents viewed was the best predictor of post-SERP ratings of query quality for both types of information problems. This is in contrast with what we expected, which is that reciprocal rank would be a better predictor of subjects' post-SERP evaluations of query quality for FF information problems. Rather, our finding implies that regardless of information problem type, people expected that a good quality query would retrieve more than one relevant result.
For FF information problems, we found that query length was positively correlated with subjects' initial query ratings as well as their post-SERP ratings. We also observed that in most cases, subjects' initial ratings of query quality did not significantly differ from their post-SERP ratings. Only when a relatively long query was followed by a low quality SERP and when a relatively short query was followed by a good SERP did subjects change their ratings of queries. In cases of EX information problems where query length was held constant, people were also more consistent in how they adjusted their ratings when no relevant results or when many relevant results were found.
With respect to evaluation approaches, we found that subjects either took a selective or exhaustive approach when evaluating queries for FF information problems, and a selective, persistent or

223

exhaustive approach when evaluating queries for EX information problems. This was slightly contrary to what we expected; we expected that most subjects would take a selective approach when evaluating FF information problem queries since the resolution of such problems only requires a single result. Subjects who took an exhaustive approach when evaluating FF problems assigned lower ratings to queries than those who took a selective approach, while the reverse was true for EX problems. In the Exit Interviews, subjects did not differentiate between FF and EX information problems, which was surprising since much is made about the differences in these types of tasks in the research community.
Finally, we found that the initial quality evaluations subjects gave to queries were fairly stable, changing very little during the reliability check. However, their post-SERP evaluations of queries for both types of information problem significantly increased during the reliability check. We found that subjects viewed significantly fewer documents and even clicked on different documents. While this might have been caused by fatigue, it is an open question as to whether query quality evaluations are stable or, if like ratings of other experience goods they are, by nature, brittle.
This study had several limitations; perhaps the most important was the limited number of topics and queries. This study was an initial exploration of this problem space. Using only one query per information problem and holding query length constant for information problems was necessary so that the number of experimental conditions would be manageable. Using a larger combination would have compromised our abilities to collect qualitative data, through interviews, about subjects' ratings. In the future, collecting a larger number of ratings for a larger number of topics and queries would likely enhance our understanding of users' evaluations of query quality.
6. CONCLUSION
Our work explored how people make judgments about query quality; how these judgments are related to features of the query, information problem and search results; how reliable these judgments are; and what people's perceptions are of query recommendation. To our knowledge, this was one of the first systematic studies of how people make query quality evaluations. Our findings provide a useful starting-point for future useroriented studies of query quality evaluation and recommendation, and might also provide fodder for those working on automatic QPP methods. Future research will examine mechanisms for allowing users to express and share query quality ratings, and develop models of users' decision-making processes regarding query recommendations.
7. ACKNOWLEDGMENTS
We thank Max Felsher for his assistance with data collection.
8. REFERENCES
[1] Amershi, S., & Morris, M. R. (2008). Cosearch: A system for co-located collaborative Web search. Proceedings of CHI '08, 1647-1656.
[2] Baraglia, R., Cacheda, F., Carneiro, V., Fernandez, D., Formoso, V., Perego, R., & Silvestri, F. (2009). Search shortcuts: A new approach to recommendation of queries. Proceedings of RecSys '09, New York, NY, 77-84.
[3] Buscher, G., Dumais, S. & Cutrell, E. (2010). The good, the bad and the random: An eye-tracking study of ad quality in Web search. Proceedings of SIGIR '10, 42-49.

[4] Carmel, D., & Yom-Tov, E. (2010). Estimating the query difficulty for information retrieval. Synthesis Lectures on Information Concepts, Retrieval and Services, #15.
[5] Debowski, S., Wood, R. & Bandura, A. (2001). The impact of guided exploration and enactive exploration on selfregulatory mechanisms and information acquisition through electronic enquiry. Journal of Applied Psychology, 86(6), 1129-1141.
[6] Duan, W., Gu, B., & Whinston, A. B. (2008). Do online reviews matter? An empirical investigation of panel data. Decision Support Systems, 45, 1007-1016.
[7] Ekstrand, M. D., Riedl, J. T., & Konstan, J. A. (2010). Collaborative filtering recommender systems. Foundations and Trends in Human-Computer Interaction, 4(2), 81-173.
[8] Hauff, C., Hiemstra, D., & de Jong, F. (2008). A survey of pre-retrieval query performance predictors. Proceedings of CIKM '08, 1419-1420.
[9] Hauff, C., Kelly, D., & Azzopardi, L. (2010). Query quality: User ratings and system predictions. Proceedings of SIGIR'10, 743-744.
[10] Hauff, C., Kelly, D., & Azzopardi, L. (2010). A comparison of user and system query performance predictors. Proceedings CIKM '10, 979-988.
[11] Kelly, D., Cushing, A., Dostert, M., Niu, X., & Gyllstrom, K. (2010). Effects of popularity and quality on the usage of query suggestions during information search. Proceeding of CHI '10, 45-54.
[12] Lioma, C., Larsen, B., & Schutze, H. (2011). User perspectives on query difficulty. Proceedings of ICTIR, 3-14.
[13] Nelson, P. (1970). Information and consumer behavior. Journal of Political Economy, 78(2), 311-329.
[14] Senecal, S. & Nantel, J. (2004). The influence of online product recommendations on consumers' online choices. Journal of Retailing, 80(2), 159-169.
[15] Silvestri, F. (2010). Mining query logs: Turning search usage data into knowledge. Foundations and Trends in Information Retrieval, 4(1-2), 1-174.
[16] Smyth, B., Balfe, E., Freyne, J., Briggs, P., Coyle, M., & Boydell, O. (2004). Exploiting query repetition and regularity in an adaptive community-based Web search Engine. User Modeling and User-Adapted Interaction, 14(5), 382-423.
[17] Smyth, B., Coyle, M. & Briggs, P. (2011). Communities, collaboration, and recommender systems in personalized Web search. Recommender Systems Handbook, 579-614.
[18] Turpin, A. & Hersh, W. (2004). Do clarity scores for queries correlate with user performance? Proceedings of the 15th Australasian Database Conference (ADC '04), 85-91.
[19] Zhao, Y., & Scholer, F. (2007). Predicting query performance for user-based search tasks. Proceedings of the Australasian Database Conference (ADC '07), 112-115.
[20] Zhu, H., Huberman, B., & Luon, Y. (2012). To switch or not to switch: Understanding social influence in online choices. Proceedings of CHI '12, 2257-2266.

224

Efficient In-Memory Top-k Document Retrieval

J. Shane Culpepper
School of CS & IT RMIT University Melbourne, VIC, 3001,
Australia shane.culpepper@rmit.edu.au

Matthias Petri
School of CS & IT RMIT University Melbourne, VIC, 3001,
Australia matthias.petri@rmit.edu.au

Falk Scholer
School of CS & IT RMIT University Melbourne, VIC, 3001,
Australia falk.scholer@rmit.edu.au

ABSTRACT
For over forty years the dominant data structure for ranked document retrieval has been the inverted index. Inverted indexes are effective for a variety of document retrieval tasks, and particularly efficient for large data collection scenarios that require disk access and storage. However, many efficiency-bound search tasks can now easily be supported entirely in-memory as a result of recent hardware advances.
In this paper we present a hybrid algorithmic framework for inmemory bag-of-words ranked document retrieval using a self-index derived from the FM-Index, wavelet tree, and the compressed suffix tree data structures, and evaluate the various algorithmic trade-offs for performing efficient queries entirely in-memory. We compare our approach with two classic approaches to bag-of-words queries using inverted indexes, term-at-a-time (TAAT) and document-at-atime (DAAT) query processing. We show that our framework is competitive with state-of-the-art indexing structures, and describe new capabilities provided by our algorithms that can be leveraged by future systems to improve effectiveness and efficiency for a variety of fundamental search operations.
Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing--indexing methods; H.3.2 [Information Storage and Retrieval]: Information Storage--file organization; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-- query formulation, retrieval models, search process; I.7.3 [Document and Text Processing]: Text Processing--index generation
Keywords
Text Indexing, Text Compression, Data Storage Representations, Experimentation, Measurement, Performance
1. INTRODUCTION
Top-k retrieval algorithms are important for a variety of real world applications, including web search, on-line advertising, relational databases, and data mining. Efficiently ranking answers to
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

queries in large data collections continues to challenge researchers as the collection sizes grow, and the ranking metrics become more intricate. Despite recent hardware advances, inverted indexes remain the tool of choice for processing efficiency-bound search tasks [17]. However, large memory systems also provide new opportunities to explore another class of indexing algorithms derived from the suffix array to potentially improve the efficiency of various inmemory ranked document retrieval tasks [25, 27].
In this paper we present a hybrid algorithmic framework for inmemory bag-of-words ranked document retrieval using a self-index derived from the FM-Index, wavelet tree, and the compressed suffix tree data structures [12, 20, 27, 22], and evaluate the various algorithmic trade-offs for performing efficient in-memory ranked querying. We compare our approach with two classic approaches to bag-of-words queries using inverted indexes, term-at-a-time (TAAT) and document-at-a-time (DAAT) query processing. We show that our framework is competitive with state-of-the-art indexing structures, and describe new capabilities provided by our algorithms that can be leveraged by future systems to improve efficiency and effectiveness for various document retrieval tasks.
Our contributions. Firstly, we propose a hybrid approach to solv-
ing a subset of important top-k document retrieval problems ≠ bagof-words queries. Secondly, we present a comprehensive efficiency analysis comparing in-memory inverted indexes with top-k selfindexing algorithms for bag-of-words queries on text collections an order of magnitude larger than any other prior experimental study. To our knowledge, this is the first comparison of this new algorithmic framework for realistic sized text collections using a standard similarity metric ≠ BM25. Finally, we describe how our algorithmic framework can be extended to efficiently and effectively support other fundamental document retrieval tasks.
2. PROBLEM OVERVIEW
In this paper, we investigate the use of self-indexing algorithms to solve the top-k document search problem. A document collection T is a contiguous string drawn from an alphabet , where  = || is the number of distinct "terms" or strings. In practice,  can be characters (UTF8 or ASCII), bytes, integers, or even phrases. Each document in T is separated by a unique end of document symbol defined to be lexicographically smaller than any s  .
DEFINITION 1. A top-k document search takes a query q  , an integer 0 < k  d, and a text T   partitioned into d documents {D1, D2, . . . , Dd}, and returns the top-k documents ordered by a similarity measure S(q, Di).
In this work, we focus primarily on bag-of-words queries, so our baseline S(q, Di) ranking function is BM25. Our S(q, Di) ranking function has the following formulation:

225

BM25 = log
tq

N - ft + 0.5 ft + 0.5

∑ TFBM25

TFBM25

=

fd,t

+ k1

fd,t ∑ (k1 + 1) ∑ ((1 - b) + (b ∑ d/avg))

Here, N is the number of documents in the collection, ft is the number of distinct document appearances of t, fd,t is the number of occurrences of term t in document d, k1 = 1.2, b = 0.75, d is the number of symbols in the dth document, and avg is the average of d over the whole collection. The free parameters k1 and b can be tuned for specific collections to improve effectiveness, but we use the standard Okapi parameters suggested by Robertson et al. [36].

3. ALGORITHMS
We now present an overview of the key data structures and algorithms used in our framework. Here, we only outline the key properties and features of wavelet trees and suffix arrays used in our search engine; for a more in-depth tutorial, see, for example Navarro and M‰kinen [30] and the references therein. Efficient operations in succinct data structures depend on two fundamental operations over a bitvector B[0, n - 1]:
RANK0/1(B, i): Return the number of 0's/1's in B[0, i]. SELECT0/1(B, i): Return the position of the ith of 0's/1's in B.
Both operations can be performed in constant time. A simple constant time RANK0/1 solution uses o(n) space in addition to storing B [23]. More space efficient RANK0/1 algorithms are possible [35].

3.1 Wavelet Trees
Efficient RANKs and SELECTs over an alphabet of size  > 2 can be performed using a wavelet tree [20]. A wavelet tree decomposes the RANKs and SELECTs operations over [0,  - 1] into RANK0/1 and SELECT0/1 operations on a binary alphabet using a binary tree. The root of the tree represents the whole alpha-
bet. Its children represent each half of the alphabet of the par-
ent node. Each leaf node in the tree represents one symbol in [0,  - 1]. When answering the RANKs query for a specific symbol s, we perform RANK0/1 operations at each level in the tree until we arrive at the leaf node representing s. The overall RANKs(T , i) can be computed by combining the RANK0/1 results at each tree level in O(log ) time. Any symbol s = T [i] is also computed in time O(log ) with a similar algorithm; we call this operation ACCESS(T , i). Using a succinct representation of RANK0/1 and SELECT0/1 [35], a wavelet tree requires nH0 + o(n log ) bits of space, where H0  log  is the zero-order entropy of T .1
Wavelet trees are a surprisingly versatile data structure, and have
attractive time and space bounds for many primitive operations in self-indexing algorithms [14]. As a result, many top-k document retrieval approaches rely heavily on wavelet trees. A subset of im-
portant wavelet tree operations include:

RANKs(T , s, sp, ep): Return the number of occurrences

of symbol s in a range T [sp, ep].

SELECTs(T , s, j, sp, ep): Return the position of the jth oc-

currence of symbol s in a range

T [sp, ep].

ACCESS(T , i):

Return symbol T [i].

1We assume logarithms are in base 2.

RMQ(T , sp, ep): RQQ(T , k, sp, ep):

Return the smallest symbol s in a range T [sp, ep]. Return the kth smallest symbol s in a range T [sp, ep].

3.2 Self-indexing
A suffix array SA[0, n-1] over T stores the offsets to all suffixes in T in lexicographical order. Any pattern P of length m occurring in T is a prefix of one or more suffixes in SA. These suffixes, due
to the lexicographical order within SA, are grouped together in a
range SA[sp, ep]. To determine SA[sp, ep], we perform two binary searches over SA and T . Each binary search comparison requires up to m symbol comparisons in T , for a total of O(m log n) time.
Using additional auxiliary data structures this cost can be reduced to O(m + log n) [25]. Suffix array construction is a well studied
problem, and many solutions with various time and space trade-offs exist in the literature [34]. However, searching for a pattern P in T using only a suffix array requires O(n log n) bits to store both T and SA, which in practice is at least 5 times the text size.
By replacing T with the Burrows-Wheeler Transform (BWT)
permuted text, the key operations of a basic SA can be emulated with much less space, close to the size of T in compressed form.
The Burrows-Wheeler Transform [7] ≠ also known as the block-
sorting transform ≠ produces a permutation of a string T , denoted T BWT , by sorting the n cyclic rotations of T into full lexicographical order, and taking the last column of the resulting n ◊ n matrix. The resulting string T BWT tends to be more compressible as symbols are grouped together based on their context in T , which makes
the BWT an important part in many state of the art compression systems [26]. To produce T BWT for a given text T , it is not necessary to construct M as there is a duality between T BWT and the SA over a text T : T BWT[i] = T [SA[i] - 1 mod n].
The original text T can be recovered from T BWT in linear time without the need for any additional information. To recover T from only T BWT we first recover the first column, F , in M by sorting the last column (L = T BWT), in lexicographical order. By mapping the symbols in L to their respective positions in F so L[i] = F [j] (usually referred to as the LF mapping, j = LF(i)) we can recover T backwards as T [n - 1] = T BWT[0] = $ and T [j - 1] = T BWT [LF(i)] if and only if T [j] = T BWT[i]. Since F is simply a sort of the n characters of the string in lexicographical
order, it can be represented succinctly as a lookup table of alphabet
characters along with the count of all symbols that appear before
the current character c. The LF mapping is computed using the
equation

LF(i) = LF(i, c) = C[c] + RANKs(T BWT , i)

(1)

where c is the symbol T BWT [i], and C[c] stores the number of symbols in T BWT smaller than c.
Performing a search in T using the BWT permuted text is straight-
forward. Recall that all rows are sorted in lexicographical order in
M. Therefore, for a pattern P, all occurrences of P in T must have a corresponding row in M within a range sp, ep . To determine the range within M, we first determine the range spm, epm within M that corresponds to Pm using C[ ]. Then, for each sym-
bol j = m-1 . . . 0 in P, we iteratively find spj , epj by calculat-
ing the number of rows within spj+1, epj+1 that are preceded by the symbol Pj in T . For a given row j, the LF mapping can be used to determine the row in M representing the symbol preceding j in T . The preceding row is determined by counting the number of occurrences of c = T BWT[j] before the current row and ranking these
occurrences within C[s]. Assume we have located spj+1, epj+1 ,

226

which corresponds to the rows prefixed by P[j + 1, m]. Then

spj = LF(spj+1 - 1, pj)

(2)

will calculate the position in F of the first occurrence of Pj within
spj+1, epj+1 , and thus compute the start of our range of rows within M that correspond to P[j, m]. Similarly, we compute

epj = LF(epj+1, pj) - 1.

(3)

Once the area sp, ep is determined, self-indexes offer a way to find any occurrence position SA[j], for sp  j  ep. This is accomplished by sampling T at regular intervals, and marking
positions of SA that point to sampled text positions in a bitmap E[0, n - 1]. Sampled suffix array positions are stored in an array G[RANK1(E, j)] = SA[j] if E[j] = 1. Given a target value SA[j], the successive values i = 0, 1, . . . are evaluated until E[LFi(j)] = 1, producing the desired answer of SA[j] = SA[LFi(j)]+i. If every th text position is sampled, we guarantee i can be found for every 0  i < , and sampling requires O((n/) log n) extra bits for G (and for E in compressed form [35]), and computes any entry of SA within  applications of LF.
Similarly, in order to recover any text substring T [l, r - 1] (including the whole T ), we can use the same sampling of text position multiples of , and store H[i] = SA-1[i ∑ ]. Thus, we extend the range to T [l, r - 1], for r =  ∑ r/ and display from the suffix array position j = SA-1[r]. Then, we can display the area backwards as T BWT[j], T BWT[LF(j)], T BWT[LF2(j)], . . .. Each
step requires one RANKs and one ACCESS operation, which has the same cost as LF. Therefore, we can display T [l, r - 1] within O(r - l + ) probes of LF.
In practice self-indexes can be reduced to a wavelet tree over T BWT with auxiliary information to emulate F (the C array) and
the sampling information. This representation of a self-index is referred to as an FM-Index [12, 13]. A wavelet tree built over T BWT uses nHk(T ) + o(n log ) bits [24] for any k   log(n) - 1 and constant  < 1, so the space requirements are reasonable. Here Hk(T )  Hk-1(T )  . . .  H0(T )  log  is the k-th order entropy of T [26], a measure of the performance of any compressor using k-th order statistical modeling on T . Many
other self-indexing variations exist with different time / space trade-
offs [30, 15]. In principle, any of these approaches are compatible
with the framework we present here, as long as the method returns
a sp, ep range of matching suffixes.

4. TOP-K DOCUMENT RETRIEVAL USING
SELF-INDEXES
In order to efficiently solve the top-k document search problem, unadorned self-indexing algorithms are not sufficient. Two approaches to enhance the self-index have been proposed. The first is to use a document array, that is, a mapping between every suffix in T to its corresponding document identifier [10, 18, 27, 41]. The second is to store, in addition to the global self-index, one selfindex of each individual document in the collection [22, 37]. These alternatives offer different theoretical frameworks that are not directly comparable, but experimental studies [10, 31] have consistently shown that the first approach offers better space and time performance in practice.
Representing the document array with a single wavelet tree can provide additional important advantages. For example, the list of distinct documents where a substring P appears, with the corresponding term frequencies, can be obtained without any additional structure [18], in O(log d) time per document retrieved, once the self-index has given the suffix array range of P. This information

top-k

prestored

top-k

top-k

top-k

g

g

D 31421432124342132143

fringe sp

fringe ep

Figure 1: Precomputed top-k results over fixed intervals g stored in a skeleton succinct suffix tree using the HSV approach. Only the fringe leaves are processed for a given sp, ep range.

can then be used to calculate simple TF◊IDF based S(q, Di) metrics at query time [37]. In addition, several other operations such as Boolean intersection can be performed efficiently using only the wavelet tree over the document array [19].
Culpepper et al. [10] showed how to use the same wavelet tree to find the top-k documents (with raw term frequency weights) for a string P. Among all of the strategies proposed, the heuristic algorithm GREEDY worked best in practice. Despite the lack of worstcase theoretical guarantees, they show unadorned wavelet trees are efficient in time and space for this task. Hon et al. [22] presented a technique with worst-case performance guarantees. The HSV approach builds on the same document listing strategy originally proposed by Sadakane [37]. While HSV was originally described using individual self-indexes for each document as in Sadakane's approach [37], the method can be applied on top of either documentlisting solution in practice. The key insight of the HSV method is to precompute the top-k results for the lowest suffix tree nodes in a predetermined sampling range. Figure 1 shows a HSV tree over D. In this example, a sp, ep range of size g is precalculated and stored in a succinct suffix tree. An arbitrary query sp, ep is received. The bulk of the query result is already precomputed as sp, ep . The remainder of the query can then be processed using RQQ queries over the fringe ranges to generate the final top-k counts.
Using g samples guarantees that any suffix array interval sp, ep for a given P falls into one of three categories: (1) The range is completely covered by the sampled interval, and the top-k answer is precomputed; (2) The range is partially covered, and at most 2g fringe leaves must to be processed at query time and merged with the sample; or (3) The range is too small to be covered. For Case (3), the complete sp, ep range must be processed at runtime, but is guaranteed to be smaller than 2g.
Navarro and Valenzuela [31] demonstrated that implementing HSV over a document array, and using the GREEDY approach of Culpepper et al. [10] to speed up document listing, is more efficient than using either GREEDY or HSV in isolation. The hybrid approach requires additional space to support HSV on top of GREEDY, but efficiency is significantly improved by limiting the number of rank queries required at query time. This approach as well as other trade-offs are explored more fully in this paper.

227

22

ft Ft

acacia ft Ft

d fd,t

p*

d fd,t

p*

d fd,t

p*

d fd,t

p*

avenue ft Ft Term Map

d fd,t

p*

d fd,t

p*

Posting Lists

Document Map WTX001-B01-1 307 WTX001-B01-2 14 WTX001-B01-3 2004 WTX001-B01-4 92 WTX001-B01-5 174 WTX001-B01-6 747

Document Cache

Document Storage

Figure 2: The three fundamental components of an inverted index. Each term in the vocabulary is mapped to a posting list of d, fd,t tuples. For each tuple, the position offsets p are also stored to support phrase or term proximity queries.

5. INVERTED INDEXES
Traditional approaches to the top-k document search problem rely on inverted indexes. Inverted indexes have been the dominant data structure for a variety of ranked document retrieval tasks for more than four decades [44]. Despite various attempts to displace inverted indexes from their dominant position for document ranking tasks over the years, no alternative has been able to consistently produce the same level of efficiency, effectiveness, and time / space trade-offs that inverted indexes can provide (see, for instance Zobel et al. [45]).
Figure 2 shows a typical inverted indexing system. The system contains three key components: (1) Term Map - The vocabulary of terms, along with the number of documents containing one or more occurrence of the term (ft), the number of occurrences of the term in the collection (Ft), and a pointer to the corresponding posting list. (2) Posting Lists - An ordered list of tuples, d, fd,t , containing the document identifier and the frequency of the term in document d. For each tuple, the ordered position offsets, p are also maintained in order to support phrase queries. For indexes that do not require phase queries, p can be omitted. (3) Document Storage - A document map to match d to the document name, and a pointer to the document in a document cache.
Ranked document retrieval requires that only the top-k documents are returned, and, as a result, researchers have proposed many heuristic approaches to improve top-k efficiency [1, 4, 5, 6, 32, 38]. These approaches can be classified in two general categories: term-at-a-time (TAAT) and document-at-a-time (DAAT). Each of these approaches have various advantages and disadvantages.
5.1 Term-at-a-Time Processing (TAAT)
For TAAT processing, a fixed number of accumulators are allocated, and the rank contribution incrementally calculated for each query term in increasing document order. When inverted files are stored on disk, the advantages of this method are clear. The inverted file for each term can be read into memory, and processed sequentially. However, when k is small relative to the total number of matching documents in collection, TAAT can be inefficient, particularly when the number of terms in the query increases, since all of the inverted lists must be processed before knowing the full rank score of each document. In early work, Buckley and Lewit [6] proposed using a heap of size k to allow posting lists to be evaluated in TAAT order. Processing is terminated when the sum of the contributions of the remaining lists cannot displace the minimum score in the heap.

Moffat and Zobel [29] improved on this pruning approach with two heuristics: STOP and CONTINUE. The STOP strategy is somewhat similar to the method of Buckley and Lewit, but the terms are processed in order of document frequency from least frequent to most frequent. When the threshold of k accumulators is reached, processing stops. In contrast, the CONTINUE method allows the current accumulators to be updated, but new accumulators cannot be added. These accumulator pruning strategies only approximate the true top-k result list.
If approximate results are acceptable, the TAAT approach can be made even more efficient using impact ordering [1, 2, 33]. The key idea of impact ordering is to precompute the TF for each document a term appears in. Next, quantize the TF values into a variable number of buckets, and sort the buckets (or blocks) for each term in decreasing impact order. Now, the top-k representative can be generated by sequentially processing each of the highest ranking term contribution blocks until a termination threshold is reached. The authors refer to this blockwise processing method as scoreat-a-time processing. Despite not using the full TF contribution for each term, Anh and Moffat [1] demonstrate that the effectiveness of impact ordered indexes is not significantly reduced, but efficiency is dramatically improved.
5.2 Document-at-a-Time Processing (DAAT)
The alternative approach is to process all of the terms simultaneously, one document at a time [8]. The advantage of this approach is that the final rank score is known as each document is processed, so it is relatively easy to maintain a heap containing exactly k scores. The disadvantage is that all of the term posting lists are cycled through for each iteration of the algorithm requiring nonsequential disk reads for multi-word queries. However, our focus in this paper is in-memory ranked retrieval, so DAAT tends to work very well in practice.
Pruning strategies to further increase efficiency also exist for DAAT processing. The most widely used pruning strategy for DAAT is MAXSCORE. Turtle and Flood [40] observed that the BM25 TF component can never exceed k1 + 1 = 2.2. So, the total score contribution for any term is at most 2.2 ∑ log(N/Nt). Using this observation, Turtle and Flood present an algorithm that allows posting values below the threshold to be skipped. As the minimum bounding score in the heap slowly increases, more and more postings can be omitted. Enhanced DAAT pruning strategies similar in spirit to MAXSCORE have been shown to further increase efficiency [4, 38].

228

T TATA$ATAT$TTAT$AATT$ BWT T A T T T $ T T T $ A A T A A T A $ A $

C WTt 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0

$0 A4

10011111010

000000000

T 11

$

A

T

FM-Index

sp

ep

D

31421432124342132143

WTd 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1

0101011010

0110101010

1

2

3

4

Document Array Wavelet Tree

Figure 3: Given the text collection T = TATA$ATAT$TTAT$AATT$ of four documents, a self-indexing system requires two wavelet trees. The first wavelet tree supports backwards search over the BWT permuted text, and the second supports statistical calculations over the document array. Note that only the items in grey are stored and used for query operations.

Turtle and Flood also describe a similar approach to improve the efficiency of TAAT strategies. However, the TAAT variant is more complex than the DAAT approach as it requires an ordered candidate list of k documents to be maintained. The candidate list is used to skip document postings in each term list which could not possibly displace the current top-k documents once the heap contains k items.
Fontoura et al. [17] compare several TAAT and DAAT based inmemory inverted indexing strategies. The authors present novel adaptations of MAXSCORE and WAND [4] to significantly improve query efficiency of in-memory inverted indexes. The authors go on to show further efficiency gains in DAAT style processing by splitting query terms into two groups: rare terms and common terms. The exact split is based on a fixed threshold selected at query time. For our baselines, we use WAND for DAAT query processing, and MAXSCORE for TAAT query processing.
6. SELF-INDEXING APPROACHES
We now describe our general approach to in-memory indexing and retrieval. Figure 3 shows the key components of our retrieval system: an FM-Index and the document array wavelet tree, WTd. In addition, our system requires a Document Map to map document identifiers to human readable document names (or URLs). No document cache is required and the original documents or snippets around each match can be recreated directly from the FM-Index by extracting the required text positions using the suffix array sampling. Only the items in grey are stored and used for characterbased top-k document retrieval. All other components are shown for illustration purposes only.

ALGORITHM GREEDY-TAAT

INPUT OUTPUT

A sorted list t of q terms. A list of k documents in rank order.

1: Initialize a max-heap R  {}

2: for i  1 to q do

3: Determine sp, ep for term ti 4: Ai  GREEDY(sp, ep, k)

5: end for

6: for i  1 to q do

7: for j  1 to k do

8:

if Ai[j]  R then

9:

UPDATE(R, Ai[j], score)

10:

else

11:

ADD(R, Ai[j], score)

12:

end if

13: end for 14: end for 15: return R[1 . . . k]

FUNCTION GREEDY (sp, ep, k)

1:   WTd.root 2: A max-heap, sorted by ep - sp, h  PUSH(, [sp, ep])

3: A priority queue PQ  {}.

4: i  0

5: while h =  and i < k do

6: , [sp, ep]  POP(h)

7: if  is leaf then

8:

PQ  ENQUEUE(.docid, ep - sp + 1)

9:

ii+1

10: else

11:

[s0, e0]  [RANK0(B, sp), RANK0(B, ep)]

12:

[s1, e1]  [RANK1(B, sp), RANK1(B, ep)]

13:

if e0 - s0 > 0 then h  PUSH(.left, [s0, e0])

14:

end if

15:

if e1 - s1 > 0 then h  PUSH(.right, [s1, e1])

16:

end if

17: end if

18: end while

19: return PQ

A simple bag-of-words search using a self-index retrieval system is outlined in Algorithm GREEDY-TAAT. Recall that the sp and ep range for any string can be found using a backwards search in the BWT permuted text using only a wavelet tree over T BWT and C. So, the sp, ep for each query term in Line (3) can be calculated in O(|ti| log ) time using an FM-Index. Now, a wavelet tree over the document array WTd can be used to retrieve exactly k documents in frequency order for each term using GREEDY or QUANTILE [10]. This algorithm is analogous to TAAT processing, and is referred
to as GREEDY-TAAT. Note that Function GREEDY can also be augmented with HSV as described in Section 4 to further increase the efficiency of constructing Ai for each query term.
We also present several variations on this general strategy. First, we consider the addition of HSV style precomputations over WTd as described by Navarro and Valenzuela [31]. Instead of storing the top-k most frequent symbols in the skeleton suffix tree, we store

229

Query length
|q| 1 2 3 4 5 6 7 8
random sample

TREC 7 & 8 queries

Total Matches Average ni

Queries ('000)

('000)

100

9.9

9.9

100

24.8

12.5

100 104.5

38.5

100 238.1

69.0

100 351.2

95.1

100 408.7

107.8

100 463.8

126.2

100 489.8

148.3

800 234.9

70.0

TREC WT10G queries

Total Matches Average ni

Queries ('000)

('000)

100

3.3

4.7

100

68.6

42.5

100 292.8

123.2

100 601.1

166.6

100 866.5

228.5

100 1041.9

280.4

100 1149.7

319.6

100 1171.8

339.2

800 621.5

181.2

Table 1: Statistics of the queries used in experiments (sampled based on query length, or sampled from the filtered MSN query log), reporting the number of queries run, the mean number of documents that contained one or more of the query terms, and the mean length of the inverted lists processed.

the top-k most important symbols sorted by term impact for each interval g to improve effectiveness. In order to capture k-values commonly used in IR systems (k = 10, 100, 1000), we prestore values of any k that is a power of 2 up to 8192 in term contribution order. Note that we go higher than 1024 since the values of k necessary to ensure good effectiveness can be greater than the desired k.
Observe that in typical bag-of-words query processing over English text, the size of the vocabulary is often small relative to the total size of the collection. As such, we also present a new hybrid approach to top-k bag-of-words retrieval using a Term Map and WTd. If we assume the vocabulary is fixed for each collection, then the sp, ep range for each term can be precalculated and retrieved using a term map, as in the inverted indexing solution. This means that the FM-Index component is no longer necessary when processing bag-of-words queries. We refer to these hybrid approaches as SEM-GREEDY and SEM-HSV. These methods reduce the overall space requirements of our approach, but also limit the full functionality of some auxiliary operations. For example, the text can no longer be reproduced directly from the index, so snippets cannot be generated on the fly, and phrase queries are no longer natively supported. However, for classic bag-of-words queries, our hybrid approach provides an interesting trade-off to consider. Our final variation is to support a term-based self-index. We refer to this approach as FM-TERM.
It is also possible to support a DAAT query processing strategy in our retrieval system, but this would require efficiently supporting RMQ over the document array. Our approach currently supports a generalization of RMQ ≠ RQQ. But, the cost of RQQ is O(log d) per k value extracted, while constant time solutions for RMQ currently exist [16]. However, an RMQ style approach as presented by Fischer and Heun [16] incurs an additional 2n bits of space, and so we do not explore the possibility further in this work.
Also note the current top-k bag-of-words approach shown in GREEDY-TAAT is based entirely on the frequency counts of each item. This means that our current implementation only approximates the top-k items. This is a well-known problem in the inverted indexing domain. This limitation holds for any character-based bag-of-words self-indexing system that does frequency counting at query time since we can not guarantee that item k + 1 in any of the term lists does not have a higher score contribution than any item currently in the top-k intermediate list. A method of term contribution precalculation is required in order to support BM25 or language-model ranking. Without the term contribution scoring,

WAND and MAXSCORE enhancements are not possible, and therefore every document in the sp, ep must be evaluated in order to guarantee the final top-k ordering. However, this limitation can be mitigated by using HSV since we can precalculate the impact contribution for each sample position and store this value instead of storing only the frequency ordering. Top-k guarantees are also possible using a term-based self-indexing system where each distinct term is mapped to an integer using HSV or other succinct representations of term contribution preprocessing. In future work, we intend to fully examine all of the possibilities for top-k guarantees using self-indexes in various bag-of-words querying scenarios.
When using character-based self-indexing approaches for bagof-words queries, there is another disadvantage worth noting. For self-indexes, there is an efficiency trade-off between locating the top-k fd,t values and accurately determining ft since the index can extract exactly k fd,t values without processing every document. For a fixed vocabulary, ft is easily precomputed, and can be stored in the term map with the sp, ep pairs. But, in general it is not straightforward to determine ft for arbitrary strings over WTd without auxiliary algorithms and data structures to support calculating the value on the fly. The FM-HSV approach allows us to prestore ft for each sampled interval which can be be used to calculate ft over sp, ep more efficiently by only processing potential fringe leaves. Calculating ft using only WTd for arbitrary strings in near constant time using no additional space remains an open problem.
7. EXPERIMENTS
In order to test the efficiency of our approach, two experimental collections were used. For a small collection, we used the TREC 7 and 8 ad hoc datasets. This collection is composed of 1.86 GB of newswire data from the Financial Times, Federal Register, LA Times, and Foreign Broadcast Information Service, and consists of around 528,000 total documents [42]. For a larger in-memory collection, we used the TREC WT10G collection. This collection consists of 10.2 GB of markup text crawled from the internet, totalling 1,692,096 documents [21].
All of the algorithms described in this paper were implemented using C/C++ and compiled with gcc 4.6.1 with -O3 optimizations. For our baselines, we have implemented the in-memory variant of WAND as described by Fontoura et al. [17] for DAAT, and an inmemory variant of MAXSCORE for TAAT. Experiments were run on a single system with 2◊ Intel Xeon E5640 Processors with a 12 MB smart cache, 144 GB of DDR3 DRAM, and running Ubuntu Linux 11.10. Times are reported in milliseconds unless otherwise

230

800

600

400

msec

200

0

FM

SE

FM-H

SE-H

AD AT TAAT

k=10,k'=80

FM

SE

FM-H

SE-H

AD AT TAAT

k=100,k'=800

FM

SE

FM-H

SE-H

AD AT TAAT

k=1000,k'=8000

Figure 4: Efficiency for 1,000 randomly sampled MSN queries against the TREC 7 & 8 collection.

600

400

msec

200

0

FM

SE

FM-H

SE-H

AD AT TAAT

k=10,k'=20

FM

SE

FM-H

SE-H

AD AT TAAT

k=100,k'=200

FM

SE

FM-H

SE-H

AD AT TAAT

k=1000,k'=2000

Figure 5: Efficiency for 1,000 randomly sampled MSN queries against the the TREC WT10G collection.

noted. All efficiency runs are reported as the mean and median of 10 consecutive runs of a query, and all necessary information is preloaded into memory.
Note that we do not carry out an full evaluation of the effectiveness of the algorithms presented in this paper. In previous work, we showed that the BM25 ranking and query evaluation framework used in our approach can be as effective as other state-of-the-art open source search engines when using k > k, and do not repeat those experiments here [11]. In these experiments, we use the minimum k values that result in retrieval performance that is comparable to the effectiveness obtained through exhaustive processing. In all experiments we use k = 8  k for the TREC 7 & 8 dataset, and k = 2  k for the TREC WT10G dataset. These values for k give results for the MAP and P@10 effectiveness measures that are not statistically significantly different compared to exhaustive processing, for both collections (paired t-test, p > 0.05). We intend to pursue additional efficiency and effectiveness trade-offs in future work.
7.1 Experimental Setup
In order to test the efficiency of our algorithms, queries of varying length were extracted from a query log supplied by Microsoft.

Each query was tested against both TREC collections, and the filter-

ing criteria used was that every word in the query had to appear in at

least 10 distinct documents, resulting in a total of 656,172 unique

queries for the TREC 7 & 8 collection, and a total of 793,334 unique

queries for the TREC WT10G collection. From the resulting filtered

query sets, two different query samples were derived.

First, 1000 queries of any length were randomly sampled from

each set, to represent a generic query log run. The 1,000 sampled

queries for TREC 7 & 8 have an average query length of 4.224, and

the average query length of the WT10G sample set is 4.265 words

per query. For the second set of experiments, 100 queries for each

query length 1 to 8 were randomly sampled from the same MSN

query sets. Table 1 shows the statistical properties of the sampled

queries that were used in the second experimental setup, including

the average number of documents returned for each query for each

query length, and the average length of postings lists processed for

each query, computed as (

|q| i=1

ni)/|q|.

7.2 Average Query Efficiency
In order to test the efficiency of our algorithms, two experiments were performed on each of the collections. The first experiment is designed to measure the average efficiency for each algorithm,

231

trec-7&8k'=8*k

k=10

200

AD AT

100

50

FM-H/SE-H

10

FM/SE

TAAT

0.1

200 100
50 10

k=100

k=1000

MedianQryTim [ems]

wt10gk'=2*k

0.1
1 2 34 5 6 7 81 2 34 5 6 7 81 2 34 5 6 7 8
Figure 6: Efficiency of query length of 1 to 8 on the TREC 7 & 8 collection (top row) and TREC WT10G collection (bottom row) for k=10,100 and 1000. For each query length, 100 randomly sampled queries are used from the MSN query log set.

given a sampling of normal queries. For this experiment, the length of the queries was not bounded during sampling, and had an average query length of just over 4 words per query as mentioned in Section 7.1.
Figures 4 and 5 show the relative efficiency of each method averaged over 1,000 randomly sampled MSN queries for TREC 7 & 8, and TREC WT10G. Each boxplot summarizes the time values as follows: the solid line indicates the median; the box shows the 25th and 75th percentiles; and the whiskers show the range, up to a maximum of 1.5 times the interquartile range, with outliers beyond this shown as separate points. In both figures, the following abbreviations are used for the algorithms: FM-GREEDY (FM), SEM-GREEDY (SE), FM-HSV (FM-H), SEM-HSV (SE-H), DAAT , and TAAT.
We report the timings for all of the self-indexing methods using the character-based indexes. We also ran the same experiments using our term-based indexes, but the performance was identical. This result is not surprising since the dominant cost in the selfindexing method is traversing the wavelet tree over the document array, and is dictated by the depth of the wavelet tree and not the overall length. Since the depth depends only on the number of documents, both approaches consistently produce similar running times. So, the only efficiency difference between character-based and term-based indexes is in space-usage which is discussed in Section 7.
We see that the self-indexing methods which must calculate all frequency scores (FM and SE) incur the most overhead as k increases. This is largely due to the multiplicative effect of collating many consecutive k values. For example, when collating frequency values from WTd, the number of rank operations is proportional to the depth of the wavelet tree. In the case of the TREC WT10G collection, which contains around 1.6 million documents, the depth of the wavelet tree is 24. So, the number of random rank probes in wavelet tree begins to significantly degrade the performance for larger k.

This effect can be marginalized by augmenting WTd with HSV. Since an HSV style index has a portion of each sp, ep ranges, only the fringe positions for each range need to be calculated at runtime, reducing the total number of page faults. For all k, the HSV indexes are efficient and remarkably resilient to outliers. In general, the WAND variant of DAAT is more efficient for large values of k, but can perform poorly for certain queries. For example, the query "point out the steps to make the world free of pollution" on the WT10G collection consistently performed poorly in our DAAT framework.
7.3 Efficiency based on Query Length
We now break down the efficiency of each of our algorithms relative to two parameters: k and q, where q is the number of terms in a query. Figure 6 shows the average of 10 runs of 100 queries per query length, q. For one-word queries, for all values of k, the inverted indexing approaches DAAT and TAAT are superior. This is not surprising since only a single term posting must be traversed to calculate BM25, and the algorithms have excellent locality of access. Still, the HSV variant is the most efficient for small k.
For |q| > 1, the results also depend on k. For k = 10 and k = 100, the self-indexing methods are more efficient than TAAT since the methods can extract exactly k values. Since the sample rates in the lower regions of the HSV methods are close to k, very little work needs to be done by the indexes. The WAND-based DAAT method remains remarkably efficient for all values of k. As k increases, the performance of the HSV-based approaches begins to degrade since the sample size for the precalculated top-k orderings grows exponentially. The performance degradation at large k is equivalent to Case (3) as described in Section 4. In essence, most of the sp, ep ranges turn out to be much smaller than any of the samples, so the complete sp, ep range must be computed at runtime, reducing the performance to FM-GREEDY when an appropriate sample is not available. Note that the performance of HSV for TREC 7 & 8 is worse than for WT10G for two reasons. First, k

232

20000 15000 10000

trec-7&8 Components
FM WTD DocMap HON SEMap InvFile PosOffsets DocCache

wt10g

S zi (eM)B

5000

0 FM

SE

FM-Term

AD ATTAAT

FM

SE

FM-Term

AD ATTAAT

Figure 7: Space usage for each component in the three indexing approaches presented in this paper for the TREC 7 & 8 collection (left) and the TREC WT10G collection (right). The dashed line in both graphs represents the total space usage of the original uncompressed text for the collection.

is four times larger in TREC 7 & 8 resulting in fewer sample points. Secondly, if only a partial match is found, the self-index approach must retrieve 8 times more intermediate documents for scoring than in the inverted indexing approaches.
Note that none of our self-indexing approaches currently employ MAXSCORE or other methods to guide scoring. In principle our approach could also benefit from similar enhancements. We intend to explore the benefits and drawbacks of various early termination and impact scoring approaches for self-indexes in future work.
7.4 Space Usage
We now address the issue of space usage for the different algorithmic approaches. Inverted indexes are designed to take advantage of a myriad of different compression techniques. As such, our baselines also support several state-of-the-art byte and word aligned compression algorithms [3, 9, 28, 39, 43]. So, when we report the space usage for an inverted index, the numbers are reported using compressed inverted indexes and compressed document collections.
Figure 7 presents a break down of space usage for each component of the inverted indexing and self-indexing approaches. From a functionality perspective, there are several different componentization schemes to consider. First, consider the comparison of an inverted index method (including the term map, the postings list with p offsets, the document map, and the compressed document cache) with an FM-Index (including WTd, the document map, and any other precomputed values ≠ for instance the HSV enhancement). We consider these two in-memory indexes as functionally equivalent, as both can support bag-of-words or phrase queries, and can recreate snippets or even the original uncompressed document. The character based variant FM is significantly larger, but able to support a range of special character and arbitrary substring queries that term-based indexes do not support. Therefore, the term-based self-indexing variant FM-TERM is much closer to the inverted indexing variant in space usage and functionality.
The second alternative are indexes that support only bag-of-words queries. Now, an inverted index method requires only the term map, the postings list without p offsets, and the document map. The character-based self-indexes are essentially the same, but the FM-

Index component is replaced with a term map component. Note that the FM component of FM-TERM is only required for phrase queries, and can also be dropped if only bag-of-words queries are required. When considering all of the current self-indexing options presented in this paper, using an FM-Index component instead of a term map appears to offer the most flexible configuration for character-based self-indexes, while the term-based variant is competitive in both time, space, and functionality with an inverted index.
8. CONCLUSION
We have presented an algorithmic framework for in-memory bagof-words query processing that is efficient in practice. We have compared and contrasted our framework with industry and academic standard inverted indexing algorithms. Our approach shows great promise for advancing the state-of-the-art in exciting new directions.
However, several challenges must be overcome before these algorithms can reach widespread acceptance. For instance, recent work has dramatically reduced the space required for self-indexing algorithms, there are still opportunities to further reduce space usage in self-indexes. Another shortcoming of bag-of-words querying with self-indexing algorithms is providing top-k guarantees. While good solutions exist for providing top-k guarantees on singleton pattern queries, optimally merging multiple queries remains problematic.
However, self-indexing algorithms can also efficiently provide functionality that is notoriously inefficient, and sometimes even impossible, using inverted indexes. In addition to basic bag-of-words queries, our approach has the capability to perform phrase queries of any length, as well as the ability to support complex statistical calculations at query time, with no additional indexing costs. In fact, phrase queries were shown to be significantly faster using FM-GREEDY than when using inverted indexing approaches in prior work [10]. Self-indexes also inherently preserve term proximity. So, not only can each term be found quickly, but the nterms surrounding the keyword can quickly be extracted, tabulated, and used for on-the-fly statistical calculations. Applications of this functionality include more efficient relevance feedback algorithms, construction of higher order language models in ranking metrics, or term dependency extraction and query expansion. In summary, all of the disadvantages outlined in this paper for self-indexing paper warrant further research, as the potential benefits of this new approach are compelling indeed.
In future work, we will explore new algorithmic approaches to reduce space usage, and to further improve efficiency for larger values of k. We also intend to investigate the combination of efficient phrase querying and proximity calculations to produce and evaluate novel ranking metrics. Finally, we will design and evaluate new approaches to support distributed in-memory query processing in order to scale our system to terabyte size collections.
9. ACKNOWLEDGMENTS
This work was supported in part by the Australian Research Council. We thank Gonzalo Navarro and Daniel Valenzuela for insightful discussions on the HSV method. We also thank Alistair Moffat for valuable feedback on a draft of this paper.
References
[1] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In SIGIR, pages 372≠379, 2006.

233

[2] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space ranking with effective early termination. In SIGIR, pages 35≠42, 2001.
[3] N. R. Brisaboa, A. FariÒa, G. Navarro, and M. F. Esteller. (S, C)-dense coding: An optimized compression code for natural language text databases. In SPIRE, volume 2857 of LNCS, pages 122≠136, 2003.
[4] A. Z. Broder, D. Carmel, H. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In CIKM, pages 426≠434, 2003.
[5] E. W. Brown. Fast evaluation of structured queries for information retrieval. In SIGIR, pages 30≠38, 1995.
[6] C. Buckley and A. F. Lewit. Optimization of inverted vector searches. In SIGIR, pages 97≠110, 1985.
[7] M. Burrows and D. J. Wheeler. A block-sorting lossless data compression algorithm. Technical Report 124, Digital Equipment Corporation, Palo Alto, California, May 1994.
[8] S. B¸ttcher, C. L. A. Clarke, and G. V. Cormack. Information Retrieval: Implementing and evaluating search engines. MIT Press, Cambridge, Massachusetts, 2010.
[9] J. S. Culpepper and A. Moffat. Enhanced byte codes with restricted prefix properties. In SPIRE, volume 3772 of LNCS, pages 1≠12, November 2005.
[10] J. S. Culpepper, G. Navarro, S. J. Puglisi, and A. Turpin. Top-k ranked document search in general text databases. In ESA, Part II, LNCS 6347, pages 194≠205, 2010.
[11] J. S. Culpepper, M. Yasukawa, and F. Scholer. Language independent ranked retrieval with NeWT. In ADCS, pages 18≠25, December 2011. See http://goanna.cs.rmit.edu. au/~e76763/publications/cys11- adcs.pdf.
[12] P. Ferragina and G. Manzini. Opportunistic data structures with applications. In FOCS, pages 390≠398, 2000.
[13] P. Ferragina and G. Manzini. Indexing compressed text. Journal of the ACM, 52(4):552≠581, 2005. A preliminary version appeared in FOCS 2000.
[14] P. Ferragina, R. Giancarlo, and G. Manzini. The myriad virtues of wavelet trees. Information and Computation, 207:849≠866, 2009.
[15] P. Ferragina, R. Gonza·lez, G. Navarro, and R. Venturini. Compressed text indexes: From theory to practice. Journal of Experimental Algorithmics, 13:12.1≠12.31, 2009.
[16] J. Fischer and V. Heun. Space-efficient preprocessing schemes for range minimum queries on static arrays. SIAM Journal on Computing, 40(2):465≠492, 2011.
[17] M. Fontoura, V. Josifovski, J. Liu, S. Venkatesan, X. Zhu, and J. Zien. Evaluation strategies for top-k queries over memory-resident inverted indexes. Proceedings of the VLDB Endowment, 4(12):1213≠1224, 2011.
[18] T. Gagie, S. J. Puglisi, and A. Turpin. Range quantile queries: Another virtue of wavelet trees. In SPIRE, pages 1≠6, 2009.
[19] T. Gagie, G. Navarro, and S. Puglisi. New algorithms on wavelet trees and applications to information retrieval. Theoretical Computer Science, 426:25≠41, 2012.
[20] R. Grossi, A. Gupta, and J. S. Vitter. Higher-order entropy-compressed text indexes. In SODA, pages 841≠850, 2003.

[21] D. Hawking. Overview of the TREC-9 web track. In TREC-8, pages 87≠102, 1999.
[22] W.-K. Hon, R. Shah, and J. S. Vitter. Space-efficient framework for top-k string retrieval problems. In FOCS, pages 713≠722, 2009.
[23] G. Jacobson. Succinct static data structures. PhD thesis, Carnegie Mellon University, 1988.
[24] V. M‰kinen and G. Navarro. Implicit compression boosting with applications to self-indexing. In SPIRE, LNCS 4726, pages 229≠241, 2007.
[25] U. Manber and E. W. Myers. Suffix arrays: A new method for on-line string searches. SIAM J. Comp, 22(5):935≠948, 1993.
[26] G. Manzini. An analysis of the Burrows-Wheeler transform. Journal of the ACM, 48(3):407≠430, May 2001.
[27] S. Mithukrishnan. Efficient algorithms for document retrieval problems. In SODA, pages 657≠666, 2002.
[28] A. Moffat and V. N. Anh. Binary codes for non-uniform sources. In DCC, pages 133≠142, 2005.
[29] A. Moffat and J. Zobel. Self indexing inverted files for fast text retrieval. ACM TOIS, 14(4):349≠379, 1996.
[30] G. Navarro and V. M‰kinen. Compressed full-text indexes. ACM Computing Surveys, 39(1):2≠1 ≠ 2≠61, 2007.
[31] G. Navarro and D. Valenzuela. Space-efficient top-k document retrieval. In SEA, LNCS 7276, pages 307≠319, 2012.
[32] M. Persin. Document filtering for fast ranking. In SIGIR, pages 339≠348, 1994.
[33] M. Persin, J. Zobel, and R. Sacks-Davis. Filtered document retrieval with frequency sorted indexes. JASIST, 47(10):749≠764, 1996.
[34] S. J. Puglisi, W. F. Smyth, and A. H. Turpin. A taxonomy of suffix array construction algorithms. ACM Comp. Surv., 39(2):4.1≠4.31, 2007.
[35] R. Raman, V. Raman, and S. S. Rao. Succinct indexable dictionaries with applications to encoding k-ary trees and multisets. In SODA, pages 233≠242, 2002.
[36] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In TREC-3, 1994.
[37] K. Sadakane. Succinct data structures for flexible text retrieval systems. J. Discr. Alg., 5(1):12≠22, 2007.
[38] T. Strohman, H. Turtle, and W. B. Croft. Optimization strategies for complex queries. In SIGIR, pages 219≠225, 2005.
[39] A. Trotman. Compressing inverted files. Information Retrieval, 6(1): 5≠19, 2003.
[40] H. Turtle and J. Flood. Query evaluation: strategies and optimizations. Information Processing and Management, 31(6): 831≠850, 1995.
[41] N. V‰lim‰ki and V. M‰kinen. Space-efficient algorithms for document retrieval. In CPM, LNCS 4580, pages 205≠215, 2007.
[42] E. M. Voorhees and D. K. Harman. Overview of the Eighth Text REtrieval Conference (TREC-8). In TREC-8, pages 1≠24, 1999.
[43] H. Yan, S. Ding, and T. Suel. Compressing term positions in web indexes. In SIGIR, pages 147≠154, 2009.
[44] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Comp. Surv., 38(2):6≠1 ≠ 6≠56, 2006.
[45] J. Zobel, A. Moffat, and K. Ramamohanarao. Inverted files versus signature files for text indexing. ACM TODS, 23(4):453≠490, 1998.

234

To Index or not to Index: Time-Space Trade-Offs in Search Engines with Positional Ranking Functions

Diego Arroyuelo
Dept. of Informatics, Univ. TÈcnica F. Santa MarÌa Yahoo! Labs Santiago, Chile
darroyue@inf.utfsm.cl

SenÈn Gonz·lez 
University of Chile. Yahoo! Labs Santiago, Chile
sgonzale@dcc.uchile.cl

Mauricio Marin
University of Santiago, Chile Yahoo! Labs Santiago, Chile
mmarin@yahoo-inc.com

Mauricio Oyarz˙n
University of Santiago, Chile Yahoo! Labs Santiago, Chile
mauricio.silvaoy@usach.cl

Torsten Suel ß
CSE Department Polytechnic Institute of NYU
Brooklyn, NY, 11201
suel@poly.edu

ABSTRACT
Positional ranking functions, widely used in web search engines, improve result quality by exploiting the positions of the query terms within documents. However, it is well known that positional indexes demand large amounts of extra space, typically about three times the space of a basic nonpositional index. Textual data, on the other hand, is needed to produce text snippets. In this paper, we study time-space tradeoffs for search engines with positional ranking functions and text snippet generation. We consider both index-based and non-index based alternatives for positional data. We aim to answer the question of whether one should index positional data or not.
We show that there is a wide range of practical time-space trade-offs. Moreover, we show that both position and textual data can be stored using about 71% of the space used by traditional positional indexes, with a minor increase in query time. This yields considerable space savings and outperforms, both in space and time, recent alternatives from the literature. We also propose several efficient compressed text representations for snippet generation, which are able to use about half of the space of current state-of-the-art alternatives with little impact in query processing time.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.2.4 [Systems]: Textual databases
Partially funded by Fondecyt Grant 1-110066. CONICYT Thesis Work Support Code 78100003. Partially supported by FONDEF D09I1185. ßSupported by NFS Grants IIS-0803605 and IIS-1117829.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

General Terms
Algorithms, Experimentation, Performance
Keywords
Positional indexing, text compression, index compression, wavelet trees, snippet generation.
1. INTRODUCTION
Web search has become an important part of day-to-day life, affecting even the way in which people think and remember things [35]. Indeed, web search engines are one of the most important tools that give access to the huge amount of information stored in the web. The success of a web search engine mostly depends on its efficiency and the quality of its ranking function. To achieve efficient processing of queries, search engines use highly optimized data structures, including inverted indexes [6, 10, 25]. State-ofthe-art ranking functions, on the other hand, combine simple term-based ranking schemes such as BM25 [10], link-based methods such as Pagerank [7] or Hits [24], and up to several hundred other features derived from documents and search query logs.
Recent work has focused on positional ranking functions [32, 27, 36, 28, 33, 40, 10] that improve result quality by considering the positions of the query terms in the documents. Thus, documents where the query terms occur close to each other might be ranked higher, as this could indicate that the document is highly relevant for the query. To support such positional ranking, the search engine must have access to the position data. This is commonly done by building an index for all term positions within documents, called a positional index. The goal is to obtain an index that is efficient in terms of both index size and access time.
As shown in [32], positional ranking can be carried out in two phases. First, a simple term-based ranking scheme (such as BM25) defined over a Boolean filter is used to determine a set of high-scoring documents, say, the top 200 documents. In the second phase, the term positions are used to rerank these documents by refining their score values. (Additional third or fourth phases may be used to do further reranking according to hundreds of additional features [38], but

255

this is orthogonal to our work.) Once the final set of topscoring documents has been determined (say, the top 10), it is necessary to generate appropriate text snippets, typically text surrounding the term occurrences, to return as part of the result page. This requires access to the actual text in the indexed web pages. It is well known [40, 21] that storing position data requires a considerable amount of space, typically about 3 to 5 times the space of an inverted index storing only document identifiers and term frequencies. Furthermore, storing the documents for snippet generation requires significant additional space.
This paper focuses on alternative approaches to performing the above two-step document ranking process and the query snippet-generation phase. The aim is to optimize both space and query processing time. One important feature of position data is that it only needs to be accessed for a limited number of promising documents, say a few dozens or hundreds of documents. This access pattern differs from that for document identifiers and term frequencies, which are accessed more frequently, making access speed much more important. For position data, on the other hand, we could consider somewhat slower but smarter alternative representations without losing too much efficiency at query time [40].
In this paper, we push this idea further and consider not storing the position data (i.e, the positional index) at all. Instead, we compute positions on the fly from a compressed representation of the text collection. We will study two alternative approaches to compressing the text collection: (1) wavelet trees [23], which are succinct data structures from the combinatorial pattern matching community, and (2) compressed document representations that support fast extraction of arbitrary documents. It has been shown that, compared to positional indexes, web-scale texts can often be compressed in much less space [21]. More importantly, these representations can be used for both positional reranking and snippet generation. One concern is how these alternatives impact query processing speed, and thus we will study the resulting trade-offs between running time and space requirement.
Thus, to index or not to index position data, that is the research question that we hope to answer in this paper. To our knowledge, such alternative approaches for implementing positional ranking functions have not been rigorously compared before. Our main result is that we can store all the information needed for query processing (i.e., document identifiers, term frequencies, position data, and text) using space close to that of state-of-the-art positional indexes (which only store position data and thus cannot be used for snippet creation), with only a minor increase in query processing time. Thus, we provide new alternatives for practical compression of position and text data, outperforming recent approaches in [34].
Following current practice in search engines [21, 14], we assume a scenario where there is enough space to maintain index data structures completely in main memory, in compressed form. In this scenario, large text collections are usually partitioned over a number of nodes in a cluster, such that each partition fits into the memory of its node. This paper focuses on how to organize data within each partition, as also assumed in previous work such as [21, 14].
2. BACKGROUND AND RELATED WORK
Let D = {D1, . . . , DN } be a document collection of size N ,

where each document is represented as a sequence Di[1..ni] of ni terms from a vocabulary  = {1, . . . , V }. Notice that every term is represented by an integer, hence the documents are just arrays of integers. We also identify each document Di with a unique document identifier (docID) i. Given a term t   and a document Di  D, the in-document positions of t in Di is the set {j|Di[j] = t}.
Throughout this paper, we assume that all term separators (like spaces, `,', `;', `.', etc.) have been removed from the text. Also, we assume that all terms in the vocabulary have been represented in a case-insensitive way. This is in order to facilitate the search operations that we need to carry out over the documents in order to compute (on the fly) the positions of a given query term. To be able to retrieve the original text (with separators and the original case) one can use the presentation layer introduced by Farin~a et al. [17, Section 4]. This also supports removing stopwords and the use of stemming, among other vocabulary techniques. This extra layer requires extra space on top of that of the compressed text, as well as extra time to obtain the original text. However, this scheme must be used on all the alternatives that we consider in this paper, and thus we disregard the overhead introduced by the presentation layer and focus only on the low-level details of compression (but keeping in mind that the original text can still be retrieved).
2.1 Inverted Index Compression
The efficiency of query processing in search engines relies on inverted indexes. These data structures store a set of inverted lists I1, . . . , IV , which are accessed through a vocabulary table. The list It maintains a posting for each document containing the term t  . Usually, a posting in an inverted list consists of a docID, a term frequency, and the in-document positions of the term. (In real systems, the docIDs, term frequencies and in-document positions are often stored separately.) Indexes whose postings store indocument positions are called positional inverted indexes.
We assume that an inverted list It is divided into blocks of 128 documents each -- the particular choice of 128 documents per block is an implementation issue. Given a block of It, the term-position data for all the documents in that block are stored in a separate block of variable size. The inverted lists of the query terms are used to produce the result for the query. Since the query results are usually large, the result set must be ranked by relevance.
For large document collections, the data stored in inverted indexes requires considerable amounts of space. Hence, the indexes must be compressed. To support efficient query processing (such as DAAT [10], WAND [9] or BMW OR [15]) and effective compression in the inverted lists, we sort them by increasing docID. Let dt[1..|It|] denote the sorted list of docIDs for the inverted list It. Then, we replace dt[1] with dt[1] - 1, and dt[i] with dt[i] - dt[i - 1] - 1 for i = 2, . . . , |It|. In the case of frequencies, every fi is replaced with fi - 1, since fi > 0 always holds. For the positions, each pi,j is replaced with pi,j - pi,j-1 - 1. Then these values are encoded with integer compression schemes that take advantage of the resulting smaller integers.
There has been a lot of progress on compressing docIDs and frequencies, with many compression methods available [41, 10]. Some of them achieve, in general, a very good compression ratio, but at the expense of a lower decompression speed [10], for example Elias  and  encodings

256

[16], or Golomb/Rice parametric encodings [22], interpolative encoding [30]. Other methods achieve a (slightly) lower compression ratio, though with much higher decompression speed, for example VByte [39], S9 [3], and PForDelta [42], among others [10]. The best compression method depends on the scenario at hand.
2.2 Positional Indexes
Unfortunately, the scenario is not the same for compressing term positions, which is a problem where it has been difficult to make much progress. For instance, previous work [40] concludes that term positions in the documents do not follow simple distributions that could be used to improve compression (as is the case of, for instance, docIDs and frequencies). As a result, a positional index is about 3 to 5 times larger than a docID/frequency index, and becomes a bottleneck in index compression. Another important conclusion from [40] is that we may only have to access a limited amount of position data per query, and thus it might be preferable to use a method that compresses well even if its speed is slightly lower.
Positions in inverted indexes are used mainly in two applications, phrase searching and positional ranking schemes. In this paper we study positional ranking, where the positions of the query terms within the documents are used to improve the performance of a standard ranking such as BM25. The rationale is that documents where the query terms appear close together could be more relevant for the query, so they should get a better score. Although we focus only on positional ranking functions, the compression schemes used in this paper allow for phrase searching as well. This scenario is left for future work.
A recent work on positional indexing is that of Shan et al [34]. They propose to use the flat position indexes [11, 14] as an alternative of positional inverted indexes. The result is that docIDs, term frequencies and position data can be stored in space close to that of positional inverted lists, yielding a reduction of space usage. However, this index does not store the text, which makes it less suitable in scenarios where text snippets must be generated.
2.3 Snippet Generation
Besides providing a ranking of the most relevant documents for a query, search engines must show query snippets and support accessing the "in-cache" version of the documents. Each snippet shows a portion of the result document, in order to help the user judge its likely relevance before accessing it. Turpin et al. [37] introduce a method to compress the text collection and support fast text extraction to generate snippets. However, to achieve fast extraction, they must use a compression scheme that uses more space than usual compressors. In a more recent work, Ferragina and Manzini [21] study how to store very large text collections in compressed form, such that the documents can be accessed when needed, and show how different compressors behave in such a scenario. One of their main concerns was how compressors can capture redundancies that arise very far apart in very long texts. Their results show that such large texts can often be compressed to just 5% of their original size.
2.4 Compressed Text Self-Indexes
Succinct or compressed data structures use as little space as possible to support operations as efficiently as possible.

Thus, large data sets (like graphs, trees, and text collections)

can be manipulated in main memory, avoiding the secondary

storage. In particular, we are interested in compressed data

structures for text sequences. A compressed self-index is a

data structure that represents a text in compressed space,

supports indexed search capabilities on the text, and is able

to obtain any text substring efficiently [31]. It can be seen

as compression tools with indexed search capabilities.

Given a sequence T [1..n] over an alphabet  = {1, . . . , V },

we define operation rankc(T, i), for c  , as the number of

occurrences of c in T [1..i]. Operation selectc(T, j) is defined

as the position of the j-th occurrence of c in T . A wavelet

tree [23] (WT for short) is a succinct data structure that sup-

ports rank and select operations, among many virtues [19].

A WT representing a text T is a balanced binary search tree where each node v represents a contiguous interval v =

[i..j] of the sorted set . The tree root represents the whole

vocabulary. v is divided at node v into two subsets, such

that

the

left

child

vl

of

v

represents

vl

=

[i..

i+j 2

],

and

the

right child vr

represents vr

=

[

i+j 2

+ 1..j].

Each tree leaf

represents a single vocabulary term. Hence, there are V

leaves and the tree has height O(log V ). For simplicity, in

the following we assume that V is a power of two. Let T v be the subsequence of T formed by the symbols
in v. Hence, T root = T . Node v stores a bit sequence Bv such that Bv[l] = 1 if T v[l]  vr , and Bv[l] = 0 otherwise. Given a WT node v of depth i, Bv[j] = 1 iff the i-th mostsignificant bit in the encoding of T v[j] is 1. In this way,

given a term c  , the corresponding leaf in the tree can

be found by using the binary encoding of c. Every node v stores Bv augmented with a data structure for rank/select

over bit sequences [31]. The number of bits of the vectors Bv stored at each tree level sum up to n, and including the

data structure every level requires n + o(n) bits. Thus, the

overall space is n log V + o(n log V ) bits [23, 31].

Since a WT replaces the text it represents, we must be able

to retrieve T [i], for 1  i  n. The idea is to navigate the

tree from the root to the leaf corresponding to the unknown T [i]. To do so, we start from the root, and check if Broot[i] =

0. If so, the leaf of T [i] is contained in the left subtree vl

of the root. Hence, we move to vl looking for the symbol at position rank0(Broot, i). Otherwise, we move to vr looking for the symbol at position rank1(Broot, i). This process is

repeated recursively, until we reach the leaf of T [i], and runs

in O(log V ) time as we can implement the rank operation

on bit vectors in constant time. To compute rankc(T, i), for

any c  , we proceed mostly as before, using the binary

encoding of c to find the corresponding tree leaf. On the

other hand, to support selectc(T, j), for any c  , we must

navigate the upward path from the leaf corresponding to

term c. Both operations can be implemented in O(log V )

time; see [31] for details.

The space required by a WT is, in practice, about 1.1≠1.2

times the space of the text [12]. In our application to IR,

this would produce an index larger than the text itself, which

is excessive. To achieve compression, we can generate the

Huffman codes for the terms in  (this is a word-oriented

Huffman coding [29]) and use these codes to determine the

corresponding tree leaf for each term. Hence, the tree is not

balanced anymore, but has a Huffman tree shape [12] such

that frequent terms will be closer to the tree root than less

frequent ones. This achieves a total space of n(H0(T ) + 1) +

o(n(H0(T ) + 1)) bits, where H0(T )  log V is the zero-order

257

empirical entropy of T [26]. In practice, the space is about 0.6 to 0.7 times the text size [12]. However, notice that we have no good worst-case bounds for the operations, as the length of the longest Huffman code assigned to a symbol could be O(V ).
2.5 Self-Indexes for IR Applications
There have been some recent attempts to apply alternative indexing techniques, such as self-indexes, in large-scale IR systems. In particular, we mention the work by Brisaboa et al. [8] and Arroyuelo et al. [5, 4]. The former [8] concludes that WT are competitive when compared with an inverted index for finding all the occurrences of a given query term within a single text. The latter [5, 4] extends [8] by supporting more IR-like operations on a WT. The result is that a WT can represent a document collection using n(H0 (T ) + 1) + o(n(H0(T ) + 1)) bits while supporting all the functionality of an inverted index. The experimental results in [5] compare with an inverted index storing just docIDs, which of course yields a smaller index. However, WTs also store extra information, such as the term frequencies and, most important for us here, the compressed text and thus the term-position data.
Recent work [21] also tried to use (among other alternatives) a compressed self-index to compress web-scale texts, in order to allow decompression of arbitrary documents. Their conclusion is that compressed self-indexes still need a lot of progress in order to be competitive with standard compression tools, both in compression ratio and decompression speed. A contribution of our present work is a compressed self-index that is able to store web-scale texts and is competitive with the best state-of-the-art compressors. We think that this is a step forward in closing the gap between theory and practice in this area [20].
3. CONTRIBUTIONS
In this paper we study what are the best ways to organize in-document positions and textual data, in order to efficiently support positional ranking and snippet generation in text search engines. One of our main conclusions is that some compressed representations of the textual data -- which are needed to support snippet generation -- can also be used to efficiently obtain the term positions needed by positional ranking methods. Therefore, no positional index is needed in many cases, thus saving considerable space at little cost in terms of running time.
Our main contributions can be summarized as follows:
1. A study of several trade-offs for compressing position data. Rather than storing a positional index, we propose to compute the term positions from a compressed representation of the text. We explore and propose several compression alternatives. Our results significantly enhance current trade-offs between running time and memory space usage, enabling in this way more design choices for web search engines. One of our most interesting results is that both position and textual data can be stored in about 71% the space of current positional inverted indexes.
2. A study of several alternatives for compressing textual data, extending the alternatives studied in previous work [21]. In particular, we show that using the scheme

in [37] (to compress text and support efficient snippet generation) before using a standard compressor yields good time-space trade-offs, extending the alternatives introduced in [18]. It is important to note that variants of the scheme in [37] have been adopted by some commercial search engines, which makes our results of practical interest.
3. We propose several improvements over wavelet trees [23], in order to make them competitive for representing document collections. We show how to improve the compression ratio by compressing the sequence associated to every node of the tree with standard compressors. The result is a practical web-scale compressed self-index that is competitive with the best state-ofthe-art compressors.
4. COMPRESSING TERM-POSITION AND TEXTUAL DATA
Compressing in-document positions, i.e., the positions where each term occur within a document, has been recognized as a difficult task [40, 21]. Indeed, positions have become a bottleneck for compression compared to docIDs and frequencies. Moreover, recent work shows that the textual data can be compressed better than positions [21]. There are two main reasons for this. First, positions have a different distribution than docIDs and frequencies [40]. Second, since positions are stored separately for each term (recall Section 2.1), the local context for terms that is exploited by text compression schemes is not available in the positional inverted lists. Usually, positions require about 3 to 5 times the space of an inverted index storing docIDs and frequencies. Thus, efficient compression of in-document positions is an important challenge.
Positional inverted indexes are the standard solution to this problem [25, 10, 6]. In particular, [40] shows a detailed study of alternative ways to compress positional inverted indexes. However, it is not clear that using the methods in [40] is the best one can do. Notice that the in-document position data can be obtained (at query time) by searching for the query terms in the documents -- a simple scan of the document suffices. Since textual data can be compressed much better than positions, this could decrease the space usage of positions. However, the question is how this impacts query performance. We investigate this issue in this paper. We assume that positions are used to support positional ranking as described in [10, 40].
Another important issue in web search engines is the ability to generate snippets for the query results that allow users to decide which of the result documents they should visit. In this context, snippets have been shown to improve the effectiveness of search engines. To provide snippets, a search engine must store a (simplified) version of the documents in the collection. In the case of web search engines, this means the entire textual web, which requires a considerable amount of resources. Thus, the textual data must be compressed [21].
4.1 Basic Query Processing Steps for Positional Ranking and Snippet Extraction
From now on we assume a search engine where positional ranking is used to score documents, and where snippets must

258

be generated for the top-scoring documents. Thus, solving a query involves the following steps:
1. Query Processing Step: Given a user query, use an inverted index to get the top-k1 documents according to some standard ranking function (e.g., BM25).
2. Positional Ranking Step: Given the top-k1 documents from the previous step, get the positions of the query terms within these documents. Then rerank the results using a positional ranking functions [10, 40].
3. Snippet Generation Step: After the re-ranking of previous step, get snippets of length s for the top-k2 documents, for a given k2  k1.
For instance, k1 = 200 (as in [40]) and k2 = 10 (as in most commercial search engines) are typical values for them. We assume s = 10 in this paper. The different values for these parameters should be chosen according to the trade-off between query time and search effectiveness that we want to achieve. Step 2 is usually supported by a positional inverted index [25, 10, 40]. Step 3 is supported by compressing the document collection and supporting the extraction of arbitrary documents. Our focus here is on alternative ways to implement the last two steps.
4.2 The Baseline: Positional Inverted Lists and Compressed Textual Data
This section describes and evaluates baseline approaches to support term positions indexing and snippet extraction.
4.2.1 Positional Inverted Lists
Positional inverted lists (PILs, for short) are the standard approach for indexing in-document position data in search engines [25, 10, 6]. In particular, we assume the representation explained in Section 2.1. To obtain position data at query time, given the docIDs of the top-k1 results for a given query, we identify the PIL blocks containing the desired positional index entries. Then these blocks are fully decompressed, and the corresponding positions are obtained. A drawback here is that we need to decompress the entire PIL block, even if we only need a single entry in it. Thus, we might end up decompressing, in the worst case, k1 blocks in each of the inverted lists involved in the query. Afterwards, these positions are used to rerank the top-k1 documents, as in [40].
The access pattern for position data is much sparser than that for docIDs and frequencies, since positions must be obtained only for the top-k1 documents. Thus, just a few positions are decompressed from the PIL in each query. Given this sparse access pattern and the high space requirement of positions (as discussed above), it is better to use compression methods with a good compression ratio, like Golomb/Rice compression. These are slower to decompress, yet the fact that only a few positions are decompressed should not impact in the overall query running time.
4.2.2 Compressed Textual Data
To compress the text collection and support decompressing arbitrary documents, a simple alternative that is used by several state-of-the-art search engines -- for instance Lucene [13] -- is to divide the whole collection into smaller text blocks, which are then compressed separately. The block

size offers a time-space trade-off: larger blocks yield better compression, although decompression time is increased. Given the popularity [13, 21] and simplicity of this approach, we use it as the baseline for the compressed text.
4.2.3 Baseline Experiments
Experimental Setup.
We show now experiments for the baseline approaches. For this we use an HP ProLiant DL380 G7 (589152-001) server, with a Quadcore Intel(R) Xeon(R) CPU E5620 @ 2.40GHz processor, with 128KB of L1 cache, 1MB of L2 cache, 2MB of L3 cache, and 96GB of RAM, running version 2.6.34.8-68.fc13.i686.PAE of Linux kernel.
We use the TREC GOV2 collection, with about 25.2 million documents and about 32.86 million terms in the vocabulary. We work just with the text content of the collection (that is, we ignore the html code from the documents). This requires about 127GB in ASCII format. When we represent the terms as integers, the resulting text uses 91,634 MB. We use a subset of 10,000 random queries from the query log provided with the TREC GOV2 collection. All methods were implemented using C++, and compiled with g++ 4.4.5, with the full optimization flag -O5.
Experiments for Step 1.
In Table 1 we show the average query time (in milliseconds per query) for the initial query processing step (Step 1 of Section 4.1). We show results for two types of queries: traditional AND queries (using DAAT query processing and BM25 ranking) and the BMW OR approach from [15], which is one of the most efficient current solutions for disjunctive queries (using a two-layer approach, which yields slightly faster query times [15]). The index for docIDs and frequencies required 9,739 MB of space, using PForDelta compression for docIDs and S16 for frequencies. Notice that the

Table 1: Experimental results for the initial query processing step (Step 1) for AND and OR queries.

top-k1 DAAT AND (ms/q) BMW OR [15] (ms/q)

50

14.75

100

14.77

150

14.80

200

14.81

300

14.81

35.70 43.39 47.90 51.74 58.19

query time for AND is almost constant (within two decimal digits) with respect to k1. The process to obtain the topk1 documents uses a heap (of size k1). However, operating the heap takes negligible time, compared to the decompression of docIDs and the DAAT process. BMW OR, on the other hand, is an early-termination technique, and thus k1 impacts the query time.
Experiments for Step 2.
In Table 2 (first two rows) we show experimental results for obtaining positions with the baseline PILs, using two compression schemes: Rice and S16, which offer the most interesting trade-offs [40]. We also show query times for different values of k1, namely 50, 100, 150, 200 and 300 (the experiments in [40] only use k1 = 200). As we can see, Rice

259

requires only about 90% the space of S16, but takes twice as much time. Comparing the query times of Step 2 for Rice and S16 with the query times of Step 1 in Table 1, we can see that position extraction is a small fraction of the overall time. Hence, we can use Rice to compress PILs and obtain a better space usage with only a minor increase in query time. For Rice, PILs use 2.91 times the space of the inverted index that stores docIDs and frequencies. For S16, this number is 3.22.
Experiments for Step 3.
Table 3 shows experimental results for the baseline for compressed textual data. Just as in [21], we divide the text into blocks of 0.2MB, 0.5MB or 1.0MB, and compress each block using different standard text compression tools. In particular, we show results for lzma (which gives very good results in [21]) and Google's snappy compressor [1], which is an LZ77 compressor that is optimized for speed rather than compression ratio. These two compressors offer the most interesting trade-offs among the alternatives we tried. As it can be seen, lzma achieves much better compression ratios than snappy. The compressed space achieved for the whole text is 8,133 MB for lzma and 27,388 MB for snappy.
The differences in extraction time are also considerable, with snappy being much faster. Note that [21] reports a decompression speed of about 35MB/sec for lzma. However, to obtain a given document we must first decompress the entire block that contains it. Hence, most of the 35MB per second do not correspond to any useful data. In other words, this does not measure effective decompression speed for our scenario, and thus we report per-query times rather than MB/s for both methods.
4.3 Computing Positions and Snippets from the Compressed Document Collection
We explore next the alternative of obtaining position data directly from the compressed text. This implies that in Step 2 of the query process, k1 documents must be decompressed, rather than only k2 in Step 3, as in the baseline.
Using Standard Text Compressors.
Our first approach is to obtain positions using the baseline for generating snippets from Section 4.2.2. In rows 3 and 4 of Table 2 we show the time-space trade-offs for this approach, using lzma and snappy compressors, and blocks of size 0.2 MB. We conclude that using lzma we can store positions and text in about half the space of PIL(the latter just storing positions). However, this approach is two orders of magnitude slower than the positional index. If we use snappy instead, we obtain an index that is 21.86% larger than PIL (Rice), and running times to obtain positions that are about an order of magnitude slower (this might be acceptable in some cases). In the following, we try to improve on both of these techniques.
Zero-order Compressors with Fast Text Extraction.
An alternative to compressing the text that could support faster position lookups is the approach from Turpin et al. [37]. The idea is to first sort the vocabulary according to the term frequencies, and then assign term identifiers according to this order. In this way, the term identifier 0 corresponds to the most frequent term in the collection, 1

to the second most frequent term, and so on. The document collection is then represented as a single sequence of identifiers, where each term identifier is encoded using VByte [2]. Note that the 128 most frequent terms in the collection are thus encoded in a single byte. Actually, [37] uses a move-to-front strategy to store the encodings: the first time a term appears in a document, it is encoded with the original code assigned as before; the remaining appearances are represented as an offset to the previous occurrence of the term. We also use this approach in our experiments.
By using an integer compression scheme (such as VByte) for the text, we are able to decompress any text portion very efficiently (no text blocks are needed this time). Table 2 shows the resulting trade-offs for this alternative (see the row for "VByte"). Notice that we improve the query time significantly, making it competitive with PILs. The higher space usage is a concern, but note that we also represent the text within this space, not just the position data as in PILs. We also tried other compression schemes, such as PForDelta and S9, obtaining poorer compression ratios and similar decompression speed. The only method that improved the compression ratio is VNibble, a variant of VByte that represents any integer with a variable number of nibbles (i.e., half bytes). As in VByte, one bit of each nibble is used as a continuation bit, so only 3 bits of each nibble are used to represent the number. The results of Table 2 show space savings of about 10% over VByte. Also, notice that we are now able to use space close to that of snappy (with blocks of 0.2 MB), yet with a better query time.
The fast query time is due to two facts. First, methods like VByte and VNibble are able to decompress hundred of million integers (which in our case correspond to terms) per second [41]. Second, VByte and VNibble are able to decompress just the desired documents, without negative impact on compressed size. However, this is basically zeroorder compression, and hence we are still far from the space usage of, for instance, lzma. We address this next.
Natural-Language Compression Boosters.
To obtain higher-order compression, we propose to use a so-called natural-language compression booster [18]. The idea is to use first a zero-order compressor on the text, then this compressed text is further compressed using some standard compression scheme. It has been shown that this can yield better compression ratios than just just using a standard compression scheme [18] (especially for smaller block sizes). In our case, we propose using Turpin et al's approach [37] as booster (using VByte and VNibble as we explained above) on the sequence of term identifiers, rather than Word Huffman or End-Tagged as in [18]. Our experiments indicate that the former are faster and use only slightly more space than the latter.
In Table 2 we show the trade-offs for this approach (see the rows for approach "Compression Boosters"). We show results for blocks of size 0.001, 0.01, 0.05, and 0.2 MB of VByte and VNibble compressed text. Overall, the reduction in space usage (compared to the original VByte approach) is considerable. Compared to lzma (0.2 MB), the result is a reduction in space usage of 16.68% (12,486 MB vs 14,987 MB), but at the cost of twice the running time as the original lzma. When using smaller blocks, however, the time to obtain positions rapidly improves, while the size does not increase too much. For snappy, on the other hand, we obtain a

260

Approach
Positional indexes Text compressors Zero-order Compressors Compression Boosters
Compressed self-indexes

Table 2: Experimental results for extracting term-position data (Step 2).

Compression Scheme

Space Usage

Position extraction time (ms/q)

(MB)

k1 = 50 k1 = 100 k1 = 150 k1 = 200 k1 = 300

PIL(Rice) PIL(S16)

28,373

1.28

2.22

3.05

3.27

5.57

31,338

0.74

1.12

1.43

1.75

2.51

lzma (0.2 MB) snappy (0.2 MB)

14,987 34,576

137.60 260.36

9.47

18.00

375.21 25.95

482.09 33.49

684.94 47.74

VByte VNibble

38,339

0.95

1.91

2.86

3.81

5.72

34,570

1.86

3.71

5.57

6.75

8.10

VByte + lzma (0.2 MB) VByte + lzma (0.05 MB) VByte + lzma (0.01 MB) VByte + lzma (0.001 MB)

12,486 13,981 16,762 22,340

256.16 70.32 19.26 6.11

484.35 133.18 36.51 11.60

716.41 192.09 52.67 16.80

906.54 246.94 68.00 21.72

1,284.87 351.71 97.04 31.10

VByte + snappy (0.2 MB) VByte + snappy (0.05 MB) VByte + snappy (0.01 MB) VByte + snappy (0.001 MB)

20,158 20,366 22,086 27,919

9.71

18.86

26.41

34.01

48.69

2.36

4.48

6.47

8.36

11.95

0.82

1.56

2.25

2.91

4.17

0.45

0.86

1.24

1.60

2.30

WT(7 KB) WT(1 KB)

40,534

1.94

3.68

5.30

6.85

9.80

56,917

0.33

0.62

1.04

1.15

1.75

WT(7 KB) + lzma WT(1 KB) + lzma

19,628 42,359

19.25 7.22

36.59 13.54

52.83 19.44

68.36 24.97

97.71 35.57

WT(7 KB) + snappy WT(1 KB) + snappy

25,122 46,778

14.35 2.07

23.76 3.61

39.38 5.88

51.02 7.32

74.56 10.47

Table 3: Experimental results for compressing the document collection (Step 3).

Compressor Block size Space usage Compression Ratio Snippet extraction time (ms/q)

(MB)

(MB)

k2 = 10 k2 = 30 k2 = 50

lzma

0.2

14,987

0.5

13,489

1.0

12,682

16.35 14.72 13.84

29

84

136

63

181

292

117

335

540

snappy

0.2

34,576

0.5

34,426

1.0

34,390

37.73 37.57 37.53

2

6

9

5

14

23

10

28

46

reduction of 41.69% in space for blocks of size 0.2 MB, with a very minor increase in query time. When we reduce the block size to 0.05 MB, the query time improves even more, and becomes competitive with the time to obtain positions from PIL (Rice). We note that using more advanced techniques from [40] we could obtain about 21 to 22 GB of space for PIL, making both techniques competitive in both space and time. However, VByte + snappy also contains the text within this space, allowing for use during snippet generation. Thus, we are able to store both text and positions in a representation that uses less space than PIL, which stores only positions.
4.4 A Compressed Self-Index for Positions and Snippets
Let T be the text obtained from the concatenation (in arbitrary order) of the documents in the collection. We represent T with a WT to obtain term positions and text snippets. Given a position i in T , one can easily obtain both the docID of the document that contains T [i] and the starting

position of a given document j by means of operations rank and select [5], assuming a table of document lengths.
Byte-Oriented Huffman WT.
Instead of a bit-oriented WT (as the ones explained in Section 2.4), we use the byte-oriented representation from [8], using the Plain Huffman encoder, which is the most efficient alternative reported in there. The idea is to first assign a Huffman code to each vocabulary term [29]. Then, we store the most significant byte of the encoding of each term in array Broot. That is, each WT node v stores an array of bytes Bv, instead of bit arrays as in Section 2.4. Next, each term in the text is assigned to one of the children of the root, depending on the first byte in the encodings. Notice that in this way the WT is 256-ary. See [8] for details.
To support rank and select, we use the simple approach from [8]. Given a WT node v, we divide the corresponding byte sequence Bv into superblocks of sb bytes each. For each superblock we store 256 superblock counters, one for each possible byte. These counters tell us how many occurrences

261

of a given byte there are in the text up to the last position of the previous superblock. Also, each superblock is divided into blocks of b bytes each. Every such block also stores 256 block counters, similarly as before. The difference is that the values of these counters are local to the superblock, hence less bits are used for them. To compute rankc(T, i), we first compute the superblock j that contains i, and use the superblock counter for c to count how many c there are in T up to superblock j - 1. Then we compute the block i that contains i and add (to the previous value) the block counter for c. Finally, we must count the number of c within block i . This is done with a sequential scan over block i . This block/superblock structure allows for time-space trade-offs. In our experiments we use sb = 216. Hence, superblock counters can be stored in 16 bits each. We consider b = 1 KB, b = 3 KB and b = 7 KB. Operation select is implemented by binary searching the superblock/block counters; thus no extra information is stored for this [8].
To obtain position data assume that, given docID i for a top-k1 document and a query term t, we want to obtain the positions of t within Di. A simple solution could be to extract document Di from the WT, and then search for t within it (as in Section 4.3). However, the decompression speed of a WT is much slower than that of the schemes used in Section 4.3, so we must use a more efficient way. An idea is to use operation select to find every occurrence of t within Di, hence working in time proportional to the number of occurrences of the term. Let d be the starting position for document Di in T . Hence, there are r  rankt(T, d) occurrences of t before document Di, and the first occurrence of t within Di is at position j  selectt(T, r + 1), the second occurrence at position j  selectt(T, r + 2), and so on. Overall, if o is the number of occurrences of t within Di, then we need 1 rank and o + 1 selects to find them.
In Table 2 we show the experimental trade-offs for WT, for the different block sizes tested. As it can be seen, WT (7 KB) requires space close to (though slightly larger than) that of the VByte approach. WT (3 KB) and WT (1 KB) obtain better times, but requiring even more space. Moreover, WT (7 KB) is slower than PIL (Rice) and uses more space. The WT, on the other hand, includes the textual data, but still this space usage could leave it out of competition. Next, we introduce extra improvements to make them more competitive.
Achieving Higher-Order Compression with the WT.
Basically, WTs are zero-order compressors, which explains their high space usage. To achieve higher-order compression, notice that Broot contains the most significant byte of the Huffman encodings of the original terms. Thus, the original text structure is at least partially preserved in the structure of Broot, which might thus be as compressible as the original text. A similar behavior can be observed in internal nodes. Thus, we propose to compress the blocks of Bv in each WT node v by using some standard compressor.
Table 2 shows results for lzma and snappy, the best compressors we tried. Notice that WT (7 KB) + lzma achieves 19,628 MB, almost half the space used by WT (7 KB). The time to obtain positions becomes, on the other hand, an order of magnitude larger. WT (7 KB) + snappy achieves slightly better times, but using more space. Also, WT (7 KB) + lzma uses slightly less space than VByte + snappy (0.2 MB), but is somewhat slower. Overall, this significant reduction in space could make WTcompetitive.

5. EXPERIMENTAL RESULTS
We now show the time-space trade-offs for the overall solution space we explored. We use here the same basic setup as in Section 4.2.3, with the same parameters (block sizes) for each alternative. We consider the most competitive indexing alternatives from previous sections for positions and snippet generation, described in Table 4. All results include the time and space of the inverted index to carry out Step 1 of the query process, as well as of all structures used in Steps 2 and 3.

Table 4: Glossary of the indexing schemes used in

Figure 1. All schemes include the inverted index.

Indexing Scheme Description

Scheme 1

WT for positions and text.

Scheme 2

WT compressed with lzma for

positions and text.

Scheme 3

WT compressed with snappy for

positions and text.

Scheme 4

Text compressed with VByte/

VNibble for positions and text.

Scheme 5

VByte compression booster on

snappy for positions and text.

Scheme 6

PIL (Rice) for positions and VByte/

VNibble for text.

Note that only Scheme 6 stores an index for position data. Figure 1 shows the different trade-offs for DAAT AND queries with BM25 ranking. Conclusions for OR queries are similar to that for AND. We only show results for k1 = 200 and k2  {10, 50}, which are representative of other values we tested.
As can be seen, Scheme 6, which uses PIL for positions and Turpin et al [37] for snippets, has one of the fastest query times among all alternatives, but space usage is high compared to other methods. This is because this scheme needs to store positions and text separately. The two points for Scheme 6 that are plotted correspond to using VByte (higher space usage) and VNibble.
Scheme 1 also offers a competitive query time (among the fastest alternatives), but still uses a considerable amount of space. The time-space trade-offs for the schemes that use WT are obtained for different block sizes within the WT nodes (1 to 7 KB). Scheme 2 and Scheme 3 compress the byte sequences of each WT node (as proposed in Section 4.4). As can be seen, the space usage is improved significantly, in some cases by a factor of two. However, query time degrades, making these alternatives less compelling.
Scheme 4 is very competitive in query time, but again its space usage is high. Scheme 5 corresponds to the compression boosters proposed in Section 4.3, and it obtains a very impressive trade-off. One of the most interesting settings is for blocks of size 0.05 MB. In this case, the overall space usage is 1.06 times the space of PIL (Rice), with a query time 1.21 times higher than Scheme 4 and 1.23 times higher than Scheme 6 (which uses PIL). For blocks of size 0.01 MB, Scheme 5 requires 1.12 times the space of PIL (Rice), with a query time that is 0.96 times the one of Scheme 4, and 0.98 the one of Scheme 6. Thus, using only slightly more space than PIL (Rice) (recall the results in Table 2), Scheme 5 includes everything needed for query processing: docIDs, frequencies, term positions, and the text needed to gener-

262

Overall query time (ms per query) Overall query time (ms per query)

Time-space trade-offs k1 = 200, k2 = 10

200

Scheme 1

Scheme 2

Scheme 3

Scheme 4

150

Scheme 5

Scheme 6

100

50

Time-space trade-offs k1 = 200, k2 = 50
250 200 150 100
50

0 25 30 35 40 45 50 55 60 65 70 75 80
Index size (GB)

0 25 30 35 40 45 50 55 60 65 70 75 80
Index size (GB)

Figure 1: Time-space trade-offs for the overall query process for the GOV2 collection, including positional ranking and snippet generation. The size of each alternative includes the size of inverted index with docIDs and frequencies, which for the TREC GOV2 collection requires 9,739MB.

ate snippets. This is one of the most important conclusions in this paper, that "not to index" can be a real alternative for positional data in practical scenarios. As stated in Section 4.3, the space usage of PILs can be reduced to about 21 GB≠22 GB for the TREC GOV2 collection [40]. However, we would still need to add the inverted index and the compressed text to that space in order to support all query processing steps.
Finally, the smallest space alternatives we tested (which are not shown in Figure 1) are the ones that use the inverted index for query processing and lzma compression for positions and snippets. This achieves about 22,225 MB of space. This scheme includes everything needed for query processing, and uses only 78% the space of PIL. However, query processing time increases significantly, to more than 400 ms per query. This scheme could be useful in some cases where the available memory space is very restricted, such that a larger index would mean going to disk.
A recent alternative [34] proposes to use flat positional indexes [11, 14] to support phrase querying; this index could also be used for positional ranking. This is basically a positional index from which docID and frequency information can also be obtained. The results reported for the GOV2 collection in [34] give an index of size 30,310 MB that includes docIDs and frequencies, but not the text needed for snippet generation, making this approach uncompetitive for our scenario.
6. CONCLUSIONS
From our study we can conclude that there exists a wide range of practical time-space trade-offs, other than just the classical positional inverted indexes. We studied several alternatives, trying to answer the question of whether it is necessary to index position data or not. As one of the most relevant points in the trade-off, we propose a compressed document representation based on the approach in [37] combined with Google's snappy compression [1]. This allows us to compute position and snippet data using less space than a standard positional inverted index that only stores position data. Even if we include the space used for document identifiers and term frequencies, this approach uses just 1.12

times the space of a positional inverted index, with the same or slightly better query time.
This means that in many practical cases, "not to index" position data may be the most efficient approach. This provides new practical alternatives for positional index compression, a problem that has been considered difficult to address in previous work [40, 21]. Finally, we also showed that compressed self-indexes such as wavelet trees [23] can be competitive with the best solutions in some scenarios.
7. REFERENCES
[1] http://code.google.com/p/snappy/. [2] V. N. Anh and A. Moffat. Compressed inverted files
with reduced decoding overheads. In Proc. of 21st Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 290≠297, 1998. [3] V. N. Anh and A. Moffat. Inverted index compression using word-aligned binary codes. Inf. Retr., 8(1):151≠166, 2005. [4] D. Arroyuelo, V. Gil-Costa, S. Gonz¥alez, M. Marin, and M. Oyarzu¥n. Distributed search based on self-indexed compressed text. Information Processing and Management, 2012. To appear. [5] D. Arroyuelo, S. Gonz¥alez, and M. Oyarzu¥n. Compressed self-indices supporting conjunctive queries on document collections. In SPIRE, LNCS 6393, pages 43≠54, 2010. [6] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval - the Concepts and Technology Behind Search, Second Edition. Pearson Education Ltd., Harlow, England, 2011. [7] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. J. of Computer Networks, 30(1≠7):107≠117, 1998. [8] N. Brisaboa, A. Farin~a, S. Ladra, and G. Navarro. Implicit indexing of natural language text by reorganizing bytecodes. Information Retrieval, 2012. To appear. [9] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level

263

retrieval process. In Proc. of 12th International Conference on Information and Knowledge Management, pages 426≠434. ACM, 2003. [10] S. Bu®ttcher, C. Clarke, and G. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, 2010. [11] C. Clarke, G. Cormack, and F. Burkowski. An algebra for structured text search and a framework for its implementation. Computer Journal, 38(1):43≠56, 1995.
[12] F. Claude and G. Navarro. Practical rank/select queries over arbitrary sequences. In SPIRE, LNCS 5280, pages 176≠187. Springer, 2008.
[13] D. Cutting. Apache Lucene. http://lucene.apache.org/.
[14] J. Dean. Challenges in building large-scale information retrieval systems: invited talk. In WSDM, page 1, 2009.
[15] S. Ding and T. Suel. Faster top-k document retrieval using block-max indexes. In Proc. of 34th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 993≠1002, 2011.
[16] P. Elias. Universal codeword sets and representations of the integers. IEEE Transactions on Information Theory, 21(2):194≠203, 1975.
[17] A. Farin~a, N. Brisaboa, G. Navarro, F. Claude, A. Places, and E. Rodr¥iguez. Word-based self-indexes for natural language text. ACM Transactions on Information Systems (TOIS), 30(1):article 1, 2012.
[18] A. Farin~a, G. Navarro, and J. Parama¥. Boosting text compression with word-based statistical encoding. Computer Journal, 55(1):111≠131, 2012.
[19] P. Ferragina, R. Giancarlo, and G. Manzini. The myriad virtues of wavelet trees. Information and Computation, 207(8):849≠866, 2009.
[20] P. Ferragina, R. Gonz¥alez, G. Navarro, and R. Venturini. Compressed text indexes: From theory to practice. ACM Journal of Experimental Algorithmics, 13, 2008.
[21] P. Ferragina and G. Manzini. On compressing the textual web. In WSDM, pages 391≠400, 2010.
[22] S. Golomb. Run-length encoding. IEEE Transactions on Information Theory, 12(3):399≠401, 1966.
[23] R. Grossi, A. Gupta, and J. S. Vitter. High-order entropy-compressed text indexes. In SODA, pages 841≠850, 2003.
[24] J. M. Kleinberg. Authoritative sources in a hyperlinked environment. J. of ACM, 46(5):604≠632, 1999.
[25] C. Manning, P. Raghavan, and H. Schu®tze. Introduction to Information Retrieval. Cambridge University Press, 2008.
[26] G. Manzini. An analysis of the Burrows-Wheeler transform. J. ACM, 48(3):407≠430, 2001.
[27] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proc. of 28th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2005.
[28] G. Mishne and M. Rijke. Boosting web retrieval through query operations. In Proc. of 27th European Conference on IR Research, 2005.

[29] A. Moffat. Word-based text compression. Software, Practice, and Experience, 19(2):185≠198, 1989.
[30] A. Moffat and L. Stuiver. Binary interpolative coding for effective index compression. Inf. Retr., 3(1):25≠47, 2000.
[31] G. Navarro and V. M®akinen. Compressed full-text indexes. ACM Computing Surveys, 39(1), 2007.
[32] Y. Rasolofo and J. Savoy. Term proximity scoring for keyword-based retrieval systems. In Proc. of 25th European Conference on IR Research, 2003.
[33] R. Schenkel, A. Broschart, S. Hwang, M. Theobald, and G. Weikum. Efficient text proximity search. In 14th String Processing and Information Retrieval Symposium, 2007.
[34] D. Shan, W. X. Zhao, J. He, R. Yan, H. Yan, and X. Li. Efficient phrase querying with flat position index. In CIKM, pages 2001≠2004, 2011.
[35] B. Sparrow, J. Liu, and M. Wegner. Google effects on memory: Cognitive consequences of having information at our fingerprints. Science, 333(6043):776≠778, 2011.
[36] T. Tao and C. Zhai. An exploration of proximity measures in information retrieval. In Proc. of 30th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, 2007.
[37] A. Turpin, Y. Tsegay, D. Hawking, and H. Williams. Fast generation of result snippets in web search. In Proc. of 30th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 127≠134, 2007.
[38] L. Wang, J. J. Lin, and D. Metzler. A cascade ranking model for efficient ranked retrieval. In Proc. of 34th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 105≠114, 2011.
[39] H. Williams and J. Zobel. Compressing integers for fast file access. Computer Journal, 42(3):193≠201, 1999.
[40] H. Yan, S. Ding, and T. Suel. Compressing term positions in web indexes. In Proc. of 32nd Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, pages 147≠154, 2009.
[41] H. Yan, S. Ding, and T. Suel. Inverted index compression and query processing with optimized document ordering. In WWW, pages 401≠410, 2009.
[42] M. Zukowski, S. H¥eman, N. Nes, and P. Boncz. Super-scalar ram-cpu cache compression. In ICDE, page 59, 2006.

264

Mining Query Subtopics from Search Log Data

Yunhua Hu
Microsoft Research Asia Beijing, China
yuhu@microsoft.com
Daxin Jiang
Microsoft Research Asia Beijing, China
djiang@microsoft.com

Yanan Qian

Hang Li

SPKLSTN Lab, Xi'an Jiaotong

Microsoft Research Asia

University, Xi'an, China

Beijing, China

yanan.qian@stu.xjtu.edu.cn hangli@microsoft.com

Jian Pei
Simon Fraser University Burnaby, BC, Canada
jpei@cs.edu.ca

Qinghua Zheng
SPKLSTN Lab, Xi'an Jiaotong University, Xi'an, China
qhzheng@mail.xjtu.edu.cn

ABSTRACT
Most queries in web search are ambiguous and multifaceted. Identifying the major senses and facets of queries from search log data, referred to as query subtopic mining in this paper, is a very important issue in web search. Through search log analysis, we show that there are two interesting phenomena of user behavior that can be leveraged to identify query subtopics, referred to as `one subtopic per search' and `subtopic clarification by keyword'. One subtopic per search means that if a user clicks multiple URLs in one query, then the clicked URLs tend to represent the same sense or facet. Subtopic clarification by keyword means that users often add an additional keyword or keywords to expand the query in order to clarify their search intent. Thus, the keywords tend to be indicative of the sense or facet. We propose a clustering algorithm that can effectively leverage the two phenomena to automatically mine the major subtopics of queries, where each subtopic is represented by a cluster containing a number of URLs and keywords. The mined subtopics of queries can be used in multiple tasks in web search and we evaluate them in aspects of the search result presentation such as clustering and re-ranking. We demonstrate that our clustering algorithm can effectively mine query subtopics with an F1 measure in the range of 0.896-0.956. Our experimental results show that the use of the subtopics mined by our approach can significantly improve the state-of-the-art methods used for search result clustering. Experimental results based on click data also show that the re-ranking of search result based on our method can significantly improve the efficiency of users' ability to find information.
Categories and Subject Descriptors
H.3 [INFORMATION STORAGE AND RETRIEVAL]: Information Search and Retrieval
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

General Terms
Algorithms, Experimentation
Keywords
Search Log Mining, User Behavior, Query Subtopics, Clustering, Search Result Clustering
1. INTRODUCTION
Understanding the search intent of users is essential for satisfying a user's search needs. How to best represent query intent is still an ongoing research problem. One consensus among the researchers is that the intents of queries can be characterized along multiple dimensions. The intents of a query can be represented by its search goals, such as informational, navigational, and transactional [7]. It can also be represented by semantic categories or topics [8, 23, 24]. Furthermore, it can be represented by subtopics (cf., [14]), denoting multiple senses or multiple facets of the query.
Most queries are ambiguous or multifaceted [14]. For example, `harry shum' is an ambiguous query, which may refer to an American actor, a vice president of Microsoft, or another person named Harry Shum. `Xbox' is a multifaceted query. When people search for `xbox', they may be looking for information on different facets of an Xbox, such as `online game', `homepage', and `marketplace'. Note that a query can be both ambiguous and multifaceted. The more frequent a query is, the more likely that it has multiple senses or facets. The major difference between the topics and subtopics of a query is that the former is more coarsegrained and related to other queries, while the latter is finegrained and is only about the query in question.
Identifying the major subtopics of a query is very important for many search tasks such as personalized search, query suggestion, and search result presentation including clustering, re-ranking, and diversification. In this paper, we aim to automatically mine the major subtopics (senses and facets) of queries from the search log data. Although there is some related work, the subtopic mining problem as defined in this paper does not seem to have been studied previously.
We performed a comprehensive study of the two phenomena, referred to as `one subtopic per search' (OSS) and `subtopic clarification by additional keyword' (SCAK) respectively. We show that we can mine the subtopics of queries from search log data by effectively using the phenomena. We then represent the senses or facets of a query

305

by a number of URLs and keywords. We observe that this can be done with high accuracy for head queries. Although the phenomena have been observed or mentioned in previous work (cf., [26]), there has not been a detailed study conducted on them, as far as we know.
If a user clicks multiple URLs after submitting a query (the multiple URLs are also referred to as multi-clicks in this paper), then the clicked URLs tend to represent the same subtopic, which is called one subtopic per search (OSS). Figure 1 shows an example of search results for the query `harry shum' on a web search engine. The result contains URLs about two Harry Shum's. The search log data indicates that URLs 1, 3, and 5 are usually clicked together in individual searches, and URLs 2 and 4 are clicked together. Each group of URLs actually corresponds to one sense, i.e., one subtopic. Gale and Church discovered a similar phenomenon in natural language texts, called `one sense per discourse' [19], which means an ambiguous word usually only has one sense in a specific context. Users often add ad-
Figure 1: Search result for query `Harry Shum'.
ditional keywords (in most cases, one additional keyword) to a query to expand the query in order to clarify its subtopic. This phenomenon is called subtopic clarification by additional keyword (SCAK). As a result, the URLs clicked after searching both with the original and the expanded queries tend to represent the same subtopic and the keyword also tends to be indicative of the subtopic. For example, people may submit `harry shum microsoft' as an expanded query to specify the subtopic. The URLs clicked in searches for both `harry shum' and `harry shum microsoft' usually represent the same subtopic, Harry Shum from Microsoft, and therefore `microsoft' becomes a keyword of the subtopic.
We employ a clustering algorithm to group URLs as well as keywords into clusters, where each cluster represents one subtopic of a query. For example, one cluster for query `harry shum' may contain the home page of the Microsoft Harry Shum, his wiki profile, and the keywords `microsoft' and `bing'. The clustering is performed based on the two phenomena of user behavior, as described above. Specifically, for each query and its expanded queries (the original query plus keywords), the related click log data is collected. Clustering is then carried out on all the clicked URLs. If two URLs are clicked together many times in the click data, then they are likely to be clustered together. If two URLs are clicked both under the original and expanded queries, then they are likely to be clustered together. We employ a special data structure consisting of a prefix tree and a suffix

tree to facilitate recursive execution of the clustering algorithm on the log data. In this way, the mining of subtopics can be conducted very efficiently.
We conducted experiments to measure the accuracy of our clustering method on a TREC data set and a new data set in which the subtopics of both queries are labeled. For ambiguous subtopics, our method can achieve a B-cubed F1 from 0.925 to 0.956. For multifaceted subtopics, our method can achieve a B-cubed F1 from 0.896 to 0.930 in terms of B-cubed F1. The results indicate that our method can effectively perform query subtopic mining.
We also evaluated our method in search result clustering. We compared our method with a state-of-the-art method of search result clustering. Experimental results show that our method can significantly improve accuracy. The improvements are 5.4% in terms of B-cubed precision and 6.1% in terms of B-cubed recall. When compared side-by-side with human generated results, we also find that our method is significantly better than the baseline.
We further evaluated our method on search result reranking, in which the user is confirmed with the subtopic she has in mind and re-ranking of search results based on user feedback is performed. We used the average click position in the log data as the evaluation measure. The results show that our method can boost the average click position 0.61 points higher, which implies a promising improvement in the user experience.
There are three major contributions in our work: 1. We have analyzed two phenomena in user search behavior that can be utilized to identify query subtopics. 2. We have developed a clustering method that can effectively and efficiently mine the subtopics on the basis of the phenomena and use them to represent query subtopics. 3. We have successfully applied the mined results to two tasks: search result clustering and re-ranking. The rest of the paper is organized as follows. Section 2 introduces related work. Section 3 describes the detailed analysis of user behavior in the log data for subtopic mining. Section 4 explains our approach to subtopic mining and Section 5 presents the experimental results. Section 6 presents two applications of our subtopic mining algorithm. Section 7 concludes the paper.
2. RELATED WORK
2.1 Search Log Mining
The intent of a query can be characterized along several dimensions, including search goals [7], semantic classes [5], topics [4], and subtopics [14]. The search goals can be informational, navigational, or transactional. The semantic classes of a query can be represented by ODP categories1. The topics or subtopics of a query can be represented by a number of queries or URLs. Topics are usually more coarse-grained and can cover multiple queries, while subtopics are more fine-grained and associated with a specific query [14]. For example, for the query `xbox', all aspects related to Xbox including online game, market place, and even other game consoles are usually included in the topic of the query. In contrast, each aspect of Xbox is represented by one subtopic of the query.
Mining topics from search log data has been intensively
1Open Directory Project: http://www.dmoz.org/.

306

studied2. Click-through bipartite graph data can be used for clustering queries and URLs. Specifically queries which share the same clicked URLs are considered similar. Methods for performing the task have been proposed (e.g., [4, 10, 15, 18, 21, 25, 27]). Beeferman et al. [4], for example, proposed conducting clustering on a click-through bipartite graph and viewing the obtained clusters as topics covering multiple queries. Radlinski et al. [25] proposed first using search session data to find similar queries, and then using a click-through bipartite graph to refine the discovered queries that are similar, and finally grouping the similar queries into the same clusters. The clusters containing the same query are then regarded as topics of the query.
There are some similarities but also significant differences between our work and Radlinski et al.'s work. 1) The problem settings are different. In their work, a subtopic is represented by multiple queries, while in our work a subtopic is represented by keywords and URLs. 2) The information used for subtopic mining is different. They used search sessions to find frequently co-occurring query pairs, considered them candidates of similar query pairs, and finally used a click-through bipartite graph to filter unlikely queries. 3) The goals are also different. The mined results in their work were utilized for manual creation of TREC data. In this paper, we quantitatively evaluated our method on two search tasks, namely search result clustering and re-ranking.
Methods for utilizing a user's click behavior in individual searches have been developed, e.g., [12, 15]. Existing work aimed at modeling and predicting a user's click behavior while our work attempts to find the subtopics of queries. The exploitation of the prefix and suffix relationship between queries was also considered in the previous work, e.g., [26]. In our work, we not only use the prefix and suffix relationship between queries, but also the clicked URLs of the queries and our goal is to conduct query subtopic mining.
2.2 Search Result Presentation
When a query is ambiguous or multifaceted, presenting its search result by sense or facet to users would improve the search experience [9]. Many methods have been proposed, including search result clustering and dynamic ranking.
Search result clustering attempts to cluster the search results according to semantic classes, topics, or subtopics [11]. For example, Chen and Dumais [13] proposed using text classification technologies to classify search results into predefined categories. Zeng proposed using text clustering techniques to cluster a research result based on the titles and snippets of web pages [29]. See also [2, 16, 17]. The major characteristic of these methods is that they use the content of documents in search result clustering.
Wang and Zhai further extended the idea and proposed exploiting the search results of the current query as well as the search results of similar queries [26]. They first build query relationships. For each query, similar queries are found from previous search sessions. They then perform clustering on the clicked URLs of the query and its similar queries. They use the titles and snippets of web pages for similarity calculation. The major difference between their work and our work is that they did not consider mining and utiliza-
2The topics or subtopics of queries cannot be obtained by simply clustering the search results (documents) of queries [25].

Table 1: Multi-clicks in search logs of `harry shum'

Multi-clicks

Frequency

"http://research.microsoft.com/en-us/people/hshum",

50

"http://en.wikipedia.org/wiki/Harry Shum",

"http://www.microsoft.com/presspass/exec/Shum/"

"http://en.wikipedia.org/wiki/Harry Shum, Jr",

95

"http://www.imdb.com/name/nm1484270/"

tion of query subtopics. In this paper, we use Wang and Zhai's method as the baseline for search result clustering.
Recently, Brandt et al. proposed a new search UI called dynamic ranking model [6]. The mined subtopics by our method can also be used in the dynamic ranking model. Specifically, when the user hovers the mouse on an URL in the UI, we can dynamically show the other URLs in the same subtopic under the mouse-hovered URL.
3. TWO PHENOMENA
3.1 One Subtopic per Search
One subtopic per search (OSS) means that the jointly clicked URLs in a specific search are likely to represent the same subtopic. We conducted analysis on the phenomenon with a search log data set from a commercial web search engine (DataSetA as described in Table 8) and validated the correctness of this hypothesis. Our explanation of the phenomenon is: 1) Search users are rational and they do not randomly click on search results (cf., [1, 20]). 2) Users usually have one single subtopic in mind for a particular search.
Table 1 shows two groups of multi-clicked URLs (i.e., the multiple URLs clicked by a user in a query) from search logs for the query `harry shum'. The three URLs in the first group are about one sense and the two URLs in the second group are about another sense. Users may accidently click URLs belonging to two different groups, but such cases are rare. This is reasonable, because if a user searches for a specific person then she usually would only click the URLs about the person and skip the URLs about other people. We call such user behavior `one subtopic per search'. We examined the accuracy of using one subtopic per search as a rule for subtopic identification. By accuracy we mean the percentage of the multi-clicks that share the same subtopics among all multi-clicks. We sampled 10,000 groups of multiclicks of individual queries in DataSetA, which are labeled based on the sense or facet. When all the URLs within the multi-clicks are about the same sense or facet, we consider that the rule has correctly identified a subtopic. Table 2 shows the accuracy versus the number of multi-clicks. We can see that the average accuracy of one subtopic per search is about 84%, suggesting that it is very helpful for identifying the subtopics of queries. Table 2 also shows that the accu-

Table 2: Accuracy of rule v.s. number of clicks

# of Clicks 2

3

4  5 Avg.

Accuracy 0.902 0.824 0.741 0.683 0.842

racy will decrease when the number of URLs in a group of multi-clicks increases. That is to say, the more URLs there are clicked in a search, the less likely that the URLs share the same subtopic. Clicking many URLs in one query may indicate that the user is uncertain about what she is looking

307

Accuracy of MultiClicks Percentage of Queries with Multi
Clicks (%)

0.880 0.87

0.85

0.832 0.83
0.826

0.81

0.802

0.79

0.798

0.779 0.77
2 34 510 1040 40100 >100

Frequency of MultiClicks

Figure 2: Accuracy of rule vesus frequency.

for or wants to learn about all aspects of the query. We also observe that the higher the frequency a group of multi-clicks has, the more likely they share the same subtopic. Figure 2 shows the relation between accuracy and frequency. When a multi-click is only observed twice in the log data, the average accuracy is 0.779. In contrast, when it happens more than 100 times, the accuracy can be as high as 0.880. We

Table 3: Accuracy of rule v.s. click position

Same Different All

subtopic subtopic

# of Multi-clicks

8,421

1,579 10,000

Ave. Position

3.57

3.63

3.50

Ave. Click Intervals 3.23

3.27

3.20

investigated the relationship between the click positions and the accuracy of the rule. Table 3 shows the results. `Same subtopic' means that the rule is correct and the results represent the same subtopic. `Different subtopic' means that the rule is incorrect and the results represent different topics. We can see that click positions do not affect the one subtopic per search phenomenon.
Figure 3 shows the percentages of queries with multiclicks. The queries with higher frequencies in search log data are more likely to have multi-clicks. For example, for the top 1% frequent queries in the search logs (corresponding to about 50% of search traffic), more than 90% of them have multi-clicks. Therefore, the phenomenon of one subtopic per search can help query subtopic mining for head queries.
3.2 Subtopic clarification by Additional Keyword
Many short queries are simply noun phrases, and long queries often contain short queries plus additional keywords, in which the keywords clarify the subtopics of the short queries. We refer to a short query as the original query, and longer queries containing the short query as expanded queries. The clicked URLs after searching with the original query and the expanded queries tend to represent the same subtopic. The keywords can also become labels of the subtopic. This is the phenomenon of subtopic clarification by additional keyword. The phenomenon can be explained in the following ways: 1) Search users are rational users. 2) Sometimes users tend to add additional keywords to spec-

100 90 80 70 60 50 40 30 20 10 0 0

20

40

60

80 100

Top n Percentage Queries (%)

Figure 3: Distribution of top n% queries having multi-clicks.

ify the subtopics in their minds. We have also empirically validated the correctness of this hypothesis.
Table 4 shows the expanded queries and their clicks with respect to the query `harry shum' in the search log data. From the table, we can see that the clicked URLs of query `microsoft harry shum' also appear in the former group of URLs corresponding to Harry Shum of Microsoft. The clicked URLs of `harry shum jr' can be found in the latter group of URLs corresponding to the actor Harry Shum. The keywords `microsoft' and `jr' can be used to represent the two groups (subtopics) respectively. Therefore, each expanded query covers one subtopic of the original query with the keyword as the label of the subtopics.
We conducted analysis on the patterns of queries. We classified queries into four types. If the query is a single phrase, usually a noun phrase, then the type is `Q'. The other three types are `Q + W ', `W + Q', and `Others', where `W ' denotes a keyword and `Q' denotes a phrase. We estimated the distribution of the four types. We randomly selected 1,000 queries from the search log data. For each query, we checked whether it might be an expansion of another query. More specifically, we enumerated all queries that could be expanded from the considered query. The most reasonable query-expanded query pair is selected and then the query type is labeled. The clicked URLs were also used to help make the judgments. The distribution of query types is presented in Table 5. From the table, we can see that the types `Q + W ' and `W + Q' consist of about 42% of the queries. `Q+W ' is more popular than `W +Q'. We further examined

Table 5: Distribution of Query Types

Query Type `Q' `Q + W ' `W + Q' `Others'

Percentage 0.455 0.255

0.165

0.125

whether the subtopics of the expanded queries are covered by those of the original queries. In other words, given two queries with the types `Q' and `Q + W ', we judged whether the subtopics of the latter are contained in the subtopics of the former. We randomly selected 500 pairs of queries with the forms `Q' and `Q + W ', where the queries in Q form are from DataSetA, and studied the relationship between the subtopics of the original query and expanded query. If subtopics of an expanded query are contained in subtopics of the original query, then we say there is `subtopic overlap'

308

Table 4: Search logs of `harry shum' ignoring click frequency

Query

Clicked URLs

harry shum

"http://research.microsoft.com/en-us/people/hshum", "http://en.wikipedia.org/wiki/Harry Shum",

"http://www.microsoft.com/presspass/exec/Shum/"

harry shum

"http://en.wikipedia.org/wiki/Harry Shum, Jr", "http://www.imdb.com/name/nm1484270/"

microsoft harry shum "http://research.microsoft.com/en-us/people/hshum", "http://www.microsoft.com/presspass/exec/Shum/"

harry shum jr

"http://en.wikipedia.org/wiki/Harry Shum, Jr"

harry shum glee

"http://en.wikipedia.org/wiki/Harry Shum, Jr", "http://www.imdb.com/name/nm1484270/"

between the two. We also checked whether two queries share identical clicked URLs, and if so we call it `URL overlap'. In the investigation we found there is no significant difference between the results of queries in the forms `Q + W ' and `W + Q'. Thus, we merged the two types. Table 6 shows

Table 6: Relation of subtopic overlap and URL over-

lap between query and expanded query pair

URL overlap None URL overlap

All query pairs

0.814

0.186

Subtopic overlap

0.801

0.199

None subtopic overlap

0.183

0.817

the results. From the results, we can see that 81.4% of expanded queries have URL overlap with the original queries, and 18.6% of expanded queries do not. For the expanded queries with URL overlap, 80.1% of them have subtopic overlaps with the original queries. For the expanded queries without URL overlap, 81.7% of them do not have subtopic overlap with the original queries. For example, `beijing' and `beijing duck', `fast' and `fast food', and `computer science' and `computer science department' do not have URL overlap, and they do not have subtopic overlap either. Hence we can exploit URL overlap to filter out unrelated `expanded queries' (they are not `true' expanded queries), and use the subtopics of the remaining expanded queries to help identify the subtopics of the original queries. This becomes another rule for subtopic mining.
We also investigated the percentage of queries the rule can be applied. Figure 4 shows the more popular (frequent) a query is, the more likely the rule is applicable. For the top 1% frequent queries (about 50% of search traffic), about 88% have expansions, and on average there are 89 expanded queries for each query. The results indicate the identification by keyword rule can be used for subtopic mining.

Percentage of Queries with Expansions (%)

100 90 80 70 60 50 40 30 20 10 0 0

20

40

60

80 100

Top n Percentage Queries (%)

Figure 4: Distribution of top n% queries having expanded queries.

4. CLUSTERING METHOD
We employ a clustering method to mine subtopics of queries leveraging the two phenomena and search log data. The flow of the method is shown in Figure 5. In the preprocessing stage, we build an index to store all the queries and their clicked URLs. False expanded queries are then pruned from the index. In the clustering stage, the URLs associated with a query and its expanded queries are grouped into clusters, each representing one subtopic. In the post-processing stage, keywords are assigned to the clusters (subtopics) to enrich the representations of subtopics.

Query & expanded query trees Query-URL clusters

Clustering

wj

C1

qi

qi

wk

C2

Preprocessing

Postprocessing

Search log data

Cluster ID

URLs Cluster Label

C1 URL Cluster 1

wj

C2 URL Cluster 2

wk

Mined subtopics of qi

Figure 5: The flow of clustering method.

4.1 Preprocessing
4.1.1 Indexing
We first index all the queries in an index consisting of a prefix tree and a suffix tree to facilitate efficient clustering. We only consider queries in three forms (`Q', `Q + W ' and `W + Q'), as discussed in Section 3.2.
We then segment queries and index them. In the prefix tree, query `Q' and its expanded queries `Q+W' are indexed in a father node and child nodes respectively. Search log data of each query is also stored in its node. With the prefix tree, we can easily find the expanded queries of any query. In the suffix tree query `Q' and its expanded queries `W+Q' are indexed as a father node and child nodes respectively. Figure 6 illustrates the data structure. For query `harry shum', we can easily find its expanded queries `Q+W' from the prefix tree, such as `harry shum jr', `harry shum glee', and `harry shum bing'. We can also easily find its expanded queries `W+Q' from the suffix tree, such as `microsoft harry shum' and `actor harry shum'.
4.1.2 Pruning
We then remove the false expanded queries from the prefix and suffix trees by using a heuristic rule: If a query `Q' does not have URL overlap with its expanded queries `Q+W'

309

Word appearance order

...

...

jr

harry shum glee

...

...

bing

(a) Query prefix tree

Word appearance order

... shum

... harry

micro soft

actor

...

...

(b) Query suffix tree

Figure 6: The data structures to index search logs.

and `W+Q' (i.e., there is no clicked URL shared by the queries), then those expanded queries will be viewed as false expanded queries and pruned from the trees. For example, `fast food' and `fast' do not have URL overlap, and thus `fast food' will be pruned as a child node of `fast'. Similarly, `hot dog' and `dog' do not have URL overlap so `hot dog' will be pruned as child node of `dog'. This heuristics is based on the discussions in Section 3.2. After pruning, only the `true' expanded queries are stored in the prefix and suffix trees.

4.2 Clustering
We conduct clustering on the clicked URLs of each query and its expanded queries. Since all the queries are indexed in the trees, the clustering can be performed locally and recursively on the trees. The clustering of clicked URLs is guided by the two phenomena described in Section 3. After clustering, each group of clustered URLs is taken as one subtopic of the query in the father node.

4.2.1 Similarity Function
The clustering is repeatedly conducted on the clicked URLs of a query and its expanded queries on the trees. The similarity function between two clicked URLs is defined as a linear combination of three similarity sub-functions.
Specifically, the similarity function between URLs ui and uj is defined as

S(ui, uj ) = S1(ui, uj ) + S2(ui, uj ) + S3(ui, uj ) (1)

Here S1 is a similarity function based on the OSS phenomenon, S2 is based on the SCAK phenomenon, S3 is based on string similarities, with , , and  as weights.
S1 is defined as

S1(ui, uj )

=

m ui ∑ m uj ||m ui ||2||m uj ||2

(2)

where m ui and m uj denote the vectors of multi-clicks of ui and uj respectively, `∑ ' denotes the dot product, and || ∑ ||2 denotes the L2 norm. Each element of the vector corresponds to one multi-click pattern, and its value represents
the frequency of the multi-clicks. Intuitively, URLs ui and uj will become similar if they frequently co-occur.
S2 is defined as

S2(ui, uj )

=

w ui ∑ w uj ||w ui ||2||w uj ||2

(3)

where wui and wuj denote the vectors of keywords associated with ui and uj respectively, ∑ denotes the dot product, and || ∑ ||2 denotes the L2 norm. Note that there is a null keyword, which is associated with the original query (father
node). Intuitively, URLs ui and uj will become similar if they are clicked in searches of expanded queries containing

the same keywords. Figure 7 shows the clustering of URLs u1, u2, u3, etc on query q and its expanded queries q + w1, q + w2, and q + w3. The URLs are represented by vectors with each element corresponding to one keyword. S3 sim-

q+w1: harry shum bing

q: harry shum u1 u4 u6

u1 u3 u7 w2+q: microsoft harry shum
u1 u2 u3 q+w3: harry shum jr
u4 u5 q+w4: harry shum glee

u4 u5 u6 (a) Query and expanded queries

q q+w1 w2+q q+w3 q+w4

u1

1

11

u2

1

u3

11

u4

1

1

1

u5

1

1

u6

1

1

u7

1

(b) Vectors of keywords associated with URLs

Figure 7: Example of clustering using subtopic clarification by keyword.
ply represents the similarity between ui and uj as strings. Intuitively, two URLs will be viewed as similar if there are many words shared by the two URLs [22]. To conduct a similarity calculation, we segment a URL into tokens based on the slash symbols, and calculate the cosine similarity of the tokens as S3.
4.2.2 Algorithm
We employ an agglomerative clustering algorithm to perform clustering. The algorithm has the advantage of ease of implementation. One can also consider employing other clustering algorithms. The specific algorithm is as follows:
Step 1: Select one URL and create a new cluster containing the URL.
Step 2: Select the next URL ui, and make a similarity comparison between the URL and all the URLs in the existing clusters. If the similarity between URL ui and URL uj in one of the clusters is larger than threshold , then move ui into the cluster. If ui cannot be joined to any existing clusters, create a new cluster for it.
Step 3: Finish when all the URLs are processed.
4.3 Postprocessing
The output of the clustering process is clusters of URLs for each query and its expanded queries. The clusters which consist of only one URL are excluded. Each cluster represents one subtopic of the query. We further extract keywords from the expanded queries and assign them to the corresponding cluster as subtopic labels. As a result, each cluster not only consists of URLs but also retains keywords as cluster labels. The subtopic popularity can be further estimated from the frequency of clicked URLs in each cluster.
Table 7 shows the mined search subtopics of query `harry shum'. The first subtopic has three keywords and two URLs and the second subtopic has three keywords and three URLs.

310

Subtopic ID 1
2

Table 7: Examples about mined subtopics of `harry shum'

Keywords with frequency URLs with frequency

`harry shum microsoft' : 201 "http://en.wikipedia.org/wiki/Harry Shum": 961

`harry shum bing' : 80

"http://research.microsoft.com/en-us/people/hshum/": 317

`microsoft harry shum' : 22 "http://www.microsoft.com/presspass/exec/Shum/" : 98

`harry shum jr' : 2746

"http://en.wikipedia.org/wiki/Harry Shum, Jr." : 2999

`harry shum glee' : 371

"http://harryshumjr.com/" : 845

`harry shum junior' : 43

"http://www.imdb.com/name/nm1484270/" : 327

5. EXPERIMENTS ON ACCURACY
5.1 Data Sets
We carried out experiments on the accuracy of our method with three data sets. The first dataset (TREC Data) is from the TREC search result diversification track3 in 2009. About 7% of URLs in the data set are not accessible on the web now, so we removed them and added new URLs in the search results of the queries at a commercial search engine. The remaining two datasets consist of queries and URLs randomly sampled from the logs of the commercial search engine in the EN-US market from June 1st, 2010 to March 31st 2011. To reduce the data size, we selected queries which occurred at least two times in one week during a year.
Human assessors labeled the subtopics by grouping the URLs associated with the queries. The guideline for the labeling is exactly the same as that in TREC. For ambiguous queries, it is usually easy to make distinctions between the subtopics of the queries. For multifaceted queries, it is sometimes difficult to make distinctions between the subtopics. We asked the assessors to refer to the examples in the TREC data as a spec in their data labeling. Table 8 gives the statistics of the datasets. `Mixed queries' means that the queries are both ambiguous and multi-faceted.

Table 8: Statistics of three data sets

# of items

TREC DataSetA DataSetB

Queries

50

100

50

URLs

6,498 9,657

4,899

Subtopics

243

1,415

/

Ambiguous queries

11

57

20

Multifaceted queries 39

19

24

Mixed queries

0

24

6

5.2 Setting
The quality of a clustering algorithm is usually measured in terms of purity and inverse purity. Recently, Amigo et al. [3] showed that B-cubed precision, recall and F1 are more suitable for the evaluation of clustering results, thus we adopted those measures for evaluation.
We split DataSetA into three parts and used 1/3 of them for parameter tuning and 2/3 of them for evaluation. The entire TREC dataset was used for evaluation. There are four parameters in our algorithm: , , and  in the similarity function, and  in the clustering algorithm. We tune the parameters heuristically. First, we set all parameters at the value 0.3. Then we increase or decrease the parameter value in increments of 0.05. After several rounds of tuning, we found the final , , , and  were 0.35, 0.4, 0.25, and 0.3, respectively.
3http://trec.nist.gov/data/webmain.html

5.3 Results
Table 9 shows the accuracy of our methods on the two data sets for ambiguous queries, multifaceted queries, and mixed queries. The experimental results indicate that our

Table 9: Accuracy of subtopic mining for different

types of queries

Data Set Query Type B-cubed B-cubed B-cubed

Precision Recall

F1

TREC Ambiguous 0.918

0.932

0.925

Multifaceted 0.893

0.899

0.896

Ambiguous 0.961

0.951

0.956

DataSetA Multifaceted 0.937

0.924

0.930

Mixed

0.952

0.941

0.946

approach performs quite well in the subtopic mining task. To understand the effect of each similarity sub-function, we conducted another experiment with the two datasets combined together and Table 10 reports the results. We can see

Table 10: Accuracy of subtopic mining with different

similarity functions on the `Combined' data set

Similarity Function B-cubed B-cubed B-cubed

Precision Recall

F1

OSS phenomenon 0.945

0.681

0.791

SCAK phenomenon 0.915

0.778

0.841

URL string

0.976

0.517

0.676

All

0.928

0.922

0.925

that all three similarity sub-functions can achieve high precision, which indicates that the two phenomena described in Section 3 really exist. The recalls of the three functions are not so high, however, mainly due to the sparseness of the available data. By combining the uses of all the similarity sub-functions, we can significantly improve the recalls while maintaining high precision.
The clustering accuracy in Table 10 differs from the accuracy in finding subtopics using the two rules in Section 3. There are two main reasons for this. First, different evaluations are made. Table 10 shows the accuracy of subtopic mining using clustering. Section 3 shows the accuracy of individual rules without using clustering. Second, different evaluation measures are utilized.

6. APPLICATIONS OF SUBTOPIC MINING
We consider two applications, search result clustering and re-ranking, and evaluate our method within the applications.
6.1 Search Result Clustering
Search result clustering is about grouping the URLs in the search result of a query by subtopics. Many methods have

311

Table 11: Accuracy comparison between our method and baseline

Data set Query Type

B-cubed Precision

B-cubed Recall

B-cubed F1

Our Method Baseline Our Method Baseline Our Method

Baseline

TREC Data Ambiguous

0.926

0.944

0.924

0.792

0.925

0.861

Multifaceted

0.864

0.872

0.905

0.808

0.884

0.839

Ambiguous

0.963

0.874

0.935

0.878

0.949

0.876

DataSetA Multifaceted

0.936

0.840

0.913

0.906

0.927

0.877

Mixed

0.952

0.839

0.933

0.920

0.942

0.878

Improvement

+5.4%

+6.1%

+5.9% (p  4.58E-06 in the sign test)

Table 12: Accuracy comparison from various per-

spectives

Assessor 1 Assessor 2 Average

Baseline Our method Baseline Our method Baseline Our method

Purity 2.88 3.41 3.15 3.60 3.02 3.51

Diversity 2.35 3.27 2.46 3.29 2.41 3.28

Coverage 2.52 3.60 2.75 4.27 2.64 3.94

Granularity 2.73 3.25 3.08 3.44 2.91 3.35

Average 2.62 3.39 2.86 3.65 2.74 3.52

been proposed for search result clustering. Wang and Zhai's method [26] can be viewed as state-of-the-art and we take it as a baseline. We conduct comparisons in two settings: accuracy comparison and side by side comparison.
6.1.1 Our Method
Our method conducts query subtopic mining offline and stores the mined results in a database. Once the search results for a query are given by the search system, our method performs search result clustering online. Specifically, our method first refers to the database and finds the subtopics of the query, each of which may contain several URLs. For each search result, our method then takes the subtopics of the query as seed clusters. There might be some URLs which do not belong to any of the mined subtopics. We calculate cosine similarity between them and the URLs in the existing clusters by using the TFIDF of terms in titles and snippets. We then assign those URLs to the existing clusters or create new clusters on the basis of the calculated similarities and regard them as the final search result clusters.
6.1.2 Accuracy Comparison
We compared our method with the baseline method on ambiguous queries, multifaceted queries, and mixed queries, respectively. Table 11 shows the detailed comparison between our method and the baseline.
From the results, we can see that our method outperforms the baseline method. When combining the two datasets, the improvement of our method over the baseline is 5.4% in terms of B-cubed precision, 6.1% in terms of B-cubed recall, and 5.9% in terms of B-cubed F1 respectively.
6.1.3 Side-by-Side Comparison
We also compared our method with the baseline side-byside. We created a new dataset referred to as DataSetB. DataSetB contains 50 queries and includes all three types of queries. First we applied both our method and the baseline to each query to obtain two sets of clustering results. Then, we showed the two results to two assessors to perform side by side evaluations. To avoid bias, we randomly swapped the sides of the two results. The assessors could not figure out which result was from which approach.
Several measures were used for evaluation of the results by each method: purity, diversity, coverage, and granular-

ity. Purity is similar to precision and coverage is similar to recall. Diversity shows the difference between clusters. Granularity means how specific the cluster is. An ideal clustering should have high purity, high diversity, high coverage and fine granularity. We asked the assessors to give a score (1 to 5) for each measure.
Table 12 shows the average scores of the methods in terms of the measures for all 50 queries. The results show that our approach is much better than the baseline. The overall improvement is about 28%.
Table 13 shows the side-by-side evaluation results averaged over the 50 queries. When conducting an evaluation, the assessors made judgments on which result was better using three labels: `better than,' `equally good,' and `worse than.' The results show that for about 60% of the queries, our method is better than the baseline; for about 21% of the queries, our method is equal to the baseline. We found that the clusters generated by our method are easier for humans to understand than the baseline.

Table 13: Side-by-Side Comparison

Better than Equal to Worse than

Assessor 1

32

10

8

Assessor 2

30

11

9

Average

31

10.5

8.5

6.1.4 Discussion
When analyzing the results, we found the clusters generated by our approach are more accurate and natural (easy to understand) than the clusters generated by the baseline. This indicates that the clustering of URLs from a user's perspective (log mining based) can achieve better results than from a content creator's perspective (content based).
Our approach outperforms the baseline approach in terms of F1 measure for ambiguous, multifaceted, and mixed queries. The improvements of our approach on recall are larger than the improvements on precision. This is because our approach starts from the mined subtopics that cover the main subtopics of queries. On the contrary, the baseline has no such information, and it tends to generate larger clusters.
The improvement of our method over the baseline on the TREC data is not as high as those on DataSetA. Because in the TREC data, each subtopic consists of about 26.7 URLs while in DataSetA each subtopic only consists of about 6.8 URLs. In other words, DataSetA has finer grained subtopics than the TREC data. When the granularity of subtopics is coarser, our method can benefit less from log mining.
Here we show three examples of results given by the two methods. For simplicity, we only show the labels (subtopics) of URLs in each cluster. The first example is an ambiguous query, the second is a multifaceted query, and the third is

312

a mixed query. We can see that our method often gives more specific clusters (subtopics) while the baseline tends to produce more general clusters (subtoipcs).

Selected subtopic

Minded subtopics

Table 14: Examples of comparison between our ap-

proach and baseline

Query

Our method

Baseline

C1: black swan movie

C1: black swan movie

Black swam (Ambiguous)

C2: black swan diet C3: black swan inn C4: black swan yoga

black swan reviews C2: black swan yoga
black swan theory

C5: black swan wine

C3: black swan reviews

C1: video game reviews C1: video game reviews

Video game (Multifaceted)

C2: video game cheats C3: video game stores C4: video game addiction

video game cheats video game stores C2: video game wiki

C5: video game trailers

video game history

C1: Manchester united C1: Manchester England

Manchester (Mixed)

C2: Manchester college C3: Manchester history C4: Manchester news

Manchester tourism Manchester news C2: Manchester united

C5: Manchester tourism

Manchester soccer

6.2 Search Result Re-Ranking
We consider another search task for the evaluation of our subtopic mining method, namely, search result re-ranking. We chose to do so because we can easily use click-through data to conduct quantitative evaluation.
6.2.1 Our Method
When the search system provides the results to the user, there might be multiple ways to present it if the query contains multiple subtopics, i.e., it is ambiguous or multifaceted. These include simple ranking, clustering of URLs by subtopics, dynamic ranking of URLs belonging to the same topic, as well as re-ranking of URLs by subtopics.
Re-ranking is conducted in the following way. The user is first asked which subtopic she is interested in, with the subtopics shown at the top of the results page. When the user selects a subtopic, the URLs belonging to the subtopic will be moved to the top (re-ranked). The relative order between URLs inside and outside of the subtopic will be kept. The subtopics are assumed to be represented by keywords of subtopics mined with our method.
Figure 8 shows an example UI in which re-ranking is performed for the query `harry shum.' The second subtopic of the query is selected and all the URLs belonging to it are re-ranked at the top. This will significantly improve a user's experience compared with the conventional approach (Figure 1). Although re-ranking is a simple, interactive approach to search results presentation (cf., [28]), there seems to be no study before on the approach to the best of our knowledge. Note that re-ranking can only be done for head queries, because it is based on log data mining, which is a shortcoming that any data mining method may suffer from.
6.2.2 Evaluation
We collected search log data of 20, 000 randomly selected searches at the commercial search engine, for which the query has at least two subtopics mined by our method. The search log data contains the ranking results of URLs as well as the clicks on URLs in the searches. We used the data to estimate how much the cost of users' searches can be saved by re-ranking based on our subtopic mining method.

Related URLs
Figure 8: Example of search result re-ranking.
We first calculated the average position of last clicked URLs in the 20, 000 searches. The result is 3.41. The number indicates the average cost of finding information in the conventional search UI based on ranking.
Next, we used the same log data to estimate the cost for users based on re-ranking. We assumed that the cost for the user to check the subtopics and click one of them is 1.0, because the subtopics are shown in one line at the top, and examining them is almost equivalent to examining the title and snippet of a URL. The cost of finding the last click within each selected subtopic can be calculated by using the search log data, because within the subtopic the ranking order is preserved. Note that because of the one subtopic per search phenomenon, the clicked URLs in a search usually belong to the same subtopic. We calculated the average position of last clicked URLs belonging to the same subtopics in the 20, 000 searches and found that it is 1.80.
The cost saved in the re-ranking may be calculated as
 = 3.41 - 1.80 - 1 = 0.61
This is the result for the saved cost per search. Similarly, we can estimate the saved cost per query, which is 0.7. We can conclude, therefore, that the re-ranking method can reduce a user's effort in finding information, and thus our method of subtopic mining is effective.
7. CONCLUSIONS AND FUTURE WORK
In this paper, we have studied the problem of query subtopic mining. Specifically, we have discovered two phenomena of user search behavior that can be used as signals to mine major senses and facets of ambiguous and multifaceted queries. One is called one subtopic per search and the other subtopic clarification by additional keyword. We have developed a clustering algorithm that can effectively and efficiently mine query subtopics on the basis of the two phenomena. We have evaluated the effectiveness of the proposed approach. On the TREC dataset, the F1-measure is 0.925 for finding ambiguous subtopics and 0.896 for finding multifaceted subtopics. When applied to search result clustering, our method can improve precision by 5.4% and recall by 6.1%. The effectiveness of our method has also been verified in search result re-ranking, using click data.
There are several issues we want to further explore to en-

313

hance our current work. First, only three types of features were used in the current clustering algorithm. We plan to investigate the use of other features to further improve the accuracy. Second, we only tried one clustering algorithm as an example. Other existing algorithms can be applied as well. Third, the mined subtopics by our method were only applied to two applications, and they can be useful in other applications as well. We also plan to try some other applications such as personalized search and search result diversification. Finally, our method can only be employed when there is enough search log data, which is also a drawback for most log mining algorithms. How to apply the approach in tail queries is also an issue we need to consider.
8. ACKNOWLEDGEMENTS
This research was done when the second author was visiting MSRA, her research was supported by the SPKLSTN Lab (National Science Foundation of China under Grant No. 60825202, 61173112, and Cheung Kong Scholar's Program).
Jian Pei's research is supported in part by an NSERC Discovery Grant, a BCFRST NRAS Endowment Research Team Program project, and a GRAND NCE project.
9. REFERENCES
[1] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incorporating user behavior information. In Proceedings of SIGIR'06, pages 19≠26, 2006.
[2] O. Alonso and M. Gertz. Clustering of search results using temporal attributes. In Proceedings of SIGIR'06, 2006.
[3] E. Amig¥o, J. Gonzalo, J. Artiles, and F. Verdejo. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Inf. Retr., 12:461≠486, August 2009.
[4] D. Beeferman and A. L. Berger. Agglomerative clustering of a search engine query log. In Proceedings of SIGKDD'00, pages 407≠416, 2000.
[5] S. Beitzel, E. Jensen, A. Chowdhury, and O. Frieder. Varying approaches to topical web query classification. In Proceedings of SIGIR'07, pages 783≠784. ACM, 2007.
[6] C. Brandt, T. Joachims, Y. Yue, and J. Bank. Dynamic ranked retrieval. In Proceedings of WSDM'11, pages 247≠256, 2011.
[7] A. Z. Broder. A taxonomy of web search. Sigir Forum, 36:3≠10, 2002.
[8] A. Z. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josifovski, and T. Zhang. Robust classification of rare queries using web knowledge. In Proceedings of SIGIR'07, pages 231≠238, 2007.
[9] M. Burt and C. L. Liew. Searching with clustering: An investigation into the effects on users' search experience and satisfaction. Online Information Review, 36, 2012.
[10] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and H. Li. Context-aware query suggestion by mining click-through and session data. In Proceeding of KDD'08, 2008.
[11] C. Carpineto, S. Osin¥ski, G. Romano, and D. Weiss. A survey of web clustering engines. ACM Computing Surveys (CSUR), 41(3):17, 2009.

[12] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In Proceedings of WWW'09, pages 1≠10, 2009.
[13] H. Chen and S. Dumais. Bringing order to the web: automatically categorizing search results. In Proceedings of CHI'00, pages 145≠152, 2000.
[14] C. L. A. Clarke, N. Craswell, and I. Soboroff. Overview of the trec 2009 web track. In Proceedings of TREC'09, pages 1≠9, 2009.
[15] N. Craswell and M. Szummer. Random walks on the click graph. In Proceedings of SIGIR'07, pages 239≠246, 2007.
[16] H. Deng, I. King, and M. Lyu. Entropy-biased models for query representation on the click graph. In Proceedings of SIGIR'09, pages 339≠346. ACM, 2009.
[17] P. Ferragina and A. Gulli. A personalized search engine based on web-snippet hierarchical clustering. Software: Practice and Experience, 38(2):189≠225, 2008.
[18] S. Fujita, K. Machinaga, and G. Dupret. Click-graph modeling for facet attribute estimation of web search queries. In Adaptivity, Personalization and Fusion of Heterogeneous Information, pages 190≠197, 2010.
[19] W. Gale, K. Church, and D. Yarowsky. One sense per discourse. In Proceedings of the workshop on Speech and Natural Language, pages 233≠237. Association for Computational Linguistics, 1992.
[20] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of KDD'02, pages 133≠142, 2002.
[21] R. Jones and K. Klinkner. Beyond the session timeout: Automatic hierarchical segmentation of search topics in query logs. In Proceedings of CIKM'08, pages 699≠708, 2008.
[22] M. Kan and H. Thi. Fast webpage classification using url features. In Proceedings of the 14th ACM international conference on Information and knowledge management, pages 325≠326. ACM, 2005.
[23] Y. Li, Z. Zheng, and H. K. Dai. Kdd cup-2005 report: facing a great challenge. SIGKDD Explor. Newsl., 7:91≠99, December 2005.
[24] H.-T. Pu, S.-L. Chuang, and C. Yang. Subject categorization of query terms for exploring web users' search interests. J. Am. Soc. Inf. Sci. Technol., 53:617≠630, August 2002.
[25] F. Radlinski, M. Szummer, and N. Craswell. Inferring query intent from reformulations and clicks. In Proceedings of WWW'10, pages 1171≠1172, 2010.
[26] X. Wang and C. Zhai. Learn from web search logs to organize search results. In Proceedings SIGIR'07, pages 87≠94, 2007.
[27] J.-R. Wen, J.-Y. Nie, and H.-J. Zhang. Clustering user queries of a search engine. In Proceedings of WWW'01, pages 162≠168, 2001.
[28] X. Yuan and N. J. Belkin. Supporting multiple information-seeking strategies in a single system framework. In Proceedings of SIGIR'07, pages 247≠254, 2007.
[29] H. Zeng, Q. He, Z. Chen, W. Ma, and J. Ma. Learning to cluster web search results. In Proceedings of SIGIR'04, pages 210≠217. ACM, 2004.

314

Multi-Aspect Query Summarization by Composite Query

Wei Song1, Qing Yu2, Zhiheng Xu3, Ting Liu1, Sheng Li1, Ji-Rong Wen2
1School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China
{wsong, tliu, lisheng}@ir.hit.edu.cn
2 Microsoft Research Asia, Beijing, 100190, China
{qingyu, jrwen}@microsoft.com
3 Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
xuzhiheng19881130@gmail.com

ABSTRACT
Conventional search engines usually return a ranked list of web pages in response to a query. Users have to visit several pages to locate the relevant parts. A promising future search scenario should involve: (1) understanding user intents; (2) providing relevant information directly to satisfy searchers' needs, as opposed to relevant pages. In this paper, we present a paradigm for dealing with informational queries. We aim to summarize a query's information from different aspects. Query aspects are aligned to user intents. The generated summaries for query aspects are expected to be both specific and informative, so that users can easily and quickly find relevant information. Specifically, we use a "Composite Query for Summarization" method, which leverages the search engine to proactively gather information by submitting multiple composite queries according to the original query and its aspects. In this way, we could get more relevant information for each query aspect and roughly classify information. By comparative mining the search results of different composite queries, it is able to identify query (dependent) aspect words, which help to generate more specific and informative summaries. The experimental results on two data sets, Wikipedia and TREC ClueWeb2009, are encouraging. Our method outperforms two baseline methods on generating informative summaries.
Categories and Subject Descriptors
H.3.m [Information Storage and Retrieval]: Miscellaneous
General Terms
Algorithms, Experimentation
This work was done when the first and third authors were visiting Microsoft Research Asia
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

Keywords
Query aspect, Query summarization, Composite query, Mixture Model
1. INTRODUCTION
Nowadays, accessing information on the Internet through search engines has become a fundamental life activity. Current web search engines usually provide a ranked list of URLs to answer a query. This type of information access does a good job for dealing with simple navigational queries by leading users to specific websites. However, it is becoming increasingly insufficient for queries with vague or complex information need. Many queries serve just as the start of an exploration of related information space. Users may want to know about a topic from multiple aspects. Organizing the web content relevant to a query according to user intents would benefit user exploration. In addition, a list of URLs couldn't directly satisfy user information need. Users have to visit many pages and try to find relevant parts within long pages, since the information may be scattered across documents. The long-standing goal of search engines should be providing relevant information, as opposed to relevant documents, to directly satisfy searchers' needs.
This paper presents a novel search paradigm that the system should automatically discover information and present an informative overview for a query from multiple aspects. We target on dealing with informational queries. A query represents a centric topic, and the query aspects are aligned to user intents covering diverse information needs. The query aspects could be specified explicitly by users through an interface or automatically mined from search logs or other resources [4, 18, 22, 25]. In this paper, we use simple methods to do aspect mining and mainly focus on multi-aspect oriented query summarization: given a query and a set of aspects, generate a summary for each query aspect, which is expected to provide specific and informative content to users directly and helps for further exploration. Figure 1 shows an example of the system output.
We further formulate the multi-aspect oriented query summarization into 2 phases: information gathering and summary generation. Different from traditional text summarization where a set of documents to be summarized is given as a system input, we propose a "Composite Query for Summarization" method, which leverages the search engine to proactively gather related information. In addition to using the search result of the original query, we also composite a set of new queries and submit them to the search engine to

325

Figure 1: An example output of multi-aspect oriented query summarization.
collect query aspect related information. For example, by concatenating the original query and the keywords of an aspect as a query, we are able to get query dependent aspect information; by submitting the aspect keywords only as a query, we could get query independent aspect information. Our motivations are:
First, the search result of the original query may not contain enough information for all aspects that users care about, because the search engine returns documents only considering whether a document is relevant to the query keywords, rather than its aspects.
Second, for better aspect oriented exploration, the information for different query aspects should be as orthogonal as possible. It is important to distinguish the aspect specific information from the general information about the whole query. By using the composite queries, we could get more specific information for each aspect.
The flexible information gathering also helps for summary generation phase. By comparing the search results of different types of composite queries, query (dependent) aspect words can be identified without complex natural language processing, based on which more specific and informative summaries could be generated,
The contributions of this paper can be summarized as follows:
∑ We formulate the multi-aspect based query summarization task. In this scenario, the system proactively discovers information and aims to provide multiple dimensional and direct information seeking in response to informational queries.
∑ We propose a "Composite Query for Summarization" method for proactive information gathering, which is a key point for our task and differs from traditional search result organization and textual summarization.
∑ We emphasize generating specific and informative summaries to directly address searchers' needs on different aspects. To achieve this, we propose a simple method to identify query aspect dependent words by comparing the search results of different types of composite queries.
∑ We conduct experiments on both real web queries and

large-scale pseudo queries based on Wikipedia1. Automatic evaluation and human judgements are used for measuring the quality of generated summaries.
The rest of the paper is organized as follows. First, we discuss related work in Section 2. In Section 3, we define the query aspect and briefly introduce optional approaches for query aspect mining. In Section 4, we detail the proposed "composite query" based method for both information gathering and aspect oriented summary generation. After that, we report our experimental results in Section 5. Section 6 states our conclusions.
2. RELATED WORK
2.1 Search Result Organization
Exploratory search becomes a new frontier in the search domain, which aims to provide additional support for information seeking beyond simple lookup [24]. Recent work has shown that well-organized search results are helpful for information exploration. For example, search result clustering [9, 11, 27], categorizing [1], facet based information exploration [6], representative queries [23] and tag clouds [10] are adopted for search result navigation. Clustering based approaches automatically group similar search result documents together [9, 11, 27]. Search result documents can also be classified into a manually constructed category taxonomy [1]. But the fixed hierarchy often lacks of flexibility to describe various user information needs. Faceted search aims to offer the ability for searchers to filter search results by specifying desired attributes [6]. However, the facets are usually pre-defined for some specific domain so that it is difficult to apply it to web search. Though most of the above methods organize search result documents into various aspects and improve user experience for information exploration, the content are still presented at document level, and users can't get relevant information directly.
2.2 Document Summarization
Single document summarization techniques have been successfully applied in web search engines (snippet generation) [19, 20]. A span of text gives users a first sight of the topics of a document. For efficiency, sentence extraction strategy is used for generating query dependent summaries [5].
Comparing with single document summarization, multidocument summarization is expected to generate a global picture for a set of documents which is given as input [15, 26]. Recently researchers utilize latent topics for multiple document summarization. For example, subtopics from the narrative of a topic (a description of a topic, which is provided by the DUC summarization track) is used to enhance summarization [17]. Wang uses topic model to extract subtopics and select sentences by topic words [21]. However, the latent topics used in these papers are usually mined unsupervised. As a result, the topics may fit to the data collection, rather than align to user intents.
Some work makes use of predefined aspects to provide (sentiment) summarization on reviews or comments [7, 14]. Our work is also inspired by [13], which incorporates user interaction into the summarization process. Given a corpus of documents, users predefine their interested facets and the
1http://en.wikipedia.org/

326

Figure 2: A snipping of returned documents for query "Saving Private Ryan" and two typical services provided by Bing Search.
system provides summaries according to the facets. The authors evaluate it on online reviews and Gene corpus (which are relatively "clean" data sets). In contrast, we focus on summarizing user intents related to a query rather than a given corpus. They don't consider the informativeness of the generated summaries, while one of our goals is to provide direct information to users.
Our work is based on query aspects but differs from existing work in several points. First, in our framework, query aspects could be mined from any resources but not limited to a set of documents to be summarized. Second, the traditional summarization task treats the documents as a given input to the system. However, in our scenario, we separate the information gathering and summarization generation phases. In this way, we view the whole web as a corpus and could proactively collect more related information for summarization. Third, we aim to generate both specific and informative content for each query aspect. Therefore, users could get relevant information directly.
3. QUERY ASPECT
Multi-aspect oriented query summarization depends on query aspects. In this section, we define the query aspect and briefly discuss query aspect mining methods both in literature and in realistic way.
An aspect represents a distinct information need relevant to the original query. Recently, various methods have been proposed for automatically discovering query intents [2, 4, 22, 25]. The NTCIR-9 Intent Task was organized to explore and evaluate the technologies of mining and satisfying different user intents for a vague query [18]. In these work, a query aspect is represented in different ways, such as a set of search queries related to the original query [2, 25], a set of query qualifiers [22] or a single intent string [18]. These definitions are in fact very similar. The main differences are: (1) Whether distinguish the original query and the query qualifier. (2) Whether select an exemplar (label) to represent a set of queries related to the same intent.
Inspired by previous work, we define an aspect as a query

qualifier - keywords that are added to an original query to form a specific user intent. For example, "reviews" and "actors" could be seen as aspects for a movie. In this work, we mainly focus on multi-aspect oriented summary generation and use very simple method to mine query aspects. However, any existing method for mining query aspects could be incorporated.We can also use the services provided by search engines to get approximate query aspects. For example, search engines provide "query suggestion" or "related searches" features. Figure 2 shows a snipping of the search result from Bing Search page for query "Saving Private Ryan", a famous movie. Thus, the aspects could be easily identified using simple rules from related searches. We could also predefine some aspect templates for certain query classes, such as movie, travel, music, people, etc. We leave this as future work.
4. MULTI-ASPECT ORIENTED QUERY SUMMARIZATION
Now, we suppose the aspects are given and aim to summarize a query according to its different aspects. We expect to generate both specific and informative summary for each aspect instead of a set of documents so that the users could get relevant information directly. First, we explain the meaning of specific and informative by an example. Suppose that for the query "Saving Private Ryan", one of the user information needs is to know the "actors" of this movie. There are some candidate sentences:
(i) "A movie page covers information about new Steven Spielberg movie 'Saving Private Ryan' including actors, film makers and behind the scenes."
(ii) "Saving Private Ryan cast are listed here including the Saving Private Ryan actresses and actors featured in the film."
(iii) "The actors of Saving Private Ryan are Tom Hanks as Captain and several men Edward Burns, Barry Pepper..."
All the three sentences contain certain information about the aspect "actors". The first one talks about the general information about the query. It is Not specific to the desired aspect. The second sentence focuses on the desired aspect, however, it does not provide relevant information directly, only gives navigational information. We say it is specific but Not informative. The third sentence should be a good candidate which provides direct answers to the desired aspect, i.e., the names of the actors. It is both specific and informative.
As the example shows, the challenges of this task include: (1) Distinguish aspect specific information from general query information. (2) Identify informative content instead of navigational information only. We take the Composite Query for Summarization method to deal with above issue, which consists of 2 phases: information gathering and summary generation. First, we proactively get aspect specific information using composite queries. Then a mixture model is used to model different types of words which present query common information or aspect specific information. Finally, we rank the candidate sentences based on the mixture model and the redundancy in search results for generating summaries.

327

4.1 Information Gathering
Existing work on text summarization doesn't pay much attention on how to collect data. A natural way is to use query search result. However, there may be not enough information for certain query aspects, if we only use the search result of the original query. For example, some users wonder whether movie "Saving Private Ryan" tells a true story, but few top documents in the search result of "Saving Private Ryan" discuss this topic.
We present a composite query based method for information gathering. Formally, we denote the original query as Q and an aspect as Ak. For example, Q refers to the original query "Saving Private Ryan" and Ak refers to one aspect "actors". In information gathering phase, we composite a new query by concatenating the original query and the aspect words, denoted as Q + Ak. The composite query is "Saving Private Ryan actors". Therefore, we can submit the composite query to the search engine to get top ranked documents. Comparing with the search result of the original query, the search result of the composite query is much more specific for the query aspect. Also, we can submit the aspect Ak itself to the search engine to get information about the aspect which is query independent.
For a query with K aspects, we have a set of composite queries {Q, Q + A1, ..., Q + AK , A1, ..., AK }. We use the top returned documents for each composite query. The search result of Q (denoted as CQ) provides overall information about the query; the search result of Q + Ak (denoted as CQ+Ak ) provides the information about the aspect Ak of the query Q. The search result of Ak (denoted as CAk ) provides information about the aspect itself which is query independent. The idea of using composite queries is straightforward and the benefits are two folded: (1) We collect more aspect related data which may be not contained in original query's search result. (2) The search engine helps us roughly classify information according to the query aspects.
Based on the collected data for query aspects, we identify aspect words by comparing the search results of different types of composite queries. These words are then used for assisting summary generation.
4.2 Summary Generation
4.2.1 Modeling Search Result
We assume the desired information for query aspect Ak is embedded in collection CQ+Ak , which consists of 3 kinds of information: query general information, aspect information, irrelevant information. Correspondingly, the words in search results could be divided into 3 categories:
Query Common Words: They tend to occur frequently across multiple aspects, such as "movie", "TV", "IMDB" for "Saving Private Ryan".
Query Aspect Words: These words provide information for an aspect, such as "cast", "list" and "Tom Hanks" for the aspect "actors".
Global Background Words: These words distribute heavily on the Web. Mostly, they are stop words or high frequency non-discriminative words.
Figure 3 shows 3 types of words and their relationship in search results of the original query and the composite

CQ+A1
Query Aspect Words

CQ+A2
Query Aspect Words

CQ+AK
Query  Aspect
Words

CQ
Words For Other undefined
Aspect

Global Background Words

Query Common Words

Figure 3: The illustration of the relationship between the search results of different composite queries and different types of words.

queries. We assume that the query aspect words describing the aspect Ak of query Q will occur more in CQ+Ak , while the query common words will occur frequently across multiple aspects. Based on the collected data by using composite queries, the observations support the assumption. Therefore, we adopt a mixture model to describe each type of words. Formally, k represents the query aspect words model for aspect Ak. B represents the query common words model. G represents the global background words model which is to draw globally high frequency terms. All these models are multinomial probability distributions over vocabulary.
The collection CQ+Ak could be generated by the mixture model. Each word w in CQ+Ak is generated according to:

pk(w) = Gp(w|G) + (1 - G)◊

(1)

(Bp(w|B) + (1 - B)p(w|k))

where pk(w) represents the probability of a term occurrence w in collection CQ+Ak , G and B are fixed parameters. The generative process could be seen as 2 steps: first decide whether this word is from G, and then decide it comes from B or k. To estimate the aspect word model k, we first estimate G and B. G is estimated using maximum likelihood estimator based on document frequency which is computed on a large collection of web pages. B is estimated by combining the search results of the original query and all query aspects, i.e., CQ  {CQ+Ak }. We use CQ to catch the general content of the query and the unknown aspects which are not defined explicitly or mined already. B could be estimated according to:



p(w|B )

=

 tf (w, CQ) + ktf (w, CQ+Ak ) w (tf (w, CQ) + k tf (w, CQ+Ak ))

(2)

where tf (w, ∑) represents the term frequency in a collection. After deriving p(w|B) and p(w|G), p(w|k) could be estimated using the expectation maximization (EM) algorithm [3] by maximizing the log-likelihood of the collection CQ+Ak :



L(CQ+Ak ) = log tf (w, CQ+Ak )pk(w)

(3)

w

For each term w in CQ+Ak , the updating formulas of the E-step and the M-step are shown below:

328

E-Step:

pw (z

=

G)

=

p(w|G ) p(w|G)+(1-)((1-B )p(w|k)+B p(w|B ))

pw (z

=

k)

=

(1-B )p(w|k ) (1-B )p(w|k)+B p(w|B )

M-Step:

p(w|k ) = wtft(fw(w,C,QC+QA+kA)k(1)-(1p-wp(wz=(Gz=))Gpw))(pzw= k(z)=k) where z is a latent variable introduced to represent which type a word is assigned to. p(z = G) and p(z = k) are corresponding probabilities. In this way, we distinguish the query aspect words from the query common words. The words with high probabilities in k represent the specific query aspect better.
Next, we consider to identify more informative aspect words. We divide the query aspect words into 2 categories: query dependent aspect words which provide direct information for the aspect, such as "Tom Hanks" and "Edward Burns" for aspect "actors"; query independent aspect words which are query independent and reflect the characteristics of the aspect itself, like "actor", "actress", and "cast" for aspect "actors". We distinguish these 2 types of query aspect words by the assumption that query dependent aspect words occur in CQ+Ak , and query independent aspect words occur in both CQ+Ak and CAk . The CAk is the search result of the aspect Ak itself, which contains many words related to the aspect. However, these words can be used for any query with such aspect, but don't bring direct information for a specific query. So we identify query dependent aspect words as QDWk = {t|t  CQ+Ak and t  CAk }. The words occur in CQ+Ak that suggests they are related to the query aspect, but don't occur in CAk that indicates they are query dependent. The relative importance of the query dependent aspect words could be read out from p(w|k).
4.2.2 Sentence Selection
To summarize aspect Ak for query Q, we extract sentences from the content of the search result documents in CQ+Ak  CQ. The candidate sentences are then ranked based on their specificity, redundancy and informativeness. The top ranked sentences are used as a summary for the desired aspect.

Candidate sentence filtering based on specificity. On-

ly part of the sentences within the search result are related

to the desired aspect. We select a candidate sentence for a

desired aspect only if it is closer to the desired aspect than

to any other aspects. To measure this, we classify each sen-

tence to one of the aspects:



k = argmax

p(w|i)

(4)

i{1,2,...,K,B,G} ws

where i is an estimated query aspect words model or the query common words model or the global background words model. A sentence within CQ+Ak  CQ is chosen as a candidate only if k equals to k. Thus, all the selected candidate sentences are more specific to the desired aspect.

Sentence clustering. The candidate sentences are selected from multi-documents. Redundancy is particular important. On one hand, the same information conveyed by sentences from different documents indicates its importance.

On the other hand, it is not good to show duplicate sentences to users. Due to the above reasons, the candidate sentences are grouped into clusters according to lexical features. We adopt a hierarchical clustering approach. Each single sentence is initiated as a cluster. If two clusters are close enough, they are merged. This procedure repeats until the smallest distance between all remaining clusters is larger than a threshold. Edit distance is used to measure the distance between two sentences. We use U (s) to represent the cluster, which the sentence s belong to. The size of this cluster U (s).size indicates the popularity of this cluster or the redundancy of the information this cluster conveys.

Measuring informativeness. Since informative summaries

are expected, we measure the informativeness of a sentence

based on:





inf o(s|k) = (1 - )

p(w|k) + 

p(w|k )

ws, wQDWk

ws, wQDWk

(5)

where QDWk represents the query dependent aspect words for aspect Ak;  is a parameter to tune the impact of the query dependent aspect words.

Sentence ranking. In each cluster, we select one sentence with highest inf o(s|k) as the exemplar to represent the cluster. The exemplars selected from all clusters are ranked according to W eightk(s):

W eightk(s) = log(1 + U (s).size) ◊ inf o(s|k) (6)
5. EXPERIMENTS
In the experiments, we assume the query aspects are given and focus on evaluating the quality of generated summaries for query aspects. The data sets we used already contain aspects for each query. Our method and baseline methods take both query and aspects as input.
5.1 Data Sets
To the best of our knowledge, few public data set can be used to evaluate the multi-aspect oriented query summarization. We constructed two data sets from well-known data sources, Wikepedia and TREC. We will introduce the data sets and experimental results in following sections.
5.1.1 Wikipedia Data
Each topic page in Wikipedia is composed of a title and a list of sub sections, which describe the topic from different aspects. For example, the title of a page is "Saving Private Ryan", and the page includes subheadings like "Plot", "Cast" and "Production". In our experiments, we treated the title of a page as a query, the meaningful subheadings (top level) as query aspects. We filtered out the meaningless subheadings like "Notes", "References" and "Further Readings" by rules. We also filtered out pages with less than 3 or larger than 10 aspects to avoid noise. We used the textual content under a subheading as the golden reference for the corresponding aspect. In all, we sampled 1000 pages (queries) from an English Wikipedia dump which was collected in January 2011. The statistics of the sampled data is listed in Table 1.

329

Table 1: The statistic of Wikipedia data set

Topics

1000

Average Length of Topics (words) 2.15

Average Aspects per Topic

5.15

Average Aspect Length (words) 798

We divided the sampled data into develop set and test set. The develop set containing 100 queries was used for parameter tuning. While the test set, which contains 900 queries, was used for comparing performance of different systems. Note that, since our method uses the search results of a search engine which may give Wikipedia pages as returned documents, we removed Wikipedia pages from the search results when doing experiments.
5.1.2 TREC 2009 Web Track Data
The trec data is widely used for search related experiment evaluation. We use the public available query set of TREC 2009 Web track. One goal of TREC 2009 Web Track is evaluating the search result diversity. The data set includes 50 topics and each topic has 3 to 8 manually edited subtopics to be covered. Each subtopic is a description of an information need. Figure 4 shows an example topic provided by TREC 2009 Web track.
We treated each topic as a query and derived query aspects from its subtopic descriptions by simple rules. We first extracted all nouns from a description. Then we excluded those terms which occur in original query, then used the remaining terms as an aspect. For example, for the query "Obama family tree", "mother information" was used as one aspect. In all, we got 50 queries and 4.9 aspects for each query on average.
5.2 Baselines
The proposed algorithm is denoted as Q-Composite. We compare it with 2 baselines. Baseline 1 is based on Ling et al [13], denoted as Ling-2008. This method first estimates an aspect prior distribution based on term co-occurrence in the corpus, then integrates the priors into a topic model, finally ranks sentences according to the distance between sentence language model and the aspect models. It is proved very effective for mining faceted summaries on relatively clean and formal data sets, like Gene corpus. But it is not oriented to the web search. Like traditional text summarization tasks, they just use a collection of documents related to the centric topic for summarization. We implemented this method and applied it to the multiple aspect based query summarization as a baseline. The aspect model of Ling-2008 was estimated on the search result of each original query and the sentences for each aspect were extracted from the search results of both the original query and the composite query, which was the same as the input of our method.The second baseline is based on the top sentences in snippets, which are provided by a search engine for each composite query Q+Ak, denoted as Snippet. The number of the top sentences depends on the total summarization length limit. Though it is simple, it is very strong. These snippets are selected from the top relevant documents of the composite query, so that they are more likely specific to the query aspect. In addition, most snippet generation algorithms are based on single document

Figure 4: An example topic in TREC 2009 web track.
summarization method, which tend to extract the sentences containing most relevant terms.
5.3 Parameter Settings
There are several parameters in our method. We tuned the parameters of our method and baselines on the develop set. In our experiments, G was set to 0.95 in order to get more discriminative words. B was set to 0.8 to balance query common information and aspect specific information. The threshold used in sentence merging procedure was set to 0.7. The parameter  was set to 0.0, which means to rank sentences based on query dependent aspect words only. For each composite query, we used the top 50 documents from the search result. The words occurring in less than 3 documents were discarded.
5.4 Experiment Design and Evaluation
Due to the different characteristics of the two data sets, we adopt different evaluation strategies and metrics.
5.4.1 Evaluation on Wikipedia Data
For Wikipedia data, we generate the summaries based on real web data. We send both the original query and the composite queries to a commercial search engine and get the search result documents and snippets. For efficiency, we train the model using the snippets and extract sentences from the content of the documents. We use the ROUGE tool for evaluation on Wikipedia Data. ROUGE is a wellknown tool for evaluating both single and multi-document summarization [12]. Basically, it is a recall-like metric. A higher ROUGE value means that more useful information is found. ROUGE-1 metric has been proved highly consistent with human judgements, so we take it for evaluation in our experiments. At evaluating time, the golden reference for each aspect is taken from the content of corresponding subheading in a Wikipedia page. Since the extracted sentences for summarization have different length, we let each system generate top sentences and the first 200, 400 and 600 words are used for evaluation.
5.4.2 Evaluation on TREC 2009 Data
For TREC data set, we generate summaries from the corpus provided by TREC rather than the whole Web, namely the ClueWeb09. Our method depends on the search engine's search result, so we need index ClueWeb09 and build a small search engine. We use a simple ranking function to give search result based on BM25 [16], anchor text and stat-

330

Table 2: Labeling guide and examples. The query is "Saving Private Ryan" and the aspect is "Actors"

Label

Gain Value Description

Examples

(Level)

Informative and spe- 5 cific

The sentence focuses on the desired aspect and provides useful

The actors of "Saving Private Ryan" include Tom Hanks, Tom Sizemor, Edward Burns.

information which can help user to

know something about the query

aspect.

Saving Private Ryan is a 1998 American war film,

Informative but not 4

The sentence conveys multi-aspect directed by Steven Spielberg and it follows

specific

information about the query. And Tom Hanks as Captain John H. Miller.

it does provide useful information

for the desired aspect.

Specific but not in- 2 formative

The sentence talks about the desired aspect but doesn't provide

Saving Private Ryan Cast and Details on TVGuide.com.

much detail information.

Not about this aspect but about the query

1

Saving Private Ryan is a 1998 American It provides some information epic war film set during the invasion of Normandy in about some aspects of the query World War II.

but not related to the desired as-

pect.

Alphabetized and searchable index of real and

Not about this query 0

The sentence does not talk about fictional events, cast, and places related to

the query.

films.

ic rank features. It generates snippets by selecting the top sentences which contain the most query terms.
Since the data does not provide golden reference at sentence level, we have to judge the quality of generated sentences manually. So it is necessary to clarify the standard for assessment. Ideally, a good query summary should make users get the desired information directly. In our scenario, we assess the summaries from two perspectives: specific and informative. First, we hope the summary can give specific information about an aspect rather than a general description covering multiple aspects. Second, it should give more direct information in contrast to navigational information so that users spend less time to obtain information.
Based on this standard, we asked labelers to label the generated sentences for 50 queries. For each system and each query aspect, the labelers had to evaluate the top 3 ranked sentences. Each sentence was assigned a gain value according to the guidelines shown in Table 2, which describes the labeling standard by using an example. Note that we skip the gain value 3, because we think that the "informative and specific" and "informative but not specific" sentences are useful to users for getting direct information, should be given higher bonus than other levels. The topic descriptions, as shown in Figure 4, were also presented to labelers as reference.
The normalized Discounted Cumulative Gain (nDCG) [8] is used to evaluate the performance. The nDCG is a metric that gives higher weights to well ranked objects. The average nDCG over all the test query aspects is used to measure the overall performance.
5.5 Experimental Results and Discussion
In this session, we present the experimental results on two data sets and analyze the performance of different systems and the impacts of key factors.

Coverage

Top search results of the composite queries

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

50

100

300

500

Top search results of the original queries

Top1 Top5 Top10 Top30

Figure 5: The average coverage of the search results of the original queries over the composite aspect queries.

5.5.1 Coverage of the Search Results of the Original Queries

Previous work focuses on organizing the search result of

the original query into multiple aspects. We argue that the

search result of an original query may not have enough infor-

mation covering all query aspects. To verify this, we conduct

a simple experiment to measure the coverage of the search

results of the original queries on the corresponding compos-

ite queries. We sampled 100 queries from the Wikipedia

data set. For each original query Q, we retrieved the set

of top N URLs from a search engine, denoted as SQN . For

each composite query Q + Ak, we retrieved the set of top M

URLs from the same search engine, denoted as SQM+Ak . We

measured

the

coverage

of

SQN

over

SQM+Ak ,

i.e.,

. . |SQ N SQ M+Ak |
M

The average coverage over all queries' aspects is shown in

Figure 5. Intuitively, the search result of Q + Ak should de-

331

scribe the query aspect better. However, the top documents in SQM+Ak rarely appear in SQN . For example, more than 60% top 1 documents retrieved by composite queries are not in the top 100 returned documents for the corresponding original queries. When considering more top documents in SQM+Ak , the coverage is even smaller. These observations indicate that, at the document level, the search results of the original queries couldn't cover most relevant information related to query aspects. By using composite queries, we could get much more relevant information. Next, we evaluate the quality of the fine-grained information units generated by systems.
5.5.2 System Comparisons
Figure 6 shows the performance comparisons of different systems on Wikipedia test set, varying the word number of summary length limit. We can see that Q-Composite outperforms both Ling-2008 and Snippet. The results on TREC 2009 data have the similar trend, which are shown in Figure 7. We have found favorable results for Q-Composite on both NDCG@1 and NDCG@3. This shows proposed method is effective to extract more informative and aspect specific sentences. Especially, Q-Composite gains great improvement on NDCG@1, which is important for presenting condensing information on result pages.
To gain more insights, we analyze the label level distributions of the generated summary sentences of the 3 systems on TREC 20009 data set, as shown in Figure 8. The Xaxis are label levels. The Y-axis is the distribution. We can see that our method provides more informative sentences (level 5 and level 4) compared with baselines. However, all systems still generate less specific and informative sentences than navigational sentences. This indicates the task is really challenging.
Q-Composite performs better than Ling-2008. The reasons may include: (1) Ling-2008 estimates the aspect model on the search result of the original query. There may be not enough information covering all query aspects as shown in section 5.5.1. Therefore, for difficult aspects, it is unable to estimate accurate models. (2) In the search result of the original query, information related to multiple aspects often mixes together. It increases the difficulty to estimate discriminative aspect models. Therefore, it is more difficult to provide specific information for desired aspect. (3) The search result is so noisy that there are many navigational sentences. For example, sentences containing "actors" may also contain words like "cast", "list" and "actress". These words are very easy to have higher weights in aspect models and the sentences are ranked high as well. However, such sentences may only contain navigational information but can't provide direct information. Another reason affecting the performance of Ling-2008 may be that we did not implement the variation with regularization, which is more complex but reported having better performance than the basic algorithm with Dirichlet model priors. In contrast, by using composite query based method, we are able to get more aspect specific information and roughly classify the information. By distinguishing query dependent aspect words and query independent aspect words, we give bonus to sentences that are aspect specific but also contain more information beyond aspect words.
Snippet performs well on Wikipedia data set. It is reasonable, since the snippet generation algorithm favorites the

ROUGE-1

0.4

0.35

0.304

0.334 0.3008.320

0.3 0.265 0.258

0.2707.287

0.25

0.239

Q-Composite

0.2 Ling-2008

0.15

Snippet

0.1

0.05

0

200

400

600

words

Figure 6: ROUGE-1 performance of Q-Composite and baseline systems on Wikipedia test data.

Performance

0.700 0.600 0.500 0.400 0.300 0.200 0.100 0.000

0.282 0.190 0.146
NDCG@1

0.608

0.570

0.498

Q-Composite Ling-2008 Snippet

NDCG@3

Figure 7: Performance comparisons between systems on TREC 2009 data set.

sentences containing many query terms. Thus the generated summaries match many query aspect terms, which benefits ROUGE-1 metric, especially when the length of summaries is short. However, the snippets don't show much informative information. From the Figure 8, we can see that Snippet provides more level 2 sentences (specific but not informative), but very few level 4 and level 5 sentences. The generated sentences usually lack of detail description about the query aspect, mostly are just navigational sentences which often fail to satisfy user information need directly. Our method could get more aspect specific information by comparing the search results of multiple composite queries. Highlighting query dependent aspect words also helps select more informative sentences. Snippet generates less irrelevant sentences. One reason is that most sentences in snippets contain original query terms, while other methods don't have such constraint. Another reason may be that using composite queries may lead to topic drift, if the search results of the composite queries contain much noise.
5.5.3 The Impact of Query Dependent Aspect Words
Our method distinguishes the query dependent aspect words and query independent aspect words. We examine the impact of these two types of words. We set the parameter  to be 0.5 in Equation 5 , which means we do not distinguish query dependent and independent aspect words. We denote it as Q-Composite-AVG. Since the human judgements on TREC 2009 data directly measure the informativeness of the generated summaries, we compare Q-Composite ( = 0.0) and Q-Composite-AVG on this data set. Figure 9 shows the level distributions of the generated top 1 sentences. We can

332

Q-Composite

0.5

0.45
Label level 0.4

0.35

Percentage

0.3 Q-Composite
0.25 Ling-2008

0.2

Snippet

0.15

0.1

0.05

0

0

1

2

4

5 Label Levels

Figure 8: Level distributions of systems on TREC 2009 data set.

Percentage

0.4

0.35

0.3

0.25

0.2

Q-Composite

0.15

Q-Composite-AVG

0.1

0.05

0

0

1

2

4

5

Label Levels

Figure 9: Level distributions of Q-Composite and Q-Composite-Avg on TREC 2009 data set.

see that Q-Composite generates more informative sentences (level 4 and level 5). In contrast, the Q-Composite-AVG generates more specific but non-informative sentences (level 2). That is because Q-Composite favorites the words not only related to the aspect but also related to the original query. The results show that distinguishing query dependent aspect words and independent words is useful for identifying more informative sentences. However, we also see that QComposite selects slightly more irrelevant sentences. This is because some composite queries bring in more noise, which leads to topic drift.
5.5.4 The Impact of Search Engine Result
Our method uses the search results returned by the search engine. In this section, we examine whether the quality of returned documents can affect system performance. We simulate some not very good results, by removing some documents from the search results or randomly picking documents. We test on the Wikipedia test set, since the evaluation can be done automatically. In details, we evenly remove 5 documents from the top 50 search results, denoted as remove5, namely the 1st, 11st, 21st, 31st and 41st documents. We construct the remove15 in the same way. We also randomly sample 50 documents from the top 1000 results (denoted as random) and select the last 50 documents (denoted as tail).
The experimental results are shown in Figure 10. When the search results are not so bad (remove5 or remove15), where most of the documents are relevant, the results are comparable. However, as the relevant documents reduce and noisy data increases, the models may be not very accurate. It shows worse results on random and tail. The results indi-

ROUGE-1

0.250 0.200 0.150 0.100 0.050 0.000

0.221

0.217

0.217

0.204

Origin Remove5 Remove15 Random

0.180 Tail

Figure 10: The impact of seach engine, on Wikipedia test set using ROUGE-1 performance.

cate our method depends on the quality of the search engine search results. For difficult composite queries, there may be no enough relevant candidate sentences for summarization. More noise may lead to topic drift as well.
6. CONCLUSIONS AND FUTURE WORK
In this paper, we presented a multi-aspect oriented query summarization task. This task aims to summarize a query from multiple aspects which are aligned to user intents. Ideally, the users could get relevant information satisfying their information needs directly. Specifically, we formulated the task into 2 main phases: information gathering and summary generation. In the information gathering phase, we proposed a composite query based strategy, which proactively gets information based on the search engine. This strategy differs from traditional search result organization and text summarization, where the set of documents to be deal with is seen as a given system input. In the summary generation phase, we took into consideration the specificity, informativeness and redundancy for sentence selection. We conducted experiments on 2 data sets. Both automatic evaluation and manually judgements were explored. We emphasized that the quality of aspect oriented summaries should be evaluated according to their specificity and informativeness. The experimental results showed that by using composite queries, much more aspect relevant information could be got and our method outperformed 2 baselines for generating informative summaries.
The proposed method attempts to directly provide well organized and relevant information to users, as opposed to relevant documents. We have several possible directions of future work. First, in this paper we assume the query aspects are given. We would examine the system performance when using automatically mined query aspects. Second, more advanced methods could be exploited to integrate multiple sources of information related to a query for generating more informative summaries. Third, the composite query strategy could be applied for search result diversification by retrieving more aspect related documents.
Acknowledgments
The 1st, 4th and 5th authors are supported by the National Natural Science Foundation of China under Grant No. 60736044, by the National High Technology Research and Development Program of China No. 2011ZX01042-001-001, by Key Laboratory Opening Funding of MOE-Microsoft Key

333

Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, HIT.KLOF.2009020.
7. REFERENCES
[1] H. Chen and S. T. Dumais. Bringing order to the web: automatically categorizing search results. In CHI, pages 145≠152, 2000.
[2] V. Dang, X. Xue, and W. B. Croft. Inferring query aspects from reformulations using clustering. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM '11, pages 2117≠2120, New York, NY, USA, 2011. ACM.
[3] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, 39(1):1≠38, 1977.
[4] Z. Dou, S. Hu, K. Chen, R. Song, and J.-R. Wen. Multi-dimensional search result diversification. In Proceedings of the 4th ACM WSDM, pages 475≠484, New York, NY, USA, 2011. ACM.
[5] J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. Multi-document summarization by sentence extraction. In Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization, pages 40≠48, Stroudsburg, USA, 2000.
[6] M. A. Hearst. Clustering versus faceted categories for information exploration. Commun. ACM, 49:59≠61, April 2006.
[7] M. Hu and B. Liu. Mining and summarizing customer reviews. In W. Kim, R. Kohavi, J. Gehrke, and W. DuMouchel, editors, Proceedings of the 10th ACM SIGKDD, Seattle, Washington, USA, August 22-25, 2004, pages 168≠177. ACM, 2004.
[8] K. J®arvelin and J. Kek®al®ainen. Ir evaluation methods for retrieving highly relevant documents. In Proceedings of the 23rd annual international ACM SIGIR, pages 41≠48, New York, NY, USA, 2000. ACM.
[9] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram. A hierarchical monothetic document clustering algorithm for summarization and browsing search results. In Proceedings of the 13th international conference on WWW, pages 658≠665, New York, NY, USA, 2004. ACM.
[10] B. Y.-L. Kuo, T. Hentrich, B. M. . Good, and M. D. Wilkinson. Tag clouds for summarizing web search results. In Proceedings of the 16th ACM WWW, pages 1203≠1204, New York, NY, USA, 2007. ACM.
[11] D. J. Lawrie and W. B. Croft. Generating hierarchical summaries for web searches. In Proceedings of the 26th ACM SIGIR, pages 457≠458, New York, NY, USA, 2003. ACM.
[12] C.-Y. Lin and E. Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Conference of the NAACL Volume 1, pages 71≠78, Stroudsburg, PA, USA, 2003. Association for Computational Linguistics.
[13] X. Ling, Q. Mei, C. Zhai, and B. Schatz. Mining multi-faceted overviews of arbitrary topics in a text

collection. In Proceeding of the 14th ACM SIGKDD, pages 497≠505, New York, NY, USA, 2008. ACM.
[14] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th international conference on World Wide Web, pages 171≠180, New York, NY, USA, 2007. ACM.
[15] A. Nenkova, L. Vanderwende, and K. McKeown. A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization. In Proceedings of the 29th Annual International ACM SIGIR, Seattle, Washington, USA, pages 573≠580. ACM, 2006.
[16] S. Robertson and H. Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3:333≠389, April 2009.
[17] C. Shen, D. Wang, and T. Li. Topic aspect analysis for multi-document summarization. In Proceedings of the 19th ACM CIKM, pages 1545≠1548, New York, NY, USA, 2010. ACM.
[18] R. Song, M. Zhang, T. Sakai, M. Kato, Y. Liu, M. Sugimoto, Q. Wang, and N. Orii. Overview of the ntcir-9 intent task. In NTCIR-9 Proceedings, pages 82≠105. Morgan and Claypool, December 2011.
[19] A. Tombros and M. Sanderson. Advantages of query biased summaries in information retrieval. In Proceedings of the 21st ACM SIGIR, pages 2≠10, New York, NY, USA, 1998. ACM.
[20] C. Wang, F. Jing, L. Zhang, and H.-J. Zhang. Learning query-biased web page summarization. In Proceedings of the 6th ACM CIKM, pages 555≠562, New York, NY, USA, 2007. ACM.
[21] D. Wang, S. Zhu, T. Li, and Y. Gong. Multi-document summarization using sentence-based topic models. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 297≠300, Stroudsburg, PA, USA, 2009. Association for Computational Linguistics.
[22] X. Wang, D. Chakrabarti, and K. Punera. Mining broad latent query aspects from search sessions. In Proceedings of the 15th ACM SIGKDD, pages 867≠876, New York, NY, USA, 2009. ACM.
[23] X. Wang and C. Zhai. Learn from web search logs to organize search results. In Proceedings of the 30th annual international ACM SIGIR, pages 87≠94, New York, NY, USA, 2007. ACM.
[24] R. White and R. Roth. Exploratory search. beyond the query-response paradigm. In Synthesis Lectures on Information Concepts, Retrieval, and Services Series, Gary Marchionini (ed.), vol. 3. Morgan and Claypool, 2009.
[25] F. Wu, J. Madhavan, and A. Halevy. Identifying aspects for web-search queries. In Journal of Artificial Intelligence Research, pages 677≠700, 2011 (40).
[26] W.-t. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. Multi-document summarization by maximizing informative content-words. In Proceedings of the 20th IJCAI, pages 1776≠1782, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.
[27] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma. Learning to cluster web search results. In Proceedings of the 27th annual international ACM SIGIR, pages 210≠217, New York, NY, USA, 2004. ACM.

334

Automatic Term Mismatch Diagnosis for Selective Query Expansion

Le Zhao
Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA
lezhao@cs.cmu.edu

Jamie Callan
Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA
callan@cs.cmu.edu

ABSTRACT
People are seldom aware that their search queries frequently mismatch a majority of the relevant documents. This may not be a big problem for topics with a large and diverse set of relevant documents, but would largely increase the chance of search failure for less popular search needs. We aim to address the mismatch problem by developing accurate and simple queries that require minimal effort to construct. This is achieved by targeting retrieval interventions at the query terms that are likely to mismatch relevant documents. For a given topic, the proportion of relevant documents that do not contain a term measures the probability for the term to mismatch relevant documents, or the term mismatch probability. Recent research demonstrates that this probability can be estimated reliably prior to retrieval. Typically, it is used in probabilistic retrieval models to provide query dependent term weights. This paper develops a new use: Automatic diagnosis of term mismatch. A search engine can use the diagnosis to suggest manual query reformulation, guide interactive query expansion, guide automatic query expansion, or motivate other responses. The research described here uses the diagnosis to guide interactive query expansion, and create Boolean conjunctive normal form (CNF) structured queries that selectively expand `problem' query terms while leaving the rest of the query untouched. Experiments with TREC Ad-hoc and Legal Track datasets demonstrate that with high quality manual expansion, this diagnostic approach can reduce user effort by 33%, and produce simple and effective structured queries that surpass their bag of word counterparts.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query formulation, Retrieval Models
Keywords
Query term diagnosis, term mismatch, term expansion, Boolean conjunctive normal form queries, simulated user interactions
1. INTRODUCTION
Vocabulary mismatch between queries and documents is known to be important for full-text search. Recent research formally defined the term mismatch probability, and showed that on average a query term mismatches (fails to appear in) 40% to 50% of the documents relevant to the query [32]. With multi-word queries, the percentage of relevant documents that match the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08...$10.00.

whole query can degrade very quickly. Even when search engines do not require all query terms to appear in result documents, including a query term that is likely to mismatch relevant documents can still cause the mismatch problem: The retrieval model will penalize the relevant documents that do not contain the term, and at the same time favor documents (false positives) that happen to contain the term but are irrelevant. Since the number of false positives is typically much larger than the number of relevant documents for a topic [8], these false positives can appear throughout the rank list, burying the true relevant results.
This work is concerned with the term mismatch problem, a long standing problem in retrieval. What's new here is the term level diagnosis and intervention. We use automatic predictions of the term mismatch probability [32] to proactively diagnose each query term, and to guide further interventions to directly address the problem terms. Compared to prior approaches, which typically handle the query as a whole, the targeted intervention in this work generates simple yet effective queries.
Query expansion is one of the most common methods to solve mismatch. We use the automatic term mismatch diagnosis to guide query expansion. Other forms of intervention, e.g. term removal or substitution, can also solve certain cases of mismatch, but they are not the focus of this work. We show that proper diagnosis can save expansion effort by 33%, while achieving near optimal performance.
We generate structured expansion queries of Boolean conjunctive normal form (CNF) -- a conjunction of disjunctions where each disjunction typically contains a query term and its synonyms. Carefully created CNF queries are highly effective. They can limit the effects of the expansion terms to their corresponding query term, so that while fixing the mismatched terms, the expansion query is still faithful to the semantics of the original query. We show that CNF expansion leads to more stable retrieval across different levels of expansion, minimizing problems such as topic drift even with skewed expansion of part of the query. It outperforms bag of word expansion given the same set of high quality expansion terms.
2. RELATED WORK
This section discusses how this work relates to the other research that tries to solve the mismatch problem. In particular, research on predicting term mismatch and on conjunctive normal form (CNF) structured queries forms the basis of this work.
2.1 Term mismatch and automatic diagnosis
Furnas et al. [7] were probably the first to study vocabulary mismatch quantitatively, by measuring how people name the same concept/activity differently. They showed that on average 80-90% of the times, two people will name the same item differently. The best term only covers about 15-35% of all the occurrences of the item, and the 3 best terms together only cover 37-67% of the cases. Even with 15 aliases, only 60-80% coverage is achieved. The

515

authors suggested one solution to be "unlimited aliasing", which led to the Latent Semantic Analysis (LSA) [6] line of research.
Zhao and Callan [32] formally defined the term mismatch probability to be P | , the likelihood that term t does not appear in a document d, given that d is relevant to the topic (d R), or equivalently, the proportion of relevant documents that do not contain term t. Furnas et al. [7]'s definition of vocabulary mismatch is query independent, and can be reduced to an average case of Zhao and Callan [32]'s query dependent definition.
The complement of term mismatch is the term recall probability: P(t | R). A low P(t | R) means term t tends not to appear in the documents relevant to the topic. This query dependent probability P(t | R) is not new in retrieval research. It is known to be part of the Binary Independence Model (BIM) [23], as part of the optimal term weight. Accurate estimation of P(t | R) requires knowledge of R -- the relevant set of a topic, which defeats the purpose of retrieval, and P(t | R) was thought to be difficult to estimate.
Recent research showed that P(t | R) can be reliably predicted without using relevance information of the test topics [8,20,32]. Zhao and Callan [32] achieved the best predictions from being the first to design and use query dependent features for prediction, features such as term centrality, replaceability and abstractness.
Previously, P(t | R) predictions were used to adjust query term weights of inverse document frequency (idf)-based retrieval models such as Okapi BM25 and statistical language models. Term weighting is not a new technique in retrieval research, neither is predicting term weights.
Our work is a significant departure from the prior research that predicted P(t | R). We apply the P(t | R) predictions in a completely new way, to automatically diagnose term mismatch problems and inform further interventions.
2.2 CNF structured expansion
Query expansion is one of the most common ways to solve mismatch. In recent years, the research community has focused on expansion of the whole query, for example using pseudo relevance feedback [17]. This form of expansion is simple to manage and effective. It also allows introduction of expansion terms that are related to the query as a whole, even if their relationship to any specific original query term is tenuous.
When people search for information, they typically develop queries in Boolean conjunctive normal form (CNF). CNF queries are used by librarians [16,12], lawyers [2,26] and other expert searchers [4,13,21]. Each conjunct represents a high-level concept, and each disjunct represents alternate forms of the concept. Query expansion is accomplished by adding disjuncts that cover as many ways of expressing the concept as possible.
For example, the query below from TREC 2006 Legal Track [2]
"sales of tobacco to children"
is expanded manually into
(sales OR sell OR sold) AND (tobacco OR cigar OR cigarettes) AND (children OR child OR teen OR juvenile OR kid OR adolescent)
CNF queries ensure precision by specifying a set of concepts that must appear (AND), and improve recall by expanding alternative forms of each concept. Compared to LSA or bag of word expansion, CNF queries offer control over what query terms to expand (the query term dimension) and what expansion terms to use for a query term (the expansion dimension).
However, these two dimensions of flexibility also make automatic formulation of CNF queries computationally

challenging, and makes manual creation of CNF queries tedious. The few experiments demonstrating effective CNF expansion either used manually created queries or only worked for a special task. Hearst [13] and Mitra et al. [21] used ranked Boolean retrieval on manual CNF queries. Zhao and Callan [31] automatically created CNF queries for the question answering task, based on the semantic structure of the question.
Along the two directions of term diagnosis and expansion, prior research has focused on identifying synonyms of query terms, i.e. the expansion dimension. Google has patents [15] using query logs to identify possible synonyms for query terms in the context of the query. Jones and colleagues [14] also extracted synonyms of query terms from query logs. They called it query substitutions. Wang and Zhai [28] mined effective query reformulations from query logs. Dang and Croft [5] did the same with TREC Web collections. Xue, Croft and Smith [30] weighted and combined automatic whole-query reformulations, similar to the way alternative structured queries were combined in [31]. If more extensive expansions were used, the more compact CNF expansion would be a reasonable next step. Because of reasons such as suboptimal quality of expansions or insufficient number of topics for evaluation, prior research on ad hoc retrieval has not seen automatic CNF expansion to outperform keyword retrieval. Perhaps the only exception is the related problem of context sensitive stemming [27,22,3], where expansion terms are just morphological variants of the query terms, which are easier to identify and more accurate (less likely to introduce false positives).
Such prior work tried to expand any query term, and did not exploit the term diagnosis dimension, thus they essentially expanded the query terms whose synonyms are easy to find.
This work focuses on selectively expanding the query terms that really need expansion, a less well studied dimension. Exploiting this diagnosis dimension can guide further retrieval interventions such as automatic query reformulation or user interaction to the areas of the query that need help, leading to potentially more effective retrieval interventions. It also reduces the complexity of formulating CNF queries, manually or automatically. The prior research on synonym extraction is orthogonal to this work, and can be applied with term diagnosis in a real-world search system.
2.3 Simulated interactive expansions
Our diagnostic intervention framework is general and can be applied to both automatic and manual expansions. However, our experiments are still constrained by the availability of effective intervention methods. That is why we use manual CNF expansion, which is highly effective. To avoid using the expensive and less controllable online user studies, we use existing user-created CNF queries to simulate online diagnostic expansion interactions.
Simulations of user interactions are not new in interactive retrieval experiments. Harman [9] simulated relevance feedback experiments by using relevance judgements of the top 10 results, and evaluated feedback retrieval on the rest of the results. White et al. [29] also used relevance judgements and assumed several user browsing strategies to simulate users' interactions with the search interface. The interaction simulations were used as user feedback to select expansion terms. Similarly, Lin and Smucker [18] assumed a set of strategies that define how the user browses the search results, to simulate and evaluate a result browsing interface using known relevance judgements.
Compared to the prior work, our simulations never explicitly use any relevance judgements, and only make a few relatively weak assumptions about the user. We simulate based on existing user created Boolean CNF queries, which can be seen as recorded

516

Figure 1. Simulated diagnostic expansion, with query examples in gray, simulated steps in dashed boxes and methods to test in bold red font.

summaries of real user interactions. These fully expanded queries are used to simulate selective expansion interactions.
3. DIAGNOSTIC INTERVENTION
This section discusses in more detail the diagnostic intervention framework, and shows how term diagnosis can be applied and evaluated in end-to-end retrieval experiments in an ideal setting.
We hope to answer the following questions. Suppose the user is willing to invest some extra time for each query, how much effort is needed to improve the initial query (in expansion effort, how many query terms need to be expanded, and how many expansion terms per query term are needed)? When is the best performance achieved? Can we direct the user to a subset of the query terms so that less effort is needed to achieve a near optimal performance? What's an effective criterion for term diagnosis?
3.1 Diagnostic intervention framework
The diagnostic intervention framework is designed as follows. The user issues an initial keyword query. Given the query, the system selects a subset of the query terms and asks the user to fix (e.g. expand) them. The performance after user intervention is used to compare the different diagnosis strategies.
This evaluation framework needs to control two dimensions, the diagnosis dimension (selecting the set of problem query terms) and the intervention dimension (determining the amount of intervention for each selected term). Diagnosis of terms with mismatch problems can be achieved using criteria such as low predicted P(t | R) or high idf. The intervention dimension when implemented as query expansion can be controlled by asking the user to provide a certain number of expansion terms.
3.2 Query term diagnosis methods
We consider two term diagnosis methods, idf based term diagnosis and predicted P(t | R) based diagnosis.
The idf based diagnosis selects the query terms that have the highest idf first. Idf is known to have a correlation with P(t | R) [8] and has been used as a feature for predicting P(t | R) [8,20,32]. A rare term (high idf) usually means a high likelihood of mismatch, while a frequent word (e.g. stopword) would have a high P(t | R).
Diagnosis based on predicted P(t | R) selects the query terms with the lowest predicted P(t | R) first. We use the best known method to predict P(t | R) [32], which was the first to use query dependent features for prediction. It used top ranked documents from an initial retrieval to automatically extract query dependent synonyms. These synonyms were used to create some of the effective query dependent features, e.g. how often synonyms of a query term appear in top ranked documents from the initial retrieval, and how often such synonyms appear in place of the original query term in collection documents. Section 4 describes implementation details.
3.3 Possible confounding factors
To exactly follow this ideal framework, for each query, many user interaction experiments are needed ≠ one experiment for each

possible diagnostic intervention setup, preferably, one user per setup. Many factors need to be controlled, such as users' prior knowledge of the topic, the quality of the manual interventions, users' interaction time and interaction method (whether retrieval results are examined), so that the final retrieval performance will reflect the effect of the diagnostic component instead of random variation in the experiment setup. These factors are difficult to eliminate even with hundreds of experiments per topic. We show how simulations may help in the section below.
4. EXPERIMENTAL METHODOLOGY
In this section, we design the retrieval experiments to evaluate diagnostic interventions. We explain how user simulations may be an appropriate substitute for costly online experiments with human subjects. We explain how to design the diagnostic intervention experiment so that it measures the effects of term diagnosis, minimizing effects from confounding factors such as the quality of the post-diagnosis interventions. We examine the datasets used for this simulation, in particular how well the manual CNF queries fit the simulation assumptions. We also describe the whole evaluation procedure, including the implementation of the mismatch diagnosis methods which (together with the user intervention) produce the post-intervention queries, the retrieval model behind query execution, and the evaluation metrics used to measure retrieval effectiveness.
We focus on interactive expansion as the intervention method, using existing CNF queries to simulate interactive expansion. This is due to both the effectiveness of query expansion to solve mismatch and the lack of user data for other types of interventions.
4.1 Simulated user interactions
As explained in Section 3.3, a large number of experiments and users are needed to evaluate diagnostic expansion using online user studies. To avoid this, we use offline automatic simulations, sketched in Figure 1. There are three players in the simulation, the user, the diagnostic expansion retrieval system and the simulation based evaluation system. Before evaluation, the user creates fully expanded CNF queries. These manually created CNF queries are used by the evaluation system to simulate selective expansions, and are accessible to the diagnostic retrieval system only through the simulation system. During evaluation, the simulation system first extracts a basic no-expansion keyword query from the CNF query, feeding the keyword query to the diagnosis system. The diagnostic expansion system automatically diagnoses which keywords are more problematic. Then, the simulation system takes the top several problem terms, and extracts a certain number of expansion terms for each selected query term from its corresponding conjunct in the manual CNF query. This step simulates a user expanding the selected query terms. The number of problem query terms to expand and the number of expansion terms to include are controlled by the simulation system, which will evaluate retrieval at five different selection levels and five different expansion levels. Finally, given

517

these expansion terms for each selected query term, the diagnostic expansion system forms an expansion query and does retrieval.
For example, based on the CNF query in Section 2.2, the diagnosis method is given the keyword query sales tobacco children. It may see children as more problematic than the other terms, then a full expansion of this problem term would produce the query sales AND tobacco AND (children OR child OR teen OR juvenile OR kid OR adolescent), whose retrieval performance is evaluated as the end result of diagnostic expansion. If the evaluation system selects two query terms sales and children for expansion, with a maximum of one expansion term each, the final query would be (sales OR sell) AND tobacco AND (children OR child). These diagnostic expansion queries are partial expansions simulated using the fully expanded queries created by real users.
Our simulation allows us to answer the same set of questions about the diagnostic expansion system which we hope to answer through online user interactions, and requires simpler experiments. In our simulations, the same set of expansion terms is always used for a given query term, those from its corresponding CNF conjunct. Doing so minimizes the variation from the expansion terms as we measure the effects of the diagnosis component. The order in which expansion terms are added for a query term is also fixed, in the same order as they appear in the CNF conjunct. This way, we can tweak the level of expansion by gradually including more expansion terms from the lists of expansion terms, and answer how much expansion is needed for optimal performance.
Our simulation makes three assumptions about the user expansion process. We examine them below. Expansion term independence assumption: Expansion terms from fully expanded queries are held back from the query to simulate the selective and partial expansion of query terms. This simulation is based on the assumption that the user (a random process that generates expansion terms) will produce the same set of expansion terms for a query term whenever asked to expand any subset of the query terms. Equivalently, given the topic, the expansion process for one query term does not depend on the expansion of other query terms. In reality, a human user focusing on a subset of the query terms can typically achieve higher quality expansion. Thus, selective expansion may actually do better than the reported performance from the simulations. Expansion term sequence assumption: Controlling to include only the first few expansion terms of a query term simulates and measures a user's expansion effort for that query term. It is assumed that the user would come up with the same sequence of expansion terms for each query term, no matter whether the user is asked to expand a subset or all of the query terms. A downside of this simulation is that we do not know exactly how much time and effort the user has spent on each expansion term. CNF keyword-query induction assumption: Instead of actually asking users to expand their initial queries, preexisting fully expanded CNF style queries are used to infer the original keyword query and to simulate the expansion process. For example, given the CNF query in Section 2.2, the original keyword query is assumed to be (sales tobacco children). This simulation assumes that the original keyword query can be reconstructed from the manual CNF query, which could be missing some original query terms (of and to in the example) or introduce new terms into the original keyword query. However, as long as we use highly effective CNF queries, it is safe to use the CNF induced keyword queries as the no-expansion baseline.
We also made an effort to ensure that our `reverse-engineered' keyword query is faithful to the vocabulary of the original query. Given the TREC official topic description of a topic, we try to use

the first term from each conjunct that appears in this description to reconstruct the keyword query. For conjuncts that do not have description terms, the first term in the conjunct is used.
For example, the topic described as sales of tobacco to children, with CNF query (sales OR sell OR sold) AND (tobacco OR cigar OR cigarettes) AND (children OR child OR teen OR juvenile OR kid OR adolescent), would have (sales tobacco children) as the unexpanded keyword query. If the description were sell tobacco to children, the keyword query would be instead (sell tobacco children), even when sales appears first in its conjunct.
4.2 Effects of confounding factors
Using user simulations instead of real users can eliminate confounding factors such as the user's prior knowledge of the topic and other details of the user interaction process.
This work tests the hypothesis that term diagnosis can effectively guide query expansion. However, two factors directly determine the end performance of diagnostic expansion, 1) the effectiveness of term diagnosis, and 2) the benefit from expansion.
Since our focus is on diagnosis, not query expansion, one of the most important confounding factors is the quality of the expansion terms, which we leave out of the evaluation by using a fixed set of high quality expansion terms from manual CNF queries to simulate an expert user doing manual expansion.
Automatic query expansion is more desirable in a deployed system, but the uncertain quality of the expansion terms can confuse the evaluation. Thus, it is not considered in this paper.
4.3 Datasets and manual CNF queries
Datasets with high quality manual CNF queries are selected to simulate and evaluate diagnostic expansion. Four different datasets have been used, those from TREC 2006 and 2007 Legal tracks, and those from TREC 3 and 4 Ad hoc tracks. They are selected in pairs, because training data is needed to train the P(t | R) prediction model. Here, the TREC 2006 (39 topics) and TREC 3 (50 topics) datasets are used for training the baseline model parameters and the P(t | R) prediction models, while TREC 2007 (43 topics) and TREC 4 (50 topics) are used for testing.
4.3.1 TREC Legal track datasets
The TREC Legal tracks contain Boolean CNF queries created through expert user interaction. They are fairly specific, averaging 3 conjuncts per topic, i.e., 3 concepts conjoined to form a query. The information needs of the Legal track topics are fictional, but mimic the real cases.
The lawyers who created the TREC Legal queries know what the collection is, and have expert knowledge of what terminology the corpus documents might use to refer to a concept being requested. The lawyers would give very high priority to the recall of the queries they create. They tried to fully expand every query term, so as not to miss any potentially relevant document. An effort to avoid over-generalizing the topic was also made. However, the lawyers never looked at the retrieval results when creating these CNF style queries. We call this a case of blind user interaction, because no corpus information is accessed during user interaction. We use the Boolean queries from [33], which achieved near best performance in TREC 2007 Legal track.
The 2006 and 2007 TREC Legal tracks share the same collection of documents. These are about 6.7 million tobacco company documents made public through litigation. They are on average 1362 words long. Many of them are OCR text, and contain spelling and spacing errors.
For relevance judgments, because of the relatively large size of the collection, a sampled pooling strategy was adopted in Legal

518

2007, with 555.7 judgments per topic and 101 judged relevant documents per topic.
More details about the dataset, the information needs, query formulation procedure, and relevance judgments can be found in the overview papers [2,26].

4.3.2 TREC Ad hoc track datasets

For the TREC 3 and 4 Ad hoc track datasets, high quality manual CNF queries were created by the University of Waterloo group [4]. An example query is (responsibility OR standard OR train OR monitoring OR quality) AND (children OR child OR baby OR infant) AND "au pair", where the "au pair" is a phrase. The information needs for the TREC 3 and 4 Ad hoc tracks are simpler (or broader), averaging 2 conjuncts per topic.
The Waterloo queries were created for the MultiText system by an Iterative Searching and Judging (ISJ) process. These queries were manually formulated with access to the results returned by the retrieval system, thesaurus and other external resources of knowledge. This constitutes a case of user and corpus interaction. Quality of the manual Boolean queries is ensured by examining the retrieval results, thus, should be better than those created from blind user interaction of the Legal tracks. Since the interaction with search results, expansion processes of the query terms may not be independent of each other. For example, in order to discover the expansion term of a query term, one may need to expand another query term first, to bring up a result document that contains the expansion term. Thus, the expansion independence assumption (of Section 4.1) is more likely to be violated by the ISJ queries than by the Legal ones.
The TREC 3 and 4 Ad hoc tracks used different collections, but they both consisted of newswire texts published before 1995. Each collection has about 0.56 million documents. The texts are non-OCR, thus cleaner than the Legal documents.
The relevance judgments of the Ad hoc tracks are deeper, because the collections are much smaller. The TREC 4 Ad hoc track made 1741 judgments per topic with 130 relevant. More details can be found in [10,11].
For all documents and queries, the Krovetz stemmer was used (more conservative than Porter), and no stopwords were removed.

4.4 Term diagnosis implementation

We explain the implementation of the diagnosis methods, idf and

predicted P(t | R), in more detail.

Idf is calculated as log

df /df , where N is the total

number of documents in the collection and df is the document

frequency of the term. This follows the RSJ formulation [23].

For P(t | R) prediction, we closely follow [32]'s method.

Automatic features used for prediction include idf, and the three

features derived from applying latent semantic analysis (LSA) [6] over the top ranked documents of an initial keyword retrieval.1

For training purposes, P(t | R) truth is calculated as

1 / | | 2 , where is the number of relevant documents

containing t and | | the total number of relevant documents for

the query, with Laplace smoothing used. Support Vector

Regression with RBF kernel is used to learn the prediction model.

There are 3 parameters: The number of top ranked documents

for LSA, which is set at 180 for the Ad hoc datasets and 200 for

the Legal track datasets, based on a monotonic relationship

between this parameter and the total number of collection

1 Recently a more efficient method of predicting P(t | R) was developed that eliminates the need for an initial retrieval [Zhao, personal communication].

documents observed [32]. The number of latent dimensions to keep is fixed at 150, and the gamma parameter which controls the width of the RBF kernel is fixed at 1.5 (as in [32]).
[32] also used a feature that indicated whether a word in the query appears as a leaf node in the dependency parse of the query. Here, the feature is assumed to be 0 for all query terms, because the unexpanded query is usually not a natural language sentence or phrase, hence parsing may be inappropriate.
A small number of the original terms in these CNF queries are phrases, windowed occurrences or other complex structures. They are assumed to have a P(t | R) value of 0.5. The LSA component of the Lemur Toolkit is not designed to handle these complex terms, preventing the use of [32]'s model. This is a small handicap to our P(t | R) prediction implementation, but not to the idf method, which is based on accurate df values calculated by the Indri search engine.
4.5 The retrieval model
To achieve a state-of-the-art performance, the retrieval model needs to rank collection documents using the Boolean CNF queries. Before the mid 1990's, unranked Boolean was popular. Later research found ranked keyword to be more effective. However, to be fair, a ranked Boolean (e.g. soft or probabilistic) model should be used to compare with other ranking approaches.
This work adopts the language model framework, using probabilistic Boolean query execution (with Lemur/Indri version 4.10) [19]. The Boolean OR operator is still the hard OR, treating all the synonyms as if they are the same term for counting termand document-frequencies (i.e. #syn operator in Indri query language). The Boolean AND is implemented as the probabilistic AND (the Indri #combine operator) to produce a ranking score.
Equations (1, 2) show how the retrieval model scores document d with query (a OR b) AND (c OR e). tf(a, d) is the number of times term a appears in document d.  is the parameter for Dirichlet smoothing, which is set at 900 for the Ad hoc datasets and 1000 for the Legal datasets based on training.

Score( (a OR b) AND (c OR e), d)

(1)

= P( (a OR b) AND (c OR e) | d)

= P( (a OR b) | d) * P( (c OR e) | d)

P( (a OR b) | d)

(2)

= ( tf(a, d) + tf(b, d) +  * (P(a | C) + P(b | C)) ) / (length(d) + )

= P(a | d) + P(b | d)

(under Dirichlet smoothing)

This language model based ranked Boolean model is not the only possibility. Other ranked Boolean models include using the Boolean query as a two-tiered filter for the keyword rank list [13] [31], using the Okapi BM25 model for the conjunction [25], using probabilistic OR for the expansion terms (in Indri query language, #or instead of #syn), or using the p-norm Boolean ranking model [24]. We have tried some basic variations of the language model ranked Boolean model. Our pilot study shows that for our datasets, tiered filtering is sometimes worse than probabilistic Boolean, mostly because of the inferior ranking of the keyword queries. Probabilistic OR (#or) is numerically similar to treating all expansion terms the same as the original term (#syn), and the two methods perform similarly in retrieval. We did not try Okapi or p-norm, because the focus of this paper is P(t | R) based diagnostic expansion, not to find the best ranked Boolean model. What is needed is one ranked Boolean model that works.

519

4.6 Evaluation measures
We use standard TREC evaluation measures for the datasets. Traditionally, pooled judgments and precision at certain cutoffs have been used in TREC. Mean Average Precision (MAP) at top 1000 is a summary statistic that cares about both top precision and precision at high recall levels, and has been used as the standard measure in TREC Ad hoc and Legal tracks.
The statAP measure [1] is the standard measure for TREC Legal 2007. StatAP is an unbiased statistical estimate of MAP designed to work with sampled pooling. It is unbiased in the sense that if all pooled documents were judged, the MAP value would have been the same as the mean of the estimated statAP. In traditional TREC pooling, the top 50 to top 100 documents from each submitted rank list are pooled, and all pooled documents are judged. In sampled pooling, only a sampled subset of the pool is judged. The idea is to use importance sampling to judge fewer documents while maintaining a reliable estimate of MAP. Highly ranked documents from multiple pooled submissions are more likely to be relevant, and they are sampled more by importance sampling. StatAP takes into account these sampling probabilities of the judged relevant documents, so that during evaluation, a judged relevant document with sampling probability p would be counted as a total of 1/p relevant documents. This is because on average 1/p ≠ 1 relevant documents are missed during the sampling procedure, and they are being represented by that one sampled relevant document.
For topics where some relevant documents have low sampling probabilities, statAP estimates can deviate from the true AP a lot, but according to [26], when averaged over more than 20 topics, statAP provides a reliable estimate.
5. EXPERIMENTS
These experiments test two main hypotheses. H1: Mismatch diagnosis can direct expansion to the query terms that need expansion. H2: Boolean CNF expansion is more effective than bag of word expansion with the same set of high quality expansion terms. To test H1, the first experiment verifies the accuracy of idf and P(t | R)-prediction based term diagnosis against the true P(t | R). The second experiment shows the effects of diagnosis by evaluating overall retrieval performance along the query term dimension (5 diagnostic selection levels) and the expansion dimension (5 expansion levels). The third experiment compares predicted P(t | R) diagnosis with idf based diagnosis. H2 is tested by the fourth experiment comparing CNF and bag-ofword expansion at various levels of diagnostic expansion.
5.1 Baseline ≠ no expansion
Listed below is the retrieval performance of the no expansion keyword retrieval baseline on the two test sets.

Dataset

Legal 2007 (MAP/statAP) TREC 4 (MAP)

no expansion 0.0663/0.0160

0.1973

5.2 Mismatch diagnosis accuracy
Our goal is to use idf or P(t | R) predictions to diagnose query terms, to rank them in a priority order for the user to fix (expand). This section is a unit test of the diagnosis component, in which accuracy is measured by how well the diagnosis method identifies the most problematic query terms (those most likely to mismatch). We measure how well the priority order (e.g. ascending predicted

Gain in retrieval (MAP) 100%

90%

80%

70%

60%

50%

40%

30%

20%

10%

0%

0

1

2

P(t | R) on TREC 2007 idf on TREC 2007 P(t | R) on TREC 4 idf on TREC 4

# query terms selected

3

4

all

Figure 2. Relative retrieval performance gains of diagnostic expansion as the number of query terms selected for expansion increases. Calculated based on the last row of Tables 2 and 3.

P(t | R)) ranks the query term with the true lowest P(t | R), thus use Mean Reciprocal Rank (MRR) as the measure. Rank correlation measures do not distinguish ranking differences at the top vs. bottom of the rank lists, thus are less appropriate here.
On the Legal 2007 dataset, predicted P(t | R) achieves an MRR of 0.6850, significantly higher than the MRR of 0.5523 of the idf method, significant at p < 0.026 by the two tailed t-test. The idf method is still much better than random chance which has an MRR of 0.383, given the average 3 conjuncts per topic.
This result that P(t | R) prediction using [32]'s method is better than idf, and idf is better than random is consistent with prior research that predicted P(t | R) [8,32].
5.3 Diagnostic expansion retrieval results
Tables 1 2 and 3 report the expansion retrieval performance of predicted-P(t | R) based and idf based diagnostic expansion, following the evaluation procedure detailed in Section 4.1. The results are arranged along two dimensions of user effort, the number of query terms selected for expansion, and the maximum number of expansion terms to include for a selected query term.
For example, results reported in column 2 row 2 selects 1 original query term of the highest idf for expansion, and a maximum of 1 expansion term is included for the selected query term. When the manual CNF query doesn't expand the selected query term, no expansion term will be included in the final query.
We are most concerned with the performance changes along each row of the tables, which are caused by the diagnosis methods. In Figure 2, we compare the relative performance gains of the different diagnosis methods as more query terms are being selected for expansion. Results based on the last row of Tables 2 and 3 are presented in Figure 2. No expansion is 0%, and full expansion of all query terms gets 100%. With only 2 query terms selected for expansion, predicted P(t | R) diagnosis is achieving 95% or 90% of the total gains of CNF expansion. Idf diagnosis is only achieving 64% or 83% of the total gains with 2 query terms, and need to fully expand 3 query terms to reach a performance close to the best (full expansion of all query terms). Thus, predicted P(t | R) based diagnosis saves 1/3 of users' expansion effort while still achieving near optimal retrieval performance.

520

Table 1. Retrieval performance of the two selective CNF expansion methods on TREC 2007 Legal track, as measured by MAP. The baseline unexpanded queries produced an MAP of 0.0663.

\ # terms selected 1

2

3

4

All

# expansions per term\ idf

P(t | R) idf

P(t | R) idf

P(t | R) idf

P(t | R) (same)

1

0.0722 0.0778* 0.0802* 0.0825** 0.0892** 0.0896*** 0.0893** 0.0904*** 0.0901***

2

0.0780* 0.0805** 0.0825* 0.0921*** 0.0916*** 0.0938# 0.0947*** 0.0961# 0.0971#

3

0.0766 0.0806** 0.0844** 0.0927*** 0.0938*** 0.0965# 0.0969*** 0.0988# 0.0997#

4

0.0770 0.0809** 0.0859** 0.0948# 0.0968# 0.0993# 0.0996# 0.1015# 0.1024#

All

0.0754 0.0798* 0.0862** 0.0958*** 0.0986# 0.1008# 0.1016# 0.1031# 0.1039#

* significantly better than the no expansion baseline by both randomization & sign tests at p<0.05. ** p<0.01 by both tests, *** p<0.001 by both, # p<0.0001 by both tests. (Same notation is used for the other tables.)
Table 2. Retrieval performance of the two selective CNF expansion methods on TREC 2007 Legal track, as measured by the TREC standard statAP measure. The baseline unexpanded queries produced a statAP of 0.0160. (Statistical significance tests are omitted, as they are inappropriate for the sampling based statAP measure [26].)

\ # query terms selected 1

2

3

4

All

# expansions per query term\ idf

P(t | R) idf

P(t | R) idf

P(t | R) idf

P(t | R) (same)

1

0.0164 0.0233 0.0184 0.0246 0.0279 0.0282 0.0279 0.0282 0.0282

2

0.0176 0.0255 0.0189 0.0289 0.0273 0.0295 0.0288 0.0302 0.0300

3

0.0175 0.0256 0.0191 0.0290 0.0280 0.0304 0.0291 0.0307 0.0304

4

0.0176 0.0256 0.0194 0.0295 0.0292 0.0311 0.0301 0.0317 0.0314

All

0.0185 0.0345 0.0381 0.0490 0.0491 0.0504 0.0493 0.0508 0.0505

Table 3. Retrieval performance of the two selective CNF expansion methods on TREC 4 Ad hoc track, as measured by the TREC standard MAP measure. The baseline unexpanded queries produced an MAP of 0.1973.

\ # terms selected 1

2

3

4

All

# expansions per term\ idf

P(t | R) idf

P(t | R) idf

P(t | R) idf

P(t | R) (same)

1

0.2087 0.2279*** 0.2341* 0.2366** 0.2350** 0.2358** 0.2356** 0.2358** 0.2358**

2

0.2135 0.2392# 0.2503** 0.2541*** 0.2552*** 0.2567*** 0.2578*** 0.2581*** 0.2581***

3

0.2187* 0.2435*** 0.2538*** 0.2539# 0.2589*** 0.2608# 0.2619# 0.2622# 0.2622#

4

0.2242* 0.2489# 0.2654*** 0.2659# 0.2706*** 0.2731# 0.2753# 0.2756# 0.2756#

All

0.2319** 0.2526*** 0.2775# 0.2835# 0.2875*** 0.2916# 0.2935# 0.2938# 0.2938#

Tables 1, 2 and 3 show that the more query terms selected for expansion, the better the performance. This is not surprising, as we are using carefully expanded manual queries. Similarly, including more expansion terms (along each column) almost always improves retrieval, except for the idf method in Table 1 with only one query term selected for expansion.
The improvement over the no expansion baseline becomes significant after expanding two query terms for the idf method, and after only expanding one query term for predicted P(t | R).
Overall, P(t | R) diagnostic expansion is more stable than the idf method. This shows up in several areas. 1) Including more expansion terms always improves performance, even when only one original query term is selected for expansion. 2) Performance improvement over the no expansion baseline is significant even when only including one expansion term for one query term. These are not true for idf diagnosis. 3) Only two query terms need to be selected for expansion to achieve a performance close to the best, 33% less user effort than that of idf based diagnosis.
The statAP measure from Table 2 correlates with the MAP measure, however, the sudden increases in statAP from the 2nd last row to the last row are not present in the case of MAP. A bit of

investigation shows that 1 to 2 topics benefited a lot from the extra (more than 4) expansion terms. The benefit is because of the successful matching of some relevant documents with low sampling probability, which increases statAP a lot, but not MAP. On the Legal 2007 dataset, the topic that benefited most from the more than 4 expansion terms is a topic about James Bond movies. Certainly, there are more than 4 popular James Bond movies.
Overall, predicted P(t | R) can effectively guide user expansion to improve retrieval. Expanding the first few query terms can result in significant gains in retrieval. The gain diminishes as more query terms are expanded, eventually leading to the best performance of expanding every query term.
5.4 P(t | R) vs. idf based diagnostic expansion
The subsection above shows that diagnosis can help reduce expansion effort, and that P(t | R) diagnosis results in more stable retrieval than idf. This section directly compares two retrieval experiments using predicted P(t | R) vs. idf guided expansion.
The two experiments both select 1 query term for full expansion (Table 1 last row, 2nd vs. 3rd column from the left). The MAP difference between 0.0754 of idf and 0.0798 of P(t | R) is not

521

Table 4. Retrieval performance of P(t | R) guided bag of word expansion on TREC 2007 Legal track, as measured by MAP/statAP. The baseline unexpanded queries produced an MAP/statAP of 0.0663/0.0160.

\ # query terms selected 1

2

3

4

All

# expansions per query term\

1

0.0755**/0.0210 0.0744 /0.0172 0.0808 /0.0271 0.0768 /0.0260 0.0764***/0.0260

2

0.0795**/0.0229 0.0814** /0.0216 0.0892** /0.0242 0.0883** /0.0243 0.0867***/0.0242

3

0.0789**/0.0217 0.0829***/0.0221 0.0880** /0.0206 0.0894** /0.0207 0.0878** /0.0206

4

0.0789**/0.0217 0.0821** /0.0205 0.0908***/0.0203 0.0993***/0.0213 0.0927***/0.0214

All

0.0791**/0.0219 0.0833** /0.0217 0.1006# /0.0207 0.1038# /0.0211 0.1014# /0.0200

Comparing diagnosis methods: P(t | R) vs. idf
y - MAP Difference (correct prediction, and better MAP)0.15
0.1

0.05

x - P(t | R) Difference

-1

-0.5

0 0
-0.05

(correct prediction, but lower MAP) -0.1

(incorrect prediction, and lower MAP)

Figure 3. Difference in prediction accuracy vs. difference in MAP for the two selective query expansion methods on 43 TREC 2007 Legal Track topics. The X axis shows the difference in true P(t | R) between the first query terms selected by each method. The Y axis shows the difference in MAP between queries expanded by each method. The differences are calculated as that from predicted P(t | R) based diagnosis minus that from idf based diagnosis. Points surrounded by a diamond represent topics in which one method selected a term that had no expansions.

statistically significant. We investigate why below. According to the diagnostic expansion framework, two causes are possible, 1) the P(t | R) predictions are poor, selecting the wrong query terms to expand, or 2) the quality of the expansion terms that idf selected happen to be higher, causing the idf method to have better MAP sometimes, thus decreasing statistical significance.
To separate the effects of diagnosis and expansion, we plot the Legal 2007 topics along two dimensions in the scatter-plot Figure 3. The x axis represents the diagnosis dimension: the difference between the true P(t | R) of the two query terms selected by lowest predicted P(t | R) and highest idf. The y axis represents the expansion performance dimension: the difference between the Average Precision (AP) values of the two methods on a topic. When idf and predicted P(t | R) happen to select the same term to expand for a given topic, that topic would be plotted on the origin (x=0, y=0) ≠ no difference in both diagnosis and expansion.
From Figure 3, firstly, most points have x < 0, meaning the P(t | R) predictions are better at finding the low P(t | R) terms than the idf based diagnosis. Secondly, most points are in the top left and

Table 5. Retrieval performance of P(t | R) guided bag of word expansion on TREC 4 Ad hoc track, as measured by MAP. The baseline unexpanded queries produced an MAP of 0.1973.

\#qt 1

2

3

4

All

#exp\

1

0.2101** 0.2102* 0.2117** 0.2113** 0.2113**

2

0.2146*** 0.2161*** 0.2200** 0.2201** 0.2201**

3

0.2160*** 0.2154*** 0.2222*** 0.2226*** 0.2218**

4

0.2204# 0.2272*** 0.2288*** 0.2309** 0.2309**

All 0.2215# 0.2290** 0.2329** 0.2343** 0.2384**

bottom right quadrants, supporting the theory that expanding the term with lower P(t | R) leads to better retrieval performance. In the bottom right quadrant, occasionally idf method picks the right terms with lower P(t | R) to expand, and does better in retrieval.
However, there are three outliers in the bottom left quadrant, which are not fully explained by our theory. At the bottom left quadrant, predicted P(t | R) does identify the right term with a lower P(t | R), but the retrieval performance is actually worse than that of idf guided expansion.
By looking into these three topics, we found that the manual queries for topics 76 and 86 do not have any expansion terms for the query terms selected by P(t | R), while the idf selected terms do have effective expansion terms. All such topics where a query term without expansion terms is selected are annotated with diamond shaped borders in the plot. Topic 55 is because of poor expansion term quality. The P(t | R) method selects the chemical name apatite for expansion, which represent a class of chemicals. The manual expansion terms seem very reasonable, and are just names or chemical formulas of the chemicals belonging to the apatite family. However, the query is really about apatite rocks as they appear in nature, not any specific chemical in the apatite family. Thus, even expansion terms proposed by experts can still sometimes introduce false positives into the rank list, and this problem cannot be easily identified without corpus interaction, e.g. examining the result rank list of documents.
If these 3 topics were removed from evaluation, predicted P(t | R) guided expansion would be significantly better than idf guided expansion, at p < 0.05 by the two tailed sign test.
Of the 50 TREC 4 topics, similarly, 4 topics are outliers. In two cases, the P(t | R) selected query terms do not have manual expansion terms. In one topic, P(t | R) prediction did not select the right term, but MAP is higher than idf diagnosis, because the idf method selected a query term with poor expansion terms. In one topic, the retrieval performance does not differ at top ranks, and the idf method only excels after 30 documents.

522

Overall, most topics confirm our hypothesis that expanding the query term likely to mismatch relevant documents leads to better retrieval, and better diagnosis also leads to better retrieval.
This analysis also shows the inherent difficulty of evaluating term diagnosis in end-to-end retrieval experiments. Even with high quality manual expansion terms, there is still some variation in the quality of the expansion interventions, which can still interfere with the assessment of the diagnosis component.
5.5 Boolean CNF vs. bag of word expansion
We compare CNF style expansion with two advanced bag-ofword expansion methods.
For a fair comparison with manual CNF expansion, our first bag of word expansion baseline also uses the set of manual expansion terms selected by predicted P(t | R). Expansion terms are then grouped and combined with the original query for retrieval.
To make this baseline strong, both individual expansion terms and the expansion term set can be weighted. The individual expansion terms are weighted with the Relevance Model weights [17] from an initial keyword retrieval, with the parameter (the number of feedback documents) tuned on the training set. Manual expansion terms that do not appear in the feedback documents are still included in the final query, but a minimum weight is used to conform to the relevance model weights. Uniform weighting of the expansion terms was also tried. It is more effective than relevance model weights when expansion is more balanced, i.e. more than 3 query terms are selected for expansion. When combining the expansion terms with the original query, the combination weights are 2-fold cross-validated on the test set.
Table 4 shows the best case of both relevance-model-weight and uniform-weight bag of word expansion. Bag of word expansion performs worse than CNF expansion in almost all the different setups. The best performance is achieved with full expansion of 4 query terms, with a MAP of 0.1038, slightly lower than that of CNF (0.1039 in Table 1), however, the statAP value of 0.0211 is much worse than that of CNF (0.0508, Table 2).
Table 5 shows the best case bag of word expansion results on the TREC 4 Ad hoc dataset. Consistent with the statAP measure on TREC Legal 2007, CNF queries are much better than bag of word expansion. For example, with full expansion of all query terms, CNF expansion (Table 3) gets a MAP of 0.2938, 23% better than 0.2384 of the bag of word expansion with the same expansion terms, significant at p < 0.0025 by the randomization test and weakly significant at p < 0.0676 by the sign test.
Some results of bag of word retrieval at low selection levels, i.e. selecting one query term to expand, perform better than idf guided CNF expansion. But since the bag of word expansion here uses better expansion terms selected by predicted P(t | R), this does not mean that bag of word is sometimes better than CNF expansion.
The second bag of word expansion baseline is the standard Lavrenko Relevance Model itself [17], which uses automatic feedback terms, instead of manual ones, for expansion. Parameters trained on Legal 2006 dataset when applied to Legal 2007 lead to an MAP of 0.0606, statAP of 0.0168, worse than the no expansion baseline. On TREC 4, it gets a MAP of 0.2488, slightly better than 0.2384, the best manual bag of word expansion, but still much worse than CNF (0.2938).
In sum, given the same set of high quality expansion terms, CNF expansion works much better than bag of word expansion.
6. CONCLUSIONS
We set out with the hypothesis that term mismatch based diagnosis can successfully guide further retrieval intervention to fix `problem' terms and improve retrieval. In this work, we

applied the term mismatch diagnosis to guide interactive query expansion. Simulated interactive query expansion experiments on TREC Ad hoc and Legal track datasets not only confirmed this hypothesis, but also showed that automatically predicted P(t | R) probabilities (the complement of term mismatch) can accurately guide expansion to the terms that need expansion most, and lead to better retrieval than when expanding rare terms first. From the user's point of view, it usually isn't necessary to expand every query term. Guided by predicted P(t | R), expanding two terms is enough for most topics to achieve close-to-top performance, while guided by idf (rareness), three terms need to be expanded. P(t | R) guidance can save user effort by 33%.
In addition to confirming the main hypothesis, experiments also showed that Boolean conjunctive normal form (CNF) expansion outperforms carefully weighted bag of word expansion, given the same set of high quality expansion terms. The unstructured bag of word expansion typically needs balanced expansion of most query terms to achieve a reliable performance.
Although the effect from adding more expansion terms to a query term diminishes, for the query terms that do need expansion, the effects of the expansion terms are typically additive, the more the expansion the better the performance. This is consistent with prior observations on vocabulary mismatch, that even after including more than 15 aliases, the effects of mismatch can still be observed, and further expansion may still help [7].
For bag of word expansion, including more manual expansion terms also helps, but requires a balanced expansion of most query terms, and is not as effective and stable as CNF expansion.
This work is mostly concerned with automatic diagnosis of problem terms in the query, and presents them to the user for manual expansion. It is still a question whether the diagnosis can help automatic formulation of effective CNF queries. We hope to let the system suggest or select expansion terms, automatically or semi-automatically with minimal user effort. Automatic identification of high quality expansion terms would be useful when the candidate expansion terms may not be of high quality, e.g. expansion terms from result documents, thesaurus or nonexpert users. Poor expansion terms in CNF queries are especially harmful, when they over-generalize the query and introduce false positives throughout the rank list. Tools such as performance prediction methods (e.g. query clarity), may help in such scenarios to detect the adverse effects of the poor expansion terms.
In the future, we also hope to diagnose precision related problems as well as mismatch problems. We can then use the diagnosis to guide disambiguation, phrasing or fielded retrieval, as well as term substitution, removal or expansion.
7. ACKNOWLEDGEMENTS
This work is supported by NSF grant IIS-1018317. Opinions in this work are solely the authors'. We thank Chengtao Wen, Grace Hui Yang, Jin Young Kim, Charlie Clarke, Gordon Cormack and NIST for helpful discussions, feedback and access to data.
8. REFERENCES
[1] J. A. Aslam and V. Pavlu. A practical sampling strategy for efficient retrieval evaluation. Report. May 2007.
[2] J. R. Baron, D. D. Lewis and D. W. Oard. TREC 2006 Legal Track Overview. In Proceedings of the fifteenth Text REtrieval Conference (TREC '06), 2007.
[3] G. Cao, S. Robertson and J. Nie. Selecting Query Term Alterations for Web Search by Exploiting Query Contexts. In Proceedings of 46th Annual Meeting of the Association for

523

Computational Linguistics: Human Language Technologies (ACL-08: HLT). 148-155, 2008.
[4] C. L. A. Clarke, G. V. Cormack and F. J. Burkowski. Shortest Substring Ranking (MultiText Experiments for TREC-4). In Proceedings of the Fourth Text REtrieval Conference (TREC-4). 1996.
[5] V. Dang and W. B. Croft. Query reformulation using anchor text. In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining (WSDM '10). 41-50, 2010.
[6] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer and R. Harshman. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6): 391-407. 1990.
[7] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. The vocabulary problem in human-system communication. Communications of ACM, 30(11): 964-971. ACM. New York, NY. November, 1987.
[8] W. Greiff. A theory of term weighting based on exploratory data analysis. In Proceedings of 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '98). 11-19, 1998.
[9] D. Harman. Towards interactive query expansion. In Proceedings of the 11th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '88). 321-331, 1988.
[10] D. Harman. Overview of the Third Text REtrieval Conference (TREC-3). In Proceedings of the 3rd Text REtrieval Conference (TREC '94), 1995.
[11] D. Harman. Overview of the Third Text REtrieval Conference (TREC-4). In Proceedings of the 4th Text REtrieval Conference (TREC '95), 1996.
[12] S. Harter. Online Information Retrieval: Concepts, Principles, and Techniques. Academic Press. San Diego, California. 1986.
[13] M. Hearst. Improving full-text precision on short queries using simple constraints. In Proceedings of the Fifth Annual Symposium on Document Analysis and Information Retrieval (SDAIR '96), 1996.
[14] R. Jones, B. Rey, O. Madani and W. Greiner. Generating query substitutions. In Proceedings of the 15th International Conference on World Wide Web (WWW '06). 387-396, 2006.
[15] J. Lamping and S. Baker. Determining query term synonyms within query context. United States Patent No. 7,636,714. USPTO March, 2005.
[16] W. Lancaster. Information Retrieval Systems: Characteristics, Testing and Evaluation. Wiley. New York, New York, USA. 1968.
[17] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '01). 120-127, 2001.
[18] J. Lin and M. D. Smucker. How do users find things with PubMed?: towards automatic utility evaluation with user simulations. In Proceedings of the 31st annual international

ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR '08). 19-26, 2008.
[19] D. Metzler and W.B. Croft. Combining the language model and inference network approaches to retrieval. Information Processing and Management Special Issue on Bayesian Networks and Information Retrieval, 40(5), 735-750, 2004.
[20] D. Metzler. Generalized inverse document frequency. In Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM '08). 399-408, 2008.
[21] M. Mitra, A. Singhal and C. Buckley. Improving automatic query expansion, Proceedings of 21st annual international ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR '98). 206-214, 1998.
[22] F. Peng, N. Ahmed, X. Li and Y. Lu. Context sensitive stemming for Web search. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '07). 639646, 2007.
[23] S. E. Robertson and K. Sp‰rck Jones. Relevance weighting of search terms. Journal of the American Society for Information Science, 27(3):129-146. 1976.
[24] G. Salton, E. A. Fox and H. Wu. Extended Boolean information retrieval. Communications of ACM, 26(11): 1022-1036. ACM. New York, NY. November 1983.
[25] S. Tomlinson. Experiments with the Negotiated Boolean Queries of the TREC 2006 Legal Discovery Track. In Proceedings of the fifteenth Text REtrieval Conference (TREC '06), 2007.
[26] S. Tomlinson, D. W. Oard, J. R. Baron and P. Thompson. Overview of the TREC 2007 Legal Track. In Proceedings of the sixteenth Text REtrieval Conference (TREC '07), 2008.
[27] E. Tudhope. Query based stemming. PhD Thesis. University of Waterloo. 1996.
[28] X. Wang and C. Zhai. Mining term association patterns from search logs for effective query reformulation. In Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM '08). 479-488, 2008.
[29] R. W. White, I. Ruthven, J. M. Jose, and C. J. Van Rijsbergen. Evaluating implicit feedback models using searcher simulations. ACM Trans. Inf. Syst. 23(3): 325-361. July, 2005.
[30] X. Xue, W. B. Croft and D. A. Smith. Modeling reformulation using passage analysis. In Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM '10). 2010.
[31] L. Zhao and J. Callan. Effective and efficient structured retrieval (poster description). In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM '09). 1573-1576, 2009.
[32] L. Zhao and J. Callan. Term necessity prediction. In Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM '10). 2010.
[33] Y. Zhu, L. Zhao, J. Callan and J. Carbonell. Structured queries for legal search. In Proceedings of the sixteenth Text REtrieval Conference (TREC '07), 2008

524

Generating Reformulation Trees for Complex Queries
Xiaobing Xue W. Bruce Croft
Center for Intelligent Information Retrieval Computer Science Department
University of Massachusetts, Amherst, MA, 01003, USA
{xuexb,croft}@cs.umass.edu

ABSTRACT
Search queries have evolved beyond keyword queries. Many complex queries such as verbose queries, natural language question queries and document-based queries are widely used in a variety of applications. Processing these complex queries usually requires a series of query operations, which results in multiple sequences of reformulated queries. However, previous query representations, either the "bag of words" method or the recently proposed "query distribution" method, cannot effectively model these query sequences, since they ignore the relationships between two queries. In this paper, a reformulation tree framework is proposed to organize multiple sequences of reformulated queries as a tree structure, where each path of the tree corresponds to a sequence of reformulated queries. Specifically, a two-level reformulation tree is implemented for verbose queries. This tree effectively combines two query operations, i.e., subset selection and query substitution, within the same framework. Furthermore, a weight estimation approach is proposed to assign weights to each node of the reformulation tree by considering the relationships with other nodes and directly optimizing retrieval performance. Experiments on TREC collections show that this reformulation tree based representation significantly outperforms the state-of-the-art techniques.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Experimentation, Performance
Keywords
Reformulation Tree, Verbose Query, Information Retrieval
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

1. INTRODUCTION
Although short keyword queries are still very common in web search, the increasing diversity of search applications and information needs has led to increasing complexity in queries. For example, verbose (or long) queries have become more and more popular in web search. In community-based Q&A, users pose natural language questions as queries. In patent retrieval, the whole document (patent application) is considered as the query. These complex queries help users express their information need naturally and save the effort of picking keywords. However, processing these queries poses a significant challenge for search systems.
Dealing with complex queries usually requires a series of query operations. For example, a typical process of dealing with a verbose query can be described as follows. The system first selects a subset of query words from the original query to remove noisy information. Then, the generated subset query is further modified to handle vocabulary mismatch. Finally, weights are assigned to queries generated at each step. Depending on the application, the above process could become more complicated. For example, in cross-lingual retrieval, the original verbose query needs to be translated into a foreign language query before applying any further operation. The above process will generate multiple sequences of reformulated queries, where each sequence records a way of modifying the original query using several query operations. These reformulation sequences capture the relationships between the reformulated queries. Fig. 1 displays some examples of the reformulation sequences, where the subset query is selected from the original query at the first step of the sequence and the second step further substitutes the subset query.
However, previous query representations cannot model these reformulation sequences well. The "bag of words" representation is widely used in information retrieval. Using this representation, the original query is transformed into a set of weighted words. Some extensions to this representation introduce phrases [19] and latent words [14][20].
Reformulation Sequence Qreductions iraqs foreign debtreduce iraqs foreign debt Qiraqs foreign debtiraqs foreign debts Qiraqs foreign debtiraqs external debt
Figure 1: The reformulation sequences generated for a verbose query Q "any efforts proposed or undertaken by world governments to seek reduction of iraqs foreign debt"

525

This representation does not explicitly model a reformulated query, which serves as the basis of the reformulation sequences. An example of the "bag of words" representation is shown in Fig. 2 (a). Recently, the "query distribution" representation [27] was proposed to transform the original query into a set of reformulated queries. For example, Xue et al [30] represents a verbose query as a set of subset queries. This representation indeed considers a reformulated query as the basic unit, but it fails to capture the relationships between the reformulated queries. Therefore, the sequences of reformulated queries still cannot be modeled using this representation. An example of the "query distribution" representation is shown in Fig. 2 (b).
In this paper, a novel query representation is proposed to transform a complex query into a reformulation tree, where the nodes at each level of this tree correspond to the reformulated queries generated using a specific query operation. Using this representation, a reformulation sequence is naturally modeled as a path from the root node to the leaf node. The construction of the reformulation tree simulates the process of applying a series of query operations to the complex query. Furthermore, weight is assigned to each node of the reformulation tree, which indicates the importance of the corresponding reformulated query. The estimation of the weight for a node considers not only the characteristics of this node itself, but also its relationships with other nodes. Different with previous reformulation models that treat retrieval models as independent steps, we estimate the weights on the reformulation tree by directly optimizing the performance of retrieval models, which considers the reformulation model and the retrieval model in a joint view.
Verbose queries, as a typical example of complex queries, have attracted much attention recently. Previous research on verbose queries either weights the query words in the original query [16, 15, 3] or selects the best subset of query words from the original query [12]. Relatively little research considers combining multiple query operations together for improving verbose queries. Therefore, as an implementation of the reformulation tree framework, a two-level tree structure is constructed for verbose queries, where the first level corresponds to the subset query selection operation and the second level corresponds to the query substitution operation. A weight estimation method is also described, which incorporates the relationships between different reformulated queries and directly optimizes the retrieval performance.
Fig. 2 (c) shows an example reformulation tree. The first level of this tree consists of two subset queries extracted from the original query, i.e., "reductions iraqs foreign debt" and "iraqs foreign debt". At the second level, each subset query is further modified to generate query substitutions. For example, "iraqs foreign debt" has been modified to "iraqs external debt". Furthermore, weight is assigned to each node of this tree, which measures the importance of each reformulated query. Compared with other representations, the reformulation sequences as shown in Fig. 1 are captured using the reformulation tree.
The contributions of this paper can be summarized as four folds. First, a tree based query representation is proposed to deal with complex queries, which models a series of query operations and captures the relationships between the reformulated queries. Second, a specific implementation, i.e., the two-level reformulation tree, is introduced for verbose queries, which combines two important operations, subset

a) Bag of Words {0.09 reduction, 0.09 iraqs, 0.09 foreign, 0.09 debt, ...}

b) Query Distribution {0.55 seek reduction iraqs, 0.23 seek reduction iraqs debt,

0.05 undertaken iraqs debt, 0.03 efforts seek reduction iraqs ... }

c) Reformulation Tree

Subset Selection:

Original Query 0.36

reductions iraqs foreign debt 0.20

iraqs foreign debt 0.12

Query Substitution:

reduce iraqs foreign debt iraqs foreign debts

0.20

0.08

iraqs external debt 0.04

Figure 2: Different query representations for a verbose query "identify any efforts proposed or undertaken by world governments to seek reduction of iraqs foreign debt"
query selection and query substitution. Third, a weight estimation method is designed by incorporating the relationships between different reformulated queries and directly optimizing retrieval performance. Fourth, detailed experiments are conducted to show that the tree-based representation outperforms other query representations for verbose queries.
2. RELATED WORK
In this section, we first describe previous work on complex queries, especially on verbose queries and then we review previous query representation approaches.
2.1 Complex Query
As described in the introduction, complex queries have been widely used in different applications. Some examples include the verbose query, the natural language question query and the document-based query.
Kumaran and Allan [11] studied shortening a verbose query through human interaction. Bendersky and Croft [2] discovered key concepts from a verbose query. These key concepts were combined with the original query to improve the retrieval performance. Kumaran and Carvalho [12] learned to automatically select subset queries using several query quality predictors. Balasubramanian et al [1] extent [12] for web long queries.
Lease et al [16] developed a regression model to assign weights to each query word in the verbose query by using the secondary features. Lease [15] further combined their regression model with the Sequential Dependence Model, which achieved significant performance improvement. Bendersky et al [3] proposed a unified framework to measure the weights of words, phrases and proximity features underlying a verbose query.
A natural language question query is widely used in a community-based Question and Answer service such as Yahoo! Answers and Quora. Previous work [8, 9, 24] studied effectively finding previously answered questions that are relevant to a new question asked by a user. Different retrieval models have been proposed to calculate the simi-

526

larity between questions. For example, a translation based retrieval model [8] were developed to deal with the vocabulary mismatch between the semantically related questions.
A document-based query allows users to directly submit a document as a query. A typical example is in patent retrieval [13], where the whole patent application is submitted to the search system in order to find the relevant patents. Many features are extracted from a patent application and these features are the basis of retrieving relevant patents.
In this paper, a tree-based representation is proposed to improve the complex query. A specific implementation for verbose queries is described. This implementation combines subset selection and query modification within the same framework, which has not been explored in previous work.
2.2 Query Representation
In this section, we review two types of query representations, "bag of words" and "query distribution".
The "bag of words" representation transforms the original query into a set of terms, either weighted or not. These terms include the words and phrases from the original query and the latent words and phrases extracted from the corpus. For example, the relevance model approach [14] adds latent words to the original query, the sequential dependency model [19] detects the phrase structure, and the latent concept expansion model [20] uses proximity features and latent words. This type of representation does not consider how to use words and phrases to form actual reformulated queries. In other words, a reformulated query is not explicitly modeled in this representation.
The "query distribution" representation transforms the original query into a set of reformulated queries, where each query is assigned a probability. This probability helps measure the importance of the query. For example, Xue et al [30] represented a verbose query as a distribution of subset queries and a modified Conditional Random Field is proposed to estimate the probability for each subset query. This type of representation indeed considers the reformulated query as the basic unit, but it assumes independence between the reformulated queries. When multiple query operations are applied, this independence assumption usually does not hold.
In this paper, the proposed "reformulation tree" representation extends the "query distribution" representation by modeling the relationships between reformulated queries using the tree structure.
Some previous work also considers the relationships between queries. Boldi et al [4] proposed to build a query-flow graph that modeled web users' search behaviors. Specifically, the directed edges between two queries indicated that they were likely to belong to the same search mission. Mei et al [17] presented a general framework to model search sequences, where a search sequence is represented as a nested sequence of search objects. The above work focuses on short keyword queries and uses query logs to capture the relationships between the queries submitted within the same search session. In contrast, in this paper, we study complex queries and model the relationships between the reformulated queries. Furthermore, the construction of the reformulation tree proposed in this paper is closely related to the final retrieval performance, while previous work studies the query graph or sequence independently from the retrieval model.

Guo et al [6] proposed a CRF-based model for query refinement, which combines several tasks like spelling correction, stemming and phrase detection. This model focuses on morphological changes of keyword queries such as spelling correction and stemming, but does not consider complex queries.

3. BACKGROUND
In this section, we summarize several basic concepts used in this paper.
A complex query (q) is more complicated than a short keyword query. Examples of complex queries include verbose queries, natural language question queries and documentbased queries. In this paper, we will focus on verbose queries.
A query operation (r) indicates a query processing technique. In this paper, we focus on two query operations, i.e. subset query selection and query substitution. Subset query selection [11, 12, 1, 30] selects a subset of query words from the original query. Query substitution [10, 26, 29] replaces the original query word with a new word. Other examples of query operations include query translation, query segmentation and so on.
A reformulated query (qr) is the output of applying a query operation. A reformulation sequence is a sequence of reformulated queries by applying a series of query operations.
A reformulation tree (T ) is a tree structure that organizes the reformulated queries generated by different query operations. Each path of T corresponds to a reformulation sequence.

4. REFORMULATION TREE
In this section, we first describe the framework for generating the reformulation tree T . Then, we compare this treebased representation with previous query representations. Finally, the principle of the weight estimation is described.

4.1 Framework
Suppose that n query operations {r1, r2, ..., rn} are required to process a complex query q. Then, q is transformed into a n-level tree T . Each node of T represents a reformulated query qr. From now on, if not explicitly indicated, we use qr to represent both a node of T and the corresponding reformulated query. The root node of T represents the original query q, which can be considered as a special reformulated query. The ith level of T are generated by applying the ith operation ri to the nodes at the (i - 1)th level. An arc is added between the nodes at the (i - 1)th level and the nodes at the ith level if the latter is the output of applying ri to the former. Therefore, each path of T corresponds to a reformulation sequence. Furthermore, weight w(qr) is assigned to each node of T , which measures the importance of the corresponding reformulated query qr.
When T is used for retrieval, the retrieval score of a document D is calculated using Eq. 1.

sc(T, D) = w(qr)sc(qr, D)

(1)

qr T

where w(qr) is the weight assigned to the node corresponding to the reformulated query qr and sc(qr, D) is the retrieval score of using qr to retrieve D. In general, sc(T, D) is calculated by combining the retrieval score of using each

527

reformulated query qr in T , where w(qr) is used as the combination weight. The calculation of sc(qr, D) depends on implementation.

4.2 Comparisons of Query Representations
In this subsection, we compare different query representations using the example in the introduction. Fig. 2 displays the "bag of words" representation, the "query distribution" representation and the "reformulation tree" representation.
In the "bag of words" representation, the basic unit is words or phrases. This representation may tell you that the important words in the original query are "reduction", "iraqs" and "debt", but how these words can be used together to form a new query is not clear.
The "query distribution" representation extends the "bag of words" representation by explicitly modeling a reformulated query. For example, this representation lists the top ranked subset queries such as "seek reduction iraqs" and "seek reduction iraqs debt". However, the relationships between the reformulated queries are not captured using this representation.
When a series of query operations are applied, we need a representation that models the reformulation sequences generated using these operations. The "reformulation tree" representation is designed to solve this problem. For example, in Fig. 2, the subset queries and the query substitutions are organized into a tree structure. This structure indicates that we need to first select subset queries and then conduct query substitution. It captures the relationships between "iraqs foreign debt" and "iraqs external debt", since the latter is the output of applying the query subsitution operation to the former.

4.3 Weight Estimation
The weight assigned to each node in the tree indicates the importance of the corresponding reformulated query. This weight should characterize both the features of this node itself and its relationships with other nodes. Therefore, the weight of qr, i.e., w(qr), is calculated in Eq. 2.

w(qr) = w(par(qr)) kfk(qr)

(2)

k

where par(qr) denotes the parent node of qr. fk is the query feature extracted from qr and k is the parameter. Eq. 2 shows that the weight of qr is not only decided by its own query features {fk} but also by the weight of its parent node par(qr). Intuitively, if qr is important, its children should also receive high weights. In this way, the relationships between reformulated queries are incorporated into the weight
estimation. Note that Eq. 2 provides the principle of weight estima-
tion. How to calculate w(qr) will depend on the implementation.

5. REFORMULATION TREE FOR VERBOSE QUERIES
In this section, we describe a two-level reformulation tree for verbose queries. We first describe the query operations used to construct the reformulation tree, i.e. subset query selection and query substitution. Then, we introduce a stagebased weight estimation method to assign weight to each node. Finally, the retrieval model is described.

5.1 Building Tree Structure
The construction of the reformulation tree for verbose queries consists of two steps: first, subset queries are selected from the original query; second, the subset queries generated in the previous step are further modified to generate query substitutions.
We follow Kumaran and Carvalho [12]'s method to generate subset queries. All query words from the original verbose query are considered. If the length of the verbose query is bigger than ten, we first rank all query words by their idf values and then pick the top ten words for the subset query generation. Then, all subset queries with length between three and six words are generated.
The passage analysis technique [29] is used to generate query substitutions. In order to replace one word from the original query, all the passages containing the rest of the query words are first extracted. Then, three methods are used to generate candidates for query substitution from these passages. Morph considers the morphologically similar words as candidates. Pattern considers the words matching the patterns extracted from the original query as candidates. Wiki considers the Wikipedia redirect pairs as the candidates. More details can be found in [29]. Finally, the top ranked candidates are used as query substitutions.
Given the above two query operations, the reformulation tree for the verbose query can be generated in this way. First, all subset queries with length between three to six are extracted from the original query. Each subset query is assigned a weight. How to estimate the weight will be described in the next subsection. According to this weight, we will pick the top ranked subset queries to construct the first level of the reformulation tree. SubNum is a parameter that controls the number of nodes at the first level. Second, among these SubNum subset queries, we further modify the top ModNum queries to generate query substitutions, which constructs the second level of the reformulation tree. ModNum is another parameter that controls the number of nodes that will be substituted.
For example, the reformulation tree used in the introduction (Fig. 2) can be constructed in two steps. This process is illustrated in Fig. 3. First, we pick the top two subset queries "reductions iraqs foreign debt" and "iraqs foreign debt" to construct the first level of the reformulation tree. Second, we modify these two subset queries respectively. For the first subset query, "reduce iraqs foreign debt" is generated by replacing "reduction" with "reduce". For the second subset query, two query substitutions, i.e. "iraqs foreign debt" and "iraqs external debt" are generated.
5.2 Weight Estimation
Eq. 2 indicates that the weight of a node in the reformulation tree depends on both its intrinsic features and the weight of its parent node. However, how to estimate the weight is still unclear. In this part, we describe a stage-based weight estimation method. The learning-to-rank based parameter estimation strategy [28] is used as the basis, which transforms a query feature into a corresponding retrieval feature.
In the initial stage, the root node (the original query q) is assigned the weight 1, i.e. w(q) = 1.
In Stage I, after the subset queries qsub are generated, we calculate the weight of qsub using Eq. 3.

528

Original Query Step 1
Original Query

reductions iraqs foreign debt iraqs foreign debt Step 2 Original Query

reductions iraqs foreign debt

iraqs foreign debt

reduce iraqs foreign debt iraqs foreign debts

iraqs external debt

Figure 3: The process of constructing a reformulation tree

w(qsub) = w(q) skubfksub(qsub)

k

=

skub fksub (qsub )

(3)

k

Eq. 3 instantiates Eq. 2 by focusing on the subset queries. fksub is the query feature extracted from qsub and skub is the corresponding parameter. Since the root node is the parent
of every subset query, its weight w(q) = 1, is used in Eq. 3. In order to estimate {skub} by directly optimizing the re-
trieval performance, we transform each query feature fksub into the corresponding retrieval feature Fksub, where Fksub is calculated in Eq. 4.

Fksub({qsub}, D) =

fksub(qsub)sc(qsub, D)

(4)

qsub

where sc(qsub, D) is the retrieval score of using qsub to retrieve D. The calculation of sc(qsub, D) depends on the retrieval model. The retrieval feature Fksub combines the retrieval score of each subset query qsub using their corresponding query feature fksub(qsub) as the combination weight. In general, Fksub indicates how well documents are ranked if fksub is used as the weight to combine subset queries.
Now, we obtain a set of retrieval features {Fksub}. The problem of estimating {skub} to combine the query features {fksub} is transformed into the problem of combining the corresponding retrieval features {Fksub} to achieve the best retrieval performance. The latter problem is typically solved
using learning to rank techniques. In this paper, the ListNet method [5] is adopted to learn {skub} on the training set.
After obtaining {skub}, we can assign the weight for each subset query according to Eq. 3.
In Stage II, we assign weights to the substituted queries.
The weight of a substituted query qmod is calculated using Eq. 5.

w(qmod) = w(qsub)

m k odfkmod(qmod)

(5)

k

Table 1: Summary of features

Features for Subset Queries fksub(qsub) MI[11] mutual information

SQLen[12] sub-query length

QS[7] query scope

QC[25] query clarity score

SOQ[12] similarity to original query

psg

count of passages containing qsub

KeyCpt[2] whether contains the key concept

Features for Substituted Queries fkmod(qmod) Morph generated using Morph

Pattern generated using Pattern

Wiki

generated using Wiki

psg

count of passages containing qmod

seg-type the number of possible segmentations

where qsub is the parent node of qmod. Compared with Eq.
3, the weights of the subset queries w(qsub) generated in
Stage I are incorporated in Eq. 5. Similarly, in order to estimate {m k od}, we transform fkmod
into the corresponding retrieval feature Fkmod using Eq. 6.

Fkmod({qmod}, D) =

w(qsub)fkmod(qmod)sc(qmod, D)

qmod
(6) where qsub is the parent node of qmod. In general, Fkmod tells how well the documents are ranked if fkmod is used as the weight to combine the substituted queries. Thus, the parameters {m k od} are learned by combining these retrieval features {Fkmod} using ListNet.

5.3 Features
In this part, we describe the query features used to characterize the subset queries and the substituted queries.
The features used to characterize the subset queries are mainly query quality predictors. This type of features have been widely used in previous research [12][30]. Examples of query quality predictors include Mutual Information [11], Query Scope [7] and Query Clarity [25]. In addition, passage information is considered. The number of passages that contain a subset query provides strong evidence for the quality of a subset query. Whether a subset query contains key concepts is also considered as a feature. These key concepts were discovered by Bendersky et al [2].
The features used to characterize the substituted queries include the type of methods of generating substituted queries. As described in Section 5.1, "Morph" indicates using the morphologically similar words as candidates, "Pattern" indicates using the pattern based method and "Wiki" indicates using the Wikipedia redirect page. The passage information is also considered as one feature. Furthermore, the number of possible segmentations of a substituted query is used as another feature. This feature can be directly obtained using the passage analysis technique [29].
The details of features are summarized in Table 1.

5.4 Retrieval Model
The retrieval score of using a reformulation tree T can be calculated using Eq. 1. For example, given the reformulation tree shown in Fig. 2, the retrieval score can be calculated as follows:

529

sc(T, D) = 0.36 ◊ sc(Original Query, D) = +0.2 ◊ sc("reductions iraqs foreign debt", D) = +0.12 ◊ sc("iraqs foreign debt", D) = +0.2 ◊ sc("reduce iraqs foreign debt", D) = +0.08 ◊ sc("iraqs foreign debts", D) = +0.04 ◊ sc("iraqs external debt", D)

In this paper, the sequential dependency model (SDM) [19] is used to calculate sc(qr, D), which has been widely used in previous work [2, 30] as a state-of-the-art technique. Using SDM, the score of a document can be calculated as follows:

sc(qr, D) = T

log(P (t|D)) + O

log(P (o|D))

tT (qr )

oO (qr )

+ U

log(P (u|D))

(7)

uU (qr )

where T (qr) denotes a set of query words of qr, O(qr) denotes a set of ordered bigrams extracted from qr and U (qr) denotes a set of unordered bigrams extracted from qr. T , O and U are parameters controlling the weights of different parts and are usually set as 0.85, 0.1 and 0.05 [19]. P (t|D), P (o|D) and P (u|D) are calculated using the language modeling approach [22, 31].
The SDM model can be easily implemented using the Indri query language [18]. Fig. 4 shows an example of Indri query for SDM model.

6. EXPERIMENTS
Four TREC collections, Gov2, Robust04, ClueWeb (Category B) and Wt10g are used for experiments. Robust04 is a newswire collection, while the rest are web collections. The statistics of each collection are reported in Table 2. For each collection, two indexes are built, one not stemmed and the other stemmed using the Porter Stemmer[23]. Stemming transforms a word into its root form, which is conducted either during indexing or during query processing. The latter case treats stemming as a part of query reformulation, which has been shown effective for web search [21]. Both cases are considered in this paper using two types of indexes. No stopword removal is done during indexing. For each topic, the description part is used as the query. A short list of 35 stopwords and some frequent stop patterns (e.g., "find information") are removed from the description query.
The query set of each collection is split into a training set and a test set. On the training set, the parameters k are learned. On the test set, the learned parameters k are used to assign weight to the reformulation tree generated from each test query. Specifically, ten-fold cross validation is used, where the query set is split into ten folds. Each time nine folds are used for training and one fold is used for test. This process repeats ten times.
Several baselines are compared. QL denotes the query likelihood language model [22, 31]. SDM denotes the sequential dependence model [19]. KC denotes the key concept method proposed by Bendersky et al [2]. Note that we do not report KC on ClueWeb, since the key concept query is not provided on ClueWeb in [2]. QL+SubQL and DM+SubQL [30] are the subset query distribution methods,

Figure 4: Example of Indri query. qr : iraqs foreign debt SDM: #weight(
0.85 #combine(iraqs foreign debt) 0.10 #combine(#1(iraqs foreign) #1(foreign debt)) 0.05 #combine(#uw8(iraqs foreign) #uw8(foreign debt)) )

Table 2: TREC collections used in experiments

Name

Docs

Topics

Gov2

25,205,179 701-850

Robust04 528,155 301-450,601-700

Wt10g 1,692,096 451-550

ClueWeb 50,220,423 1-100

which combine the original query with a distribution of subset queries. QL+SubQL uses QL for both the original query and the subset queries, while DM+SubQL uses SDM for the original query and uses QL for the subset queries. In this paper, QL+SubQL and DM+SubQL are trained using the global features mentioned in [30]. SDM, KC, QL+SubQL and DM+SubQL are the state-of-the-art techniques for verbose queries. SDM and KC can be classified as the "bag of words" representation, while QL+SubQL and DM+SubQL can be considered as the "query distribution" representation. Therefore, the comparisons with these baselines help show the advantages of the "reformulation tree" representation.
The proposed reformulation tree approach is denoted as RTree, which uses SDM as the underlying retrieval model. Two parameters are used during the tree construction, SubNum and ModNum, where SubNum denotes how many subset queries are kept in the reformulation tree and ModNum denotes among those kept subset queries how many are further modified to generate query substitutions. In this paper, SubNum takes all subset queries generated and ModNum is set as 10. The effect of these parameters will be explored in the following part of this paper.
The standard performance measures, mean average precision (MAP), precision at 10 (P10) and the normalized discounted cumulative gain at 10 (NDCG10), are used to measure retrieval performance. In order to improve readability, we report 100 times the actual values of these measures. The two-tailed t-test is conducted for significance.
The Lemur 4.10 toolkit is used to build the index and run the query. The ireval package provided in the toolkit is used for evaluation and significance test.
6.1 Example
In Table 3, we show some examples of the generated reformulation trees. As mentioned previously, some stopwords and stop patterns are removed from the original query. Those words are kept to improve readability. Note that they are not used for retrieval and subset query generation.
Table 3 shows that the subset queries and the substituted queries are effectively combined within the same framework. For example, given the original query "what allegations have been made about enrons culpability in the california energy crisis", the reformulation tree first generates high quality subset queries "enrons culpability california energy crisis" and "california energy crisis" and then further modifies "california energy crisis" as two substituted queries "california gas prices" and "california electricity crisis".

530

Table 4: Comparisons of retrieval performance. denotes significantly different from the baseline.

Gov2

Robust04

Wt10g

ClueWeb

MAP P10 NDCG10 MAP P10 NDCG10 MAP P10 NDCG10 MAP P10 NDCG10

QL

22.46 49.13 35.94

SDM

23.98 51.01 38.81

KC

24.88 50.87 37.72

QL+SubQL

23.36 50.81 37.96

DM+SubQL 24.82 53.36 40.55

RTree

26.70 53.96 40.78

vs. QL

18.9% 9.8% 13.5%

vs. SDM

11.3% 5.8% 5.1%

vs. KC

7.3% 6.1% 8.1%

vs. QL+SubQL 14.3% 6.2% 7.4%

vs. DM+SubQL 7.6% 1.1% 0.6%

Non-stemmed Index 22.40 39.12 39.63 16.44 28.97 23.30 40.04 40.60 16.76 31.65 23.87 40.76 40.75 17.45 30.82 22.85 39.28 39.98 16.81 29.79 23.65 40.32 41.15 18.25 31.13 25.07 42.33 42.64 19.44 34.02 11.9% 8.2% 7.6% 18.2% 17.4% 7.6% 5.7% 5.0% 16.0% 7.5% 5.0% 3.9% 4.6% 11.4% 10.4% 9.7% 7.8% 6.7% 15.6% 14.2% 6.0% 5.0% 3.6% 6.5% 9.3%

27.86 29.82 29.55 28.48 30.71 32.80 17.7% 10.0% 11.0% 15.2% 6.8%

10.97 21.63 11.53 22.76 n/a n/a 11.01 21.84 11.54 21.94 12.94 26.43 18.0% 22.2% 12.2% 16.1% n/a n/a 17.5% 21.0% 12.1% 20.5%

14.10 15.04 n/a 14.16 14.85 18.05 28.0% 20.0% n/a 27.5% 21.5%

QL

25.43 52.21

SDM

27.85 54.03

KC

27.52 53.83

QL+SubQL

26.19 53.36

DM+SubQL 28.49 55.91

RTree

29.85 56.38

vs. QL

17.4% 8.0%

vs. SDM

7.2% 4.3%

vs. KC

8.5% 4.7%

vs. QL+SubQL 14.0% 5.7%

vs. DM+SubQL 4.8% 0.8%

38.42 40.14 39.10 39.51 41.95 41.84 8.9% 4.2% 7.0% 5.9% -0.3%

Porter-stemmed Index 25.49 43.13 42.89 19.61 32.68 26.83 44.94 44.78 20.87 35.77 25.97 41.65 42.29 21.01 34.02 25.81 43.01 43.23 20.11 32.06 26.99 44.86 44.94 21.98 35.05 28.00 45.10 45.30 23.80 37.42 9.8% 4.6% 5.6% 21.4% 14.5% 4.4% 0.4% 1.2% 14.0% 4.6% 7.8% 8.3% 7.1% 13.3% 10.0% 8.5% 4.9% 4.8% 18.3% 16.7% 3.7% 0.5% 0.8% 8.3% 6.8%

31.31 33.21 32.29 30.93 33.45 35.84 14.5% 7.9% 11.0% 15.9% 7.1%

12.64 23.57 13.01 24.90 n/a n/a 12.94 25.00 13.29 24.08 13.69 25.82 8.3% 9.5% 5.2% 3.7% n/a n/a 5.8% 3.3% 3.0% 7.2%

15.46 15.87 n/a 16.37 15.58 18.09 17.0% 14.0% n/a 10.5% 16.1%

Table 3: Examples of the reformulation tree. The top ranked nodes are displayed. In the original query Q, the stopwords and stop structures are italicized.
Q: what allegations have been made about enrons culpability in the california energy crisis 0.194 Q 0.133 enrons culpability california energy crisis 0.047 california energy crisis
0.009 california gas prices 0.008 california electricity crisis
Q: give information on steps to manage control or protect squirrels 0.148 Q 0.060 control protect squirrels 0.048 control squirrels
0.013 control of ground squirrels 0.012 control squirrel
Q: what is the state of maryland doing to clean up the chesapeake bay 0.089 Q 0.063 maryland chesapeake bay
0.015 md chesapeake bay 0.009 maryland chesapeake bay watershed 0.034 chesapeake bay 0.007 chesapeake bay watershed 0.006 chesapeake bay river

6.2 Retrieval Performance
The first experiment is conducted to compare the retrieval performance of the proposed RTree method with the baselines. The baseline methods include QL, SDM, KC, QL+Sub QL and DM+SubQL. The results are shown in Table 4. The best performance is bolded.
Table 4 shows that RTree outperforms all the baseline methods. Specifically, RTree performs better than the "bag of word" representations, SDM and KC. Using the nonstemmed index, RTree significantly improves SDM and KC on all four collections with respect to all three performance measures. For example, on ClueWeb, RTree improves SDM by 12.2% and 20.0% with respect to MAP and NDCG10, respectively. On Wt10g, RTree improves KC by 11.4% and 11.0% with respect to MAP and NDCG10, respectively. On the Porter-stemmed index, similar results are also observed. These results show that the "reformulation tree" representation is more effective than the "bag of words" representation on modeling verbose queries, since the former explicitly models the reformulated query, while the latter only considers words and phrases.
Furthermore, RTree also outperforms the "query distribution" representations, QL+SubQL and DM+SubQL. Using the non-stemmed index, RTree outperforms QL+SubQL and DM+SubQL on all four collections with respect to all three measures. Most of the improvements are significant. For example, on ClueWeb, RTree improves QL+SubQL by 17.5% and 27.5% with respect to MAP and NDCG10, respectively. Also, RTree improves DM+SubQL by 12.1% and 21.5% with respect to MAP and NDCG10, respectively. The results using the Porter-stemmed index are similar. These observations indicate that the "reformulation tree" representation is better than the "query distribution" representation,

531

number of queries

70
60
50 SDM
40 KC
30 RTree
20
10
0

number of queries

relative increase/decrease
(a) Gov2
140 120 100
80 60 40 20
0

SDM KC RTree

number of queries

relative increase/decrease

(b) Robust04

50

45

40

35 SDM
30

25

KC

20

RTree

15

10

5

0

number of queries

relative increase/decrease
(c) Wt10g
35 30 25 20 15 10
5 0

SDM RTree

relative increase/decrease
(d) ClueWeb Figure 5: Analysis of relative increases/decreases of MAP over QL.

since the former effectively combines different reformulation operations within the same framework.
It is not surprising that RTree brings more improvements over the baselines using the non-stemmed index than using the Porter-stemmed index, since some effect of query substitutions, especially those generated using the morphologically similar words, is already provided by the Porter stemmer.
6.3 Further Analysis
Table 4 shows that RTree significantly outperforms the baseline methods. In this part, we make detailed comparisons between RTree and the baseline approaches.
First, we compare RTree with SDM and KC. Specifically, we analyze the number of queries each approach increases or decreases over QL. Fig 5 shows the histograms of SDM, KC and RTree based on the relative increases/decreases of MAP over QL. The non-stemmed index is used in Fig. 5. Similar results are observed using the Porter-stemmed index.

Table 5: Comparisons with QL+SubQL and

DM+SubQL. "+", "=" and "-" denote that RTree

performs better, equal or worse than QL+SubQL

and DM+SubQL with respect to MAP.

RTree

vs. QL+SubQL

vs. DM+SubQL

+

=

-

+

=

-

Gov2 71.81% 0.67% 27.52% 63.09% 0.67% 36.24%

Robust04 68.27% 0.00% 31.73% 63.05% 0.00% 36.95%

Wt10g 62.89% 2.06% 35.05% 62.89% 1.03% 36.08%

ClueWeb 65.31% 2.04% 32.65% 70.41% 2.04% 27.55%

Table 6: The effect of subset query selection and

query substitution with respect to MAP

MAP

Gov2 Robust04 Wt10g ClueWeb

SDM

23.98 23.30 16.76 11.53

KC

24.88 23.87 17.45

n/a

QL+SubQL 23.36 22.85 16.81 11.01

DM+SubQL 24.82 23.65 18.25 11.54

RTree-Subset 25.80 24.76 18.11 11.73

RTree

26.70 25.07 19.44 12.94

Fig. 5 shows that RTree improves more queries than SDM and KC. For example, on Gov2, RTree improves 110 queries out of the total 150 queries, while SDM and KC improve 89 and 91, respectively. On Robust04, RTree improves 174 queries out of the total 250 queries, while SDM and KC improve 129 and 153 queries, respectively. At the same time, RTree also hurts less queries than SDM and KC. These observations indicate that RTree is more robust than both SDM and KC.
Furthermore, we compare RTree with QL+SubQL and DM+SubQL. QL+SubQL and DM+SubQL only consider subset query selection, while RTree combines both subset query selection and query substitution. The comparisons between them indicate whether RTree effectively combines two query operations to improve verbose queries. Specifically, we analyze the percent of queries where RTree performs better than QL+SubQL and DM+SubQL, respectively. The results using the non-stemmed index are reported in Table 5.
Table 5 shows that RTree consistently outperforms QL+ SubQL and DM+SubQL for 60%-70% queries on all four collections. These results indicate that RTree provides an effective way to combine different query operations, which significantly improves the retrieval performance of verbose queries.
6.4 Subset Selection vs. Query Substitution
RTree combines subset query selection and query substitution together using a two-level reformulation tree. Previous experiments have demonstrated the general effect of this approach. In this part, we split the effect of subset query selection and query substitution. Specifically, we propose a one-level reformulation tree, which only consists of subset queries. This one-level reformulation tree is denoted as RTree-Subset. The comparisons between RTree-Subset and other approaches using the non-stemmed index are shown in Table 6.
In Table 6, RTree-Subset outperforms the baseline methods, which indicates the effect of subset queries in the reformulation tree. When query substitutions are introduced, RTree further improves RTree-Subset. Thus, both subset selection and query substitution account for the performance

532

MAP

0.3 0.295
0.29 0.285
0.28 0.275
0.27 5

10

20

30

all

MAP

0.29 0.285
0.28 0.275
0.27 0.265
0.26 5

(a) Gov2

10

20

30

all

MAP

0.25 0.245
0.24 0.235
0.23 0.225
0.22 5

(b) Robust04

10

20

30

all

MAP

0.15 0.145
0.14 0.135
0.13 0.125
0.12 5

(c) Wt10g

10

20

30

all

(d) ClueWeb

Figure 6: The effect of the parameter SubNum. x-axis denotes SubNum and y-axis denotes MAP.

of RTree. RTree-Subset also performs better than other subset query selection methods such as QL+SubQL and DM+SubQL.
6.5 Parameter Analysis
As described in Section 5.1, there are two parameters used during the process of constructing the reformulation tree, SubN um and M odN um. SubN um denotes the number of subset queries used in the reformulation tree and M odN um denotes the number of subset queries that are modified to generate query substitutions. In this subsection, we explore the effect of these two parameters. The Porter-stemmed index is used. Fig. 6 shows the effect of the parameter SubN um, where SubN um takes the values 5, 10, 20, 30 and "all", where M odN um is fixed as 10. Here, "all" indicates using all subset queries generated.
Fig. 6 shows that the best number of subset queries used in the reformulation tree is inconsistent. On Gov2, the performance becomes stable after using the top 20 subset queries. On Robust04 and Wt10g, the performance keeps increasing when more subset queries are considered. On ClueWeb, the performance drops when more than the top 20 queries are used. One possible explanation for these observations is provided. Robust04 and Wt10g are relatively

Table 7: The effect of the parameter M odN um with

respect to MAP

M odN um Gov2 Robust04 Wt10g ClueWeb

3

29.52 27.08 22.61 13.77

5

29.63 27.11 22.72 13.75

10

29.66 27.16 22.75 13.71

small collections, thus using more subset queries is likely to retrieve more relevant documents. However, when the size of the collection becomes very large such as Gov2, using more subset queries does not help much retrieval all relevant documents. If the collection is not only big but also contains much noise such as ClueWeb, using more subset queries even hurts the performance.
Table 7 displays the retrieval performance when M odN um takes three different values, i.e. 3, 5 and 10, where SubN um is set as 10.
Table 7 shows that there is not much performance change when M odN um is bigger than 3, which indicates that modifying the top three subset queries is enough to achieve most of the performance of RTree.

7. EFFICIENCY
We now discuss the efficiency of using the reformulation tree model for retrieval. The online cost of this model comes from three aspects, i.e. reformulated query generation, query feature extraction, and retrieval.
The efficiency of the reformulated query generation depends on the reformulation operations involved. For example, generating the subset queries is very efficient. In contrast, generating query substitutions using the passage analysis is more time consuming, since it needs to analyze a lot of passages.
The efficiency of the query feature extraction also depends on the query features used. Some query features are expensive such as query clarity, while some features are relatively cheap such as the frequency in query logs.
Both of these steps can be optimized if large scale query logs are available. We can limit the reformulated queries to those appearing in query logs. In this way, instead of generating queries, we simply search the query logs, which can be efficiently implemented using the index. Also, all query features can be precomputed, which speeds up the query feature extraction.
In terms of the efficiency of retrieval, Eq. 1 shows that the retrieval score of each reformulated query (sc(qr, D)) is required. At first glance, this appears to be inefficient, since we need to run multiple queries. However, this can be easily optimized. Eq. 7 shows that sc(qr, D) consists of the scores of the words and bigrams in qr. Since all the reformulated queries in the reformulation tree are generated from the same original query, they share many words and bigrams. Thus, the scores of these words and bigrams can be reused by different reformulated queries. For example, the words and bigrams in the subset queries all come from the original query. Thus, we only need to calculate the scores for every word and bigram in the original query and then reuse these scores for each subset query. Further, although query substitutions may introduce new words and bigrams, the number of these new words and bigrams is limited. For example, Table 7 shows that query substitutions generated

533

from the top three subset queries are sufficient to achieve good retrieval performance.
8. CONCLUSION
Complex queries pose a new challenge to search systems. In order to combine different query operations and model the relationships between the reformulated queries, a new query representation is proposed in this paper, where the original query is transformed into a reformulation tree. A specific implementation is described for verbose queries, which combines subset query selection and query substitution within a principled framework. In the future, this query representation will be applied to other search tasks involving complex queries such as the cross-lingual retrieval and diversifying the search results.
Acknowledgments
This work was supported in part by the Center for Intelligent Information Retrieval and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
9. REFERENCES
[1] N. Balasubramanian, G. Kumaran, and V. Carvalho. Exploring reductions for long web queries. In SIGIR10, pages 571≠578, 2010.
[2] M. Bendersky and W. B. Croft. Discovering key concepts in verbose queries. In SIGIR08, pages 491≠498, Singapore, 2008.
[3] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In WSDM10, New York City, NY, 2010.
[4] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and S. Vigna. The query-flow graph: model and applications. In CIKM08, pages 609≠618, 2008.
[5] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. In ICML07, pages 129≠136, 2007.
[6] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and discriminative model for query refinement. In SIGIR08, pages 379≠386, Singapore, 2008.
[7] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In SPIRE04, pages 43≠54, 2004.
[8] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar questions in large question and answer archives. In CIKM05, pages 84≠90, 2005.
[9] V. Jijkoun and M. de Rijke. Retrieving answers from frequently asked questions pages on the web. In CIKM05, pages 76≠83, 2005.
[10] R. Jones, B. Rey, O. Madani, and W. Greiner. Generating query substitutions. In WWW06, pages 387≠396, Ediburgh, Scotland, 2006.
[11] G. Kumaran and J. Allan. A case for shorter queries, and helping users creat them. In ACL07, pages 220≠227, Rochester, New York, 2007.
[12] G. Kumaran and V. R. Carvalho. Reducing long queries using query quality predictors. In SIGIR09, pages 564≠571, Boston, MA, 2009.

[13] L. Larkey. A patent search and classification system. In DL99, pages 179≠187, Berkeley, CA, 1999.
[14] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR01, pages 120≠127, New Orleans, LA, 2001.
[15] M. Lease. An improved Markov random field model for supporting verbose queries. In SIGIR09, pages 476≠483, Boston, MA, 2009.
[16] M. Lease, J. Allan, and W. B. Croft. Regression rank: learning to meet the oppotunity of descriptive queries. In SIGIR05, pages 472≠479, Salvador, Brazil, 2005.
[17] Q. Mei, K. Klinkner, R. Kumar, and A. Tomkins. An analysis framework for search sequences. In CIKM09, pages 1991≠1994, 2009.
[18] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Information Processing and Management, 40(5):735≠750, 2004.
[19] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In SIGIR05, pages 472≠479, Salvador, Brazil, 2005.
[20] D. Metzler and W. B. Croft. Latent concept expansion using markov random fields. In SIGIR07, pages 311≠318, Amsterdam, the Netherlands, 2007.
[21] F. Peng, N. Ahmed, X. Li, and Y. Lu. Context sensitive stemming for web search. In SIGIR07, pages 639≠646, Amsterdam, the Netherlands, 2007.
[22] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR98, pages 275≠281, Melbourne, Australia, 1998.
[23] M. F. Porter. An algorithm for suffix stripping. Program, 14(3):130≠137, 1980.
[24] S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and Y. Liu. Statistical machine translation for query expansion in answer retrieval. In ACL07, pages 464≠471, Prague, Czech Republic, June 2007. Association for Computational Linguistics.
[25] Y. Z. S. Cronen-Townsend and W. B. Croft. Predicting query performance. In SIGIR02, pages 299≠306, 2002.
[26] X. Wang and C. Zhai. Mining term association patterns from search logs for effective query reformulation. In CIKM08, pages 479≠488, Napa Valley, CA, 2008.
[27] X. Xue and W. B. Croft. Representing queries as distributions. In SIGIR10 Workshop on Query Representation and Understanding, pages 9≠12, Geneva, Switzerland, 2010.
[28] X. Xue and W. B. Croft. Modeling subset distributions for verbose queries. In SIGIR11, pages 1133≠1134, 2011.
[29] X. Xue, W. B. Croft, and D. A. Smith. Modeling reformulation using passage analysis. In CIKM10, pages 1497≠1500, 2010.
[30] X. Xue, S. Huston, and W. B. Croft. Improving verbose queries using subset distribution. In CIKM10, pages 1059≠1068, 2010.
[31] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In SIGIR01, pages 334≠342, New Orleans, LA, 2001.

534

Proximity-based Rocchio's Model for Pseudo Relevance Feedback

Jun Miao1, Jimmy Xiangji Huang2, Zheng Ye2
Information Retrieval and Knowledge Management Research Lab 1Department of Computer Science & Engineering, 2School of Information Technology
York University, Toronto, Canada
jun@cse.yorku.ca, {jhuang, yezheng}@yorku.ca

ABSTRACT
Rocchio's relevance feedback model is a classic query expansion method and it has been shown to be effective in boosting information retrieval performance. The selection of expansion terms in this method, however, does not take into account the relationship between the candidate terms and the query terms (e.g., term proximity). Intuitively, the proximity between candidate expansion terms and query terms can be exploited in the process of query expansion, since terms closer to query terms are more likely to be related to the query topic.
In this paper, we study how to incorporate proximity information into the Rocchio's model, and propose a proximitybased Rocchio's model, called PRoc, with three variants. In our PRoc models, a new concept (proximity-based term frequency, ptf ) is introduced to model the proximity information in the pseudo relevant documents, which is then used in three kinds of proximity measures. Experimental results on TREC collections show that our proposed PRoc models are effective and generally superior to the state-of-the-art relevance feedback models with optimal parameters. A direct comparison with positional relevance model (PRM) on the GOV2 collection also indicates our proposed model is at least competitive to the most recent progress.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models, Relevance feedback
General Terms
Algorithms, Performance, Experimentation
Keywords
Pseudo Relevance Feedback, Rocchio's Model, Proximitybased Term Frequency, Query Expansion
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

1. INTRODUCTION
Pseudo relevance feedback (PRF) via query expansion (QE) is an effective technique for boosting the overall performance in Information Retrieval (IR). It assumes that topranked documents in the first-pass retrieval are relevant, and then used as feedback documents in order to refine the representation of original queries by adding potentially related terms. Although PRF has been shown to be effective in improving IR performance [4, 6, 9, 13, 23, 26, 28, 30, 36, 37, 40, 42] in a number of IR tasks, traditional PRF can also fail in some cases. For example, when some of the feedback documents have several incoherent topics, terms in the irrelevant contents are likely to misguide the feedback models by importing noisy terms into the queries. This could influence the retrieval performance in a negative way. It is partially because the query itself was ignored in the process of expansion term selection. To be more specific, the term associations between candidate terms and the query terms have been ignored in traditional PRF models.
Term proximity is an effective measure for term associations, which has been studied extensively in the past few years. Most of these studies focus on the term proximity within the original query and adapt this in ranking documents [3, 5, 8, 10, 15, 24, 31, 33]. Various methods of integrating proximity information into a retrieval process are introduced in these papers, and it has proven to be useful in discriminating between the relevant and non-relevant documents.
In the field of PRF, based on the assumption that terms closer to the query terms are more likely to be relevant to the query topic, there are several studies which investigated how to give more weight to these terms in the process of pseudo relevance feedback. For example, to this end, Vechtomova et al. [34] imported a distance factor which combined with Mutual Information (MI) for selecting query expansion terms. Lv et al. [16] proposed a positional relevance model (PRM) by using position and proximity information to solve this problem and obtain significant performance.
However, as far as we are aware, there is little work done on incorporating proximity information into the traditional Rocchio's feedback model. Although the Rocchio's model has been introduced in the information retrieval field for many years, it is still effective in obtaining relevant documents. According to [41], "BM25 [27] term weighting coupled with Rocchio feedback remains a strong baseline which is at least as competitive as any language modeling approach for many tasks." This observation is also supported in our preliminary experiments of this paper. Therefore, it

535

is promising to make an extension of the Rocchio's model to take into account the proximity information.
In addition, it is unknown how to tackle the challenge of modeling the traditional statistics of expansion terms and the relationship between expansion terms and the query terms in a unified framework. In this paper, we propose a proximity-based feedback model based on the traditional Rocchio's model, called PRoc. Unlike the PRM model, we focus on the proximity of terms rather than the positional information. In our method, we estimate the weights of candidate expansion terms by taking their distance from query terms into account. Specifically, if a term is far away from the query terms in the feedback documents, it is proposed to be punished by discounting its weight because it is likely to be irrelevant to the query topic.
The main contribution of this paper is as follows. First, we study how to adapt the traditional Rocchio's model [28] for proximity information, and propose a proximity-based feedback model, called PRoc, in which the traditional statistics of expansion terms and the proximity relationship between expansion terms and the query terms are taken into account. Second, we propose to use three proximity measures. Unlike previous methods, the importance of query terms has been taken into account to measure the proximity information by proposing a new concept, namely proximity-based term frequency. Finally, extensive experiments on standard TREC collections have been conducted to evaluate our proposed feedback model from different aspects. We compare our proposed feedback model with strong feedback baselines. Our model can achieve significantly better performance over RM3 and the classic Rocchios' model.
The remainder of this paper is organized as follows: in Section 2 we review the related work. In Section 3, three methods for measuring the proximity and our proposed model, PRoc, are presented in details. In Section 4, we introduce the settings of the experiments. In Section 5, the experimental results are presented and discussed. A direct comparison is made to compare PRoc with the most recent work PRM. Finally, we conclude our work with a brief conclusion and future research directions in Section 6.
2. RELATED WORK
2.1 Pseudo Relevance Feedback
In information retrieval, PRF via query expansion is referred to as the techniques, algorithms or methodologies that reformulate the original query by adding new terms into the query, in order to achieve a better retrieval performance. There are a large number of studies on the topic of PRF. Here we mainly review the work about PRF which is the most related to our research.
A classical relevance feedback technique was proposed by Rocchio in 1971 for the Smart retrieval system [28]. It is a framework for implementing (pseudo) relevance feedback via improving the query representation, in which a set of documents are utilized as the feedback information. Unique terms in this set are ranked in descending order of their TFIDF weights. In the following decades, many other relevance feedback techniques and algorithms were developed, mostly derived under Rocchio's framework. For example, a popular and successful automatic PRF algorithm was proposed by [26] in the Okapi system; Amati et al. [2] proposed a query expansion algorithm in his divergence from

randomness (DFR) retrieval framework. In addition, with the development of language model [21] in IR, a number of techniques (e.g. [13, 32, 42]) have been developed to fit in the language modeling framework. In addition, Robertson [25] proposed a theoretical feedback model that supports the assumption by using of the difference of term distributions to select and re-weight the expansion terms.
For PRF in the language modeling framework, we always exploit feedback information (e.g., the top-ranked documents set, F = D1, D2, . . . , D|F |), in order to re-estimate a more accurate query language model. For example, the model based feedback approach [42] is not only theoretically sound, but also performs well empirically. The essence of model based feedback is to update the probability of a term in the query language model by making use of the feedback information. Much like model-based feedback, relevance models [13] also estimate an improved query model. The difference between the two approaches is that relevance models do not explicitly model the relevant or pseudo-relevant document. Instead, they model a more generalized notion of relevance [18]. Lv et al. [14] have conducted a comparable study of five representative state-of-the-art methods for estimating improved query language models in ad hoc information retrieval, including RM3 (a variant of the relevance language model), RM4, DMM, SMM (a variant of modelbased feedback approach), and RMM [13, 32, 42]. They found that SMM and RM3 are the most effective in their experiments, and RM3 is more robust to the setting of feedback parameters.
However, most of these PRF approaches estimate the importance (or probability) of the candidate expansion terms based on their statistics, while the proximity information is always ignored.
2.2 Term Proximity in Previous Work
Term proximity is the co-occurrences of terms within a specified distance. Particularly, the distance is the number of intermediate terms in a document. Plenty of work has been done to integrate term proximity into both probabilistic and language models. Keen [10, 11] firstly attempted to import term proximity in the Boolean retrieval model by introducing a "NEAR" operator. Buttcher et al. [3] proposed an integration of term proximity scoring into Okapi BM25 and obtain improvements on several collections. Rasolofo et al. [24] added additional weight to the top documents which contain query term pairs appearing in a window through a two-phase process, but the improvement is somewhat marginal. Song et al. [31] presented a new perspective on term proximity. Query terms are grouped into phrases, and the contribution of a term is determined by how many query terms appear in the context phrases. In order to make it clear that how we could model proximity and incorporate a proximity measure, Tao et al. [33] systemically studied five proximity measures and investigated how they perform in the KL-divergence retrieval model and the Okapi BM25 retrieval model. Under the language modeling framework, Zhao et al. [45] used a query term's proximate centrality as a hyper parameter in the Dirichlet language model. Lv et al. [15] integrated the positional and proximity information into the language model by a different way. They defined a positional language model at each position in documents by create virtual documents based on term probation.
All the above work focuses on how to utilize the proxim-

536

ity information of query terms to avoid documents which contain scattered query terms. This kind of documents should be punished because they are very likely to be irrelevant. For example, a document contains both "Japan" and "Earthquake" is possible to be irrelevant to the topic "Earthquake in Japan" if these two terms are not close in the context. It could be biased to only "Earthquake" and mention some technologies in "Japan" about "Earthquake". Term proximity is effective to discriminate against these types of documents. Although there have been plenty of efforts in integrating proximity heuristic into existing retrieval models, research work about how to utilize this information for pseudo relevance feedback is still limited. Vechtomova et al. [34] combined several distance factors with Mutual Information for selecting query expansion terms from windows or passages surrounding query term occurrences. However, marginal improvements were observed in the experiments. Lv et al. [16] presented two methods to estimate the joint probability of a term w with the query Q at every position in each feedback document. This is an extension of the state-of-the-art relevance model [13], and significant improvements were obtained on two collections. Besides the work presented [16, 34], it is difficult to find other systematical work about formally modeling term proximity heuristic in the context of pseudo feedback, especially in the classic Rocchio's model.
In this paper, we propose PRoc models which integrates the proximity information of terms into the traditional Rocchio's framework. Three kinds of proximity measures are introduced to estimate relevance and importance of the candidate terms. In order to integrate term proximity into the Rocchio's model, we re-interpret the definition of term frequency and introduce a new concept, proximity term frequency. Unlike in [34], we conduct our study on a mature feedback model which has proven to be effective. Instead, the work of Vechtomova et al. was based on the Mutual Information which failed to perform as well as the traditional feedback model. In contrast to the work in [16], we try to employ proximity heuristic in a formalistic framework which extensively differs from the language modeling framework. Indeed, we do not concern ourselves with the position of each candidate term as in [16]. Meanwhile, to confirm the effectiveness of our model, we compare the performance of PRoc with that of PRM in Section 5. The experimental results show that our proposed PRoc is at least competitive to the most recent work, PRM.
3. PROXIMITY-BASED ROCCHIO'S MODEL
In this section, we present the proposed proximity-based Rocchio's model, called PRoc. Specifically, we first briefly introduce the traditional Rocchio's model, and present the adaption of Rocchio's model for proximity information by proposing a new concept, namely proximity-based term frequency (ptf ). Then we describe in details about how to adopt ptf in three investigated proximity measures.
3.1 Adaption of Rocchio's Model
Rocchio's model [28] is a classic framework for implementing (pseudo) relevance feedback via improving the query representation. It models a way of incorporating (pseudo) relevance feedback information into the vector space model (VSM) in IR. In case of pseudo relevance feedback, the Roc-

chio's model without considering negative feedback documents has the following steps:

1. All documents are ranked for the given query using a particular retrieval model. This step is called first-pass retrieval, from which the |R| highest ranked documents are used as the feedback set.

2. Each document in the feedback set R is represented as a weighted term vector, annotated by r, by a certain weighting function, for example originally by the TFIDF weights [29].

3. The representation of the query is finally refined by

taking a linear combination of the initial query term

vector with the feedback document vector:

Q1 =   Q0 +  

r |R|

(1)

rR

where Q0 and Q1 represent the original and first iteration query vectors, r is the expansion term weight vector, and  and  are tuning constants controlling how much we rely on the original query and the feedback information. In practice, we can always fix  at 1, and only study  in order to get better performance.

Many other relevance feedback techniques and algorithms [2,

4, 26] are also derived under the Rocchio's framework. For

example, Carpineto et al. proposed to compute the weight of

candidate expansion terms based on the divergence between

the probability distributions of terms in the top ranked doc-

uments and the whole collection. In this paper, we also take

advantage of this distributional view. But we re-interpret

the definition of term frequency in the KLD formula 2 in-

stead of the distribution estimated from a set of top docu-

ments. We use the following function to rank the candidate

terms:

score(w)

=

P

(w|d)



log(

P (w|d) P (w|C)

)

(2)

where P (w|d) is the probability of candidate expansion term w in feedback document d, P (w|C) is the probability in the retrieval collection C.
Traditionally, candidate terms are ranked by their weights in the feedback documents, and the weights are affected by term frequencies in these documents extensively. However, the normal term frequency cannot capture the characteristic that whether a candidate term occurs near or far away from the query, such that the candidate term may not be relevant to the query topic. In other words, if the occurrence of a term is far away from the query terms, it should not be counted in the effective term frequency because this term is very likely to be irrelevant to the query topic. Thus, we propose a new concept, proximity term frequency (ptf ), which models the frequency of a term as well as the semantics to the query in terms of proximity. In order to adapt the proximity information, we re-interpret the definition of term frequency by proposing three kinds of proximity measures: window-based method, kernel-based method and the hyperspace analogue to language method. The main research challenge now we are facing is how to evaluate ptf. In the following subsections, we introduce three measures to compute the ptf . Meanwhile, the importance of query terms is also taken into account. A very frequent query term is likely to be close to many candidate terms, which makes it difficult to distinguish the related feedback terms. Inverse document frequency (idf ) of query terms is integrated to calculate ptf.

537

3.2 Window-based Method
The first method adopts a simple window-based n-gram frequency counting method, which has been popular in previous studies on using term proximity for IR (e.g. [17, 20]).
The basic idea of the window-based n-gram counting method is to segment the document into a list of sliding windows, with each window having a fixed window size wSize. If a document has a length of l, and the window size is set to wSize, the document is then segmented into l-wSize sliding windows, where each window contains wSize consecutive tokens. For example, if a document has four tokens A, B, C, and D, and the window size is 3, there are two windows in this document, namely A, B, C and B, C, D. The n-gram frequency is then defined as the number of windows in which all n-gram terms co-occur. There could be two variants of the n-gram models, namely the ordered and unordered ngram models. The ordered n-gram model takes the order of occurrences of the n-gram terms into account. For the same n-gram terms, the n-grams in which the composing n-gram terms appear in different orders are considered as different n-grams. In contrast, the unordered model ignores the order of occurrences of the n-gram terms. Actually, only a rough distance between terms is considered in this measure. If two terms are in the window, they are strongly related and the co-occurrence is counted in ptf .

|Q|

ptf (t) = C(t, qi)IDF (qi)

(3)

i=1

where qi is a query term, C(t, qi) is the number of windows in which the candidate term and the query terms co-occur, |Q| is the number of query terms, and IDF (qi) equals to log(N - Nt + 0.5)/(Nt + 0.5). N is the number of documents in the collection, and Nt is the number of documents that contain qi.
The n-gram counting method has the advantage of being straight-forward, and can be easily deployed in practice. It does not take the actual distance between query terms into account directly, and any n-gram terms appear together within a window is counted as one occurrence of the n-gram. If a term is very close to a query term, its co-occurrence count with the query term will be more than that of a term far away from this query term in the sliding windows. This variant of PRoc is denoted by PRoc1 in the rest of this paper.

3.3 Kernel-based Method

Following to previous studies [16, 44], an alternative method we use is a kernel-based method to count the term frequency in a document. There are a number of kernel functions (e.g. Gaussian, Triangle, Cosine, and Circle [44]) which were used for measuring the proximity. Gaussian kernel has been shown to be effective in most cases. In this paper, we also use the Gaussian kernel to measure the proximity between a candidate expansion term t and a query term q.

K(t,

q)

=

exp[

-(pt - pq)2 22

]

(4)

where pt and pq are respectively the positions of candidate term t and query term q in a document,  is a tuning parameter which controls the scale of Gaussian distribution. In other words,  has a similar effect as the parameter wSize in window-based method. In order to keep the consistency with other proximity measures, we also use wSize to denote .

Different from the window-based method, the kernel-based method is a soft proximity measure. In particular, even if the appearance of a candidate term and a query term is not in a window of wSize, its weight can still be slightly boosted.
In this method, beside the average proximity to the query, we also take into account the importance of different query terms. Therefore, we build a representational vector for the query, in which each dimension is the weight of a query term by the inverse document frequency formula below, and then the proximity-based term frequency ptf in the Kernel-based method is computed as follows:

|Q|

ptf (t) = K(t, qi)IDF (qi)

(5)

i=1

where qi is a query term, |Q| is the number of query terms, and IDF (qi) is the same as in PRoc1. N is the number of documents in the collection, and Nt is the number of documents that contain qi. The second variant of PRoc is denoted by PRoc2 in the rest of this paper.

3.4 HAL Method
The Hyperspace Analogue to Language (HAL) [12] is a computational modeling of psychological theory of word meaning by considering context only as the words that immediately surround the given word. The basic motivation is that when a human encounters a new concept, its meaning is derived via other concepts occurred within the same context.
An shown in [39], the HAL Space is automatically built from a corpus of text, defined as follows: for each term in a specified vocabulary V , a |V | ◊ |V | matrix is built by moving a sliding windows of length wSize across the corpus, where |V | is the number of terms in vocabulary V . All words within the window are considered as co-occurring with each other with strengths inversely proportional to the distance between them. The weightings of each co-occurred terms are accumulated over the corpus. Then, a term can be represented by a semantic vector, in which each dimension is the weight for this term and other terms as follows:

wSize

HAL(t ||t) =

w(k)n(t, k, t )

(6)

k=1

where k is the distance from term t to t, n(t, k, t') is the cooccurrence frequency within the sliding windows when the distance equals k, and w(k) = wSize - k + 1 denotes the strength.
In this paper, we adapt the the original HAL model similarly as in [12]. In particular, in order to measure the proximity between a candidate expansion term and the original query, we restrict the context to the query terms, not all the co-occurred terms in the feedback documents. With this adaption, the resulting vector for each candidate term denotes a proximity relationship with the entire query. Like the Kernel-based method, we also take into account the importance factor of query terms in the same way. Then, the HAL based ptf is as follows:

|Q|
ptf (t) = vec(t) ∑ vec(Q) = HAL(t||qi)IDF (qi) (7)
i=1
IDF (qi) is the as in PRoc1 In the adaption of proximity information in PRF, ptf re-
places the traditional term frequency in our approach.

538

The weighted HAL model includes the information of term distances and co-occurrence frequencies completely. It is the first time that this linguistic model is adopted to measure the proximity. The third variant of PRoc is denoted by PRoc3 in the rest of this paper.
4. EXPERIMENTAL SETTINGS
4.1 Test Collections
In this section, we describe four representative test collections used in our experiments: Disk4&5, WT2G, WT10G, and GOV2, which are different in size and genre. The Disk4&5 collection contains newswire articles from various sources, such as Association Press (AP), Wall Street Journal (WSJ), Financial Times (FT), etc., which are usually considered as high-quality text data with little noise. The WT2G collection is a general Web crawl of Web documents, which has 2 Gigabytes of uncompressed data. This collection was used in the TREC 8 Web track. The WT10G collection is a medium size crawl of Web documents, which was used in the TREC 9 and 10 Web tracks. It contains 10 Gigabytes of uncompressed data.
The GOV2 collection, which has 426 Gigabytes of uncompressed data, is crawled from the .gov domain. This collection has been employed in the TREC 2004, 2005 and 2006 Terabyte tracks. GOV2 is a very large crawl of the .gov domain, which has more than 25 million documents with an uncompressed size of 423 Gigabytes. There are 150 ad-hoc query topics, from TREC 2004 - 2006 Terabyte tracks, associated to GOV2. In our experiments, we use 100 topics in TREC 2005 - 2006. The TREC tasks and topic numbers associated with each collection are presented in Table 1. As we can see from this table, we evaluate the proposed approach with a relative large number of queries. In all the

Table 1: The TREC tasks and topic numbers asso-

ciated with each collection.

Collection

Task

Queries

Docs

Disk4&5

TREC 2004, Robust

301-450 528,155

WT2G

TREC8, Web ad-hoc 401-450 247,491

WT10G TREC9, 10, Web ad-hoc 451-550 1,692,096

GOV2 TREC04-06, Web ad-hoc 701-850 25,178,548

experiments, we only use the title field of the TREC queries for retrieval. It is closer to the actual queries used in the real application and feedback is expected to be the most useful for short queries [42].
In the process of indexing and querying, each term is stemmed using Porter's English stemmer [22], and stopwords from InQuery's standard stoplist [1] with 418 stopwords are removed. The MAP (Mean Average Precision) performance measure for the top 1000 documents is used as evaluation metric, as is commonly done in TREC evaluations. The MAP metric reflects the overall accuracy and the detailed descriptions for MAP can be found in [35]. We take this metric as the primary single summary performance for the experiments, which is also the main official metric in the corresponding TREC evaluations.
4.2 Baseline Models
In our experiments, we compare our PRoc models with the traditional combination of BM25 and Rocchio's feedback model. In addition, we also compare the proposed

models with the state-of-the-art feedback models in the KLdivergence language modeling (LM) retrieval framework. In particular, for the basic language model, we use a Dirichlet prior (with a hyperparameter of ) for smoothing the document language model as shown in Equation 8, which can achieve good performance generally [43]. Besides, this is also utilized as the basic model in [16].

p(w|d)

=

c(wd) + p(w|C) |d| + 

(8)

where c(wd) is the frequency of query term w in document d, p(w|C) is the probability of term w in collection C and |d| is the length of document d. We train the parameter in the document language model in all the experiments in order to make fair comparisons, and focus on evaluating different ways of approaching the query-related topic for PRF.
For PRF in language modeling framework, we first compare our proposed model with the relevance language model [13, 14], which is a representative and state-of-the-art approach for re-estimating query language models for PRF [14]. Relevance language models do not explicitly model the relevant or pseudo-relevant document. Instead, they model a more generalized notion of relevance R. The formula of RM1 is:

p(w|R)  p(w|D)p(D)P (Q|D)

(9)

D

The relevance model p(w|R) is often used to estimate the feedback language model F , and then interpolated with the original query model Q in order to improve its estimation as follows:

Q = (1 - )  Q +   F

(10)

This interpolated version of relevance model is called RM3. Lv et al. [14] systematically compared five state-of-the-art approaches for estimating query language models in ad-hoc retrieval, in which RM3 not only yields impressive retrieval performance in both precision and recall metric, but also performs steadily. In particular, we apply Dirichlet prior for smoothing document language models [42].

4.3 Parameter Settings
As we can see from all the PRF retrieval models in our experiments, there are several controlling parameters to tune. In order to find the optimal parameter setting for fair comparisons, we use the training method presented in [7] for both the baselines and our proposed models, which is popular in the IR domain for building strong baselines. In particular, first, for the smoothing parameter  in LM with Dirichlet prior, we sweep over values from 500 to 2000 with an interval of 100. Meanwhile, we sweep the values of b for BM25 from 0 to 1.0 with an interval of 0.1. Second, for parameters in PRF models, we empirically set the number of top documents to 20 in baseline PRF approaches and our PRoc models, the number of expansion terms (k  10, 20, 30, 50), and the interpolation parameter (  0.0, 0.1, . . . , 1.0). The window size for PRoc models are from 10 to 1500 with an interval 10. To evaluate the baselines and our proposed approach, we use 2-fold cross-validation, in which the TREC queries are partitioned into two sets by the parity of their numbers on each collection. Then, the parameters learned on the training set are applied to the test set for evaluation purpose as in [19].

539

Table 2: BM25 vs LM on the four TREC collections

BM25 LM

disk4&5 0.2216 0.2247

WT2G 0.3124 0.2995

WT10G 0.2055 0.2063

GOV2 0.3034 0.3040

5. EXPERIMENTS AND ANALYSIS
5.1 Comparison of Basic Retrieval Models
As we mentioned in the previous section, the results of both models are obtained by 2-fold cross-validation. Therefore, it is fair to compare them on these four collections. BM25 slightly outperforms LM with Dirichlet prior on the WT2G collection. The results of these two models are almost the same over the Disk4&5, WT10G and GOV2 collections. This comparison indicates that the classic BM25 model is generally comparative to LM, and it is reasonable to use them as the basic models of the PRF baselines and our proposed model.
5.2 Comparison with PRF Models
From Table 2 to Table 6, we can clearly see that the average performance of PRF models is superior to the basic models in most cases. The classic Rocchio's model achieves improvements of 13.31%, -0.41%, 2.34% and 4.22% over BM25 on the Disk4&5, WT2G, WT10G and GOV2 collections, while RM3 obtains significant improvements over LM (4.05%, 7.98%, 3.64% and 3.32%) on all the four collections1. The effectiveness of pseudo relevance feedback is reconfirmed in this set of experiments. The classic Rocchio's model, fails to obtain improvement on the WT2G collection. This indicates that the Rocchio's model is not so robust as RM3 in this case. However, the Rocchio's model outperforms RM3 on the Disk4&5 collection significantly while RM3 performs better than the classic Rocchio's model on the WT2G collection. On the WT10G and GOV2 collections, their results are very close. Therefore, the Rocchio's model is generally comparable to RM3 so that it is still competitive to be a strong baseline.
In general, the performance of our proposed PRoc models is close on all the four collections, and all of them obtain more improvements over the basic models than the Rocchio's model and RM3. Specifically, from Table 3 to Table 6, we observe that all the three proximity-based Rocchio's models outperform the classic Rocchio's model (2.00% - 11.54%) and state-of-the-art RM3 (4.78% - 12.75%) significantly on all the four collections, which demonstrates the effectiveness of the three PRoc models. Although the measures in our PRoc models are different, all of them can successfully model the proximity information to some extent. Furthermore, the PRoc models perform more robustly than the classic Rocchio's model. It indicates that proximity plays an important role in discriminating relevant expansion terms from irrelevant ones.
In addition, from Table 4 we observe that PRoc3 outperforms the other two on the WT2G collection. On the other three collections, the performance of all the three PRoc models is very close. Generally, PRoc3 is slightly more effective than the other two PRoc models.
1The computation of these percentages is based on the average performance of the Rocchio's model and RM3 in Table 36 and MAPs in Table 2.

5.3 Effectiveness of Window Size
In our proposed PRoc models, there are two important parameters: (1)  in the feedback models controlling how much we rely on the original query and the feedback information and (2) window size parameter wSize in the calculation of the proximity-based frequency. In our preliminary experiments, we observed that the influence of  is similar to that in [38], which investigated this parameter thoroughly. Since we mainly focus on the study of proximity evidence, detailed discussion about  will not be made in this paper.
wSize is a key parameter for most proximity measures because it determines the distance in which terms are considered to be related. Thus, how to find an appropriate window size is very important for adapting the proximity measures. In this section, we attempt to discover some useful evidence for obtaining optimal wSize values. Particularly,  in PRoc2 is also interpreted as the window size.
From Figure 1 to 4, we show how the performance of our PRoc models changes with wSize on different collections. We investigate a large range of wSize from 10 to 1500, and the numbers of expansion terms are 10, 20, 30 and 50. Generally, the values of wSize affect the performance of all the PRoc models extensively. In the second subfigure of Figure 4, the best MAP is 0.3205 when number of expansion terms is 50, and it falls to 0.2286 when wSize is 1500. Almost 30% of performance is lost in this case.
For PRoc1, PRoc2 and PRoc3, although they are based on different measures, their curves fluctuate similarly on the same collection with different numbers of expansion terms. In contrast, the curves of each PRoc model are various extensively on different collections. This demonstrates that the influence of wSize is collection-based. However, the best wSize values for PRoc models are not the same, not even close to each other. For example, on the disk4&5 collection, optimal wSize values for PRoc1 are 80, 50, 100 and 80 over 10, 20, 30 and 50 expansion terms, and the corresponding values for PRoc1 on WT10G is 100, 30, 10 and 10. Thus, the optimal values of wSize depend on the proximity measures and the collections.
Another phenomenon is that the more the expansion terms are selected, the more the performance is affected by wSize. Normally, the performance of PRoc models drops when wSize takes a relatively large value. However, to what extent the performance is affected is determined by the number of expansion terms. Specifically, in Figure 2, while wSize is relatively small, the performance of PRoc models with 50 expansion terms is the best. However, when wSize is larger than 200, the curves for 50 expansion terms are constantly below all the others. As an additional example, when the number of expansion terms is 30, the performance of PRoc models is the second worst in most cases when wSize is 1500.
This is reasonable because the accumulated influence of proximity information for 50 expansion terms is larger than that of the small numbers. When wSize increases, it is very likely that more noise is adopted in the expansion term selection. The more expansion term are there, the more noisy information is involved. Thus, the wSize must be set very carefully when the number of expansion is larger than 30 in our case.
Additionally, the influence of wSize on PRoc1 is more significant than on the other two over WT10G. Meanwhile, wSize affects PRoc2 more significantly over GOV2 than PRoc1 and PRoc3. Overall, the PRoc3 model is less sensi-

540

MAP

MAP

0.27 0.265
0.26 0.255
0.25 0.245
0.24 10 80 200

PRoc1

fb term #=10 fb term #=20 fb term #=30 fb term #=50

500 700

1000 1200

1500

window size

MAP

0.27 0.265
0.26 0.255
0.25 0.245
0.24 10 80 200

PRoc2

fb term #=10 fb term #=20 fb term #=30 fb term #=50

500 700

1000 1200

1500

window size

MAP

0.27 0.265
0.26 0.255
0.25 0.245
0.24 10 80 200

PRoc3

fb term #=10 fb term #=20 fb term #=30 fb term #=50

500 700

1000 1200

1500

window size

Figure 1: PRoc1, PRoc2 and PRoc3 over disk4&5 with 10, 20, 30 and 50 expansion terms

0.36 0.355
0.35 0.345
0.34 0.335
0.33 0.325
0.32 0.315
0.31 10 80 200

PRoc1

fb term #=10 fb term #=20 fb term #=30 fb term #=50

500 700

1000 1200

1500

window size

MAP

0.36 0.355
0.35 0.345
0.34 0.335
0.33 0.325
0.32 0.315
0.31 10 80 200

PRoc2

fb term #=10 fb term #=20 fb term #=30 fb term #=50

500 700

1000 1200

1500

window size

MAP

0.36 0.355
0.35 0.345
0.34 0.335
0.33 0.325
0.32 0.315
0.31 10 80 200

PRoc3

fb term #=10 fb term #=20 fb term #=30 fb term #=50

500 700

1000 1200

1500

window size

Figure 2: PRoc1, PRoc2 and PRoc3 over WT2G with 10, 20, 30 and 50 expansion terms

PRoc1

PRoc2

PRoc3

0.24

0.24

0.24

fb term #=10

fb term #=10

fb term #=10

fb term #=20

fb term #=20

fb term #=20

fb term #=30

fb term #=30

fb term #=30

0.23

fb term #=50

0.23

fb term #=50

0.23

fb term #=50

0.22

0.22

0.22

MAP

MAP

0.21

0.21

0.21

0.2

0.2

0.2

0.19

0.19

0.19

0.18 10 80 200

500 700

1000 1200

1500

window size

0.18 10 80 200

500 700

1000 1200

1500

window size

0.18 10 80 200

500 700

1000 1200

1500

window size

Figure 3: PRoc1, PRoc2 and PRoc3 over WT10G with 10, 20, 30 and 50 expansion terms

0.33 0.32 0.31
0.3 0.29 0.28 0.27
10 80 200

PRoc1

fb term #=10 fb term #=20 fb term #=30 fb term #=50

500 700

1000 1200

1500

window size

MAP

0.32 0.31
0.3 0.29 0.28 0.27 0.26 0.25 0.24 0.23 0.22
10 80 200

PRoc2

fb term #=10 fb term #=20 fb term #=30 fb term #=50

500 700

1000 1200

1500

window size

MAP

0.33 0.32 0.31
0.3 0.29 0.28 0.27
10 80 200

PRoc3

fb term #=10 fb term #=20 fb term #=30 fb term #=50

500 700

1000 1200

1500

window size

Figure 4: PRoc1, PRoc2 and PRoc3 over GOV2 with 10, 20, 30 and 50 expansion terms

MAP

MAP

541

Table 3: PRoc compares with BM25+Rocchio and LM+RM3 on Disk4&5. The percentages in the parenthesis are the improvement gains over the classic Rocchio's model and RM3. A "*" indicates a statistically significant improvement over the classic Rocchio's model baseline, and a "+" indicates a statistically significant improvement over the RM3 model baseline according to the Wilcoxon matched-pairs signed-ranks test at the 0.05 level. The bold phase style means that it is the best result.

# of feeback terms 10 20 30 50
Average

PRoc1
0.2509+ 0.2567+ 0.2589+ 0.2578+
0.2561 (2.00%, 9.54%,)

PRoc2
0.2523+ 0.2596+ 0.2595+ 0.2602+
0.2579 (2.71%, 10.31%,)

PRoc3
0.2571+ 0.2647+ 0.2662+ 0.2662+
0.2636 (4.98%, 12.75%,)

BM25 + Rocchio 0.2463 0.2504 0.2545 0.2533 0.2511

LM + RM3 0.2289 0.2326 0.2356 0.2382 0.2338

Table 4: PRoc compares with BM25+Rocchio and LM+RM3 on WT2G

# of feeback terms 10 20 30 50

PRoc1
0.3415+ 0.3501+ 0.3452+ 0.3513+

PRoc2
0.3415+ 0.3403+ 0.3456+ 0.3406+

PRoc3
0.3385+ 0.3424+ 0.3461+ 0.3525+

BM25 + Rocchio 0.3091 0.311 0.3082 0.3162

Average

0.3470 (11.54%, 7.30%,) 0.3420 (9.93%, 5.75%,) 0.3449 (10.86%, 6.65%,)

0.3111

LM + RM3 0.3212 0.3225 0.3255 0.3242 0.3234

Table 5: PRoc compares with BM25+Rocchio and LM+RM3 on WT10G

# of feeback terms 10 20 30 50

PRoc1
0.2290+ 0.2272+ 0.2260+ 0.2202+

PRoc2
0.2267+ 0.2219+ 0.2264+ 0.2206+

PRoc3
0.2308+ 0.2287+ 0.2261+ 0.2245+

BM25 + Rocchio 0.2143 0.2147 0.2084 0.2039

Average

0.2256 (7.28%, 5.52%,) 0.2239 (6.47%, 4.72%,) 0.2275 (8.18%, 6.41%,)

0.2103

LM + RM3 0.2098 0.2168 0.2151 0.2136 0.2138

Table 6: PRoc compares with BM25+Rocchio and LM+RM3 on GOV2

# of feeback terms 10 20 30 50

PRoc1
0.3271+ 0.3303+ 0.3323+ 0.3242+

PRoc2
0.3294+ 0.3325+ 0.3316+ 0.3313+

PRoc3
0.3288+ 0.3315+ 0.3320+ 0.3313+

BM25 + Rocchio 0.3126 0.3164 0.3175 0.3181

Average

0.3285 (3.89%, 4.58%,) 0.3312 (4.74%, 5.44%,) 0.3309 (4.65%, 5.35%,)

0.3162

LM + RM3 0.3071 0.3141 0.3167 0.3183 0.3141

tive than the other two PRoc models according to our experiments.
In summary, a big challenge is to find an optimal value since the value space is very large without any constraints. It is very time-consuming to try every possible values in the relevance feedback process. In order to narrow the value space of wSize, we attempt to find a rule to direct the searching of optimal values. Based on our experimental results, we conjecture that there are two factors affecting the choice of wSize: the average document length (ADL) and the size of a collection. Intuitively, if the average document length is large, it is more likely to have more than one topic in a document which leads to involve more irrelevant terms. Besides, as the increase of the size of collection, it is more likely to bring irrelevant documents into the feedback process.
In order to minimize the negative influence of noise, the values of wSize should be relatively small especially when the ADL or collection size is large. Only the closest terms will be considered to avoid the selection of irrelevant terms. In our experiments, this rule is supported by some evidence. The ADL of Disk4&5 is 334 and there are only 528,155 documents in this collection. The performance of all the PRoc models is not affected by wSize so extensively as that on

the other collections. When there are only 10 expansion terms, the optimal value of wSize can be as large as 1500. On WT10G, which has 1,692,096 documents and an average document length of 426, the optimal wSize values are larger than 50 but smaller than 200. For WT2G, even it has less documents (247,491) than other collections, the optimal wSize values are in a range of (30, 50) because of its long ADL (722). GOV2 is the largest collection with 25,178,548 documents in our experiments, and its ADL(679) is only smaller than that of WT2G. As a result, the optimal values of wSize for GOV2 is the smallest one. It is always 10 in our case. In summary, we can use this rule to narrow the search space of optimal wSize values. If a collection has plenty of documents or its ADL is large (e.g., more than 700), it is always good for us to start from 20 or smaller. Otherwise, we can try a larger starting value like 50 or more.
5.4 Comparison with PRM
We also compare our proposed model with the recently developed position relevance model (PRM) [16], which is an extension of the relevance model. In particular, PRM takes into account term positions and proximity with the intuition that words closer to query words are more likely to

542

Table 7: PRoc compares with the classic Rocchio's model, RM3, PRM1 and PRM2 on Tera06 dataset. The bold phase style means that it is the best result.
PRoc3 Rocchio RM3 PRM1 PRM2 MAP 0.3283 0.3156 0.3131 0.3322 0.3319 P@10 0.5800 0.5800 0.5043 0.5306 0.5490 P@30 0.5260 0.5167 0.4660 0.4884 0.4871 P@100 0.3756 0.3664 0.3576 0.3671 0.3741
be related to the query topic, and assigns more weights to candidate expansion terms closer to the query. To make the comparison fair, we train our parameters on the Terabyte05 topics and use Terabyte06 2 topics on the GOV2 collection for testing as Lv. et al. did in [16]. Since we do not give results for the Million Query Track so far, we do not compare our method with PRM on the ClueWeb collection with the topics of this track. In [16], parameter  in the Dirichlet smoothing is set to an optimal value of 1500, and we set b in our basic model, BM25, empirically to 0.3 [44]. As we mentioned previously, the performance of BM25 and LM with Dirichlet smoothing does not differ significantly on the GOV2 collection. Therefore, this setting will not affect the comparison. Since PRoc3 is the most robust and performs the best generally, it is selected to make this comparison. There are two versions of PRM, PRM1 and PRM2. The results of RM3, PRM1 and PRM2 are directly from [16].
In Table 7, PRoc3 outperforms the classic Rocchio's model and RM3 significantly in terms of the MAP metric, and it is only slightly inferior to PRM1 and PRM2 by 1.19% and 1.1% respectively. On the P@10, P@30 and P@100 metrics, PRoc3 obtains the best results over all the other four models and outperforms RM3, PRM1 and PRM2 significantly. All these significant tests are based on the Wilcoxon matchedpairs signed-ranks test at the 0.05 level. This shows that the retrieval accuracy of our proposed model is better than that of the PRF models in the language modeling framework in this case. Since the results of PRM1 and PRM2 are optimized, it is reasonable to state that our propose model is at least comparable to the most recent progress.
6. CONCLUSIONS AND FUTURE WORK
In this paper, a novel feedback model PRoc is proposed by incorporating proximity information into the classic Rocchio's model. Specifically, we model the statistics of expansion terms and their proximity relationship with query terms by introducing a new concept ptf . Three proximity measures, namely window-based method, kernel-based method and the HAL method, are then proposed for evaluating the relationship between expansion terms and query terms. The corresponding PRoc models based on these measures, PRoc1, PRoc2 and PRoc3, are evaluated extensively on four standard TREC collections. In general, PRoc is very effective and outperforms the state-of-the-art feedback models in different frameworks. Comparing the three variants of PRoc, PRoc3 is more effective and robust than PRoc1 and PRoc2. Meanwhile, our proposed PRoc is at least competitive to the most recent work, PRM. Additionally, we carefully analyze the influence of the parameter of wSize, and an empirical rule to narrow the value space of the window size is suggested.
2http://trec.nist.gov/data/terabyte.html

In the future, we will try to discover more about how to effectively adapt proximity into the probabilistic retrieval models. Another possible research direction is to find the exact relationship between the window size factor and the information of collections, e.g., the length distribution of documents. It is also interesting to apply our work to other retrieval frameworks, like DFR or the language modeling framework.
7. ACKNOWLEDGMENTS
This research is supported by the research grant from the Natural Sciences & Engineering Research Council (NSERC) of Canada and the Early Researcher Award/ Premier's Research Excellence Award. We thank four anonymous reviewers for their thorough review comments on this paper.
8. REFERENCES
[1] James Allan, Margaret E. Connell, W. Bruce Croft, Fangfang Feng, David Fisher, and Xiaoyan Li. INQUERY and TREC-9. In TREC, 2000.
[2] G. Amati. Probabilistic models for information retrieval based on divergence from randomness. PhD thesis, Department of Computing Science, University of Glasgow, 2003.
[3] Stefan Bu®ttcher, Charles L. A. Clarke, and Brad Lushman. Term proximity scoring for ad-hoc retrieval on very large text collections. In Proceedings of the 29th annual international ACM SIGIR conference, SIGIR '06, pages 621≠622, New York, NY, USA, 2006. ACM.
[4] G. Romano C. Carpineto, R. de Mori and B. Bigi. An information-theoretic approach to automatic query expansion. ACM Transactions on Information Systems (TOIS), 19(1):1≠27, 2001.
[5] Charles L.A. Clarke, Gordon V. Cormack, and Elizabeth A. Tudhope. Relevance ranking for one to three term queries. Information Processing Management, 36(2):291 ≠ 311, 2000.
[6] Kevyn Collins-Thompson. Reducing the risk of query expansion via robust constrained optimization. In CIKM '09: Proceeding of the 18th ACM conference on Information and knowledge management, CIKM '09, pages 837≠846, New York, NY, USA, 2009. ACM.
[7] Fernando Diaz and Donald Metzler. Improving the estimation of relevance models using large external corpora. In SIGIR '06: Proceedings of the 29th annual international ACM SIGIR conference, pages 154≠161, New York, NY, USA, 2006. ACM.
[8] Ben He, Jimmy Xiangji Huang, and Xiaofeng Zhou. Modeling term proximity for probabilistic information retrieval models. Inf. Sci., 181(14):3017≠3031, 2011.
[9] Xiangji Huang, Yan Rui Huang, Miao Wen, Aijun An, Yang Liu, Josiah Poon. applying data mining to pseudo-relevance feedback for high performance text retrieval. In IEEE ICDM 2006, page 295≠306, 2006.
[10] E. Michael Keen. The use of term position devices in ranked output experiments. J. Doc., 47:1≠22, 1991.
[11] E. Michael Keen. Some aspects of proximity searching in text retrieval systems. J. Inf. Sci., 18:89≠98, 1992.
[12] Ruth Ann Atchley Kevin Lund, Curt Burgess. Semantic and associative priming in high-dimensional semantic space. In Proceedings of the 17th CogSci, pages 660≠665, 1995.
[13] Victor Lavrenko and W. Bruce Croft. Relevance based language models. In Proceedings of the 24th annual international ACM SIGIR conference, SIGIR '01, pages 120≠127, New York, USA, 2001. ACM.

543

[14] Yuanhua Lv and ChengXiang Zhai. A comparative study of methods for estimating query language models with pseudo feedback. In CIKM '09: Proceeding of the 18th ACM conference on Information and knowledge management, pages 1895≠1898, New York, NY, USA, 2009. ACM.
[15] Yuanhua Lv and ChengXiang Zhai. Positional language models for information retrieval. In Proceedings of the 32nd international ACM SIGIR conference, SIGIR '09, pages 299≠306, New York, NY, USA, 2009. ACM.
[16] Yuanhua Lv and ChengXiang Zhai. Positional relevance model for pseudo-relevance feedback. In Proceeding of the 33rd SIGIR conference, SIGIR '10, pages 579≠586. ACM, 2010.
[17] Donald Metzler and W. Bruce Croft. A markov random field model for term dependencies. In SIGIR '05: Proceedings of the 28th annual International ACM SIGIR Conference, pages 472≠479, New York, NY, USA, 2005. ACM.
[18] Donald Metzler and W. Bruce Croft. Latent concept expansion using markov random fields. In SIGIR '07: Proceedings of the 30th annual international ACM SIGIR conference, pages 311≠318, New York, NY, USA, 2007. ACM.
[19] Donald Metzler, Jasmine Novak, Hang Cui, and Srihari Reddy. Building enriched document representations using aggregated anchor text. In SIGIR '09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 219≠226, New York, NY, USA, 2009. ACM.
[20] Vassilis Plachouras and Iadh Ounis. Multinomial randomness models for retrieval with document fields. In Proceedings of ECIR, pages 28≠39, 2007.
[21] Jay M. Ponte and W. Bruce Croft. A language modeling approach to information retrieval. In SIGIR '98: Proceedings of the 21st annual international ACM SIGIR conference, pages 275≠281, New York, NY, USA, 1998. ACM.
[22] M. Porter. An algorithm for suffix stripping. Program, 14:130≠137, 1980.
[23] Karthik Raman, Raghavendra Udupa, Pushpak Bhattacharyya, and Abhijit Bhole. On improving pseudo-relevance feedback using pseudo-irrelevant documents. In ECIR, pages 573≠576, 2010.
[24] Yves Rasolofo and Jacques Savoy. Term proximity scoring for keyword-based retrieval systems. In Fabrizio Sebastiani, editor, Advances in Information Retrieval, volume 2633 of Lecture Notes in Computer Science, pages 79≠79. Springer Berlin, Heidelberg, 2003.
[25] Stephen E. Robertson. On term selection for query expansion. Journal of Documentation, 46:359≠364, January 1991.
[26] Stephen E. Robertson, Steve Walker, Micheline Hancock-Beaulieu, Mike Gatford, and A. Payne. Okapi at TREC-4. In TREC, 1995.
[27] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. Okapi at TREC-3. In TREC, pages 109≠126, 1994.
[28] J. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, The SMART retrieval system: Experiments in automatic document, pages 313≠323, 1971.
[29] Gerald Salton. The SMART Retrieval System. Prentice Hall, New Jersey, 1971.
[30] Gerard Salton and Chris Buckley. Improving retrieval

performance by relevance feedback. Journal of the American Society for Information Science, 41:288≠297, 1990.
[31] Ruihua Song, Michael Taylor, Ji-Rong Wen, Hsiao-Wuen Hon, and Yong Yu. Viewing term proximity from a different perspective. In Craig Macdonald, Iadh Ounis, Vassilis Plachouras, Ian Ruthven, and Ryen White, editors, Advances in Information Retrieval, volume 4956 of Lecture Notes in Computer Science, pages 346≠357. Springer Berlin / Heidelberg, 2008.
[32] Tao Tao and ChengXiang Zhai. Regularized estimation of mixture models for robust pseudo-relevance feedback. In SIGIR '06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 162≠169, New York, NY, USA, 2006. ACM.
[33] Tao Tao and ChengXiang Zhai. An exploration of proximity measures in information retrieval. In Proceedings of the 30th annual international ACM SIGIR conference, SIGIR '07, pages 295≠302, New York, USA, 2007. ACM.
[34] Olga Vechtomova and Ying Wang. A study of the effect of term proximity on query expansion. Journal of Information Science, 32(4):324≠333, August 2006.
[35] Ellen M. Voorhees and Donna Harman. Overview of the sixth text retrieval conference. Information Processing and Management: an International Journal, 36:3≠35, July 2000.
[36] Ryen W. White and Gary Marchionini. Examining the effectiveness of real-time query expansion. Inf. Process. Manage., 43(3):685≠704, 2007.
[37] Jinxi Xu and W. Bruce Croft. Improving the effectiveness of information retrieval with local context analysis. ACM Trans. Inf. Syst., 18(1):79≠112, 2000.
[38] Zheng Ye, Ben He, Xiangji Huang, and Hongfei Lin. Revisiting rocchio's relevance feedback algorithm for probabilistic models. pages 151≠161. AIRS, 2010.
[39] Zheng Ye, Xiangji Huang, and Hongfei Lin. A bayesian network approach to context sensitive query expansion. In SAC, pages 1138≠1142, 2011.
[40] Zheng Ye, Jimmy Xiangji Huang, and Hongfei Lin. Finding a good query-related topic for boosting pseudo-relevance feedback. Journal of the American Society for Information Science and Technology (JASIST), 62(4):748-760, 2011.
[41] ChengXiang Zhai. Statistical language models for information retrieval a critical review. Found. Trends Inf. Retr., 2:137≠213, March 2008.
[42] Chengxiang Zhai and John Lafferty. Model-based feedback in the language modeling approach to information retrieval. In CIKM '01: Proceedings of the tenth international conference on Information and knowledge management, pages 403≠410. ACM, 2001.
[43] Chengxiang Zhai and John Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179≠214, 2004.
[44] Jiashu Zhao, Jimmy Xiangji Huang, and Ben He. CRTER: using cross terms to enhance probabilistic information retrieval. In Proceedings of the 34th international ACM SIGIR conference, SIGIR '11, pages 155≠164, New York, USA, 2011. ACM.
[45] Jinglei Zhao and Yeogirl Yun. A proximity language model for information retrieval. In Proceedings of the 32nd international ACM SIGIR conference, SIGIR '09, pages 291≠298, New York, USA, 2009. ACM.

544

A Generalized Hidden Markov Model with Discriminative Training for Query Spelling Correction

Yanen Li, Huizhong Duan and ChengXiang Zhai
Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801
{yanenli2, duan9, czhai}@illinois.edu

ABSTRACT
Query spelling correction is a crucial component of modern search engines. Existing methods in the literature for search query spelling correction have two major drawbacks. First, they are unable to handle certain important types of spelling errors, such as concatenation and splitting. Second, they cannot efficiently evaluate all the candidate corrections due to the complex form of their scoring functions, and a heuristic filtering step must be applied to select a working set of top-K most promising candidates for final scoring, leading to non-optimal predictions. In this paper we address both limitations and propose a novel generalized Hidden Markov Model with discriminative training that can not only handle all the major types of spelling errors, including splitting and concatenation errors, in a single unified framework, but also efficiently evaluate all the candidate corrections to ensure the finding of a globally optimal correction. Experiments on two query spelling correction datasets demonstrate that the proposed generalized HMM is effective for correcting multiple types of spelling errors. The results also show that it significantly outperforms the current approach for generating top-K candidate corrections, making it a better firststage filter to enable any other complex spelling correction algorithm to have access to a better working set of candidate corrections as well as to cover splitting and concatenation errors, which few existing method in academic literature can correct.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Query Alteration
Keywords
Query Spelling Correction, Generalized Hidden Markov Models, Discriminative Training for HMMs
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-0757-4/11/07 ...$15.00.

1. INTRODUCTION
The ability to automatically correct potentially misspelled queries has become an indispensable component of modern search engines. People make errors in spelling frequently. Particularly, search engine users are more likely to commit misspellings in their queries as they are in most scenarios exploring unfamiliar contents. Automatic spelling correction for queries helps the search engine to better understand the users' intents and can therefore improve the quality of search experience. However, query spelling is not an easy task, especially under the strict efficiency constraint. In Table 1 we summarize major types of misspellings in real search engine queries. Users not only make typos on single words, (insertion, deletion and substitution), but can also easily mess up with word boundaries (concatenation and splitting). Moreover, different types of misspelling could be committed in the same query, making it even harder to correct. Unfortunately,

Table 1: Major Types of Query Spelling Errors

Type

Example

Correction

Insertion In-Word Deletion

esspresso vollyball

espresso volleyball

Substitution Mis-use

comtemplate contemplate capital hill capitol hill

Cross-Word

Concatenation Splitting

intermilan power point

inter milan powerpoint

no existing query spelling correction approaches in the literature are able to correct all major types of errors, especially for correcting splitting and concatenation errors. To the best of my knowledge, the only work that can potentially address this problem is [24] in which a Conditional Random Field (CRF) model is proposed to handle a broad set of query refinements. However, this work considers query correction and splitting/merging as different tasks, hence it is unable to correct queries with mixed types of errors, such as substitution and splitting errors in one query. In fact splitting and merging are two important error types in query spelling correction, and a major research challenge of query spelling correction is to accurately correct all major types of errors simultaneously. On the other hand, while modern industrial systems such as Google can handle this problem relatively well, it's unclear what they have done, especially what resources they have employed in order to solve the challenges.
Another major difficulty in automatic query spelling correction is the huge search space. Theoretically, any sequence

611

of characters could potentially be the correction of a misspelled query. It is clearly intractable to enumerate and evaluate all possible sequences for the purpose of finding the correct query. Thus a more feasible strategy is to search in a space of all combinations of candidate words that are in a neighborhood of each query word based on editing distance. The assumption is that a user's spelling error of each single word is unlikely too dramatic, thus the correction is most likely in the neighborhood by editing distance. Unfortunately, even in this restricted space, the current approaches still cannot enumerate and evaluate all the candidates because their scoring functions involve complex features that are expensive to compute. As a result, a separate filtering step must first be used to prune the search space so that the final scoring can be done on a small working set of candidates. Take [7] as a two-stage method example, in the first stage, a Viterbi or A* search algorithm is used to generate a small set of most promising candidates, and in the second stage different types of features of the candidates are computed and a ranker is employed to score the candidates. However, this two-stage strategy has a major drawback in computing the complete working set. Since the filtering stage uses a non-optimal objective function to ensure efficiency, it is quite possible that the best candidate is filtered out in the first stage, especially because we cannot afford a large working set since the correction must be done online while a user is entering a query. The inability of searching the complete space of candidates leads to non-optimal correction accuracy.
In this paper, we propose a generalized Hidden Markov Model (gHMM) for query spelling correction that can address deficiencies of the existing approaches discussed above. The proposed gHMM can model all major types of spelling errors, thus enabling consideration of multiple types of errors in query spelling correction. In the proposed gHMM, the hidden states represent the correct forms of words, and the outcomes are the observed (potentially) misspelled terms. In addition, each state is associated with a type, indicating merging, splitting or in-word transformation operation. The proposed HMM is generalized in the sense that it would allow adjustment of both emission probabilities and transition probabilities to accommodate the non-optimal parameter estimation. Unfortunately, such an extension of HMM makes it impossible to use a standard EM algorithm for parameter estimation. To solve this problem, we propose a perceptron-based discriminative training method to train the parameters in the HMM.
Moreover, a Viterbi-like search algorithm for top-K paths is designed to efficiently obtain a small number of highly confident correction candidates. This algorithm can handle splitting/merging of multiple words. It takes into account major types of local features such as error model, language model, and state type information. The error model is trained on a large set of query correction pairs from the web. And web scale language model is obtained by leveraging the Microsoft Web N-gram service [1].
We conducted extensive evaluation on our proposed gHMM. For this purpose, we have constructed a query correction dataset from real search logs, which has been made publicly available. Experimental results verify that the gHMM can effectively correct all major types of query spelling errors. It also reveal that the gHMM can run as efficient as the common used noisy channel model, while it achieves much

better results for obtaining the candidate space of query corrections. Therefore, in addition to being used as standing alone query correction module, the proposed gHMM can also be used as a more effective first-stage filtering module to more effectively support any other complicated scoring functions such as those using complex global features.
2. RELATED WORK
Spelling correction has a long history [8]. Traditional spellers focused on dealing with non-word errors caused by misspelling a known word as an invalid word form. A common strategy at that time was to utilize a trusted lexicon and certain distance measures, such as Levenshtein distance [13]. The size of lexicon in traditional spellers is usually small due to the high cost of manual construction of lexicon. Consequently, many valid word forms such as human names and neologisms are rarely included in the lexicon. Later, statistical generative models were introduced for spelling correction, in which the error model and n-gram language model are identified as two critical components. Brill and Moore demonstrated that a better statistical error model is crucial for improving a speller's accuracy [3]. But building such an error model requires a large set of manually annotated word correction pairs, which is expensive to obtain. Whitelaw et al. alleviated this problem by leveraging the Web to automatically discover the misspelled/corrected word pairs [12].
With the advent of the Web, the research on spelling correction has received much more attention, particularly on the correction of search engine queries. Many research challenges are raised, which are non-existent in traditional settings of spelling correction. More specifically, there are many more types of spelling errors in search queries, such as misspelling, concatenation/splitting of query words, and misuse of legitimate yet inappropriate words. Research in this direction includes utilizing large web corpora and query log [4, 5, 2], training phrase-based error model from clickthrough data [10] and developing additional features [7]. However, two important challenges are under addressed in these approaches, i.e., correcting splitting and concatenation errors, and ensuring complete search in the candidate space to evaluate an effective scoring function.
Query alteration/refinement is a broader topic which naturally subsumes query spelling correction. Beside correcting the misspelled query, query alteration/refinement also need to modify the ineffective query so that it could be more suitable for the search intent. For this purpose, many research topics have been studied. Query expansion expands the query with additional terms to enrich the query formulation [14, 15, 16]. Query segmentation divides a query into semantically meaningful sub-units [17, 18]. Other query reformulation methods intend to replace the inappropriate query terms with effective keywords to bridge the vocabulary gaps [19]. Particularly, there is research attempt [24] to use a unified model to do a broad set of query refinements such as correction, segmentation and even stemming. However, it treats query correction and splitting/merging as separate tasks, which is not true for real world queries. Also, it has very limited ability for query correction. For example, it only allows one letter difference in deletion/insertion/substitution errors.
Query spelling correction also shares similarities with many other NLP tasks, such as speech recognition and machine translation. In many of these applications, HMM has been

612

found very useful [21, 20]. Our generalized HMM model for query spelling correction is novel in that it models all the major types of misspellings in a unified framework. A discrete training method is also proposed to train the parameters in the model. It is demonstrated that the gHMM model we use is very effective for the task of query spelling correction.
3. PROBLEM SETUP AND CHALLENGES
Formally, let  be the alphabet of a language and L  + be a large lexicon of the language. We define the query spelling correction problem as:
Given a query q  +, generate top-K most effective corrections Y = (y1, y2, ..., yk) where yi  L+ is a candidate correction, and Y is sorted according to the probability of yi being the correct spelling of the target query.
It is worth noting that from a search engine perspective, the ideal output Y should be sorted according to the probability of yi retrieving the most satisfying results in search. However, in practice it is very difficult to measure the satisfaction as unlike in ad hoc retrieval where the query is given in its correct form, here the real query is unknown. As a result, different corrections could simply lead to queries with different meanings and it would be very subjective to determine which query actually satisfies the user. In this paper, we are mostly concerned with the lexical and semantic correctness of queries with the assumption that correction of mis-spelled query terms most likely would lead to improved retrieval accuracy.
The problem of query spelling correction is significantly harder than the traditional spelling correction. Previous researches show that approximately 10-15% of search queries contain spelling errors [5]. First, it is difficult to cover all the different types of errors. The spelling errors generally fall into one of the following four categories: (1) in-word transformation, e.g. insertion, deletion, misspelling of characters. This type of error is most frequent in web queries, and it is not uncommon that up to 3 or 4 letters are misspelled; (2) mis-use of valid word, e.g. "persian golf"  "persian gulf". It is also a type of in-word tranformation errors; (3) concatenation of multiple words, e.g. "unitedstatesofamerica"  "united states of america"; (4) splitting a word into parts, e.g. "power point slides"  "powerpoint slides". Among all these types, the splitting and concatenation errors are especially challenging to correct. Indeed, no existing approaches in the academic literature can correct these two types of errors. Yet, it's important to correct all types of errors because users might commit different types of errors or even commit these errors at the same time. A main goal of this work is to develop a new HMM framework that can model and correct all major types of errors including splitting and concatenation.
Second, it is difficult to ensure complete search of all the candidate space because the candidate space is very large. The existing work addresses this challenge by using a twostage method, which searches for a small set of candidates with simple scoring functions and do re-ranking on top of these candidates. Unfortunately, the simple scoring function used in the first stage cannot ensure that the nominated candidate corrections in the first stage always contain the best correction, thus no matter how effective the final scoring function is, we may miss the best correction simply because of the use of two separate stages. In this paper, we address

this challenge by developing a generalized HMM that can both be efficiently scored to ensure complete search in the candidate space and accurately correct all types of errors in a unified way.
4. A GENERALIZED HMM FOR QUERY SPELLING CORRECTION
Our algorithm accepts a query as input, and then generates a small list of ranked corrections as output by a generalized Hidden Markov Model (gHMM). It is trained by a discriminative method with labeled spelling examples. Given a query, it scores candidate spelling corrections in a one-stage fashion and outputs the top-K corrections, without using a re-ranking strategy. Other components of our algorithm include a large clean lexicon, the error model and the language model. In this section we will focus on the gHMM model structure, the discriminative training of it, as well as the efficient computation of spelling corrections.
4.1 The gHMM Model Structure
We propose a generalized HMM Model to model the spelling correction problem. We call it a generalized HMM because there are several important differences between it and the standard HMM model which will be explained later. Without loss of generality, let an input query be q = q[1:n] and a corresponding correction be y = y[1:m] where n, m are the length of the query and correction, which might or might not be equal. Here we introduce hidden state sequence z = z[1:n] = (s1, s2, ..., sn) in which z and q have the same length. An individual state si is represented by a phrase corresponding to one or more terms in correction y[1:m]. Together the phrase representing z is equal to y. Therefore, finding best-K corrections Y = (y1, y2, ..., yk) is equivalent to finding best-K state sequences Z = (z1, z2, ..., zk). In addition, there is a type t associated with each state, indicating the operation such as substitution, splitting, merging etc. Also, in order to facilitate the merging state we introduce a NULL state. The NULL state is represented by an empty string, and it doesn't emit any phrase. There can be multiple consecutive NULL states followed by a merging state. Table 2 summarizes the state types and the spelling errors they correspond to. Having the hidden states defined, the hypothesized process of observing a mis-spelled query is as follows:
1. sample a state s1 and state type t1 from the state space  and the type set T ;
2. emit a word in q1, or empty string if the s1 is a NULL state according to the type specific error model;
3. transit to s2 with type t2 according to the state transition distribution, and emit another word, or multiple words in q[1:n] if s2 is a merging state;
4. continue until the whole (potentially) mis-spelled query q is observed.
Figure 1 illustrates our gHMM model with a concrete example. In this example, there are three potential errors with different error types, e.g. "goverment"  "government" (substitution), "home page"  "homepage" (splitting), "illinoisstate"  "illinois state" (concatenation). The state path shown in Figure 1 is one of the state sequences

613

Table 2: State Types in gHMM

State Type
In-word Transformation
Mis-use

Operation Deletion Insertion Substitution Transformation

Spelling Errors Insertion Deletion Substitution Word Mis-use

Merging

Merge Multiple Words

Splitting

Splitting

Split one Word

Concatenation

to Multiple Words

that can generate the query. Take state s3 for example, s3 is represented by phrase homepage. Since s3 is a merging state, it emits a phrase home page with probability P (home page|homepage). And s3 is transited from state s2 with probability P (s3|s2). With this model, we are able to come up with arbitrary corrections instead of limiting ourselves to an incomprehensive set of queries from query log. By simultaneously modeling the misspellings on word boundaries, we are able to correct the query in a more integrated manner.

query:

goverment

home

page

of

illinoisstate

emission: goverment

home page

of

illinoistate

state

sequence: government

NULL

homepage

of

illinois state

s1
type: in-word transformation

s2
type: NULL

s3
type: merging

s4
type: in-word transformation

s5
type: splitting

Figure 1: Illustration of the gHMM Model

4.2 Generalization of HMM Scoring Function

For a standard HMM [23], let  = {A, B, } be the model parameters of the HMM, representing the transition probability, emission probabilities and initial state probabilities respectively. Given a list of query words q[1:n] (obtained by splitting empty spaces), the state sequence z = (s1, s2, ..., sn) that best explains q[1:n] can be calculated by:

z

=

arg

max z

P (z|q[1:n],

A,

B,

)

(1)

However, theoretically the phrase in a state can be chosen arbitrarily, so estimating {A, B, } is such a large space is almost impossible in the standard HMM framework. In order to overcome this difficulty, the generalized Hidden Markov Model proposed in this work generalizes the standard HMM as follows: (1) gHMM introduces state type for each state, which indicates the correction operations and can reduce the search space effectively; (2) it adopts feature functions to parameterize the measurement of probability of a state sequence given a query. Such treatment can not only map the transition and emission probabilities to feature functions with a small set of parameters, but can also add additional feature functions such as the ones incorporating state type information. Another important benefit of the feature function representation is that we can use discriminative training

on the model with labeled spelling corrections, which will lead to a more accurate estimation of the parameters.
Formally, in our gHMM model, there is an one-to-one relationship between states in a state sequence and words in the original query. For a given query q = q[1:n] and the sequence of states z = (s1, s2, ..., sn), we define a context hi for every state in which an individual correction decision is made. The context is defined as hi =< si-1, ti-1, si, ti, q[1:n] > where si-1, ti-1, si, ti are the previous and current state and type decisions and q[1:n] are all query words.
The generalized HMM model measures the probability of a state sequence by defining feature vectors on the contextstate pairs. A feature vector is a function that maps a context-state pair to a d-dimensional vector. Each component of the feature vector is an arbitrary function operated on (h, z). Particularly, in this study we define 2 kinds of feature vectors, one is j(si-1, ti-1, si, ti), j = 1...d, which measures the interdependency of adjacent states. We can map this function to a kind of transition probability measurement. The other kind of feature function, fk(si, ti, q[1:n]), k = 1...d measures the dependency of the state and its observation. We can consider it as a kind of emission probability in the standard HMM point of view. Such feature vector representation of HMM is introduced by Collins [22] and successfully applied to the POS tagging problem.
Specifically, we have designed several feature functions as follows: we define a function of (si-1, ti-1, si, ti) as

1(si-1, ti-1, si, ti) = logPLM (si|si-1, ti-1, ti) (2)
to measure the language model probabilities of two consecutive states. Where PLM (si|si-1) is the bigram probability calculated by using Microsoft Web N-gram Service [1]. The computation of PLM (si|si-1) may depend on the state types, such as in a merging state.

We have also defined a set of functions in the form of fk(si, ti, q[1:n]), which are dependent on the query words and state type, measuring the emission probability of a state. For example, we define

8

<logPerr(si, qi)

f1(si, ti, q[1:n]) = :

0

if qi is in-word transformed to si and qi / Lexicon L otherwise
(3)

as a function measuring the emission probability given the state type is in-word transformation and qi is out of dictionary. e.g. "goverment"  "government". Perr(si, qi) is the emission probability computed by an error model which measures the probability of mis-typing "government" to "goverment". (see Section 5.2).

8

<logPerr(si, qi)

f2(si, ti, q[1:n]) = :

0

if ti is splitting and qi  Lexicon L
otherwise
(4)

to capture the emission probability if the state is of splitting type and qi is in dictionary. e.g. "homepage"  "home page".

614

8 <logPerr(s, qi)

f3(si, ti, q[1:n]) = :

0

if ti is Mis-use and qi  Lexicon L
otherwise
(5)

to get the emission probability if a valid word is transformed to another valid word.
Note that in Equation (3), (4), and (5), we use the same error model Perr(si, qi) (see Section 5.2 for detail) to model the emission probabilities from merging, splitting errors etc. in the same way as in-word transformation errors. However we assign different weights to the transformation probabilities resulted from different error types via discriminative training on a set of labeled query-correction pairs.
Overall, we have designed a set of feature functions that are all relied on local dependencies, ensuring that the topK state sequences can be computed efficiently by Dynamic Programming.
After establishing the feature vector representation, the log-probability of a state sequence and its corresponding types logP (z, t|q[1:n]) is proportional to:

X n Xd

Score(z, t) =

j j (si-1, ti-1, si, ti)

(6)

i=1 j=1

X n X d

+

µkfk(si, ti, q[1:n])

i=1 k=1

where j , µk are the component coefficients needed to be estimated. And the best state sequence can be found by:

zt = arg max Score(z, t)

(7)

z,t

Note that the form of Score(z, t) is similar to the objective function of a Conditional Random Field model [26], but with an important difference that there is no normalization terms in our model. Such difference also enables the efficient search of top-K state sequences (equivalent to top-K corrections) using Dynamic Programming, which will be introduced shortly.
4.3 Discriminative Training
Motivated by ideas introduced in [22], we propose a perceptron algorithm to train the gHMM model. To the best of our knowledge, this is the first attempt to use discriminative approach to train a HMM on the problem of query spelling correction. Now we describe how to estimate the parameters j , µk from a set of <query, spelling correction> pairs. The estimation procedure follows the perceptron learning framework. Take the j for example. We first set all the j at random. For each query q, we search for the most likely state sequence with types z[i1:ni], ti[1:ni] using the current parameter settings. Such search process is described in Algorithm 2 by setting K = 1. After that, if the best decoded sequence is not correct, we update j by simple addition: we promote the amount of j by adding up j values computed between the query and labeled correction y , and demote the amount of j by the sum of all j values computed between the

query and the top-ranked predictions. We repeat this pro-
cess for several iterations until converge. Finally in step 11 and 12, we average all oj,i in each iteration to get the final estimate of j, where oj,i is the stored value for the parameter j after i's training example is processed in iteration
o. Similar procedures can apply to µk. The detailed steps
are listed in Algorithm 1. Note that in step 7 and 8 the feature functions j(qi, y i, t i) and fk(qi, y i, t i) depend on unknown types t i that are inferred by computing the best word-level alignment between qi and y i.

Algorithm 1: Discriminative Training of gHMM

input : A set of <query, spelling correction> pairs q[i1:ni], y[1i:mi] for i = 1...n
output: Optimal estimate of ^j, µ^k, where
j  {1, ..., d}, k  {1, ..., d }

1 Init Set ^j, µ^k to random numbers; 2 for o  1 to O do

3 for i  1 to n do

/* identify the best state sequence and the

associated types of the i'th query with the

current parameters via Algorithm 2:

*/

4

z[i1:ni], ti[1:ni] = arg maxu[1:ni],t[1:ni] Score(u, t)

/* where u[1:ni]  Sni , Sni is all possible state

sequences given q[i1:ni]

*/

5

if z[i1:ni] = y[1i:mi] then

6

update and store every j, µk according to:

7 8

j µk

= =

µjk++PPnini==ii11

j (qi, fk (q i ,

y y

i, i,

t t

ii))--P Pnini==ii 11

j (qi, fk (q i ,

zi, zi,

ti ti

) )

9

else

10

Do nothing

/* Average the final parameters by:

*/

11 12

^j µ^k

= =

PO PoO=1
o=1

Pn Pin=1
i=1

oj ,i /nO, µok,i/nO,

where where

j k

 

{1, ..., d} {1, ..., d }

13 return parameters ^j, µ^k;

This discriminative training algorithm will converge after several iterations. Here we state the following theorem on its convergence:

Theorem 1: For any training example (qi, y i) which is separable with margin , then for Algorithm 1:

Number

of

mistakes



R2 2

where R is a constant satisfying

i, z  {G(qi) - {y i}} : ||(qi, y i, t i) - (qi, zi, ti)|| R,

where G(qi) is a set of predictions generated by Algorithm 2, and the  functions can be calculated from Eq. (6) by ignoring  and µ. The proof of Theorem 1 is not shown due to the space limitation.

4.4 Query Correction Computation
Once the optimal parameters are obtained by the discriminative training procedure introduced above, the final top-K corrections can be directly computed, avoiding the need for a

615

separate stage of candidate re-ranking. Because the feature functions are only relied on local dependencies, it enables the efficient search of top-K corrections via Dynamic Programming. This procedure involves three major steps: (1) candidate states generation; (2) score function evaluation; (3) filtering.
At the first step, for each word in query q, we generate a set of state candidates with types. The phrase representations in such states are in Lexicon L and within editing distance  from the query word. Then a set of state sequences are created by combining these states. In addition, for each state sequence we have created, we also create another state sequence by adding a NULL state at the end, facilitating a (potential) following merging state. It is important to note that if the  is too small, it will compromise the final results due to the premature pruning of state sequences. In this work  = 3 is chosen in order to introduce adequate possible state sequences.
At the score function evaluation step, we update the scores for each state sequence according to Eq. (6). The evaluation is different for sequence with different ending state types. Firstly, for a sequence ending with a NULL state, we don't evaluate the scoring function. Instead, we only need to keep track of the state representation of its previous state. Secondly, for a sequence ending with a merging state, it merges the previous one or more consecutive NULL states. And the scoring function takes into account the information stored in the previous NULL states. For instance, to 1(si-1, ti-1 = N U LL, si, ti = merging), we have

1(si-1, NULL, si, merging) = logPLM (si-2|si) (8)

i.e. skipping the NULL state and pass the previous state representation to the merging state. In this way, we can evaluate the scoring function in multiple consecutive NULL states followed by a merging state, which enables the correction by merging multiple query words. Thirdly, for a sequence ending with a splitting state, the score is accumulated by all bigrams within the splitting state. For example,

1(si-1, ti-1, si, ti = splitting)

(9)

kX -1 = logPLM (w1|si-1) + logPLM (wi+1|wi)

j=1

where si = w1w2...wk. On the other hand, the evaluation of fk(si, ti, q[1:n]) is easier because it is not related to previous states. The error model from the state representation to the query word is used to calculate these functions.
At the final step, we filter most of the state sequences and only keep top-K best state sequences in each position corresponding to each query word. In sum, we have proposed and implemented an algorithm via Dynamic Programming (see Algorithm 2) for efficiently computing top-K state sequences (corrections). If there are n words in a query, and the maximum number of candidate states for each query word is M , the computational complexity for finding top-K corrections is O(n ∑ K ∑ M 2).

5. OTHER COMPONENTS
Besides the gHMM model and its discriminative training, our spelling correction algorithm also relies on other important components, such as a large trusted lexicon, the error model, and the language model. In this section we will describe these components.

Algorithm 2: Decoding Top-K Corrections

input : A query q[1:n], parameters , µ output: top K state sequences with highest likelihood

/* Z[i, si]: top K state sequences for sub-query q[1:i] that ending with state si. For each z  Z[i, si], phrase denotes the representation and score

denotes the likelihood of z given q[1:i].

*/

/* Z[i]: top state sequences for all Z[i, si].

*/

1 Init Z[0] = {}

2 for i  1 to n do

/* for term qi, get all candidate states

*/

3 S  si, si : edit dist(si, qi)  , si has type si.type

4 for si  S do

5

for z  Z[i - 1] do

6

a  new state sequence

7

a.phrase  z.phrase  {si}

8

update a.score according to si.type and Eq.

(6), Eq. (8) and Eq. (9)

9

Z[i, si]  a

/* delay truncation for N U LL states

*/

10

if si.type = N U LL and i = n then

11

sort Z[i, si] by score

12

truncate Z[i, si] to size K

13 sort Z[n] by score 14 truncate Z[n] to size K 15 return Z[n];

5.1 A Large-scale Trusted Lexicon
As mentioned in Section 3, how to define a proper lexicon L is important to a successful speller. We find that with a clean vocabulary, it will significantly improve the performance of spelling correction. However, to obtain such a clean vocabulary is usually difficult in practice. To do this, we make use of the Wikipedia data. Particularly, we select the top 2 million words from Wikipedia by their word frequencies, and automatically curate the obtained words by removing those frequent but illegitimate words from the vocabulary. This curate process involves checking if the word appears in the title of a Wikipedia article, comparing the bigram probability of other words etc. Finally we obtained 1.2 million highly reliable words in the vocabulary. Please note that all words in this vocabulary are unigrams; numbers and punctuations are not included. And no stemming and other forms of modification are conducted.
5.2 Error Model Training
Error Model. The feature functions fk() depend on an error model, which measures the probability that one word is misspelled into another. Previous studies have shown that a weighted editing distance model trained with a sufficient large set of correction pairs could achieve a comparable performance with a sophisticated n-gram model [6]. Meanwhile, a higher order model has greater tendency to overfit if the training data is not large enough. Given these considerations, we adopt the Weighted Edit Distance (WED) to estimate the error model. More specifically, we first compute the best character-level alignment of two words, and the WED between these two words is the summation of the WED of all aligned character pairs. We model the character-level WED

616

as the character transformation probability. In addition, null character is included in the vocabulary to accomodate the insertion and deletion operations. In order to compute such a probability, a large set of query-correction pairs is obtained by leveraging the spelling services from Google and Bing; and then this probability is estimated as the expected number of transformation from one character to another in these aligned pairs. The training queries are from the MSN search query log released by the Microsoft Live Labs in 2006 (6.5 million queries). They are submitted to the spelling ser- vices, and the corrections are recorded once consensus is reached.
5.3 Use of Web N-gram Model
Another important factor in selecting and ranking the correction candidates is the prior probability of a correction phrase. It represents our prior belief about how likely a query will be chosen by the user without seeing any input from the user. In this work we make use of the Web n-gram service provided by Microsoft [1]. Web n-gram model intends to model the n-gram probability of English phrases with the parameters estimated from the entire Web data. It also differentiates the sources of the data to build different language models from the title, anchor text and body of Web pages, as well as the queries from query log. In our study, we find the title model is the most effective for query spelling correction. We hypothesize that this may be because the training data for query model is much noisier. Particularly, misspelled queries may be included in the training data, which makes it less effective for the task of spelling correction. Despite trained with the Web data, Web n-gram model may also suffer from data sparseness in higher order models. To avoid this issue, we make use of the bigram model in building our spelling system.
6. EXPERIMENTS AND RESULTS
In order to test the effectiveness and efficiency of our proposed gHMM model, in this section we conduct extensive experiments on two web query spelling datasets. We first introduce the datasets, and describe the evaluation metrics we use for evaluation. Then we compare our model with other baselines in terms of accuracy and runtime.
6.1 Dataset Preparation
The experiments are conducted on two query spelling correction datasets. One is the TREC dataset based on the publicly available TREC queries (2008 Million Query Track). This dataset contains 5892 queries and the corresponding corrections annotated by the MSR Speller Challenge 1 organizers. There could be more than one plausible corrections for a query. In this dataset only 5.3% of queries are judged as misspelled.
We have also annotated another dataset that contains 4926 MSN queries, where for each query there is at most one correction. Three experts are involved in the annotation process. For each query, we consult the speller from two major search engines (i.e. Google and Bing). If they agree on the returned results (including the case if the query is just unchanged), we take it as the corrected form of the input query. If the results are not the same from the two, as least one human expert will manually annotate the most likely
1http://web-ngram.research.microsoft.com/spellerchallenge/

corrected form of the query. Finally, about 13% of queries are judged as misspelled in this dataset, which is close to the error rate of real web queries. This dataset is publicly available to all researchers.
We divide the TREC and MSN datasets into training and test sets evenly. Our gHMM model as well as the baselines are trained on the training sets and finally evaluated on the TREC test set containing 2947 queries and MSN test set containing 2421 queries.

6.2 Evaluation Metrics

We evaluate our system based on the evaluation metrics
proposed in Microsoft Speller Challenge [1], including ex-
pected precision, expected recall and expected F1 measure.
As used in previous discussions, q is a user query and Y (q) = (y1, y2, , yk) is the set of system output with posterior probabilities P (yi|q). Let S(q) denote the set of plausi-
ble spelling variations annotated by the human experts for
q. Expected Precision is computed as:

precision = 1 X |Q|

X

Ip(y, q)P (y|q)

(10)

qQ yY (q)

where Ip(y, q) = 1 if y  S(q), and 0 otherwise. And expected recall is defined as:

recall = 1 X |Q|

X

Ir(Y (q), a)/|S(q)|

(11)

qQ aS(q)

where Ir(Y (q), a) = 1 if a  Y (q) for a  S(q), and 0 otherwise. Expected F1 measure can be computed as:

F1

=

2 ∑ precision ∑ recall precision + recall

(12)

6.3 Overall Effectiveness
We first investigate the overall effectiveness of the gHMM model. For suitable query spelling correction baselines, especially approaches that can handle all types of query spelling errors, we first considered using the CRF model proposed in [24]. This method aims at a broad range of query refinements and hence might be also applicable to query correction. However, we decided not to compare this model for the following reasons. Firstly, we communicated with the authors of [24] and knew that the program is un-reusable. Secondly, as mentioned in Section 1 this work suffers from several drawbacks for query spelling correction: (1) it is unable to correct queries with mixed types of errors, such as substitution and splitting errors in one query, because the model treats query correction and splitting/merging as different tasks; (2) this model only allows 1 character error for substitution/insertion/deletion. And the error model is trained on the <query, correction> examples that only contain 1 character error. Such design is over simplified for real-world queries, in which more than 1 character errors are quite common. In fact, within the queries that contain spelling errors in the MSN dataset, there are about 40.6% of them contain more than 1 character errors. Therefore it is expected model in [24] will have in inferior performance
Because of the reasons stated above, the best baseline method that we can possibly compare with is the system

617

that achieved the best performance in Microsoft Speller Challenge [25] (we call it Lueck-2011). This system relies on candidate corrections from third-party toolkits such as hunspell and Microsoft Wrod Breaker Service [11] , and it re-ranks the candidates by a simple noisy channel model. We communicated with the author and obtained the corrections by running the Web API of this baseline approach. We also include a simple baseline called Echo, which is just echoing the original query as the correction response with posterior probability 1. It reflects the basic performance for a naive method. Experiments are conducted on TREC and MSN datasets.
We report the results of all methods in Table 3. In this experiment up to top 10 corrections are used in all approaches. The results in Table 3 indicate that gHMM outperforms Lueck-2011 significantly on recall and F1 on the TREC dataset. Lueck-2011 has a small advantage on precision, possibly due to the better handling the unchanged queries. On the MSN dataset which is considered harder since it has more misspelled queries, gHMM also achieves high precision of 0.910 and recall of 0.966, which are both significantly better than that of the Lueck-2011 (0.896 and 0.921). On another important performance metric, which measures the F1 on misspelled queries (F1 Mis), gHMM outperforms Lueck-2011 by a large margin (0.550 vs. 0.391 on TREC and 0.566 vs. 0.363 on MSN). These results demonstrate that gHMM is very effective for handling all types of spelling errors in search queries overall.

Table 3: gHMM Compared to Baselines

Dataset Method Precision Recall F1

F1

Mis

TREC

Echo

0.949

Lueck-2011 0.963

gHMM

0.960

0.876 0.911 N/A 0.932 0.947 0.391 0.976 0.968 0.550

MSN

Echo

0.869

Lueck-2011 0.896

gHMM

0.910

0.869 0.869 N/A 0.921 0.908 0.363 0.966 0.937 0.566

6.4 Results by Error Types
Further, we also break down the results by error types that are manually classified so that we can see more clearly the distribution of types of spelling errors and how well our gHMM model addressing each type of errors. We present the results of this analysis in Table 4, only with our model on both datasets. Top 40 corrections are used since it achieves the best results. The breakdown results show that most queries are in the group of "no error", which are easier to handle than the other three types. As a result, the overall excellent performance was mostly because the system performed very well on the "no error" group. Indeed, the system has substantially lower precision on the queries with the other three types of errors. The concatenation errors seem to be the hardest to correct, followed by the splitting errors, and the in-word transformation errors (insertion, deletion and substitution, word mis-use) seem to be relatively easier.
6.5 gHMM for Working Set Construction
Since the gHMM can efficiently search in the complete

Table 4: Results by Spelling Error Type

Dataset Error Type % Queries Precision Recall

TREC

no error transformation concatenation splitting

94.9 3.3 1.3 0.5

0.990 0.388 0.348 0.500

0.982 0.840 0.877 0.792

MSN

no error transformation concatenation splitting

86.9 11.1 1.7 0.6

0.978 0.493 0.150 0.429

1.0 0.762 0.600 0.571

F1 0.986 0.531 0.498 0.613 0.989 0.599 0.240 0.490

Note: % of queries might sum up to more than 100% since there might be multiple types of errors in one query.

candidate space and compute the top-K spelling corrections in a one-stage manner, it is very interesting to test its effectiveness for constructing a working set of candidate corrections to enable more complex scoring functions to be used for spelling correction. For this purpose, we compare gHMM with the common used noisy channel model, whose parameters, namely error model probabilities and bigram language probabilities are estimated by the procedure mentioned in previous sections. We use recall to measure the completeness of the constructed working set, because it represents the percentage of true corrections given the number of predicted corrections. Table 5 shows the recall according to different number of outputs. It indicates that the recall of gHMM is steadily increasing by a larger number of outputs. By only outputting top-5 corrections, gHMM reaches recall of 0.969 in TREC and 0.964 in MSN. In contrast, the noisy channel model has a substantial gap in term of recall compared to gHMM. This result strongly demonstrates the superior effectiveness of gHMM in constructing a more complete working set of candidate corrections, which can be utilized by other re-ranking approaches which could further improve the correction accuracy.

Table 5: gHMM vs. Noisy Channel Model on Recall

Dataset TREC
MSN

Method N-C gHMM N-C gHMM

1 0.869 0.887 0.866 0.920

5 0.896 0.969 0.870 0.964

10 0.899 0.976 0.873 0.966

20 0.901 0.981 0.876 0.9667

40 0.902 0.983 0.886 0.967

Note: N-C refers to the noisy channel model

6.6 Efficiency
The runtime requirement of query correction is very stringent. Theoretically, the gHMM with local feature functions can search top-K corrections efficiently by our proposed tokK Viterbi algorithm. Here we make a directly comparison between the runtime of gHMM and a basic noisy channel that only needs to compute the error model probabilities and bigram language probabilities. Such a basic model is also implemented with Viterbi algorithm. It is run on a Windows server equipped with 2 Quad-core 64 bit 2.4 GHz CPUs and 8 GB RAM. All necessary bigram language probabilities are crawled from Microsoft Web N-gram Service and cached in local memory. We plot the runtime per query (in milliseconds) according to the number of predicted correc-

618

tions in Figure 2. According to Figure 2, the computation of top-1 correction by gHMM is fast (34 ms) if the number of output is set to 1. It increases as the number of output increases because the search space is increased. Interestingly, the runtime of gHMM and the noisy channel model is of the same order of magnitude. This empirical evidence confirms the theoretical result that top-K spelling corrections can be computed efficiently via our proposed top-K Viterbi algorithm.

increases as the lexicon size increases. However the recall is not sensitive to the lexicon size.

Figure 4: Correction Results by Lexicion Size

Figure 2: Runtime Comparison
7. DISCUSSION
Finally we discuss how the proposed gHMM model behaves according to the variation of important factors. Such as the number of spelling corrections in the output and the size of lexicon L etc.
7.1 Number of Predicted Corrections
Our datasets include all plausible corrections for a query. However it's unknown that how many corrections an algorithm should output to achieve a perfect recall. In this section we investigate how the number of predicted spelling corrections affects the precision and recall. We have carried out experiments on five correction sizes (1,5,10,20,40) on both datasets. Figure 3 summarizes the results. As expected, a bigger number of corrections leads to higher recall, and the most drastical increase of recall lies from 1 correction to 5.
Figure 3: Results by Number of Corrections
7.2 Effect of the Lexicon Size
The size of the trusted lexicon is an important factor influencing the speller correction result. In order to investigate the effect of lexicon size, we conducted a set of experiments on both datasets using different lexicon sizes (ranging from 100,000 to 900,000). Results in Figure 4 show that the effect of lexicon size is significant on precision: the precision

7.3 Clean vs. Noisy Lexicon
In Table 6, we show the effects of using a clean lexicon for improving the precision and recall of the spelling system. We can see that for both test sets, there is noticeable improvement in precision. By removing the noise in the automatically constructed lexicon, our system is able to find matches for candidate queries more precisely.
Table 6: Clean vs. Noisy Lexicon Dataset Lexicon Type Precision Recall F1 TREC clean lexicon 0.960 0.983 0.971
noisy lexicon 0.950 0.984 0.966 MSN clean lexicon 0.910 0.967 0.938
noisy lexicon 0.896 0.967 0.930
Note: maximum corrections are set to 40 in this experiment
8. CONCLUSIONS AND FUTURE WORKS
In this paper, we presented a novel generalized hidden Markov model (gHMM) for query spelling correction that can address two major deficiencies of previous approaches to query spelling correction, i.e., inability of handling all the major types of spelling errors, and inability of searching efficiently in the complete candidate space. We have also proposed a novel discriminative training method for the gHMM model which enables us to go beyond regular HMM to incorporate useful local features for more effective query spelling correction. Experiment results on two query correction datasets show that gHMM can effectively handle all the major types of spelling errors and outperforms a stateof-the-art baseline by a large margin.
Moreover, our generalized HMM, equipped with the discriminative training, scores the query corrections directly and output a final ranked list of spelling corrections, without needing a filtering stage to prune the candidate space as typically required by an existing method. We have demonstrated that as an efficient one-stage approach, the proposed gHMM can also be used as a filter to construct a more complete working set than the existing noisy channel filter, making it possible to combine it with any complicated spelling correction methods to further improve accuracy. In other words, the proposed gHMM model can serve as a better candidate generation method in a two-stage framework where any sophisticated and potentially more effective spelling correction method can be applied to re-rank the generated candidates

619

for more accurate corrections. In addition, we have also curated a large spelling correction dataset from real-world queries and made it available to the research community.
In this work, we only focused on local feature functions in order to ensure efficient evaluation of all the candidates in the search space. However, some global features, such as the overall editing distance, frequency of the query in a query log can be potentially utilized to further improve the correction accuracy. How to add the global features to the gHMM model, while still ensuring efficient search and evaluation of all the candidates in the search space, is an interesting direction for future work.
9. ACKNOWLEDGMENTS
This paper is based upon work supported in part by a Microsoft grant, the National Science Foundation under grant CNS-1027965, and MIAS - the Multimodal Information Access and Synthesis center at UIUC, part of CCICADA, and a DHS Center of Excellence.
10. REFERENCES
[1] http://research.microsoft.com/enus/collaboration/focus/cs/web-ngram.aspx.
[2] F. Ahmad and G. Kondrak. Learning a spelling error model from search query logs. In HLT/EMNLP. The Association for Computational Linguistics, 2005.
[3] E. Brill and R. Moore. An improved error model for noisy channel spelling correction. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong, 2000.
[4] Q. Chen, M. Li, and M. Zhou. Improving query spelling correction using web search results. In EMNLP-CoNLL, pages 181≠189. ACL, 2007.
[5] S. Cucerzan and E. Brill. Spelling correction as an iterative process that exploits the collective knowledge of web users. In EMNLP, 2004.
[6] H. Duan and B.-J. P. Hsu. Online spelling correction for query completion. In Proceedings of the 20th international conference on World wide web, WWW '11, pages 117≠126. 2011, ACM.
[7] J. Gao, X. Li, D. Micol, C. Quirk, and X. Sun. A large scale ranker-based system for search query spelling correction. In C.-R. Huang and D. Jurafsky, editors, COLING, pages 358≠366. 2010.
[8] K. Kukich. Techniques for automatically correcting words in text. ACM computing surveys, 24(4), 1992.
[9] M. J. D. Powell. An efficient method for finding the minimum of a function of several variables without calculating derivatives. The Computer Journal, 7(2):155≠162, 1964.
[10] X. Sun, J. Gao, D. Micol, and C. Quirk. Learning phrase-based spelling error models from clickthrough data. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL '10, pages 266≠274, Stroudsburg, PA, USA, 2010.
[11] K. Wang, C. Thrasher, and B.-J. P. Hsu. Web scale nlp: a case study on url word breaking. In Proceedings of the 20th international conference on World wide web, WWW '11, pages 357≠366, New York, NY, USA, 2011. ACM.
[12] C. Whitelaw, B. Hutchinson, G. Chung, and G. Ellis.

Using the web for language independent spellchecking and autocorrection. In EMNLP, 890≠899. ACL, 2009.
[13] Levenshtein, V I. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet Physics Doklady, 10(8), 707-710, 1966.
[14] Jinxi Xu and W. Bruce Croft. Query expansion using local and global document analysis. In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '96. ACM, New York, NY.
[15] Yonggang Qiu and Hans-Peter Frei. Concept based query expansion. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '93. ACM, New York, NY, USA, 160-169.
[16] Mandar Mitra, Amit Singhal, and Chris Buckley. Improving automatic query expansion. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '98.
[17] B. Tan and F. Peng. Unsupervised query segmentation using generative language models and wikipedia. In Proceeding of the 17th international conference on World Wide Web, WWW '08, pages 347≠356. 2008.
[18] Y. Li, B.-J. P. Hsu, C. Zhai, and K. Wang. Unsupervised query segmentation using clickthrough for information retrieval. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR '11, pages 285≠294. 2011.
[19] Xuanhui Wang, ChengXiang Zhai. Mining Term Association Patterns from Search Logs for Effective Query Reformulation. In CIKM'08. 479-488.
[20] Stephan Vogel, Hermann Ney, and Christoph Tillmann. HMM-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics, Volume 2 (COLING '96). Stroudsburg, PA, USA, 836-841.
[21] B.H. Juang. Hidden Markov models for speech recognition. In Technometrics, Vol. 33, No. 3, 1991.
[22] M. Collins. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In EMNLP '02, pages 1-8, Stroudsburg, PA, USA, 2002.
[23] L. R. Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings of the IEEE, pages 257≠286, 1989.
[24] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and discriminative model for query refinement. In Proceedings of the 31st annual international ACM SIGIR, SIGIR '08, pages 379≠386. 2008.
[25] G. Luec. A data-driven approach for correcting search quaries. In Spelling Alteration for Web Search Workshop, July 2011.
[26] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282≠289, San Fransisco, 2001. Morgan Kaufmann.

620

Learning to Predict Response Times for Online Query Scheduling

Craig Macdonald1, Nicola Tonellotto2, Iadh Ounis1
1 University of Glasgow, Glasgow G12 8QQ, UK 2 National Research Council of Italy, Via G. Moruzzi 1, 56124 Pisa, Italy
{craig.macdonald, iadh.ounis}@glasgow.ac.uk1, {nicola.tonellotto}@isti.cnr.it2

ABSTRACT
Dynamic pruning strategies permit efficient retrieval by not fully scoring all postings of the documents matching a query ≠ without degrading the retrieval effectiveness of the topranked results. However, the amount of pruning achievable for a query can vary, resulting in queries taking different amounts of time to execute. Knowing in advance the execution time of queries would permit the exploitation of online algorithms to schedule queries across replicated servers in order to minimise the average query waiting and completion times. In this work, we investigate the impact of dynamic pruning strategies on query response times, and propose a framework for predicting the efficiency of a query. Within this framework, we analyse the accuracy of several query efficiency predictors across 10,000 queries submitted to in-memory inverted indices of a 50-million-document Web crawl. Our results show that combining multiple efficiency predictors with regression can accurately predict the response time of a query before it is executed. Moreover, using the efficiency predictors to facilitate online scheduling algorithms can result in a 22% reduction in the mean waiting time experienced by queries before execution, and a 7% reduction in the mean completion time experienced by users.
Categories and Subject Descriptors: H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval
Keywords: Dynamic Pruning, Query Efficiency Prediction
1. INTRODUCTION
Large-scale information retrieval (IR) systems ≠ such as Web search engines ≠ are not just concerned with the quality of search results (also known as effectiveness), but also with the speed with which the results are obtained (efficiency). These aspects form a natural tradeoff, in that many approaches that increase effectiveness may have a corresponding impact on efficiency due to their complex nature [31].
Hence, as users exhibit preferences for faster search engines [6], to deploy techniques to improve effectiveness, search engines need to identify other opportunities for efficiency optimisations. One such technique is the use of caching, either of the search results for a query, or the posting lists of terms from the inverted index [3]. Increasingly, caching is being used for the posting lists of all terms, such that retrieval
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

occurs from in-memory indices, and thus all expensive disk I/O operations are avoided [12, 15].
The exhaustive scoring of every document that contains at least one query term also degrades efficiency unnecessarily, because very few of these documents will make the top retrieved set of documents that the user will see. Dynamic pruning strategies such as MaxScore [30] and Wand [5] address this by omitting (pruning) the scoring of documents that cannot make the top-K retrieved documents set.
Of course, more than one machine is typically involved in answering a query to a Web search engine [7, 12]. For instance, more than one query server can service queries for a replicated index, thereby improving overall response time. However, no previous work has examined the most effective way to schedule queries across the query servers. In this paper, we argue that scheduling algorithms that take into account the queued workload for that server can be used to ensure efficient use of query servers resources.
However, accurate scheduling requires estimations of the execution times of queries. Yet, as we will show in this paper, one query may take substantially longer than another query with apparently similar statistics. Indeed, we explain why some queries are difficult to prune, whereby many documents are fully scored before later being expelled from the retrieved set by other documents. Hence, the difficulty is determined by how early the final top documents are retrieved when traversing the posting lists, to allow successful pruning. Accurately estimating this difficulty and hence the response time of a query will permit the efficient scheduling of queries in a replicated setting.
In this paper we propose a framework of query efficiency predictors that can estimate the execution time of a given query. Experiments using 10,000 queries from a Web search engine query log demonstrate the accuracy of these predictors. Moreover, we show how query efficiency prediction can be used to reduce query response time by the improved scheduling of queries across replicated query servers. Indeed, scheduling while making use of efficiency predictions can result in a 22% reduction in the average waiting time experienced by queries, and a 7% reduction in the average query completion time experienced by users.
The remainder of this paper is as follows: Section 2 introduces background material on efficient search engines; Section 3 details the contributions of this work; Section 4 contains the experimental setup common to all experiments; Section 5 examines the response times of various retrieval strategies; Section 6 defines our query efficiency prediction framework; Section 7 shows how query efficiency predictors can be used to improve the scheduling of queries to replicated query servers; Concluding remarks follow in Section 8.

621

2. BACKGROUND
To spread the retrieval load of a large document corpus on a search engine, the inverted index can be partitioned into shards and distributed across multiple query servers, and/or identical indices replicated across query servers [12]. For distributed settings, queries arriving at the front-end server or broker are sent to multiple query servers, while the broker collates the results. For replicated settings, queries arriving at the broker are routed to the next available query server [7]. However, we know of no work that discusses different approaches for the routing or scheduling of queries across replicated query servers. Indeed, to the best of our knowledge, this is the first work addressing the scheduling of queries across replicated query servers.
Next, while the inverted index was traditionally stored on disk, with the predominance of inexpensive memory, search engines are increasingly caching the entire inverted index in memory, to assure low latency responses [12, 15].
Despite these optimisations, the matching of documents still uses the classical inverted index data structure to score and rank documents [12]. Moreover, due to its data bound nature, this represents the largest contribution to the time for a search engine to retrieve documents in response to a query. Indeed, to create a ranking of documents for a query, the posting lists for each query term must be traversed [23].
Various techniques that prune documents that are unlikely to be retrieved have been devised to improve efficiency. Some rely on pre-sorting the posting lists of different terms by the impact of the postings [2], or by removing documents that are unlikely to be retrieved (with possible loss of retrieval effectiveness) [4]. However, we focus on techniques that are safe-to-rank-K ≠ i.e. cannot degrade retrieval effectiveness to a given rank K ≠ and use docid sorted posting lists, as deployed by at least one major search engine [12]. In particular, dynamic pruning strategies aim to avoid the scoring of postings for documents that cannot make the top K retrieved set. All state-of-the-art safe dynamic pruning strategies [5, 28, 30]1 aim to avoid scoring parts of the posting lists, to save disk access, decompression and score computation costs. This is implemented by maintaining additional information during retrieval, namely: a threshold  , which is the minimum score that documents must achieve to have a chance to be present in the final top K results; and for each query term, a term upper bound (t), which is the maximal contribution of that particular term to any document score. The upper bound is usually obtained by pre-scanning the posting lists of each term at indexing time, to record the maximum score found in each posting list [10].
We use two such dynamic pruning strategies that score postings in a document-at-a-time (DAAT) manner, namely MaxScore [30] and Wand [5]. Compared to the scoring of every posting of every document (which we refer to as Full), both strategies can efficiently score documents while beingsafe-to-rank K, and do not require impact-sorted posting lists. Instead, the postings for every term have the same global ordering, such that they can be read in parallel.
MaxScore achieves efficient retrieval by omitting the scoring of postings for documents that will not make the final retrieved set. In contrast, Wand takes a different approach, by repeatedly calculating a pivot term. The next document
1We omit the database-focused algorithms of Fagin et al. [14] - these assume random access on the posting list, which is generally not possible when compression is used.

containing the pivot term is the pivot document, which will be the next document to be fully scored. A major benefit of Wand over MaxScore is that skipping [24] forward in posting lists can be used by Wand, which reduces posting list decompression overheads, and can reduce IO, with resulting improvements in efficiency [15, 22]. Indeed, Wand represents the state-of-the-art in dynamic pruning. Moreover, both MaxScore and Wand can equally be applied to inverted indices stored on disk or in-memory [15].
Moffat et al. [23] stated that the response time of a query is related to the posting list lengths of its constituent query terms. However, as we will show in Section 5, dynamic pruning causes queries with similar surface properties (e.g. query terms, posting list lengths) to widely vary in the time required to process. Hence, to have a better estimation of a query's response time, we propose a novel framework for query efficiency prediction. Moreover, we investigate the advantages of query efficiency prediction when applied to improved online scheduling of queries to replicated query servers. The notion of query efficiency prediction first appeared in [29], where initial experiments for disk-based indices showed some promise. In contrast, this work performs a more detailed study into efficiency prediction for inmemory indices, and uses efficiency prediction for enhancing replicated retrieval architectures.
We contrast query efficiency prediction with works from the literature on query performance prediction that have tried to predict the effectiveness of a query [17], either before retrieval commences (pre-retrieval prediction) [18], or by inspecting the scores or contents of retrieved documents (post-retrieval prediction) [1, 11]. Performance predictors have also been combined using machine learning [19, 21]. However, no previous work has attempted the different task of efficiency prediction. This work defines pre-retrieval efficiency predictors, and applies these to the scheduling of queries in a replicated retrieval setting.
3. CONTRIBUTIONS
The major contributions of this paper are:
∑ We demonstrate and explain the varying nature of response times for dynamic pruning strategies with in-memory indices.
∑ We propose a framework for query efficiency prediction, and instantiate various predictors within that framework.
∑ We conduct experiments to determine the accuracy of the query efficiency predictors for retrieval with in-memory indices.
∑ We propose the use of online scheduling algorithms to reduce the overall query response time when routing queries to replicated query servers.
∑ We show how query efficiency predictors can be used within online scheduling algorithms.
In the following, we firstly introduce the experimental setup deployed in this paper (Section 4), before experimentally investigating the pruning behaviour of dynamic pruning strategies (Section 5) and then defining and evaluating the query efficiency predictors (Section 6). The proposed query efficiency predictors are applied to online query scheduling in Section 7.

622

Length 1

2

3

4

5 6+ All

Train 3,888 2,717 1,387 637 185 - 8,314

Test 3,387 2,781 1,395 534 181 - 8,278

Total 6,775 5,498 2,782, 1,171 366 203 16,775

Table 1: Breakdown of the query log used in experiments by query length.

4. EXPERIMENTAL SETUP
All of the experiments in the following sections are conducted using a 50 million document corpus called TREC ClueWeb09 category B. We index this corpus using the Terrier IR platform [26]2, applying Porter's English stemmer, and removing standard stopwords. In the posting lists of the inverted index, docids are encoded using Elias Gammaencoded deltas and term frequencies using Elias Unary [13]. Each posting list also includes skip points [24], one every 1,000 postings. The resulting inverted index size is 22GB.
For testing retrieval efficiency, we extract a stream of user queries from a real search engine log, and measure the query response time and the number of postings scored when retrieving with all index data structures loaded in memory. Experiments are made using a dual quad-core Intel Xeon 2.6GHz, with 8GB RAM. In particular, we select the first 16,775 queries of the MSN 2006 query log [9], applying Porter's English stemmer and removing standard stopwords (empty queries are removed). This amounts to 10,000 queries of more than one term (single term queries cannot be pruned). Table 1 shows the distribution of queries by length3. Moreover, this sample exhibits the expected power law distributional of query occurrences: e.g. 1 query occurs 88 times, and 6,279 queries occurred once.
During retrieval, we apply three retrieval strategies: an exhaustive DAAT Full, where all postings for each document are exhaustively scored; MaxScore [30]; and Wand [5]. Moreover, as our inverted index is larger than the available RAM, we discard the posting lists of terms that do not occur in the 16,775 queries. This is justified as it simulates an in-memory index environment without having to partition the index across multiple servers. Hence, while our query response times may be larger than would suffice for an interactive environment, this does not detract from the validity of the experiments. Documents are ranked for each query using BM25, with Terrier's default parameter settings, while the number of documents retrieved is set to K = 1, 000.

5. ANALYSIS OF RESPONSE TIMES
To predict the efficiency of queries, we must first understand the characteristics of response times given by various retrieval strategies. While Moffat et al. [23] claimed that the main components of a query's response time are related to the number of terms in the query, and the length of the term's posting lists, there is some other anecdotal literature evidence that the response time of a dynamic pruning strategy can vary widely, even for queries with the same total number of postings to consider [3]. While Gan & Suel [16] assumed that the total postings was a sufficiently accurate estimation for caching decisions, Ozcan et al. [27] instead used actual response times, due to the variation in response times. In this section, we fully analyse and explain the variation in response times of different retrieval strategies. We

2http://terrier.org/ 3As there are very few queries with 6 or more terms, we omit these in the following experiments, and hence they do not appear in the train and test sets.

also show that the total number of postings does not accurately predict the response times of all pruning strategies.
Figures 1(a)-(d) show the response time distributions for different query lengths and different strategies, for 10,000 queries with more than one query term. Ideally, most queries should have small response times ≠ indicated by the highest frequency of response time being small, i.e. towards the left.
From Figure 1, we can see that different retrieval strategies exhibit different response time distributions. Indeed, while MaxScore shows only a small improvement w.r.t. Full, Wand markedly improves the response times for any query length, as it has the most number of queries with low response times, despite having to traverse the same posting lists of the same length as the other strategies. Moreover, a clear variance in the response times of queries for each strategy can be observed, even across queries of the same length. This is expected, as different queries have different numbers of postings that must be traversed.
Yet, due to their pruning behaviour, the number of postings to process does not fully depict the retrieval times of all pruning strategies. To illustrate this, Figure 2 shows scatter plots for total postings and response time for Full, MaxScore and Wand. From Figure 2(a), a strong correlation between the total number of postings and the query response times can be observed for Full, independently from the number of terms in the query. This correlation is explained by the fact that the strategy scores every posting for each query term, without any pruning. However, the correlations observed in Figure 2(b) & (c) for the MaxScore and Wand strategies are weaker. In particular, the response time for a query with a given number of postings can be markedly lower than another query with the same number of postings ≠ for instance, queries with 2 million postings usually take between 1.8 to 3.2 seconds with MaxScore, and 0.9 to 2.0 seconds for Wand. Hence, the amount of pruning possible varies between queries, even for those same length and number of postings. Moreover, as Wand is able to prune some queries more aggressively than MaxScore, its exhibits less correlation between total postings and response times.
To explain why different queries vary in their pruning difficulty, we turn to Figure 3. This shows the distribution of weighting model scores for two example queries, each with two query terms occurring in two documents, where the top K = 1 ranked document is required. Vertical arrows denote the scores of postings, with the upper bound (t) for each term represented by a horizontal dashed line. The threshold  after scoring each document is shown. As this is a DAAT process, the postings lists of all terms for a query are processed in parallel. After scoring the first document in query 1, the threshold (score of the Kth retrieved document) is  = 7. Hence, for the second document, after scoring the posting for the first term, MaxScore can assert that each document will not reach the retrieved set, as score(t1) + (t2) = 2 + 2 < 7. Hence the posting for the second query term of that document is pruned (dotted). In contrast, for query 2, all postings must be considered for the second document, as the score of the first term's posting for that document is not low enough that it cannot be retrieved - i.e. score(t1) + (t2) = 3 + 3  6. We say that the query 1 is easier to prune than query 2.
We now quantify the pruning difficulty for both MaxScore and Wand. In particular, for the 10,000 queries with more than one query term, we compute the pruning ratio p, defined as the percentage of total postings for a query that were actually scored by the dynamic pruning strategy. For

623

Density 0.0000 0.0005 0.0010 0.0015 0.0020

Density 0.0000 0.0005 0.0010 0.0015 0.0020
Density 0.0000 0.0005 0.0010 0.0015 0.0020

Density 0.0000 0.0005 0.0010 0.0015 0.0020

Retrieval Strategy
Full MaxScore Wand

Retrieval Strategy
Full MaxScore Wand

Retrieval Strategy
Full MaxScore Wand

Retrieval Strategy
Full MaxScore Wand

0

2000

4000

6000

8000

Response Time (millisec)

(a) 2 terms

0

2000

4000

6000

8000

Response Time (millisec)

(b) 3 terms

0

2000

4000

6000

8000

Response Time (millisec)

(c) 4 terms

0

2000

4000

6000

8000

Response Time (millisec)

(d) 5 terms

Figure 1: Response time distributions for different query lengths and different pruning strategies.

Response Time (millisec) 0 2000 4000 6000 8000 10000

Response Time (millisec) 0 2000 4000 6000 8000 10000

Response Time (millisec) 0 2000 4000 6000 8000 10000

 
  

 

   

 



 

0e+00

2e+07

4e+07

Total # Postings

6e+07

(a) Full

   





  





   
  

 

  
 


0e+00

2e+07

4e+07

Total # Postings

6e+07

(b) MaxScore





 





  

 

 

  



 



 


0e+00

2e+07

4e+07

Total # Postings

6e+07

(c) Wand

Figure 2: Total postings vs. response times for different retrieval strategies.

0.20

0.20

Score

!"# !"$ 4 2

!"$

!"# !"%

(t)

4

2

!"% (t)

0.15

0.15

Score

Density

Density

0

Postings for term 1 N

0

Postings for term 1 N

4

4

Score

Score

(t)

2

(t)

2

0

Postings for term 2 N

0

Postings for term 2 N

(a) Query 1: Easy to prune

(b) Query 2: Difficult to prune

Figure 3: Two example queries with different prun-

ing difficulties, each with two query terms with the

same number of postings.

0

0.05

0.10

0 10 20 30 40 50 60 70 80 90 % of total postings scored
(a) MaxScore

0 10 20 30 40 50 60 70 80 90 % of total postings scored
(b) Wand

Figure 4: The bucketed pruning ratios for two dynamic pruning strategies using 10,000 queries with more than one term.

0

0.05

0.10

instance, a value of p = 35% means that 35% of the total number of postings of the query were actually scored, while the remaining 65% of the postings were pruned. The pruning ratios of the 10,000 queries have been bucketed in buckets with size 1%. The distribution of these results are reported in Figure 4, for both MaxScore and Wand.
In both cases, a large percentage of queries are not pruned at all (i.e. p = 100%, right extremes of Figure 4) ( 20% for MaxScore and  10% for Wand). On closer inspection, we find that most of these unpruned queries consist of two query terms. This pruning difficulty has two main causes. Firstly, most of these queries have a very small total number of postings, so it is difficult to process enough of them to achieve a high enough threshold to start pruning. Alternatively, when the total number of postings is large, one of the

two terms has a very low discriminative power, i.e. a term with a very low IDF and consequently a very low maximum score. In this case, the strategy is forced to behave like for single term queries (where no pruning is possible), where the single term is the most discriminative (high IDF) one, with the other just adding some background noise.
Overall, while in Figure 4(a) MaxScore does not exhibit a strong pruning ability (e.g. the pruning ratio distribution is evenly distributed among the queries), in Figure 4(b) Wand performs very well. In fact, most queries have a small pruning ratio, i.e. less than 30%, which shows that Wand is often able to score only a small portion of the total number of scored postings, thereby attaining higher efficiency.
Therefore, due to their pruning nature, we argue that the total number of postings may not be enough to predict Max-

624

1000 2000 3000 4000

 

Response Time (millisec)



 

 

  



  

 




 








# Terms
2 3
4 5

0.0e+00 5.0e+06 1.0e+07 1.5e+07 2.0e+07 Scored # Postings

0

Figure 5: Scored postings vs. response times for , broken down by query lengths.

Score or Wand's scored postings and hence the query response time. In fact, Figures 2 (b) & (c) show that, for the same total number of postings, the query response time varies markedly, particularly for Wand. Instead, the number of scored postings is the central statistic to be predicted that defines the resulting response time. Indeed, Figure 5 shows that, for different query lengths, the number of scored postings is correlated to the query response times. However, the pruning ratio or the number of scored postings cannot be obtained before retrieval commences. We posit that various statistics of the query terms that are available offline can be used to predict the response time of a query before retrieval commences. In the next section, we propose a framework for query efficiency prediction, and evaluate various efficiency predictors within this framework. We then show one novel application of the query efficiency prediction framework, namely the online scheduling of queries to replicated query servers.
6. PREDICTING QUERY EFFICIENCY
Having an accurate estimate of the efficiency of a query can be useful to a search engine in many ways. For instance, in a replicated setting, if the availability of query servers can be predicted by the broker, then a new query could be scheduled to the query server expected to be idle soonest.
To be of use, efficiency predictors must be pre-retrieval i.e. available before retrieval for a query has commenced, and easily obtained at retrieval time (similar to some query performance predictors [18]). Indeed, a post-retrieval efficiency predictor is of no use, as the response time of the query is known after retrieval has occurred. Hence, our efficiency predictors are based on the available statistics of the query terms (frequency, number of postings, IDF etc). We note that some pre-retrieval query performance predictors (e.g. AvPMI [17]) use the joint probability of a pair of query terms. However, calculation of the joint probability requires n-gram language models, or posting list scans. We avoid such pair statistics, even though some search engines may record additional posting lists for some phrases [12].
In the remainder of this section, we propose a framework for query efficiency prediction, and define many predictors (Section 6.1). These predictors are evaluated compared to the response times of 10,000 queries (see Section 6.2 for prediction experimental setup), both individually (Section 6.3), and in combination (Section 6.4).

Term Statistics s(t) 1. Arithmetic mean score 2. Geometric mean score 3. Harmonic mean score 4. Max score 5. Approximation of max score [22] 6. Variance of score 7. # Postings 8. # Maxima 9. # Maxima greater than average score 10. # Postings with max score 11. # Postings within 5% of max score 12. # Postings with score within 5% of final threshold 13. Promotions into K 14. IDF
Aggregators A() a. Maximum b. Variance c. Sum

Table 2: All tested term-based efficiency prediction statistics and aggregators.

6.1 A Query Efficiency Prediction Framework
As we have shown in Section 5, pruning difficulty varies between queries, and hence the response time of a query cannot be directly predicted. Moreover, as it is infeasible to generate offline features for the universe of possible queries, we instead use statistics that are computed for each term. These are then aggregated (e.g. sum, max) across all of the constituent terms of a query, to form different features for a given query. In this manner, a query efficiency prediction feature fij(Q) is defined by a term-level statistic sj(t) for a query term t, and an aggregation function Ai:

fij(Q) = Ai({sj(t) t  Q})

(1)

We use a learning framework to obtain predictions for response times given a set of input features. In particular, we deploy linear regression [8] as our learning framework, both for single and multiple features. Linear regression has previously been successfully used for combining query performance predictors [17].
In terms of features, we rely on more than the frequency counts and IDFs of the query terms that have been traditionally applied by query performance predictors. In particular, recall that dynamic pruning strategies require an upper bound (t) on the weighting model score for each term. This is normally identified at indexing time, by scoring the posting lists of all terms [10]. This provides an opportunity to calculate other term-based statistics, such as the maximum, mean and standard deviation of the weighting model scores observed for each posting list, as well the number of documents that would be inserted into the top K documents set, if there was only a single query term in that query.
For each term-based statistic, we create several query efficiency predictors by aggregating using three different statistical functions: sum, max and variance. The top part of Table 2 lists all used term statistics, while the bottom part lists the functions that are used to aggregate the term statistics into features for a given query. From Table 2, we highlight some representative term statistics: Arithmetic, geometric, harmonic score means : Means of the weighting model scores from the term's postings. Max of score: The exact upper bound (t) of the scores in the posting list. Approximation of max score: An approximation of (t), using the maximum observed term frequency [22].

625

# Postings: The number of postings in a term's posting list. The sum of this statistic is the number of postings scored by Full, and the upper bound on that which can be scored by dynamic pruning strategies. # Maxima: The number of times that the maximum score occurs. A term that has fewer maxima in the score distribution may be easier to prune. Promotions into K: If this query term was the only query term, how many documents containing this term would make it into the top K retrieved documents. A term with a low number of promotions probably has its highest scored documents towards the start of the posting list. # Postings with score within 5% of the final threshold: This takes into account the number of postings that attain a score very close to the Kth scored document for a query containing only that term. IDF: The inverse document frequency of the term. IDF and other similar statistics are the basis of several query performance predictors (e.g. AvICTF [18]).
The proposed statistics and aggregation functions listed in Table 2 are not exhaustive ≠ while others may exist, for reasons of brevity, we focus on those listed above. Moreover, while not all predictors are likely to highly correlate with query response time, in the remainder of this section, we show that some predictors can be of use within linear regression, both in isolation, and when combined.
6.2 Prediction Experimental Setup
In the following, we address two research questions: 1. What are the most accurate efficiency prediction features when used separately in linear regression (Section 6.3)? 2. Can features be combined within linear regression for increased accuracy (Section 6.4)?
To address these research questions, we use the 10,000 queries with more than one query term (the prediction of the efficiency of single term queries is trivial as no pruning takes place). Table 1 provides the distribution of queries w.r.t. query length. We form training and testing subsets, by splitting the query sets of each length into two equal sets chronologically4. The training set will be used to train the regression models in the following sections, while all results are reported on the testing set. Accuracy is measured by the Pearson's correlation r (-1  r  1: 1 is a perfect correspondence; -1 is a reversed correspondence) and the Root Mean Square Error (RMSE) between the predicted and actual response times (as obtained in Section 4) for all queries.
6.3 Results: Single Predictors
To test the most accurate efficiency predictors based on single features, we compute the correlation and the RMSE between the predicted and actual response times on the test queries, after training on the corresponding training set with the same query length. The five most accurate efficiency predictors for each retrieval strategy, and their respective correlations and RMSE values for various query lengths are reported in Table 3. Sum # Postings (i.e. total number of postings) is the baseline prediction feature. Statistically significant differences in term of Pearson correlation from this
4Other experiments using 5-fold cross validation gave similar conclusions.

baseline ≠ according to a Fisher Z-transform ≠ are denoted with an asterisk (*).
We firstly analyse the results in Table 3 for each retrieval strategy in turn. For Full, the most accurate efficiency predictor is the total number of postings (Sum # Postings), as each posting is scored. Some other predictors, namely Sum # Maxima and Sum # Maxima > average score exhibit similar accuracies. This is easily explainable, as a docid-sorted index typically has no trend on score distribution, so the maxima are evenly distributed along the scores of a postings list, and the number of maxima tends to be half of the total number of entries.
For the MaxScore retrieval strategy, which does prune, the baseline predictor of Sum # Postings is less accurate than for Full. However, the best predictors are the baseline predictor and, again, Sum # Maxima and Sum # Maxima > average score, mostly because MaxScore cannot prune many queries (as shown in Figure 4(a)). For Wand, the predicted vs. actual response time correlations of the baseline predictor feature are again overall lower, as Wand prunes more aggressively than MaxScore (Figure 4(a) & (b)), and is also the only pruning strategy able to skip whole portions of postings lists. For these reasons, the observed lower correlation with number of postings is expected. Instead, the Sum # Maxima > avg and Sum # Maxima features are promising, but not significantly better than the baseline.
Finally, we note that no feature based on IDF appeared in the top prediction features in Table 3. This suggests that query efficiency is a different task from query performance (effectiveness) prediction, with different statistics required.
Overall, we conclude that the Sum # Postings is a good single predictor for Full and MaxScore, exhibiting errors of less than 0.25 seconds. For Wand, some other efficiency predictor features show promise, but do not significantly improve over the baseline. We consider that the proposed efficiency features are `weak features'. Hence, in the next section, we show how significantly improved efficiency prediction can be achieved by combining multiple prediction features within the regression, instead of only a single feature.
6.4 Results: Combined Predictors
Table 4 reports the correlations and RMSEs between predicted and actual response times observed on the test query set when combining all efficiency prediction features within a linear regression5. On analysing the results, we find that for the Full retrieval strategy, the learned model possibly overfits, and cannot outperform the baseline Sum # Postings feature. Similarly, the fact that MaxScore cannot prune many queries (see Figure 4(a)) ensures that the baseline feature is the most appropriate. However, for Wand, the regression model combining all 42 features is significantly more accurate at predicting response times than the baseline for all query lengths, and markedly improves the RMSEs values (approx 25%-30% reduction, with a prediction error of less than 0.2 seconds observed for queries of length  3).
Overall, we conclude that the proposed framework for efficiency prediction is accurate for predicting the response times of the state-of-the-art Wand strategy, where the total number of postings is not a sufficiently good predictor. For Full and MaxScore, no or little pruning occurs, and hence the total number of postings is sufficient.
5Predicted negative times are set to 0.

626

Features Agg. Term Statistic

Sum Max Max Max Sum Sum

# Postings # Maxima # Maxima > avg # Postings # Maxima # Maxima > avg

2

r

RMSE

0.920 0.879* 0.870* 0.879* 0.919 0.914

0.274 0.329 0.340 0.328 0.274 0.283

Query Length

3

4

r

RMSE

r

RMSE

Full

0.945 0.261 0.957 0.258

0.889* 0.361 0.896* 0.392

0.884* 0.369 0.892* 0.400

0.890* 0.361 0.896* 0.392

0.945 0.261 0.957 0.258

0.941 0.270 0.954 0.268

5

r

RMSE

0.963 0.902* 0.899* 0.902* 0.963 0.960

0.256 0.407 0.413 0.408 0.256 0.266

Sum Max Max Max Sum Sum

# Postings # Maxima # Maxima > avg # Postings # Maxima # Maxima > avg

0.871 0.837* 0.832* 0.837* 0.871 0.870

0.279 0.310 0.314 0.310 0.278 0.281

0.891 0.835* 0.833* 0.835* 0.891 0.890

0.292 0.352 0.353 0.352 0.291 0.292

0.910 0.858* 0.857* 0.858* 0.910 0.910

0.301 0.372 0.373 0.372 0.300 0.301

0.922 0.870* 0.871* 0.870* 0.923 0.922

0.304 0.387 0.387 0.388 0.303 0.304

Sum Max Max Max Sum Sum

# Postings # Maxima # Maxima > avg # Postings # Maxima # Maxima > avg

0.812 0.780* 0.786* 0.778* 0.814 0.822

0.287 0.309 0.305 0.310 0.285 0.280

0.840 0.777* 0.782* 0.776* 0.841 0.846

0.253 0.294 0.291 0.294 0.252 0.249

0.857 0.782* 0.785* 0.781* 0.858 0.861

0.249 0.301 0.299 0.302 0.247 0.245

0.870 0.793* 0.797* 0.792* 0.871 0.874

0.249 0.308 0.306 0.309 0.248 0.246

Table 3: Pearson correlations and RMSE values (in seconds) on the test query set of the best single predictor features for different query lengths and retrieval strategies. Significantly different Pearson correlations from Sum # Postings are denoted *.

# Feat.
Sum # Post. 1 Combination 42

2

r

RMSE

0.920 0.274 0.885* 0.323

Query Length

3

4

r

RMSE

r

RMSE

Full

0.945 0.261 0.957 0.258

0.926* 0.301 0.942* 0.299

5

r

RMSE

0.963 0.256 0.949 0.299

Sum # Post. 1 Combination 42

0.871 0.279 0.891 0.292 0.910 0.301 0.922 0.304 0.841* 0.306 0.875 0.311 0.892 0.329 0.902 0.339

Sum # Post. 1

0.812 0.287 0.840 0.253 0.857 0.249 0.870 0.249

Combination 42 0.912* 0.200 0.922* 0.181 0.921* 0.188 0.928* 0.189

Table 4: Pearson correlations and RMSE values (in seconds) on the test set queries when using all prediction features. Significantly different Pearson correlations from Sum # Postings are denoted *.

7. APPLYING QUERY EFFICIENCY PREDICTION TO QUERY SCHEDULING
While in general the main retrieval efficiency measure is the average time required to process the queries (average response time), when a stream of queries is received by a search engine, it might not be possible to start processing a new query as soon as it arrives. Instead, when the system is busy processing a query, subsequent queries are queued [7]. Therefore, the actual time delay experienced by a user while waiting for search results (completion time) is given by the execution time (response time) of the query, plus the time the query spent waiting to be processed (waiting time).
Classically, queued queries have been processed in a FIFO manner (i.e. first-come first-served) [7]. However, this only results in minimising queueing time if each query has an equal response time [20]. Instead, we propose that queues of queries can be scheduled to execute out of arrival order, by deploying specific scheduling algorithms [20]. In this way, for instance, quick queries may be scheduled between longer queries, to reduce the mean time delay experienced by the user population of the search engine.
As mentioned in Section 2, an oft-deployed approach to increase query throughput and reduce the average comple-

tion time is to replicate the number of query servers available for a given index shard. Hence, a new query arriving at the broker is sent to a query server that is idle, or least loaded. However, as the number of CPU cores in each query server is finite, it is natural that queries are queued until the query server is able to process them. In this manner, we propose two possible architectures for such a replicated retrieval system, as illustrated in Figure 6 for a single index shard. In the broker queue architecture, newly arrived queries are queued by the broker, before routing to the next available query server. In contrast, in the queue per server architecture, each new query is routed directly to a query server, which queues that query until it can be processed. In both architectures, there are potentials to improve the overall efficiency by the appropriate online scheduling of queries.
For instance, in the broker queue architecture, the queue can be sorted to balance throughput. For queue per server, a query can be sent to the query server with the lowest loading, for instance by measuring the number of queries it has queued to process. However, as shown in Section 5, different queries can take different amounts of time to complete, dependent on the nature of the pruning experienced by the query. As a result, a newly arrived query can be scheduled onto a query server that already has a queue of

627

(a) Broker queue
(b) Queue per server
Figure 6: Architectures for replicated query servers.
expensive queries, rather than another server which has a queue of inexpensive queries, resulting in poor load balancing across query servers. Instead, in this section, we show how query efficiency predictors can be used to improve the online scheduling of queries to distribute load across query servers, and result in improved mean completion time.
In the following we test different methods of online scheduling of queries across replicated query servers, both with a single broker queue and with a queue per server. For the former, we consider three scheduling methods [20]: First Come First Served (FCFS): Incoming queries are kept sorted in arrival order, and as soon as a server is idle, it starts processing the first query in the queue [7]. FCFS does not need any information about the execution time of a query, and hence forms our baseline. Actual Shortest Job First (ASJF): Incoming queries are kept sorted in increasing order of expected service time, and scheduled to the servers in that order. ASJF assumes knowledge of the actual execution time a query will take, and hence represents scheduling with perfect knowledge, forming an oracle. ASJF is the optimal schedule to reduce the mean queueing time experiences by each query [20]. Predicted Shortest Job First (PSJF): As ASJF, but uses predicted execution times provided by our framework from Section 6 instead of actual times.
When the queue per server architecture is used, an incoming query is dispatched by the broker to a server as soon as it arrives, and then queued by that server until retrieval can commence. In this scenario, we test three methods for dispatching queries to server: Queue Length (QL): Queries are scheduled onto the query server with the shortest queue length. This represents our baseline. Actual Execution (AE): Queries are scheduled onto the query server with the smallest queue duration time, assuming that the actual execution times of the queued queries are known a-priori. This represents scheduling with perfect knowledge, forming an oracle. Predicted Execution (PE): Queries are scheduled onto the query server with the smallest queue duration time, using predicted execution times provided by our framework from Section 6.

Comparing the two architectures, there are likely advantages in deploying the queue per server architecture, as this does not require the broker to know the predicted response time of a query, which uses statistics likely to be stored with the index. However, scheduling quality might be degraded by the use of multiple queues [25].
7.1 Simulation Experimental Setup
In the following, we perform simulation experiments to address the following research questions: 3. Can scheduling improve the average completion time of queries in a replicated query servers setting? 4. Can query efficiency prediction improve over Sum # Postings when scheduling queries for Wand? 5. Which replicated retrieval system architecture, broker queue (Section 7.2) or queue per server (Section 7.3), produces lower average completion times?
We devise an experiment that simulates the execution of query processing across multiple query servers, using the actual query arrival times of the 8,278 test queries from the MSN query log for various pruning strategies. This query set includes the original test set of 5,000 multi-term queries used in previous section, as well as single term queries from the same time period (approximately the 2nd 2.5 hours of May 1st 2006). The arrival rate of new queries varies between 20 and 100 per minute.
Predicted and actual query response times are as obtained from the experiments in Section 6 - i.e. for Full and MaxScore, we use the Sum # postings, while, for Wand, to show the benefit of our efficiency prediction framework, we compare Sum # postings and the 42 feature regression model. Hence, in the following experiments, the suffix (1) denotes the Sum # postings predictor, while (42) denotes the 42 feature regression model. For single term queries, no pruning is possible, and hence we use # postings to predict the response time of all retrieval strategies.
Simulation has previously been shown to accurately model distributed and replicated search engine architectures [7]. Our simulation experiments do not model the cost of computing an efficiency prediction. Indeed, as all features for each query term are in memory, the floating point operations to compute the predicted response time require very few CPU cycles. We also fix the number of cores per server to 1, such that contention for memory bandwidth within query servers need not be modelled. However, the number of replicated query servers is varied in the range 1..4.
Finally, following scheduling practises [20], we measure the scheduling success by comparing average wait times (AWTs) and average completion times (ACTs), as measured from the queries' arrival times.
7.2 Results: Broker Queue
Table 5 reports the ACTs and AWTs in seconds, for different scheduling strategies, pruning strategies and number of servers in the broker queue architecture. The optimal results are obtained when every query is served as soon as it arrives, i.e. they represent the average response times, with no waiting time. Other results are reported for other scheduling algorithms. In addition to executing queries in their arrival order (FCFS), three variants of shortest job first are deployed: ASJF represents the best case scheduling, when the broker's queue is sorted by the known response time of each query; in PSJF(1), queries are scheduled by their total num-

628

Scheduling Algorithm
Optimal
FCFS ASJF PSJF(1) PSJF(42)
FCFS ASJF PSJF(1) PSJF(42)
FCFS ASJF PSJF(1) PSJF(42)
FCFS ASJF PSJF(1) PSJF(42)

Full

ACT AWT ACT AWT

+ query servers

0.691

0

0.573

0

1 query server

309.514 308.822 119.603 119.030

68.379 67.688 24.540 23.967

71.460 70.769 26.596 26.023

-

-

-

-

2 query servers

2.005 1.314 1.012 0.439

1.224 0.533 0.822 0.249

1.226 0.535 0.829 0.256

-

-

-

-

3 query servers

0.821 0.130 0.641 0.068

0.776 0.085 0.621 0.047

0.777 0.086 0.621 0.048

-

-

-

-

4 query servers

0.723 0.031 0.588 0.015

0.713 0.022 0.585 0.012

0.714 0.022 0.586 0.013

-

-

-

-

ACT
0.401
10.748 2.674 5.314 3.230
0.524 0.488 0.516 0.492
0.424 0.419 0.423 0.420
0.406 0.405 0.406 0.405

AWT
0
10.347 2.274 4.913 2.829
0.123 0.087 0.115 0.091
0.023 0.018 0.022 0.019
0.005 0.004 0.005 0.004

Table 5: Average completion time (ACT) and average waiting time (AWT) for different scheduling strategies, pruning strategies and number of servers, for broker queue architecture.
ber of postings; in PSJF(42), queries are scheduled by their predicted response time using all 42 prediction features.
On analysing Table 5, we firstly note that, as expected, MaxScore and Wand improve over the efficiency of the exhaustive Full. Next, on analysing the scheduling strategies, we find that FCFS performs very poorly when applied to a single server. This is a well-known problem of FCFS [20]: a query requiring a long time to process will delay all subsequent queries that, even if they require a short time to process, must wait for the long query to complete. Nevertheless, AWTs are decreased when dynamic pruning strategies are used: for example, Wand gives a benefit of 97% to the average completion time of FCFS on a single server (309.514 to 10.748 seconds). Moreover, dynamic pruning also helps in reducing the average completion time with FCFS scheduling when multiple servers are used.
Next, we compare the scheduling FCFS and ASJF, and observe that in all cases, ASJF results in markedly reduced ACTs and AWTs. This in line with our expectations from the scheduling literature [20]. The margin of improvement decreases as more servers are added, as there is less contention for query servers, and hence scheduling has less effect. Indeed, the query load within this query set is adequately handled by four query servers, as the results are very close to the optimal completion time, i.e. no queries are queued and average waiting times are close to 0. Nevertheless, the overall reduced AWTs and ACTs exhibited by ASJF allow us to conclude that there is the potential for scheduling to improve the efficiency of both non-replicated and replicated retrieval settings.
Finally, we examine the use of predicted execution times instead of actual ones (i.e. PSJF). In particular, as expected, for Full and MaxScore, the performance of PSJF(1) is almost the same as ASJF scheduling. However, PSJF(1) ≠ i.e. Sum # Postings ≠ does not produce accurate estimations of response time for the considerably more efficient Wand. Indeed, using the learned model with all 42 prediction features Wand results in ACT and AWTs that are much closer to the best case ASJF performances ≠ e.g., for Wand, PSJF(42) results in a 70% reduction in ACT for a single query server compared to FCFS (from 10.748 seconds to 3.230 seconds).

Scheduling Algorithm
Optimal
QL AE PE(1) PE(42)
QL AE PE(1) PE(42)
QL AE PE(1) PE(42)

Full

ACT AWT ACT AWT

+ query servers

0.691 0 0.573 0

2 query servers

2.186 1.495 1.150 0.577

2.054 1.363 1.047 0.474

2.055 1.363 1.058 0.485

-

-

-

-

3 query servers

0.935 0.244 0.715 0.142

0.845 0.158 0.654 0.081

0.853 0.161 0.656 0.083

-

-

-

-

4 query servers

0.774 0.082 0.622 0.046

0.733 0.042 0.594 0.021

0.734 0.043 0.596 0.022

-

-

-

-

ACT
0.401
0.593 0.540 0.593 0.551
0.461 0.430 0.458 0.433
0.422 0.408 0.419 0.409

AWT
0
0.192 0.139 0.192 0.150
0.060 0.029 0.057 0.032
0.021 0.007 0.018 0.008

Table 6: Average completion time (ACT) and average waiting time (AWT) for different scheduling strategies, pruning strategies and number of servers, for queue per server architecture.
Hence, these results attest the usefulness of the proposed query efficiency prediction framework, and its benefits for the online scheduling of replicated query servers.
7.3 Results: Queue per Server
We now study scheduling in the queue per server architecture. In particular, Table 6 reports the ACTs and AWTs in this architecture6. Three different scheduling methods are tested for measuring the outstanding queue for a query server: Queue Length (QL) ≠ baseline, Actual Execution (AE) ≠ best-case scheduling ≠ and Predicted Execution (PE), which uses predicted response times from the query efficiency prediction framework. Two variants of PE are employed, one using total number of postings alone (1) and one using all 42 prediction features (42). A particular advantage of PE in the queue per server architecture is that the broker does not require the predicted response time of a query, but instead only the predicted response times of the queries already queued on that server.
From Table 6 we draw several observations. Firstly, choosing the query server for the next query using QL is worse than AE, across different numbers of servers. Moreover, PE produces results that are only slightly worse than the bestcase AE, and in all cases better than QL. Indeed, with more than three servers, AWTs reduce towards 0 across all pruning strategies. As expected, the PE(1) results are good for Full and MaxScore, however for Wand, PE(42) results in ACTs and AWTs that are much closer to AE than PE(1). Indeed, for two query servers, PE(42) results in a 22% reduction in AWT for Wand compared to QL (from 0.192 seconds to 0.150 seconds), and a 7% reduction in ACT (0.593 to 0.551 seconds). Once again, this shows the accuracy of the query efficiency framework at encapsulating Wand's pruning behaviour.
Lastly, we compare Tables 5 & 6 to address our final research question on the two replication architectures. In general, response times are slightly lower using the broker queue architecture. For instance, the ACT of Wand with three query servers using PSJF(42) is 0.420, while PE(42) is slightly higher at 0.433. This is explainable in that the bro-
6We omit the results for the single server scenario, as these results are equivalent to FCFS in Table 5.

629

ker queue can be appropriately sorted by expected execution length, while in queue per server, the scheduling of queries is less refined [25]. However, we recognise that queue per server does not require the broker to be able to predict execution times. This is likely beneficial for fully distributed retrieval settings, when the index is sharded across multiple query servers and multiple servers serving each shard, where it is unlikely that the broker can know the predicted response times of all shards.
Overall, we conclude that the online scheduling of queries to query servers can improve the average wait and completion times of queries - indeed, PSJF improved over FCFS for the broker queue architectures, and PE improved over QL for the queue per server architecture. Moreover, such scheduling is made possible by our proposed novel framework for query efficiency prediction, particularly for the most efficient Wand dynamic pruning retrieval strategy.
8. CONCLUSIONS
Dynamic pruning techniques can improve the efficiency of queries, but result in different queries taking different amounts of time. We empirically investigated the efficiency of different queries for in-memory inverted indices, and showed how and why the amount of pruning that could be applied for a query can vary. Next, we proposed a framework for predicting the efficiency of a query, which uses linear regression to learn a combination of aggregates of term statistics. Experiments for 10,000 queries retrieving from an in-memory index of the TREC ClueWeb09 collection (50 million documents) showed that our learned predictors encapsulating 42 features could successfully predict the response time of the state-of-the-art Wand dynamic pruning retrieval strategy.
Moreover, we proposed the online scheduling of queries across replicated query servers, driven by predicted response times of queries. Two different scheduling architectures were proposed, differing in the location of the queueing. Simulation experiments showed not only the advantages of scheduling, but also the benefit of more accurate predicted response times within the scheduling algorithms.
We believe that there are many applications for the proposed query efficiency framework. For instance, we will investigate the application of query efficiency predictors for query routing across multi-shard indices. The efficiency predictors may also be used for controlling the efficiency/effectiveness tradeoff (e.g. achieved by diluting the top K safeness requirement or reducing the term upper bounds [5]).
Acknowledgements
Craig Macdonald and Iadh Ounis acknowledge the support of EC-funded project SMART (FP7-287583).
9. REFERENCES
[1] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, and selective application of query expansion. In Proc. ECIR 2004, pp 127≠137.
[2] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In Proc. SIGIR 2006, pp 372-379.
[3] R. Baeza-Yates, A. Gionis, F. P. Junqueira, V. Murdock, V. Plachouras, and F. Silvestri. Design trade-offs for search engine caching. ACM Trans. on Web, 2:20:1≠20:28, 2008.
[4] R. Blanco. Index compression for information retrieval systems. PhD thesis, University of A Coruna, 2008.

[5] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. In Proc. CIKM 2003, pp 426≠434.
[6] J. D. Brutlag, H. Hutchinson, and M. Stone. User preference and search engine latency. In Proc. ASA Joint Statistical Meetings 2008.
[7] F. Cacheda, V. Plachouras, and I. Ounis. Performance Analysis of Distributed Architectures to Index One Terabyte of Text. In Proc. ECIR 2004, pp 394≠408.
[8] J. M. Chambers and T. J. Hastie. Statistical Models. Wadsworth & Brooks, 1992.
[9] N. Craswell, R. Jones, G. Dupret, and E.Viegas, editors. Proc. Web Search Click Data Workshop at WSDM 2009.
[10] W. B. Croft, D. Metzler, and T. Strohman. Search Engines ≠ Information Retrieval in Practice. Addison-Wesley, 2009.
[11] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proc. SIGIR 2002, pp 299≠306.
[12] J. Dean. Challenges in building large-scale information retrieval systems: invited talk. In Proc. WSDM 2009.
[13] P. Elias. Universal codeword sets and representations of the integers. Transactions on Information Theory, 21(2):194≠203, 1975.
[14] R. Fagin, A. Lotem, and M. Naor. Optimal aggregation algorithms for middleware. J. of Computer & System Sciences, 66(4):614≠656, 2003.
[15] M. Fontoura, V. Josifovski, J. Liu, S. Venkatesan, X. Zhu, and J. Zien. Evaluation strategies for top-k queries over memory-resident inverted indexes. Proc. VLDB Endowment 2011, 4(12):1213≠1224.
[16] Q. Gan and T. Suel. Improved techniques for result caching in web search engines. In Proc. WWW 2009, pp 431≠440.
[17] C. Hauff. Predicting the Effectiveness of Queries and Retrieval Systems. PhD thesis, Univ. of Twente, 2010.
[18] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In Proc. SPIRE 2004, pp 43≠54.
[19] K. Kwok, L. Grunfeld, H. Sun, P. Deng, and N. Dinstl. TREC 2004 robust track experiments using PIRCS. In Proc. TREC 2004, pp 191≠200.
[20] J. Y-T. Leung, editor. Handbook of Scheduling, Chapman & Hall, 2004.
[21] S. Luo and J. Callan. Using sampled data and regression to merge search engine results. In Proc. SIGIR 2002, pp 19≠26.
[22] C. Macdonald, I. Ounis, and N. Tonellotto. Upper bound approximations for dynamic pruning. ACM Trans. on Information Systems, 29(4), 2011.
[23] A. Moffat, W. Webber, J. Zobel, and R. Baeza-Yates. A pipelined architecture for distributed text query evaluation. Information Retrieval, 10(3):205≠231, 2007.
[24] A. Moffat and J. Zobel. Self-indexing inverted files for fast text retrieval. ACM Trans. on Information Systems, 14(4):349≠379, 1996.
[25] P. Morse. Queues, Inventories and Maintenance: The Analysis of Operational Systems with Variable Demand and Supply. Dover, 2004.
[26] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A high performance and scalable information retrieval platform. In Proc. OSIR Workshop 2006, pp 18≠25.
[27] R. Ozcan, I. S. Altingovde, and O. Ulusoy. Cost-aware strategies for query result caching in web search engines. ACM Trans. on Web, 5:9:1≠9:25, 2011.
[28] M. Persin. Document filtering for fast ranking. In Proc. SIGIR 1994, pp 339-348.
[29] N. Tonellotto, C. Macdonald, and I. Ounis. Query efficiency prediction for dynamic pruning. In Proc. LSDS-IR 2011 at CIKM 2011.
[30] H. Turtle and J. Flood. Query evaluation: strategies and optimizations. Information Processing and Management, 31(6):831≠850, 1995.
[31] L. Wang, J. Lin, and D. Metzler. Learning to efficiently rank. In Proc. SIGIR 2010, pp 138≠145.

630

Diversity by Proportionality: An Election-based Approach to Search Result Diversification

Van Dang and W. Bruce Croft
Center for Intelligent Information Retrieval Department of Computer Science University of Massachusetts Amherst, MA 01003
{vdang, croft}@cs.umass.edu

ABSTRACT
This paper presents a different perspective on diversity in search results: diversity by proportionality. We consider a result list most diverse, with respect to some set of topics related to the query, when the number of documents it provides on each topic is proportional to the topic's popularity. Consequently, we propose a framework for optimizing proportionality for search result diversification, which is motivated by the problem of assigning seats to members of competing political parties. Our technique iteratively determines, for each position in the result ranked list, the topic that best maintains the overall proportionality. It then selects the best document on this topic for this position. We demonstrate empirically that our method significantly outperforms the top performing approach in the literature not only on our proposed metric for proportionality, but also on several standard diversity measures. This result indicates that promoting proportionality naturally leads to minimal redundancy, which is a goal of the current diversity approaches.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ≠ retrieval models
General Terms
Algorithms, Measurement, Performance, Experimentation.
Keywords
Search result diversification, proportional representation, proportionality, redundancy, novelty, Sainte-Lagu®e.
1. INTRODUCTION
Search result diversification techniques have been studied as a method of tackling queries with unclear information
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

needs. Standard retrieval models and evaluations are based on the assumption that there is a single specific topic associated with the relevant documents for a query. Diversification models [4, 1, 26], on the other hand, identify the probable "aspects" of the query and return documents for each of these aspects, making the result list more diverse. Aspects denote the multiple possible intents, interpretations, or subtopics associated with a given query. By explicitly representing and providing diversity in the result list, these models can increase the likelihood that users will find documents relevant to their specific intent and thereby improve effectiveness.
This problem of finding a diverse ranked list of documents, with respect to the aspects of the query, has been studied primarily from the perspective of minimizing redundancy. In other words, existing work focuses on penalizing result lists with too many documents on the same aspect, which increases the redundancy of coverage, and promoting lists that contain documents covering multiple aspects. Most of the effectiveness measures for diversity [7, 8] are also based on this notion of redundancy. They penalize the redundancy in a ranked list of documents by judging each of the documents given the context of those retrieved at earlier ranks [9].
In this paper, we approach the same task from a different perspective. We view the problem of finding a good result list of any given size as the task of finding a representative sample of the larger set of ranked documents. Hence, the quality of the subset (a result list) should be measured by how well it represents the whole set (a much larger sample of the ranking). Using a simple (and well-worn) example, in a ranked list for a query "java", 90% of the documents may be about the programming language and 10% about the island. From our perspective, a result list containing ten documents where only one of them was about the island would be more representative than a result list containing five documents on each subtopic. Consequently, we treat the problem of finding a diverse result of documents as finding a proportional representation for the document ranking.
Finding a proportional representation is a critical part of most electoral processes. The problem is to assign a set of seats in the parliament to members of competing political parties in a way that the number of seats each party possesses is proportional to the number of votes it has received. In other words, the members in the elected parliament must be a proportional representation of these parties. If we view each position in our ranked list as a "seat", each aspect of the

65

query as a "party" and the aspect popularity as the "votes" for this "party", the problem of diversification becomes very similar to this seat allocation problem.
Based on the above analogy, we propose a novel technique for search result diversification. It is an adaptation of the Sainte-Lagu®e method, a standard technique for finding proportional representations that is used in the official election in New Zealand1. Generally, our technique starts with an empty ranked list of a certain size. It sequentially visits each "seat" in the list and determines for each of them to which aspect it should be allocated in order to maintain proportionality. Then it selects the best document for the selected aspect to occupy this "seat". In addition, we also present a new effectiveness measure that captures proportionality in search results. We demonstrate empirically that our method is more effective than the top performing approach in the diversity literature not only according to the proportionality measure but also using several standard metrics including -NDCG [7] and NRBP [8] that existing work has been designed to optimize. This indicates that optimizing search results for proportionality naturally leads to minimal redundancy and a diverse, effective result list.
In the next section, we briefly mention some related work. Section 3 presents our approach to proportionality and the effectiveness measure based on it. Section 4 describes in detail our proportionality-driven framework for search result diversification. Section 5 and 6 contains the experimental setup and results, as well as analyses and discussions. Finally, Section 7 concludes.
2. RELATED WORK
The literature of diversification has been concentrating on the notion of novelty and redundancy. These two notions are considered under the context of user behavior with the assumption that users examine the result lists top down and eventually stop reading. Therefore, a document at any rank providing the same information as those at earlier ranks is considered redundant. Likewise, a novel document is one that provides information that has not been covered by any of the previous documents. As a result, a ranked list is considered more diverse if it contains less redundancy, or equivalently, more novelty.
This is clearly demonstrated through several standard effectiveness metrics such as -NDCG [7] and NRBP [8]. They measure the diversity of a ranked list by explicitly rewarding novelty and penalizing redundancy observed at every rank. Similarly, diversification techniques [4, 29, 1, 5, 26] attempt to form a diverse ranked list by repeatedly selecting documents that are different to those previously selected. In other words, they try to accommodate novelty at every position in the list.
In this paper, we present a different perspective on diversity. This view of diversity emphasizes proportionality, which is the property that the number of documents returned for each of the aspects of the query is proportional to the overall popularity of that aspect. Consequently, the framework we derive is driven by this notion of proportionality, thus is different from the existing work.
Several metrics have been proposed to measure the proportionality in the outcome of an electoral process, an excellent summary of which is provided by Gallagher and Lijphart
1 http://www.elections.org.nz/voting/mmp/sainte-lague.html

[17, 19]. They can be classified into two broad categories: the first concentrates on the absolute difference between the percentage of seats and the percentage of votes, the second focuses on the the ratio between them. These measures appear mathematically simple but attempt to address complex specific issues of elections that are not always relevant to our context. As a result, we apply Gallagher's Index [17], or the least square index, which is reasonably suited to our problem.
For completeness, we will provide a brief survey of techniques in the current literature of diversification. They can be classified as either implicit or explicit approaches. The former [4, 29] assumes that similar documents cover similar aspects without modelling the actual aspects. They iteratively select documents that are similar to the query but different to the previously selected documents in terms of vocabulary [4] or divergence from one language model to another [29]. More recent work [25, 22] applies the portfolio theory to document ranking, which views diversification as a mean of risk minimization. Explicit approaches [23, 1, 5, 26], on the other hand, explicitly models aspects of a query with a taxonomy [1], top retrieved documents [5] or query reformulations [23, 26] and thus can directly select documents that cover different aspects. Experimentally, explicit approaches have been demonstrated to be superior to implicit approaches [26].
3. PROPORTIONALITY
In this paper, we view the task of diversification as finding a proportional representation for a document ranking. In this section, we will explain the notion of proportionality as well as describing an effectiveness measure for it.
3.1 Definition of Proportionality
Let T = {t1, t2, ..., tn} indicate a set of aspects for a query q and D denote a large set of documents related to q. Let pi indicate the popularity of the aspect ti  T , which is the percentage of documents in D covering this aspect. Additionally, let S be any subset of D. We define S to be proportional to D, or a proportional representation of D, with respect to T if and only if the number of documents in S that is relevant to each of the aspects ti  T is proportional to its popularity pi.
Let us revisit the example in Section 1, in which there are 90% of documents in D about the "java" programming language and the rest 10% is about an island named "java". Let {x, y} denote any subset of D with x documents about programming and y documents about the island. In this case, {9, 1} is proportional and thus is a proportional representation of D. While {8, 2} is not proportional, it is more proportional than {7, 3}.
Let R indicate a ranking of documents in D and S now represent a sub-ranking of R. We define S to be proportional to R if the subset of documents S provides at every rank is a proportional representation of D.
3.2 Effectiveness Measure
This notion of proportionality is, in fact, frequently used in evaluating the outcome of elections in which seats are assigned to members of competing political parties. This problem can be stated as follows. We have a limited number of seats in the parliament and a number of competing parties. Each party has its own members. Through election

66

campaigns, each party obtains a number of votes from people around the country. The goal is to assign members of different parties to the seats such that the number of seats each party gets is proportional to the votes it receives.
Several metrics have been proposed to measure such proportionality. Most of them are based on the difference between the percentage of votes each party receives and the percentage of seats it gets. Among those, the least square index (LSq) [17] is one of the standard metrics for measuring dis-proportionality:

LSq =

1 2

(vi - si)2 

(vi - si)2

i

i

where vi and si are the percentage of votes and the percentage of seats the i-th party received. Let us illustrate this with an example in which we have ten seats and three competing parties, namely A, B and C. Let us assume both A and B receive 50% of the votes and C gets 0%. Clearly, the proportional assignment which provides A and B each with five seats and C with none will result in LSq = 0. The value for LSq will increase when the seat assignment becomes more disproportional.
We will now turn our attention to the proportionality of a retrieved list of ten documents for the query "satellite", which we assume to have two aspects: "satellite internet" and "satelilte phone" with equal popularity of 50%. Due to the possible presence of non-relevant documents, we have to create a third "aspect" to account for non-relevant documents. As a result, proportionality requires this list to contain five relevant documents for each of the two aspects and zero documents for the "non-relevant" aspect. This situation seems to be very similar to the election described above. Unfortunately, we cannot apply LSq to measure the dis-proportionality of this result list due to two differences.
First, each member typically belongs to exactly one political party. As a result, one party gets more seats than it should always indicates that some other party is getting less than they deserve. A document, however, might be related to multiple aspects of a query. It then is possible that an aspect can be "rewarded" with additional documents while others still have as many relevant documents as they deserve.
Second, it is equally bad for any party to get any more seats than it should. In our case, however, selecting for the result list a document that is relevant to an aspect that already has enough relevant documents in the list is not as bad as selecting a non-relevant document.
Taking both differences into consideration, we argue that LSq, since is designed for the seat allocation problem, puts too much penalty on overly representing query aspects. LSq fails to recognize that some of these situations do not create any undesirable consequences in our setting, and thus should not be penalized. Therefore, we propose a new metric, disproportionality at rank K, calculated as follows:

DP @K =

ci(vi

-

si)2

+

1 2

n2N

R

(1)

aspect ti

where vi is the number of relevant documents that the aspect ti should have, si is the number of relevant documents the system actually found for this aspect, nNR is the number of non-relevant documents, and

ci =

1 0

vi  si otherwise

Formula (1) has two important properties. The first is that it penalizes a result set for under-representing any aspect of the query (si < vi) but not for over-representing them (si > vi), which addresses the first issue associated with LSq. The second is that while the over-representation of a query aspect is not penalized, the over-representation of the "non-relevant" aspect (nNR > 0) is, which overcomes the second issue associated with LSq.
A perfectly disproportional set of documents in the context of information retrieval would be a set with all nonrelevant documents. Thus, the Ideal-DP is given by:

Ideal DP @K =

vi2

+

1K2 2

aspect ti

The last step is to derive our proportionality measure by normalizing the DP score with Ideal-DP in order to make it comparable across queries:

P

R@K

=

1

-

DP @K IDeal DP @K

Finally, the Cumulative Proportionality (CPR) measure for rankings is calculated as follows:

CP R@K = 1

K
P R@i

K

i=1

4. PROPOSED METHOD
In this section, we first introduce the Sainte-Lagu®e method, a standard technique for finding proportional representations that is used to solve the seat allocation problem described in Section 3.2. We then demonstrate the analogy between this problem and our problem of propotionalitybased diversification, which helps us derive our technique from the Sainte-Lagu®e method.

4.1 The Sainte-LaguÎ Method
This method considers all of the available seats iteratively. For each of them, it computes a quotient for all of the parties based on the votes they receive and the number of seats they have taken. This seat is then assigned to the party with the largest quotient, which helps maintain the overall proportionality. We assume the selected party will then assign one of its members to this seat. Finally, it increases the number of seats assigned to the chosen party by one. The process repeats until all seats are assigned. Pseudo code for this procedure is provided as Algorithm 1. In this procedure, P = {P1, P2, ..., Pn} is the set of parties and Mi = {m(1i), m(2i), ..., m(lii)} is the set of members of the party Pi. vi and si indicate the number of votes Pi receives and the number of seats that have been assigned to Pi so far.

Algorithm 1 The Sainte-Lagu®e method for seat allocation

1: si  0, i

2: for all available seats in the parliament do

3: for all parties Pi do

4:

quotient[i]

=

vi 2si +1

5: end for

6: 7:

km argthme abxeisqtumoteimenbte[ri]of Pk

8: Assign the current seat to m

9: Mk  Mk \ {m} 10: sk  sk + 1 11: end for

67

4.2 Diversity by Proportionality

4.2.1 Framework
Let q indicate the user query, T = {t1, t2, ..., tn} indicate the aspects for q whose popularity is {p1, p2, ..., pn}. In addition, let R = {d1, d2, ..., dm} be the ranked list of documents returned by an initial retrieval and P (di|tj) indicate some estimate of the probability that the document di is relevant to the aspect tj. The task is to select a subset of R to form a diverse ranked list S of size k.
As mentioned earlier, existing techniques [1, 26] generally favor an S with smaller redundancy. Our idea, on the other hand, is to favor an S with higher proportionality. The optimal S, consequently, is a ranked list in which the number of relevant documents for each of the aspects ti is proportional to its popularity pi. This objective is, in fact, very similar to that of the seat allocation problem above. As a result, we derive a general proportionality framework for diversification directly from the procedure presented above, which is described as Algorithm 2.
This framework can be explained as follows. We start with a ranked list S with k empty seats. For each of these seats, we compute the quotient for each aspect ti following the Sainte-Lagu®e formula. We then assign this seat to the aspect ti with the largest quotient, which marks this seat as a place holder for a document about the aspect ti . After that, we need to employ some mechanism to select the actual document with respect to ti to fill this seat. Depending on that mechanism, we then need to update the number of seats occupied by each of the aspects ti accordingly. This process repeats until we get k documents for S or we are out of candidate documents. The order in which each document is put into S determines its ranking. Assuming each document selected for ti is truly relevant to ti, the SainteLagu®e method guarantees proportionality in the final set of documents.
Different choices of document selection mechanisms, which subsequently determine the choices of seat occupation update procedures, will result in different instantiations of our framework. We now present two such instantiations.

Algorithm 2 A Proportionality Framework

1: si  0, i

2: for all available seats in the ranked list S do

3: for all aspects ti  T do

4:

quotient[i]

=

vi 2si +1

5: end for

6: 7:

i d

 

arg maxi find the

quotient[i] best document

with

respect

to

ti

8: S  S  {d}

9: update si, i accordingly

10: end for

4.2.2 A Naive Adaptation
We first present a straightforward adaptation from the seat allocation problem above. The Sainte-Lagu®e method assumes that each member belongs to exactly one party. When a member is assigned to a certain seat, the party naturally takes up the entire seat. Directly applying this technique to our context means assuming each document is associated with a single aspect. As such, we have to determine the aspect for each of the documents dj  R, which we

assume to be the aspect ti  T to which dj is most relevant:
arg max P (dj|ti)
ti T
As a result, we construct for each aspect ti a list of documents associated with it in decreasing order of relevance, noted as Mi = {d(1i), d(2i), ..., d(lii)} where li is the number of documents in Mi. It follows naturally that the best document for an aspect ti is the first in the list Mi. We refer to this native adaptation as PM-1 and codify it as Algorithm 3.

Algorithm 3 PM-1

1: si  0, i

2: for all seats in the ranked list S do

3: 4:

for q

all aspects uotient[i] =

ti

T
vi

2si +1

do

5: end for

6: 7:

i d

 

arg maxi pop Mi

q

uotient[i]

8: S  S  {d}

9: si  si + 1 10: end for

4.2.3 A Probabilistic Interpretation
We now provide a probabilistic interpretation of the SainteLagu®e method, which removes the naive assumption that a document can only be associated with a single aspect. Instead, we assume all documents dj  D are relevant to all aspects ti  T , each with a probability P (dj|ti). This probabilistic interpretation, which we call PM-2, is described by Algorithm 4.
A first point to note is that PM-2 has a different mechanism for document selection. Once a seat is given to the aspect ti with the largest quotient, we need to assign to this seat a document that is relevant to ti . In the context of multi-aspect documents, however, among several documents all of which are relevant to ti , it is sensible to promote documents that may be slightly less relevant to ti but are at the same time relevant to other aspects, compared to those that are slightly more relevant to ti but are non-relevant to all others. This is, after all, what motivates diversification: we want more users to be able to find what they want. Therefore, PM-2 introduces the parameter :

d  arg max ◊qt[i]◊P (dj |ti )+(1-) qt[i]◊P (dj |ti)

dj R

i=i

that trades relevance to ti with relevance to more aspects. We abbreviate quotient[i] to qt[i] due to the space limitation.
A second point is that when a document d is selected for the current seat, since it is assumed to be relevant to all aspects ti  T , each aspect occupies a certain "portion" of this seat as opposed to a single aspect taking up the entire seat as previously. Intuitively, the degree of occupation of the seat is proportional to the normalized relevance to d:

si  si +

P (d|ti) tj T P (d|tj )

where si is the "number", which is now better regarded as "portion", of seats occupied by ti.
PM-2 can be summarized as a two-step procedure as follows. For each of the k seats in S, it first employs the SainteLagu®e formula to determine which aspect this seat should go to in order to best maintain the proportionality. Then,

68

Algorithm 4 PM-2

1: si  0, i

2: for all seats in the ranked list S do

3: for all aspects ti  T do

4:

quotient[i]

=

vi 2si +1

5: end for

6: 7:

i d

 

aarrggmmaaxxdi jquRotien◊t[iq]uotient[i

]

◊

P

(dj

|ti

)

+

(1

-

)

i=i quotient[i] ◊ P (dj |ti)

8: S  S  {d}

9: R  R \ {d}

10: for all aspects ti  T do

11:

si  si +

P (d|ti) tj P (d|tj )

12:

Since d is assumed relevant to all aspects, each of these aspects will take up a certain "portion" of this seat

13: end for

14: end for

it selects the document that, in addition to being relevant to this aspect, is relevant to other aspects as well. Finally, it updates the "portion" of seats in S occupied by each of the aspects ti according to how relevant it is to the selected document.

5. EXPERIMENTAL SETUP

Query and Retrieval Collection. Our query set consists of 98 queries, 50 of which are from the diversity task of the TREC 2009 Web Track (WT-2009) [10] and the other 48 are from TREC 2010 Web Track (WT-2010) [11]. Our evaluation is done on the ClueWeb09 Category B retrieval collection2, which is also used in both WT-2009 and WT2010. This collection contains approximately 50 million web pages in English. During query and indexing time, both the query and the collection are stemmed using the Porter stemmer. In addition, we perform stopword removal using the standard INQUERY stopword list.

Baseline Retrieval Model. We use the standard querylikelihood model within the language modeling framework [14] to conduct the initial retrieval run. This run serves both as a mean to provide a set of documents for the diversity models to diversify and a baseline to verify their usefulness. We also use this model as the estimate of relevance P (dj|ti) between the document dj and the aspect ti.
Spam filtering is known to be an important component of web retrieval [3]. Following Bendersky et al. [3], we use a spam filtering technique as described by Cormack et al. [12] with the publicly available Waterloo Spam Ranking for the ClueWeb09 dataset, which assigns a "spamminess" percentile S(d) to each document d in the collection. In particular, let p(di|q) indicate the score the retrieval model assigns to the document di, the final score of di is given by:

P (di|q) =

p(di|q) -

if S(di)  60 otherwise

Diversity Models. We evaluate PM-2, the proportionalityaware model we propose for search result diversification. In addition, we will also present results obtained by PM-1 for comparison. Our first diversity baseline model for comparison is MMR [4], which is considered standard in the diversity literature. Since the explicit approach for diversification is
2http://boston.lti.cs.cmu.edu/Data/clueweb09/

generally superior to the implicit approach, we also compare our models to xQuAD, which has been demonstrated to outperform many others in this class [26]. In fact, xQuAD is among the top performers in both diversity tasks of TREC 2009 and TREC 2010 [10, 11]. In addition to these two baselines, we also compare our results to those published by TREC whenever appropriate.
Experiment Design. We use Lemur/Indri 3 to conduct the baseline query-likelihood retrieval run with the toolkit's default parameter configuration. All of the diversification approaches under evaluation are applied on the top-K retrieved documents. All of these models except for PM-1 has a single parameter  to tune. Readers should refer to the original papers [4, 26] for the interpretation of this parameter in the respective models. We consider for  values in the range {0.05, 0.1, 0.15, ..., 1.0}. Our two-fold cross validation enforces complete separation between tuning and testing. In particular, each system is tuned on WT-2009 and tested on WT-2010 and vice versa. We present the result averaged across two folds unless stated otherwise. PM-1, since it is parameter-free, has no tuning involved.
As for the parameter K, we tested K  {50, 100, 500, 1000} and found that all four models achieved their best at K = 50. Therefore, all results presented here are achieved with K = 50.
Evaluation Metric. We first report our results using CPR, the proportionality metric we propose in Section 3. Since this metric certainly favors our models as they are designed to capture proportionality in the search results, we also report the results of several standard metrics that existing work was designed to optimize. This includes those used in the official evaluation of the diversity task WT-2010 [11]: -NDCG [7], ERR-IA (a variant of ERR [6]) and NRBP [8]. These measures penalize redundancy at each position in the ranked list based on how much of that information the user has seen and how likely it is that the user is willing to scan down to that position. In addition, we also report our results using Precision-IA [1] and subtopic recall, which indicate respectively the precision across all aspects of the query and how many of those aspects are covered in the results. Last but not least, all of these measures are computed using the top 20 documents each model retrieves also to be consistent with the official TREC evaluation [10, 11].
3http://www.lemurproject.org

69

Query Aspects. Explicit approaches such as xQuAD and PM-2 assume the availability of the query aspects and their popularity. We first consider the official sub-topics identified by TREC's assessors for each of the queries as its aspects. This simulates the situation where we know exactly what aspects the query has and provides a controlled environment to study the effectiveness of different diversification approaches. As for the aspect popularity, since it is not available in TREC's judgment data, we assume uniform probability for all aspects, which is also consistent with existing work [26, 27, 28].
In order to simulate more practical settings in which we do not know but have to guess the aspects of the query, we follow Santos et al. [26] by adopting suggestions provided by a commercial search engine as aspect representations. However, the search engine is unable to provide suggestions for four of the queries in our set. As a result, these experiments are conducted on the subset of 94 queries for which we can obtain aspect representation. We also assume uniform aspect distribution since it was demonstrated to be the most helpful [26].
It is worth noting that the aspects obtained from the search engine certainly do not completely align with the judged aspects provided by TREC assessors. In other words, there will be overlap between the two sets but there will also be generated aspects that are not in the judged set. We will refer to this problem as the misalignment between different sets of aspects and we do not attempt to evaluate the relevance of these misaligned aspects (those that are not in the judged set) in this paper.
6. RESULTS
6.1 Proportionality
In this section, we evaluate how well different methods maintain proportionality in the search results using both TREC sub-topics and suggestions from a commercial search engine as aspect descriptions. Table 1 shows the Cumulative Proportionality score for each system as well as the Win/Loss ratio ≠ the number queries each system improves and hurts respectively. The letters Q, M, X and P indicate statistically significant differences (p-value < 0.05) to Query-likelihood, MMR, xQuAD and PM-1 respectively.
From Table 1, we first notice that although all diversity models are able to provide improvement over the initial retrieval, the magnitude of improvement is very different. The improvement from MMR, for example, is insignificant while the improvement from the other three is more substantial.
Among the four diversity models, PM-2 outperforms all others on both sets of aspects with statistical significance, which demonstrates the effectiveness of our method at capturing proportionality. MMR is the least effective since it is completely unaware of the query aspects, and thus is unable to capture the proportionality among them. xQuAD, on the other hand, does take into account the query aspects to penalize redundancy. For each document selected, xQuAD downweights each of the aspects based on the degree of its relevance to the selected document so that the aspects that have less relevant documents will have higher priority in the next round. Further details about xQuAD can be found in [26]. Hence, xQuAD indeed has the effect of implicitly promoting proportionality, which explains why it significantly outperforms query-likelihood, and also MMR on one of the

Table 1: Performance of all techniques in CPR. The letters Q, M , X and P indicate statistically significant differences to Query-likelihood, MMR, xQuAD and PM-1 respectively (p-value < 0.05).

CPR

Win/Loss

Suggestions Sub-topics

Query-likelihood MMR xQuAD PM-1
PM-2
Query-likelihood MMR xQuAD PM-1
PM-2

0.4012 0.4018 0.4534Q,M 0.4462Q,M 0.4771X,P
Q,M
0.3977 0.4016 0.4242Q 0.4067 0.4696X Q,,M P

36/32 48/31 46/34 49/33
36/26 41/29 34/40 48/31

aspect sets. PM-2, despite the conceptual difference, can be explained using xQuAD's framework of reweighting aspects as well. From this perspective, the biggest difference between the two is that PM-2 uses a more proportionality aware aspect weighting function which is based on the Sainte-Lagu®e algorithm. This result indeed confirms the effectiveness of this proportionality-driven aspect weighting function.
It is worth noting that PM-1, despite being a parameterfree naive version of PM-2, is comparable to xQuAD. There is no statistically significant difference between these two. Comparing PM-1 to PM-2, however, reveals the weakness of its naive assumption. PM-1 associates each of the documents with exactly one aspect, thus it has the risk of associating documents with the wrong aspects. In addition, PM-1 fails to promote documents relevant to multiple aspects. Both of these account for its inferiority to PM-2 with both sets of aspects.
6.2 Standard Redundancy-based Metrics
We now compare our proposed techniques to MMR and xQuAD using standard metrics from the diversity literature as mentioned earlier. Instead of showing the results averaged across two folds, we show the results obtained in each fold separately so that we can compare our results with the official results from TREC. It should be noted that the comparison between our results and those from TREC should be taken as indicative only, since our systems and theirs use different initial retrieval run. Table 2 shows the results for all of the techniques we studied as well as the best performing system on ClueWeb09 Category B reported by TREC. In addition to the scores in each metric, Table 2 also presents the Win/Loss ratio each system achieves over the query likelihood baseline in terms of -NDCG.
The first observation from Table 2 is that all systems perform worse in all metrics on WT-2009 than they do on WT2010. The effectiveness of all systems certainly depends on the quality of documents retrieved by the initial retrieval run. Since all of our systems rerank the top 50 returned documents for each query, we examine these documents in both precision-IA and sub-topic recall. The former indicates how many relevant documents for each aspect we have for reranking and the latter indicates how many of the aspects for which we have relevant documents. The results are shown in Table 3, which suggests that the top 50 documents for queries in WT-2009 cover less topics (i.e. many sub-

70

Table 2: Performance of all techniques in several standard redundancy-based measures. The Win/Loss

ratio is with respect to -NDCG. The letters Q, M , X and P indicate statistically significant differences to

Query-likelihood, MMR, xQuAD and PM-1 respectively (p-value < 0.05).

-NDCG

Win/Loss ERR-IA

Prec-IA S-Recall

NRBP

Suggestions Sub-topics

Query-likelihood MMR xQuAD PM-1
PM-2

0.2979
0.2963
0.3300Q,M 0.3076 0.3473P

WT-2009
16/19 23/15 18/17 19/19

0.1953
0.1922
0.2207Q,M 0.2027 0.2407P

0.1146 0.1221
0.1190 0.1140 0.1197

0.4327 0.4447 0.4700
0.4440 0.4633

0.1689 0.1657 0.1950Q,M 0.1738
0.2172

Query-likelihood MMR xQuAD PM-1 PM-2

0.2875 0.2926 0.2995 0.2870
0.3200

16/15 14/19 16/18 17/19

0.1895 0.1919 0.1973 0.1830
0.2139

0.1095
0.1108
0.1089 0.0929X 0.1123P

0.4212 0.4351 0.4403 0.4111
0.4472

0.1634 0.1655 0.1700 0.1560
0.1884

WT-2009 Best (uogTrDYCcsB) [10] 0.3081

N/A

0.1922

N/A

N/A

0.1617

Suggestions Sub-topics

Query-likelihood MMR xQuAD PM-1
PM-2
Query-likelihood MMR xQuAD PM-1 PM-2
WT-2010 Best (uogTrB67xS) [11]

0.3236 0.3349Q 0.4074Q,M 0.4323X Q,M 0.4546X,P
Q,M
0.3268 0.3361Q 0.3582Q,M 0.3664X 0.4374X,P
Q,M
0.4178

WT-2010
19/14 29/14 32/13 34/10
17/14 31/6 25/15 33/10 N/A

0.2081 0.2161 0.2671Q,M 0.3071X Q,M 0.3271X
Q,M
0.2131 0.2206 0.2372Q,M 0.2409 0.3087X,P
Q,M
0.2980

0.1713 0.1740 0.2028 0.1827
0.2030
0.1730 0.1746 0.1785 0.1654 0.1841
N/A

0.5479 0.5694Q 0.6410Q,M 0.6323Q,M
0.6503Q,M
0.5355 0.5507 0.5775Q 0.5996 0.6279X Q,M
N/A

0.1656 0.1750 0.2206Q,M 0.2654X Q,M 0.289X
Q,M
0.1722 0.1819 0.1964Q 0.1952 0.2690X,P
Q,M
0.2616

Table 3: Quality of the baseline run for the WT-

2009 and WT-2010 query sets in sub-topic recall and

precision-IA.

S-Recall@50 Prec-IA@50

WT-2009 0.54

0.0821

WT-2010 0.7003

0.1486

topics do not get any relevant documents) and also contain considerably less relevant documents for each of the topics than WT-2010. Therefore, there is far less room for improvement on WT-2009 than there is on WT-2010, which leads to the fact that all systems perform better on WT-2010.
Regarding the comparison among diversification techniques, we see a very similar trend as in the previous case with proportionality. In particular, MMR is least effective method due to its lack of awareness of the query aspects. PM-2, on the other hand, outperforms all other methods in almost all metrics with statistically significant improvement in many cases. PM-2 with automatically generated aspects even outperforms the best performing system in TREC evaluation. It should be noted that the best performing system in TREC 2010 of which results we report also use suggestions generated by a search engine as aspect descriptions. This further confirms the effectiveness of PM-2: it provides results with not only a higher degree of proportionality but also a lower degree of redundancy.
6.3 Improvement Analysis
The analyses in this section are conducted on the entire query set as there is no need to consider WT-2009 and WT2010 separately. In addition, we only present our analyses with the manually generated set of aspects because we have

similar findings with the other set, only to a slightly lesser extent due to the aspect misalignment problem.
We are interested in two aspects of the improvement each technique provides over the initial retrieval: (1) the robustness [21] of the improvement, and (2) the reasons that account for this improvement. Robustness refers to the number of queries each technique improves and hurts together with the magnitude of the performance change. To understand the robustness of each model, we provide a more detailed view of the Win/Loss ratio that was provided earlier in Table 1 and Table 2. In particular, instead of showing how many queries each system improves and hurts over the entire query set, we now look at these numbers with respect to the percentage of the improvement. The histogram in Fig. 1 shows, for various ranges of relative increases (positive ranges) and decreases (negative ranges) in CPR and -NDCG, the number of queries improved and hurt with respect to the query likelihood baseline.
It can be seen from Fig. 1 that most of the performance changes resulting from using MMR is in the two low ranges -[0%,25%] and +[0%, 25%], which indicates that MMR rarely improves or hurts a query drastically. Combined with the fact that it helps and hurts about the same number of queries (35/33), MMR can only provide slight improvement over the baseline.
In contrast, PM-2 and xQuAD provide substantial improvement (> +100%) for several queries. In addition, compared to MMR, these two models hurt about the same number of queries but they improve many more. As a result, PM-2 and xQuAD significantly outperform MMR in most cases. Comparing PM-2 and xQuAD, although they help and hurt about the same number of queries, PM-2 has a much larger magnitude of improvement. This is demon-

71

Table 4: CPR and -NDCG breakdown by ranges

of accuracy of P (dj|ti).

Acc.P (dj |ti)

[0,0.1) [0.1, 0.2) [0.2, 0.3) [0.3, 1.0]

#q

28

21

26

23

CPR

QL MMR xQuAD PM-1 PM-2

0.1049 -0.0142 +0.0062 +0.0028 +0.0193

0.4152 +0.0249 +0.0326 +0.0286 +0.0613

0.5296 -0.0015 +0.0666 +0.0500 +0.0903

0.6040 -0.0013 +0.1097 +0.1059 +0.1417

-NDCG

QL MMR xQuAD PM-1 PM-2

0.07 -0.0079 +0.0202 +0.0212 +0.0288

0.3078 +0.0146 +0.0509 +0.0221 +0.0616

0.4143 +0.0133 +0.0768 +0.0677 +0.1189

0.4885 +0.0013 +0.0863 +0.1252 +0.1548

Figure 1: Robustness of all techniques with respect to the baseline query-likelihood.
strated through Fig. 1 with the fact that PM-2 has more queries in the highest range (>+100%).
The effectiveness of each model depends on two factors: the quality of the initial retrieved set of documents and the model's power to select a diverse subset from that pool of documents. Since all models operate on the same pool, the former factor becomes irrelevant. As for the model power, the key component of both our method and xQuAD is the query likelihood estimate of relevance P (dj|ti) between an aspect ti and a document dj. While xQuAD uses P (dj|ti) to penalize redundancy at every rank, PM-2 uses it to accommodate proportionality. Intuitively, the more accurate P (dj|ti) is at telling which document is relevant to which of the aspects of the query, the more diverse the final ranked list will be. In this experiment, we study how well these techniques perform at different level of accuracy P (dj|ti) provides.
In order to quantify the accuracy of P (dj|ti), we do as follows. Let D be the set of top 50 documents returned for the query q, which has a set of aspects {t1, t2, ..., tn}. We rank all documents dj  D for each of the aspects ti with P (dj|ti) and record the NDCG score. We then use the average of NDCG across all aspects as the measure of accuracy of P (dj|ti). Table 4 presents the absolute improvement each model has over the baseline (in both CPR and -NDCG separately) on different ranges of accuracy of P (dj|ti). We

also show the number queries and how the baseline query likelihood performs in each of these ranges.
Since MMR does not use P (dj|ti), its performance obviously does not correlate with the accuracy of P (dj|ti). PM2 consistently provides larger improvement than xQuAD across all ranges of accuracy and metrics. In addition, the gap between the improvement in both CPR and -NDCG of PM-2 and xQuAD is generally larger as the accuracy of P (dj|ti) increases. This clearly indicates using P (dj|ti) to optimize proportionality is much more effective than to minimize redundancy, which explains the all-round superiority of PM-2.
In summary, we have demonstrated that MMR is the least effective because it helps and hurts about the same number of queries. PM-2 and xQuAD both help more queries than they hurt, but PM-2 is able to provide substantially larger improvement over the baseline than xQuAD, helping PM-2 to be statistically significantly better than xQuAD even though they both outperform MMR and the baseline. The reason for PM-2's superiority over xQuAD is that PM-2 uses P (dj|ti) to accommodate proportionality at every rank, which is more effective than using it to penalize redundancy. The results obtained with PM-2 contain not only a higher degree of proportionality but also a lower degree of redundancy.
6.4 Failure Analysis
Our techniques sequentially go over all "seats" in the result ranked list and decide for each of them which aspect it should go to. After the aspect is determined, PM-1 simply chooses the best document for this aspect according to P (dj|ti) while PM-2 might promote documents that are slightly less relevant to this aspect but relevant to other aspects as well.
The problem arises when the initial retrieval fails to find relevant documents for some of the aspects. When a "seat" is assigned to an aspect without relevant documents, P (dj|ti) will mistakenly provide some false positive non-relevant documents to fill in that seat, leading to undesirable results. In this section, we will investigate this effect.
Sub-topic recall of the baseline run is certainly the best metric for studying the effect of coverage. Table 5 shows how different systems behave on different ranges of sub-topic recall of the top 50 documents retrieved by the baseline run. For each of the sub-topic recall ranges, Table 5 provides the percentage of queries that each system helps and hurts (marked as "%Q+" and "%Q-" respectively) together with

72

the its relative improvement ("%Imp.") in both CPR and NDCG with respect to the baseline. We also show for each range the number of queries as well as the performance of the baseline for references.
We first examine the percentage of queries helped and hurt by each system. At the low recall range ([0,0.5)), both PM-1 and PM-2 hurt more queries than they improve. As the recall goes up, these numbers improve. This trend is especially clear with -NDCG. This clearly demonstrates the effect sub-topic recall has on our techniques.
xQuAD by its nature does not have the same problem. As a result, the negative effect of low subtopic recall on xQuAD is smaller than it is on our methods: xQuAD has better Win/Loss ratios than both PM-1 and PM-2 on low ([0,0.5)) and medium ([0.5,0.75)) recall range. This helps further explain what we saw earlier in Table 2: although the same techniques perform worse on WT-2009 than they do on WT-2010, PM-1 and PM-2 are the ones with the largest performance difference in terms of Win/Loss ratio. The reason is the subtopic recall of the baseline for WT-2009 is considerably lower than that for WT-2010 (as demonstrated previously in Table 3), which affects our systems the most.
MMR despite not having this problem, it hurts about the same number of queries as our techniques in the low and medium recall ranges due to its overall ineffectiveness. In addition, it helps significantly less queries compared to ours.
With respect to the relative improvement over the baseline, even though xQuAD has better Win/Loss ratios than PM-2 on the low and medium recall ranges, PM-2 still manages to provide larger improvement than xQuAD. Additionally, the gap between the two models becomes substantially larger in the high recall range. This provides additional evidences to support the effectiveness of PM-2.
In summary, even though our proportionality-aware method PM-2 is very effective overall, it depends critically on the coverage of the baseline run.
6.5 Discussion: On Noisy Aspect Descriptions
Given that automatically generated aspects can be helpful for diversification, it is important to know how to generate them. Using query suggestions from commercial search engines in effect is using a "black box" for this important component. Hence, this section aims to provide a preliminary discussion on whether we can use aspects generated by existing work in query suggestion and reformulation for diversification.
While most of those reformulation techniques focus on making user queries more effective [2, 18, 24, 20, 15], some aim to generate reformulations that cover different aspects of the original query [16]. We have adapted these techniques [16] to generate a set of clusters for each of our queries, where each cluster is assumed to represent an aspect of the original query. Details can be found in [16]. We concatenate all queries in each cluster to form a "document", from which we then construct a language model. Finally, we use Indri's weighted query representation of this model as the aspect description and the frequency of the cluster as the popularity of the aspect. The resulting query set consists of 77 queries for which the reformulation technique can provide clusters.
We now re-evaluate all of our techniques using this set of aspects. The results are presented in Table 6. Interestingly, we observe that the performance of xQuAD, PM-1 and PM2 is substantially lower than with the previous two aspect

sets. We observe that this set of aspects, in comparison with the set obtained from the search engine, contains (1) considerably less of the TREC sub-topics and more of other aspects that are not identified by TREC's assesors, and also (2) some unclear aspect descriptions. The low performance of all systems, in fact, clearly demonstrates the aspect misalignment issue and the noisiness of this set.
With these noisy aspects, PM-2 is still better than xQuAD with most of the metrics. The performance of xQuAD is, in fact, even lower than that of the query likelihood baseline in both CPR and -NDCG. PM-2 still manages to provide improvement, but it is more comparable to MMR. Interestingly, PM-1 is now the best performing approach.
To conclude, aspects generated by the current query reformulation technique are generally not very "effective" for diversification. This notion of "effectiveness", however, has to be taken with care. We have been penalizing all systems for finding documents for aspects that are different to TREC sub-topics. In practice, these unjudged aspects might be relevant to the query as well. This raises the question of how reliable the current evaluation paradigm is that relies on pre-defining a fixed set of aspects for each queries. We will investigate this issue in future work.
7. CONCLUSIONS AND FUTURE WORK
In this paper, we present a different perspective on search result diversification: diversity by proportionality. We consider a result list to be more diverse if the number of documents relevant to each of the aspects is proportional to the overall popularity of that aspect. We then propose Cumulative Proportionality (CPR), an effectiveness measure for proportionality which is based on metrics commonly used for evaluating outcomes of elections. Motivated by the Sainte-Lagu®e method for assigning seats in a parliament to members of competing political parties, we also present a proportionality-driven framework for diversification. It sequentially determines for each of the "seats" in the result list the aspect that best maintains the overall proportionality with respect to the previously selected topics. It then determines for this seat the best document with respect to that topic. Using this framework, we derive PM-1 ≠ a naive adaptation of the seat allocation mechanism, from which we then develop the probabilistic interpretation, which we called PM-2.
Our results have demonstrated that, with both manually and automatically generated aspect descriptions, PM-2 is statistically significantly better than the top performing redundancy-based technique not only in CPR, but also on several other standard redundancy-based measures. This indicates that promoting proportionality will result in minimal redundancy, as desired by the current standard in diversity.
For future work, we will compare the aspects generated by existing reformulation techniques to the TREC sub-topics in order to quantify the aspect misalignment problem. If many of these misaligned aspects are indeed sensible, we might have to re-examine if predefining a set of topics for each query is a valid strategy for evaluating diversification techniques. In addition, Santos et al. has pointed out that learning to dynamically provide different diversification strategies for different queries based on how ambiguous they are [27] and what intent they have [28] significantly improves the performance of xQuAD. We plan to investigate these approaches since they are potentially beneficial for our model.

73

Table 5: Performance breakdown by S-Recall of the initially retrieved documents. "%Q+" and "%Q-" indicate

respectively the percentage of queries helped and hurt by each technique. "%Imp." indicates the relative

improvement of each technique over the baseline query likelihood (QL).

S-Recall Ranges

[0,0.5)

[0.5,0.75)

[0.75,1.0]

#Queries

39

27

32

%Q+ %Q- %Imp. %Q+ %Q- %Imp. %Q+ %Q- %Imp.

CPR

QL (CPR) MMR xQuAD PM-1 PM-2

0.2504 10% 41% 26% 36% 23% 41% 23% 41%

-6% -6% -5%
0%

0.4689 56% 26% +4% 74% 19% +15% 63% 30% +6% 67% 30% +18%

0.5279 53% 28% +1% 56% 38% +22% 63% 31% +25% 69% 28% +31%

-NDCG

QL (-NDCG) MMR xQuAD PM-1 PM-2

0.1610 13% 38% 31% 33% 31% 33% 28% 36%

+5% +6% +5% +8%

0.3791 48% 33% +3% 74% 22% +19% 56% 37% +7% 67% 30% +24%

0.4349 53% 28% +3% 63% 31% +24% 72% 22% +34% 75% 22% +42%

Table 6: Performance of all techniques with noisy aspect descriptions. Q, M and X indicate significant

difference to Query-Likelihood, MMR and xQuAD respectively.

CPR -NDCG ERR-IA Prec-IA S-Recall NRBP

Query-likelihood 0.3669 0.2637

0.1644

0.1113

0.4107 0.1332

MMR xQuAD PM-1 PM-2

0.3824 0.3598 0.3943
0.3703

0.2769 0.2601
0.2944X 0.2828

0.1722 0.1620
0.1961X 0.1888

0.1353Q 0.1169M 0.1306
0.1157M

0.4450
0.4052 0.4189 0.4010

0.1387 0.1299
0.1685X 0.1641

8. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval, and in part by Vietnam Education Foundation. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
9. REFERENCES [1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of WSDM, pages 5-14, 2009. [2] R. Baeza-Yates, C. Hurtado and M. Mendoza. Query recommendation using query logs in search engines. In The ClustWeb Workshop, pages 588-596, 2004. [3] M. Bendersky, D. Fisher, and W.B. Croft. UMass at TREC 2010 Web Track: Term dependence, spam filtering and quality bias. In Proceedings of TREC, 2010. [4] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings SIGIR, pages 335-336, 1998. [5] B. Carterette and P. Chandar. Probabilistic models of ranking novel documents for faceted topic retrieval. In Proceedings of CIKM, pages 1287-1296, 2009. [6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of CIKM, pages 621-630, 2009. [7] C.L.A. Clarke, M. Kolla, G.V. Cormack, O. Vechtomova, A. Ashkan, S. Buttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of SIGIR, pages 659-666, 2008. [8] C.L.A. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. In Proceedings of ICTIR, pages 188-199, 2009. [9] C.L.A. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A comparative analysis of cascade measures for novelty and diversity. In Proceedings of WSDM, pages 75-84, 2011. [10] C.L.A. Clarke, N. Craswell, and I. Soboroff. Overview of the
TREC 2009 Web track. In TREC, 2009. [11] C.L.A. Clarke, N. Craswell, I. Soboroff, and G.V. Cormack.
Overview of the TREC 2009 Web track. In TREC, 2009. [12] G.V. Cormack, M.D. Smucker, and C.L.A. Clarke. Efficient and
effective spam filtering and re-ranking for large web datasets. Apr 2010.

[13] N. Craswell, O. Zoeter, M.J. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In Proceedings of WSDM, pages 87-94, 2008.
[14] W.B. Croft, D. Metzler, and T. Strohman. Search Engines: Information Retrieval in Practice. Addison-Wesley, 2009.
[15] V. Dang and W.B. Croft. Query reformulation using anchor text. In Proceedings of WSDM, pages 41-50, 2010.
[16] V. Dang, X. Xue, and W.B. Croft. Inferring query aspects from reformulations using clustering. In Proceedings of CIKM, pages 2117-2120, 2011.
[17] M. Gallagher. Proportionality, disproportionality and electoral systems. In Electoral Studies, 10(1):33-51, 1991.
[18] R. Jones, B. Rey and O. Madani. Generating query substitutions. In Proceedings of WWW, pages 387-396, 2006.
[19] A. Lijphart. Electoral systems and party systems: A study of twenty-seven democracies, 1945-1990. Oxford University Press, 1994.
[20] Q. Mei, D. Zhou and K. Church. Query suggestion using hitting time. In Proceedings of CIKM, pages 469-477, 2008.
[21] D. Metzler and W.B. Croft. Latent concept expansion using markov random fields. In Proceedings of SIGIR, pages 311-318, 2007.
[22] D. Rafiei, K. Bharat and A. Shukia. Diversifying web search results. In Proceedings of WWW, page 781-790, 2010.
[23] F. Radlinski and S. Dumais. Improving personalized web search using result diversification. In Proceedings of SIGIR, pages 691-692, 2006.
[24] X. Wang and C. Zhai. Mining term association patterns from search logs for effective query reformulation. In Proceedings of CIKM, pages 479-488, 2008.
[25] J. Wang and J. Zhu. Portfolio theory of information retrieval. In Proceedings of SIGIR, pages 115-122, 2009.
[26] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proceedings of WWW, pages 881-890, 2010.
[27] R. L. T. Santos, C. Macdonald, and I. Ounis. Selectively diversifying web search results. In Proceedings of CIKM, pages 1179-1188, 2010.
[28] R. L. T. Santos, C. Macdonald, and I. Ounis. Intent-aware search result diversification. In Proceedings of SIGIR, pages 595-604, 2011.
[29] C. Zhai, W.W. Cohen, and J. Lafferty. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. In Proceedings of SIGIR, pages 10-17, 2003.

74

Explicit Relevance Models in Intent-Oriented Information Retrieval Diversification
Sa˙l Vargas, Pablo Castells and David Vallet
Universidad AutÛnoma de Madrid Escuela PolitÈcnica Superior, Departamento de IngenierÌa Inform·tica
{saul.vargas,pablo.castells,david.vallet}@uam.es

ABSTRACT
The intent-oriented search diversification methods developed in the field so far tend to build on generative views of the retrieval system to be diversified. Core algorithm components ≠in particular redundancy assessment≠ are expressed in terms of the probability to observe documents, rather than the probability that the documents be relevant. This has been sometimes described as a view considering the selection of a single document in the underlying task model. In this paper we propose an alternative formulation of aspect-based diversification algorithms which explicitly includes a formal relevance model. We develop means for the effective computation of the new formulation, and we test the resulting algorithm empirically. We report experiments on search and recommendation tasks showing competitive or better performance than the original diversification algorithms. The relevancebased formulation has further interesting properties, such as unifying two well-known state of the art algorithms into a single version. The relevance-based approach opens alternative possibilities for further formal connections and developments as natural extensions of the framework. We illustrate this by modeling tolerance to redundancy as an explicit configurable parameter, which can be set to better suit the characteristics of the IR task, or the evaluation metrics, as we illustrate empirically.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Search and Retrieval ≠ retrieval models
General Terms
Algorithms, Measurement, Performance, Experimentation, Theory
Keywords
Diversity, relevance models, language models, generative models
1. INTRODUCTION
The value of diversity as a fundamental dimension of information utility has started to be cared for in the Information Retrieval (IR) research community for over a decade [4]. Diversity-enhancement methods have been developed [1,7,21,27,29], diversity evaluation methodologies and metrics have been proposed [1,8,24,29], and a
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08...$15.00.

diversity task has been included in the latest TREC campaigns [9]. Theories of IR diversity build on a revision of the classic document independence assumption in IR: the marginal utility of a document indeed highly depends on ≠in general, decreases with≠ the relevance of the documents the user has previously seen. Considering this, IR systems' output diversification is posited as an effective strategy to cope with the uncertainty (ambiguity and/or incompleteness) involved in user queries, as imperfect expressions of true user needs. By trading diminishing marginal relevance for increased query aspect coverage, diversification strategies seek to maximize the range of users (the precise potential intents behind the query) who will get at least some relevant document, thereby improving the overall gain [1,7,10,21].
Two trends can be broadly distinguished in the diversification methods reported in the literature, based on whether or not they use an explicit representation of query intents [22]. Whereas intent-oriented methods include an explicit query aspect space in their formulation, implicit diversification schemes typically rely at some level on inter-document similarity, under the assumption that dissimilar documents should cover diverse query aspects. Interestingly, intent-implicit approaches generally build ≠as far as their formalization goes≠ on an explicit relevance model, often lending from the Probability Ranking Principle [7,27,29]. Whereas, in contrast, the intent-explicit methods tend to elaborate, in their problem statement, formalization, and algorithmic formulation, on generative views of the retrieval system to be diversified, where relevance is implicit [1,21,25]. Core algorithm components ≠in particular redundancy assessment≠ are expressed in such approaches in terms of the probability to observe documents, rather than the probability that the documents be relevant. This has been sometimes described as a view considering the selection of a single document in the underlying task model [28].
In this paper we propose an alternative formulation of aspectbased diversification algorithms which explicitly includes a formal relevance model. Our research has a theoretical motivation in seeking an alternative, nuanced understanding of the intentexplicit diversity problem, and the resulting variants in the formulation and development of aspect-based diversification methods. On the other hand, the explicit relevance approach has advantages of its own. We report experiments in search and recommendation tasks where the approach shows competitive or better performance than its original counterparts. Moreover, the relevancebased formulation opens the way for further extensions and elaborations with models involving an explicit representation of relevance. As a particular case, we show that the framework provides a sound basis for tuning redundancy penalization in a principled way, as a smooth consistent extension of the diversity model.
The rest of the paper is organized as follows. We briefly introduce and discuss intent-oriented diversification schemes in the next section, paying specific attention to their document-oriented gen-

75

erative formal foundation. We introduce our relevance-based revision of such schemes after that in section 3, including details for the development, estimation and computation of the components of our resulting diversification framework. We empirically test the effectiveness of our approach in section 4. Section 5 describes a formal extension of the proposed framework to model and adjust the diversification to different degrees of tolerance to redundancy. We discuss the results and implications of our approach in section 6, and conclude with a summary and final comments in section 7.

2. RELEVANCE MODELS VS. GENERATIVE MODELS
The different views on relevance implied by the Probability Ranking Principle (PRP) [20] and the Language Modeling (LM) approaches to IR [16] raised interest and debate in the research community by the turn of the past decade [23]. The absence of a clear explicit notion of relevance in the early LM formulations has been often considered to involve an underlying assumption of single relevant document selection [3]. Even though ≠or precisely because≠ there does not seem to be a unique common understanding on such issues in the field, and a unique view on whether or how LM actually capture relevance, we see theoretical ≠and potentially practical≠ interest in exploring new formulations of the diversity problem ≠and the derivation of algorithms thereupon≠ which explicitly build on the probability of relevance.
As stated in the introduction, we focus on approaches to diversity which are founded on an explicit representation of query intents. As two prototypical representative instances of this approach, we focus on IA-Select [1] and xQuAD [21] as the algorithms of reference for our study. Both have proved to be quite effective in IR diversity tasks, outperforming prior non-explicitly intentoriented approaches (see [22] for a comprehensive empirical analysis and comparisons). We start by briefly reviewing the formulation of these diversification schemes, and the fundamental principles on which they build, in order to contrast them later to a relevance-based alternative.
The definition of the IA-Select approach is developed in [1] around a quality component ( | , ) broadly defined as the likelihood that a document satisfies the query given the user intent . The diversification problem is stated as finding a subset of documents ≠of a given size≠ that maximizes the probability that at least one of them is relevant. This is formulated as finding the set that minimizes:

( | )= ( | ) 1- 1- ( | , )

(1)



While solving this problem is NP-Hard, the authors provide a practical approximation to the optimal solution by greedily reranking a baseline set of documents picking one document at a time that maximizes the following objective function:

( | , , ) = ( | ) ( | , ) 1 - ( | , ) (2)

where is the set of documents that were selected in the previous iterations. Agrawal et al generically describe ( | , ) as a term quantifying to what extent the document responds to the user need expressed by when the intended sense of the query is . The authors do not enforce a strict probabilistic rigor in the development of this component, and in fact omit an explicit specification, thus leaving its implementation somewhat open to different

realizations of the expressed principle. For their own experiments, the implementation of ( | , ) is hinted as the product of a baseline retrieval scoring function (the system to be diversified)
by the probability that belongs to category ≠which may be read as ( | ). No explicit probability of relevance is introduced in their development and ≠if we may indulge on a rather informal note≠ one might find in the ( | , ) notation some reminiscence of a probability distribution over documents.

xQuAD does have a quite precise probabilistic formulation [21]. It is stated and developed upon a document generative model formulation, whereby the resulting key terms of the algorithms reflect the probability to observe documents, rather than the explicit probability of the latter to be relevant. Specifically, the xQuAD scheme consists of a greedy algorithm with the same essential structure as IA-Select, but with a differently defined objective function:

( | , , ) = (1 - ) ( | ) +  ( , ¨ | )

(3)

That is, the objective is a linear combination of the probability to observe a document given the query (which may be understood as the baseline retrieval function), and the probability to observe the document given the query, assuming ≠strictly sticking to the probabilistic notation≠ no previously selected document had be observed given the query (which represents the marginal utility of the document). The marginal utility component is in turn developed by marginalizing over the set of query intents, which after some Bayesian derivations and mild conditional independence assumptions results in:1

( , ¨ | )  ( | ) ( | , ) 1 - ( | , )

Thus the xQuAD algorithm is formulated and developed upon (conditional) document distributions, as ultimately embodied in the ( | ) and ( | , ) components.
Therefore neither IA-Select, nor xQuAD, or other explicitly intent-oriented diversification schemes that we are aware of, include relevance as an explicit magnitude or random variable.

3. RELEVANCE-BASED DIVERSIFICATION ALGORITHMS
3.1 Relevance-Based IA-Select
Taking up from a more literal interpretation of Agrawal's initial problem statement, we investigate a revision of the IA-Select formulation by defining ( | , )  ( | , , ), where is the binary relevance random variable (and is used as a shorthand for
= 1). With this formulation, the objective function to be maximized in the problem statement (equation 1) then explicitly becomes:

( | )=

( | ) 1- 1- ( | , , )


1 The original version of xQuAD [21] would use ( | ) rather than ( | , ). Later publications [22] present however the latter form, which we have also found to work better in our experiments, whereby we favor it here. Our analysis and considerations apply just the same to both variants in any case.

76

And the marginal utility objective function for the greedy approximation algorithm (equation 2) gets then defined as:
( | , , ) = ( | ) ( | , , ) 1 - ( | , , ) (4)

In order to implement this formulation we need a means to estimate ( | , , ). Before addressing that, we formulate a similar revision of the xQuAD algorithm.

3.2 Relevance-Based xQuAD
For this algorithm we actually reconsider the own initial formula-
tion of the approach. Rather than expressing the objective function in terms of document probabilities as in equation 3, we define the
objective function on an explicit relevance model, as:

( | , , ) = (1 - ) ( | ) +  ( , ¨ | )

(5)

where means is relevant ≠that is, ( | )  ( | , )≠ and ¨ means no document in is relevant. Taking this starting
point, by similar steps to the original xQuAD, we derive:

( ,¨ | ) = ( | ) ( ,¨ | , )

= ( | ) ( | , , ) 1- ( | , , )

where we have assumed and are conditionally independent given and . Substituting all this in the objective function gives:
( | , , ) = (1 - ) ( | , ) +
+ ( | ) ( | , , ) 1- ( | , , )

It is easy to see that the above function becomes the relevancebased version of IA-Select (equation 4) when  = 1. We hence notably find that, on a relevance model foundation, xQuAD turns out to be a generalization of IA-Select. We shall thus deal with the two reformulations as a single approach henceforth.
We now turn to the problem of estimating the two models involved in the new objective function, so as to come up to an effectively computable form for the latter.

3.3 Aspectual Relevance Model
Two main components: the aspect model ( | ), and the relevance model ( | , , ), need to be estimated in order to compute the objective function ( | , , ) of our diversification approach.

First, for the relevance model, by applying Bayes' rule we have:

(| ,

, )=

(| , , ) (| , (| , )

)

The ( | , ) component can be obtained from the retrieval system, as we shall discuss later in the next subsection.

The marginalization of the conditional aspect distribution with respect to relevance gives ( | , ) = ( | , , ) ( | , ) +
( | , , ¨ ) 1 - ( | , ) . Therefore by substitution in the
numerator above, the relevance model can be rewritten as:

(| ,

, )=

(| ,

)-

( | , ,¨ ) 1- (| , )

(| ,

)

(6)

Now we assume that under non-relevance, aspects are independent from documents and queries, that is ( | , , ¨ )  ( |¨ ). In other words, given non-relevance, we consider there is no

particular relation the aspect is enforced to meet with respect to the query and the document ≠other than not forming a relevant tuple. Since most aspect / document / query tuples are not related in practice, as relevance is highly sparse, such negative condition can be considered as negligible. Comparable assumptions can be found in probabilistic developments in the literature (see e.g. [29]) ≠only with no aspect variable involved. Furthermore, we assume no particular bias between aspects towards relevance or nonrelevance, i.e. we approximate ( |¨ )  ( ). The aspect prior can be estimated from the coverage of aspects in the document collection, if the latter is considered as a fair representative sample space for users' information need intents. Or, in the absence of further information, the prior can be taken as uniform.

Now, ( | , ) for equation 6 can be approximated in different ways, depending on the nature of the aspect space and the available observations. When aspects consist of categorical data in such a way that ( | ) can be directly estimated from the data, we use the approach:

( | , )

( | ) ( | )/ ( ) ( | ) ( | )/ ( )

using Bayes' rule, marginalization over aspects (in the denominator), and assuming again conditional independence of documents and queries given a query aspect (if a uniform aspect prior is assumed, ( ) and ( ) further cancel out). In our experiments, we take this approximation when ODP is used as the aspect space for search diversification (see section 4.1). We use it in movie and music recommendation tasks as well, where movie genres and artist tags, respectively, are taken as the aspect space (section 4.2).

When aspects belong to the query space, ( | ) can be estimated from the baseline retrieval function (as suggested e.g. in [21,22]), in a similar way as ( | ) is estimated for queries (see equation 8 below). In that case, we take the approach:

(| ,

)

( |) (|) (|)

(7)

using Bayes' rule and assuming conditional independence of

documents and queries given a query aspect. Note that this inde-

pendence assumption is rather mild in this context, since the estimation of ( | ) applies to documents sampled from the

result set for ≠i.e. ( | ) in practice is meaning

,,

which is a form of blind relevance feedback approximation to

( | , ), being the set of documents returned for that are to

be diversified. In our experiments, we shall use this approximation

when TREC subtopics (manually provided by human assessors for

each query) are taken as the aspect space by the diversification

algorithm (see section 4.1).

The generative model ( | ) in equation 7 can be either estimated (more or less) directly from the baseline search system ≠when the retrieval function implements a language model≠ or it can be approximated from a relevance model as (see e.g. [3] for this type of derivation):

(

|

)

(| , )  (| ,

)

(8)

Finally, as to the estimation of ( | ), we consider two alternatives. For categorical aspects, we marginalize over documents in the result set:

( | )

(| ) ( |)



77

where we assume conditional independence of query aspects and queries given a document in , and we use the estimate for
( | ) discussed above (e.g. equation 8). For manually-provided query subtopics (TREC data), we assume a uniform distribution
( | )  1/ , where is the number of subtopics of query .

3.4 Relevance Model Estimation
Estimating the explicit probability of relevance of a document for a query has been researched in the IR literature from different points of view, distinguished from each other by the source from which relevance distributions are estimated.
Retrieval systems based on the PRP rank documents by decreasing probability of relevance ( | , ) given a query [20]. However, rather than directly estimating this probability, these systems compute a function that is equivalent in rank to ( | , ), and easier to estimate. The retrieval function is obtained by a series of monotonous operations on the probability of relevance, which involve lossy transformations, in the sense that it is not possible to recover ( | , ) back from the final expression. There is thus not in general an analytic approach to obtain an explicit relevance model from a PRP retrieval system [3].
On the other hand, a number of studies have researched the relation between retrieval function score distributions and relevance [2], and some have devised approaches to estimate the probability of relevance from score values [18]. In essence, these approaches analyze the distribution of the scoring function, and their correspondence with known relevance information, thus training a model of relevance given system scores, by regression on the available data [18].
We propose a similar but quite easier approach to estimate the probability of relevance, which just requires the availability of either precision estimates, or click statistics, for the retrieval system being diversified. Rather than mapping scores to probabilities of relevance, we use a common relevance estimate for all queries, based only on the position of documents. Namely, we estimate the probability of relevance by:
( | , )  ( | ( , ), )
where ( , ) is the rank position of document in the result list returned by the baseline retrieval in response to query . Note that this step involves, in a way, a form of rank-based retrieval system output normalization where (similarly to e.g. rank-sim normalization [17]) we only use the document order returned by the system, regardless of the score values. After this step, estimating the probability of relevance of a document for a query amounts to estimating the probability of relevance at each rank position ( | , ) for the retrieval system . To simplify the notation, we shall henceforth drop the `s' subscript from
( | , ), but it should be implicitly understood, as this distribution estimate is system-dependent.

3.4.1 Estimate on Relevance Judgments
If relevance judgments are available to train the relevance model
or, equivalently, we have an estimate of precision at of the
retrieval system (for ranging up to the size of the document set to be diversified), the positional relevance probability can be estimated as:

( | , )  @ - ( - 1) @( - 1)

(9)

where @ denotes the precision at of the system on query . In our experiments we estimate precision by splitting the set of

Figure 1. Relevance distribution estimate on the top 200 positions for different retrieval systems on different datasets (y axis displayed in log scale). The estimates for Lemur and pLSA are based on @ measurements, whereas the AOL curve is derived from click statistics from the AOL query log. The Lemur curve is computed on TREC 2009/10 diversity task data. The relevance curve of pLSA is derived from a
recommendation task on MovieLens data (see section 4.2).

test queries (with their relevance judgments) into two disjoints subsets of equal size for 2-fold cross-validation. For the queries in one subset, we approximate ( | , )  ( | ) using an average precision estimate @  @ , computed in the complementary
query subset.
As an illustrative example, Figure 1 shows the average relevance distribution estimate resulting for the Lemur Indri search system and the pLSA recommender ≠which we use as baselines in our experiments in section 4. The precision estimates are taken from the TREC 2009/10 diversity task data for Lemur, and from the MovieLens2 dataset for pLSA (more details in section 4.2). For Lemur, the distribution decreases from ( |1)  0.21 to around 10 by = 200. pLSA displays a higher relevance probability due to the nature of the recommendation task on this dataset. Compared to more involved methods such as the regression techniques described in [18], this approach is equivalent to simply using the raw relevance data (a histogram of positional relevance) without regularization, rather than a parameterized fit (e.g. by a logistic function).

3.4.2 Estimate on Click Statistics
A positional relevance model ( | ) can also be built from simple click statistics for the baseline retrieval system [19]. Assuming a cascade browsing model [15], and the simplifying assumption that a document is clicked if and only if it is relevant, we may consider the approximation:

( | )  (¨ | - 1) ( | )

(10)

where ( | ) is the probability that a document at position is clicked, and (¨ | - 1) denotes the event that the user continues (does not stop) browsing after position - 1. This term
can in turn be decomposed by marginalization into the relevance
and non-relevance cases:

2 http://www.grouplens.org/node/73

78

(¨ | ) = (¨ | , ) ( | ) +

+ (¨ | , ¨ ) 1 - ( | )

(11)

Assuming the probability to stop is independent from the position of the document given its relevance, and combining equations 10 and 11 we get:

( | )  (¨

( |) | ) ( | - 1) + (¨ |¨ ) 1 - ( | - 1)

Starting from ( |1) ~ ( |1), the above equation provides a recursive means to estimate the probability of relevance at each position in the ranking by just having some statistics (e.g. from a query log) of the ratio of clicks at each position, i.e. a background estimate of ( | ). The two remaining parameters represent a user model (as has been well-studied before, see e.g. [15]), where
(¨ |¨ ) reflects the user's "patience", and (¨ | ) is related to how many relevant documents the user is willing to get. This user model allows to account for the position bias in click statistics, and can be estimated in different ways. In the absence of specific data or criteria about this, a simplification such as
( | ) = 1 ≠a single relevant document is enough≠ and ( |¨ ) = 0 ≠the user never gives up until finding a relevant document≠ has been found to be acceptable for many purposes [8,15], which in our case just yields:

(

|

)



1

( -

(

|

|) - 1)

(12)

If click data became too sparse below a certain position, ( | ) might be estimated by regression techniques from this
point on, since we need an estimate of ( | ) for as many positions as the diversification system is intended to rerank.

As an illustrative example, Figure 1 shows the relevance curve derived from the AOL query log dataset using this approach, which we shall also test in our experiments. The curve is not significantly far below Indri on TREC 2009/10 (in fact it is above it in the top ten). Since AOL can be assumed to be a well optimized engine, this (small) difference can be attributed to a divergence in user behavior beyond the first web search results page (i.e. the behavior on the first and subsequent pages do not fit the same model), and/or the imprecision of the model. On the other hand, the AOL curve reflects practical (and personal) utility for real Web users, which is probably more demanding than topical (less subjective) relevance as applied by TREC assessors.

The estimation of relevance models continues to be a research topic in the IR field, and our approach might benefit from any improvement on this point ≠the more accurate the relevance model, the better the diversification algorithm can be expected to perform. Interestingly however, even with rough relevance probability estimates like the ones described here, the relevance-based diversification method seems to achieve good performance. Testing our framework with simple approaches as defined in equations 9 and 12, we already observe comparable or better results than the formulations based on generative models, as we report in the next section.

4. EXPERIMENTAL RESULTS
We have tested our relevance-based diversification framework on two IR domains: ad-hoc search, as defined in the TREC Web
track diversity task [9], and movie recommendation [24].

4.1 Search Diversity
In order to test our framework on search diversity, we use the data from the TREC diversity task, namely, the ClueWeb collection category B, the topic data, and the relevance assessments from the TREC 2009 and 2010 editions [9]. We use the Lemur Indri retrieval model3 (version 5.2) as the baseline search engine. We rerank the top 100 documents returned by the search system by the original a) IA-Select and b) xQuAD algorithms, and c) by our relevance-based reformulation. We test two query aspect spaces for diversification: 1) the Open Directory Project (ODP) categories as in [1], and 2) the subtopics manually provided in the TREC 2009/10 diversity task, as a reference for comparison. We do not apply an exhaustive optimization or tuning of the diversifiers, since the goal of the experiment is the comparison of alternatives, rather than reaching the maximum performance possible. To this respect, our results are roughly comparable in range to those reported e.g. in [21]. With ODP categories, we use the Textwise4 classification service to estimate ( | ), by normalization of the score returned by the classifier. For TREC subtopics, we derive an estimate by submitting them as queries to the baseline search system, as discussed earlier in section 3.3. We use a simple uniform aspect prior estimate, and compute the rest of components as described in that section. We test both equations 9 and 12 to estimate the rank-relevance model
( | ), using relevance judgments from the TREC dataset and AOL click statistics, respectively. TREC relevance information is used in a 2-fold cross-validation, where the relevance judgments of TREC 2009 are used to estimate @ and derive ( | ) in TREC 2010 topics, and vice-versa. Figure 2 compares the performance of the original xQuAD algorithm and the relevance-based reformulation (RxQuAD, using relevance judgments ≠equation 9≠ for the ( | ) estimate), measured by ERR-IA, for  ranging from 0 to 1. It can be seen that the relevance-based approach consistently outperforms the original xQuAD version. The plots also give an idea of the sensitiveness of the algorithms to the choice of the  parameter. It is interesting to notice that using TREC subtopics, the best result is reached for  = 1 , that is with maximum diversity. This makes sense, as the diversifiers use a "relevance-safe" aspect space, inasmuch as
Figure 2. Comparative performance (measured by ERR-IA) of relevance-based diversification (RxQuAD) and xQuAD, ranking over values of . The algorithms are tested on TREC 2009/10 data for ClueWeb category B, using ODP (left) and TREC subtopics (right) as the query aspect space, and TREC relevance judgments for the ( | ) estimate.
3 http://lemurproject.org/indri 4 http://textwise.com

79

Table 1. Comparative evaluation on TREC 2009/10 data for ClueWeb category B, using ODP categories (above) and TREC subtopics (below). The best result for each metric (in the ODP and TREC blocks respectively) is highlighted in bold. For our RxQuAD scheme, we show the results with ( | ) estimates from the TREC relevance judgments (Qrels), and click statistics from AOL (Clicks). The value of  that is used for xQuAD and RxQuAD is indicated on the corresponding row. Values marked with  and  indicate, respectively, significant and non-significant improvements over IA-Select and xQuAD (in this order). Similar convention with  and  indicates values below xQuAD or IA-Select. Statistical significance is established by Wilcoxon p < 0.05 in all cases.

ODP categories

Lemur IA-Select xQuAD
RxQuAD

 0.9 Qrels 0.1
Clicks 0.4

-nDCG @20
0.2587 0.2651 0.2675 0.2858 0.2841

IA-Select xQuAD
RxQuAD

1.0 Qrels 1.0 Clicks 1.0

0.3541 0.3445 0.3543 0.3512

ERR-IA @20
0.1630 0.1681 0.1656 0.1828 0.1831
0.2346 0.2241 0.2349 0.2320

nDCG- S-precision

IA@20

@r

0.2396 0.2423 0.2451 0.2655

0.0788 0.0935 0.0893 0.1045

0.2605 0.1036

S-recall @20
0.4636 0.4483 0.4864 0.4898 0.4830

0.3213 0.3127 0.3192 0.3166

0.1300 0.1149 0.1205
0.1185

0.5787 0.5704 0.5782 0.5748

TREC subtopics

subtopics are tightly related to the query, and therefore diversifying for them does not result in such a potential relevance loss tradeoff as with ODP categories.
Table 1 shows the comparative results on a more complete set of diversity metrics, computed with the evaluation script from the TREC diversity task. We also show subtopic precision at r (Sprecision@r) [29] where r is the subtopic recall (S-recall) in the set of documents being diversified. In other words, S-precision@r is the number of subtopics in the reranked set, divided by the first position where all the subtopics have been covered ≠it thus complements S-recall@20 with a measure of how early in the ranking all possible aspects are covered. Note that -nDCG, ERR-IA, and nDCG-IA account for both diversity and relevance, whereas Srecall and S-precision@r are pure diversity metrics. For xQuAD and our approach, the results correspond to the best  in terms of ERR-IA, selected manually with a precision of 0.1.
It can be seen that the relevance-oriented approach improves the two original diversifiers with ODP categories on all metrics (except on subtopic recall for the click-based configuration against xQuAD). With TREC subtopics, IA-Select is considerably effective ≠which we believe is due to the mostly single-valued aspect coverage per document, a situation where IA-Select seems to work particularly well≠, and has similar effectiveness to our approach, with some advantage on S-precision@r. Our approach performs better than xQuAD on this aspect space in all cases though. In general, the best results for our framework are obtained with the baseline-specific estimate of ( | ) (equation 9) on TREC relevance judgments (`Qrels' rows in Table 1), as one might expect. We may however observe that the results obtained with the AOL click statistics (`Clicks' rows) are almost as good as the results with the relevance judgments. This suggests that our approach is not particularly demanding or sensitive to the nature of the required relevance information to estimate ( | ).
4.2 Recommendation Diversity
In order to test our approach on a different domain other than adhoc search, we conduct additional experiments on recommender system tasks. For this purpose we use two well-known datasets: the 1M version of the MovieLens collection, which includes one million ratings (on a 1-5 scale) by 6,040 users for 3,900 items; and

an extract from Last.fm provided by “. Celma [6], including the full listening history of 992 users up till May 2009. The Last.fm data involves 176,948 artists and a total of 19,150,868 user accesses to music tracks. We use two recommender system baselines: a probabilistic Latent Semantic Analysis (pLSA) recommender [14], which is among the top performing on this data; and a simple non-personalized recommender which recommends movies by popularity (i.e. by the number of ratings), which has shown to be competitive in these settings [11].
We follow the adaptation of the diversity problem to recommendation scenarios proposed in [25], by which items (here movies and music artists) are taken as the equivalent of documents, and users play the part of queries. Movie genre is used as the equivalent of query aspects in MovieLens, and social tags (assigned to artists by Last.fm users) are used for the same purpose in the music dataset. MovieLens includes 19 genres, whereas in Last.fm we use a total of 123,819 different tags.
The user-item interaction data (user ratings in MovieLens, artist playcounts in Last.fm) is split into training and test sets, following common experimental practice in the Recommender Systems field [13], where the test data are used as the equivalent of relevance judgments. In our experiments we take five random 80-20% splits of the MovieLens data and repeat for 5-fold cross-validation. In Last.fm we do a temporal 60-40% data split of the song access records based on their timestamp (see [6] for further details on the evaluation methodology we adhere to).
Since the aspect space is categorical, and aspects are associated to items in a binary way (movies either belong or do not belong to a genre, and similarly for artist tags), we take the simple approximation ( | )   /  , where  = 1 when "has" the aspect (genre or tag) , and zero otherwise. In other words, we assume a uniform conditional aspect distribution among the set of aspects covered by each item. Different from the search setting, we estimate the background aspect prior from the overall distribution of aspects in the set of items, given that the item-aspect association is explicit, manual, reliable, and therefore can be considered fairly informative. We estimate ( | ) by equation 9, using test ratings as relevance judgments for the computation of @ , with a random 2-fold split of test users. We use very slight variations of the derivations in section 3.3, better suit-

80

Table 2. Comparative evaluation on MovieLens (left) and Last.fm (right) data, for diversification over a pLSA recommender baseline (top block), and recommendation by item popularity (bottom). The best result of each metric is highlighted in bold for each block. The improvement of the baselines is statistically significant in all cases. Improvements with respect to IA-Select and xQuAD (in this order) are marked with  when statistically significant, and  otherwise. Values with  indicate a significant decrease respect to them. Statistical significance is established by Wilcoxon p < 0.001 in all cases. The value of  that is used for xQuAD and RxQuAD is indicated next to each row and dataset block.

pLSA IA-Select xQuAD RxQuAD

MovieLens

Last.fm

-nDCG ERR-IA nDCG-IA S-precision S-recall

 @20

@20

@20

@r

@20

-nDCG ERR-IA nDCG-IA S-precision S-recall

 @20

@20

@20

@r

@20

- 0.3108 0.1880 0.2189 0.0658 0.6448

- 0.1209 0.0684 0.1566 0.2710 0.0738

- 0.3079 0.1885 0.2195 0.0684 0.6567

- 0.1198 0.0778 0.1795 0.2669 0.0727

0.9 0.3564 0.2053 0.2415 0.0959 0.7453 1.0 0.1264 0.0794 0.1857 0.2866 0.0765

0.5 0.3467 0.2116 0.2294 0.1195 0.7806 1.0 0.1428 0.1030 0.1914 0.3543 0.0906

Item popularity - 0.1944 0.1013 0.1101 0.0677 0.7205

- 0.1329 0.0849 0.2022 0.1468 0.0604

IA-Select

- 0.2239 0.1448 0.1377 0.0666 0.7045

- 0.1250 0.0910 0.2139 0.1471 0.0602

xQuAD

0.9 0.2315 0.1243 0.1343 0.1010 0.7893 1.0 0.1345 0.0947 0.2284 0.1587 0.0641

RxQuAD

0.9 0.2413 0.1494 0.1429 0.1947 0.8413 1.0 0.1421 0.1058 0.2136 0.2096 0.0790

ed for model estimation on recommendation input data. We also found it effective to normalize the top-level diversity component in the xQuAD schemes before its linear combination with
( | , ) (equations 3 and 5). We use a distribution-based normalization technique [12] for both versions (the original xQuAD and our variation) which showed to be effective in our experiments.
Figure 3 shows the performance of our relevance-based algorithm (RxQuAD) compared to xQuAD on MovieLens (left) and Last.fm
(right), in terms of ERR-IA, for  ranging from 0 to 1. The two collections are quite different both in their volumetric statistics (size, etc.), the nature of the user-item interaction data (movie ratings vs. music track playcounts), and the nature and distribution of item aspects (editorial genres vs. community-contributed tags), which accounts for the different behavior of the algorithms with
respect to the  parameter. In particular, the performance of our approach shows a drastic drop from  = 0.9 to 1 in MovieLens, whereas it improves consistently with  on Last.fm, peaking at  = 1 (which corresponds, as pointed out in section 3.2, to the relevance-based version of IA-Select). We attribute this difference to the fact that the baseline performance on Last.fm is quite low, whereby diversifying involves a lower relevance loss (hence the optimum improvement with maximum diversification). MovieLens allows for better baseline performance, whereby moderating the diversity degree is more appropriate, and an extreme diversification results in a drastic accuracy loss. Overall the improvement respect to xQuAD is clear.
Table 2 shows results on further metrics, showing also the diversification of the popularity-based recommender baseline, in addition
to pLSA. As in the experiments in search diversity, the  parameter in xQuAD and RxQuAD is chosen to optimize for ERR-IA on each dataset. We see that our approach is consistently better in most cases. Only over pLSA in MovieLens we observe mixed
results, with xQuAD producing better values on -nDCG and nDCG-IA respectively, while RxQuAD is best on ERR-IA, and pure diversity ≠as measured by S-precision@r and S-recall. RxQuAD achieves clearer improvements on the popularity baseline. So it does on Last.fm for both baselines ≠except on nDCGIA against popularity. This suggests that RxQuAD finds more room for improvement over the original algorithms on weak baselines (popularity recommendation vs. pLSA) and/or difficult datasets (Last.fm compared to MovieLens), with a low baseline

effectiveness, whereas on a strong baseline run, there is no clear winner. We also observe that the IA-Select algorithm is not always effective on these recommendation tasks. We attribute this to the strong redundancy penalization of IA-Select (as we shall discuss later), which may involve a loss of relevant documents, particularly over a strong baseline like pLSA. The ineffectiveness is mostly observed in terms of -nDCG, a metric which (by a default  = 0.5) applies a softer redundancy discount which IASelect may mismatch. Furthermore, because of the ( | ) term, items with multiple aspects (which abound in Last.fm) are demoted by IA-Select, which explains some low subtopic recall values.
Figure 3. Comparative performance (measured by ERR-IA) of relevance-based diversification (RxQuAD) and xQuAD, ranging over values of . The algorithms are tested on the MovieLens 1M dataset (left), using movie genre as the query aspect space; and Last.fm (right) using social tags as information need aspects. pLSA is the baseline recommender system diversified in the two top graphics, and the popularitybased item recommender in the bottom ones.

81

5. RELEVANCE-BASED REDUNDANCY MANAGEMENT
Beyond the interest and potential advantages of the relevancebased diversification model as a stand-alone development, an explicit relevance model provides the basis for the introduction and derivation of further extensions on a formal probabilistic basis. We show this by extending our framework with an explicit model of the tolerance to redundancy: different tasks, or different users, introduce different conditions on how redundancy should be handled and penalized. We show next how this can be accounted for by a smooth generalization of our framework.

5.1 Tolerance to Redundancy

Let

denote, as in section 3.4, a binary random variable that is

true when a user, in some retrieved document list browsing con-

text, stops reading documents. And let

denote the fact that a

user stops browsing at some point after reading some documents in

a set . We may refine the xQuAD diversity component as

( , ¨ | ), where the marginal utility of the document is

defined in terms of the user stopping before reaching . This re-

sults into a nuanced reformulation of the objective function:

( | , , ) = (1 - ) ( | , ) +

+ ( | ) ( | , , ) 1- ( | , , )

This form of the objective generalizes the original one by abstracting from the reasons why a document ≠in the context of a particular ranking≠ would not add value to the effective utility of the result list.
Now, as we did in section 3.4 (equation 11, but here with further conditioning variables), we may marginalize the stopping probability with respect to relevance:
( | , , )= ( | , , , ) ( | , , )+
+ ( | , , , ¨ ) (¨ | , , )
where again different simplifications can be considered. First, within the objective function for greedy document selection, we should consider ( | , , , ¨ ) = 0 for  , as the utility of the next document (which the objective function means to assess) would not be an issue if the user had stopped browsing already somewhere in . Another reasonable simplification is to assume the user's decision to stop at a specific document only depends on finding relevance, i.e. ( | , , , )  ( | ), whereby the model reduces to:
( | , , ) ( | , , ) ( | )
This way the original diversification algorithm is generalized to a form where an additional parameter ( | ) represents the user tolerance to redundancy ≠or in some sense, how many documents it takes for the user to be satisfied:
( | , , ) = (1 - ) ( | , ) +

+ ( | ) ( | , , ) 1- ( | , , ) ( | )

The introduction of this additional parameter allows to better match this characteristic of users and/or retrieval tasks. It allows to control (raise or soften) the penalization that should be applied to documents possessing aspects that are already covered in the original formulation of xQuAD. The latter implicitly assumes
( | ) = 1, that is, the user stops as soon as he finds a relevant document (zero tolerance to redundancy), which reflects

again an implicit assumption that users are willing to select a single document ≠which is often not the case.
An equivalent parameter might be inserted in the original xQuAD formulation to soften redundancy penalization, but it would lack the formal justification that the relevance-based approach enables. Furthermore, the xQuAD redundancy penalization is already rather mild compared to RxQuAD, since the discounting term of the novelty component is based on document probabilities ( | ), which tend to range on much lower values (since they should sum to 1 over all documents covering an aspect) compared to a Bernoulli relevance distribution ( | , , ). The addition of a tolerance parameter to xQuAD would only make this worse ≠unless it ranged beyond [0,1], which would bring the scheme even farther from a formal probabilistic basis.
On the other hand, tolerance to redundancy has also been explicitly modeled and introduced in the context of metric formalization upon user models [5,8,15,24]. Therefore the use of this parameter in our diversification algorithm has the potential of a better optimization for such metrics by bringing the diversification model closer to the principles and assumptions which are built into the metrics.

5.2 Empirical Observation
In order to illustrate the effect of adjustable redundancy, we display as a heat map in Figure 4 the performance values of the generalized RxQuAD with different values of ( | ), meas-
ured by -nDCG with different values of  (also reflecting different degrees of redundancy tolerance). For this test, we select the TREC subtopics in the search task (with ( | ) estimated on relevance judgments), and the MovieLens dataset for the recom-
mendation task. We keep the same values for  as were selected in the previous experiments, and the pLSA baseline in the recommendation task. It can be observed that the redundancy penalization effect of ( | ) is consistent with the equivalent parameter in the metric, i.e. the values evolve on a diagonal pattern: higher ( | ) values in the algorithm perform better for higher
 in the metric, and vice versa. The MovieLens graphic is smoother as the results are averaged over about 6,000 users (vs. 100 topics in TREC), and averaged again over 5 folds.

Indri + RxQuAD on ClueWeb / TREC 1

pLSA + RxQuAD on MovieLens / Genre 1

( |) ( |)

0



1 0



1

Figure 4. Parameterized tolerance to redundancy in the RxQuAD diversification algorithm by ( | ), evaluated
with corresponding metric configurations ( parameter in nDCG), by increments of 0.1. The values are displayed as a heat map where the darker colors (rank-normalized per col-
umn) represent higher -nDCG values. The left map shows results for search diversity (over Indri) on ClueWeb with TREC subtopics, and the right map shows results recommendation diversity (over pLSA) on MovieLens with genres. A
diagonal trend can be observed in the relative metric values.

82

6. DISCUSSION
In practical terms, using a relevance model in the redundancy component results in a higher redundancy penalization than is applied with a document generation model in xQuAD, as discussed in the previous section. Adding to one over all documents, the variations of ( | ) are quite small compared to ( | ), whereby subtopic coverage may tend to overdo redundancy in the overall effect of the algorithm. In contrast, the conditional Bernoulli relevance distribution ( | , , ) does not add to one in general over documents (unless the model assumes a unique relevant document), and may thus range over a significantly higher scale. Our relevance-oriented formulation hence enables a stronger redundancy penalization. This strength can be softened if needed, by the generalization for redundancy adaptation described in the previous section.
Compared to the original xQuAD, IA-Select goes to the opposite extreme in redundancy penalization: according to the approach described by Agrawal et al [1], redundancy is penalized by
( | ), which may have values close to 1 when documents cover a single aspect with high probability. The effect is that once an aspect is covered by some document in , any other document covering this aspect is considered to add near zero marginal utility [28]. As a particular consequence, there is little discrimination between degrees of redundancy (i.e. an aspect that has been covered just once vs. further times before), and once all aspects have been covered, the diversified ranking degrades to the original order defined by the baseline retrieval system (see [26] for further analysis). Our relevance-oriented formulation does not result in such extremes either, as far as the probability of relevance, even at the high rank positions does usually not get as close to 1 as ( | ) may get. In practice, IA-Select may also soften the penalization by multiplying ( | ) by the baseline retrieval function. The penalization is also milder when documents cover several aspects, in which case ( | ) ranges over lower values. The algorithm implementation designer has no explicit control over these factors though ≠in contrast with our proposed extension for redundancy adjustment≠, which may also account for the instability of IASelect across heterogeneous experiments observed in section 4.
Besides these considerations, a relevance-based redundancy assessment better matches the structure of redundancy-sensitive
metrics such has ERR-IA and -nDCG, which are formalized in terms of probabilities of relevance [8]. This may partly account for the observed improved performance. The introduction of an explicit redundancy control parameter allows further gauging how aggressive the novelty-seeking component should be, enabling a finer adjustment to the role of redundancy in specific IR tasks and metrics.
Modeling tolerance to redundancy illustrates the potential advantages that an explicit relevance model brings about. Tolerance to redundancy could be introduced in the original formulations of IA-Select and xQuAD as well e.g. as a scalar parameter in the redundancy penalization component, but it is not clear how this might be given a principled ≠not simply heuristic≠ justification. As noted by Welch et al [28], "common to a majority of prior research [on search diversification] is the single relevant document assumption", which makes it difficult to explicitly formalize variable degrees of acceptable redundancy. The lack of this limitation might be credited as a virtue of an explicit relevance model. Though it has also been argued that a document generative model does not intrinsically negate the selection of several relevant documents [16], it is not clear how this multiple selection could be explicitly reflected upon a generative model. Welch et al [28]

actually address this by explicitly modeling the number of relevant documents that the user is seeking to get. The alternative we show here has a considerably simpler development and does not require the introduction of this additional, somewhat artificial variable (the number of sought documents). Instead, our approach builds on models of user behavior, which are being extensively researched in the field (e.g. [8,15] among many other works).
A tradeoff of the relevance model is that it needs to be trained on relevance information ≠a tradeoff which is shared with PRP approaches in contrast to language modeling to build information retrieval systems. We have shown however that the need for training in our framework is not demanding in terms of the involved complexity, the data requirements, or the required accuracy, since a simple approach with few data proves to be good enough to produce quite competitive results. Furthermore, a rough rank-relevance estimate from a commercial search engine click log showed to be sufficient to obtain almost as good results as with expensive editorial relevance judgments.
7. CONCLUSION
IR diversification approaches proposed so far in the field use either an explicit representation of information need aspects, or an explicit relevance model, but not both. We have proposed and developed a revision of prior intent-oriented diversification schemes with the introduction of an explicit relevance model in the formulation of the approach. We observe that an explicit relevance model results in comparable or even better performance than prior approaches in terms of diversity evaluation metrics, in different application domains (search and recommendation) on different datasets. The approach thus favorably compares to its original alternatives, and might open new lines for effectiveness improvements.
From a theoretical standpoint, the relevance-oriented formulation provides an alternative ≠perhaps more direct≠ description of the diversity problem, whereupon the algorithmic scheme can be directly derived. The relevance-based foundation may be better suited for the description of diversification processes and their underlying principles: marginal utility, diminishing returns, relative value of documents, and so forth. Concepts such as relevance and utility find clear, unambiguous and more direct reflection in the framework itself. Furthermore, the formulation provides a more direct match of metric schemes in which relevance models underlie [5,8,15,24], therefore potentially providing for a better optimization against such metrics.
An additional side-effect of the relevance-oriented formulation is the unification of the IA-Select and xQuAD approaches into a common scheme. The proposed framework opens moreover new directions for further formal developments where relevance is an intrinsic variable. As a particular case, we show the formal extension of our framework to describe and adjust the algorithm to different degrees of tolerance to redundancy, the consistency of which is empirically validated.
8. ACKNOWLEDGMENTS
This work was supported by the national Spanish projects TIN2011-28538-C02-01 and S2009TIC-1542.
9. REFERENCES
[1] Agrawal, R., Gollapudi, S., Halverson, A., and Ieong, S. Diversifying search results. 2nd ACM International Conference on Web Search and Data Mining (WSDM 2009). Barcelona, Spain, February 2009, 5-14.

83

[2] Arampatzis, A. and Robertson, S. Modeling score distributions in information retrieval. Information Retrieval 14(1), 2011, 26-46.
[3] Bache, R., Baillie, M., and Crestani, F. Language models, probability of relevance and relevance likelihood. ACM International Conference on Information and Knowledge Management (CIKM 2007). Lisbon, Portugal, 2007, 853-856.
[4] Carbonell, J. G. and Goldstein, J. The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. 21st Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR 1998). Melbourne, Australia, August 2998, 335-336.
[5] Carterette, B. An analysis of NP-completeness in novelty and diversity ranking. Information Retrieval 14(1), February 2011, 89-106.
[6] Celma, “. and Herrera, P. A New Approach to Evaluating Novel Recommendations. 2nd ACM International Conference on Recommender Systems (RecSys 2008). Lausanne, Switzerland, 2008, 179-186.
[7] Chen, H. and Karger, D. R. Less is More. 29th Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR 2006). Seattle, WA, USA, August 2006, 429-436.
[8] Clarke, C. L. A., Craswell, N., Soboroff, I., and Ashkan, A. A Comparative Analysis of Cascade Measures for Novelty and Diversity. 4th ACM International Conference on Web Search and Data Mining (WSDM 2011). Hong-Kong, China, February 2011, 75-84.
[9] Clarke, C. L. A., Craswell, N., Soboroff, I, and Cormack, G. V. Overview of the TREC 2010 Web Track. TREC 2010, Gaithersburg, MD, USA.
[10] Clarke, C. L. A., Kolla, M., Cormack, G. V., Vechtomova, O., Ashkan, A., B¸ttcher, S., and MacKinnon, I. Novelty and diversity in information retrieval evaluation. 31st Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR 2008). Singapore, July 2008, 659-666.
[11] Cremonesi, P., Koren, Y., Turrin, R. Performance of recommender algorithms on top-n recommendation tasks. 4th ACM International Conference on Recommender Systems (RecSys 2010). Barcelona, Spain, September 2010, 39-46.
[12] Fern·ndez, M., Vallet, D., and Castells, P. Using Historical Data to Enhance Rank Aggregation. 29th Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR 2006). Seattle, WA, USA, August 2006, pp. 643-644.
[13] Herlocker, J. L., Konstan, J. A., Terveen, L. G., and Riedl, J. T. Evaluating collaborative filtering recommender systems. ACM Transactions on Information Systems 22, 1, 2004, 5-53.
[14] Hofmann, T. Latent semantic models for collaborative filtering. ACM Transactions on Information Systems 22(1), January 2004, 89-115.
[15] Hu, B., Zhang, Y., Chen, W., Wang, G., and Yang, Q. Characterizing Search Intent Diversity into Click Models. 20th International Conference on World Wide Web (WWW 2011). Hyderabad, India, March 2011, 17-26.

[16] Lafferty, L., Zhai, C. Probabalistic Relevance Models Based on Document and Query Generation. In Croft W.B. and Lafferty J. (Eds.), Language Modeling for Information Retrieval. Kluwer Academic Publishers, Dordrecht, Netherlands, 2003.
[17] Lee, J. H. Analyses of multiple evidence combination. 20th ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR 1997). Philadelphia, PA, USA, July 1997, 267-276.
[18] Nottelmann, H. and Fuhr, N. From Retrieval Status Values to Probabilities of Relevance for Advanced IR Applications. Information Retrieval 6(3-4), September 2003, 363-388.
[19] Pan, B., Hembrooke, H., Joachims, T., Lorigo, L., Gay, G., and Granka, L.A. In Google We Trust: Users' Decisions on Rank, Position, and Relevance. Journal of ComputerMediated Communication 12(3), June 2007, 801-823.
[20] Robertson, S. E. The Probability Ranking Principle in IR. Journal of Documentation 33(4), 1977, 294-304.
[21] Santos, R. L. T., Macdonald, C., and Ounis, I. Exploiting query reformulations for web search result diversification. 19th International Conference on World Wide Web (WWW 2010). Raleigh, NC, USA, April 2010, 881-890.
[22] Santos, R. L. T., Macdonald, C., and Ounis, I. On the Role of Novelty for Search Result Diversification. Information Retrieval. In Press.
[23] Spark-Jones, K., Robertson, S. E., Hiemstra, D., Zaragoza, H. Language Modelling and Relevance. In Croft W.B. and Lafferty J. (Eds.), Language Modeling for Information Retrieval. Kluwer Academic Publishers, Dordrecht, Netherlands, 2003.
[24] Vargas, S. and Castells, P. Rank and Relevance in Novelty and Diversity Metrics for Recommender Systems. 5th ACM Int. Conf. on Recommender Systems (RecSys 2011). Chicago, IL, October 2011, 109-116.
[25] Vargas, S., Castells, P., and Vallet, D. Intent-Oriented Diversity in Recommender Systems. 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2011). Beijing, China, August 2011, 1211-1212.
[26] Vargas. S., Castells, P., and Vallet, D. On the Suitability of Intent Spaces for IR Diversification. International Workshop on Diversity in Document Retrieval (DDR 2012) at the 5th ACM International Conference on Web Search and Data Mining (WSDM 2012). Seattle, WA, USA, February 2012.
[27] Wang, J. and Zhu, J. Portfolio theory of information retrieval. 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2009). Boston, MA, USA, July 2009, 115-122.
[28] Welch, M. J., Cho, J., and Olston, C. Search result diversity for informational queries. 20th International Conference on World Wide Web (WWW 2011). Hyderabad, India, March 2011, 237-246.
[29] Zhai, C., Cohen, W. W., and Lafferty, J. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003). Toronto, Canada, July 2003, 10-17.

84

Top-k Learning to Rank: Labeling, Ranking and Evaluation
Shuzi Niu, Jiafeng Guo, Yanyan Lan, Xueqi Cheng
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, P.R. China
niushuzi@software.ict.ac.cn, {guojiafeng, lanyanyan, cxq}@ict.ac.cn

ABSTRACT
In this paper, we propose a novel top-k learning to rank framework, which involves labeling strategy, ranking model and evaluation measure. The motivation comes from the difficulty in obtaining reliable relevance judgments from human assessors when applying learning to rank in real search systems. The traditional absolute relevance judgment method is difficult in both gradation specification and human assessing, resulting in high level of disagreement on judgments. While the pairwise preference judgment, as a good alternative, is often criticized for increasing the complexity of judgment from O(n) to O(n log n). Considering the fact that users mainly care about top ranked search results, we propose a novel top-k labeling strategy which adopts the pairwise preference judgment to generate the top k ordering items from n documents (i.e. top-k ground-truth) in a manner similar to that of HeapSort. As a result, the complexity of judgment is reduced to O(n log k). With the topk ground-truth, traditional ranking models (e.g. pairwise or listwise models) and evaluation measures (e.g. NDCG) no longer fit the data set. Therefore, we introduce a new ranking model, namely FocusedRank, which fully captures the characteristics of the top-k ground-truth. We also extend the widely used evaluation measures NDCG and ERR to be applicable to the top-k ground-truth, referred as -NDCG and -ERR, respectively. Finally, we conduct extensive experiments on benchmark data collections to demonstrate the efficiency and effectiveness of our top-k labeling strategy and ranking models.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms
Algorithms, Performance, Experimentation, Theory
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08... $15.00.

Keywords
Learning to Rank, Top-k, Preference Judgment, Evaluation
1. INTRODUCTION
In the past few years, learning to rank has been widely recognized as an important technique for information retrieval (IR). A vital part to employ learning to rank in real search systems is the acquisition of reliable and high quality labeled datasets, both for training and evaluation. In traditional IR literature, assessors are requested to determine the relevance of a document under some pre-defined gradations, which is called absolute relevance judgment method. However, there are some significant drawbacks for this evaluation process. Firstly, the specifics of the gradations (i.e. how many grades to use and what those grades mean) must be defined, and it is not clear how these choices will affect relative performance measurements [26]. Secondly, the assessing burden increases with the complexity of the relevance gradations; the choice of label is not clear when there are more factors to consider, leading to high level of disagreement on judgments [4].
Recently pairwise preference judgment has been investigated as a good alternative [20, 26]. Instead of assigning a relevance grade to a document, an assessor looks at two pages and judges which one is better. Compared with absolute relevance judgment, the advantages lie in that: (1) There is no need to determine the gradation specifications as it is a binary decision. (2) It is easier for an assessor to express a preference for one document over the other than to assign a pre-defined grade to each of them [7]. (3) Most state-of-the-art learning to rank models, pairwise or listwise, are trained over preferences. As noted by Carterette et al. [7], "by collecting preferences directly, some of the noise associated with difficulty in distinguishing between different levels of relevance may be reduced." Although preference judgment likely produce more reliable labeled data, it is often criticized for increasing the complexity of judgment (e.g. from O(n) to O(n log n) [20]), which poses a big challenge in wide use. Do we actually need to judge so many pairs for real search systems? If not, which pairs do we choose? How to choose? These questions become the original motivation of this paper.
As we know, in real Web search scenario, it is well accepted that users mainly care about the top results [30]. In other words, the ordering of the top results (typically the results on the first one or two pages) is critical for users' search experience. It indicates that a labeling strategy shall take effort to figure out the top results and judge the preference orders among them, but pay less attention to the exact

751

preference orders among the rest results. Based on this observation, we propose a novel top-k labeling strategy which adopts the pairwise preference judgment to generate the top k ordering items from a set of n items in a manner similar to that of HeapSort. The obtained ground-truth from this topk labeling strategy is a mixture of the total order of the top k items, and the relative preferences between the set of top k items and the set of the rest n - k items, referred as topk ground-truth. With this top-k labeling strategy, we can not only capture enough information for learning to rank [30], but also largely reduce the complexity of judgment to O(n log k).
With top-k ground-truth, we find that traditional ranking models, either pairwise or listwise, are no longer suitable for the labeled data set. It is natural to introduce a mixed ranking model, with the listwise model capturing the total order of the top k items and the pairwise model capturing the relative preference between the set of top k items and the set of the rest n - k items. Such a mixed model can thus combine the advantages of both pairwise and listwise approaches to fully exploit the information in the top-k ground-truth. We refer such a mixed ranking model as FocusedRank, since it emphasizes more on the ordering of the top items.
For evaluation, traditional IR evaluation measures (e.g., MAP, NDCG and ERR), which are mainly defined on the absolute judgment, cannot be directly applied to the top-k ground-truth. To address this problem, we extend NDCG and ERR to -NDCG and -ERR by taking a function of the position of items as the absolute relevance label. The proposed evaluation measures thus emphasize the importance of the ordering of the top k items. Unlike the evaluation measures based on preference judgments [6], -NDCG and -ERR keep the same form as NDCG and ERR thus enjoy all the merits of traditional IR evaluation measures.
Finally, we conduct extensive experiments on benchmark data collections. Major experimental findings include: (1) With top-k labeling strategy, the time cost on labeling one pair is much less than that on one item in absolute relevance judgment, and the overall time cost is comparable with that in absolute relevance judgment. (2) With topk labeling strategy, the level of agreement on judgments is higher than that on absolute relevance judgment. (3) With FocusedRank, the ranking performance is significantly better than the state-of-the-art pairwise and listwise ranking models.
To sum up, we propose a top-k learning to rank framework1, a novel and complete framework including labeling strategy, ranking model and evaluation measures. Our main contributions are as follows:
1. We propose a novel top-k labeling strategy which adopts pairwise preference judgment to obtain reliable groundtruth for learning to rank. As a result, the complexity of judgment is reduced to O(n log k).
2. We introduce a new ranking model named FocusedRank to capture the characteristics of the top-k groundtruth, and it outperforms the state-of-the-art pairwise and listwise ranking models on benchmark datasets.
1Note that Xia et al. also mentioned the top k ranking problem in [30]. The difference is that they focus on the ranking models under the circumstances of traditional labeling strategy and evaluation measure.

3. We derive two new evaluation measures named -NDCG and -ERR applicable to the top-k ground-truth. They both emphasize the importance of top-k ordering and enjoy the merits of traditional IR measures with the similar formulation.
2. RELATED WORK
In this section, we briefly review some related work on labeling strategy, ranking model and evaluation measure in learning to rank literature.
2.1 Labeling Strategy
In learning to rank, labeling strategies can be divided into two categories: absolute judgment and relative judgment [21, 26, 32, 7].
In absolute judgment, assessors are usually requested to assign a graded score to an item independent of the other items [15, 4, 27, 28, 29] under some pre-defined gradations. Such a labeling strategy has been widely adopted in both industry and academia to construct benchmark datasets in IR, e.g. TREC data sets (since 2000), Microsoft learning to rank datasets [18] and Yahoo! Learning to Rank challenge 2010 data set. One difficult problem in using absolute judgment is to clearly define the specifics of the gradations, i.e. how many grades to use and what those grades mean. Some previous studies tried to figure out the proper number of relevance gradations [22, 19]. However, as noted by Cox [12], "there is no single number of response alternatives for a scale which is appropriate under all circumstances". If the descriptions of each degree are not clearly defined, a multi-grade judgment method can be easily misused in the evaluation process [34]. Moreover, the assessing burden in absolute judgment increases with the complexity of the relevance gradations. When there are more factors to consider, the choice of label is not clear, resulting in high level of disagreement on judgments [4].
In contrast, relative judgement aims to directly judge the relative order of a set of items [26]. As a typical form of relative judgment, pairwise preference judgment asks assessors to express a preference for one item over the other [6]. One concern of using this strategy is the complexity of judgment since the number of item pairs is polynomial in the number of items. Carterette et al. [7] attempted to reduce the number of pairs for judging by using transitivity of relevance among documents. Nir Ailon [1] proposed a formal pairwise method based on QuickSort which can reduce the number of preference judgments from O(n2) to O(n log n). Compared with O(n) in absolute judgment, this is still not affordable for assessors. To increase the efficiency of relative judgment, R. Song et al. [26] further proposed to select the best one each time from the remaining items, which is only applicable to small datasets. Different from the above related work, we propose a novel top-k labeling strategy to largely save the effort of preference judgment, by exploiting what Web search users actually care about on ranking.
2.2 Ranking Model
So far learning to rank has been mainly addressed by pointwise, pairwise, and listwise ranking models. In pointwise models [17], ranking is transformed to regression or classification on individual items to represent the absolute label on each item. In pairwise models [14, 11, 3], ranking is transformed to classification on item pairs to represent the

752

preference between two items. In listwise models [33, 31, 5], instances as document lists are generated through the comparison over item pairs, and it is superior in modeling more discriminative judgments. Therefore, we can conclude that, pointwise models is well suited for absolute relevance judgment; while both pairwise and listwise models are applicable in either absolute or relative judgment scenario.
In previous work [30], Xia et al. extended three listwise ranking models, namely top-k ListMLE, top-k ListNet and top-k RankCosine, to fit the top-k scenario. Note that they addressed this scenario under the circumstances of traditional labeling strategy and evaluation measure. They conducted experiments on the top-k ListMLE, and claimed that the top-k ListMLE can outperform traditional pairwise and listwise ranking models. However, it cannot avoid the computational complexity on the entire permutation such as that in top-k ListNet.
In our paper, we take the above ranking models as the baselines to show the superiority of FocusedRank.
2.3 Evaluation Measure
To evaluate the effectiveness of a ranking model, many IR measures have been proposed. Here we give a brief introduction to several popular ones which are widely used in learning to rank. See also [16] for other measures.
Precision@k [2] is a measure for evaluating top k positions of a ranked list using two grades (relevant and irrelevant) of relevance judgment. With Precision as the basis, Average Precision (AP) and Mean Average Precision (MAP) [2] are derived to evaluate the average performance of a ranking model.
While Precision considers only two graded relevance judgments, Discounted Cumulated Gain (DCG) [13] is an evaluation measure that can leverage the relevance judgment in terms of multiple ordered categories, and has an explicit position discount factor in its definition. By normalizing DCG@k with its maximum possible value, we will get another popular measure named Normalized Discounted Cumulated Gain (NDCG).
To relax the additive nature and the underlying independence assumption in NDCG, another evaluation measure, Expected Reciprocal Rank (ERR), is proposed in [8]. It implicitly discounts documents which are shown below very relevant documents, and is defined as the expected reciprocal length of time that the user will take to find a relevant document.
Although MAP, NDCG and ERR are widely used in IR, they all adopt absolute relevance labels in their formulation, which imposes restrictions on direct application in the scenario of relative judgment. Therefore, new measures such as bpref , ppref , and nwppref [6] have been proposed. However, these measures have not been widely accepted by IR community. In this paper, we extend traditional IR evaluation measures to our relative judgment scenario with similar formulation.
3. TOP-K LEARNING TO RANK
In this section, we will introduce our top-k learning to rank framework in detail, which involves the labeling strategy, the ranking model and the evaluation measure.
3.1 Top-k Labeling Strategy
According to previous work and the above discussions,

pairwise preference judgment is superior to traditional absolute relevance judgment in the acquisition of reliable judgments from human assessors. However, it is often criticized for increasing the complexity of judgment. In this section, we propose a novel top-k labeling strategy by exploiting what Web search users actually care about on ranking. Our labeling strategy can not only capture enough information for learning to rank, but also largely save the effort of preference judgment.
3.1.1 Motivation
In real Web search applications, users usually pay more attention to the top-k items [9, 25, 10]. For example, according to a user study [30], in modern search engines, about 62% of search users only click on the results within the first pages, and 90% of search users click on the results within the first three pages. It shows that the ordering of the top k results is critical for users' search experience. Two ranked lists of results will likely provide the same value to users (and thus suffer the same loss), if they have the same ranking results for the top positions [30]. Moreover, a good ranking on the top results is much more important than a good ranking on the others. Therefore, a labeling strategy shall take effort to figure out the top k results, judge the preference orders among them carefully, but pay less attention to the exact preference orders among the rest results.
3.1.2 Labeling Strategy
Based on the above analysis, we propose a novel top-k labeling strategy using the pairwise preference judgment as the basis. The basic assumption for our labeling strategy is the transitivity of preference judgments of relevance [24, 7]. That is, if i is preferred to j and j is preferred to k, the assessor will also prefers i to k. With this assumption, our labeling strategy generate the top k ordering items from a set of n items in a manner similar to that of HeapSort. It mainly takes the following three steps:
Step1 Randomly select k items from the set of n items and build a min-heap with t as the root based on the pairwise preference judgments by assessors. Here a min-heap is a complete binary tree with the property: if B is a child node of A, A is less relevant than B.
Step2 Randomly select an item r from the rest n - k items and the preference judgement is conducted between t and r by assessors. We then update the heap if necessary and obtain a new min-heap with the k most relevant items up till now. It is repeated until all the items have been selected.
Step3 Sort the final k items in the min-heap in a descending order, and append the rest items after the k items.
The detailed labeling algorithm is shown in Algorithm1. With the above top-k labeling strategy, the obtained groundtruth is a mixture of the total order of the top k items and the relative preference between the set of top k items and the set of the rest n - k items, referred to as top-k ground-truth.
3.1.3 Complexity Analysis
Here we consider the judgment complexity of each step in top-k labeling strategy:
(1)The judgment complexity of building a min-heap with k items in Step 1 is O(k);

753

Algorithm1: Top-k Labeling based on HeapSort

1 Input: (1) D, an item set; (2) k, top item number.

2 begin

3

randomly select k items from D denoted as Dk.

4

construct a min-heap Hk over Dk with preference

judgment.

5

for each item d in D - Dk do

6

obtain preference judgment over pair (d, Dk[1]).

7

if the judgment is d more relevant than Dk[1]

8

Dk[1] = d,

9

update Hk over Dk with preference judgment.

10

end if

11 end for

12 sort Hk to obtain top k items in descending order

denoted as LD.

13

append D - Dk to LD.

14 end

15 Output: LD.

(2)The judgment complexity in Step 2 is O((n - k) log k) according to the complexity analysis for HeapSort;
(3)The judgment complexity in Step 3 is O(klogk).
Therefore, the total judgment complexity of top-k labeling strategy is about O(n log k). Compared with QuickSort strategy adopted by Nir Ailon [1] for preference judgment, our top-k labeling strategy significantly reduces the complexity from O(n log n) to O(n log k), where usually k n. The judgment complexity of our strategy is nearly comparable with that of the absolute judgment (i.e. O(n)). Experimental results in the following section also verify the efficiency of our labeling strategy which is consistent with the theoretical analysis.
3.2 Top-k Ranking Model
With the top-k ground-truth obtained from our labeling method, traditional ranking models no longer fit the labeled dataset. On one hand, pairwise ranking models can capture the information of the relative preference, but fail to model the total order of the top k items since they ignore the position information. On the other hand, listwise ranking models can capture the information of the total order, but suffer from the great computational complexity due to a large undifferentiated item set in top-k ground-truth. To address this problem, we propose FocusedRank, a mixed ranking model with listwise ranking model capturing the total order of the top k items and pairwise ranking model capturing the relative preference between the set of top k items and the set of the rest n - k items. Such a mixed model can thus combine the advantages of both pairwise and listwise approaches to fully exploit the information in the top-k ground-truth.
3.2.1 Notations
Given m training queries {qi}m i=1, let xi = {x(1i), ∑ ∑ ∑ , x(nii)} be the items associated with qi, where ni is the number of documents of this query, Ti be the set of top k items and Fi be the set of other ni - k items. Denote the total order of Ti as a permutation i, where i(x(ji)) stands for the position of item x(ji)  Ti. Denote Pi = {(x(ui), x(vi)) : x(ui)  Ti, x(vi)  Fi} as the set of pairs constructed between the set of top k items and the set of the rest n - k items. We relate the top-k ground-truth to relevance labels by defining yi = {y1(i), ∑ ∑ ∑ , yn(ii)} as position-aware relevance labels of

the corresponding items, and yi(T ) = {yj(i) : x(ji)  Ti.}. In our paper, we use yj(i) = k + 1 - i(x(ji)), if x(ji)  Ti, and yj(i) = 0, otherwise. That is, suppose k = 10, the relevance labels for the top 10 items are defined in a descending order
from 10 to 1, while the relevance labels for the rest items
are defined as 0.

3.2.2 FocusedRank
In FocusedRank, we adopt a listwise loss to model the total order of top k items, and a pairwise loss to model the preference of top k items to the other items. The general loss function of FocusedRank on a query qi is presented as follows2 .

L(f ; qi)= ◊Llist(f ; Ti, yi) + (1 - )◊Lpair(f ; Pi, yi), (1)

where Llist stands for a listwise ranking model and Lpair stands for a pairwise ranking model,  is a trade-off coefficient to balance the two terms. As examples, we combine three popular listwise ranking models (i.e. SVMMAP , AdaRank and ListNet) with three popular pairwise ranking models (i.e. RankSVM, RankBoost and RankNet) respectively to get three specific forms of FocusedRank, namely FocusedSVM, FocusedBoost and FocusedNet accordingly.

(1) FocusedSVM: RankSVM plus SVMMAP

Both RankSVM [14] and SVMMAP [33] apply the SVM technology to optimize the number of misclassified pairs and the average precision, respectively. Therefore, we combine
these two ranking models together to get a new FocusedRank method, named FocusedSVM. Specifically, RankSVM is adopted to model the pairwise preference of Pi, and SVMMAP is employed to model the total order of Ti. Therefore, Llist and Lpair in Eq. (1) has the following specific form, respectively.

Llist= maxz(iT) (1-AP (z(iT), yi(T))+wT(zi, Ti)-wT(yi(T), Ti)) Lpair= (x(ui),x(vi))Pi max{0, 1-(yu(i) -yv(i))(wT x(ui) -wT x(vi))}.
Like RankSVM and SVMMAP , FocusedSVM can then be formulated as an optimization problem as follows.

min 1 w 2 + C 2

m i=1[(i) + (1 - )

 ] (i)
(x(ui),x(vi))Fi u,v

s.t. : (yu(i) - yv(i))(wT x(ui) - wT x(vi))  1-u(i,)v, (x(ui), x(vi))  Pi,

wT(yi(T), Ti)-wT (zi, Ti)  1-AP (yi(T), z(iT))-(i), z(iT),

u(i,)v  0, (i)  0, i = 1, ..., m,

where zTi stands for any incorrect label and  is the same

as that in SVMMAP .

Similar to SVM,

1 2

w

2 controls the

complexity of the model w, and C is a trade-off parameter

between the model complexity and hinge loss relaxations.

(2) FocusedBoost: RankBoost plus AdaRank

Both RankBoost [11] and AdaRank [31] adopt the boosting technology to output a ranking model by combining the week rankers, where the combination coefficients are determined by the probability distribution on document pairs and ranked lists respectively. Hence we combine these two
2In application, Llist and Lpair should be normalized to a comparable range, and we adopt this trick in our experiments.

754

Algorithm2: Learning Algorithm for FocusedBoost

1 Input: training data in terms of top-k ground-truth.

2 Given: initial distributionD1 onTi andD1 onPi, i=1,∑ ∑ ∑,m.

3 For t = 1, ∑ ∑ ∑ , T

4

train weak ranker ft to minimize: rt= m i=1Dt(Ti)Llist

5

+(1 - choose

) t

m
=i=12 1log((11x-+(uirr)tt,x)(v.i))Pi

Dt(x(ui), x(vi))Lpair .

6 update

Dt+1

=

1 Zt+1

Dt

(Ti

)

exp(-E

(

t s=1

s

fs

,

Ti,

yiT

)),

Dt+1

=

1 Zt+1

Dt (x(ui) ,

x(vi) )

exp(t(wT

x(ui)

-

wT

x(vi))),

where,

Zt+1=

m i=1Dt(Ti) exp(-E(

t s=1

s

fs

,

Ti

,

yiT

)).

Zt+1=

m i=1

(x(ui),x(vi))PiDt(x(ui), x(vi)) exp(t(wTx(ui)-

wTx(vi) )).

7 Output: f (x) = t tft(x).

ranking models together to get a new FocusedRank method, named FocusedBoost. Specifically, RankBoost is adopted to model the preference of Pi, and AdaBoost is to model the total order of Ti. Therefore, Llist and Lpair in Eq. (1) has the following specific form, respectively.
Llist = exp (-E(f, Ti, yiT )), Lpair = exp (-(yu(i) - yv(i))(wT (x(ui) - x(vi)))),
where E(f, Ti, yi(i)) stands for the IR evaluation to optimize. As in RankBoost and AdaRank, the detailed algorithm is
shown in Algorithm2.
(3) FocusedNet: RankNet plus ListNet
Both RankNet and ListNet aim to optimize a cross entropy between the target probability and the modeled probability. The probability of the former is defined based on the exponential function of difference between the scores of any two documents in all document pairs given by the scoring function f . The probability of the latter is the permutation probability of a ranking list using Plackett-Luce model [23], which is also based on the exponential function. Hence we combine these two ranking models together to get a new FocusedRank method, named FocusedNet. Specially, RankNet is adopted to model the pairwise preference of Pi, and ListNet is to model the total order of Ti. Therefore, Llist and Lpair in Eq. (1) has the following specific form, specifically.

Llist

=

-
i

Pyi(T ) () log

Pz(iT ) (),

Lpair = -PØu,v log Pu,v(f ) - (1 - PØu,v) log(1 - Pu,v(f )),

where, PØu,v = 1, if yu(i)  yv(i), and PØu,v = 0, otherwise. In addition, we have that

z(iT ) = {zj(i) = wT x(ji), x(ji)  Ti},

Pu,v(f )

=

1

exp(wT x(ui) - + exp(wT x(ui)

wT x(vi)) - wT x(vi)

)

,

|Ti |
Ps() =
j=1

(s(j))

|Ti | l=j

(s(l)

)

,

s

=

yi(T ),

z(iT ),

where s(j) denotes the score of the object at position j of permutation .

3.3 Top-k Evaluation Measure
As aforementioned, traditional IR evaluation measures (e.g., MAP3, NDCG and ERR) are mainly defined on the absolute judgment, thus cannot be directly applied to the top-k ground-truth. To address this problem, we extend NDCG and ERR to -NDCG and -ERR by taking the position of items as the absolute label using the way defined in Section 3.2.1. As a result, the derived evaluation measures actually emphasize the importance of the ordering of the top k items.

3.3.1 -NDCG
We first give the precise definition of NDCG as follows.

1 l 2rj - 1

N DCG@l =

,

Nl j=1 log2(1 + j)

where rj is the relevance label of the item with position j in the output ranked list, and Nl is a constant which denotes the maximum value of NDCG@l given the query.
Using the notations in Section 3.2.1, we can extend NDCG to -NDCG with the following definition.

 - N DCG@l = 1 l

2yj(i) - 1 ,

(2)

Nl j=1 log2(1 + j)

where Nl is a constant which denotes the maximum value of -NDCG@l given the query.

3.3.2 -ERR
We first give the precise definition of ERR as follows.

ERR =

n

1 n

R(ri)

i-1
(1

-

R(rj

)),

R(r)

=

2r - 1 2rmax ,

i=1

j=1

where n is the document number of a query, and rmax is the highest relevance label in this query.
Similar as -NDCG, we can extend ERR to -ERR with the following definition.

n
 - E RR=
s=1

1 ni

R(ys(i)

s-1
) (1
t=1

-

R(yt(i)),

R(r) =

2r - 1 2 , ym (i)ax

(3)

where ym (i)ax is the relevance label in the top position, as defined in Section 3.2.1.

4. EXPERIMENTAL RESULTS
In this section, we empirically evaluate our proposed topk labeling strategy and ranking model. Firstly, we conducted user studies to compare the effectiveness and efficiency of our top-k labeling strategy with traditional absolute judgment based on the dataset from the Topic Distillation task of TREC2003. Secondly, we compared FocusedRank with its corresponding traditional ranking models and top-k ListMLE [30] based on both the ground-truths from the absolute judgment and the top-k ground-truths. The experimental results show the superiority of our labeling strategy and ranking model to previous work.
3Since MAP is mainly designed for binary judgment scenario, we omit the modification on it.

755

Table 1: Comparison results of time efficiency

Method

Time per judgment(s) Time per query(min) Judgment complexity #Judgments per query

Top-k labeling

5.51

13.13

O(n log k)

142.76

Five-grade judgment

13.87

11.78

O(n)

50

4.1 Top-k Labeling
To study whether top-k labeling is "easier" to make than absolute relevance judgments, we compare the Top-k labeling strategy (i.e., k = 10) with the popular five-graded ("bad", "fair", "good", "Excellent", "perfect") absolute relevance judgment method. We investigate which one is a better judgment method under two basic metrics, namely time efficiency [26], and agreement among assessors [7].
4.1.1 Experiment Design
We describe our experimental design from the following four aspects:
Data Set: We adopted all the 50 queries from the Topic Distillation task of TREC2003 as our query set. For each query, we then randomly sampled 50 documents from its associated documents for judgment. Existing Web pages in corpus of TREC2003 were employed in labeling to avoid the time delay in downloading content from Internet. In TREC topics, most of the queries have clear intent in the form of query descriptions, which are also exhibited along with the queries in labeling for better understanding.
Labeling Tools: We designed labeling tools for two judgment methods separately, i.e., the top-k labeling tool T 1 and the traditional five-graded relevance judgment tool T 2. In T 1, a query with its description is shown at the top, and two associated Web pages are placed at the main area. An assessor is then asked to decide which one is more relevant. In T 2, a query with its description is shown at the top followed with five grade buttons, and a Web page is placed at the main area. An assessor is asked to decide which grade should be assigned to that page. A timer is introduced to both tools for computing time per judgment. Assessors can click the clock button to stop the timer if they want to have a break or leave for a while. This will ensure the computing accuracy.
Assessors: There are five assessors participating our user study. These assessors are all graduate students who are familiar with Web search. They all received a training in advance on how to use the tools and on the specifications of the five grades.
Assignment: To make the comparison valid, the assignment should meet the following requirements. Firstly, for each method, all the selected documents should be judged at least once to obtain a complete data set. Secondly, for each assessor, he/she is most likely to memorize some information on the documents for a given query after judging. Therefore, to compare the methods independently, we shall ensure that each assessor will not see the same query under different tools to minimize the possible order effect. Finally, each tool has to be utilized by all the assessors to avoid the possible differences between individuals. Therefore, we adopted the following assignment to satisfy all the requirements: (1)

AB AB AB

AB 0.6749 0.1138 0.1047

AB 0.2766 0.8198 0.3779

AB 0.0485 0.0664 0.5174

Table 2: Assessor agreement for preference judgments in top-k labeling results

AB AB AB

AB 0.6272 0.2825 0.1534

AB 0.2913 0.5232 0.3826

AB 0.0815 0.1944 0.4640

Table 3: Assessor agreement for inferred preference judgments in five-graded labeling results

all the 50 queries are divided into five folds {Qi}5i=1, where each fold has 10 queries; (2) for i = 1, . . . , 4, each assessor Ui judges Qi with T 1 and Qi+1 with T 2, and the assessor U5 judges Q5 with T 1 and Q1 with T 2. At least such two different assignments are needed to compute the agreement among assessors.
4.1.2 Evaluation Results and Discussions
Two basic metrics are utilized to evaluate the labeling strategies.
(1) Time efficiency: Time efficiency [26] is used to measure the cost of labeling. It is dependent on the time per judgment and the complexity of judgment (the total number of judgments for each query). Statistically, less time per judgment suggests easier judgment.
(2) Agreement: Here we measure the agreement between two assessors over all judgments as [7] to average out differences in expertise, prior knowledge, or understanding of the query. For comparison, we inferred preferences from the absolute judgements: if the judgment on item A was greater than the judgment on item B, we inferred that A was preferred to B (denoted by A B). A tie between A and B is denoted by AB.
As shown in Table 1, it is obvious that the average time per judgment in absolute judgments is longer than that of the preference judgments in top-k labeling strategy, e.g. about 2  3 times. The results verifies the common sense that the preference judgment is easier than absolute judgment. Meanwhile, from the average number of judgments conducted on each query, we can find that the top-k labeling strategy will take more judgments than the absolute judgments, at a scale around the theoretical value log k (i.e. k=10). Most importantly, we can see that the total judgment time spent on each query is comparable between the two methods. The results indicate that by adopting the top-k labeling strategy, the complexity of pairwise preference judgment becomes similar to that of the absolute judgment. Therefore, it is feasible to use top-k labeling in practice.
The agreement among assessors for preference judgment

756

Methods SVMM AP RankSVM FocusedSVM AdaRank RankBoost FocusedBoost
ListNet RankNet FocusedNet Top-k ListMLE

N@1
0.4006 0.4118 0.4060 0.3789 0.3972 0.3947 0.4114 0.4013 0.3968 0.4057

Graded MQ2007 (Three-grade relevance judgments)

N@2 N@3 N@4 N@5 N@6 N@7 N@8

0.4039 0.4058 0.4083 0.3941 0.4051 0.4098 0.4110 0.4086 0.4103 0.4091

0.4111 0.4051 0.4077 0.3955 0.4062 0.4110 0.4134 0.4076 0.4126 0.4115

0.4128 0.4099 0.4079 0.4024 0.4095 0.4132 0.4153 0.4118 0.4153 0.4143

0.4165 0.4173 0.4123 0.4066 0.4150 0.4164 0.4204 0.4156 0.4191 0.4188

0.4217 0.4216 0.4179 0.4119 0.4197 0.4235 0.4243 0.4211 0.4245 0.4247

0.4266 0.4267 0.4234 0.4169 0.4260 0.4282 0.4300 0.4267 0.4301 0.4293

0.4322 0.4320 0.4299 0.4225 0.4316 0.4317 0.4332 0.4330 0.4341 0.4346

N@9
0.4363 0.4380 0.4351 0.4287 0.4374 0.4368 0.4389 0.4379 0.4403 0.4392

N@10
0.4419 0.4447 0.4400 0.4345 0.4438 0.4422 0.4442 0.4451 0.4459 0.4443

ERR
0.3146 0.3178 0.3196 0.3061 0.3101 0.3199 0.3206 0.3157 0.3223 0.3168

Methods SVMM AP RankSVM FocusedSVM AdaRank RankBoost FocusedBoost
ListNet RankNet FocusedNet Top-k ListMLE

-N@1
0.4530 0.4545 0.4539 0.3896 0.4438 0.4529 0.4541 0.4490 0.4623 0.4570

-N@2
0.5023 0.4932 0.5087 0.4438 0.4825 0.4943 0.4937 0.4875 0.5108 0.4991

Top-k MQ2007 (Top-k judgments)

-N@3 -N@4 -N@5 -N@6 -N@7

0.5389 0.5315 0.5448 0.4745 0.5243 0.5312 0.5314 0.5269 0.5460 0.5356

0.5702 0.5664 0.5773 0.5062 0.5567 0.5633 0.5669 0.5586 0.5783 0.5719

0.5951 0.5930 0.6024 0.5358 0.5850 0.5918 0.5922 0.5865 0.6028 0.5969

0.6178 0.6135 0.6224 0.5575 0.6066 0.6123 0.6132 0.6077 0.6240 0.6176

0.6346 0.6306 0.6404 0.5777 0.6234 0.6288 0.6299 0.6249 0.6409 0.6339

-N@8
0.6479 0.6445 0.6527 0.5952 0.6361 0.6420 0.6419 0.6390 0.6529 0.6476

-N@9
0.6591 0.6564 0.6646 0.6089 0.6476 0.6523 0.6530 0.6512 0.6633 0.6585

-N@10
0.6690 0.6655 0.6739 0.6190 0.6571 0.6628 0.6613 0.6603 0.6735 0.6673

-ERR
0.6227 0.6205 0.6287 0.5637 0.6131 0.6187 0.6195 0.6143 0.6336 0.6228

Table 4: Performance comparison on Graded MQ2007 and Top-k MQ2007

in top-k labeling and for inferred preference judgment in absolute judgment is shown in Table 2 and Table 3, respectively. Each cell (X1,X2) is the probability that one assessor would say X2(column) given that another assessor said X1(row). Therefore, they are row normalized. From the results, we can see that by adopting preference judgment, top-k labeling can largely improve the agreement among assessors over the absolute judgment. The overall agreement among assessors reaches 74.5% under top-k labeling, while it is only 54.7% under absolute judgment. We conducted 2 test to compare the ratio of the number of pairs agreed on to the number disagreed on for both top-k labeling and absolute judgment. The difference is significant (2 = 420.7, df = 1, p < 0.001). We also investigate the agreement among assessors only on preference pairs (by ignoring ties), where the agreement ratio is 89.7% under top-k labeling, and 83.1% under absolute judgment. We test the difference between the two methods using the ratio of agreed preference pairs to disagreed preference pairs, which is also significant (2 = 28.5, df = 1, p < 0.001).
From the above results, we can conclude that the topk labeling strategy is both efficient and effective to obtain reliable judgments from human assessors, as compared with traditional absolute judgment.
4.2 Performance of FocusedRank
In this section, we empirically evaluate the performance of our proposed ranking model, i.e. FocusedRank. Specially, we conducted extensive experiments to compare FocusedRank with different state-of-the-art ranking models based on both the ground-truths from the absolute judgment and the top-k ground-truths. Note that k is set to 10 in our experiments.

Besides, we also investigated the impact of the balance factor  in our proposed FocusedRank.
4.2.1 Experimental Settings
For comparison, we constructed two datasets, each with both the absolute judgment and top-k labeling. One dataset comes from the benchmark LETOR4.0 collection. There are two homologous datasets with different labeling in LETOR4.0, one is referred to as MQ2007 with three-graded relevance judgments, and the other is MQ2007-list with the total order judgments as the ground-truth. The two datasets share the same queries. The only difference lies in that the documents of a query in MQ2007 is the subset of the documents of the corresponding query in MQ2007-list. Thus the intersection of two document sets on each query is adopted. Those from MQ2007 comprise the ground-truth with absolute judgments, referred as Graded MQ2007. While those from MQ2007-list become the top-k ground-truth by only preserving the total order of top k documents on each query, referred as top-k MQ2007.
The other dataset is the one manually constructed in previous user study experiments with 50 queries from the TREC2003 Topic Distillation task. The one with the five-graded absolute relevance judgments is denoted as Graded TD2003, and the one with top-k labeling is denoted as Top-k TD2003.
We divided each dataset into five subsets, and conducted 5-fold cross-validation. In each trial, three folds were used for training, one fold for validation, and one fold for testing. For RankSVM and SVMMAP the validation set in each trial was used to tune the coefficient C. For RankNet and ListNet it was used to determine the number of iterations. For our FocusedRank, the validation set was used to tune the balance factor .

757

Methods SVMM AP RankSVM FocusedSVM AdaRank RankBoost FocusedBoost
ListNet RankNet FocusedNet Top-k ListMLE

N@1
0.5055 0.5019 0.5126 0.5278 0.5230 0.5486 0.5229 0.4971 0.5265 0.4994

Graded TD2003 (Five-grade relevance judgments)

N@2 N@3 N@4 N@5 N@6 N@7 N@8

0.5356 0.5409 0.5505 0.5436 0.5094 0.5275 0.5480 0.5304 0.5660 0.5190

0.5335 0.5446 0.5498 0.5619 0.5179 0.5356 0.5514 0.5612 0.5642 0.5356

0.5374 0.5657 0.5633 0.5740 0.5362 0.5532 0.5663 0.5638 0.5706 0.5504

0.5515 0.5816 0.5642 0.5774 0.5456 0.5554 0.5736 0.5642 0.5780 0.5540

0.5584 0.5829 0.5656 0.5797 0.5507 0.5706 0.5804 0.5765 0.5804 0.5717

0.5669 0.5867 0.5642 0.5845 0.5555 0.5790 0.5885 0.5817 0.5848 0.5746

0.5743 0.5847 0.5761 0.5872 0.5623 0.5856 0.5949 0.5837 0.5898 0.5798

N@9
0.5763 0.5905 0.5840 0.5944 0.5640 0.5876 0.5944 0.5895 0.5948 0.5861

N@10
0.5801 0.5991 0.5885 0.5982 0.5750 0.5955 0.5985 0.5983 0.6082 0.5885

ERR
0.5102 0.5072 0.5129 0.5132 0.5119 0.5466 0.5225 0.5059 0.5660 0.5083

Methods SVMM AP RankSVM FocusedSVM AdaRank RankBoost FocusedBoost
ListNet RankNet FocusedNet Top-k ListMLE

-N@1
0.2248 0.2246 0.2243 0.2029 0.2082 0.2583 0.2685 0.2776 0.2473 0.2389

-N@2
0.2822 0.2893 0.2965 0.2979 0.2931 0.3168 0.2731 0.2946 0.3268 0.2861

Top-k TD2003 (Top-k judgments)

-N@3 -N@4 -N@5 -N@6 -N@7

0.2884 0.2921 0.2958 0.2995 0.2921 0.3124 0.2694 0.2946 0.3237 0.3055

0.2973 0.3083 0.3192 0.3116 0.3103 0.3191 0.2789 0.3021 0.3223 0.3222

0.3271 0.3253 0.3235 0.3289 0.3255 0.3346 0.3044 0.3227 0.3507 0.3480

0.3359 0.3387 0.3415 0.3379 0.3365 0.3517 0.3142 0.3372 0.3605 0.3552

0.3459 0.3561 0.3662 0.3427 0.3576 0.3634 0.3252 0.3470 0.3705 0.3668

-N@8
0.3606 0.3703 0.3801 0.3574 0.3654 0.3732 0.3404 0.3688 0.3769 0.3702

-N@9
0.3769 0.3794 0.3819 0.3656 0.3782 0.3861 0.3514 0.3763 0.3849 0.3913

-N@10
0.3858 0.3872 0.3886 0.3766 0.3789 0.3980 0.3624 0.3813 0.4058 0.4007

-ERR
0.3839 0.4025 0.4041 0.3777 0.3970 0.4214 0.3962 0.4199 0.4603 0.4028

Table 5: Performance comparison on Graded TD2003 and Top-k TD2003

Besides, in our experiments, when applying traditional ranking models on top-k ground-truths, we relate the top-k ground-truth to absolute labels as defined in Section 3.2.1. While applying top-k ranking models (i.e. FocusedRank and top-k ListMLE) on absolute judgment datasets, we randomly generate a total order of the documents according to graded labels and preserve the top-k order for learning.
To measure the effectiveness of ranking performance, NDCG [13] and ERR [8] are used on ground-truths from absolute judgments, while -NDCG and -ERR are adopted on top-k ground-truths.
4.2.2 Comparison results
The performance comparison between different ranking models on the two datasets is shown in Table 4 and Table 5, respectively.
From the results on the Graded MQ2007 as shown in the upper part of Table 4, we can see that the overall performance of FocusedRank is comparable with traditional pairwise and listwise ranking models in terms of both NDCG (N @j) and ERR. It shows that even though FocusedRank is proposed for the top-k ground-truth, it can work quite well on traditional absolute judgment datasets under traditional IR measures. Such results also reveals that, learning the ordering of the top items well is critical for the success of a learning to rank algorithm. Similar results can also been found on the Graded TD2003 datasets as shown in the upper part of Table 5.
From the results on top-k ground-truths in both tables (i.e. the bottom parts), we can find that FocusedRank can significantly outperform the corresponding pairwise and listwise ranking models in terms of both -NDCG (-N @j) and -ERR. For example, considering FocusedBoost, the relative

improvement over AdaRank and RankBoost is about 7.08% and 0.87% in terms of -NDCG@10, respectively, and the relative improvements in terms of -ERR is about 9.76% and 0.91%, respectively. Besides, we can also observe that under all the metrics, the best performance is almost reached by our FocusedRank (The best performance is denoted by number in bold). The results indicate that FocusedRank is particularly suitable for the top-k ground-truth. By combining the advantages of both pairwise and listwise approaches, FocusedRank can fully exploit the information in the top-k ground-truth and thus outperforms each single model.
Moreover, when we compare FocusedRank with the stateof-the-art top-k ranking model, i.e., top-k ListMLE, we can see comparable performances on both absolute judgment datasets and top-k ground-truths. In fact, some of our FocusedRank model, e.g. FocusedNet, can consistently outperform top-k ListMLE on Top-k MQ2007 in terms of both -NDCG and -ERR. The results demonstrate that FocusedRank, as a mixed ranking model, can effectively cope with the top-k learning to rank problem.
4.2.3 The impact of the balance factor 
Here we investigate the impact of the balance factor  in FocusedRank. By varying  from 0 to 1 with a step of 0.05, the curves of the ranking performance of FocusedRank in terms of -NDCG4 and -ERR are shown in Figure 1 and Figure 2. Each performance value on test sets shown in the figures is averaged using five-fold cross validation as the same way used in LETOR.
In Figure 1, the performance variations on graded MQ2007 are represented as curves with open symbols while that on
4For space limitation, we just show the results of @5,@10 for NDCG and -NDCG.

758

0.70 0.68 0.66 0.64 0.62 0.60 0.58 0.56
0.0

0.44

0.42

0.40

0.38

0.36

0.34

N @ 5 Top-k M Q 2007 N @ 10 Top-k M Q 2007 -ER R Top-k M Q 2007

0.2

0.4



0.32

N @ 5 G raded M Q 2007 N @ 10 G raded M Q 2007 0.30

ER R G raded M Q 2007 0.28

0.6

0.8

1.0

0.68

0.44

0.64 0.40
0.60 0.36
0.56

0.52

0.32

0.48
0.44 0.0

N @ 5 Top-k M Q 2007

N @ 5 G raded M Q 2007

N @ 10 Top-k M Q 2007 N @ 10 G raded M Q 2007 0.28

-ER R Top-k M Q 2007 ER R G raded M Q 2007

0.2

0.4

0.6

0.8

1.0



0.68 0.66 0.64 0.62 0.60 0.58
0.0

0.44

0.42

0.40

0.38

0.36

0.34

N @ 5 Top-k M Q 2007 N @ 10 Top-k M Q 2007 -ER R Top-k M Q 2007

0.2

0.4

0.32

N @ 5 G raded M Q 2007 N @ 10 G raded M Q 2007 0.30

ER R G raded M Q 2007 0.28

0.6

0.8

1.0



(a) FocusedSVM

(b) FocusedBoost

(c) FocusedNet

Figure 1: Performance variation of FocusedRank with  on Graded MQ2007 and Top-k of MQ2007 under corresponding evaluation measures.

Top-k MQ2007 are represented as curves with filled symbols. The results of FocusedSVM, FocusedBoost and FocusedNet are shown in Figure 1(a), (b) and (c), respectively. To make the variation trend more clear, each figure adopts double y-axes, where the black curves of -NDCG@5 (-N@5), -NDCG@10 (-N@10) and -ERR use the left y axis, while the other blue curves of NDCG@5 (N@5), NDCG@10 (N@10) and ERR utilize the right y axis. Similarly, in Figure 2 we also use double y-axes. The performance variations on graded TD2003 are represented as curves with open symbols while that on Top-k TD2003 are represented as curves with filled symbols. The results of FocusedSVM, FocusedBoost and FocusedNet are shown in Figure 2(a), (b) and (c), respectively.
From the results shown in Figure 1 and Figure 2, we find that: (1) There is a consistent trend5 for the three types of FocusedRank on the two groups of different datasets. That is, as  increases, the ranking performance first grows to reach its maximum, and then drops. (2) The overall variation of each performance curve is small. Take the most varied curves for example, the variance of the mean is 2.7% for the performance curve of -NDCG@5 on Top-k MQ2007, and the variances of the mean is 6.2% for that on Top-k TD2003.
Thus, we come to a conclusion that the performance of FocusedRank is relative stable with respect to .
5. CONCLUSIONS
In this paper, we propose a novel top-k learning to rank framework, including labeling, ranking and evaluation, which can be effectively adopted for real search systems. Firstly, a top-k labeling strategy is proposed to obtain reliable relevance judgments from human assessors via pairwise preference judgment. With this labeling strategy, we can largely reduce the complexity of pairwise preference judgment to O(n log k). Secondly, a novel ranking model FocusedRank is
5The small size of the datasets may be the reason for the non-smooth variation curves with 50 queries on Graded TD2003 and Top-k TD2003, compared with more than one thousand queries on Graded MQ2007 and Top-k MQ2007.

presented to capture the characteristics of the top-k groundtruth. Thirdly, two new top-k evaluation measures are derived to fit the top-k ground-truth. We verify the efficiency and reliability of the proposed top-k labeling strategy through user studies, and demonstrate the effectiveness of top-k ranking model by comparing with state-of-the-art ranking models.
There are many interesting issues for further investigation under our top-k learning to rank framework. (1) The top-k labeling strategy could be improved to further reduce the judgment complexity. For example, we may introduce the "Bad" judgment like [7] for pages that are clearly irrelevant to further save labeling effort. (2) With top-k ground-truth, the design for new ranking models remains a valuable problem to investigate. (3) It is also possible to find new top-k based evaluation measures for better comparison between different systems.
6. ACKNOWLEDGMENTS
This research work was funded by the National Natural Science Foundation of China under Grant No. 60933005, No. 61173008, No. 61003166 and 973 Program of China under Grants No. 2012CB316303.
7. REFERENCES
[1] N. Ailon and M. Mohri. An efficient reduction of ranking to classification. COLT '08, pages 87≠98, 2008.
[2] C. Buckley and E. M. Voorhees. Retrieval system evaluation, chapter TREC: experiment and evaluation in information retrieval. MIT press, 2005.
[3] C. Burges, T. Shaked, and et al. Learning to rank using gradient descent. ICML '05, pages 89≠96, 2005.
[4] R. Burgin. Variations in relevance judgments and the evaluation of retrieval performance. IPM, 28:619≠627, 1992.
[5] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. ICML '07, pages 129≠136, 2007.

759

0.50 0.45 0.40 0.35 0.30 0.25
0.0

0.60

0.58

0.56

0.54

0.52

0.50

0.48

N @ 5 Top-k TD 2003 N @ 10 Top-k TD 2003 -ER R Top-k TD 2003

0.2

0.4



0.46

N @ 5 G raded TD 2003 N @ 10 G raded TD 2003 0.44

ER R G raded TD 2003 0.42

0.6

0.8

1.0

0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10
0.0

0.62

0.60

0.58

0.56

0.54

0.52

N @ 5 Top-k TD 2003 N @ 10 Top-k TD 2003 -ER R Top-k TD 2003

0.2

0.4



N @ 5 G raded TD 2003 0.50 N @ 10 G raded TD 2003

ER R G raded TD 2003 0.48

0.6

0.8

1.0

0.50

0.62

0.45 0.40 0.35 0.30 0.25
0.0

0.60

0.58

0.56

0.54

N @ 5 Top-k TD 2003 N @ 10 Top-k TD 2003 -ER R Top-k TD 2003

0.2

0.4



N @ 5 G raded TD 2003 0.52

N @ 10 G raded TD 2003

ER R G raded TD 2003 0.50

0.6

0.8

1.0

(a) FocusedSVM

(b) FocusedBoost

(c) FocusedNet

Figure 2: Performance variation of FocusedRank with  on Graded TD2003 and Top-k TD2003 under corresponding evaluation measures.

[6] B. Carterette and P. N. Bennett. Evaluation measures for preference judgments. SIGIR '08, pages 685≠686, 2008.
[7] B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais. Here or there: preference judgments for relevance. ECIR'08, pages 16≠27, 2008.
[8] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. CIKM '09, pages 621≠630. ACM, 2009.
[9] S. Cl¥emen∏con and N. Vayatis. Ranking the best instances. JMLR, 8:2671≠2699, 2007.
[10] D. Cossock and T. Zhang. Subset ranking using regression. Learning theory, 4005:605≠619, 2006.
[11] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. JMLR, 4:933≠969, 2003.
[12] E. P. C. III. The optimal number of response alternatives for a scale: A review. Journal of Marketing Research, 17, No. 4:407≠422.
[13] K. J®arvelin and J. Kek®al®ainen. Ir evaluation methods for retrieving highly relevant documents. SIGIR '00, pages 41≠48, 2000.
[14] T. Joachims. Optimizing search engines using clickthrough data. KDD '02, pages 133≠142, 2002.
[15] J. Kek®al®ainen. Binary and graded relevance in ir evaluations-comparison of the effects on ranking of ir systems. IPM, 41:1019≠1033, 2005.
[16] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27:2:1≠2:27, 2008.
[17] L. P., B. C., and W. Q. Mcrank: learning to rank using multiple classification and gradient boosting. In NIPS2007, pages 845≠852.
[18] T. Qin, T.-Y. Liu, and et al. Letor: A benchmark collection for research on learning to rank for information retrieval. Information Retrieval, 13:346≠374, 2010.
[19] T. R., S. W. Jr., and V. J.L. Towards the

identification of the optimal number of relevance categories. JASIS, 50:254≠264, 1999. [20] K. Radinsky and N. Ailon. Ranking from pairs and triplets: information quality, evaluation methods and query complexity. WSDM '11, pages 105≠114, 2011. [21] F. Radlinski and T. Joachims. Query chains: learning to rank from implicit feedback. KDD '05, pages 239≠248, 2005. [22] F. G. Rebecca and N. Melisa. The neutral point on a likert scale. Journal of Psychology, 95:199≠204, 1971. [23] P. R.L. The analysis of permutations. Applied Statistics, 24(2):193≠202, 1974. [24] M. Rorvig. The simple scalability of documents. JASIS, 41:590≠598, 1990. [25] C. Rudin. Ranking with a p-norm push. In COLT, pages 589≠604, 2006. [26] R. Song, Q. Guo, R. Zhang, and et al. Select-the-best-ones: A new way to judge relative relevance. IPM, 47:37≠52, 2011. [27] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. SIGIR '98, pages 315≠323. ACM, 1998. [28] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. IPM, 36:697≠716, 2000. [29] E. M. Voorhees. Evaluation by highly relevant documents. SIGIR '01, pages 74≠82. ACM, 2001. [30] F. Xia, T.-Y. Liu, and H. Li. Statistical consistency of top-k ranking. In NIPS, pages 2098≠2106, 2009. [31] J. Xu and H. Li. Adarank: a boosting algorithm for information retrieval. SIGIR '07, pages 391≠398, 2007. [32] Yao. Measuring retrieval effectiveness based on user preference of documents. JASIS, 46:133≠145, 1995. [33] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support vector method for optimizing average precision. SIGIR '07, pages 271≠278, 2007. [34] B. Zhou and Y. Yao. Evaluating information retrieval system performance based on user preference. JIIS, 34:227≠248, 2010.

760

Dual Role Model for Question Recommendation in Community Question Answering

Fei Xu1,2, Zongcheng Ji1,2, Bin Wang1,3
1Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China 2Graduate University of Chinese Academy of Sciences, Beijing, China
2{feixu1966, jizongcheng}@gmail.com 3wangbin@ict.ac.cn

ABSTRACT
Question recommendation that automatically recommends a new question to suitable users to answer is an appealing and challenging problem in the research area of Community Question Answering (CQA). Unlike in general recommender systems where a user has only a single role, each user in CQA can play two different roles (dual roles) simultaneously: as an asker and as an answerer. To the best of our knowledge, this paper is the first to systematically investigate the distinctions between the two roles and their different influences on the performance of question recommendation in CQA. Moreover, we propose a Dual Role Model (DRM) to model the dual roles of users effectively. With different independence assumptions, two variants of DRM are achieved. Finally, we present the DRM based approach to question recommendation which provides a mechanism for naturally integrating the user relation between the answerer and the asker with the content relevance between the answerer and the question into a unified probabilistic framework. Experiments using a real-world data crawled from Yahoo! Answers show that: (1) there are evident distinctions between the two roles of users in CQA. Additionally, the answerer role is more effective than the asker role for modeling candidate users in question recommendation; (2) compared with baselines utilizing a single role or blended roles based methods, our DRM based approach consistently and significantly improves the performance of question recommendation, demonstrating that our approach can model the user in CQA more reasonably and precisely.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Filtering
General Terms
Algorithms, Design, Experimentation
Keywords
Community Question Answering, Role Analysis, Question Reommendation, Dual Role Model, PLSA
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

1. INTRODUCTION
Community Question Aswering (CQA) is a web service where people can seek information (posting a question and getting the answer of it from others) and share knowledge (answering a question). Yahoo! Answers1 and Baidu Zhidao2 are two typical examples of CQA system. Compared with the traditional information retrieval, CQA bases on the community, which is a form of social network, so it can make best of user's collective wisdom to meet the information needs of users more easily and accurately.
In CQA system, there are a large number of questions posted every day. Take Yahoo! Answers for example, there are about 207 thousands new questions asked daily [1]. If we can automatically recommend the new question to appropriate users to answer, it will help the question be resolved as soon as possible, which will improve the CQA system's performance. In addition, it will meet the answerers' needs to answer questions. As we can see, question recommendation is a very important component in a CQA system.
The core issue of question recommendation is how to represent the users' interests (profile) and the questions, which is called the representation model. Based on that, we can assess the match between a question and each user, and then recommend the question to top N users who are the most consistent with it. Of course, we can solve question recommendation from another perspective, which is matching a user with each question and recommending the appropriate questions to him. Both of these types of recommendation tasks aim to make new questions answered as early as possible and satisfy the user better. Essentially, the key issues of both of them are the representation models for users and questions. As our target is recommending a new question to the appropriate users to answer, this paper focuses on the first type of recommendation task. At present, a lot of representation models have been proposed. Dror et al [5] represented the user and question as vectors consisting of multi-channel features and casted question recommendation as a classification problem. Other methods [2, 4, 6] utilized latent semantic models (PLSA, LDA, etc.) to model the user and question as the distribution of several topics.
As we can see, each user in CQA plays two different roles (dual roles) simultaneously: the asker and the answerer. That is, a user not only posts his questions, but also is able to answer someone else's questions. Intuitively, the profiles of the two roles of users are different from each other, which existing methods have not paid attention to. For example, a piano teacher wants to learn some computer knowledge which he is not familiar with. Thus he is most likely to ask lots of questions related to computer, and answer many piano-related questions based on his specialty. As an asker, a user may post some questions in
1 http://answers.yahoo.com/
2 http://zhidao.baidu.com/

771

the field that he is not familiar with. In contrast, as an answer, the user will solve the question which he is good at and interested in.
Are there distinctions between users' roles? How do different roles affect the performance of question recommendation? Whether we can legitimately combine the characteristics of different roles to improve the effectiveness of the recommendation system? All of these important issues are worthy of our concern. However, current recommendation methods have not in-depth studied the different characteristics of users' roles and their different influences on question recommendation. All of previous methods only modeled the user using a single role, or simply mixed the two roles together to represent the user without considering the distinctions between roles.
This paper systematically investigates the distinctions between users' dual roles and how they affect the performance of question recommendation differently. To the best of our knowledge, this is the first work on studying these important issues. While Nam et al. [30] observed that users in CQA are divided into askers and answerers and only a few of them both ask and answer in the same category through statistics, they have not theoretically analyzed the distinctions between users' different roles and their different influences on question recommendation. Moreover, we propose the Dual Role Model (DRM) to model the dual roles of users effectively. Finally, we present the DRM based approach to question recommendation, which takes full advantage of users' different roles to improve the effect of question recommendation. There are three primary contributions of our work.
First, DRM which considers the two different roles of users separately provides a more precise and appropriate user representation model for question recommendation in CQA. Specifically, we utilize DRM to analyze the latent topic information of different roles for modeling the user. According to different independence assumptions, two variants of DRM are achieved: (1) independent DRM that assumes that users are independent of each other and models each user individually; (2) dependent DRM which considers the dependence between users.
Next, we carried out systematic experiments on a real-world data to explore the distinctions between users' roles and compare the effects of recommendation methods that are based on asker role, answerer role or blending both of these roles. The results show that not only the two roles but also their influences on question recommendation are different from each other distinctly. In addition, simply mixing the roles together will impair the performance of recommender methods.
Finally, our DRM based recommendation approach allows us to naturally integrate the user relation between the answerer and the asker with the content relevance between the answerer and the question into a unified probabilistic framework, which is more interpretable. Most previous methods only consider the content relevance. There have been several approaches that make use of the user relation [3, 5], however in these approaches, the user relation is either obtained through somewhat heuristic statistics outside of the model or combined with the content relevance by a linear interpolation.
The remainder of this paper is organized as follows: Section 2 introduces some prior work related to our approach. Section 3 is the preliminary description of question recommendation in CQA. Section 4 discusses our dual role model and how to use it in question recommendation. Experimental results are presented in Section 5. At last, we conclude the paper and discuss about the future work in Section 6.

2. RELATED WORK
In this part, we review previous work which is related to our approach: recommender system, question recommendation.
2.1 Recommender System
Because question recommendation is a type of recommender system, we first review general recommender systems. Recommender systems can be divided into three stages based on how recommendations are made: content-based recommendations, collaborative filtering and hybrid approaches [18]. In contentbased recommendations, the user will be recommended items similar to the ones the user preferred in the past. In collaborative filtering, the user will be recommended items that people with the similar tastes and preferences liked in the past, that is, user will help each other find what they may like. In order to combine the advantage of both previous methods together, hybrid approaches are proposed. All these recommender systems firstly attempt to profile user preferences based on his history logs, and then recommend items according to the relevancy between him and items. Different kinds of methods are used to capture the model of users, such as classifying [24, 28], PLSA [13], matrix factorization [29], and ranking-oriented approach [17]. However, the user in these general recommender systems only plays one single role, which is significantly different from question recommendation. Therefore, we should pay close attention to this difference as we have mentioned in the above section.
2.2 Question Recommendation
With CQA system becoming popular in recent years, many people turn their attention to question recommendation in CQA, e.g., [2, 3, 4, 5, 6, 16]. Overall, there are two main lines to solve this problem in previous work.
On one hand, question recommendation is consider as a classifier problem which is similar to [5]. In [5], Dror et al. proposed a representation model based on multi-channel vector space model, where the user and question are represented as the vector with multiple dimension features from multi-channel data. Then, the matching degree between a user and a question is learned from their respective features using a binary classifier. Although this model treats user attributes in the answered-channel and askedchannel as two groups of features respectively, all the features are integrated into a single vector space model to represent the user's dual roles without considering the distinctions between user's different roles and their different influences on question recommendation.
On the other hand, we can learn a ranking model to generate a recommendation list for question recommendation. In these earlier works, various extensions of Probabilistic Latent Semantic Analysis (PLSA) or other topic models are developed. Wu et al. [2] presented an incremental automatic question recommendation framework based on PLSA. Question recommendation in their work considered both the users' interests and feedback. Guo et al. [4] developed a general generative model based on basic Latent Dirichlet Allocation (LDA) model for questions and answers in CQA. In this approach, they combined topic-level information about questions and users with word-level information to improve question recommendation. In order to deal with the data sparsity, Qu et al. [6] used a user-word aspect model instead of direct aspect model [9] to model user preferences. However, all of these methods have used a single role, or simply blended roles to represent the user, which have not distinguished user's different roles and considered how they affect the performance of question recommendation differently.

772

3. PRELIMINARIES

Given the question set

... | | and the user set

... | | , where | | is the number of questions and | | is the

number of users. Each question in Q is denoted as a triple

,,

. The is the text content of the question. For instance,

may include the title or the detailed description of the question.

If we assume that words are independent, can be denoted as a

bag of words ... | | , where | | is the number of words in .

The denotes the answerer of the question (it is also the an-

swerer role of user ). The denotes the asker of the question (it

is also the asker role of user ). If there are multiple answerers in

the question, all answerers will be separated. If the question is not

answered, is null.

Based on the previous discussion in the section of related work,

we choose the idea of ranking to solve the problem of question

recommendation. For a new posted question, the question an-

swerer recommendation task is to suggest a ranked list of users

who are suitable to answer it. To tackle with this problem, we

need to resolve the two sub-problems: question and user represen-

tation, the method of ranking recommendation candidates.

Since Probabilistic Latent Semantic Analysis (PLSA) [20]

can effectively mines the latent semantic information of users and

questions, it has been widely used to obtain the question and user

representation in question recommendation, e.g., [2, 3, 4, 6].

PLSA assumes that users and questions are generated from a mix-

ture of some latent topics. We can compute the consisten-

cy between the distribution on topics of a user and a question to

determine whether to recommend the question to the user.

We summarized the previous PLSA based methods for question

recommendation and discovered that they can be divided into two

main categories: (1) methods that model the user indirectly. Simi-

larly to [2], it takes a question as one document and use PLSA to

model the question to gain its distribution on topics at first. Then

the user can be represented as the average of topic distributions of

all the questions that he accesses; (2) methods that obtain the

model of the user directly. In these methods, all the questions that

a user accesses are treated as one document. Then PLSA is used

directly to get the topic information of the user. A typical ap-

proach is the user-word aspect model applied by Qu et al. [6].

This model is proposed by Popescul et al. [7], which improves

Hofmann's aspect model [9] for collaborative filtering.

However, when these PLSA based methods modeling the user,

they did not pay attention to the user's dual roles and their distinc-

tions. In order to effectively analyze characteristics of different

roles and make use of both of user roles to improve the perfor-

mance of question recommendation, we propose a Du-

al Role Model (DRM) based on PLSA to model the user in CQA

precisely. According to different independence assumptions, we

implement two variants of DRM. In the next section, we will de-

tail generation processes of these variants and describe the DRM

based method for question recommendation.

4. MODEL DESCRIPTION

4.1 Independent DRM
With the assumption that all users are independent of each other in independent DRM (IDRM), we separately model the dual roles of each user. As Figure 1 illustrates, the IDRM can be divided into two steps. First, we employ the PLSA to analyze the topic information of all the questions, and then model the answerer role and asker role of each user based on questions which he answers or asks.

z

q |t| |Q|

Figure 1: Independent DRM.

We introduce the latent variable

to indicate each

topic under users and questions. The model of user's answerer

role can be represented as its topic distribution | . Similarly,

the asker role is characterized by

| and the latent topic

information of the question is | . According to the first step

of IDRM in the Figure 1, the generative model for question/word

co-occurrences is defined as: a latent topic is obtained with

probability , and then a question q is generated with probabil-

ity | and a word is generated with probability | .

Therefore, we can compute the joint probability , of ob-

serving a question together with a word based on topic varia-

ble as follows:

,

|

|

Then considering all question/word pairs , set , the log likelihood is

in question

,

,

,
where q, w is the frequency of word in the question . We use the Expectation Maximization (EM) method to learn
the model parameters , | and | : E-Step,

|,



|

|

|

|

M-Step,

,

|,

,

|

,

|,

|

,

|,

After obtaining all questions' representations, we perform the second step to get the representations of users' different roles. The user's answerer role is defined as the combination of topic distributions of all questions that he answers, and the modeling method is similar for the asker role. Intuitively, we can give an example to illustrate the feasibility of this approach. For example, if a user answers lots of questions related to using computer, so the profile

773

of his answerer role is very likely to be related to this topic. Specifically, the role models | and | are estimated as:

|

 

| |

|

 

| |

where, and

is the set of questions that the user answers, is the set of question he asks.

4.2 Dependent DRM

Different from the IDRM, the assumption made in dependent

DRM (DDRM) is that there is dependence between users. As we

can see in Figure 2, DDRM assumes that the answer and the asker

are dependent on each other when not observing the latent varia-

ble. The assumed generative model is as follows. We first pick a

latent topic to some prior . We then generate the answerer ,

the asker , and the content

of question with corre-

sponding probability | , | , and 

|

,.

Thus, the joint probability distribution of a triple , ,

of

question is defined as:

,,

|

|

|

,

In the above equation, , is the frequency of word in the content .
Accordingly, the log likelihood in DDRM is

log , ,

,,

and we can also train the model using EM method as follows: E-step,

|, ,



|

|

|

,

|

|

|

,

M-step,

|, ,
,,

|

,

|, ,

,,

|

|, ,

,

|

|, ,

,

The IDRM and the DDRM respectively model the user's dual roles from different perspectives. Compared with previous models that do not take the dual roles and their distinctions into account, DRM provides a more precise and appropriate user representation model for question recommendation. Apart from different independence assumptions between users, we can see that the IDRM is a type of method modeling user role indirectly while the DDRM is a method which learns the role model directly.

4.3 Question Recommendation
Based on any one of the above DRM variants, we build the
DRM based method for question recommendation that takes full
advantage the characteristics of different user roles. When a new question arriving, we compute posterior probability | for

z

|t|

q

|Q|

Figure 2: Dependent DRM.

each candidate user , and then recommend this question to the top N users. | is obtained by:

|

,

,,

|

||

|

|

/| |

|

,

where, the first step uses the Bayesian formula for an equivalent

transformation. In the second step, the question is decomposed

into its content and its asker . In addition, we only need to

consider the candidate's answerer role when we evaluate whether

he is suitable to answer this question. Therefore, the is repre-

sented as his answerer role . The third step and fourth step

is based on the role models and the question model obtained in

DRM, where the generation probability | is normalized by

the length of question content | |.

In this recommendation approach, |

| denotes the

consistency of the answerer and the asker over topics, which

models the user relation between the answerer and the asker. Cor-

respondingly,

|

| , measures the con-

sistency of the answerer and the question content over topics,

which models the content relevance between the answerer and the

question. As we can see, our DRM based method takes full ad-

vantage of users' dual roles to improve the performance of ques-

tion recommendation. Moreover, this method utilizes a unified

probabilistic framework to naturally associate the user relation

with the content relevance together, which is more interpretable.

Compared with the DRM, both the methods described in [2]

and [6] employed a single role model to represent the user and

ignored the user relation when recommending question to users.

In the next section, these methods will be used as two groups of

baselines in our experiments.

5. EXPERIMENTS
We evaluate the proposed approach using a real-world data from Yahoo! Answers and conduct different experiments to address the following questions: (1) Are there any distinctions between users' dual roles and how they affect the result of question recommendation? (2) Does the proposed DRM improve the effectiveness of question recommendation compared with other base-

774

line methods? (3) Which of the two variants of DRM for question recommendation, namely IDRM and DDRM, is more effective?
5.1 Data Sets
In order to obtain the data sets for experiments, we used Yahoo! Answers API3 to crawl 246490 resolved questions posted in 2011 from Yahoo! Answers. All the questions are lowercased and all stop words are removed from questions using a standard list of 418 common terms before further experiments.
In our question set , we divide the whole question set and user set , into three subsets according to the user participation degree. For each subset, we split it into the training set and the testing set based on the asked time of questions. The training set is used solely for parameter estimation and the test set is used for evaluation purposes. In each subset, we take about 9/10 of questions as training set, and the rest as testing set. The data set statistics of all subsets are listed in Table 1. Each dataset contains a question set and a user set. For instance, the question set and user set of User-10 are and . We selected users who asked or answered more than 10 questions as the user set and then collected questions which were asked or answered by users in as the question set . Other subsets are similar to User-10.

User-10 User-15 User-20

Question Number
32009
28404
25690

Answer Number
97911
89144
80677

User Number 2515
1339
870

Table 1: Statistic of Yahoo! Answers data set.

5.2 Evaluation Metric
In traditional recommender systems, precision is a commonly used measure to evaluate the performance. However, precision is not suitable in the CQA context. There are so many questions asked in a CQA community every day [1] that the user can only access a very small portion of all questions. While the questions one accessed are those he is interested in, we can not guarantee that the remaining unaccessed questions are those he does not like. That is, in some cases, a user did not access a question just because he had no chance to see the question in CQA system. Therefore, we employ a new metric proposed in [6] to evaluate the effectiveness of question recommendation in CQA.
For a question in testing set, the user who provides the best answer (named the best answerer, Adamic et al. [27] have verified that answers selected as the best ones are mostly indeed the most suitable for the questions.) of this question is seemlier to answer it compared with other answerers, so it is more reasonable to recommend this question to the best answerer than other answerers. Based on this intuition, we only recommend the question to the users who actually answered it instead of all possible users in the whole dataset. Then the recommendation accuracy for this question is defined according to the rank of the user who provides the best answer. (We only keep the questions which have more than one answer and are already labeled with the best answers in the testing set.) Therefore, according to the evaluation metric applied

3 http://developer.yahoo.com/answers

in [6], we utilize the best answerer's rank as the ground truth of

our evaluation metric:

||

1

|| 1

where | | is the length of recommending list, which is equally the

number of answers, and

is the rank of the best answerer.

5.3 Role Analysis
We first discuss an interesting subject: the distinction between the user's two roles. Based on latent topic analysis of user roles in DRM, the distinction between the answer role and the asker role is defined as the difference between their topic distributions. The larger the difference between the topic distributions is, the greater the distinction between roles is. In information theory, the Kullback-Leibler divergence (KL divergence) is a commonly used non-symmetric measure of the difference between two probability distributions. We apply the KL divergence to assess the distinction between user roles. According to the modeling results of user roles in DRM, we obtain the latent topic distributions of each user role:

|

|

|



|

|

|

|



|

Based on the above two equations, the distinction between answer role and asker role of the user is

||

|

log

| |

In DRM, the number of topics is a parameter that has siginificant impact on the performance. We utilize crossvalidation to estimate the parameters. Based on experiments of tuning parameter, we empirically set topic number to 70 to train our DRM.
First, we analyze the average of KL divergence of all users in the user set of each subset to measure the overall distinction between user roles. The results are summarized in Table 2. Across
data subsetsthe overall role distinction in IDRM is about 1.3 to
1.5, and that in DDRM is about 2.4. Compared with IDRM, the role distinction in DDRM is greater and relatively more stable over different data subsets.

IDRM DDRM

User-10 1.349 2.440

User-15 1.379 2.467

User-20 1.502 2.441

Table 2: The overall distinction between user roles.
Furthermore, we take User-10 as an example to detail the distribution of role distinction, which is illustrated in Figure 3. For the DDRM, role distinction of 65.5% of users is more than 1.0, and most of them is in the range of [0.5, 3.5]. For the DDRM, role distinction of 74.3% of users is more than 2, most of which is in the range of [1.5, 4.5]. This shows that there are clear differences between different roles of most of users in CQA.
Another important result to note is that the role distinction in DDRM is more obvious and relatively more stable than that in IDRM, which can be observed in both of overall and detailed role analysis. This result may be due to that DDRM models the depen-

775

Figure 3: Distribution of the role distinction.

User Ro1e Answerer
Asker

Topic ID 41 (using computer)
30 (programming)

Top Words
free, best, windows, antivirus, virus, anti, software, spyware, download, program, xp, vista, hard, drive
c, program, file, java, programming, write, language, convert, system, net, code, array, php, number

Table 3: An example of the difference between user roles.

Role
Answerer Role Asker Role
Blended Roles

VSM VSM-an VSM-as VSM-bl

PLSA1 (modeling user indi-
rectly)

PLSA2 (modeling user directly)

PLSA1-an

PLSA2-an

PLSA1-as

PLSA2-as

PLSA1-bl

PLSA2-bl

Table 4: All versions of baselines considering users' different roles.

dence between users which is more effective to capture the peculiarities of different user roles.
After discussing the role analysis of all users, we take a typical user in DDRM as an illustrative example to show the modeling results of user roles. Table 3 lists the topic with maximum probability and corresponding top words of the topic for both of user roles. As the result shows, this user is most likely to be a junior programmer (such as junior college students from school of computer science). He has some basic knowledge of computer and is familiar with using computer, so he solved many questions about that topic. While he may be just getting started with computer programming, a lot of questions he asked are related to programming.

5.4 Question Recommendation

5.4.1 Result Comparison
In this section, we explored how different roles of users affect the result of question recommendation. Moreover, we compared our DRM-based question recommendation method with other methods.
The models proposed in previous work are classified into two main categories. The first one is the word-level Vector Space Model (VSM) which is directly used to compute the similarity between users and questions. VSM only make use of word-level information to model users and questions. For example, the user and question are represented as vectors with tf.idf word weights, and then cosine similarity between them is defined as:

,

∑ || || ∑ || ||

.

,.

,

.

,

.

,

where .

, is the word 's tf.idf weight in question ,

and .

, is the sum of 's tf.idf weights in questions that

asks or answers.

The second one is PLSA based methods. As we have specified

in section 3, these methods model the user either indirectly or

directly. For the former, we took the model in [2] (PLSA1) as

776

VSM PLSA1 PLSA2 DRM

VSM-an VSM-as VSM-bl PLSA1-an PLSA1-as PLSA1-bl PLSA2-an PLSA2-as PLSA2-bl IDRM DDRM

User-10 0.555 0.456 0.541 0.639 0.445 0.622 0.638 0.447 0.619 0.669* 0.685* 7.2%

User-15 0.600 0.492 0.591 0.650 0.482 0.642 0.651 0.485 0.647 0.675* 0.690* 6%

User-20 0.612 0.466 0.581 0.671 0.448 0.628 0.674 0.441 0.622 0.683* 0.697* 3.4%

Table 5: Recommendation accuracies of different methods for question recommendation. Each underlined value means the best result for each baseline group. `*' means the corresponding improvement over all baselines is statistically significant.

baseline. For the latter, we implemented the "user-word aspect model" presented in [6] (PLSA2) as another baseline. The details of PLSA1 and PLSA2 have been described in section 3.
In order to explore the impact of different user roles on question recommendation, we implemented the different versions of the three groups of baselines. These versions are based on the answerer role, the asker role, or the blended roles. Table 4 shows the labels of all baseline methods. Each version of a baseline is trained on the question set that users access under the corresponding role. Specifically, the blended roles mean all the question that each user answers or asks are simply mixed together as one set.
Based on cross-validation, we selected the best topic number for PLSA1 and PLSA2. The recommendation results of baselines and our DRM are summarized in Table 5, where the best result for each baseline group is underlined and the best result in each data subset is highlighted.
We first compare the performances of different roles in each baseline group. From Table 5, we observe that the answerer role always wins the best result in all baseline groups across data subsets. Especially, the answerer role is obviously better than the asker role over all the recommendation results. When data sets become denser and denser from User-10 to User-20, the effect of the answerer role becomes better and better as we expect. On the contrary, the result of the asker role appears an unexpected decrease in User-20. Furthermore, we examine the recommendation results of blended roles. As we can see, simply mixing the asker role into the answerer role not only fails to improve but worsens recommendation results of answerer role instead. According to above experiments, we conclude that different user roles reflect the different aspects of the user, moreover, there are clear distinctions between their influences on question recommendation. When modeling the user in CQA, we must distinguish the different user roles. When recommending new questions to users, it would be more appropriate to use the answerer role model to represent the candidate users.
Since the recommendation methods based on blended roles do not work well, whether our DRM based method can make full use of user's dual roles to improve the recommendation result? Tested

on each data subset, our model exhibits good performance, significantly outperforming all baselines. The relative improvement of DDRM over the best baseline result is 7.2% for User-10, 6% for User-15, and 3.4% for User-20. In addition, DDRM is significantly better than IDRM across data sets, which means considering the dependence between uses is more effective to model user roles. This result is also consistent with the above role analysis.
Another interesting result to note is that the PLSA1 which models the user indirectly is almost equivalent to PLSA2 which models the user directly on three data subsets, suggesting that it is feasible to model the user indirectly by combining the topic information questions that he accesses. Additionally, it is clear that all methods based on latent topic analysis (PLSA1, PLSA2, and DRM) always perform better than word-level VSM, which demonstrates that the latent topic based model can be more effective to represent the profile of user. And this result also verifies the conclusion drew in [6]. Moreover, it is part of the reason for that Guo et al. [4] introduced topic-level model to improve heuristic word-level methods.
5.4.2 Parameter Sensitivity
We note that the topic number K is an important parameter in our proposed DRM. Therefore, we are interested in analyzing the sensitivity of the recommendation performance of DRM with respect to the topic number. We tested these two DRM variants with 8 different values of K, which is illustrated in Figure 4. Like previous role analysis, we only present the final results in data subset User-10. The results for other subsets are similar. As we can see from Figure 4, the recommendation accuracy gradually increases when the topic number varies from 10 to 40. Then we observe that the effectiveness of both DRM based recommendation approaches begins to be relatively stable when topic number is more than 40.
6. CONCLUSION & FUTURE WORK
The user in CQA plays two different roles (dual roles) simultaneously, which is different from the user in a general recommender system. In this paper, we have systematically investigated the

777

Figure 4: Sensitivity to the topic number of DRM.
distinctions between users' dual roles and how they affect the performance of question recommendation differently. Moreover, in order to represent the user in CQA with dual roles more reasonably and precisely, we proposed a Dual Role Model (DRM) to model the user's different roles. With different independence assumptions, two variants of DRM were achieved, which were independent DRM (IDRM) and dependent DRM (DDRM). Finally, we presented the DRM based approach to question recommendation which can take full advantage of the particularities of users' different roles. Based on a unified probabilistic framework, our DRM based method naturally combines the user relation between the answerer and the asker with the content relevance between the answerer and the question.
Our experiments were carried out on a real-world data crawled from Yahoo! Answers. First, the results of user role analysis showed that there are evident differences between the answerer role and asker role of users in CQA. Comparing the effects of the two roles on question recommendation, we discovered that the answerer role model is more appropriate to represent the candidate users when recommending new questions to users. Additionally, an interesting result was that simply mixing the asker role into the answerer role not only failed to improve but impaired recommendation results of answerer role instead. Furthermore, we compared our DRM based recommendation methods with baseline methods based on a single role or blended roles. Experiment results on three data subsets showed our DRM significantly outperforms all baselines, where the relative improvement of DRM over the best baseline result is 7.2% for User-10, 6% for User-15, and 3.4% for User-20. In addition, DDRM is more effective than IDRM across data subsets, suggesting it is more effective to model users' dual roles. Finally, the parameter sensitivity analysis showed our DRM approach is robust.
There are two interesting future research directions to explore. One of the most interesting directions is to further study how the roles of users will vary over time, and whether that will have influence on question recommendation. The other interesting direction is how to diversify the recommendation results to satisfy users better.
7. ACKNOWLEDGMENTS
We thank the anonymous reviewers for their useful comments. This work is supported by the National Science Foundation of China under Grant No. 61070111.

8. REFERENCES
[1] L. Rao. Yahoo mail and im users update their status 800 million times a month. TechCrunch, Oct282009. http://techcrunch.com/2009/10/28/yahoo-mail-and-imusersupdate-their-status-800-million-times-a-month/.
[2] Hu Wu, Yongji Wang and Xiang Cheng. Incremental Probabilistic Latent Semantic Analysis for automatic question recommendation. In RecSys'08, pages 99-106, 2008.
[3] Damon Horowitz and Sepandar D. Kamvar. The anatomy of a large-scale social search engine. In WWW'10, pages 431440, 2010.
[4] Jinwen Guo, Shengliang Xu, Shenghua Bao, and Yong Yu. Tapping on the potential of Q&A community by recommending answer providers. In CIKM'08, pages 921-930, 2008.
[5] Gideon Dror, Yehuda Koren, Yoelle Maarek and Idan Szpektor. I want to answer, who has a question? Yahoo! Answers recommender system. In SIGKDD'11, pages 11091117, 2011.
[6] Mingcheng Qu, Guang Qiu, Xiaofei He, Cheng Zhang, Hao Wu, Jiajun Bu, and Chun Chen. Probabilistic question recommendation for question answering communities. In WWW'09, pages 1229-1230, 2009.
[7] Alexandrin Popescul, Lyle H. Ungar, David M. Pennock and Steve Lawrence. Probabilistic models for unified collaborative and content-based recommendation in sparse-data environments. In UAI'01, pages 437-444, 2001.
[8] Baichuan Li, Irwin King and Michael R. Lyu. Question routing in community question answering- putting category in its place. In CIKM'11, pages 2041-2044, 2011.
[9] Thomas Hofmann and Jan Puzicha. Latent class models for
collaborative filtering. In IJCAI'99, pages 688≠693. 1999.
[10] Thomas Hofmann. Probabilistic latent semantic indexing. In SIGIR'99, pages 50-57, 1999.
[11] Luo Si and Rong Jin. Flexible mixture model for collaborative filtering. In ICML'03, 2003.
[12] David M. Blei , Andrew Y. Ng and Michael I. Jordan. Latent dirichlet allocation. In Journal of Machine Learning Research, pages 993-1022, 2003.
[13] Thomas Hofmann. Collaborative filtering via gaussian probabilistic latent semantic analysis. In SIGIR'03, pages 259-266, 2003.
[14] Tom Chao Zhou, Chin-Yew Lin, IrwinKing, Michael R. Lyu, Young-In Song and Yunbo Cao. Learning to suggest questions in online forums. In AAAI'11, pages 1298-1303, 2011.
[15] Qiaoling Liu and Eugene Agichtein. Modeling answerer behavior in collaborative question answering systems. In ECIR'11, pages 67-79, 2011.
[16] Ke Sun, Yunbo Cao, Xinying Song, Young-In Song, Xiaolong Wang and Chin-Yew Lin. Learning to recommend questions based on user rating. In CIKM'09, pages 751-758, 2009.
[17] Nathan N. Liu and Qiang Yang. EigenRank: A rankingoriented approach to collaborative filtering. In SIGIR'08, pages 83-90, 2008.

778

[18] Adomavicius G., and Tuzhilin A. Toward the next generation of recommender systems: A survey of the state-of-theart and possible extensions. In IEEE Trans. on Knowledge and Data Engineering, 17(6), 2005.
[19] P. Kantor, F. Ricci, L. Rokach, and B. Shapira. Recommender Systems Handbook: A complete guide for research scientists and practitioners. Springer, 2010.
[20] Thomas Hofmann. Unsupervised learning by probabilistic latent semantic analysis. Maching Learning Journal, Vol. 42, No. 1-2, pages. 177-196, 2001.
[21] Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee and Soyeon Park. A framework to predict the quality of answers with non-textual features. In SIGIR'06, pages 228-235, 2006.
[22] Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. Finding semantically similar questions based on their answers. In SIGIR'05, pages 617-618, 2005.
[23] G. Linden, B. Smith, and J. York. Amazon.com recommendations: Item-to-item collaborative filtering. In IEEE Internet Computing, 07(1):76-80, 2003.
[24] Jiahui Liu, Peter Dolan, Elin R¯nby Pedersen. Personalized news recommendation based on click behavior. In IUI'10, pages 31-40, 2010.

[25] Y. Cao, H. Duan, Chin-Yew Lin, Y. Yu and Hsiao-Wuen Hon. Recommending questions using the MDL-based tree cut model. In WWW'08, pages 81-90, 2008.
[26] S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and R. Harshman. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391407, 1990.
[27] J. Zhang, L. A. Adamic, E. Bakshy and Mark S. Ackerman. Every-one knows something: Examining knowledge sharing on Yahoo! Answers. In WWW'08, pages 665-674, 2008.
[28] M. Pazzani and D. Billsus. Learning and revising user profiles: The identification of interesting web sites. Machine Learning. vol. 27, pages 313-331, 1997.
[29] Y. Koren, R. M. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Journal of Computer, 42(8):30-37, 2009.
[30] Kevin K. Nam, Mark S. Ackerman, Lada A. Adamic. Questions in, knowledge in? A study of Naver's question answering community. In CHI'09, pages 779-788, 2009.
[31] Pawel Jurczyk, Eugene Agichtein. Discovering authorities in question answer communities by using link analysis. In CIKM'07, pages 919-922, 2007.

779

Vote Calibration in Community Question-Answering Systems

Bee-Chung Chen
LinkedIn Mountain View, CA bchen@linkedin.com
Xuanhui Wang
Facebook Menlo Park, CA xuanhui@fb.com
ABSTRACT
User votes are important signals in community question-answering (CQA) systems. Many features of typical CQA systems, e.g. the best answer to a question, status of a user, are dependent on ratings or votes cast by the community. In a popular CQA site, Yahoo! Answers, users vote for the best answers to their questions and can also thumb up or down each individual answer. Prior work has shown that these votes provide useful predictors for content quality and user expertise, where each vote is usually assumed to carry the same weight as others. In this paper, we analyze a set of possible factors that indicate bias in user voting behavior ≠ these factors encompass different gaming behavior, as well as other eccentricities, e.g., votes to show appreciation of answerers. These observations suggest that votes need to be calibrated before being used to identify good answers or experts. To address this problem, we propose a general machine learning framework to calibrate such votes. Through extensive experiments based on an editorially judged CQA dataset, we show that our supervised learning method of content-agnostic vote calibration can significantly improve the performance of answer ranking and expert ranking.
Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications ≠ Data Mining.
Keywords: Reputation, user modeling, crowdsourcing, community question-answering.
1. INTRODUCTION
Community question answering (CQA) systems form the crowdsourced alternative to search engines for providing information. Popular and effective CQA systems such as Yahoo! Answers 1
This work was conducted when all authors were affiliated with Yahoo! 1answers.yahoo.com
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

Anirban Dasgupta
Yahoo! Labs Sunnyvale, CA anirban@yahoo-inc.com
Jie Yang
Google Mountain View, CA cnjieyang@gmail.com
provide an environment for people to share knowledge and experience, which complement search engines by allowing questions to be posed in natural language and be answered by human beings through active content (i.e., answers) creation, instead of mining existing Web pages. Since the basis of CQA systems is the wide participation, the hurdle for users to proffer answers is typically minimal. Hence, CQA systems are also prone to spam and other kinds of abuse. Commercial spam itself is a widespread problem, and one that can mostly be tackled by conventional machine learning. Other forms of abuse, e.g., low quality content, willfully wrong answers are difficult for machines to detect, and again crowdsourcing quality content identification is an engaging, scalable way of ensuring that high quality questions and answers are identified and the creators appropriately rewarded. Different CQA systems have thus implemented different kinds of voting mechanisms coupled with user reputation systems in order to surface high quality content and discourage undesired behavior.
In this paper, we focus on votes in CQA sites. Take Yahoo! Answers for example, which is heavily dependent on the best answer voting scheme. An asker (user who asks a question) can vote for the best answer to his/her question. If the asker does vote, the best answer is declared and the question is labeled as resolved. If the asker does not vote for the best answer within certain period of time, other users in the community can vote for the best answer to that question. The answer that receives the largest number of best-answer votes within a certain period of time is then declared as the best answer to this question, and the question is declared resolved. Other than best-answer votes, users can also demonstrate their quality or opinion preferences by casting thumb-up or thumbdown votes on each individual answer.
Although such voting mechanisms are designed with the intuition that they should be able to identify high quality answers and the corresponding answerers, there has been little study on the actual effectiveness of these systems in achieving this goal. On the other hand, many studies [1, 6, 15, 3, 8, 17, 10] have treated the user-voted best answers as the ground truth source of high quality answers and have developed models to predict whether an answer would be voted as the best answer based on features extracted from the answer, past activities of the answerer, etc. As a counter-point, other studies, e.g., [14, 9], report best answers not to be entirely high quality ones. In fact, as part of this work, we also observe that user-voted best answers do not always have high quality, possibly because of bias in users' voting behavior. A few examples of potential bias are:
∑ Users may vote more positively for their friends' answers.

781

Even users who have interacted with each other in the CQA before might have a positive or negative bias depending on their past experience.
∑ Users may use votes to show their appreciation to answerers instead of trying to identify high quality content.
∑ Users who are trying to game the system in order to obtain high status can create multiple accounts and vote for one another.
∑ For questions about opinions, users tend to vote for the answers that share same opinions as theirs, but not necessarily of high quality.
Thus, instead of treating users' best-answer votes as the ground truth for answer quality, we ask trained human editors to judge answers based on a set of well-defined guidelines. Our first observation is that raw user votes have low correlation with editorial judgment. Based on this finding, we ask "is it possible to calibrate the votes to mitigate the effects of potential bias, in order to increase the correlation between votes and editorially judged answer quality?"
Contributions: We make the following contributions.
∑ We propose the novel problem of vote calibration in CQA systems. The basic idea is to associate every vote with a weight. Intuitively, votes from reputable voters who consistently vote for high quality answers should carry more weight, while votes susceptible to bias should carry less weight.
∑ In Section 5, based on exploratory data analysis we identify a variety of potential factors that possibly bias the votes.
∑ We develop a novel model for vote calibration based on supervised learning in Section 4. Intuitively, with each vote k we associate a feature vector xk which includes features capturing potential bias. Then, the importance weight of vote k is a function of the weighted sum of feature values, i.e., f (xk), where  is a vector of regression coefficients, xk is the inner product of xk and , and f (∑) is a monotone function. Based on a set of editorially labeled answers,  is determined by minimizing the error of using calibrated votes f (xv) to predict the editorial labels.
∑ In Section 7, we experimentally show that the proposed vote calibration model outperforms a number of alternative methods for the answer ranking problem and the expert ranking problem. In particular, our model significantly outperforms the state-of-the-art SVM model [12] for expert ranking.
We note that since our focus is on vote calibration, we take a contentagnostic approach (i.e., no use of features extracted from answer content) similar to [12]. This makes our model applicable to votes on other types of content. Also, although we use Yahoo! Answers as the example CQA system throughout the paper, our model can be easily applied to other CQA systems. In fact we believe that our approach has the following useful properties ≠ (i) it is robust to vote gaming, having incorporated a variety of such gaming behavioral features, and (ii) it provides a general framework for calibrating user votes in general crowdsourced rating platforms.
2. RELATED WORK
Although there have been many studies on CQA, to our knowledge, the problem of how to calibrate user votes to improve the correlation between votes and answer quality has not been systematically studied before. The related work can broadly be categorized into three threads.

Predicting user-voted best answers: There is a large body of work on building models to predict user-voted best answers (e.g., [1, 6, 15, 3, 8, 17, 10]). The assumption behind this line of work is that quality judgments of answers are expensive to obtain and it seems to be reasonable to use the readily available user-voted best answers as the target to predict. Different from this line of work, we do not try to predict user-voted best answers, but show that user-voted best answers may not be high quality ones because of potential bias in users' voting behavior. Our focus is on how to calibrate votes to minimize the discrepancy between votes and quality.
Predicting editorial judgments: Along with [2, 7, 16, 9, 14, 12], we define answer quality by a set of guidelines and obtain the ground-truth quality scores through an editorial process. For example, Agichtein et al. [2] used features extracted from text, user interaction graphs and votes to predict editorial judgments. Suryanto et al. [16] developed a number of graph-based methods to estimate user expertise, and evaluated these methods using editorially judged quality and relevance. Sakai et al. [9, 14] noticed bias in users' voting behavior that can cause user-voted best answers to be unsuitable for model evaluation, and proposed a number of evaluation methods based on editorial judgments. Similar to these studies, we also want to predict editorially labeled quality scores. Although user votes have commonly been used as features, calibration of each individual vote has not be studied.
Content-agnostic user expertise estimation: User expertise score estimation is an important use case of vote calibration. In the Section 7, we will show that using a simple average of the calibrated votes of a user as his/her expertise score can outperform the best methods reported in Liu et al. [12], where a variety of methods, such as PageRank-based method, HITS-based methods, competition-based methods, are empirically compared based on editorial judgments. Earlier work on user expertise is described in the following. Zhang et al. [18] proposed a PageRank-style algorithm for online forums. Jurczyk et al. [11] leveraged the HITS algorithm. Pal et al. [13] exploited the observation that experts usually answer questions that have not yet received good answers. Bouguessa et al. [4] developed a method to identify the number of users to be considered as experts based on a mixture of two Gamma distributions.
3. DATASET
Our dataset consists of editorial data and voting data.
3.1 Editorial Data
To determine the quality of an answer, we designed a set of guidelines together with a set of carefully selected examples for each quality grade and ask a human editor to judge it. We randomly sampled around 10,000 resolved questions in five popular categories (4.2K from each of "Sports" and "Entertainment and Music", 1.2k from each of "Business and Finance" and "Consumer Electronics", and 500 from "News and Events") from US Yahoo! Answers and, for each question, sampled three answers. These questions were selected by a diffusion process first starting with a random seed of `active users ≠ those who have answered at least 10 questions and provided 5 best answers; we then sampled resolved questions that these users answered, and then again from all answers to that question. For construction of features when modeling, we however looked at our entire dataset.
When judging an answer, an editor first read the question and answer, and then gave a quality grade to the answer according to a

782

Type ABA CBA Non-Best #Ans

Editorial Quality Grade Excellent Good Fair Bad
4.1% 22.9% 65.3% 7.8% 4.9% 24.5% 60.4% 10.3% 2.6% 18.2% 65.7% 13.5%
698 4,337 13,899 2,591

#Ans 3,086 4,263 14,176 21,525

Table 1: Best Answer vs. Editorial Quality

pre-determined set of editorial guidelines. In the interest of space, we only give a high-level description of the guidelines.
∑ Excellent: The answer provides a significant amount of useful information, fully answering the question.
∑ Good: The answer provides a partial but not entirely persuasive answer to the question. Minor typos and grammatical problems are acceptable.
∑ Fair: The answer fails to add much value to the knowledge base and is neither good nor bad -- it's simply not sufficiently useful, interesting or entertaining to promote.
∑ Bad: The answer is either abusive, not germane to the question or so poor in substance or style.
Each answer was judged by one editor only. We ignored all the nonEnglish questions and answers and also ignored all the answers that the editors could not make a confident judgement on. In total, we obtain 21,525 editorially judged answers on 7,372 questions (note that some questions do not have three answers).
User-voted best answers vs. quality: Table 1 shows the distribution of editorial quality grades for three types of answers: ABA denotes the asker-voted best answer, while CBA denotes the communityvoted best answer through the majority rule. On Yahoo! Answers, CBA votes happen only when the asker of a question does not pick the best answer within certain period of time. Non-Best denote the answers that are not best answers. Notice that these three types of answers are disjoint. Each row of Table 1 sums up to one. The numbers of answers of the three types and the four editorial grades are reported in the last column and row. We make the following observations.
∑ The distribution of editorial grades for best answers are not very different from that of non-best answers, although the percentage of excellent or good best answers is slightly higher than that of excellent or good non-best answers. This indicates low correlation between users' best-answer votes and answer quality.
∑ A significant percentage (> 70%) of best answers are not even good (i.e., fair or bad). Anecdotally, users sometimes give best-answer votes to show appreciation, instead of identifying good answers. Also, a fair answer is in fact the best answer among other bad answers. These observations show the need to calibrate votes in order to use them to identify good content.
∑ Many non-best answers are actually good or excellent. This is due to the fact that only one answer can be selected as the best answer to each question even when there are many excellent ones.
Numeric quality scores: To build our vote calibration model, we give a numeric score to each editorial grade: Excellent = 1, Good = 0.5, Fair = 0, Bad = -0.5. We note that slight changes of assignments of numeric scores to editorial grades do not make a significant difference for the results as long as the order is preserved.

They also do not affect the evaluation metrics in any way, since we use ranking metrics.
3.2 Voting Data
We collect the voting data in the following manner. We start with the set of answerers of the editorially judged answers and collect all of the resolved questions that were answered by this set of answerers in a one-year period. We then collect all of the answers to this set of questions, together with all of the votes on these answers. As a result, our voting data includes 1.3M questions, 7.0M answers, 0.5M asker best answer votes, 2.1M community best answer votes and 9.1M thumb up/down votes.
Uniqueness of our data: Voters' identities are important to calibrate their votes. To our knowledge, no public CQA dataset releases the identities of the voters. Thus, we were not able to evaluate our methods based on public datasets. Instead, we collect our own data, where each vote is associated with the anonymized IDs of the voter and recipient. Due to user confidentiality agreements, our data is not publicly available.
4. VOTE CALIBRATION MODEL
We present our vote calibration model in this section. Let zk  {+1, -1} denote the value of vote k. The basic idea is simple. We associate each vote k with an importance weight wk so that the calibrated value zkwk of a vote would have higher correlation with answer quality. In order to determine these importance weights, we define a set of features to capture potential bias in users' voting behavior. Let xk denote the feature vector of vote k. Then, we predict importance weight wk using its corresponding features; i.e., wk = f (xk), where  is a vector of regression coefficients, xk is the inner product of the two column vectors (xk is the transpose of xk), and f (∑) is a monotone function to calibrate the value xk (for example, to constrain wk to be between 0 and 1). The parameter that needs to be determined is . Based on a set of answers labeled with ground-truth quality scores and the votes on these answers, we estimate  by minimizing the error of predicting quality scores using the calibrated votes.
In the rest of this section, we first introduce the notations and describe how to generate +1 and -1 vote values from the three kinds of votes on Yahoo! Answers, then explain how to predict quality scores using calibrated votes, and finally present a training algorithm for estimating the model parameters.
4.1 Notations
We use q to denote a question ID and u to denote an answerer ID. Since each user can provide at most one answer to each question in Yahoo! Answers, (q, u) form the ID of the answer to q provided by answerer u. We use v to denote a voter ID and t to denote a vote type. In Yahoo! Answers, there are three types of votes:
∑ Asker votes: These are the best answer votes by the asker of questions.
∑ CBA votes: These are the best answer votes by users in the community, called community best answer (CBA) votes.
∑ Thumb votes: These are the thumb-up and thumb-down votes on each individual answers.
Since a user can provide at most one vote for each answer and vote type, we use zq(tu)v  {+1, -1} to denote the value of the type-t vote that voter v gives the answer provided by answerer u to question q. Let xquv denote the feature vector associated with the vote that voter v gives answer (q, u). Notice that we did not add a superscript t on xquv to reduce notational cumbersome; our method

783

can easily handle features defined specifically for a particular vote
type. Let yqu denote the ground-truth numeric quality score of answer (q, u). We discuss how we convert editorial grades into numeric scores in Section 3.1. Finally, we use Vq(tu) to denote the set of all the voters who cast type-t vote on answer (q, u), and Q(ut) to denote the set of questions to which answerer u provides answers
that receive type-t votes.

4.2 Vote Value before Calibration
In this paper, we focus on binary votes. Extension to numeric ratings is future work. We generate binary votes for the three vote types in the following way.
∑ Asker votes: If the asker of a question votes for an answer as the best answer, then that answer gets a +1 vote from the asker and all the other answers get -1 votes from the asker.
∑ CBA votes: Similar to asker votes, if a voter votes for an answer as the best answer, then that answer gets a +1 vote from the voter and all the other answers get -1 vote from the voter.
∑ Thumb votes: If a voter thumbs up an answer, then the answer gets a +1 vote from the voter. If a voter thumbs down an answer, then the answer gets a -1 vote from the voter.

4.3 Calibrated Vote Aggregation
We now describe how to predict the quality scores using calibrated votes. Intuitively, we predict the quality score yqu of an answer by a weighted sum of (1) the average vote value of the answer (q, u) and (2) the average vote value of the answerer u. Because there are multiple types of votes, we compute average vote value for each type separately and then do a weighted average, where the weight on each type will be learned from a training dataset.

Calibrated vote value: Using the notations defined in Section 4.1,

the value of calibrated type-t vote that voter v gives answer (q, u)

is zq(tu)vf (xquvt), where zq(tu)v is the vote value before calibration, xquv is the feature vector associated with this vote and t is a vector of regression coefficients for type t. Notice that we have a type-

specific t, which gives us the flexibility to calibrate votes of different types differently. Although f (∑) can be any differentiable

monotone function, for concreteness, in this paper we use the sig-

moid

function:

f (x)

=

1 1+e-x

.

For

comparison

purposes,

we

also

consider f (∑) as the identity function; i.e., f (x) = x. We call the

votes calibrated through the sigmoid function sigmoid-calibrated

votes and call the votes calibrated through the identity function

linearly-calibrated votes.

It is instructive to compare sigmoid calibration with linear cali-

bration based on their definitions.

∑ Sigmoid calibration is based on a nonlinear function of the features, while linear calibration is based on a linear function of the features. Thus, sigmoid calibration has the potential to capture nonlinear characteristics in vote features.

∑ The range of the sigmoid function is between 0 and 1. Thus, the calibrated vote value is bounded by -1 and +1. The effect of a single vote is always bounded. This is not the case for linear calibration, where a vote may have unreasonably large value when some features take large values.

In Section 7, we will compare these two kinds of calibration experimentally and show the benefit of using the sigmoid function.

Average vote value of an answer: When there are multiple votes on an answer, we use the average calibrated vote value on the answer to predict its quality. The choice of average instead of the sum

is prompted by the fact that the total number of votes received is often biased severely by the age of the answer, how prominently it is displayed etc. To make this average stable, we add some number t of "pseudo votes" to the average. Specifically, using the notations defined in Section 4.1, the average type-t vote value of answer (q, u) is computed as

tt + AnsValue(qtu) (t) =

vVq(tu) zq(tu)v f t + |Vq(tu)|

(xquv

t

)

,

(1)

where t and t are two tuning parameters. In this paper, we set t = 1 and t = the average uncalibrated vote value over all type-t votes, which is computed by the sum of all type-t uncalibrated vote values divided by the total number of type-t votes in the system. Notice that t is computed based on a large number of votes and should be a quite stable constant. If an answer receives no vote, its AnsValue would be exactly t. Thus, t can be thought of as the "prior mean" of AnsValue. If an answer receives a small number of votes, its AnsValue would still be close to t, where how close they are depends on t -- the larger t is, the closer they are. However, if an answer receives many votes, then its AnsValue would not be influenced much by t.

Average vote value of an answerer/user: When an answer receives no vote or a small number of votes, we can also use the average value of the votes received by the answerer to predict the quality of the answer. Similar to Equation 1, the average type-t vote value of user u is computed as

tt + UsrValue(ut)(t) =

qQ(ut)
t +

vVq(tu) zq(tu)v f qQ(ut) |Vq(tu)|

(xquv

t

)

.

(2)

Notice that this formula is almost the same as Equation 1, except

that here we sum over all of the type-t votes received by answerer

u in the numerator and compute the total number of type-t votes

received by answerer u in the denominator.

Quality prediction function: Given the answer-level and userlevel average vote values of all types on an answer, its quality is predicted by a weighted sum of these average vote values. Specifically, we predict the quality score yqu of answer (q, u) by

Score qu(b, , ) = b
+ t,0 AnsValue(qtu) (t) + t,1 UsrValue(ut)(t),
t
where b is a bias term,  = {t,0, t,1}t consists of the weights of different components, and  = {t}t consists of all of the regression-coefficient vectors. Since answer quality is predicted by aggregating calibrated votes, we call this model calibrated vote aggregation model. When sigmoid calibration is used, we call it sigmoid-calibrated vote aggregation (SVA) model. When linear calibration is used, we call it linearly-calibrated vote aggregation (LVA) model. Notice that b,  and  are the model parameters that need to be learned from a training dataset.

User expertise: To estimate the expertise score of a user, we sim-
ply use the average calibrated vote values of that user. Specifically, the expertise score of user u is Expertise u(b, , ) = b +
t t,1 UsrValue(ut)(t). Since this is a special case of the full model, we do not specifically discuss this special case.

4.4 Training Algorithm
Given a training dataset consisting a set of answers together with ground-truth quality scores and votes on them, we determine the

784

fraction resolved by CBA 0.45 0.55 0.65 0.75

model parameters  = (b, , ) by minimizing the following loss function

()

=

1 2

(yqu

-Score

qu

())2

+

1 2

b

2

+

2 2



2

+

3 2

 2,

qu

where  2 denotes the sum of squares of individual elements in , which serves as a regularizer to prevent potential overfitting. 1, 2 and 3 are the regularization weights of each regularizer. We minimize this loss function using gradient descent.

Gradients: The formulas of the gradients are as follows. Let equ() = yqu - Score qu() denote the difference between the ground-truth quality score and the predicted quality score.

d db

() = -

equ() + 1b

qu

d dt,0

() = -

equ() AnsValue(qtu) (t) + 2 t,0

qu

d dt,1

() = -

equ() UsrValue(ut)(t) + 2 t,1

qu

d dt

() = -

equ()

t,0

d dt

AnsValue(qtu)

(t

)

qu

+

t,1

d dt

UsrValue(ut)(t)

+ 2 t,

where

d dt

AnsValue(qtu) (t)

=

d dt

UsrValue(ut)(t)

=

vVq(tu) zq(tu)v f (xquv t) xquv t + |Vq(tu)|

qQ(ut)

vVq(tu) zq(tu)v f t + qQ(ut)

(xquv |Vq(tu) |

t

)

xquv

.

If f (∑) is the sigmoid function in sigmoid calibration, its derivative f (x) is f (x)(1 - f (x)). For linear calibration, f (x) = 1 always. However, it can be shown that linear calibration can be transformed into a regular linear regression problem. Thus, we solve linear calibration by using a standard linear regression package.

Loss minimization: There are many gradient descent algorithms, which can be easily applied to minimize our loss function. In our implementation, we use L-BFGS [5].

5. EXPLORATORY ANALYSIS
Before we define the features to be used in our calibration model, we discuss a range of potential indicators of voting bias in this section. In the interest of space, we mainly use community bestanswer (CBA) votes to show our analysis. Many of the insights also apply to other types of votes.
Characteristics of CBA votes: First, the importance of CBA votes is seen in Figure 1, which plots the fraction of questions resolved by CBA votes as a function of the number of answers to the question. On average around 61% of the total number of questions in our corpus get resolved by CBA votes, and this fraction reaches its peak for questions with smaller number of answers (being near 70% for questions with less than three answers). One possible explanation for the U-shaped curve of Figure 1 could be that the number of answers being too small indicates that the question is probably not interesting, likely posed by a casual user who is less inclined to give a best answer vote herself ≠ and thus the community has to step in.
Furthermore, CBA votes are rarely unanimous. Figure 2 shows how many answers to a question on average receive at least one CBA vote as a function of the number of answers to the question.

fitted line average frac(0.61)

0

10

20

30

40

50

#answers

Figure 1: Fraction of questions resolved by CBA votes

mean(#CBA vote recipients) 123456

0

10

20

30

40

50

#answers to a question

Figure 2: Number of answers receiving CBA votes

The y-axis shows the number of answers that receive at least one CBA vote, averaged over all questions that have the same number of answers. There is a clear linear trend up for questions with  30 answers. Beyond that, the trend is almost flat and noisier. Sparsity of data is likely the main reason behind the noise. The flatness of the trend can be possibly explained by the fact that a community member looking to vote an answer as CBA is likely only to look at the top few answers in the page; some answers can also be submitted after the CBA deadline passes.
In the following, we discuss our observations about some potential sources of bias that we see in our dataset. For each of these sources, it is important to note that we are not claiming that this bias immediately implies that the corresponding CBA votes provide no signal to predict answer quality. Our goal here is to identify a variety of indicators of potential bias, so that our model can use them to calibrate votes.
Self Voting: Figure 3 shows the extent of self voting (where a user votes for his/her own answers) in our dataset. Self votes contribute to 33% of the total CBA votes, and when we consider users who cast at least 20 votes, the percentage of self votes goes even above 40%. Self votes are certainly unlikely to represent any quality judgment, they can at most be taken as a signal of the user's familiarity with the voting mechanism of the CQA site, which in itself could provide a useful signal.
Vote Spread and Reciprocity: We define in/out spread of a user u as the ratio between the number of distinct users who cast/receive a vote to/from u and the number of votes that go into/out of u, which measures the spread of u's incoming/outgoing votes over different users. This measure could capture potential biased behavior in which users are either receiving most of their votes from a small number of users, or are themselves casting the votes in a concentrated manner. This phenomenon might be due to gaming, a user's

785

out reciprocity 0.40 0.43 0.46
in reciprocity 0.40 0.44 0.48

mean(#self CBA votes) 0 40 80

in spread 0.5 0.7 0.9
1000 10000 #users
out spread 0.5 0.7 0.9

0

50

100

150

200

#CBA votes cast by a user

Figure 3: Number of self CBA votes by a user, along with fitted curve showing the trend.

0

50 100 150 200

#CBA votes received

0

50 100 150 200

#CBA votes cast

Figure 5: (a) Out Reciprocity and (b) In Reciprocity of CBA votes.

1e+05

ratio #users

#users receiving >= x reciprocal CBA votes 1000 5000 20000

#users

1000

1 4 7 10 14 18 #CBA votes recvd

1 4 7 10 14 18 #CBA votes cast

Figure 4: (a) In Spread and (b) Out Spread of CBA votes. The right end point of the x-axis includes all users with > 20 votes

narrow interests, or a scenario where only a small number users provide high quality answers. We report out spread and in spread in the Figure 4. The red curve together with the left hand y-axis represents in/out spread as a function of the number of votes that go into/out of a user. The green curve with the right hand y-axis shows, in log-scale, the number of users who received/cast a certain number of CBA votes. The spread decreases with increasing number of CBA votes cast/received. It is lowest on average for users who have cast/received more than 20 votes ≠ which could indicate gaming by a small number of active users.
One interesting user-user interaction, almost a "quid pro quo" action, is when two users cast CBA votes on each other thereby creating a reciprocal relation between the two. We call a vote a reciprocal vote if the recipient also vote back for an answer authored by the voter. We define in/out reciprocity of user u as the fraction of incoming/outgoing votes that are reciprocal. Figure 5 shows in/out reciprocity as a function of the total number of incoming and outgoing CBA votes of a user. Both the curves are concave, and stabilize around 0.46 for users with large number of votes ≠ again indicating potential gaming by heavy users. The number of users involved in such a reciprocal voting pattern is also seen in Figure 6 ≠ over 20K users in our dataset are involved in at least 10 reciprocal votes. Considering this pattern as a feature when calibrating CBA votes certainly seems advisable.
Other Interaction Bias: In the remaining part of the section, we investigate whether CBA votes are independent of previous interaction between the voter and recipient. Our strategy is to use a hypothesis testing framework to analyze whether the null hypothesis -- that CBA voting is independent of previous interaction -- holds true individually for different interaction types. The different types of interaction we consider are the following: Whether one user answered another's question, whether they thumbed up/down each other, and whether they gave CBA/ABA votes to each other (recall that ABA denote asker best-answer). We represent each of these by a formal relation as follows. Let t be a time point. ANSt denotes the set of all pairs (u, v) of users such that u answered a question posed by v before time t. TUt denotes the set

0

50

100

150

200

#reciprocal CBA votes recvd

Figure 6: Number of users having at least x reciprocal votes

of the pairs (u, v) such that u thumbed up an answer by v before time t. TDt, CBAt, ABAt are similarly defined based on thumb-down votes, CBA votes and ABA votes that user u gives user v. Our goal is to investigate whether a CBA vote that v gives to u at time t + 1 is independent of whether (u, v)  Rt for R  {ANS, TU, TD, CBA, ABA}. Before describing our observation, we briefly describe our methodology.
Chi-squared Statistic: Let CBA(u) denote the set of recipients of the CBA votes cast by user u. Let R be any of the relation described above. Then, the chi-squared statistic for measuring independence of R and CBA(u) can be computed based on the following counts, show in Table 2. Let N = a + b + c + d. Then, the chi-squared statistic X2(CBA, R) is defined as

X2(CBA, R)

=

(a

+

N (ad - bc)2 b)(c + d)(a + c)(b

+

d)

For measuring the effect of previous interaction on a CBA vote, we will compute this statistic based on the dynamic interaction stream. We go through the list of user actions performed in increasing order of time, and maintain the current set of user pairs who are in relation R. At any point, when we are processing a CBA vote, we use the relation R constructed up until this point to find which of the above cells we should increment the counts of. The X2 statistic is computed after the entire stream is processed ≠ this is then used for the test.

Randomization Tests: In order to get the confidence interval of the

(u, v)  R (u, v)  R

v  CBA(u)

a

b

v  CBA(u)

c

d

Table 2: Table for chi-squared statistic.

786

TU TD CBA ANS ABA

empirical (X^ ) 66941.65 5830.44 55515.30 13391.79 9616.96

max random 35718.70 7369.88 29976.85 8677.06 5921.43

min random 35263.40 7046.85 29360.74 8496.51 5755.67

X^ - 
261.76
-18.81
168.97
103.60
82.56

Table 3: Effect of previous interaction (row indicates type) on CBA.

above X2 value under the null hypothesis, we resort to randomization of the data. The way the data is randomized is as follows ≠ for each question, we first decide on a list of candidate answers who are eligible to receive CBA votes. We select these to be the answerers who have received at least one CBA vote for this question. The randomized version of the X2 statistic is then computed by the following process ≠ we again traverse the list of interactions in increasing order of time. Each CBA vote is now replaced by a random choice from the list of candidates to this question ≠ the updates to the counts in Table 2 are now made according to this random choice. Finally, the X2 statistic is computed at the end.
Effect of Interaction: The above randomization was performed 20 times. In Table 3, we present the statistics obtained. The four columns present the X2 statistic obtained on our data, the maximum and minimum over the 20 randomizations, and the normalized value of the statistic based on the empirical mean and variance. Based on the 20 randomizations, the null hypothesis of the CBA being independent of the relation was rejected with probability 0.1 for each of the relation TU, TD, CBA, ANS and ABA. Note that this only means that there is some correlation between CBA votes and past interaction of any of the above types ≠ it does not establish a direct bias. Such correlation could, for instance, occur if active users are also experts. Nevertheless, this result suggest past interaction could be useful features for vote calibration.

6. FEATURES
Based on the exploratory analysis, in this section we define the features that we use to calibrate votes. We consider two types of features: voter features, which try to capture a user's voting behavior, and relation features, which try to capture potential bias associated with the relation between the voter and the answerer (vote recipient).

Definitions: We say that user v gives user u a vote if v gives a vote to an answer authored by u. In this relation, v is the voter and u is the answerer. A vote from v to u is a reciprocal vote if u has given v a vote before. A vote from user v on an answer is a majority vote if the answer receives at least one additional vote and has the largest number of votes among other answers to the same question.

Notations: We will use the following notations.

∑ nVotes(v, u) = # of votes from user v to user u.

∑ nVotesBy(v) = # of votes cast by user v.

∑ nVotesTo(u) = # of votes given to user u.

∑ nUsersBy(v) = # of unique users receiving votes cast by v.

∑ nUsersTo(u) = # of unique users who give votes to user v.

∑ nRecVotes(v) = # of reciprocal votes cast by user v.

∑ nMajVotes(v) = # of majority votes cast by user v.

∑ nVotersQ(q) = # voters voting for any answer to question q.

∑

ratio(x, y, n)

=

x+n y+n

is a smoothed ratio between two counts x

and y, where n is a pseudo count to stabilize the ratio and µ denote

the average of x/y over all of the (x, y) pairs for which we want to compute the ratio with y  n.

Voter features: We define the following features for voter v, for each vote type separately (when appropriate).
∑ Vote volume: nVotesBy(v), nVotesTo(v), nUsersBy(v) and nUsersTo(v).
∑ Vote spread: ratio(nUsersBy(v), nVotesBy(v), 3), which measures the spread of the votes of v over different users.
∑ Vote reciprocity: ratio(nRecVotes(v), nVotesBy(v), 3), which measures how often v casts reciprocal votes. We also include the raw count nRecVotes(v) as a feature.
∑ Self vote: % of times v votes for his/her own answers.
∑ Majority vote: ratio(nMajVotes(v), nVotesBy(v), 3) and the raw count nMajVotes(v).

Relation features: We define the following features for each pair (v, u) of users, where v is the voter and u is the answerer, for each
vote type separately.

∑ Voting probability: ratio(nVotes(v, u), nVotesBy(v), 3), which measures the probability that v would vote for u.

∑ Receiving probability: ratio(nVotes(v, u), nVotesTo(u), 3), measuring the probability that u would receive votes from v.

∑ Vote-back probability: ratio(nVotes(u, v), nVotesBy(u), 3), which

measures the probability that u would vote back to v.

∑

Voter

contribution:

Average

of

1 nVotersQ(q)

over

all of the

questions

on

which

v

vote

for

u,

where

1 nVotersQ(q)

represents

the

contribution

of

v among all the voters who vote for any answer to question q.

Feature transformation: For each of the features C that are counts, we consider log(1 + C) as an additional feature. For ratio features R, we also include a quadratic term R2.
Intercept: Same as in linear regression, we always include a feature, called intercept, which value is always 1.

7. EXPERIMENTAL RESULTS
With the above features, we evaluate our methods based on our editorially labeled dataset described in Section 3.
7.1 Evaluation Setup
We evaluate our models at two different levels:
∑ User-level expert ranking: User expertise score estimation is a much studied topic [12, 18, 11]. As described in Section 4.3, our models can be used to estimate such expertise scores for each user. Here, we evaluate how well we rank users based on the predicted user-level scores.
∑ Answer ranking: Our models can be used to predict the quality of each individual answer based on calibrated votes on the answer (if any) and answerer. Here, we evaluate how well we rank answers based on the predicted answer-level scores.
Note that our focus is on vote calibration. Thus, we take a contentagnostic approach to the above two ranking problems. Since the most related work [12] on content-agnostic methods is in the expert ranking setting, this forms our main comparison setting. Next, we compare variants of our methods based on both user-level and answer-level score estimation.
An ideal way to evaluate user-level scores is to collect a groundtruth data which has the expertise grade for each user. As noted by a few research studies, such as [12], obtaining such a dataset by human judgment is very costly because all the activities of a user need to be examined in order to assess her expertise. An alternative approach is to use certain heuristics to define experts (e.g., users with the "top contributor" badge on Yahoo! Answers), which are

787

also prone to have biases or be abused. Thus in our paper, we take an indirect approach similar to [12] in using the editorially judged (question, answer) pairs to evaluate user-level expertise. The evaluation hypothesis is that the answers provided by a user with higher expertise should have higher quality. Thus, we rank all the answers to a question using the user-level expertise scores of the answers and use the standard ranking metrics for evaluation.
Data: We try to predict the editorial quality scores on answers using either user-level or answer-level scores. We obtain 21,525 editorially judged answers as described in Section 3.1. All of the methods to be compared have access to the voting data described in Section 3.2.
Cross-validation: We use 10-fold cross validation to evaluate our models. We randomly split users into 10 groups. We use the editorially judged answers authored by users in 9 groups to train a model and then use this model to predict the user-level scores of the users in the remaining group and the answer-level scores for the answers they authored. This process is repeated 10 times to obtain the predicted scores for all the users and answers. It is important to note that the score of each user is predicted by a model that does not use any label information about that user.

together. Each method provides a ranked list of (question, answer) pairs for each category. Here, the ranked lists are much longer than per-question ranked lists. We thus adopt the commonly used Mean Average Precision (MAP) and Precision at 10 (P@10) for evaluation by treating Excellent and Good as relevant but Fair and Bad as irrelevant labels.
Methods for Comparison: The variants of our methods to be compared include (1) SVA.Full: SVA model with both voter and relation features; (2) SVA.Voter: SVA model with only voter features; (3) LVA.Full: LVA model with both voter and relation features; (4) LVA.Voter: LVA model with only voter features; (5) NoCalib: SVA model that only uses the intercept feature (which value is always one, thus no calibration for individual votes; however, different vote types are weighted differently). All these methods can be applied for either answer ranking or user-level expert ranking. We compare with the following baseline methods:
∑ Smoothed Best Answer Rate (BAR) estimates the probability that an answer provided by a user u would be a best answer. Let Ans(u) be the number of answers that u provided and BA(u) be the number of the best answers that u received in our voting data. We estimate best answer rate by

Evaluation Metrics: In Yahoo! Answers, a resolved question has 3.6 answers on average. When creating our editorial dataset, we sampled at most 3 answers for each question. Thus, the ranked list per question is short in our evaluation and the ability to distinguish the performance of different methods is limited. In order to overcome this limitation, we proposed three types of evaluation schemes as follows.
∑ Question Level: For each question, we rank the  3 answers to this question and evaluate using NDCG metrics. The NDCG metrics are sensitive to the highly ranked answers and thus are more suitable for question level evaluation. The NDCG@k is defined to be

NDCG@k

=

1 Zk

k i=1

Gi log2(i + 1)

where Gi is computed based on the label of the i-th ranked answer and Zk represents a normalization factor to guarantee that the NDCG@k for the perfect ranking (among all the permutations) is 1. In our paper, we set Gi = 3, 1, 0, 0 for Excellent, Good, Fair, Bad.
∑ Global Level: In the global level evaluation, we pool all the (question, answer) pairs together and rank all of them according to a model. After obtaining this ranked list f , we compare it to the ground-truth ranked list g, ranked according to editorial grades, using the Kendall  rank correlation:

BAR(u)

=

BA(u) + BARavg Ans(u) + 

where BARavg is the probability of a randomly chose answer to be a best answer and  is the smoothing parameter.
∑ Smoothed Competition Win Rate (CWR) is based on the competition defined in [12]. In this model, an answerer whose answer is the best answer for a question win a competition over each of other answerers to the same question, as well as the asker. Thus, we have (winner, loser) pairs from our data. The CWR is the rate that a user is the winner in all the pairs that the user appeared. Similarly, we also have a smoothing parameter  for this method.
∑ SVM is the model proposed in [12]. It computes a single score for each user based on the competition pairs. Let the score vector be w. The problem is formulated in the SVM format

minimize

1 2

||w||2

+

C

ij

s.t. wi - wj  1 - ij , ij  0 for i j

where i j means user i wins over user j in a competition.
∑ BAR+BA: All the above methods are for user-level scores, which can be applied on expert ranking. For the answer ranking evaluation, we combine BAR with a binary score indicating whether the answer is the best answer based on a weighted sum of the two.

|{(u, v) : u f v&u g v}| - |{(u, v) : u f v&u g v}|

1 2

n(n

-

1)

which combines both concordant and discordant pairs in the evaluation. It is important to note that the editorial grade of an answer is on an absolute scale across all answers, instead of being relative to other answers to the same question. Thus, it makes sense to pool all (question, answer) pairs together in to a single ranked list. A positive number means that two rank have a positive correlation. The higher the number, the better.

7.2 Experiment Results
7.2.1 Overall comparison
We compare our methods with all the baselines in Table 4. In this table, we show the three types of evaluation setting for both userlevel expert ranking and answer-level ranking. All the parameters,  for BAR and CWR, C for SVM, and a coefficient for BAR+BA, are tuned and we report their optimal results. In our methods, all the regularization parameters are set to 1.

∑ Category Level: Between the above two levels, we have the category level evaluation. In Yahoo! Answers, each question belongs to a category. The category level evaluation is to pool all the questions belonging to the corresponding category

Significance test: We use the paired t-test to assess the significance of the performance difference in NDCG@k, P@10 and MAP between two methods. Each query (i.e., question or category) gives a point in the significance test. For Kendall's rank correlation, each

788

Table 4: Overall comparison. Symbol + indicates significant improvement over BAR, BAR+BA, CWR and SVM; * indicates significant improvement over NoCalib (p-value < 0.05).

Metric

Expert Ranking

Answer Ranking

BAR CWR SVM NoCalib SVA.Full BAR+BA NoCalib SVA.Full

Question NDCG@1 0.675 0.675 0.676 Level NDCG@2 0.748 0.752 0.752
NDCG@3 0.800 0.801 0.802

0.694+ 0.760+ 0.808+

0.701+ 0.765+ 0.811+

0.690 0.760 0.807

0.701+ 0.766+ 0.811+

0.712+ 0.769+ 0.814+

Category P@10 Level MAP

0.336 0.315 0.307 0.383+ 0.407+ 0.366 0.353 0.354 0.386+ 0.396+

0.365 0.381

0.475+ 0.425+

0.501+ 0.444+

Global Kendall 0.147 0.144 0.143 0.182+ 0.203+* 0.152 0.185+ 0.209+*

method generates a single number which makes t-test inapplicable. In this case, we use bootstrap sampling to construct the distribution of Kendall's correlation and assess significance based on that.
We analyze our results from the following perspectives:
Supervised vs Unsupervised: For either expert ranking or answer ranking methods, we have supervised methods (NoCalib and SVA.Full) and unsupervised methods (all others). In Table 4, the best results are highlighted. SVA.Full outperforms all other methods by a large margin. For example, for expert ranking, SVA.Full improves over BAR by 3.8% on NDCG@1, 8.0% on MAP and 38% on Kendall; for answer ranking, SVA.Full improve over BAR+BA by 3.2% on NDCG@1, 16% on MAP and 37% on Kendall. All these improvement are statistically significant. Among the unsupervised methods, BAR is slightly worse than SVM and CWR on NDCG metrics but consistently better on MAP and Kendall metrics. This shows that BAR is a robust baseline to for expert ranking. Also, NoCalib (which uses three types of votes) consistently outperforms BAR. This shows that in CQA systems, considering all types of voting information is helpful, which has been unfortunately ignored in previous work. Our methods are effective to combine all the votes to improve the utility of CQA.
Effect of Calibration: We analyze the effect of calibration in Table 4 by comparing NoCalib and SVA.Full. From this table, we can see that SVA.Full is significantly better than NoCalib on most of the metrics, especially for answer ranking. For example, the improvement is 1.5% for NDCG@1 and 4.5% on MAP for the answer ranking. All these improvement are statistically significant at p-value < 0.05. Together with Section 5, this result confirms the existence of bias in the current voting data and shows that vote calibration using our methods is effective.
7.2.2 Comparison of Calibration Models
In this section, we compare different models of vote calibration and different sets of features. In Figure 7(a), we compare variants of SVA and LVA together with the NoCalib using the category-level metrics on two sets of users: a set with all users and a set with heavy users who have more than 50 answers in our voting data (there are 11 such users). We again see SVA and LVA outperform NoCalib. Furthermore, we have the following observations:
SVA vs LVA: From both figures, we can that SVA (SVA.Full and SVA.Voter) is consistently better than LVA (LVA.Full and LVA.Voter) on both expert ranking and answer ranking. The difference is more significant in the answer ranking setting. This shows that proper normalization is important for the vote calibration in our models.
Full vs Voter: By comparing different set of features, full and

voter, we see that the models with full features are also better than those with only voter features. This shows that the relation features are useful in vote calibration.

7.2.3 Impact on Heavy Users
We conducted a stratified study on heavy users by selecting them according to their level of activities in our dataset. Specifically, we set a threshold t and select only those users who have at least t answers in our voting data set. We vary t from 2 to 50 and plot the results in Figure 8. In this figure, the larger t is, the more active the set of users are. Clearly, both figures show that accuracy increases as user activity level increases. This makes intuitive sense because we have more information about heavy users and thus their expertise scores and the quality of the answers they provide can be better estimated. On both expert ranking and answer ranking, our models are consistently better than NoCalib and the relative order of different methods stay the same. Furthermore, for expert ranking, we see a larger margin between models with calibration and NoCalib. This suggests that calibration is more important for heavy users.

7.2.4 Feature Importance and Tuning Parameters

Feature importance: We now investigate the importance of different features defined in Section 6. To assess the importance of a set of features, we use that set of features alone to build our vote calibration model and compute the Kendall's correlation as the importance score of that set of features. The top 5 sets of features in the order of their importance are: Majority vote, vote spread, self vote, vote reciprocity and voting probability.

Tuning parameters: Recall that our model has two sets of tuning parameters t and k. To prevent overfitting, all the above results are based on simply setting t = 1 and k = 1 without tuning. Here, we investigate the sensitivity of SVA.Full to these tuning parameters. We use Kendall's correlation as the evaluation metric.

Setting 0.01 0.1

1 10

Fix t = 1, vary k 0.209 0.209 0.209 0.205 Fix k = 1, vary k 0.208 0.209 0.209 0.200

As can be seen, our model is not sensitive to tuning parameter settings.

8. CONCLUSION
In this paper, we introduce vote calibration to CQA systems. By analyzing potential bias in users' voting behavior we propose a set of features to capture such bias. Using supervised models we show that our calibrated models are better than the non-calibrated versions on both user expertise and answer quality estimation.

789

MAP 0.40 0.42 0.44 0.46 0.48 0.50 0.52

0.46

0.44

0.42

MAP

0.40

SVA.Full SVA.Voter LVA.Full LVA.Voter NoCalib

SVA.Full SVA.Voter LVA.Full LVA.Voter NoCalib

Expert Ranking

Answer Ranking

Expert Ranking

Answer Ranking

(a) All users

(b) Heavy users with  50 answers

Figure 7: Comparison of Calibration Models

0.38

0.36

0.52

0.48

MAP

MAP 0.36 0.38 0.40 0.42 0.44

0.44

SVA.Full SVA.Voter LVA.Full

LVA.Voter NoCalib BAR

SVA.Full SVA.Voter LVA.Full

LVA.Voter NoCalib

0.40

0

10

20

30

40

50

0

10

20

30

40

50

#answers threshold
(a) Expert ranking

#answers threshold
(b) Answer ranking Figure 8: Results on heavy users

9. REFERENCES
[1] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman. Knowledge sharing and yahoo answers: everyone knows something. In WWW, 2008.
[2] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne. Finding high-quality content in social media. In WSDM, 2008.
[3] J. Bian, Y. Liu, D. Zhou, E. Agichtein, and H. Zha. Learning to recognize reliable users and content in social media with coupled mutual reinforcement. In WWW, 2009.
[4] M. Bouguessa, B. Dumoulin, and S. Wang. Identifying authoritative actors in question-answering forums: the case of yahoo! answers. In KDD, 2008.
[5] R. H. Byrd, P. Lu, and J. Nocedal. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific and Statistical Computing, 1995.
[6] B. Dom and D. Paranjpe. A bayesian technique for estimating the credibility of question answerers. In SDM, 2008.
[7] F. M. Harper, D. Raban, S. Rafaeli, and J. A. Konstan. Predictors of answer quality in online q&a sites. In CHI, 2008.
[8] L. Hong, Z. Yang, and B. D. Davison. Incorporating participant reputation in community-driven question answering systems. In SIN, 2009.
[9] D. Ishikawa, N. Kando, and T. Sakai. Ntcir-8 community qa pilot task. The 8th NTCIR Workshop, 2010.
[10] B. John, C. A. Yeow-Kuan, and G. D. Hoe-Lian. What makes a high quality user-generated answer? IEEE Internet Computing, 2010.

[11] P. Jurczyk and E. Agichtein. Discovering authorities in question answer communities by using link analysis. In CIKM, 2007.
[12] J. Liu, Y.-I. Song, and C.-Y. Lin. Competition-based user expertise score estimation. In SIGIR, 2011.
[13] A. Pal and J. A. Konstan. Expert identification in community question answering: exploring question selection bias. In CIKM, 2010.
[14] T. Sakai, D. Ishikawa, N. Kando, Y. Seki, K. Kuriyama, and C.-Y. Lin. Using graded-relevance metrics for evaluating community qa answer selection. In WSDM, 2011.
[15] M. Surdeanu, M. Ciaramita, and H. Zaragoza. Learning to rank answers on large online qa collections. In ACL-HLT, 2008.
[16] M. A. Suryanto, E. P. Lim, A. Sun, and R. H. L. Chiang. Quality-aware collaborative question answering: methods and evaluation. In WSDM, 2009.
[17] X.-J. Wang, X. Tu, D. Feng, and L. Zhang. Ranking community answers by modeling question-answer relationships via analogical reasoning. In SIGIR, 2009.
[18] J. Zhang, M. S. Ackerman, and L. Adamic. Expertise networks in online communities: structure and algorithms. In WWW, 2007.

790

Category Hierarchy Maintenance: a Data-Driven Approach
Quan Yuan, Gao Cong, Aixin Sun, Chin-Yew Lin, Nadia Magnenat-Thalmann
School of Computer Engineering, Nanyang Technological University, Singapore 639798
{qyuan1@e., gaocong@, axsun@, nadiathalmann@}ntu.edu.sg
Microsoft Research Asia, Beijing, China 100080
{cyl@microsoft.com}

ABSTRACT
Category hierarchies often evolve at a much slower pace than the documents reside in. With newly available documents kept adding into a hierarchy, new topics emerge and documents within the same category become less topically cohesive. In this paper, we propose a novel automatic approach to modifying a given category hierarchy by redistributing its documents into more topically cohesive categories. The modification is achieved with three operations (namely, sprout, merge, and assign) with reference to an auxiliary hierarchy for additional semantic information; the auxiliary hierarchy covers a similar set of topics as the hierarchy to be modified. Our user study shows that the modified category hierarchy is semantically meaningful. As an extrinsic evaluation, we conduct experiments on document classification using real data from Yahoo! Answers and AnswerBag hierarchies, and compare the classification accuracies obtained on the original and the modified hierarchies. Our experiments show that the proposed method achieves much larger classification accuracy improvement compared with several baseline methods for hierarchy modification.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Filtering
Keywords
Category Hierarchy, Hierarchy Maintenance, Classification
1. INTRODUCTION
With the exponential growth of textual information accessible, category hierarchy becomes one of the most widely-adopted and effective solutions in organizing large volume of documents. Hierarchy provides an organization of data by different levels of abstraction, in which each node (or category) represents a topic that is shared by the data in it. The connection between two nodes denotes supertype-subtype relation. Examples include Web directories provided by Yahoo! and Open Directory Project (ODP), hierarchies for community-based question-answering services by Yahoo! Answers (YA) and AnswerBag (AB), product hierarchies by online retailers like Amazon and eBay, as well as the hierarchies for news
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

browsing at many news portal websites. Figure 1 shows a small portion of Yahoo! Answers hierarchy. Questions in the same category are usually relevant to the same topic.
Hierarchy enables not only easy document browsing, but also searching of the documents within user defined categories or subtrees of categories. Additionally, hierarchy information can be utilized to enhance retrieval models to improve the search accuracy [4]. On the other hand, users' information needs can only be satisfied with the documents accessible through the hierarchy (but not the category hierarchy itself). That is, the usefulness of a hierarchy heavily relies on the effectiveness of the hierarchy in properly organizing the existing data, and more importantly accommodating the newly available data into the hierarchy. Given the fast growth of text data, continuously accommodating large volume of newly available text data into a hierarchy is nontrivial. Automatic text classification techniques are often employed for efficient categorization of newly available documents into category hierarchies. However, hierarchy often evolves at a much slower pace than its documents. Two major problems often arise after adding many documents into a hierarchy after some time.
∑ Structure Irrelevance. A category hierarchy may well reflect the topical distribution of its data at the time of construction. However, as new topics always emerge from the newly coming documents, there is no proper category in the hierarchy to accommodate these new documents, leading to putting these documents in less relevant categories. As the result, some categories contain less topically cohesive documents. Moreover, some categories become less discriminative with respect to the current data distribution. One example is the two categories Printers and Scanners in YA, for there emerged many questions about multi-functional devices which are related to both printers and scanners, leading to ambiguity between these two categories.
∑ Semantics Irrelevance. Semantics may change over time which calls for a better organization of the documents [17]. For instance, when creating the hierarchy, experts are more likely to put category Petroleum under Geography. However, after the disaster of BP Gulf Oil Spill, a lot of news articles in category Petroleum are about the responsibility of the Obama Administration. These documents have stronger connection to category Politics than Geography. It is therefore more reasonable to put these documents under Politics for better document organization.
These two problems not only hurt user experiences in accessing information through the hierarchy, but also result in poorer classification accuracy for the classifiers categorizing newly available documents because of the less topically cohesive categories [15].

791




  























Figure 1: Portion of Yahoo! Answers Hierarchy
Consequently, the poorer classification accuracy further hurts user experience in browsing and searching documents through the hierarchy. This calls for category hierarchy maintenance, a task to modify the hierarchy to make it better reflect the topics of its documents, which in turn would improve the classification accuracy. Although a category hierarchy is relatively stable, many websites have modified or adjusted their hierarchies in the past. In May 2007, Yahoo! Answers added a new category Environment into her hierarchy, and added several categories like Facebook and Google under Internet later. By comparison, eBay adjusted her hierarchy more frequently, because there always emerge new types of items, like tablets and eReaders.
Hierarchy modification is nontrivial. Manual modification of category hierarchy is a tedious and difficult task, because it is hard to detect the semantic changes as well as the newly emerged topics. This motivates the data-driven automatic modification of a given hierarchy to cope with semantic changes and newly emerged topics. This is a challenging task because of at least two reasons, among others. First, the resultant modified category hierarchy (hereafter called modified hierarchy for short) should largely retain the semantics of the existing hierarchy and keep its category labels semantically meaningful. Second, the categories in the modified hierarchy shall demonstrate much higher topical cohesiveness, which in turn enables better classification accuracy in putting new documents into the modified hierarchy.
To the best of our knowledge, very few work has addressed the hierarchy modification problem (see Section 2). Tang et al. propose a novel approach to modifying the relations between categories aiming to improve the classification accuracy [17]. However, their proposed method does not change the leaf categories of the given hierarchy, and thus cannot solve the aforementioned problems. For example, the method may move the leaf category Petroleum to be child category of Politics. However, it is more reasonable to partition the documents in Petroleum into two categories: one being the child category of Geography, and the other child category of Politics. The method [17] fails to do so since it is unable to detect the newly emerged hidden topic "Petroleum politics".
In this paper, we propose a data-driven approach to modify a given hierarchy (also called as the original hierarchy) with reference to an auxiliary hierarchy using three operations (namely, sprout, merge, and assign). An auxiliary hierarchy is a category hierarchy that covers a similar set of topics as does the given hierarchy (e.g., the Yahoo! hierarchy and ODP can be used as auxiliary hierarchy to each other). Similar to the concept of bisociation [8], our approach discovers finer and more elaborate categories (also known as hidden topics) by projecting the documents in the given hierarchy to the auxiliary hierarchy. This operation, similar to a cross-product operation between the categories from the given hierarchy and the categories from the auxiliary hierarchy, is named sprout 1. The similar hidden topics are then merged to form new

1We would like to thank an anonymous reviewer for suggesting the connection with bisociation [8] and the name sprout

categories in the modified hierarchy. The assign operation rebuilds the parent-child relations in the modified hierarchy. The category labels in the modified hierarchy are either borrowed from or generated based on both the original and the auxiliary hierarchies. We emphasize that the reuse of category labels from original and auxiliary hierarchies largely ensures semantically meaningful category labels in the modified hierarchy. When such an auxiliary hierarchy is unavailable, the given hierarchy can be used as an auxiliary hierarchy. Because of the three operations (i.e., sprout, merged, and assign), we name our approach the SMA approach. The main contributions are summarized as follows.
1) We propose a novel data-driven approach SMA to automatically modify a category hierarchy making it better reflect the topics of its documents. The proposed approach exploits the semantics of the given hierarchy and an auxiliary hierarchy, to guide the modification of the given hierarchy.
2) We evaluate the proposed approach using data from three realworld hierarchies, Yahoo! Answers, Answerbag, and ODP. The user study shows that the modified hierarchy fits with the data better than the original one does. As we argue that the categories in the modified hierarchy are more topically cohesive compared to the original hierarchy, we employ text classification as an extrinsic evaluation. Our experimental results show that the classifiers trained on the modified hierarchy achieve much higher classification accuracy (measured by both macro-F1 and micro-F1), than the classifier built on the original hierarchy, or the classifiers modeled on the hierarchies generated by three baseline methods, including the state-of-the-art method in [17] and the hierarchy generation method in [2].
The rest of this paper is organized as follows. Section 2 surveys the related work. We describe the research problem and overview the proposed approach in Section 3. The three operations are detailed in Section 4. The experimental evaluation and discussion of the results are presented in Section 5. Finally, we conclude this paper in Section 6.
2. RELATED WORK
Hierarchy Generation. Hierarchy generation focuses on extracting a hierarchical structure from a set of categories, each containing a set of documents. The generation process can be either fully automatic [2, 5, 11] or semi-automatic [1, 7, 22]. The semi-automatic approaches involve interaction with domain experts in the hierarchy generation process. In the following, we review the fully automatic approaches in more detail.
Aggarwal et al. use the category labels of documents to supervise hierarchy generation [2]. They first calculate the centroids of all categories and use them as the initial seeds. Similar categories are merged and clusters with few documents are discarded. The process is iterated to build the hierarchy. User study is employed to evaluate the quality of the generated hierarchy.
Punera et al. utilizes a divisive hierarchical clustering approach, which first splits the given set of categories into two sets of categories, and each such set is partitioned recursively until it contains only one category [11].
An algorithm for generating hierarchy for short text is proposed by Chuang et al. [5]. They first create a binary-tree hierarchy by hierarchical agglomerative clustering, and then construct a multiway-tree hierarchy from the binary-tree hierarchy. They use both classification measurement and user evaluation to evaluate the generated hierarchy.
Recently, Qi et al. employ genetic algorithms to generate hierarchy [12]. Given a set of leaf categories, a group of hierarchies are randomly generated as seeds, and genetic operators are applied

792

to each hierarchy to generate new ones. The newly generated hierarchies are evaluated and the hierarchies with poor classification accuracy are removed. The process is repeated until the classification accuracy is not improved.
Different from hierarchy generation which assumes a set of categories as input, our hierarchy modification method takes a hierarchy as the input. Hierarchy generation does not change the given categories hence it cannot solve the structure irrelevance problem.
Hierarchy Modification. Tang et al. present a method of modifying a hierarchy to improve the classification accuracy [17]. The method introduces three operations. The promote operation lifts a category to upper level; the merge operation generates a new parent for a category and its most similar sibling; the demote operation either demotes a category as a child of its most similar sibling, or makes the sibling a child of the category. For each category in the given hierarchy, promote operation is tested, followed by merge and demote operations, in a top-down manner. The operation comes into effect if it can improve the classification accuracy. The approach iterates the process until no improvement can be observed or some criterion is met. In experiments, this method outperforms clustering-based hierarchy generation method in terms of classification accuracy. However, this method does not change the leaf categories, leaving the topically incohesive leaf categories untouched. In addition, the method [17] has a high time-complexity. Due to the high time complexity of the method [17], Kiyoshi et al. propose an approach [10] to addressing the efficiency issue.
Discussion. With the existing work on either hierarchy generation or hierarchy modification, the leaf categories in the modified hierarchy (i.e., either generated or modified) remain unchanged. Clearly, without changing leaf categories, the topical incohesiveness among documents in the same leaf category remains unaddressed. Consequently, the likely poorer classification accuracy for these topically incohesive categories results in poorer document organization in the hierarchy. In this paper, we therefore propose an automatic approach to modify a given hierarchy where the leaf categories could be split or merged so as to better reflect the topics of the documents in the hierarchy.
3. SMA APPROACH OVERVIEW
We observe that each category in a hierarchy may contain several "hidden topics", each of which is topically cohesive, e.g., category Computer contains hidden topics like Internet Programming, Operating Systems, etc. With more documents adding to a category hierarchy, new "hidden topics" emerge within a single category leading to topical incohesiveness among its documents (see Section 1). Our proposed approach, therefore, aims to find the hidden topics within each category and then sprout categories based on its hidden topics, merge similar hidden topics to form new categories, and then assign the parent-child relation among categories. We name our approach SMA after its three major operations.
The key challenges in the approach include: (i) How to detect the "hidden topics" at the appropriate granularity? (ii) How to evaluate the similarity between "hidden topics"? and (iii) How to assign the parent-child relation between the unmodified and modified categories? Further, recall from Section 1, the modified hierarchy has to largely retain the semantics of the existing hierarchy, with meaningful category labels and topically cohesive categories. In the following, we give a high-level overview of the SMA approach and then detail the three major operations in the next Section.
The framework of our SMA algorithm is illustrated in both Figure 2 and Algorithm 1, where Hc is the category hierarchy to be modified, Hn is the modified hierarchy, Hn is the intermediate hi-









  
 






Figure 2: Overview of SMA

Algorithm 1: SMA algorithm for hierarchy modification

Input: Hc: category hierarchy to be modified Ha: auxiliary hierarchy  : minimum coverage ratio  : maximum loss ratio
Output: Hn: modified category hierarchy

1 Hn  Hc;

2 h  number of levels of Hc;

3 foreach Level from 2 to h of Hn do

4 foreach Category Ci of Hn on level do

5

Ci  ProjectedCategories(Ci, Ha,  ,  );

6

Sprout(Ci, Ci , Hn, Ha);

7 n  number of categories on level of Hc;
8 Merge( , n , Hn); 9 Assign( , Hn);

10 HnRelabel(Hc, Ha, Hn); 11 return Hn

erarchy during the modification process, and Ha is the auxiliary hierarchy.
Auxiliary Hierarchy. Briefly introduced in Section 1, an auxiliary hierarchy Ha is a hierarchy covering similar topics as the given hierarchy Hc. For example, Yahoo! hierarchy and ODP hierarchy can be auxiliary hierarchy to each other. Similarly, Yahoo! Answers and AnswerBag can be auxiliary hierarchy to each other.
The auxiliary hierarchy Ha plays an essential role in finding hidden topics. Note that the hidden topics are not readily present in the auxiliary hierarchy, and our approach does not simply use the structure of auxiliary hierarchy as part of the modified hierarchy. Instead, they contain semantics from both the original hierarchy and the auxiliary hierarchy. We use the following example to illustrate. Suppose that the original hierarchy has two categories, Action movie and Comedy movie, and the auxiliary hierarchy contains two categories America and Asia. Our approach will find that Action movie has two hidden topics, namely American action movie and Asian active movie; Comedy movie also has two hidden topics, namely American comedy movie and Asian comedy movie.
The auxiliary hierarchy also plays an important role in guiding the merge operation, which is to merge similar hidden topics to generate the categories of the modified hierarchy. Continue with the earlier example, after merging the generated hidden topics, we may get new categories≠American movie and Asian movie (if "action vs. comedy" is evaluated to be less discriminative compared with "American vs. Asian"). The semantics of the hierarchy to be modified, together with the semantics of the auxiliary hierarchy, will be exploited to define the similarity between hidden topics.
Validated in our experiments (Section 5), our approach is equally applicable when the original hierarchy Hc is used as the auxiliary hierarchy to itself.
Algorithm Overview. Shown in Figure 2 and Algorithm 1, Hn is first initialized to Hc (line 1). In a top-down manner, the SMA algorithm modifies the hierarchy level by level. Note that the root

793

category is the only category at level 1. Starting from level 2, for each category Ci in this level, the documents contained in Ci is projected to the auxiliary hierarchy Ha. A set of categories from Ha each of which contains a reasonable number of documents originally from Ci is identified to represent Ci's hidden topics (line 5). The two parameters, minimum coverage ratio  and maximum loss ratio  , adjust the number of hidden topics. New finer categories are then sprouted from Ci according to the hidden topics and the documents in category Ci are assigned to these finer categories (or hidden topics) (line 6). Given the expected number of categories n on level (line 7), the merge operation forms n number of new categories on level of the intermediate hierarchy Hn (line 8). If the current level is not the lowest level in the hierarchy, the parentchild relations between the modified categories and the unmodified categories on the next level are assigned (line 9). The last step in the SMA algorithm is to generate category labels with reference to both the original and auxiliary hierarchies (line 10).
4. SPROUT, MERGE, AND ASSIGN
We detail the three operations to address the challenges in the SMA framework (i.e., to identify hidden topics, evaluate the similarity between hidden topics, and assign the parent-child relation).
4.1 Sprout Operation
The sprout operation first discovers the hidden topics for the documents in a category Ci and then sprouts the category. Without loss of generalization, a leaf category is represented by all documents belonging to the category; a non-leaf category is represented by all documents belonging to any of its descendent categories.
4.1.1 Discovery of Hidden Topics
Ideally, for a category we find a set of its hidden topics, which are comprehensive and cohesive, and have no overlap. This is however a challenging task. We proceed to give an overview of the proposed method. Given a category Ci in the intermediate hierarchy Hn during the modification process (see Algorithm 1), we assign all its documents into the categories of the auxiliary hierarchy Ha, and get a set of candidate categories from Ha in a tree-structure. Each candidate category contains a number of documents from Ci. Then, with the consideration of both cohesion and separation, we select a set of categories from the tree as hidden topics. The selection process is modeled as an optimization problem. We now elaborate the details.
Document Projection. To assign documents from Ci to Ha, we represent a document by its word feature vector, and a category in Ha by its centroid vector. Based on cosine similarity between the document and the centroids, we recursively assign each document d  Ci to Ha from its root to a leaf category along a single path of categories. If a good number of documents from Ci are assigned to a category Ca in Ha, then the topic of Ca is relevant to Ci, and the semantics of Ca can be used to describe a hidden topic of Ci. Thus, multiple categories in Ca can be identified to describe all hidden topics of Ci. For example, large number of documents from category Programming assigned into two categories Security and Network in an auxiliary hierarchy, implies that Programming has two hidden topics: Network Programming and Security Programming. We have also tried to build a classifier on Ha to assign documents from Ci to Ha using Naive Bayes and support vector machine, respectively, and the set of generated hidden topics is almost the same.
The process of assigning documents from a category Ci in Hn to categories in Ha is called projection. We denote the set of docu-

ments projected from category Ci to category Ca by (Ci  Ca). If Ca is a leaf category, then (Ci  Ca) denotes the set of documents from Ci that are projected into Ca; if Ca is a non-leaf category, then (Ci  Ca) denotes the set of documents projected into any of the descendent leaf categories of Ca in Ha.
Candidate Topic Tree. Based on the projection, we select categories from Ha to represent the hidden topics of Ci. A selected category can be either a leaf category or a non-leaf category. Before describing the selection process, we introduce the notions of major category and minor category. Let  denote the minimum coverage ratio parameter.
Definition 1 (Major Category). A category Ca from Ha is a major category for category Ci if |(Ci  Ca)|/|Ci|   .
Definition 2 (Minor Category). A category Ca from Ha is a minor category for category Ci if |(Ci  Ca)|/|Ci| <  .

   



 

  !"
#$ %

 


 


 
  


 
  
  






   '!(!""'"'&#$% !

Figure 3: Generate candidate topic tree for Ci using Ha
For example, suppose  = 15%. As shown in Figure 3, category Ci is projected to the categories in Ha. In the left tree, in which each number besides a node represents the percentage of documents of Ci projected to the node, the nodes in dark color are major categories while the others are minor categories.
Naturally, only the major categories are considered candidate categories to represent hidden topics of Ci because a good number of documents in Ci are projected into them. However, not all major categories need to be selected because of two reasons. First, let Cp be the parent of a major category Ca. By definition, the parent of a major category is also a major category. Selecting both Ca and Cp would lead to semantic overlap. Second, assume all Cp's other child categories are minor categories, but altogether those minor categories contain a large number of documents. Then selecting Ca but not Cp would lead to a significant loss of documents from Ci (hence semantic loss). We therefore define the notion of loss ratio.

Definition 3 (Loss Ratio). The loss ratio of a leaf category is

defined as 0. For a non-leaf category Ca, let Ca be the set of minor

categories among Ca's child categories. The loss ratio of Ca with

respect to Ci, the category being projected, is the ratio between

the projected documents in all its child minor categories and Ca's

projected documents, i.e.,

. C Ca |(CiC )|
| (Ci Ca )|

We set a threshold maximum loss ratio  . After projecting doc-

uments from Ci to categories in Ha, we only keep the major categories whose parent's loss ratio is smaller than  . Note that, if a

non-leaf category is not selected in the above process, the subtree

rooted at this category is not selected. After the selection, we obtain

a sub-hierarchy from Ha containing only eligible major categories,
which is called candidate topic tree for Ci, denoted by TCi . For example, suppose  = 30%. The candidate topic tree for Ci
is shown on the right hand side of Figure 3. Although node C5 is
a major category, it is not part of the candidate topic tree since the

loss ratio ((10%+10%)/50% = 40%) of its parent node C3 is larger than  .

794

Hidden Topic Selection. We next present how to choose a set of
nodes from TCi to represent hidden topics of Ci. Ideally, we expect the hidden topics to be comprehensive but not overlap with each
other. Hence, we use tree-cut to define the selection [18].

Definition 4 (Tree-Cut). A tree-cut is a partition of a tree. It is a list of nodes in the tree, and each node represents a set of all leaf nodes in a subtree rooted by the node. The sets in a tree-cut exhaustively cover all leaf nodes of the tree, and they are mutually disjoint.

There exist many possible tree-cuts for TCi to generate hidden topics. Two example tree cuts for the candidate topic tree in Figure 3 are {C1, C2} and {C1, C3}. Among all possible tree-cuts, we aim to choose the tree-cut such that each resultant hidden topic (cate-
gory) is cohesive and well separated from other categories in the
tree-cut. In the following, we prove that the tree-cut containing
only leaf nodes of the candidate topic tree satisfies this require-
ment. Note that a leaf node in TCi is not necessary a leaf category in Ha. For example, in Figure 3, C3 is leaf node of the candidate topic tree, but not a leaf category in the auxiliary hierarchy.
We proceed to show the proof. We first define the Sum of Square
Error (SSE) of cohesion for a category Ca.

SSE(Ca) = (d - ca)2,

dCa

where d is a document and ca is the centroid of category Ca.

Given a set of categories {Ca} (1  a  k), the Total-SSE and

Total Sum of Square Between (Total-SSB), denoted by E and B

respectively, are E =

k a=1

SSE

(Ca)

and

B

=

k a=1

|Ca|(c

-

ca)2,

where c is the centroid of documents in all categories {Ca}. It is

verified that, given a set of documents, the sum of E and B is a

constant value [16]: E +B =

k a=1

dCa (d-c)2. Thus, maximizing

separation is equivalent to minimizing cohesion error. We therefore

formulate the problem of selecting categories from TCi to represent hidden topics for category Ci as following:

Ci = arg min SSE(Ca), where S is a tree-cut on TCi . (1)

S

Ca S

This problem can be reduced to the maximum flow problem by

viewing TCi as a flow network. Thus, it can be solved directly by Ford-Fulkerson method [6]. However, its complexity is relatively

high. Note that we need to solve the optimization problem for every

category in the original hierarchy, and thus an efficient algorithm is

essential.

Lemma 1: The SSE of a category is not smaller than the Total-SSE
of its child categories. Proof: Suppose there is a category Cp with k child categories {Ci}ki=1.

For a child category Ci, the Sum of Square Distance (SSD) of its

data to a data point x is: SSD(Ci) = dCi (d - x)2. We get the mini-

mum value when x

=

1 |Ci |

dCi

d

=

ci

which

let

d dx

dCi (d-x)2 = 0.

Thus, when x is the mean of data in Ci (or ci), the SSD of Ci be-

comes SSE of Ci, and gets its minimum value. One step further, we

have

(d - ci)2  (d - cp)2,

dCi

dCi

where cp is the mean of data of Cp. This demonstrates that the SSE

of Ci is smaller than the SSD of data of Ci to the overall mean, and

this result leads to

k

k

(d - ci)2 

(d - cp)2.

i=1 dCi

i=1 dCi

This demonstrates that the SSE of a category is not smaller than the

Total-SSE of its child categories.

Procedure ProjectedCategories

Input: Ci: the category to be sprouted Ha: the auxiliary hierarchy  : minimum coverage ratio  : maximum loss ratio
Result: Ci []: the list of projected categories for Ci

1 Ci []  {root category of Ha}; 2 repeat

3 Ca  Ci [].getNextUnprocessedCategory(); 4 C_List[]  child categories of Ca;

5 M_List[]  {};

6 foreach Category C of C_List[] do

7

if

| (Ci C)| |Ci |



then

8

M_List[].add(C);

9

if

1-

CM_List [] | (Ci C)| | (Ci Ca )|

<



then

10

Ci [].add(M_List[]);

11

Ci [].remove(Ca);

12 else

13

mark Ca as processed

14 until No more unprocessed category in Ci [] 15 return Ci []

Lemma 1 enables us to use an efficient method to solve Eq.1 as follows. According to Lemma 1, specializing a category by its child categories can reduce Total-SSE. That is, among all possible tree-cuts in TCi , the cut that contains only leaf categories has the minimum value of Total-SSE.
In summary, for a category Ci in Hn, we build a candidate topic tree TCi and the leaf nodes of TCi are used to represent the hidden topics of Ci. The pseudocode is given in Procedure 2 ProjectedCategories. As discussed, according to Lemma 1, we only need the leaf nodes of the candidate topic tree TCi as the result. Instead of explicitly building TCi and then finding TCi 's tree cut containing only leaf nodes, we find TCi 's leaf nodes directly in our procedure. More specifically, we start from the root category of Ha in a topdown manner (the root node is a major category by definition as its coverage ratio is 1). Each time we get a unprocessed categories Ca from the list of projected categories Ci [], and check its child categories (lines 3-4). The major categories among the child categories are put into a major category list (lines 6-8) for further testing on loss ratio. If the loss ratio of Ca is smaller than maximum loss ratio  , then Ca is replaced by its child major categories (lines 9-11); otherwise, Ca is selected as a candidate category (line 13). We iterate the process until all major categories are processed (line 14).
4.1.2 Sprout Category
For a category Ci of Hn, we sprout it based on the projected categories Ci returned by Procedure ProjectedCategories. Recall that each of the projected category Ca  Ci represents a hidden topic of Ci and contains a good number of documents projected from Ci, i.e., (Ci  Ca). We sprout Ci with |Ci | number of categories. However, not all documents from Ci are contained in all these newly sprouted categories, i.e., CaCi |(Ci  Ca)|  |Ci|. For those documents in Ci but not contained in any of the newly sprouted categories, we assign them to their nearest sprouted categories. As the result, each document in Ci is now contained in one and only one sprouted category of Ci.

795

$X[LOLDU\ KLHUDUFK\

&RPSXWHU

6HFXULW\

1HWZRUN



3URWRFRO
0RGLI\LQJ &RPSXWHU KLHUDUFK\

&DEOH  +LGGHQ WRSLFV



1HWZRUN

3URJUDPPLQJ

 6SOLW FDWHJRU\  0HUJH FDWHJRU\

&RPSXWHU

1HWZRUN 6HFXULW\

1HWZRUN 1HWZRUN

6HFXULW\

3URWRFRO

3URWRFRO &DEOH 3URJUDPPLQJ 3URJUDPPLQJ

1HWZRUN 6HFXULW\ 6HFXULW\ 3URJUDPPLQJ

&RPSXWHU
1HWZRUN &DEOH

1HWZRUN 3URWRFRO 3URWRFRO 3URJUDPPLQJ

:$1



:$1 6HFXULW\

 $VVLJQ FDWHJRU\ UHODWLRQ

:$1 3URWRFRO 0RGLILHG KLHUDUFK\ EHIRUH UHODEHO

Figure 4: SMA operations by example. The hidden topics and sprouted categories for a category of the original hierarchy are in the same color. The Network and Programming categories have 3 and 2 hidden topics, respectively, leading to 5 sibling sprouted categories. These 5 categories are merged into 3 categories and the category WAN is reassigned to 2 of the merged categories.
Example 4.1: Shown in Figure 42, suppose after applying procedure ProjectedCategories, we find Network is projected to Security, Protocol, and Cable. According to the three hidden topics, we sprout Network into three categories Network Security, Network Protocol, and Network Cable.
The sprout operation may be reminiscent of the work on hierarchy integration, aiming to integrate a category from a source hierarchy into similar categories in the target hierarchy, which has a different purpose from our mapping. Most of proposals (e.g., [3, 21]) on hierarchy integration employ a hierarchical classifier built on the target hierarchy to classify each document in the source hierarchy into a leaf node of the target hierarchy, which is too fine a granularity to represent hidden topics as in our approach. Frameworks that can map a category to categories on proper levels in target hierarchy are proposed (e.g., [19]). However, they do not take the cohesion and separation between mapping categories into account, which are essential to find good hidden topics in our approach. Thus, they cannot be applied to our work.
4.2 Merge Operation
The sprout operation in Section 4.1 generates a set of sprouted categories, each representing a hidden topic. The merge operation aims to combine the newly sprouted categories with similar hidden topics.
Suppose we are now working on level of the intermediate modified hierarchy Hn and we have a set of sprouted categories originated from the categories on level . Our task is to merge some of these sprouted categories such that the number of resultant categories on level is the same as before (i.e., n ). Note that the number of resultant categories can also be specified by users. To ease the presentation, the modified hierarchy has the same size as the given hierarchy in our discussion.
During merge, we need to consider an important constraint -- we can only merge categories under the same parent category. Thus, existing clustering algorithms need to be modified to accommodate such a constraint. Another key issue here is how to define the similarity between two sprouted categories by considering their semantics enclosed in Hc and Ha. In the following, we first define a similarity measure and then describe our merge method.
We consider two aspects when defining the similarity for a pair of sprouted categories C1 and C2 on level : (i) the distribution of their
2For clarity, we recommend viewing all diagrams in this paper from a color copy.

documents over categories of Hc and Ha, and (ii) the similarity

between the categories within Hc and Ha, respectively.

Let Lc be the set of categories on level in the original hierarch

Hc, Ls be the set of categories sprouted from Lc, and La be the

set of projected categories in auxiliary hierarchy representing the

hidden topics of the categories in Lc. That is, La = CiLc Ci . For a sprouted category Cs  Ls, its document distribution over

Lc is defined to be the ratios of its documents in each of the cate-

gories in Lc. That is, the document distribution of Cs can be mod-

eled as a |Lc|-dimensional vector vcs. The j-th element of vcs is

|CsC j | |Cs |

(i.e.,

the

portion

of Cs's

documents

also

contained

in Cj),

where Cj is the j-th category of Lc. Similarly, we get the data dis-

tribution vector vas for Cs over La based on the ratio of documents

in Cs projected to each of the categories in La. Because vcs and

vas usually have different dimensionality for different sprouted cat-

egory Cs's, we extend vcs to be |Hc|-dimensional (each category is

one dimension) by filling up zeros for corresponding categories in

Hc but not in Lc. Similarly vas is extended to be |Ha|-dimensional. We use two matrices Mc, Ma to represent the similarity between
categories of Hc and Ha, respectively. Mc is a |Hc|- by-|Hc| matrix and Ma is a |Ha|-by-|Ha| matrix. Each element mi j in a matrix

represents the similarity in the corresponding hierarchy between

a pair of categories Ci and Cj, which is defined by Inverted Path

Length [14]:

mi j

=

1 1+path(Ci,C j )

,

where

path(Ci, Cj) is

the length

of

path between Ci and Cj in the hierarchy.

Considering both document distribution and structural similar-

ity from the two hierarchies, the similarity between two sprouted

categories C1 and C2 on level of Hn is defined as:

Sim(C1, C2) = (vTc1 ∑ Mc ∑ vc2) + (vTa1 ∑ Ma ∑ va2).
This similarity definition considers both the similarity estimated based on Hc and the similarity estimated based on Ha. With similarity between two sprouted categories defined, we proceed to detail the merge operation.
We first explain the notion of sibling sprouted category through an example. Let Ci1 and Ci2 be the two categories sprouted from category Ci, and Cj1 and Cj2 be the two categories sprouted from Cj. If Ci and Cj in Hm are both children of category Cp, then naturally, all the newly sprouted categories Ci1, Ci2, Cj1, Cj2 are children of Cp. These four example categories are known as sibling sprouted categories. All the five sprouted categories shown in Figure 4 are sibling sprouted categories.
The merge operation is as follows. We first calculate the similarity between sibling sprouted categories on level . Then, we pick up the pair of categories with the largest similarity, and merge them into a category, and recompute its similarities with its sibling sprouted categories. The process iterates until the number of remaining categories on equals n , the number of categories on level of the original hierarchy Hc. When all the sibling sprouted categories under the same parent node are merged into a single category, we shrink the single category into its parent node. Note that we cannot merge two sprouted categories on level if they have different parent node.

Example 4.2: Recall Example 4.1. We sprout Network into three categories Network Security, Network Protocol, and Network Cable. Suppose there is another category Programming on the same level of Network and sprouted into Security Programming, and Protocol Programming (see Figure 4). Based on the similarity, Network Security and Security Programming are merged together (both are about the Security topic), and Network Protocol and Protocol Programming are merged to generate a new category about protocol.

796

4.3 Assign Operation
After modifying categories (by sprout and merge) on level of H n, the original parent-child relations between the categories on level and the categories on next level + 1 do not hold any longer. Hence we need to reassign the parent-child relation.
Based on the fact that a non-leaf category of a hierarchy subsumes the data of all its descendants, we rebuild the children for each of the modified categories on by checking document containment. If the documents of a category on level are also contained in a category on level + 1, then the latter category is assigned to be the children of the former category. In other words, for each category C on , we calculate the intersection of documents between C and the categories on + 1. The intersections form new children for category C. Because one category has only one parent category, if a category has intersections with more than one category on level , the category will be split into multiple categories, each containing the intersection with one category on level .
Example 4.3: Recall Example 4.2. Suppose WAN was a child category of Network before sprout (see Figure 4). After sprout and merge, Network no longer exists and WAN lost its parent. We compare the documents of WAN and the two newly formed categories after merge (i.e., Network Security & Security Programming and Network Protocol & Protocol Programming). If WAN has overlap with both categories, then WAN have two hidden topics (about security and protocol). Thus, we divide WAN into two categories and assign them to different parent nodes, shown in Figure 4.
4.4 Relabel
Unlike most of previous work, our approach is able to automatically generate readable labels for every modified category. By projecting documents from Hn to Ha, we can consider the three hierarchies Hn, Hc and Ha, which contain the same set of data. For every document in Hn, we trace its labels in Hc and Ha, and use them together as the label of the document; the semantic of the new label is the intersection of semantics of its two component labels. We then aggregate such labels for all documents in a category of Hn, and use them as candidate labels for the category. The candidate labels for a category are ranked according to the proportion of documents in their corresponding original categories from Hc and Ha. In this paper, the top-1 ranked label is chosen.
The labels generated in this way are mostly readable and semantically meaningful, as reflected in our user study (see Section 5.3) and case study (see Section 5.4). Nevertheless, a manual verification of the labels for the newly generated categories can be employed when the proposed technique is used in real applications.
5. EXPERIMENTS
We designed two sets of experiments. The first set of experiment, similar to that in [17], is to evaluate whether the modified hierarchy improves the classification accuracy. Discussed in Section 1, if a category hierarchy better reflects the topics of its contained documents and each category in the hierarchy is topically cohesive, then better classification accuracy is expected than that on a hierarchy with less topically cohesive categories. The second set of experiments employs a user study to manually evaluate the semantic quality of the modified hierarchy following the settings in [2,5]. Finally, we report a case study comparing a part of Yahoo! Answers hierarchy with its modified hierarchy.
5.1 Data Set
We use data from three real-world hierarchies: Yahoo! Answers, AnswerBag, Open Directory Project, denoted by HYA, HAB and

Table 1: Statistics of the three hierarchies

Hierarchy

HYA

HAB

HODP

Number of documents

421,163 148,822 203,448

Number of leaf nodes

75

195

460

Number of non-leaf nodes 40

70

98

Height

4

5

4

HODP, respectively. Since the modified category hierarchy contains a different set of leaf nodes, the labels for documents given in the original dataset do not stand in modified hierarchy. Manual annotation of documents in the modified hierarchy is therefore unavoidable. To make the annotation manageable, we selected the documents from two major topics Sports and Computers from these three hierarchies (because the annotators are familiar with both topics). Nevertheless, the number of documents in the two major topics in the three hierarchies ranges from 148,822 to 421,163, and the number of categories ranges from 115 to 558. These numbers are large enough for a valid evaluation. Table 1 reports the statistics on the three hierarchies.
HYA: Obtained from the Yahoo! Webscope datatset3, HYA contains 421,163 documents (or questions) from Sports and Computers & Internet categories.
HAB: We collected 148,822 questions from Recreation & Sports and Computers categories from AnswerBag to form HAB. Categories with fewer than 100 questions are pruned and all affected questions are moved to their parent categories.
HODP: The set of 203,448 documents from Sports and Computers categories are collected4 in HODP. Categories containing fewer than 15 documents or located on level 5 or deeper are removed in our experiments.
The preprocessing of the documents in all three hierarchies includes stopword removal and stemming. Terms occurred no more than 3 times across the datasets are also removed.
5.2 Evaluation by Classification
The proposed SMA algorithm modifies a category hierarchy to better reflect the topics of its documents, which in turn should improve the classification performance. Following the experimental setting in [17], we evaluate the effectiveness of hierarchy modification by comparing the classification accuracies obtained by the same hierarchical classification model applied on the original category hierarchy and the modified hierarchy, respectively.
Another three methods for hierarchy modification are employed as the baselines, namely, Bottom Up Clustering (BUC), Hierarchical Acclimatization (HA) [17], and Supervised Clustering (SC) [2]. Table 2 gives a summarized comparison of the three baselines with the proposed SMA, and Section 5.2.1 briefs the baseline methods.
The modified hierarchies by all the methods evaluated in this paper have the same size as the original hierarchy (i.e., same number of levels, and same number of categories in each level). For each hierarchy modification method, we evaluate the percentage of classification accuracy increment obtained by the same classification model (e.g., Support Vector Machine) on the modified hierarchy over the original hierarchy. The classification accuracy is measured by both micro-average F1 (Micro-F1) and macro-averaged F1 (Macro-F1) [20]. The former gives equal weight to every document while the latter weighs categories equally regardless the number of documents in each category.
We remark that this is a fair evaluation for all the methods, each generating a hierarchy with the same size as that of the original
3Available at http://research.yahoo.com/Academic_Relations. 4Available at http://www.dmoz.org/rdf.html.

797

Table 2: Comparison of baseline methods with SMA

Aspect/Methods Utilize original hierarchy Change leaf category

BUC HA[17] SC [2]

◊

◊

◊

◊

SMA 

Utilize auxiliary hierarchy ◊

◊

◊ Optional

hierarchy, where the same classification method is applied to the modified hierarchies to evaluate the improvement of each modified hierarchy over the original one in terms of classification accuracy.
5.2.1 Baseline Methods
Baseline 1: Bottom Up Clustering (BUC). In this method, each leaf category is represented by the mean vector of its contained documents. The categories are then clustered in a bottom-up manner using K-means to form a hierarchy.
Baseline 2: Hierarchical Acclimatization (HA). The HA algorithm is reviewed in Section 2, In simple words, it employs promote, demote and merge operations to adjust the internal structure, but leaves the leaf nodes unchanged [17].
Baseline 3: Supervised Clustering (SC). Given a set of documents with labels, SC first calculates the mean vector of each category as the initial centroid and then reassigns the documents to the categories based on the cosine similarity with their centroids. Then, similar categories are merged and minor categories are removed. These procedures are repeated, and during each iteration, a constant portion of features with smallest term-frequencies are set to zero (projected out) [2]. The process stops when the number of features left is smaller than a pre-defined threshold. This method cannot generate category labels. We take the most frequent words in a category to name it.
5.2.2 Experiments on Yahoo! Answers
From the data of HYA, we randomly selected 500 questions as test data (used for classification evaluation with manual annotations). To evaluate the possible improvement in classification accuracy, the same set of test documents are classified on the original (or unmodified) HYA, and the modified HnYA's. Two classifiers, multinominal Naive Bayes (NB) and Support Vector Machine (SVM) classifiers are used as base classifiers for hierarchical classification. We build Single Path Hierarchical Classifier (SPH) [9] as it performs better than other hierarchical classification methods for question classification according to the evaluation [13]. In the training phase of SPH, for each internal node of the category tree, SPH trains a classifier using the documents belonging to its descendent nodes. In the testing phase, a test document is classified from the root to a leaf node in the hierarchy along a single path.
SMA Settings. Recall that SMA uses auxiliary hierarchy in the modification process. We evaluated SMA with three settings, to modify HYA using HYA, HAB, and HODP as auxiliary hierarchy, respectively. The three settings are denoted by SMAYA|YA, SMAYA|AB, and SMAYA|ODP, respectively. The first setting is to evaluate the effectiveness of using the original hierarchy as auxiliary hierarchy, and the last two are to evaluate the effectiveness of using external hierarchies.
Parameter Setting. Before evaluating the test documents on the modified hierarchies, we set the parameters required by SMA for hierarchy modification. Recall that SMA requires two parameters: minimum coverage ratio  and maximum loss ratio  . Usually parameters are set using a development set or through crossvalidation. In our case, however, there is no ground truth on how

good a modified hierarchy is and manual assessment of every modified hierarchy for parameter tuning is impractical. We therefore adopt a bootstrapping like approach described below.
After the test data selected, the remaining data is used for hierarchy modification. We split the remaining data of HYA into 3 parts: P1, P2, P3, and the proportion of their sizes is 12:3:1. Using P1 for HYA and a given auxiliary hierarchy, we obtain a modified hierarchy HnYA. Naturally, all documents in P1 have category labels from hierarchy HnYA. We then build a classifier using all documents in P1 and their labels from HnYA. The classifier classifies documents in P2 and P3. Assume that the classifier gives reasonably good classification accuracy, then all documents in P2 and P3 have their category labels assigned according to HnYA. With these labels, we can evaluate the classification accuracy of documents in P3 by the classifier built using P2 on HnYA. Intuitively, if a hierarchy H1 better organizes documents than another hierarchy H2, then the classifier trained on H1 is expected to have higher classification accuracy for P3 than a classifier built on H2. We then select the parameters leading to the best classification accuracy for P3. In our experiments, the parameters (i.e.,  and  ) set for SMAYA|YA, SMAYA|AB, and SMAYA|ODP are (0.29 and 0.11), (0.17 and 0.17), (0.38 and 0.08), respectively.
Test Data Annotation. With the chosen parameters, each SMA setting generated a modified hierarchy using P1 as HYA and its corresponding auxiliary hierarchy. The preselected 500 test questions are used as test data to fairly evaluate the modified hierarchies by the three SMA settings and the baseline methods. Recall that the 500 questions are not included in the three parts (P1, P2, and P3) for parameter setting. Because BUC and HA do not change the leaf categories, the original labels of the 500 questions remain applicable. For SC and SMA, both changing leaf categories, we invited two annotators to label the 500 questions to their most relevant leaf categories in the modified hierarchies. We synthesized the results of the annotators, and assigned the labels for questions. If two annotators conflicted about a label, a third person made the final judgment. The dataset and their annotations are available online 5.
Classification Results. Classification accuracy measured by MicroF1 using NB and SVM as base classifiers for the six methods (i.e., three baselines BUC, HA, SC, and the three SMA settings) on modified hierarchies is reported in Figure 5(a). For comparison, the classification accuracy on the original (or unmodified) Yahoo! Answers hierarchy is also reported under column named HYA. Figure 5(b) reports Macro-F1.
As shown in Figures 5(a) and 5(b), all the three settings of SMA achieve significant improvement over the results obtained on the original hierarchy. For example, using NB as the base classifier, SMAYA|YA improves Micro-F1 over the results on the original hierarchy by 41.0% and improves Macro-F1 by 40.3%. NB achieves better accuracy than SVM probably because NB was used as the base classifier for parameter setting. The three baseline modification methods only slightly improve the classification accuracy over the original hierarchy and even deteriorate the accuracy in some cases. Recall that SC and SMA modify leaf categories while BUC and HA only modify internal structures of hierarchy without changing leaf categories. All the methods that change leaf categories outperform the methods that keep leaf categories unchanged. Note that SMAYA|YA significantly outperforms SC. One possible reason could be that SMAYA|YA utilizes the semantics of the original hierarchy in hierarchy modification while SC does not.
We observe that the auxiliary hierarchies employed by SMA have effect on the classification accuracy of the modified hierarchy.
5 http://www.ntu.edu.sg/home/gaocong/datacode.htm

798

MicroF1

0.9 NB
0.85 SVM 0.8
0.75 0.7
0.65 0.6
0.55 0.5 HYA
0.8 NB
0.75 SVM 0.7
0.65 0.6
0.55 0.5
0.45 0.4 HYA

BUC

HA

SC

SMAYA|YA SMAYA|AB SMAYA|ODP

(a) Micro-F1

BUC

HA

SC

SMAYA|YA SMAYA|AB SMAYA|ODP

(b) Macro-F1

MacroF1

Figure 5: Micro-F1 and Macro-F1 on the modified hierarchies by three baselines and three SMA settings, and on the original Yahoo! Answers hierarchy.

Table 3: Classification accuracy on modifying AnswerBag

Measure/Hierarchy HAB SMAAB|YA Improvement(%)

Macro-F1 (NB)

0.3444 0.5638

63.7%

Macro-F1 (SVM) 0.3691 0.4933

33.6%

Micro-F1 (NB)

0.4671 0.6669

42.8%

Micro-F1 (SVM) 0.4371 0.5697

30.3%

Measured by Macro-F1, SMAYA|YA without using an external hierarchy slightly outperforms its counterpart SMAYA|AB or SMAYA|ODP, which uses an external hierarchy; while in terms of Micro-F1, SMAYA|YA performs worse than do its counterparts.
5.2.3 Experiments on AnswerBag
In this set of experiments, SMA is used to modify the AnswerBag hierarchy. The main purpose is to evaluate whether SMA remains effective when the size of the auxiliary hierarchy is smaller than the one to be modified. Specifically, we use HYA as auxiliary hierarchy to modify HAB and evaluate the classification accuracy as we did in the earlier set of experiments. Note that the HAB has 265 categories which is more than twice of the 115 categories contained in HYA. The parameters  and  were set as 0.17 and 0.08, respectively, using the parameter setting approach described earlier. The classification accuracy is reported in Table 3. Observe that SMAAB|YA improves the classification accuracy (Macro- and Micro-F1) by 30% to 63%, compared with the result obtained before hierarchy modification. This demonstrates that the proposed SMA approach is effective for different hierarchies, even if the size of the auxiliary hierarchy is smaller than the hierarchy to be modified.
5.3 User Study
A good category hierarchy must be semantically meaningful: (i) Its category labels should be easy to understand, facilitating data browsing; and (ii) Its category structure should reflect the topics of its data. We would like to note that it is challenging to evaluate these. We evaluate the modified hierarchy by SMAYA|AB through two types of user study by following the methods [2, 5], respectively.

Table 4: Comparison on appropriateness of category labels

Judgement

Number of documents

HnYA is better than HYA

12

HnYA is not as good as HYA

1

Both are equally good

81

Neither is good

6

Table 5: Averaged scores of HYA and HnYA (by SMAYA|AB)

Measure/Hierarchy HYA HnYA

Cohesiveness

5.00 6.00

Isolation

4.00 4.67

Hierarchy

5.00 5.33

Navigation Balance 4.50 4.50

Readability

6.00 5.67

Through the study, we aim to quantify both the appropriateness of the category labels and the structure of the modified hierarchy.
Category Labels. Following a similar setting as in [2], we randomly selected 100 questions from the labeled test set originated from Yahoo! Answers. For each question, we gave the path of the categories in HYA from the second level category to the leaf category, and similarly the category path from HnYA (by SMAYA|AB). We asked three students to annotate which category path better reflects the topic of the question. Which hierarchy a category path was originated from was not provided to the annotators. Given a question, each volunteer is asked to rate each path from 1(lowest) to 5(highest) based on its quality. hen, we select one of the following choices based on the averaged ratings (rYA and rnYA for the two paths, respectively). (1)HnYA is better than HYA, if rnYA, rYA  [3, 5] and rnYA > rYA; (2)HnYA is not as good as HYA, if rnYA, rYA  [3, 5]; (3)Both are equally good, if rnYA, rYA  [3, 5] and rnYA = rYA; (4)Neither is good, if rnYA, rYA  [1, 3). The statistics of the labels are reported in Table 4. The table shows that the number of questions having better labels in HnYA is larger than that in HYA although for majority of questions, the category paths from the two hierarchy are equally good. This result also suggests that the generated labels well reflects the content of categories.
Category Structure. Following the evaluation approaches in [5], we evaluate the quality of Yahoo! Answers hierarchy and the modified hierarchy by five measures. Cohesiveness: Judge whether the instances in each category are semantically similar. Since it is impractical to read all questions in a large category, we randomly select 50 questions from each category for cohesiveness evaluation. Isolation: Judge whether categories on the same level are discriminative from each other.We also use the 50 randomly selected questions to represent each category. Hierarchy: Judge whether the concepts represented by the categories become finer from top to bottom. Navigation Balance: Judge whether the number of child categories for each internal category is appropriate. Readability: Judge whether the concept represented by each category is easy to understand.
We invited three students to evaluate the two hierarchies, HYA and HnYA (by SMAYA|AB) and assigned scores ranging from 0 to 7 on each measure. The mean of the scores is reported in Table 5.
The cohesiveness of the modified hierarchy is better than the original one. A possible reason is that our approach detected the hidden topics and merged the most similar ones together. The isolation of the modified hierarchy is slightly better. This is probably because the proposed method takes isolation into consideration. To find out the reasons that caused the relatively low isolation of the original hierarchy, we get the list of categories with low scores from

799

 









&&&





 





 
   

 


 !" #!

 

 #


 $#


 





% " 

 

   



  

&&&

 

   

Figure 6: Portion of Yahoo! Answers hierarchy and its modified hierarchy.

the annotators. As an example, a number of questions that are related to motor-cycling were put under Other - Auto Racing by their askers, resulting in low isolation between the two categories. The modified hierarchy does not deteriorate hierarchy quality, navigation balance, and readability of the original hierarchy on average. In summary, the modified hierarchy is of high quality comparable to the original hierarchy generated by domain experts.
5.4 Case Study
As a case study, we select three categories Software, Internet and Hardware from Yahoo! Answers as an example to illustrate the differences before and after modifying HYA. The modified hierarchy HnYA is by SMAYA|AB utilizing AnswerBag as auxiliary hierarchy.
The two hierarchies are shown in Figure 6. We make the following observations: 1) Different from the original hierarchy, Software and Internet become three categories ≠ Operating System & Application Software, Internet & E-mail and Internet Software. The third category is formed based on the overlapping part of the original two categories, which contains questions about instant messaging (IM) and blog software. This demonstrates that the proposed approach can discover and detach the overlapping hidden topics. 2) Two pairs of categories of the original hierarchy, (Laptops & Notebooks and Desktops), and (Printers and Scanners), are merged into two categories in the modified hierarchy, because of the high similarity between the categories within each pair. This shows that categories with high overlap in semantics are merged. 3) For Hardware, some hidden topics are discovered and new categories are formed, like Storage and CPU & Memory & Motherboard, whose questions come from Desktops, Add-ons and Other - Hardwares in the original hierarchy. These newly formed categories are more isolated from each other.

6. CONCLUSION
Category hierarchy plays a very important role in organizing data automatically (through classifiers built on the hierarchy) or manually. However, with newly available documents added into a hierarchy, new topics emerge and documents within the same category become less topically cohesive. Thus the hierarchies suffer from problems of structure irrelevance and semantic irrelevance, leading to poor classification accuracy of the classifiers developed for automatically categorizing the newly available documents into the hierarchy, which in turn leads to poorer document organization. To address these problems, we propose a novel approach SMA to modify a hierarchy. SMA comprises three non-trivial operations (namely, sprout, merge, and assign) to modify a hierarchy. Experimental results demonstrate that SMA is able to generate a modified

hierarchy with better classification accuracy improvement over the original hierarchy than baseline methods. Additionally, user study shows that the modified category hierarchy is topically cohesive and semantically meaningful.
7. ACKNOWLEDGEMENTS
Quan Yuan would like to acknowledge the Ph.D. grant from the Institute for Media Innovation, Nanyang Technological University, Singapore. Gao Cong is supported in part by a grant awarded by Microsoft Research Asia and by a Singapore MOE AcRF Tier 1 Grant (RG16/10).
8. REFERENCES
[1] G. Adami, P. Avesani, and D. Sona. Bootstrapping for hierarchical document classification. In CIKM, pages 295≠302, 2003.
[2] C. C. Aggarwal, S. C. Gates, and P. S. Yu. On the merits of building categorization systems by supervised clustering. In KDD, pages 352≠356, 1999.
[3] R. Agrawal and R. Srikant. On integrating catalogs. In WWW, pages 603≠612, 2001.
[4] X. Cao, G. Cong, B. Cui, C. S. Jensen, and Q. Yuan. Approaches to exploring category information for question retrieval in community question-answer archives. ACM Trans. Inf. Syst., 30(2):1≠38, 2012.
[5] S.-L. Chuang and L.-F. Chien. A practical web-based approach to generating topic hierarchy for text segments. In CIKM, pages 127≠136, 2004.
[6] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms, Second Edition. The MIT Press and McGraw-Hill Book Company, 2001.
[7] S. C. Gates, W. Teiken, and K.-S. F. Cheng. Taxonomies by the numbers: building high-performance taxonomies. In CIKM, pages 568≠577, 2005.
[8] A. Koestler. The Act of Creation. Penguin Books, New York, 1964. [9] D. Koller and M. Sahami. Hierarchically classifying documents
using very few words. In ICML, pages 170≠178, 1997. [10] K. Nitta. Improving taxonomies for large-scale hierarchical
classifiers of web documents. In CIKM, pages 1649≠1652, 2010. [11] K. Punera, S. Rajan, and J. Ghosh. Automatically learning document
taxonomies for hierarchical classification. In WWW (Special interest tracks and posters), pages 1010≠1011, 2005. [12] X. Qi and B. D. Davison. Hierarchy evolution for improved classification. In CIKM, pages 2193≠2196, 2011. [13] B. Qu, G. Cong, C. Li, A. Sun, and H. Chen. An evaluation of classification models for question topic categorization. JASIST, 63(5):889≠903, 2012. [14] G. Siolas and F. d'AlchÈ Buc. Support vector machines based on a semantic kernel for text categorization. In IJCNN (5), pages 205≠209, 2000. [15] A. Sun, E.-P. Lim, and Y. Liu. What makes categories difficult to classify?: a study on predicting classification performance for categories. In CIKM, pages 1891≠1894, 2009. [16] P.-N. Tan, M. Steinbach, and V. Kumar. Introduction to Data Mining. Addison-Wesley, 2005. [17] L. Tang, J. Zhang, and H. Liu. Acclimatizing taxonomic semantics for hierarchical content classification from semantics to data-driven taxonomy. In KDD, pages 384≠393, 2006. [18] N. Tomuro. Tree-cut and a lexicon based on systematic polysemy. In NAACL, 2001. [19] W. Wei, G. Cong, X. Li, S.-K. Ng, and G. Li. Integrating community question and answer archives. In AAAI, 2011. [20] Y. Yang and X. Liu. A re-examination of text categorization methods. In SIGIR, pages 42≠49, 1999. [21] D. Zhang and W. S. Lee. Web taxonomy integration through co-bootstrapping. In SIGIR, pages 410≠417, 2004. [22] L. Zhang, S. Liu, Y. Pan, and L. Yang. Infoanalyzer: a computer-aided tool for building enterprise taxonomies. In CIKM, pages 477≠483, 2004.

800

When Web Search Fails, Searchers Become Askers: Understanding the Transition

Qiaoling Liuß, Eugene Agichteinß, Gideon Dror, Yoelle Maarek, Idan Szpektor,
ßEmory University, Atlanta, GA, USA Yahoo! Research, Haifa, Israel
{qliu26, eugene}@mathcs.emory.edu, {gideondr, idan}@yahoo-inc.com, yoelle@ymail.com

ABSTRACT
While Web search has become increasingly effective over the last decade, for many users' needs the required answers may be spread across many documents, or may not exist on the Web at all. Yet, many of these needs could be addressed by asking people via popular Community Question Answering (CQA) services, such as Baidu Knows, Quora, or Yahoo! Answers. In this paper, we perform the first large-scale analysis of how searchers become askers. For this, we study the logs of a major web search engine to trace the transformation of a large number of failed searches into questions posted on a popular CQA site. Specifically, we analyze the characteristics of the queries, and of the patterns of search behavior that precede posting a question; the relationship between the content of the attempted queries and of the posted questions; and the subsequent actions the user performs on the CQA site. Our work develops novel insights into searcher intent and behavior that lead to asking questions to the community, providing a foundation for more effective integration of automated web search and social information seeking.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Storage and Retrieval
Keywords
query analysis, community question answering
1. INTRODUCTION
While Web search engines have significantly progressed in effectiveness and efficiency over the last decade, there still exist certain user needs that cannot be satisfied. This could be due to a number of reasons, such as the difficulty of expressing a complex need as a short search query, the lack of existing relevant content on the Web (e.g., for unique or "tail" needs that keep appearing), and for more "social" needs, for which the user prefers to interact with a real human.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

In fact, Hitwise in August 2011 reported that only 6680% of the searches are successful1, and Hassan et al. [15] obtained a similar success rate of search goals (73%) via human labeling. We argue that many such unsatisfied searches could be addressed by asking people via Community Question Answering (CQA) services, such as Baidu Knows, Quora, or Yahoo! Answers. It already happens in practice. For example, we have observed that about 2% of web search sessions performed by users who are also members of the Yahoo! Answers community, lead to a question posted to the community. Consider Figure 1a, which depicts a sample search submitted to a major search engine. The searcher is not satisfied with the results, and eventually posts a related question on the Yahoo! Answers site, which is then answered to the searcher's satisfaction. Understanding and improving the synergy between searching and community question answering is at the heart of this work.
Specifically, our goal is to better understand the behavior of these users, as well as characterize the types of Web searches that could be effectively handled by CQA sites. Insights acquired during such analysis should bring multiple benefits to both search engines and CQA systems. On one hand, search engines always need to better understand when searchers are unsatisfied by the returned results. More specifically, Web search engines would find value in analyzing the search session patterns of such unsuccessful queries, the associated underlying query intents, and possibly reflect these findings in search effectiveness metrics. One can even imagine new search experiences that would allow users to turn to the community for certain types of needs better addressed by people than by traditional Web search. Additionally, CQA systems could potentially improve the asking experience by taking advantage of the context provided by unsuccessful queries preceding a posted question. One can imagine several ways to leverage this context, such as automatically giving examples of irrelevant answers to clarify the question to the community.
To the best of our knowledge, our work is the first to perform a large-scale study of the transformation of searchers into askers. That is, we start our analysis with web search sessions, trace the searcher through her visit to a CQA site, and analyze the resulting questions posted for the community.
We focus on one of the most visited, and more mature, CQA systems existing today, namely Yahoo! Answers, which
1www.hitwise.com/us/about-us/ press-center/press-releases/ experian-hitwise-reports-google-share-of-searche/

801

(a)

(b)

Figure 1: Example search (a) followed by a question posted by the same user on the Yahoo! Answers site with a satisfactory answer from the community (b).

with more than 1 billion posted answers2 is highly visible

such as occurrence of personal pronouns or sentiment

in most search engines result pages. We built a corpus of

indicators (Section 4).

query-to-question transitions and studied it in order to un-

derstand when and why searchers become askers. A privacy-
preserving subset of the data has recently been made publicly available through Yahoo's Webscope program3.

Research Question 3: How do searchers behave after transferring to the CQA site?

More specifically, our study is organized around the following three research questions, each associated with a set of hypotheses:

∑ Hypothesis 5: We further hypothesize that the content and the topics of the questions posted after a search session differ substantially from the general ques-

Research Question 1: When do searchers turn to CQA

tion distribution (Section 5.1).

for answers?

∑ Hypothesis 6: We hypothesize that the question ses-

∑ Hypothesis 1: Queries and information needs of search sessions that lead to posting questions are hypothesized to share common characteristics, and differ from general web searches in words and information needs (Section 3.1).

sions after switching from searching, exhibit different characteristics than general question sessions and search sessions explored in depth in previous research work [17][10]. We study these different types of sessions in terms of duration and persistence for specific users and examine their behavior over time (Section 5.2).

∑ Hypothesis 2: We hypothesize that searchers who switch to CQA exhibit common search behavior. For instance, they tend to click more on CQA results on

The rest of this paper is dedicated to answering the above questions and verifying the associated hypotheses.

the search result page, and their search sessions are

longer, allowing to characterize different types of users in the same spirit as [7] (Section 3.2).

2. ACQUIRING DATA
In order to understand how searchers become askers, we

collected a dataset that contains both the search session part

Research Question 2: How do search queries relate to the associated questions posted on CQA sites?

and asking session part of each user who conducted a search session that resulted in posting a question. Our dataset is derived from joining a sample of the query logs of the Yahoo!

∑ Hypothesis 3: Queries and questions follow different

search engine and the Yahoo! Answers question logs, both

word distributions. More specifically, words in queries

for June 2011.

are hypothesized to follow different distributions than

To create this dataset, we first created a mapping of users

those appearing in questions, and a clear vocabulary

between the two logs, based on the clicks on the same Yahoo!

gap between these can be observed (Section 4).

Answers question page following the same query on the same

time frame, as they appear in both logs. Then, we extracted

∑ Hypothesis 4: Questions are typically more specific

user actions from the query and question logs, e.g. posting

than queries and include additional context (e.g., per-

queries and clicking on results from query logs, as well as

sonal background) absent from the original queries.

posting questions and re-viewing them from the question

We hypothesize that these differences are reflected in

logs. We constructed search sessions from these extracted

the lexicographic differences between questions and queries, actions, with a 30 minutes timeout as a session boundary.

2http://yanswersblog.com/index.php/archives/2010/
05/03/1-billion-answers-served/ 3http://webscope.sandbox.yahoo.com/

Question sessions have no temporal boundary, since every action in the session unambiguously refers to the question posted by the asker.

802

Table 1: Statistics of the constructed datasets. Description
Total sampled search sessions SearchOnly sessions Search sessions that include question sessions SearchAsk sessions: search sessions with a single relevant question posted after searching

Number of sessions
1,287,238 (100%) 1,233,279 (95.8%)
53959 (4.2%) 21231 (1.65%)

Query length distribution (cumulative) 0.0 0.2 0.4 0.6 0.8 1.0

Once we had search sessions and question sessions, and mapping between some of them, we created two datasets. The first, termed SearchAsk dataset, contains search sessions that turned into question sessions. We only kept such sessions that resulted in posting one and only one question for simplicity of analysis later on. In addition, we only kept sessions in which the posted question is "relevant" to a previously issued query (if the query and the question share at least one non-stopword, they were considered relevant). By observing that some users actually searched for Yahoo! Answers to navigate to its home page before they posted a question there, we deleted such special navigational clicks and corresponding queries from the user action sequences. The second dataset, termed SearchOnly dataset, consists of search sessions that did not turn into question sessions. In both datasets, we only kept sessions for users that posted at least once in Yahoo! Answers, since these users are aware of the site and know how to post a question there, thus removing the potential investment of effort for newcomers to join the site, and filtering our the users that simply do not know where to ask questions.
Table 1 reports the statistics of the datasets we obtained. As shown in the table, 95.8% of all the search sessions are SearchOnly sessions, while SearchAsk sessions account for 1.65%. Despite the sparsity of the SearchAsk sessions, we still believe that understanding how searchers become askers in such sessions can be helpful for improving the search experience of these users and perhaps more users. Indeed, the two datasets allow us to investigate the differences between sessions in which users posted a question following attempted searches, mainly due to search failure or searcher frustration, and sessions in which users that have experience of asking questions on Yahoo! Answers did not bother or did not need to ask questions at that time. Recall, that the users in these datasets satisfy two conditions: (1) having clicked at least a Yahoo! Answers question page within this month; and (2) having asked at least one question on Yahoo! Answers within the month. Yet, we believe that such users still represent the general, though somewhat experienced, web searchers.
3. FROM SEARCHING TO ASKING: QUERY AND BEHAVIOR ANALYSIS
As a first step, we study the characteristics of queries leading to a question post on Yahoo! Answers (Section 3.1), and the characteristics of searcher behavior before question asking (Section 3.2).
3.1 Characteristics of Queries leading to Questions
The first interesting question is which queries are more likely to be unsuccessful for automated search, but instead are more amenable to be answered by a CQA site. To get such queries, we examine each SearchAsk session, and

SearchAsk SearchOnly

1

2

5 10 20

50

Query length (number of words)

Figure 2: Distribution of query length

Table 2: Statistics of words per query

Avg # Avg # Avg % Avg

words stop- stop- word

words words length

SearchAsk queries 6.5

2.4

28% 5.1

SearchOnly queries 3.4

0.72 11% 6.0

extract the queries that are issued before the question is posted, and are relevant to the question. We call such queries SearchAsk queries. For comparison, we also extract the queries in each SearchOnly session which are called SearchOnly queries. In the following, we explore how SearchAsk queries are different from SearchOnly queries in terms of length, words, frequency, and results.
Query Length Distribution.
Figure 2 compares the distribution of query length (in terms of number of words in the query) for the SearchAsk and SearchOnly queries. We can see that SearchAsk queries tend to be longer than SearchOnly queries, as 85% of the SearchOnly queries contain at most 5 words, while about 50% of the SearchAsk queries contain more than 5 words. Therefore, searchers issuing longer queries are more likely to turn to Yahoo! Answers to post a relevant question.
Table 2 compares the average word length per query and the average number of stopwords for the SearchAsk queries and SearchOnly queries. We can see that, on average, queries turning to questions tend to contain more words (but shorter words) than queries that do not turn to questions. The main reason could be that SearchAsk queries contain more stopwords (which are often short) than SearchOnly queries. Indeed, the percentage of stopwords in SearchAsk queries is over 2.5 times higher than in SearchOnly queries.

803

Query frequency distribution (cumulative) 0.5 0.6 0.7 0.8 0.9 1.0 Search Search Only Ask

Query SERP with url from Yahoo! Answers Query SERP without url from Yahoo! Answers

SearchAsk SearchOnly

1

10 100 1000 10000

Query frequency in 1-month query log

Figure 3: Distribution of query frequency

Query Words Distribution.
To better understand the difference between the content of SearchAsk queries and SearchOnly queries, we compare their word distributions and show the main difference in Table 3. We can see that SearchOnly queries are more likely to be navigational, e.g., to reach websites like Facebook or YouTube, or to find information related to the searcher's common tasks such as looking up the weather, hunting for coupons, or finding a cooking recipe. In contrast, SearchAsk quries are more likely to start with question words (e.g., `how', `what'), and tend to use more verbose natural language to express the needs of the searchers (e.g., `want', `to', `know') rather than using only keywords.
Query Frequency Distribution.
To verify the hypothesis from the above word distribution analysis that SearchAsk queries are more likely to be unique, we compute the frequency4 of SearchAsk queries and SearchOnly queries in our 1-month query log. Figure 3 shows the results. We can see that over 90% of SearchAsk queries are tail (actually unique) queries, indicating the variety of the needs of searchers and the ways to express them. In contrast, SearchOnly queries contain more popular queries, e.g., around 20% of SearchOnly queries occur in more than 100 search sessions.
Query Results Distribution.
To better understand user needs behind SearchAsk queries, we further examine the results returned in their search engine result pages (SERPs). We found a significant difference between SearchAsk and SearchOnly queries based on whether a SERP contains a Yahoo! Answers question page. As shown in Figure 4, a Yahoo! Answers question page occurs in the SERPs for half of the queries that eventually turn to questions, but for only 13% of SearchOnly queries. It is clear that SearchAsk queries are more likely to have a Yahoo! Answers question page in the SERP. This is not surprising. First, having a Yahoo! Answers question page in search results indicates that the query could be relevant to an existing Yahoo! Answers question. Therefore, answers from a human might be more suitable to address the need behind the query, encouraging the searcher to post a question on Yahoo! Answers. Second, more impressions often leads to more clicks. After landing on the Yahoo! Answers
4The frequency of a query in this paper is computed as the number of search sessions containing the query.

0.0

0.2

0.4

0.6

0.8

1.0

Probability

Figure 4: Distribution of query results

site, the searcher might realize that a community might be able to answer her information need, and try posting a question.
Summary of Query Characteristics.
As a summary of the above analysis, we conclude that queries that are more likely to fail in search and lead to a question post on Yahoo! Answers tend to be longer, and use more verbose natural language to express the searchers' needs. The needs behind such queries tend to be more unique and complex than those associated with SearchOnly queries.
3.2 Searcher Behavior Before Asking Questions
To understand how searchers become askers, we analyze the searcher behavior in search sessions, with an associated question posted by the same user on Yahoo! Answers.
Last Action Before Question Asking.
First, we examine what searchers do right before they start question asking, i.e., we examine the last user action prior to a question being posted. We found that the last search action before question asking is a click on Yahoo! Answers question result in 47.8% of the sessions, a click on other result in 31.2% of the sessions, and a query in 17.4% of the sessions. We notice that in about half of the sessions, the searcher posts a question right after clicking on a Yahoo! Answers question page from the search engine results. There may be several reasons for this. First, such a click indicates that the query is relevant to the clicked question, and therefore it probably carries an information need that would benefit from a human response. Second, when the clicked Yahoo! Answers question page cannot satisfy the search need, it encourages the user to post a new question on Yahoo! Answers. Of course, it is also possible that a searcher had already decided to post a question when seeing the original SERP, and she then clicked on a Yahoo! Answers question result simply to navigate to the Yahoo! Answers site.
Distribution of Clicks.
To better understand the effects of clicking on a Yahoo! Answers question result on the transformation of searchers into askers, we compute and compare the likelihood of such clicks in SearchAsk and SearchOnly sessions. Figure 5 shows the results. First, 21% of SearchOnly sessions and 81% of SearchAsk sessions contain a Yahoo! Answers question page in the SERPs. Next, after seeing a Yahoo! Answers question page in the SERPs, 81% of the searchers who turned to

804

Words First words Content words
Words First words Content words

Table 3: Frequent words in SearchAsk queries and SearchOnly queries More likely in SearchAsk queries
to, a, be, i, how, do, my, can, what, on, in, the, for, have, get, with, you, if, yahoo, it how, what, can, be, why, i, do, my, where, yahoo, if, when, 0000, a, will, 00, best, who, which, should yahoo, 00, use, 0, work, song, old, help, make, need, like, change, year, good, long, mail, answer, email, want, know
More likely in SearchOnly queries facebook, youtube, google, lyric, craigslist, free, online, new, bank, game, map, ebay, county, porn, tube, coupon, recipe, home, city, park facebook, youtube, google, craigslist, ebay, the, you, gmail, casey, walmart, amazon, *rnrd, justin, facebook.com, mapquest, netflix, face, fb, selena, home facebook, youtube, google, craigslist, lyric, free, bank, map, ebay, online, county, porn, tube, coupon, recipe, anthony, weather, login, park, ca

Search Search Only Ask

1+ clicks on Yahoo! Answers question result session SERPs with url from Yahoo! Answers session SERPs without url from Yahoo! Answers

1/1 Begin

0.12 / 0.13

0.15 / 0.03

Click ques result

0.47 / 0.21

0.25 / 0.40

0.06 / 0.01

0.14 / 0.23

Query

0.04 / 0.05

Ask /End

0.0

0.2

0.4

0.6

0.8

1.0

Probability

Figure 5: Distribution of clicks

0.30 / 0.23

0.51 / 0.43

0.48 / 0.66

Click other result

0.30 / 0.25

0.09 / 0.29

askers had clicked on a Yahoo! Answers question result while 19% of them hadn't; in contrast, 43% of the searchers in SearchOnly sessions seeing a Yahoo! Answers question result clicked on it while 57% of them didn't. Therefore, users in SearchAsk sessions are about twice as likely as in SearchOnly sessions to click on a Yahoo! Answers question page in the search results once seeing it. This indicates that searchers are more likely to post a question once clicking on a Yahoo! Answers question result.
Transitions between Actions.
To better understand searcher actions, we further compute the probability of transitions between actions in SearchAsk and SearchOnly sessions respectively, and compare them in Figure 6. The transition probability between two actions ai and aj in SearchAsk (SearchOnly) sessions is computed using Maximum Likelihood estimation: P (ai, aj) = Nai,aj /Nai , where Nai,aj is the number of transitions from action ai to action aj in all SearchAsk (SearchOnly) sessions, and Nai = ak Nai,ak . SearchAsk transition probabilities are shown in red before the slash symbol, while SearchOnly transition probabilities are shown in black after the slash symbol. If we look at the transitions for SearchOnly sessions from the figure, we can see that after issuing a query, the searcher is very likely to click on other result, then with perhaps more queries and clicks on other result, and then ends the session. Clicking on a Yahoo! Answers question result is very unlikely. However, in SearchAsk sessions, the searcher has a higher probability on clicking a Yahoo! Answers question result. After the click, the searcher in SearchAsk sessions would post a question on Yahoo! Answers for around half of the time.

Figure 6: Transition probabilities for actions in SearchAsk (in red, before the slash symbol) and SearchOnly (in black, after the slash symbol) sessions. Note that two other actions (Pagination and Click interface) are ignored for simplicity.
Action Sequences Before Question Asking.
To better understand how searchers become askers, we examine the user action sequences in SearchAsk sessions before the question post, and compare them with action sequences in SearchOnly sessions. Table 4 shows a sample of top frequent user action sequences. The top frequent path in SearchOnly sessions indicates navigational needs of the searchers, i.e., they issue a query, click on a search result and leave the session. Such navigational cases account for 30% of total SearchOnly sessions. In contrast, the top frequent path in SearchAsk sessions indicates more "social" needs of the searchers, i.e., they issue a query, click on a search result of Yahoo! Answers question page, and then ask a question on Yahoo! Answers. Yet, the path distribution is more balanced for SearchAsk sessions. Moreover, clicks on Yahoo! Answers question results are common in the paths.
Session Size Distribution.
Finally, we compare the distribution of session sizes for SearchAsk and SearchOnly sessions. Session size can be measured in several ways, e.g., by the number of (unique) queries issued by the searcher in the session, by the number of actions performed in the session, or by the duration that the session lasts. We use the first option in this paper. The results are shown in Figure 7. While only one query

805

Table 4: Top frequent user action sequences in

SearchAsk sessions and SearchOnly sessions (B: Be-

gin a session, Q: Query, Cqr: Click on a Yahoo! Answers question result, Cor: Click on other result, A: Ask a question, E: End a session)

SearchAsk sessions Distribution

B Q Cqr A B Q Cor A B Q Q Cqr A BQA

10% 3.8% 3.3% 2.8%

B Q Cor Q Cqr A

2.0%

SearchOnly sessions Distribution

B Q Cor E B Q Cor Q Cor E BQE

30.2% 7.1% 6.1%

B Q Cor Cor E B Q Q Cor E

3.6% 3.6%

Table 5: Statistics of length difference between a

query and its associated question (number of words).

Median Avg Max

|question| - |query| 42

66 1431

|subject| - |query|

3

4 27

|content| - |query| 31

55 1428

Table 6: Overlap of content words (CW) between a query and its associated question.

CW?  CWquery CW? = CWquery CW?  CWquery CW?  CWquery CW?  CWquery

?=question
31.4% 1.8% 0.7% 66.1%

?=subject
14.6% 6.2% 3.7% 75.5%

?=content
14% 0.4% 17.1% 68.5%

Sessioin size distribution (cumulative) 0.0 0.2 0.4 0.6 0.8 1.0

SearchAsk SearchOnly

1

2 3 5 10 20

50

Session size (number of unique queries)

Figure 7: Distribution of session size

is issued in the half of SearchOnly sessions, at least three different queries are issued in the half of SearchAsk sessions. The average session size is 2.5 for SearchOnly sessions and 3.8 for SearchAsk sessions. This shows that searchers tend to issue more queries in SearchAsk sessions, possibly because SearchOnly sessions contain more navigational needs, while SearchAsk sessions are associated with more difficult or complex needs, and thus require more effort in finding answers.
4. QUERIES VS. QUESTIONS: CONTENT ANALYSIS
After discovering the unique attributes of queries that lead to asking a question, we next want to understand better the process of turning a search session, as captured by a query, into a question posted on Yahoo! Answers.
The most expected difference between queries and questions is their length. Table 5 shows these differences. From the table we can see that a question has 66 more words than its associated query on average. This indicates two things: first, as expected, questions are much more verbose, being natural language expressions, compared to the concise queries; second, since Yahoo! Answers questions are not

Figure 8: Word distributions over question words
limited in length, additional knowledge of the problem to be solved is added. Interestingly, the subject of the question is very close in length to the query, which shows that searchers still think in search-style writing for the subject. However, the content part of the question is significantly longer, and much more information is added in this question part.
We next look at word distribution differences, since they may point at the lexical gap between queries leading to questions and their associated posted questions. Figure 8 depicts the word occurrence distribution over word ranking by frequency for search-related questions. The most notable difference between the two distributions is that questions tend to be more personal and verbose, as captured by the abundant usage of the pronouns such as `I', `me', `it' and `this', connectives such as `but', `because', `recently', and `just', as well as sentiment indicators such as `help', `please', and `thanks'. Queries, on the other hand, tend to focus more on the things or actions that are searched for, with content words like `best', `free', `download' and `games' as well as question words like `how', `why' and `what' occurring more frequently than in the associated questions corpus. Interestingly, one to four digit figures, such as car model years,

806

Table 7: Examples showing semantics difference between the query and the question.

ID Type of Query

Question (Category, Subject, and Content)

context

added

1 N/A

what to serve with Food & Drink>Cooking & Recipes

chicken salad

what can you serve with chicken salad?

2 thought best nba players with- Sports>Basketball

out a championship

Greatest NBA players to never win championship?

Patrcik ewing, reggie miller, charles barkley, karl malone? Who else?

3 task

pt cruiser ac fix

Cars & Transportation>Car Makes>Chrysler

how much does it cost to fix an ac system in a pt cruiser?

4 task

solve n^2-2n-3=5000 Education & Reference>Homework Help

Algebra question, Need Help Pls!!!!?

An owner of a key rings company found that the profit earned (in thousands of

dollars) per day by selling n number of key rings is given by n^2 - 2n - 3, where

n is the number of key rings in thousands. Find the number of key rings sold on

a particular day when the total profit is $5000. Thanx

5 limit

chocolate croissant Dining Out>United States>San Jose

menlo park

Where can I get a good Chocolate Croissant near Menlo Park, CA?

Something with thick, dark chocolate? And please, don't say La Boulanger.

6 situation, chicago fried chicken Dining Out>United States>Chicago

task

Where can I get really good fried chicken in the Lakeview area in

Chicago?

I really want fried chicken after watchin a special on TV. But I cant find any place

near me that has decent priced chicken thats not fast food and is homemade and

delicious. Any one know of a place?

7 situation, douglas az

Education & Reference>Higher Education (University +)

task

Radiology schools in Arizona?

Does any one know any schools in az that offer radiology degree programs, I

moved to Douglas az and don't know any schools near to study radiology. If any

one can help that would be great :)

8 attribute, how many bottles to Pregnancy & Parenting>Newborn & Baby

situation, buy for a newborn

How many bottles should I purchase for my new baby? And what

task

brand is best?

I am 9 mo. pregnant and still need to buy bottles. I will be trying to breast

feed but I am unsure of how many bottles and what sizes I should buy. Is there

anything else I will need for feeding and what brand do you recommend? Thanks!

also appear more in question-related queries than in their associated questions, probably since they capture much of the essence of the target information need.
To further understand the semantic difference between composing a query and its related question, we measured the distribution of query-question pairs in which the same words are used for both query and question, the pairs in which one is included in the other, and those pairs in which each contains words that do not occur in the other. Table 6 presents these statistics, while Table 7 provides examples of such pairs, annotated with the type of context added when switching from query to question, as been classified by [23], i.e., task, situation, attribute, limit, and thought.
Some interesting question composition patterns are evident from this analysis. First, in the majority of pairs (66%), both queries and questions contain unique words that do not occur in the other. This is somewhat surprising, since we would expect more complete inclusion of the query terms in the question. However, it seems that with the freedom of writing a free text question, searchers tend to rephrase some of the terms they used in their queries. For example, abbreviations and short terms are turned into their more complete forms, e.g. `AZ' into `Arizona' and `newborn' into `new baby' (see example 7 and 8 in Table 7). In addition, while 31% of the pairs do show complete inclusion of the query terms in the question, many times the query terms do not all appear in the question's subject or content, but spread in both question parts. Table 7 shows that most of the extensions of

the query into a question include additional details that are related to the search task. Yet, many times details of the personal situation are added, such as the state of mind, e.g. "after watching a special on TV " (example 6 in Table 7).
One interesting future research is to automatically generate questions from queries [25][26]. However, adding context information to the question, such as the situation or limit is a difficult challenge. Still, expanding the query expression to an explicit question form may be possible for many cases, e.g. examples 1 and 3 in Table 7.
5. ASKING AFTER SEARCHING: QUESTION ANALYSIS
As our final analysis, we are interested in discovering unique activity patterns in Yahoo! Answers that searchers posting a question have, compared to typical asker behavior in Yahoo! Answers. Specifically, we first examine the differences in lexicon, that is whether different words are used when composing a question (Section 5.1). Then, we analyze the difference in asker behavior after posting such questions, in terms of "traditional" CQA activities (Section 5.2).
5.1 Characterizing Questions Posted after a Search Session
As expected, we find that there is a large difference between the word distribution for the corpus of all questions posted in June 2011 and the distribution of the corpus of

807

Table 8: Categories with largest differences in

assignment probability between questions coming

from search and general questions

Categories more likely for Categories more likely for

general questions

questions following search

Polls & Surveys (Entertain- Maintenance & Repairs

ment & Music)

(Cars)

Singles & Dating

Law & Ethics

Religion & Spirituality

Dogs (Pets)

Politics

Pregnancy

Friends

Maintenance & Repairs

(Home & Garden)

Mathematics

Renting & Real Estate

Diet & Fitness

Accounts & Passwords

Lesbian, Gay, Bisexual, and Other - Yahoo! Mail

Transgendered

Other - Beauty & Style

Military

Basketball

Problems with Service

Baby Names

Garden & Landscape

Adolescent

Cooking & Recipes

questions posted by searchers. In addition, the entropy of generating a word from the search-related question corpus is much lower, showing a more focused vocabulary. But what are the reasons for this large difference? It turned out to be mainly topical.
To measure this topical difference between the two types of questions, we looked at the distribution of categories to which the questions in the two compared corpora are assigned. Table 8 shows the categories with largest differences in assignment probability, those that are preferred more in the general question corpus and in the search-related question corpus respectively. These lists show that searchers tend to ask informational questions [14] to get fact- or adviceoriented answers, such as how to fix the car or maintain one's garden, how to bake cookies, but also questions related to Yahoo products, such as Yahoo! Mail. On the other hand, regular askers are more likely to ask conversational questions [14] with a social flavor, such as discussions around music or sports events, politics and religions, and opinions on possible baby names. We manually labeled 100 questions randomly sampled from the search-related question corpus, and found none are conversational, showing a very different distribution compared to that 38% of Yahoo! Answers questions are conversational as reported in [14]. We conjecture that this is because searchers usually turn to search engines to find information instead of starting conversations. Another kind of questions that are less likely searched first over the web are personal questions, in which the asker is interested in adding very personal details. These include topics such as diet and fitness advices, dating and style opinions. Finally, there are questions that are too complex, for which the asker knows the answer cannot be found on the web. A good example are Math questions, such as example 4 in Table 7.
To further investigate the differences between the two question types, we removed the strong bias caused by the different category distributions within the two corpora by sampling questions from the general question corpus based on the category distribution of the search-related question corpus. By comparing the word distribution between the sampled corpus and the search-related question corpus, we found that hardly no topical differences remained. That is, the topical variation in the two corpora is more or less completely captured by the level of assigned categories, without

Table 9: Statistics of words in SearchAsk questions

and sampled general questions

Avg. corpus

Sampled general SearchAsk

statistics

questions

questions

# words

78.3

73.7

# words per sentence

13.5

13.7

# sentences

5.8

5.4

% stopwords

66.3

65.0

word length

4.22

4.17

Table 10: Statistics about user follow-up activities around their posted questions.
SearchAsk Ask Search

Avg duration

30h

32h 19.4m

Median duration

2.2h

3.7h 11.6m

Avg #actions

6.41

7.45

-

Median #actions

5

5

-

more subtle topical differences evident. Still, there may be stylish variations in question composition between searchers and typical askers. Table 9 provides the stylish statistics for the general-sampled and search-related question corpora. The significant difference between the two corpora is the number of words per question: for the same topics, general questions contain 6% more words compared to searchrelated questions. Yet, interestingly, this attribute is due to more sentences that are written on average per general question, while if we look at the number of words per sentence, we see that surprisingly search-related questions have slightly more words in each sentence. This could be related to more information-focused nature of the questions posted after a search session, and suggests further investigation.
5.2 Asker Follow-up Activity after a Search
As our final question behavior analysis, we wanted to test whether a searcher interacts more or less with Yahoo! Answers after posting the question. To that end, we measured both the number of actions that both searchers and regular askers perform around a specific question they posted, as well as the duration of this set of actions. Follow-up actions after a posted question include: browsing the question page (e.g. checking for new answers), adding more details to the question, selecting a best answer, reporting abusive answers, voting for answers, and deleting the question.
Table 10 provides the average statistics of these actions, while Figures 9 and 10 depict the distribution of number of actions and their duration for searchers and regular askers. From the table we can see that searchers perform fewer yet similar number of actions as typical askers do, but in a much shorter duration. As can be seen by Figure 9, in terms of number of actions, the difference of about one more action on average for regular askers is small though constant. For the duration of the interaction, regular askers spend about 7% more time on average around the question, but looking at the median, the difference is substantially larger, with half the searchers spending 2.2 hours or less while the typical askers tend to spend about 68% more time, or 3.7 hours, at the median. As an interesting comparison, we also measured the average time the searchers spent searching before asking questions, to show the substantial difference between

808

Cumulative distribution function 0.0 0.2 0.4 0.6 0.8 1.0

SearchAsk Ask

1

2

5 10 20

50

# of user actions on question after it is posted

Figure 9: Count of user actions in questions

1.0

0.8

0.6

Cumulative distribution function

0.4

0.2

0.0

SearchAsk Ask Search

0

10m 1h

1d

1m

Duration of user actions on the question after it is posted

Figure 10: Duration of user actions in questions

an interactive search session and an offline asking session, a difference that is clear to the searchers, since they are willing to spend several hours waiting for an answer to arrive, compared to a few minutes actively searching.
In summary, we showed that searchers are expecting a faster response time for their questions, which often aim to address practical problem solving tasks. On the other hand, general Yahoo! Answers askers are willing to put more effort in following up their questions. One possible reason for this behavior is that Yahoo! Answers site is often viewed by users from a more social perspective, as indicated by many users asking socially-focused (e.g., conversational) questions.
6. RELATED WORK
As we study the transformation of unsatisfied searches into questions posted on a popular CQA site, our work is related to the work on query log analysis, searcher behavior and satisfaction prediction, and CQA question analysis.
On the search side, significant research has been done on analysis of queries and searcher behavior based on query logs. For example, understanding query intent and user goals has attracted much research effort [22][6]. Difficult queries [9][8], long and tail queries [4][5], and question-like

queries [21] have also received special research attention. Besides, searcher satisfaction and frustration [11][15][2][16] has also been actively studied, which utilized query log information for satisfaction prediction, such as relevance measures, as well as user behavior during the search session, including mouse clicks and time spent between user actions.
Donato et al. [10] identified the research missions that often associate with complex information needs and require collecting information from many pages. In our work we focused on studying the types of queries that arguably are difficult for a web search engine to satisfy, often require human to answer [19][20][18], and thus could be better handled by CQA sites. Liu et al. [18] argued that some of these needs can be satisfied with existing answers from CQA archives by harnessing the unique structure of such archives for detecting web searcher satisfaction. Our work in this paper further observed that many searchers not satisfied with search results finally posted a related question on a CQA site, which inspired our analysis of how searchers become askers.
White and Dumais [24][12] studied search engine switching behavior and developed models to predict the switching and its rationale. Although different types of searchers are focused on (they focused on searchers who turn to another search engine and issue more queries, while we focused on searchers who turn to CQA sites and post questions), we are both interested in characterizing the types of queries and searcher behavior that lead to the switchings. Our analysis shows both similar (e.g. longer sessions are more likely to involve a switch) and different characteristics (e.g. different last action before switching) compared to their study.
On the CQA side, there is also research effort devoted to question analysis, e.g. distinguishing conversational and informational questions [14], identifying high quality questions [3], and investigating the effects of contexts in questions on answer quality [23]. In our work, we use their classification of contextual factors to analyze the semantic difference between the query and question posted by the same user for the same need. There is also some previous work related to asker behavior analysis. For example, Adamic et al. [1] analyzed the content properties and user interaction patterns across different Yahoo! Answers categories. Gyongyi et al. [13] studied several aspects of user behavior in Yahoo! Answers, including users' activity levels, interests, and reputation. Yet, they did not study the effort that askers spend in tracing their posted questions as we studied in this paper.
7. CONCLUSIONS
Web search needs are becoming increasingly sophisticated, and the expectations have grown accordingly. As a result, quite a few search sessions end in posting a question in a Community Question Answering service, as the searcher realizes that such a service could better answer her need.
This work studies the unique properties of SearchAsk sessions: search sessions that turn into question composition. To the best of our knowledge, this paper presents the first large-scale analysis of the user transition from searching to asking. What makes our work unique is the study of the explicit connection between the search query and the corresponding question from the same user for the same need. It provides insights into some specific needs that searchers try to express on search engines, yet are not satisfied by search results, and turn to human answerers instead. We analyzed the various aspects of SearchAsk sessions, includ-

809

ing the differences between general search-engine queries and those belonging to a SearchAsk session, the transformation of a query into a natural language question and the question composition patterns, as well as other asking behavior of searchers, compared to general askers in a CQA service.
Our findings may contribute both to search-engine optimization, as well as to better user experience in CQA sites. For example, we found out that searchers are not as patient as regular askers when waiting for answers to their questions. This finding may influence CQA sites to promote questions coming from searchers, if they want to retain their engagement. As another example, our analysis of the transitions between user actions in SearchAsk sessions, and especially the fact that question asking is typically preceded by viewing a CQA page, may help search engines. They might decide to detect such cases and explicitly promote the option of asking a question to the searcher, even before she resorts into doing it on her own. Furthermore, as this paper demonstrates, modeling the transformation of a query meant for an automated search engine into a fully specified question meant for human, provides a valuable tool for query intent and satisfaction analysis.
In future work, we intend to develop some of the directions mentioned above. One of the most intriguing ones in our view is for search engines to automatically trigger a dialog for posting questions in the right CQA forum, whenever a SearchAsk need is detected. This is just one application made possible by our study, which lays a foundation for more effective integration of automated web search and social information seeking.
8. ACKNOWLEDGMENTS
This work was supported by the National Science Foundation grant IIS-1018321 and by the Yahoo! Faculty Research and Engagement Program. The authors also would like to thank Elad Yom-Tov for the helpful discussion and the anonymous reviewers for their valuable comments.
9. REFERENCES
[1] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman. Knowledge sharing and yahoo! answers: everyone knows something. In WWW, 2008.
[2] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find it if you can: a game for modeling different types of web search success using interaction data. In SIGIR, pages 345≠354, 2011.
[3] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne. Finding high-quality content in social media. In WSDM, pages 183≠194, 2008.
[4] P. Bailey, R. W. White, H. Liu, and G. Kumaran. Mining historic query trails to label long and rare search engine queries. ACM Trans. Web, 4:15:1≠15:27, September 2010.
[5] M. Bendersky and W. B. Croft. Analysis of long queries in a large scale search log. In WSCD, 2009.
[6] A. Broder. A taxonomy of web search. SIGIR Forum, 36:3≠10, September 2002.
[7] G. Buscher, R. W. White, S. Dumais, and J. Huang. Large-scale analysis of individual and task differences in search result page examination strategies. In WSDM, pages 373≠382, 2012.

[8] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In SIGIR, 2006.
[9] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In SIGIR, 2002.
[10] D. Donato, F. Bonchi, T. Chi, and Y. Maarek. Do you want to take notes?: identifying research missions in yahoo! search pad. In WWW, pages 321≠330, 2010.
[11] H. A. Feild, J. Allan, and R. Jones. Predicting searcher frustration. In SIGIR, pages 34≠41, 2010.
[12] Q. Guo, R. W. White, Y. Zhang, B. Anderson, and S. T. Dumais. Why searchers switch: understanding and predicting engine switching rationales. In SIGIR, pages 335≠344, 2011.
[13] Z. Gyongyi, G. Koutrika, J. Pedersen, and H. Garcia-Molina. Questioning yahoo! answers. Evolution, 2008.
[14] F. M. Harper, D. Moy, and J. A. Konstan. Facts or friends?: distinguishing informational and conversational questions in social q&a sites. In CHI, pages 759≠768, 2009.
[15] A. Hassan, R. Jones, and K. L. Klinkner. Beyond dcg: user behavior as a predictor of a successful search. In WSDM, pages 221≠230, 2010.
[16] S. B. Huffman and M. Hochster. How well does result relevance predict session satisfaction? In SIGIR, 2007.
[17] R. Jones and K. L. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In CIKM, 2008.
[18] Q. Liu, E. Agichtein, G. Dror, E. Gabrilovich, Y. Maarek, D. Pelleg, and I. Szpektor. Predicting web searcher satisfaction with existing community-based answers. In SIGIR, pages 415≠424, 2011.
[19] M. R. Morris, J. Teevan, and K. Panovich. A comparison of information seeking using search engines and social networks. In ICWSM, 2010.
[20] M. R. Morris, J. Teevan, and K. Panovich. What do people ask their social networks, and why?: a survey study of status message q&a behavior. In CHI, 2010.
[21] B. Pang and R. Kumar. Search in the lost sense of "query": question formulation in web search queries and its temporal changes. In HLT, 2011.
[22] D. E. Rose and D. Levinson. Understanding user goals in web search. In WWW, pages 13≠19, 2004.
[23] S. Suzuki, S. Nakayama, and H. Joho. Formulating effective questions for community-based question answering. In SIGIR, pages 1261≠1262, 2011.
[24] R. W. White and S. T. Dumais. Characterizing and predicting search engine switching behavior. In CIKM, pages 87≠96. ACM, 2009.
[25] S. Zhao, H. Wang, C. Li, T. Liu, and Y. Guan. Automatically generating questions from queries for community-based question answering. In IJCNLP, pages 929≠937, 2011.
[26] Z. Zheng, X. Si, E. Y. Chang, and X. Zhu. K2q: Generating natural language questions from keywords with user refinements. In IJCNLP, 2011.

810

Content-based Retrieval for Heterogeneous Domains: Domain Adaptation by Relative Aggregation Points

Makoto P. Kato

Hiroaki Ohshima

Katsumi Tanaka

Kyoto University, Japan
{kato,ohshima,tanaka}@dl.kuis.kyoto-u.ac.jp

ABSTRACT
We introduce the problem of domain adaptation for content-based retrieval and propose a domain adaptation method based on relative aggregation points (RAPs). Content-based retrieval including image retrieval and spoken document retrieval enables a user to input examples as a query, and retrieves relevant data based on the similarity to the examples. However, input examples and relevant data can be dissimilar, especially when domains from which the user selects examples and from which the system retrieves data are different. In content-based geographic object retrieval, for example, suppose that a user who lives in Beijing visits Kyoto, Japan, and wants to search for relatively inexpensive restaurants serving popular local dishes by means of a content-based retrieval system. Since such restaurants in Beijing and Kyoto are dissimilar due to the difference in the average cost and areas' popular dishes, it is difficult to find relevant restaurants in Kyoto based on examples selected in Beijing. We propose a solution for this problem by assuming that RAPs in different domains correspond, which may be dissimilar but play the same role. A RAP is defined as the expectation of instances in a domain that are classified into a certain class, e.g. the most expensive restaurant, average restaurant, and restaurant serving the most popular dishes. Our proposed method constructs a new feature space based on RAPs estimated in each domain and bridges the domain difference for improving content-based retrieval in heterogeneous domains. To verify the effectiveness of our proposed method, we evaluated various methods with a test collection developed for content-based geographic object retrieval. Experimental results show that our proposed method achieved significant improvements over baseline methods. Moreover, we observed that the search performance of content-based retrieval in heterogeneous domains was significantly lower than that in homogeneous domains. This finding suggests that relevant data for the same search intent depend on the search context, that is, the location where the user searches and the domain from which the system retrieves data.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

General Terms
Algorithms, Experimentation
Keywords
Content-based retrieval, domain adaptation
1. INTRODUCTION
Content-based retrieval enables a user to input examples as a query, and retrieves relevant data based on the similarity to the examples. In multimedia retrieval, it is difficult to represent complex search intents by using keywords due to the difference in medium between input and output; thus, content-based retrieval has been successfully applied to a wide range of multimedia retrieval including image retrieval [17] and spoken document retrieval [9], as well as geographic object retrieval [15].
Content-based geographic object retrieval proposed by Kato et al. [15] provides a means by which a user can select geographic objects (e.g. restaurants) as a query in a domain s/he knows well (denoted by source domain D(S)) for retrieving objects from a domain s/he does not know well (denoted by target domain D(T )). This search method is helpful, especially when the searcher has little knowledge of the target domain, as it is usually difficult to make a concrete query without knowledge on the domain in which the user wants to search [2]. Suppose that a user who lives in Beijing visits Kyoto, Japan, and wants to search for restaurants, say, relatively inexpensive restaurants serving popular local dishes. If the user has little knowledge of Kyoto and no idea about the average cost and what the popular local dishes are there, it might be difficult for him/her to specify attributes (e.g. a category and price range of restaurants), or make a keyword query that represents his/her search intent. By using a content-based geographic object retrieval system, a user is able to make an example query without knowledge of the target domain by selecting familiar source domain examples that are relevant to his/her search intent.
A challenge in content-based geographic object retrieval is to find relevant instances from a target domain D(T ) based on examples selected from a heterogeneous source domain D(S). When the two domains are dissimilar, examples selected from D(S) are not always similar to relevant instances from D(T ). For example, assume that the user who visits Kyoto selects relatively inexpensive restaurants serving popular local dishes in Beijing. Such restaurants in Beijing and Kyoto are dissimilar since the average cost and popular local dishes are different in the two domains. Therefore, the content-based geographic object retrieval system has to find instances semantically similar to the given examples against superficial dissimilarity between instances from heterogeneous domains. Although similar problems have been recognized as domain

811

adaptation [20], domain adaptation for content-based retrieval has not been addressed in the literature. We discuss related work on domain adaptation in Section 2, and formalize and characterize domain adaptation for content-based retrieval in Section 3.
We propose a method for accurately predicting relevant instances in a domain based on selected examples in another heterogeneous domain. We assume that different domains include corresponding points that may not be the same in terms of their representation but play the same role in heterogeneous domains. For the Kyoto and Beijing example, both domains include the most expensive restaurant, the average restaurant, and a restaurant serving the most popular dishes there. These kinds of points are identified through aggregation based on relative and domain-independent operation; we call these points relative aggregation points (RAPs). We assume that RAPs in different domains correspond, and construct a new feature representation for each instance based on RAPs from the source and target domains. Several kinds of similarities are measured between RAPs and instances, each of which is used to represent instances in the new feature space. Incorporating features represented by RAPs bridges the gap between heterogeneous domains and enables us to capture the place of an instance in a domain.
Our experiment was designed to provide answers to two research questions on domain adaptation for content-based retrieval. The first research question is whether domain adaptation for contentbased retrieval is necessary. In other words, are relevant data different for the same search intent in different locations and datasets? We answered this research question based on a test collection developed for content-based geographic object retrieval, which includes relevant data for 100 queries (20 search intents in five domains). Our experimental result showed that mean average precision (MAP) and normalized discounted cumulative gain (nDCG) [12] in out-domain settings (D(S) = D(T )) were significantly lower than those in in-domain settings (D(S) = D(T )). This finding suggests that relevant data can vary in different search locations and datasets and has important implication for relevance judgment in information retrieval tasks. The second research question is whether domain adaptation based on RAPs can bridge the domain difference and significantly improve the search performance of content-based retrieval in heterogeneous domains. In our experiment based on the aforementioned test collection, our proposed method showed significant improvements on MAP and nDCG over baseline methods.
The remainder of this paper is structured as follows. We describe related work on domain adaptation in Section 2, and formalize and characterize domain adaptation for content-based retrieval in Section 3. In Section 4, we propose a method for domain adaptation based on RAPs. In Section 5, we describe the test collection developed for content-based geographic object retrieval and show experimental results in Section 6. We conclude with a summary of our findings and future work in Section 7.
2. RELATED WORK
Domain adaptation [20] has been addressed in several tasks such as part of speech tagging [5], text classification [4, 16, 27, 28], and learning to rank [7, 11, 26]. The motivation behind domain adaptation is that a model trained in a source domain D(S) does not work well in a heterogeneous target domain D(T ). The aforementioned content-based retrieval problem is also a domain adaptation problem since it is a process for modeling a search intent from examples selected in a domain, and inferring relevant instances in another by using the model. In this section, we describe related work on domain adaptation and clarify our contributions of this paper.

Blitzer et al. proposed structural correspondence learning (SCL) for domain adaptation [4, 5]. SCL first extracts pivot features, which behave in the same way in different domains. The correlation with the pivot features is then used to identify correspondences among features from different domains. Each instance is represented by the correspondence together with its own features. SCL has been successfully applied to part of speech tagging and sentiment classification, and is considered a state-of-the-art method for domain adaptation. Since our proposed method is similar to SCL in terms of incorporating corresponding points not corresponding features, we included SCL as a baseline and compared it with our proposed method.
Several domain adaptation methods for document classification tasks have been proposed recently. Dai et al. proposed a domain adaptation method based on co-clustering [10]. The method clusters words and documents simultaneously in a target domain, restricting word clusters by labels from a source domain and transferring the labels to documents in the target domain. Ling et al. incorporated spectral clustering for domain adaptation to construct a new feature space based on the clustering results [16]. Spectral clustering is applied to documents in a source and target domain under the source domain supervision, so that documents in the target domain are separated as much as possible. Their method then represents documents by their vector that encodes clusters to which a document belongs. Xue et al. proposed a method based on probabilistic latent semantic analysis (pLSA) for domain adaptation [28]. They assumed that different domains include common latent topics, applied pLSA to documents under the source domain supervision, and classified documents from a target domain using common topics obtained through the pLSA. Wang et al. tackled a problem of cross-language document classification [27]. They argued that cross-language document classification is a challenge because of cultural discrepancies and translation ambiguities. It is an interesting and similar motivation to our geographic object retrieval example in which the cultural discrepancy may prevent training an accurate model.
In addition to document classification tasks, some studies have focused on domain adaptation for learning to rank tasks. Cai et al. proposed active query selection for ranking adaptation, which seeks instances to be labeled in the target domain for an effective training [7]. Wang et al. tackled the problem of learning to rank, assuming that there is a common latent space in different domains, and mapped instances onto the space preserving the label order [26]. Gao et al. proposed a method based on re-weighting labeled instances for learning to rank in heterogeneous domains [11]. The main idea of their approach is that the label information of source domain instances similar to ones from a target domain is more helpful in predicting the label of target domain instances than dissimilar source domain instances. Although we also focus on an information retrieval task, our problem described in the following section uses pairs of an instance and binary label; thus, we cannot simply adapt their methods to our problem and compare them with our proposed method.
Nakajima and Tanaka proposed a relative query processing method [18]. A relative query consists of a set of instances and an instance in the set. Their relative query processing method represents an instance x in a set X as a vector from the centroid of X to x. The differences between domain adaptation for contentbased retrieval and the relative query processing method are the following two aspects: (i) a content-based retrieval system returns ranked instances, while the relative query aims to find only a relevant instance, and (ii) domain adaptation for content-based retrieval addresses a problem that the same features may have dif-

812

ferent meanings in different domains, while the relative query processing method does not and only addresses the relativity of the feature value. Moreover, we included their method as a baseline in our experiment and showed the difference in effectiveness between our proposed method and the relative query processing method.
We assume that different domains include points that play the same role, like the assumption of common features in most of the previous work. However, we would like to emphasize a fundamental difference; we do not assume that common features in different domains correlate with labels in the same way. The reason for this is that even the same features may indicate different meanings in different domains. For example, sushi restaurants in Beijing and Kyoto that are exactly the same may be relevant in Kyoto, but irrelevant in Beijing for a user who wants a restaurant serving local dishes. Moreover, a test collection we developed for the contentbased geographic object retrieval is different from commonly used test collections such as 20 Newsgroups1 in some aspects. Our test collection includes different types of features: continuous, such as the cost and distance from the nearest station, discrete, such as category information, and high-dimensional, such as the description of a restaurant. The label distribution is usually balanced in the sentiment and category classification tasks addressed in previous work, while the amount of relevant instances to a search intent is relatively small in our test collection and is different in different search intents and domains. The unbalanced label distribution might impair the performance of previously proposed methods, as revealed in our experiment.
3. DOMAIN ADAPTATION
FOR CONTENT-BASED RETRIEVAL
In this section, we define the problem of domain adaptation for content-based retrieval and discuss domain difference.
A domain consists of a feature space X and a marginal distribution P (X) [20]; thus, we represent a domain as D = (X , P (X)). A label space is denoted by Y . Instances X  X are sampled following P (X), and a label y  Y is assigned to each instance x  X. Label-instance pairs are denoted by D = {(x1, y1), (x2, y2), . . .}.
We set two domains in content-based retrieval: a source domain D(S) = (X (S), P (X(S))), from which instances to be selected as a query are sampled, and a target domain D(T ) = (X (T ), P (X(T ))), from which instances to be retrieved are sampled. From instances X(S) sampled from the source domain D(S), the user can select a subset Q  X(S) that consists of instances relevant to his/her search intent. The example selection can be interpreted as a labeling process on the instances X(S), and label-instance pairs are denoted by D(S) = {(x(S), +1) | x(S)  Q}  {(x(S), -1) | x(S)  (X(S) - Q)}, where a label space Y (S) comprises -1 (nonselected examples) and +1 (selected examples). Therefore, a query in content-based retrieval is represented as the set of labelinstance pairs D(S). On another front, instances sampled from the target domain D(T ) are denoted by X(T ), and label-instance pairs are denoted by D(T ), in which a label represents binary relevance or graded relevance for a search intent; thus, a label space Y (T ) is a set of integer values. A content-based retrieval system ranks the instances X(T ) based on a ranking function f : X(T )  R trained by label-instance pairs D(S) and target instances X(T ), where a total order  on the instances X(T ) is given by x(iT )  x(jT )  f (x(iT ))  f (x(jT )). To evaluate content-based retrieval systems,
1http://people.csail.mit.edu/jrennie/ 20Newsgroups/

an arbitrary information retrieval metric is applied to a totally ordered instance set (X(T ), ).
For example of content-based geographic object retrieval, a user can select a region (e.g. Beijing) as the source domain D(S) and another (e.g. Kyoto) as the target domain D(T ). In this case, their feature spaces X (S) and X (T ) are the same: a set of all possible restaurant instances. A set of restaurant instances in D(S) is denoted by X(S) = {x(1S), x(2S), x(3S)}, and we assume that the set is obtained by sampling instances following the marginal distribution P (X(S)). The user can select a subset Q = {x(1S), x(2S)} of X(S), which can be represented as D(S) = {(x(1S), +1), (x(2S), +1), (x(3S), -1)}. A content-based geographic object retrieval system receives D(S) as a query, and ranks a set of restaurant instances X(T ) = {x(1T ), x(2T ), x(3T )} sampled from D(T ).
The most important difference from domain adaptation tasks addressed in previous work is that no label information in the target domain is available during the training process in content-based retrieval. Relevant instances in the target domain depend on the user's search intents; thus, it is not realistic to prepare labels for all possible search intents. Note that in many cases all the instances from the target domain are not available for training in ad-hoc information retrieval due to the amount of data. Instead of using all the data, one can sample some data from the target domain or re-rank top ranked instances using them as target instances. For contentbased geographic object retrieval, it is feasible to use all the target instances because the size of instances in a region the user focuses on is generally small (1,000 at most).
The difference between domains is characterized by the difference in feature spaces and marginal distributions. In content-based geographic object retrieval, for example, features spaces in different domains are the same, i.e. X (S) = X (T ). A restaurant instance is represented as three elements in any domain: the price range, category, and description of a restaurant in natural language. On the other hand, marginal distributions in different domains can be different, i.e. P (X(S)) = P (X(T )). For example, there are many expensive restaurants in Tokyo, many Japanese-style restaurants in Kyoto, and many restaurants serving spicy dishes in Beijing. Recent studies proposed methods for quantifying the domain difference by using A-distance for estimating the learning bound for domain adaptation [3, 21]. In our experiment, we demonstrated the domain difference measured by A-distance dA for ensuring that domains we used are different from each other. We used the inequation D(S) = D(T ) if and only if the A-distance dA(D(S), D(T )) is greater than a threshold .
In summary, domain adaptation for content-based retrieval is defined as follows: given label-instance pairs D(S) from a source domain D(S) and instances X(T ) from a target domain D(T ) such that D(S) = D(T ), domain adaptation for content-based retrieval is the problem of predicting a ranking function f that maximizes an information retrieval metric of a totally ordered instance set (X(T ), ).
4. DOMAIN ADAPTATION BASED ON
RELATIVE AGGREGATION POINTS
We propose a domain adaptation method for content-based retrieval in this section. Our assumption is that different domains include corresponding points that may not be the same in terms of their representation, but play the same role in heterogeneous domains. We first introduce RAPs that we believe to be such points and then propose a method for constructing a new feature representation for each instance based on RAPs. We also introduce several

813

Source Domain
Category
3 2 1

Target Domain
Category
2 3
1

Cost

Cost

Figure 1: RAPs and construction of a new feature representation. Black stars are RAPs, and white circles are instances. Stars that have the same number are corresponding.

examples of the RAP for geographic object retrieval at the end of this section. Although this domain adaptation method is a framework and can be applied to any type of instances, we use an example of a restaurant instances for simplicity. A simplified restaurant instance x has two attributes: the cost xb (continuous) and category xc (a set of category values such as Japanese-style and sushi), and can be represented as a d-dimensional vector x if needed.

4.1 Feature Representation based on Relative Aggregation Points

A RAP is the expectation of instances in a domain that are classi-

fied into a certain class. Let us consider the maximum cost point in

a domain D = (X , P (X)), which is the most expensive restaurant. Given a set of instances X sampled from the domain D, we can eas-

ily identify the most expensive restaurants within X, and represent

these restaurants as MAX(X) = argmaxxX xb, where xb is the cost of x. However, a set of instances X might not be representa-

tives sampled from the domain D. For example, the set of instances

X may include an extremely expensive out-lier despite the low av-

erage cost of the domain. Therefore, we estimate the expectation of

the most expensive instances in all possible instance sets sampled

from a domain. Given a domain D and a subset function  : 2X  2X (2X is a

collection of subsets of X ), the RAP a is defined as follows: Z

a =

xP (x|)dx,

(1)

xX

where x is the vector representation of an instance x, and P (x|) is the probability that x is sampled from a subset (X) of any instance set X sampled from the domain D. The probability P (x|) is given by marginalizing out a set of instances X:
Z

P (x|) =

P (x|, X)P (X)dX,

(2)

X 2X

where P (x|, X) is the probability that x is sampled from a subset (X). Intuitively, the subset function  indicates the deterministic membership of an instance, while probability P (x|) indicates probabilistic membership, which is more stable than the deterministic one.
RAPs are relative because they can vary in different domains for the same subset function . Figure 1 illustrates RAPs as black stars in two domains, where the feature space includes two dimensions: cost and category (e.g. the Japaneseness of a restaurant). Black star 1 is the minimum cost point, star 2 is the average cost point, and star 3 is the maximum cost point, all of which are different between the two domains. We assume that RAPs in different domains correspond and features of RAPs also correspond. In Figure 1, we regard costs of the minimum cost points as the same, i.e. b(1S) = b(1T ), and

categories of the maximum cost points in different domains as the same, i.e. c(3S) = c(3T ). For the example of Kyoto and Beijing, the maximum cost point in Kyoto and that in Beijing are different:
a restaurant serving Kyoto cuisine for 10,000 JPY and one serv-
ing Beijing duck for 5,000 JPY. By using the maximum cost point
in Kyoto and Beijing, we assume that 10,000 JPY corresponds to
5,000 JPY and Kyoto cuisine corresponds to Beijing duck. Kyoto
cuisine in Kyoto is considered the same as Beijing duck in Bei-
jing since those dishes are delicacies in each domain. Based on the
above assumption, we can model each instance by the similarity
to RAPs, where instances in different domains are similar if their
features are similar to ones of RAPs.
We give a concrete feature representation based on RAPs. Let H = {h1, h2, . . . hm} denote a set of similarity functions between two instances, and A = {a1, a2, . . . an} denote a set of RAPs in a domain D. For example, H includes the similarity in terms of the cost and that in terms of the category. By using a function  : X  Rmn, we transform an instance x  X into a mndimensional vector (x). The function  is defined as follows:

(x) = (1(x), 2(x), . . . , n(x))

(3)

where i(x) = (h1(x, ai), h2(x, ai), . . . , hm(x, ai)) and each dimension corresponds to the similarity to a RAP measured by a similarity function. Combining the vector representation x of an instance x and the feature representation based on RAPs (x), we obtain an augmented feature representation (x, (x)), where  is a parameter that determines the importance of the feature representation based on RAPs. Note that the vector x and mn-dimensional vector (x) are normalized by their L2-norm in practice.
RAPs are estimated separately in the source and target domains. Letting (S) denote a function based on RAPs estimated in the source domain and (T ) denote one in the target domain, a source instance x(S)  X(S) is converted into (x(S), (S)(x(S))), while x(T )  X(T ) is converted into (x(T ), (T )(x(T ))). Thus, our method enables us to take into account ordinary similarity (i.e. the similarity between x(S) and x(T )), as well as the similarity based on RAPs (i.e. the similarity between (S)(x(S)) and (T )(x(T )).
The motivating example, relatively inexpensive restaurants serv-
ing popular local dishes, is processed by using RAPs in the follow-
ing way. Relatively inexpensive restaurants should be similar to
the minimum cost point in terms of the cost, and restaurants serv-
ing local dishes may be similar to restaurants of a category that
frequently appears in a domain. It is also possible that expensive
or average restaurants serve popular local dishes, and the similar-
ity to such restaurants in terms of the category is helpful to capture
what the popular local dishes are. Therefore, we can find relevant
restaurants in another heterogeneous domains by calculating the
similarity between similarities to those RAPs.

4.2 Examples of Relative Aggregation Point
Our proposed method is a framework, and effective RAPs highly depend on the task. We present some examples of the RAPs that are effective for the content-based geographic object retrieval. Note that methods for estimating RAPs from finite samples are not essential for the purpose of this paper; thus, we describe these methods in Appendix A.

AVG. The most normal restaurant may be considered equivalent
in different domains. The average (AVG) RAP is the average of instances in a domain. The subset function AVG is defined as AVG(X) = {x|x  X}; thus, the probability P (x|AVG) is equal to the marginal distributionR P (x). Therefore, the AVG RAP is defined as follows: aAVG = xX xP (x)dx.

814

MAX/MIN. The maximum (MAX) (and minimum (MIN)) cost
may correspond in different domains as mentioned earlier. The MAX RAP is the expectation of instances that has the maximum cost in a domain. The subset function MAX is defined as MAX(X) = argmaxxX xb, where xb is the cost of an inRstance x. The probability P (x|MAX) is given by P (x|MAX) = bR P (x|b)P (b|MAX)db, using the probability P (b|MAX) of cost b, which is the maximum cost in any instance set, and the probability of x conditionedR by b. TRherefore, the MAX RAP is defined as follows: aMAX = xX x bR P (x|b)P (b|MAX)dbdx.
The MIN RAP is obtained in the same way as the MAX RAP, where the subset function MIN is defined as MIN(X) = argminxX xb.
FREQ. Relatively frequent (FREQ) categories may correspond
in different domains. For example, Japanese-style in Kyoto and Chinese-style in Beijing represent the same meaning, i.e. a popular category in a domain. The subset function FREQ is defined as FREQ(X) = {x|x  X  (c  xc)freqX (c) > freqA(c)}, where freqX (c) is the frequency of category value c in a set of instances X, and A is a set of instances sampled from the entire domain (or all the domains we have). This subset function FREQ returns instances with categories that are more frequent in X than those in A. A FREQ RAP is obRtained aRs follows in a similar manner to MAX/MIN: aFREQ = xX x CC P (x|C)P (C|FREQ)dCdx, where C is a collection of subsets of category values.
CLS. RAPs can be automatically defined through clustering. The
subset function CLS is defined as CLS(X) = {x|x  X  x  S}, where a set of instances S is obtained through clustering applied to instances in both the source and target domains. Since P (x|CLS) is independent from a way oRf samplings, the CLS RAP is simply defined as follows: aCLS = xX xP (x|S)dx.
5. TEST COLLECTION
In this section we describe a method for developing our test collection for content-based geographic object retrieval2, and show the statistics of the test collection including the degree of differences between domains.
5.1 Instances
Our test collection includes 46,945 restaurant instances crawled from a Japanese restaurant Web site GourNavi3, which is one of the biggest sites that provide restaurant information in Japan. We opted to target at restaurant data for several reasons. Restaurant search is usually conducted with an ambiguous search intent that cannot be easily expressed by using keywords. Moreover, the difference between domains (or regions) is originally derived from the cultural difference and inherits its properties; thus, the difference is generally large and difficult to understand. Therefore, restaurant search is one of the most attractive applications of content-based geographic object retrieval.
Restaurant instances in our test collection include several attributes such as the cost, category, name, genre, and introduction of a restaurant. The name (e.g. Fujiyama), genre (e.g. traditional Japanese sushi bar), and introduction (a detailed description of a restaurant written in natural language, which comprises about 200
2The dataset is available at http://www.mpkato.net/ datasets/. 3http://www.gnavi.co.jp/

Table 1: The number of instances (#), the average cost (Avg.),

and relatively frequent category values in each domain.

Domain # Avg. (JPY) Frequent category values

Kyoto 510

4,530 Japanese-style

Tokyo 511

6,410 Western-style

Sapporo 514

3,490 Seafood

Fukuoka 500

3,760 Nabe, Seafood

Nagoya 529

3,590 Izakaya

words) are text, which were processed by MeCab4, a Japanese morphological analyzer. Only nouns and adjectives were used to construct a text vector xt whose elements represent the term frequency in those text attributes. Feature selection was carried out excluding terms that occured less than three times in our test collection. The category attribute is a set of category values, e.g. {bar, sushi, Japanese-style}. We treated the category attribute the same as text attributes (i.e. bag-of-words), and represented a category vector as xc. Although category values are hierarchically structured in the GourNavi Web service (e.g. top categories include Japanese-style and Western-style, and sub categories under the Japanese-style category include sushi and tempura), we did not use the hierarchical structure for simplicity. The text and category attributes were separately processed and were weighted by the tf-idf method. To be more specific, the i-th element of the text vector xt for an instance x is tf(ti) log (N/df(ti)), where tf(ti) is the frequency of a term ti in the instance x, df(ti) is the number of instances that include the term ti, and N is the number of instances in our test collection. The category vector xc is weighted in the same manner. To process the cost attribute together with text and category attributes, we discretized the cost and represented it as a probability distribution on discrete values. The cost b was assumed to follow a normal distribution N (z; b, ), and was discretized by the probability of each z in a set of real values Z. The cost b was represented by a |Z|dimensional vector xb whose element corresponds to each element z  Z and is N (z; b, ). In our test collection, Z includes every 500 from -5,000 to 40,000, and  = 1,000. This discretization is not essential but is convenient for computation of the similarity between restaurant instances. Through the above processes, an instance was represented as a d-dimensional vector x = (xt, xc, xb), where d =15,670 in our test collection.
5.2 Domains
We chose five major cities (Kyoto, Tokyo, Sapporo, Fukuoka, and Nagoya) in Japan. We selected a region from each city that contain about 500 restaurant instances, and used the five regions as domains in our test collection. Table 1 shows the number of instances, the average cost, and relatively frequent category values in each domain. The average cost in Tokyo is especially high, that in Kyoto follows, and there is no significant difference across other three domains. There are many Japanese-style restaurants in Kyoto, many Western-style ones in Tokyo, and many izakaya ones in Nagoya. The number of seafood restaurants is relatively large in Sapporo and Fukuoka, and that of nabe restaurants is slightly large in Fukuoka. Those differences in the average cost and category indicate the difference between marginal distributions, and may affect the performance of content-based geographic object retrieval.
To measure the difference between the five domains and to ensure that domains in our test collection are significantly different from each other, the A-distance dA was calculated between all the pairs of domains in our test collection. The A-distance was introduced by Ben-David et al. [3], and was defined as the upper bound of the difference between the two marginal distributions P (X(S))
4http://mecab.sourceforge.net/

815

and P (X(T )). To compute the A-distance from finite samples of the source and target domains, we approximated the distance by a method introduced by Rai et al. [21].
We calculated the A-distance of all the pairs of domains in our test collection, which ranges from 0.537 (Fukuoka-Nagoya) to 0.745 (Kyoto-Tokyo). Rai et al. [21] reported the A-distance across eight types of user reviews in the multi-domain sentiment dataset5 , and the A-distance ranges from 0.0459 (Kitchen-Apparel) to 0.762 (DVD-Book). Comparing the A-distance in our test collection and the multi-domain sentiment dataset, we found that our domains are as different from each other as domains used in some previous work [4, 21], even though all the instances are restaurants and all the domains are domestic cities. Therefore, we will use the inequation D(S) = D(T ) for any pair of the domains hereinafter since it was revealed that any pair of the domains is significantly different, i.e. dA(D(S), D(T )) > , as mentioned in Section 3.
5.3 Search Intents
To select relevant instances in our test collection, which were used as a query in the source domain and as the ground truth in the target domain, search intents had to be documented for being understood by subjects. To this end, we manually gathered questions from the restaurant category in Yahoo! Chiebukuro 6 (Japanese Yahoo! Answers), where the asker asks restaurants that are relevant to his intents. We opted to use Yahoo! Chiebukuro as the resource of search intents because complex and realistic intents are posted there. A hundred of questions were extracted as search intents from Yahoo! Chiebukuro, and were classified into two types in terms of domain-dependency for characterizing each search intent. Domain dependency is the degree of variability of a search intent in different domain, and was subjectively evaluated by two assessors we hired only for this task. We asked them to score the extracted search intent on a scale from one (independent from the domain) to five (dependent on the domain). As their agreement on 100 questions was low: 0.233 in terms of quadratic weighted kappa [24], we separated the labels into domain-dependent (a score is three or more) and domain-independent (a score is less than three), and excluded all the questions on which the two assessors disagreed. As a result, 57 questions remained, of which 20 (35.1%) questions were domain-dependent and 37 (64.9%) questions were domain-independent. We selected ten questions each from the sets of domain-dependent and domain-independent questions, and included them as 20 search intents in our test collection.
An example of domain-dependent search intents is "I'm going to <v> for business trip with my boss. Do you have any recommendations for dinner? It would be great if you would tell us restaurants for from 3,000 to 5,000 JPY that provide <v>-specific dishes with Japanese sake. We are 30's and 40's. Either meat or fish dish is OK." On the other hand, an example of domain-independent search intents is "Please show me Western restaurants in <v>. I want a Italian or French restaurant that has a lunch course for 5,000 JPY without wine," where the variable <v> is replaced with the name of a domain, or a region (e.g. Kyoto).
5.4 Procedure of Relevance Judgment
We recruited 1,000 subjects living in five regions through an online questionnaire service, and asked them to select instances relevant to each search intent in their home area. The five regions are Kyoto city in Kyoto, 23 special wards in Tokyo, Sapporo city in Hokkaido, Fukuoka city in Fukuoka, and Nagoya city in Aichi,
5http://www.cs.jhu.edu/~mdredze/datasets/ sentiment/ 6http://chiebukuro.yahoo.co.jp/

which correspond to the five domains, Kyoto, Tokyo, Sapporo, Fukuoka, and Nagoya. We asked the subjects to select relevant instances in a domain corresponding to their home area because it is difficult to judge the relevance without domain knowledge. Note that the recruitment was controlled so that the number of subjects is uniform for each sex, age, and region combination, i.e. 20 subjects were sampled from each (male and female), (20's, 30's, 40's, 50's, and 60's), and (Kyoto, Tokyo, Sapporo, Fukuoka, and Nagoya) combination. The online questionnaire service we used is hosted by a domestic company, which delivered a mail to users and asked to visit our Web system. Subjects were rewarded through the company if they finished the whole task we instructed.
The task we asked the subjects to work on consists of two steps. In the first step, the subjects were asked to examine 30 restaurant instances randomly sampled from a domain for subjects' better understanding of the domain. To make the subjects carefully examine each restaurant instance, we asked subjects to find a couple of restaurants they wanted to visit. In the second step, two search intents were assigned and presented in random order to each subject. The subjects were asked to select as many restaurants relevant to a presented search intent as possible. The subjects were allowed to use a simple search engine, and can search for restaurants by specifying the upper and/or lower limits of the cost, the top category (e.g. Japanese-style, Western-style, and Chinese-style), and sub category (e.g. sushi, Italian, and French). The search engine returns a list of restaurant instances that satisfy all the specified conditions, and the search result ranking was randomized to reduce the ranking bias. Relevance judgments for each search intent in each domain were conducted by 20 subjects, where two subjects were selected from each sex and age group.
5.5 Post-Processing of Relevance Judgments
As Alonso and Baeza-Yates indicated [1], it is important to keep the quality of relevance judgments when the assessment is conducted through an online service, or crowd sourcing. We ensured the quality of our test collection by conducting the following two steps. First, subjects who finished a task within five minutes were filtered out, as it was supposed to take more than five minutes in a preliminary system test. Second, we manually assessed all the relevant instances for each search intent in each domain, and removed subjects who made obviously wrong relevance judgments. For example, we removed a subject who selected a restaurant for 1,000 JPY as relevant when the subject was asked to find restaurants for 5,000 JPY. After the removal of low-quality subjects, there remained 579 subjects and 6,082 relevance judgments. On average, 11.6 subjects worked on each search intent in each domain, and made 121 relevance judgments. Finally, we merged duplicate relevance judgments and generate graded relevance by regarding the multiplicity as the grade. The average number of relevant instances per domain and search intent was 42.1, and the standard deviation was 33.3.
Our test collection contains many subjective search intents and needed as many subjects as possible for avoiding the individual bias, whereas it was difficult to ask individual to evaluate all the restaurant instances due to the cost limitation. To gain the coverage of relevance judgments, we assigned 20 subjects to each search intent and randomized the ranking result returned by the simple search system. However, the relevance judgment we conducted is not always complete since it is not guaranteed that all the restaurant instances in a domain were examined by the subjects. As Buckley and Voorhees's study on retrieval evaluation with incomplete information suggests [6], traditional evaluation metrics such as MAP

816

and precision at ten are not robust to substantially incomplete relevance judgments.
To assess the completeness of our relevance judgments, we used a method of estimating the population size by multiple census proposed by Schumacher and Eschmeyer [23]. Their method conducts multiple samplings, which correspond to individual's relevance judgments in our case, and estimates the population size, which corresponds to the number of relevant instances in our case. Having estimated the number of relevant instances, we can estimate the fraction of judged relevant instances to all the relevant instances. In our test collection, 62.9% of relevant instances were estimated to be actually judged by the subject. According to the Buckley and Voorhees's work [6], the moderate completeness of our test collection possibly affects the system ranking based on the evaluation metric: the Kendall correlation between the system ranking based on MAP with incomplete relevance judgments and that with complete relevance judgments was 0.9 on average in three test collections from the Text REtrieval Conference (TREC). Although we cannot argue that our test collection is complete and is able to accurately estimate the effectiveness of methods, the effect of the moderate completeness of our test collection is supposed to be small. Moreover, we used an evaluation metric for graded relevance, where relevant instances with a low grade make less impact on the evaluation result than ones with a high grade. As the maximum grade per search intent and region is 4.34 on average in our test collection, relevant instances that were not judged by any subjects are not seriously affect the evaluation metric compared to the binary relevance case.
6. EXPERIMENT
We answer the following two research questions through our experiment: whether domain adaptation for content-based retrieval is necessary, and whether domain adaptation based on RAPs can significantly improve the search performance of content-based retrieval in heterogeneous domains.
6.1 Experimental Settings
We selected a domain as source D(S) and another as target D(T ) from our test collection. The input in our problem was a set of pairs of a binary label and instance D(S) from the source domain D(S) as mentioned in Section 3; thus, a label +1 was assigned to relevant (selected) source domain instances, while -1 was assigned to irrelevant (nonselected) source domain instances, where the grade for relevance was ignored since the user does not grade but just selects instances as a query in content-based retrieval. A content-based retrieval method receives D(S) and target instances X(T ) and predicts an optimal ranking function f to rank instances X(T ) from the target domain D(T ). We used both MAP and nDCG [12] in our experiment for evaluating the results in terms of binary relevance and graded relevance. For MAP, relevant instances were defined as ones that were judged as relevant by at least one subject. For nDCG, we set the rank threshold to 10 because it is the common number of search results shown in a search engine result page.
Simple baselines are methods without any knowledge of the target domain: nearest neighbor search (NNS), one-class SVM (OSVM), and SVM. NNS ranks target instances based on the similarity to the centroid of selected instances in D(S). OSVM is a method incorporating the one-class SVM [8, 22], which trains a model for binary classification using only selected instances in D(S), and the ranking function f is defined as f (x) = wT x - . Note that x is a vector representation of an instance x, and w and  are parameters that determine the decision boundary of the one-class SVM. The NNS and OSVM methods do not use nons-

elected examples, which are considered typical content-based retrieval methods. SVM is a method based on a linear support vector machine using label-instance pairs D(S), which trains a model for binary classification, and the ranking function f is defined as f (x) = wT x + b, where w and b are parameters that determine the decision boundary of the SVM.
Baseline domain adaptation methods are transductive support vector machine (TSVM), structural correspondence learning (SCL), and relative cluster mapping (RCM), all of which use knowledge of both the source and target domains. TSVM proposed by Joachims [13] has been used as a baseline for domain adaptation tasks [16, 28]. The ranking function of this baseline is predicted in the same way as the SVM method. SCL was also compared with our proposed method, which is a state-of-the-art method for domain adaptation [4, 5]. SCL constructs a new feature space and represents each instance with additional features generated based on pivots (features that behave similarly in the source and target domains). We applied the SVM method to augmented vectors of instances for predicting the ranking function f . RCM was selected as a baseline since the problem definition and motivation are similar to ours [18]. RCM represents an instance by the vector from the centroid of a set of instances to a vector representing the instance itself. We then applied the SVM in the same manner as SCL.
Our proposed RAP-based domain adaptation method includes a parameter , and has variants that use different RAPs. We set  = 1 (default) and included all the RAPs mentioned in Section 4.2 for the comparison. The best setting for those factors were explored after comparison. A set of similarity functions involves the cosine of the cost xb, that of the category xc, that of the text xt, and that of x. The CLS RAPs were obtained by using CLUTO7 with default parameters. The number of clusters was set to 30. The training was conducted using the SVM in the same manner as the baseline domain adaptation methods.
The SVM and TSVM methods were implemented using SVMlight8, and the OSVM was implemented using LIBSVM9. All parameters were set to default by the software. SCL was implemented according to the paper authored by Blitzer et al. [4, 5]10. For SCL, we selected features that occur more than five times in the source and target domains and showed the highest mutual information with respect to labels in the source domain. The number of dimensions h was set to 50 according to a study using SCL for comparison [19], and the number of pivots was set to 100, which is smaller than the one used in the comparison study, due to the lack of frequently occurring features in our test collection.
6.2 Is Domain Adaptation Necessary for Content-based Retrieval?
We first present an answer to the first research question: is domain adaptation for content-based retrieval necessary? The SVM method, which is a baseline that does not use any domain adaptation technique, was applied to all the search intents. In an indomain setting (D(S) = D(S)), we used a method similar to k-fold cross validation, where instances in a domain are split into k bins, of which k - 1 bins are used as the source and the rest are used as the target. In our study, we set k to 5 and recorded the average
7http://glaros.dtc.umn.edu/gkhome/views/ cluto/ 8http://svmlight.joachims.org/ 9http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 10To test our implementation of SCL, we applied it to a dataset Blitzer et al. developed [4], which is the one we referred to in Section 5.2. The average accuracy was 0.774, which is close to the average accuracy Blitzer et al. reported.

817

Source

Table 2: The average nDCG@10 over all the search intents in each combination of the source and target domains. The bold font indicates in-domain settings, while the others are outdomain settings.
Target Kyoto Tokyo Sapporo Fukuoka Nagoya Kyoto 0.620 0.463 0.520 0.511 0.431 Tokyo 0.460 0.596 0.449 0.501 0.389 Sapporo 0.492 0.432 0.638 0.524 0.506 Fukuoka 0.505 0.472 0.510 0.588 0.477 Nagoya 0.476 0.466 0.538 0.527 0.599
nDCG@10 over k runs. For an out-domain setting (D(S) = D(T )), we applied a similar method to the in-domain setting for comparison. A domain was used as the source and another was used as the target, where instances in the source and the target domains were split into k bins, of which k - 1 bins in the source domain were used as input and a bin in the target domain was used as instances to be retrieved. The query in our experiment was different for the combination of the source and search intent. The number of unique queries was 100 (5 domains ◊ 20 intents). The target domain was selected from the five domains; thus, we conducted 500 runs (5 domains ◊ 100 queries).
Table 2 lists the average nDCG@10 over all the search intents in each pair of the source and target domains. It is clear that the average nDCG@10 in out-domain settings (D(S) = D(T )) was lower than that in in-domain settings (D(S) = D(S)). There was a negative correlation between the average nDCG@10 and the Adistance (Pearson's coefficient r = -0.678, p < 0.05). The negative correlation indicates that the more different domains are, the more difficult it is to achieve high search performance. The average nDCG@10 for the in-domain and out-domain settings are 0.608 and 0.482, respectively. With a Welch's t-test, we found a significant difference between the in-domain and out-domain settings, t(179) = 5.89, p < 0.00111. Moreover, the effect size of the domain setting was 0.588 in terms of Cohen's d, which is considered medium.
The significant difference in search performance between the indomain and out-domain settings suggests that domain adaptation for content-based retrieval is necessary. This result also implies that relevant data can vary even for the same search intent when the search context is different. The difference in search context in our case consists of the difference between datasets from which the system retrieves data, the difference between search locations, and the difference between user's home areas. Recently, studies have suggested that relevant data can vary for different users [25] and different search devices [14]. In addition to such hypotheses, our results pose new hypotheses stating that relevant data can vary for different datasets and/or locations where the user searches, which are still open questions since we did not separate the two effects.
6.3 How Effective Is Domain Adaptation based on Relative Aggregation Points?
We compared our RAP-based domain adaptation method with baselines, NNS, OSVM, SVM, TSVM, SCL, and RCM, to demonstrate its effectiveness. We conducted 400 runs (4 out-domains ◊ 100 queries) in this experiment. Tables 3 and 4 list the average MAP and nDCG@10 for each search intent in the outdomain setting, respectively. "Transductive" methods use knowl-
11We also tested the difference between the in-domain and outdomain settings in terms of MAP (0.609 vs. 0.479) and found a significant difference, t(157) = 5.35, p < 0.001, Cohen's d = 0.585.

Table 3: The average MAP for each search intent in the out-

domain setting.

Inductive

Transductive

Intent NNS OSVM SVM TSVM SCL RCM RAP

1

0.539 0.546 0.490 0.417 0.482 0.490 0.490

2

0.387 0.452 0.522 0.439 0.512 0.521 0.574

Domain-dependent

3

0.347 0.319 0.317 0.344 0.321 0.317 0.320

4

0.409 0.350 0.672 0.558 0.688 0.672 0.668

5

0.488 0.458 0.512 0.506 0.515 0.513 0.522

6

0.198 0.227 0.327 0.329 0.324 0.327 0.333

7

0.078 0.083 0.155 0.150 0.153 0.155 0.144

8

0.192 0.219 0.254 0.206 0.251 0.254 0.260

9

0.488 0.491 0.548 0.561 0.548 0.548 0.570

10 0.261 0.293 0.335 0.305 0.329 0.335 0.337

11 0.442 0.401 0.630 0.581 0.631 0.630 0.654

Domain-independent

12 0.180 0.177 0.280 0.233 0.284 0.280 0.284

13 0.586 0.571 0.645 0.627 0.638 0.645 0.640

14 0.115 0.114 0.161 0.109 0.161 0.161 0.156

15 0.354 0.293 0.545 0.506 0.541 0.545 0.538

16 0.416 0.399 0.702 0.609 0.686 0.701 0.705

17 0.270 0.270 0.379 0.317 0.384 0.379 0.385

18 0.256 0.226 0.401 0.312 0.398 0.401 0.399

19 0.586 0.595 0.641 0.532 0.646 0.641 0.645

20 0.288 0.286 0.238 0.203 0.236 0.238 0.239

Total

0.344 0.338 0.438 0.392 0.436 0.438 0.443

Table 4: The average nDCG@10 for each search intent in the

out-domain setting.

Inductive

Transductive

Intent NNS OSVM SVM TSVM SCL RCM RAP

1

0.191 0.192 0.175 0.176 0.171 0.177 0.173

2

0.173 0.249 0.406 0.277 0.415 0.406 0.420

Domain-dependent

3

0.149 0.123 0.084 0.151 0.103 0.084 0.086

4

0.145 0.087 0.389 0.315 0.367 0.386 0.395

5

0.182 0.167 0.242 0.250 0.260 0.242 0.283

6

0.118 0.162 0.267 0.213 0.261 0.267 0.268

7

0.024 0.046 0.174 0.121 0.170 0.174 0.168

8

0.213 0.251 0.303 0.219 0.311 0.303 0.309

9

0.199 0.184 0.254 0.223 0.258 0.254 0.282

10 0.130 0.178 0.277 0.226 0.269 0.278 0.266

11 0.208 0.172 0.589 0.582 0.617 0.589 0.631

Domain-independent

12 0.148 0.121 0.288 0.162 0.296 0.288 0.295

13 0.201 0.198 0.295 0.262 0.283 0.295 0.325

14 0.068 0.056 0.184 0.038 0.194 0.184 0.183

15 0.285 0.230 0.483 0.513 0.509 0.483 0.492

16 0.164 0.136 0.487 0.375 0.496 0.486 0.486

17 0.107 0.135 0.418 0.230 0.475 0.418 0.440

18 0.222 0.174 0.368 0.206 0.356 0.368 0.323

19 0.305 0.345 0.492 0.378 0.493 0.492 0.540

20 0.118 0.109 0.127 0.081 0.128 0.120 0.121

Total

0.168 0.166 0.315 0.250 0.322 0.315 0.324

edge of the target domain, while "Inductive" methods do not. The bold font indicates the highest score per search intent. Oneway ANOVA showed a significant effect of methods on MAP and nDCG, F (6, 2793) = 16.1, p < 0.001 and F (6, 2793) = 40.0, p < 0.001, respectively. With a paired t-test12, we found significant differences between baselines and the RAP-based domain adaptation method at  = 0.05, which are indicated by a dagger  at the "Total" row in Tables 3 and 4.
Our proposed RAP-based domain adaptation method achieved significant improvements over all the baseline methods in terms of MAP, and over NNS, OSVM, and TSVM in terms of nDCG@10. It can be seen that the RAP-based domain adaptation method achieved high MAP and nDCG@10 especially for domaindependent search intents. SCL achieved comparable nDCG@10 scores but failed to improve the MAP. SCL selected some features as pivots such as "private room" and "seafood," which are considered common in any domains. The MAP and nDCG@10 of
12Holm's method was used to adjust the significance level for multiple comparisons.

818

0.328

0.326

nDCG@10

0.324

0.322

0.320

0.318

0.5

1.0

1.5

2.0



Figure 2: The average nDCG@10 curve for the parameter .

RCM was almost the same as SVM, since the centroid vector of instances had small values for all the dimensions in our case: the value distribution was flattened by taking an average over all the instance vectors. The TSVM was worse than SVM in many of the search intents, probably because the number of relevant instances per search intent was not balanced across domains, the same label distribution in the source and target domains is assumed with TSVM. SVM showed higher MAP and nDCG@10 than methods using only selected examples, i.e. NNS and OSVM. The difference between SVM and those methods is that SVM did use nonselected examples. Therefore, this result suggests that using not only selected examples but also nonselected examples can improve the search performance of content-based retrieval when most of the relevant instances are selected in the source domain.
Our proposed RAP-based domain adaptation method showed considerable improvements of nDCG@10 for search intents 5, 11, and 19. Search intent 5 was "Please tell me a tranquil restaurant appropriate for entertaining foreign guests in <v>. Our budget is around 8,000 JPY per person," search intent 11 was "I am looking for Western restaurants in <v> for Christmas dinner for 7,500 JPY per person (but not picky about the cost)," and search intent 19 was "Please show me Japanese restaurants or ones serving meat for celebrating my father's birthday in <v>. He cannot eat Italian dishes or drink alcohol. Our maximum budget is 40,000 JPY for three people." These intents include ambiguous conditions, e.g. a category in search intent 5, and budget in the search intent 11, which can be different in different domains. For example, Japanese restaurants were often judged as relevant in Kyoto for search intent 5, while Western restaurants were usually relevant in Tokyo. As shown in Table 1, Kyoto include many Japanese restaurant and is famous for Japanese-style restaurants because it is one of the oldest cities in Japan. On the other hand, there are many Westernstyle restaurants in Tokyo because it is a modern international city in Japan. The difference in frequent categories might be lessened by RAPs in our proposed method, which leads to high search performance. Similarly, relevant restaurants were more expensive in Tokyo than those in Kyoto for search intent 11, and the cost difference was probably bridged by RAPs related to the cost.
6.4 Parameter Tuning
There are a parameter  and choices of RAPs in our proposed RAP-based domain adaptation method. We tested several values for  using all the RAPs mentioned in the experimental setting. The average nDCG@10 over all the search intents are plotted in Figure 2. The setting  = 0.5 was relatively close to the SVM (no RAP was used), while the setting  = 2.0 was relatively close to where only features derived from RAPs were used. The best  was 1.3 in our test collection. These results suggest that both original and domain-dependent features are necessary to better model the similarity between instances in different domains.
We also tested the effectiveness of each RAP. Table 5 lists the relative MAP and nDCG@10 for all combinations of RAPs. The

Table 5: The relative MAP and nDCG@10 for all the combina-

tion of RAPs.

MAP nDCG@10

CLS, MAX/MIN, AVG

100.0%

98.4%

CLS, MAX/MIN

99.7%

95.4%

CLS, MAX/MIN, Mode

99.4%

94.5%

CLS, MAX/MIN, AVG, FREQ 98.3%

95.9%

MAX/MIN, AVG

96.1%

16.1%

CLS, FREQ

95.2%

97.3%

CLS, AVG, FREQ

93.9%

94.5%

CLS, AVG

92.5%

99.6%

CLS

91.2%

100.0%

AVG

37.7%

-8.7%

(None)

0.0%

0.0%

MAX/MIN, AVG, FREQ

-68.7%

-19.7%

MAX/MIN, FREQ

-139.2%

-27.9%

MAX/MIN

-144.1%

-26.3%

AVG, FREQ

-225.9% -102.0%

FREQ

-383.6% -105.8%

percentage indicates the gain from (None), a method with no RAP, normalized by the maximum gain of MAP or nDCG@10. The FREQ RAP was the least effective in our test collection and rather impaired search performance. In contrast, the CLS RAP was the most effective in our experiment and improved search performance even when only CLS was used. The MAX/MIN and AVG achieved high MAP scores when they were used together. The best performance in terms of both MAP and nDCG@10 was obtained by the combination of CLS, MAX/MIN, and AVG. As future work, we will develop a method of finding effective RAPs for a given task.

7. CONCLUSIONS
We introduced the problem of domain adaptation for contentbased retrieval, and proposed a domain adaptation method based on RAPs. We developed a test collection for content-based geographic object retrieval, and provided answers to two research questions.
The first research question was whether domain adaptation for content-based retrieval is necessary. Our experimental results showed that evaluation metrics in an out-domain setting (D(S) = D(T )) were significantly lower than those in an in-domain setting (D(S) = D(T )). Therefore, we conclude that the answer to the first research question is yes: search performance without domain adaptation significantly decreases when the source and target domains are heterogeneous, and domain adaptation for content-based retrieval is necessary.
The second research question was whether domain adaptation based on RAPs can bridge the domain difference and significantly improve the search performance of content-based retrieval in heterogeneous domains. Our experimental results answered the second research question by showing significant improvements over baseline methods.
Future work involves applying the RAP-based method to other datasets. In practice, developing a method for finding effective RAPs is the most important part of future work, as it is difficult to find effective RAPs manually. Although we assume that feature spaces in the source and target domains are the same in this paper, it is possible to apply our proposed RAP-based domain adaptation method to heterogeneous feature spaces such as image and music feature spaces. The RAP-based method requires only a withindomain similarity function not a between-domain similarity function for domain adaptation, and enables a user to find popular music by selecting popular pictures. There is also a need to further study the effect of the domain differences, such as the difference between datasets from which the system retrieves data, between search locations, and between user's profiles, on search tasks.

819

8. ACKNOWLEDGMENTS
This work was supported in part by the following projects:
Grants-in-Aid for Scientific Research (Nos. 24240013, 24680008, and 22∑4687) from MEXT of Japan, and a Kyoto University GCOE Program entitled "Informatics Education and Research for
Knowledge-Circulating Society."
9. REFERENCES
[1] O. Alonso and R. Baeza-Yates. Design and implementation of relevance assessments using crowdsourcing. In Proc. of ECIR, pages 153≠164, 2011.
[2] N. J. Belkin. Helping people find what they don't know. Communications of the ACM, 43:58≠61, 2000.
[3] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. Analysis of representations for domain adaptation. In Proc. of NIPS, pages 137≠144, 2006.
[4] J. Blitzer, M. Dredze, and F. Pereira. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proc. of ACL, pages 440≠447, 2007.
[5] J. Blitzer, R. McDonald, and F. Pereira. Domain adaptation with structural correspondence learning. In Proc. of EMNLP, pages 120≠128, 2006.
[6] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In Proc. of SIGIR, pages 25≠32, 2004.
[7] P. Cai, W. Gao, A. Zhou, and K. Wong. Relevant knowledge helps in choosing right teacher: active query selection for ranking adaptation. In Proc. of SIGIR, pages 115≠124, 2011.
[8] Y. Chen, X. Zhou, and T. Huang. One-class svm for learning in image retrieval. In Proc. of ICIP, pages 34≠37, 2001.
[9] T. Chia, K. Sim, H. Li, and H. Ng. A lattice-based approach to query-by-example spoken document retrieval. In Proc. of SIGIR, pages 363≠370, 2008.
[10] W. Dai, G.-R. Xue, Q. Yang, and Y. Yu. Co-clustering based classification for out-of-domain documents. In Proc. of KDD, pages 210≠219, 2007.
[11] W. Gao, P. Cai, K.-F. Wong, and A. Zhou. Learning to rank only using training data from related domain. In Proc. of SIGIR, pages 162≠169, 2010.
[12] K. J‰rvelin and J. Kek‰l‰inen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems, 20(4):422≠446, 2002.
[13] T. Joachims. Transductive inference for text classification using support vector machines. In Proc. of ICML, pages 200≠209, 1999.
[14] M. Kamvar, M. Kellar, R. Patel, and Y. Xu. Computers and iphones and mobile phones, oh my!: a logs-based comparison of search users on different devices. In Proc. of WWW, pages 801≠810, 2009.
[15] M. P. Kato, H. Ohshima, S. Oyama, and K. Tanaka. Search as if you were in your home town: geographic search by regional context and dynamic feature-space selection. In Proc. of CIKM, pages 1541≠1544, 2010.
[16] X. Ling, W. Dai, G.-R. Xue, Q. Yang, and Y. Yu. Spectral domain-transfer learning. In Proc. of KDD, pages 488≠496, 2008.
[17] Y. Liu, D. Zhang, G. Lu, and W. Ma. A survey of content-based image retrieval with high-level semantics. Pattern Recognition, 40(1):262≠282, 2007.
[18] S. Nakajima and K. Tanaka. Relative queries and the relative cluster-mapping method. In Proc. of DASFAA 2004, pages 843≠856, 2004.
[19] S. Pan, X. Ni, J. Sun, Q. Yang, and Z. Chen. Cross-domain sentiment classification via spectral feature alignment. In Proc. of WWW, pages 751≠760, 2010.
[20] S. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345≠1359, 2010.

[21] P. Rai, A. Saha, H. DaumÈ III, and S. Venkatasubramanian. Domain adaptation meets active learning. In Proc. of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 27≠32, 2010.
[22] B. Schˆlkopf, J. Platt, J. Shawe-Taylor, A. Smola, and R. Williamson. Estimating the support of a high-dimensional distribution. Neural computation, 13(7):1443≠1471, 2001.
[23] F. X. Schumacher and R. W. Eschmeyer. The estimation of fish populations in lakes and ponds. Journal of the Tennessee Academy of Sciences, 18:228≠249, 1999.
[24] J. Sim and C. Wright. The kappa statistic in reliability studies: use, interpretation, and sample size requirements. Physical therapy, 85(3):257≠268, 2005.
[25] J. Teevan, S. Dumais, and E. Horvitz. Potential for personalization. ACM Transactions on Computer-Human Interaction, 17(1):1≠31, 2010.
[26] B. Wang, J. Tang, W. Fan, S. Chen, Z. Yang, and Y. Liu. Heterogeneous cross domain ranking in latent space. In Proc. CIKM, pages 987≠996, 2009.
[27] H. Wang, H. Huang, F. Nie, and C. Ding. Cross-language web page classification via dual knowledge transfer using nonnegative matrix tri-factorization. In Proc. of SIGIR, pages 933≠942, 2011.
[28] G. Xue, W. Dai, Q. Yang, and Y. Yu. Topic-bridged plsa for cross-domain text classification. In Proc. of SIGIR, pages 627≠634, 2008.

APPENDIX

A. ESTIMATION OF RELATIVE AGGRE-

GATION POINTS

We approximately estimated RAPs by using finite samples X

from E[x]

a doRmain. Expectation = xX xP (x)dx 

E|X1[x| ]PcaxnXbexa,pwprhoexriem|aXte|disasthfoellnouwms-:

ber of instances X. This approximation is directly applied to the

equation for the AVG RAP.

Similarly, expectation E[x|S] can be approRximated by instances

XS
1 |XS |

=P X  S as follows: E[x|S] xXS x, which is applied to the

= xX xP (x|S)dx  equation for the CLS RAP.

The equations for the MAX/MIN and FREQ RAPs

aRre the sameR form and can be written as follows: wW P (w|) xX xP (x|w)dxdw, where w is an attribute value (i.e. a cost or a category) and W is the entire set of w. By

using the equation

approximatiP on for the as follows: wWX

PC|(XLwSw||R)APP,xwXe wcaxn,awpphreorxeimXawte

the is a

set of instances that have the attribute value w, and WX is a set of

attribute values included in the instance set X.

For the MAX RAP, we assume that the cost follows a nor-

mal distribution g, and the number of sampled instances from a

domain is constant l. The probability of the cost b that is the

maximum cost in a set of sampled instance is then defined as P (b|MAX) = {G(b)}l, where G is the cumulative distribution

function of the normal distribution g. For the MIN RAP, the probability P (b|MIN) is defined as P (b|MIN) = 1 - {1 - G(b)}l.

For the FREQ RAP, we assume that the frequency of a category

value c follows a binomial distribution B, and is independent from

the frequency of other values. In addition, the number of sampled

instances from a domain is assumed to be constant l. The probabil-

ity of a category value c that is more frequent in instances sampled

from a defined

daosmPa(inc|DFRthEaQn)in=onPesli=sa0mPpllje=di+fr1oBmDa(nio)tBheDr

D is then (j), where

BD(i) is the probability that a category value c occurs i times in

l instances sampled Qfrom D. Thus, P (C|FREQ) is represented as P (C|FREQ) = cC P (c|FREQ) based on the independent

assumption.

820

Mixture Model with Multiple Centralized Retrieval Algorithms for Result Merging in Federated Search

Dzung Hong
Department of Computer Science Purdue University
250 N. University Street West Lafayette, IN 47907, USA
dthong@cs.purdue.edu
ABSTRACT
Result merging is an important research problem in federated search for merging documents retrieved from multiple ranked lists of selected information sources into a single list. The state-of-the-art result merging algorithms such as Semi-Supervised Learning (SSL) and Sample-Agglomerate Fitting Estimate (SAFE) try to map document scores retrieved from different sources to comparable scores according to a single centralized retrieval algorithm for ranking those documents. Both SSL and SAFE arbitrarily select a single centralized retrieval algorithm for generating comparable document scores, which is problematic in a heterogeneous federated search environment, since a single centralized algorithm is often suboptimal for different information sources.
Based on this observation, this paper proposes a novel approach for result merging by utilizing multiple centralized retrieval algorithms. One simple approach is to learn a set of combination weights for multiple centralized retrieval algorithms (e.g., logistic regression) to compute comparable document scores. The paper shows that this simple approach generates suboptimal results as it is not flexible enough to deal with heterogeneous information sources. A mixture probabilistic model is thus proposed to learn more appropriate combination weights with respect to different types of information sources with some training data. An extensive set of experiments on three datasets have proven the effectiveness of the proposed new approach.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Design, Performance
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

Luo Si
Department of Computer Science Purdue University
250 N. University Street West Lafayette, IN 47907, USA
lsi@cs.purdue.edu
Keywords
Federated Search, Result Merging, Mixture Model
1. INTRODUCTION
Federated search (also known as distributed information retrieval) [17, 23, 29] is an important research area of information retrieval. Unlike traditional information search systems such as Google or Bing, which index webpages or documents that can be crawled and collected, federated search targets on information distributed in independent information providers. Many contents in this environment may not be arbitrarily crawled and searched by traditional search engines, due to various reasons such as copyright, security and data protection. Only the owners of those documents can provide a full searching service to their set of documents. We refer to a collection of documents with its own and customized search engine as an information source. The size of this type of information (i.e., hidden Web contents) has been estimated to be many times larger than Web contents searchable by traditional search engines [3].
Federated search offers a solution for searching hidden Web contents by building a bridge between users, who have little knowledge about which kind of information sources she is looking for, and the information sources that reveal limited information about their documents through sourcespecific search engines. To achieve this goal, federated search includes three main research problems: resource representation, resource selection and result merging. Resource representation learns important information about the sources such as their contents and their sizes. Resource selection selects a subset of information sources which are most useful for users' queries. Result merging combines documents retrieved from selected sources into a single ranked list before presenting the list to the end users.
Among the above main problems, result merging substantially suffers from the heterogeneity of information sources. Each information source may adopt a different, customized retrieval model. A query can also be processed in many ways. Even if different sources use similar retrieval algorithms, they may have different source statistics (e.g., different values of inverse document frequencies). All of those make it difficult to compare documents of different sources. A simple solution that downloads all document contents and ranks them with a single method for each user query may yield good results, but it is also costly in an online setting.

821

Other solutions such as downloading parts of the documents [6] or incorporating scores from the resource selection component into source-specific document scores (e.g., CORI [4]) also suffer when information sources do not provide enough information or vary greatly in their scales of document ranking scores.
The state-of-the-art result merging algorithms merge documents by learning how to map document scores in ranked lists of multiple information sources to comparable document scores. The basic idea is to utilize a centralized sample database created with all sample documents obtained in resource representation. For each query, these algorithms rank documents in the centralized sample database with a single retrieval algorithm, and then build a mapping function between source-specific document scores (or ranks) and comparable document scores. By mapping document scores/ranks returned from all selected sources to a common scale, it is possible to construct the final ranked list. Algorithms of this class such as SSL [27], SAFE [24] and WCF [12] have shown promising results. However, despite using various learning algorithms, those methods still do not fully address the heterogeneity of retrieval algorithms in different information sources. The problem lies in the fact that all these existing methods arbitrarily select a single fixed centralized retrieval algorithm for learning the mapping, which is problematic in a heterogeneous federated search environment, as a single algorithm is often suboptimal for learning comparable scores for different sources.
In this paper, we propose a novel result merging algorithm that utilizes multiple centralized retrieval algorithms. This method can generate more accurate results in result merging due to the flexibility of using multiple types of centralized retrieval algorithms for estimating comparable document scores. In particular, the paper shows that it is not desirable to learn a fixed set of weights (e.g., with a logistic regression approach) for different centralized retrieval algorithms in estimating comparable document scores. A mixture probabilistic model is proposed to automatically learn the appropriate weights for different types of information sources with some training data. The mixture model approach is more flexible in calculating comparable document scores for a heterogeneous set of information sources. Empirical studies have been conducted with three federated search datasets to show the advantages of the proposed result merging algorithm. In particular, one new dataset is created from the Wikipedia collection of ClueWeb data.
The rest of the paper is organized as follows. Section 2 discusses some research work generally related with the work in this paper. Section 3 discusses two specific state-ofthe-art results merging algorithms (SSL and SAFE) as they are directly related with the proposed research. Section 4 proposes the novel result merging algorithm with multiple centralized retrieval algorithms. Section 5 introduces experimental methodology. Section 6 presents the detailed experimental results and provides some discussions. Section 7 concludes and points out some future research directions.
2. RELATED WORK
Federated search includes three main research problems: resource representation, resource selection and result merg-

ing. There is a large volume of previous research work in all of those research problems. This section first briefly introduces most related prior research in resource representation and resource selection. Then it will provide more details about the literature of result merging.
Resource representation is to collect information about each information source. Such information usually includes sources' sizes, document frequencies, term frequencies, and other statistics. The START protocol [9] is one of the first attempt to standardize the communication between information sources and a broker (or centralized agent) in order to collect, search and merge documents from individual sources. However, this approach can only work in cooperative environments. In an uncooperative environment, it is more practical to collect source statistics with sampling algorithms. The query-based sampling method [4] is a popular algorithm for sampling documents from a set of information sources. In principal, query-based sampling sends randomly generated terms as queries to a source, and downloads the top documents as sample documents for each query. When this process is done, the set of all sample documents can be collected in a centralized sample database to build a single index. The centralized sample database is often a good representative of the (imaginary) complete database of all documents in a federated search environment.
Resource selection is to select a subset of information sources most relevant to the user's query. Resource selection has been studied intensively during the last two decades. Many algorithms have been developed, such as GlOSS [10], CORI [4], ReDDE [26], CRCS [22], topic models [2], the classification-based model [1] and many others. The Relevant Document Distribution Estimation (ReDDE) resource selection algorithm and its variants have been shown to generate good and robust resource selection results in different types of federated search environments. ReDDE selects relevant sources by first ranking sample documents in the centralized sample database. Then, each document among the top of the list can contribute a score to its containing source. The magnitude of the score depends on both document's rank and the source's size. Finally, the relevance of a source is measured by the combined score of all of its sample documents.
Result merging is to collect the ranked list of documents from each selected source and combine them into a single ranked list to present to users. Result merging in federated search is similar to data fusion [32, 25], or merging process in multilingual information retrieval [30]. In data fusion, different retrieval models are applied to a single information source, and the problem is to get the best combination of retrieval algorithms. Whereas, in federated search, there are multiple information sources with different (often unknown) retrieval models. Similar to information fusion, multilingual information retrieval also assumes that the whole collection index is available to the merger during the process, which is not always the case in federated search.
One scenario is that the broker can download all returned documents from selected sources, and apply a centralized retrieval algorithm to produce the final ranked list. However, in practice, this method is rarely used since the high cost of communication and time may impair user experience. In another simple case, when all sources implement

822

the same retrieval model, documents' scores (or ranks) returned by the source may be comparable with each other. Thus, merging their scores (or ranks) directly (also known as Raw Score Merging), or in a round-robin fashion may give good results with low cost. However, it is noticed that even if all sources share the same model, some statistics such as document frequency of a term are still different across different sources. It is generally not practical to assume that all independent sources share such a same set of collection statistics.
Some other algorithms in the early generation of federated search also relied on term statistics for making decision. Craswell et al. suggested that by partially downloading a part of the top returned documents, we can approximate term statistics to build the final rank list [6]. Xu and Croft requested that document statistics of query terms should be provided to the broker, in such a way that they can calculate the global inverse document frequencies [34]. However, these algorithms again require some type of collaboration from the independent sources, which is often unavailable.
CORI result merging algorithm [4] is a relatively simple, yet effective algorithm. The intuition is that comparable document scores should depend on two factors: (i) how good a document is compared to other documents from the same source; and (ii) how good the source containing a particular document is compared to other sources. CORI makes a linear combination of those two factors and gets the final score of a document as:
D + 0.4 ◊ D ◊ C D=
1.4
where D is the global score, D is the original score within the source, and C is the normalized source score from the resource selection step.
The merging algorithm proposed by Rasolofo et al. [19] also explores the combination between document scores and source weights. Unlike CORI, their source weights are not directly related with the sources' relevance scores. Rather, the weight of a source depends on the total number of documents that it returns. The algorithm assumes that a source containing more relevant documents may return a longer ranked list, which is not always the case for information sources using different types of ranking algorithms.
The intuition of combining document and resource scores can also be seen in a variant of the PageRank algorithm in distributed environments [31]. In this work, Wang and DeWitt employed the source's ServerRank and the document's LocalRank to derive the global PageRank values.
Semi-Supervised Learning (SSL) [27] and the Sample Agglomerate Fitting Estimate (SAFE) [24] result merging algorithms offer a better trade-off in efficiency and effectiveness. Both methods try to map source-specific document ranks into comparable document scores generated by a single centralized retrieval algorithm. We will provide more detailed information about SSL and SAFE in the following section as they are directly related with the new research in this paper.

3. RESULT MERGING BY SEMI-SUPERVISED LEARNING & SAMPLE-AGGLOMERATE FITTING ESTIMATE

3.1 Semi-Supervised Learning Merging

Semi-Supervised Learning Merging (SSL) [27] uses curve fitting model to calculate comparable document scores from different sources for result merging. Specifically, given a user's query, SSL sends the query to the centralized sample database and retrieves the sample ranked list with relevance score of each document. Upon receiving documents from a selected information source, SSL checks for overlapping documents exist in the sample database. Those overlapping documents are characterized by two features: the relevance scores in the central sample database, and the relevance ranks in the specific source. The task is to estimate the relevance scores of all non-overlapping documents in the centralized complete database (the imaginary dataset of all documents of all sources). Assume that there is a linear mapping between centralized relevance scores and sourcespecific document ranks, then that mapping can be inferred by using a regression method on the overlapping documents. Having said that, let Rij be the source-specific rank of document di in source Cj, and Sij be the relevance score of document di in the centralized sample database, we can build a linear relationship.

Sij = aj ◊ Rij + bj

where aj, bj are two parameters depending on each pair of an information source and a query.

With enough overlapping documents for a source and a query, we can train a regression matrix

R1j

R2j

 

∑

∑

∑

Rnj

1

S1j 

1 1

◊

aj bj

=

S2j

 

∑

∑

∑

  

1

Snj

In the above equation, let us denote the first matrix by X, the second matrix by W , and the third matrix by Y . By minimizing the square loss error, we can derive the solution to the parameters W as
W = (XT X)-1XT Y

One main problem of SSL is that if there is not enough overlapping documents (three requested in the original SSL work) for building a linear mapping, the model will back off to the CORI result merging formula, which is often much less effective.
3.2 Sample-Agglomerate Fitting Estimate Merging
Sample-Agglomerate Fitting Estimate (SAFE) [24] overcomes the SSL's problem of not having enough overlapping documents by estimating the ranks of unoverlapping documents in the centralized sample database. If we assume that the sampling process is uniform, then each sample document will represent the same number of unseen documents in the selected information source. Therefore, a sample document ranked at position i-th in the source-specific sample ranked

823

Table 1: Transformation Functions

Name LIN SQRT LOG POW

f (x) f (x) =x f (x) = x f (x) = log x f (x) = 1/x

Model

S = a ◊R + b S =a ◊ R+b

S = a ◊ log R + b

S=a

◊

1 R

+

b

ple database to learn the comparable document scores by curve-fitting. However, unlike SSL and SAFE, MoRM employs multiple retrieval algorithms for the centralized sample database. Therefore it is more flexible to address the heterogeneity of information sources in federated search environments for improving the accuracy of result merging.
4.1 MoRM's Framework

list

will

have

an

approximate

rank

i◊

|C| |Cs |

in

the

source-

specific full rank, where |C| is the source's estimated size,

and |Cs| is the source's sample size. By using the estimated

source-specific ranks together with true centralized ranks (of

overlapping documents), SAFE could apply regression with

more information than SSL. A problem may occur when

there are not enough sample documents of a selected infor-

mation source in the centralized sample database. However,

this is rarely the case, if ReDDE (or its variants) is used

for selecting information sources, since this method usually

selects a source if it has a significant number of documents

in the centralized ranked list with respect to the query.

Another contribution of SAFE to SSL is that, instead of using the raw rank information of documents, SAFE applies different transformation functions to the rank, in order to find the best regression. More specifically, there are four different transformations as in Table 1. Each transformation function is applied to the source-specific ranked list to learn the set of parameters (aij, bij). Then, SAFE selects the best transformation by comparing the goodness of curve-fitting of all models based on their coefficient of determination R2 values [11]. Specifically, for a linear regression equation X ◊ w = Y , the coefficient of determination is calculated as follows.

R2

=

||Y^ ||2 ||Y ||2

=

Y TPY Y TY

where P = X(XT X)-1XT

4. MIXTURE OF RETRIEVAL MODELS FOR RESULT MERGING
SSL and SAFE are state-of-the-art algorithms for result merging in federated search. However, because of their choosing of a single centralized retrieval algorithm for calculating comparable document scores, these algorithms still do not fully address the heterogeneity of different information sources in federated search environments . A single centralized retrieval algorithm may have good curve-fittings for some information sources, but may also be less fit for some others. This paper proposes to use multiple centralized retrieval algorithms to retrieve a set of ranking scores for each document. Moreover, rather than assigning a fixed set of weights to combine the above scores, our model learns a more appropriate combination of weights with respect to different types of information sources. We assume that there is an underlying distribution (i.e., latent groups) of sources according to their adopted retrieval models. Learning the proposed model thus becomes learning the distribution of groups and the combination weights associated with each group. This model is called the Mixture of Retrieval Models (MoRM) for result merging. MoRM is related with SSL and SAFE in the way that it uses the centralized sam-

In this section, we describe the general framework of MoRM for document merging. The following steps are applied when a query comes:

∑ A resource selection algorithm such as ReDDE [26] selects a subset of information sources that are most relevant to the user's query.
∑ The query is then forwarded to the selected information sources. Each source will return a ranked list of documents. Scores of the returned documents will help the model's performance but are not required. Document ranks are usually sufficient for the next step.
∑ MoRM also issues the query to the centralized sample database and retrieves documents using a set of predetermined algorithms. At the end of this step, it obtains a set of ranked lists of sample documents.
∑ For each ranked list, MoRM tries to learn a mapping between source-specific document ranks and the centralized document scores. Ranks of sample documents that are not in the source-specific ranked list are estimated in a similar way as in the SAFE algorithm. All transformation functions as listed in Table 1 are tested in order to find the best curve fitting parameters. The best transformation function is applied to predict the comparable scores of all returned documents.
∑ All comparable scores of a document are combined using a set of combination weights learned from a training dataset. These final scores are used to rank documents.

In the following sections, we will propose a simple logistic regression model (for learning a single set of combination weights) and then propose the mixture of retrieval models (for learning multiple sets of combination weights) for the task of estimating documents' comparable scores.
4.2 Logistic Regression for Learning Comparable Scores

A learning algorithm such as logistic regression may address the problem of combining different document scores seamlessly. We chose logistic regression to demonstrate the approach of learning a single set of combination weights for ranking documents. Logistic regression is a discriminative model that models the probability that a binary event happens by a sigmoid function. In this case, our predictive functions are:

P (ycqd

=

1|w, xqcd)

=

1+

1 exp (-w

∑

xqcd)

=

(w

∑

xqcd)

(1)

and

P (ycqd = -1|w, xqcd) = 1 - P (ycqd = 1) = (-w ∑ xqcd) (2)

824

where our notations for this model are as follows: let the
superscript q refer to the query q, and the subscript cd refer
to the d-th document of source c (such a document is called Dcd). We also use xqcd to denote the feature vector of document Dcd (the set of comparable scores of Dcd according to
different centralized retrieval algorithms), and w for the set of weights associated with xqcd. Our target is to predict ycqd, which is the relevance of document Dcd with respect to the query q. The possible values of ycqd are:

ycqd =

1 -1

if document Dcd is relevant to query q otherwise

Finally, in the equations (1) and (2) above, we also use (z) to indicate the sigmoid function
1 (z) =
1 + exp(-z)

and apply this property: (-x) = 1 - (x).

Given C, the number of sources; Q, the number of queries, and all the returned documents Dcd with respect to the training queries, we can write the likelihood function of the model as

|Q| C Dc

|Q| C Dc

L(w) =

P (ycqd|w, xqcd) =

(ycqdw ∑ xqcd)

q=1 c=1d=1

q=1 c=1d=1

where we have combined equations (1) and (2) above. Learning the combination weight w can be done by maximizing the log-likelihood function using the iterative re-weighted least squares method [8].

4.3 Mixture of Retrieval Models for Learning Comparable Document Scores

We now describe the mixture model of retrieval algorithms (MoRM) for result merging. MoRM offers more prediction capability by automatically learning multiple sets of combination weights, each of them is associated with a "soft" information source cluster. The word "soft" means that we use probability to assign a source to its cluster, rather than fixing a hard assignment. Specifically, assuming that there are K of such clusters, and let ck be the probability that the source c belongs to group k, then the following constraints must be hold:

K k=1

ck

=

1

for c = 1, 2, ∑ ∑ ∑ , C

To make our formulations simpler, in this section, we will first derive the formulations for only one query, and drop the superscript q of ycd and xcd. At the end of this section, we will extend the formulations for the set of training queries. Furthermore, we will denote cdk for P (ycd|wk, xcd) (the probability that the document Dcd has relevance ycd to the query in question, given that the collection c belongs to cluster k. In short, cdk = (ycdwk ∑ xcd)).
Let w = {wk|k = 1, ∑ ∑ ∑ , K}, and  = {ck|c = 1, ∑ ∑ ∑ , C;
k = 1, ∑ ∑ ∑ , K}. Given a query q, let  = {w, } denote the set of parameters, in which each combination weight wk is associated with the k-th cluster. MoRM assumes the same combination weight for all sources of a cluster for building robust combination model with a limited amount of training data, hence we will set k = ck = c k for all sources c, c .

The probability that a document Dcd has relevance ycd given all parameters is calculated as follows.

K

K

P (ycd|, xcd) = kP (ycd|wk, xcd) = kcdk (3)

k=1

k=1

The component k acts as the prior of the clusters' distribution, which adjusts the belief of relevance according to each cluster. This equation is also known as the mixture of logistic regression. Given that model, the likelihood function for the training dataset with respect to one query is as follows.

C Dc K

L() =

k cdk

(4)

c=1 d=1 k

where Dc is the number of documents returned by the source c.

It is difficult to optimize the above function directly, since taking its logarithm still presents the summation inside the log. Therefore, we will utilize the Expectation Maximization (EM) algorithm [7] to learn the parameters. The derivation of EM algorithm is discussed in the following section.

4.4 Learning MoRM using EM Algorithm

Let zc be a K-dimensional latent variable associated with source c. zc has only one element which equals to 1 and the all other elements equal 0 (i.e., a 1-of-K representation). Therefore, zc must satisfy the following constraints.

K k=1

zck

=

1

for c = 1, 2, ∑ ∑ ∑ , C

where zck is the k-th element of zc.

We then use zc as the indicator of the membership of source c. If c belongs to cluster k, then zck = 1, and the other elements of zc equal 0. Given that k is the prior distribution of cluster k as above, and note that ck = k for all c, we can write the prior distribution of zck as follows.

P (zck = 1) = k

(5)

Then the prior distribution of the whole vector zc can be written as

K

P (zc) = kzck

(6)

k=1

Define another random variable Z = {zc|c = 1, ∑ ∑ ∑ , C} associated with all sources. Since each source is independent of each other, the prior of Z is just the multiplication over all sources.

CK

P (Z) =

k zck

(7)

c=1k=1

Similarly, if we know that the source c has the member-

ship vector zc, then the probability that the document Dcd

of that source has relevance ycd is

K k=1

cdk

zck

,

since

cdk

is

the conditional probability with respect to cluster k. There-

fore, the likelihood function of the model is obtained by

multiplying the above term over all sources and documents.

C Dc K

P (X, Y |Z, ) =

cdk zck

(8)

c=1d=1 k=1

825

where X denotes all document feature vectors, and Y denotes the relevance vector of all documents. Multiplying the equations (7) and (8) above, one can calculate the complete likelihood function

CK

Dc

P (X, Y , Z|) =

k zck

cdk zck

(9)

c=1k=1

d=1

Taking the logarithm of the above function yields the complete log-likelihood as follows.

CK

Dc

log P (X, Y , Z|) =

zck{log k + log cdk} (10)

c=1k=1

d=1

The EM algorithm involves two steps. For the E-step, we need to calculate the posterior probability P (Z|X, Y , ). Using (9), we can derive the following relation.

P (X, Y , Z|) C K

Dc

zck

P (Z|X, Y , ) =



k cdk

P (X, Y |)

c=1k=1

d=1

(11)

where we can use the proportional sign  because the de-

nominator P (X, Y |) does not depend on Z.

We wish to calculate the expectation of the variable Z under the above posterior distribution, since that term will be useful in the following M step. Given that all zc are independent, and the right-hand side of equation (11) can be factorized over c, we can derive the expectation of each variable zck as

E[zck] = =

{zck{0,1}} zck k {zcj ,1jK} zcj j

 Dc

zck

d=1 cdk

 Dc

zcj

d=1 cdj

k

Dc d=1

cdk

K j=1

j

Dc d=1

cdj

= (zck)

(12)

where we have defined a new variable (zck).

In the M-step, the updated parameters new are calculated according to the following formula

new = arg max Q(, old)


(13)

where

Q(, old) = E log P (X, Y , Z|) | P (Z|X, Y , )

Taking the expectation of log P (X, Y , Z|) (as derived in equation (10)) with respect to the posterior distribution gives us the following objective function for the M-step.

CK

Dc

{new, wnew} = arg max

(zck)(log k + log cdk)

,w c=1k=1

d=1

To find the new value of k, we only need to maximize the first part of the above function

CK

new = arg max

(zck) log k



c=1k=1

subject to the constraint

K k=1

k

=

1

Using Lagrange multiplier and setting the gradient to 0, one can solve the optimal values of k as

knew =

C c=1



(zck

)

K k=1

Cc=1  (zck )

(14)

Searching for the value of wknew is a bit trickier, since we have to solve the following optimization problem.

C Dc K

wnew = arg max

(zck) log cdk

w

c=1 d=1k=1

(15)

In fact, the gradient of the above objective function with respect to wk is equal to:

C Dc
(zck)(1 - cdk)ycdxcd
c=1 d=1

Therefore, one can apply a gradient descent algorithm to find the optimal value of wk.

In the implementation of the algorithm discussed so far,

there is an issue about (zck). As equation (12) has shown,

computing (zck) involves calculating the product

Dc d=1

cdk

.

This could lead to numerical underflow since cdk is a proba-

bility smaller than 1. Therefore, we need to calculate (zck)

under the log space. Let

Dc
(zck)  k cdk = (zck)
d=1

and max = maxK k=1 (zck). Therefore

(zck) =

(zck ) K j=1(zcj )

=

exp{log (zck) - log max}

K j=1

exp{log

(zck

)

-

log

max

}

Since each log (zck) is computable under the log space, the above equation will avoid the underflow problem. Finally, we extend our formulations to the set of training queries. In this case, the E-step becomes:

(zck) =

k

|Q| q=1

Dc d=1

cqdk

K j=1

j

|Q| q=1

Dc d=1

cqdj

In the M-step, the update formula of k remains the same (equation (14)), while the optimization function in equation (15) becomes

C |Q| Dc K

wnew = arg max

(zck) log cqdk

w

c=1 q=1 d=1k=1

and the objective gradient with respect to wk is

C |Q| Dc
(zck)(1 - cqdk)ycqdxqcd
c=1 q=1 d=1

5. EXPERIMENTAL METHODOLOGY
In this section, we will describe the methodology and datasets of this work. The experiments were conducted on three datasets: two standard TREC datasets, and one Wikipedia dataset for federated search based on the ClueWeb.

826

Table 2: Statistics of Three Testbeds

Size # of # of documents (x1000) # of # of relevant docs/query

Testbed

(GB) inf. sources Min Avg

Max

queries Min Avg

Max

TREC123

3.2

100

0.7 10.8

39.7

100 37 483.7 1,994

TREC4-Kmeans 2.0

100

0.3 5.7

82.7

50 0 127.2

416

ClueWeb-Wiki 252

100

4.4 58.6 434.5

106 1 20.1

93

25

20

Number of sources

15

10

5

0

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

Number of Documents

x 105

Figure 1: Histograms of the Number of Documents per Information Source in ClueWeb-Wiki. The number of bins is 30, the number of documents ranges from 4,400 to 434,525.

∑ TREC123-100col-bysource (TREC123): 100 collections (information sources) were created from TREC CDs 1,2 and 3 [4]. They are organized by publication source and publication date. This testbed comes with 100 queries (TREC topics 51-150) with judgments.
∑ TREC4-100col-Kmeans (TREC4-Kmeans): 100 collections were created from the TREC 4 data. A twopass K-means clustering algorithm is used to organize the dataset by topic [33]. This testbed comes with 50 queries (TREC topics 201-250) with judgments.
∑ Wikipedia-100col-Kmeans (ClueWeb-Wiki): 100 collections were created from the Wikipedia dataset of the ClueWeb [13]. Similar to TREC4-Kmeans, we applied clustering algorithm [33] to divide the dataset into 100 collections. This testbed comes with 106 queries with judgments1.
5.1 ClueWeb Wikipedia Dataset for Federated Search
Table 2 provides more statistics of the three datasets, including sizes, number of information sources; the max, min and average of the number of documents of each source. We also provide the query statistics of each dataset, including
1The partition assignments are available at http://www.cs.purdue.edu/homes/dthong/clueweb

the number of queries; the max, min and average of the number of relevant documents per query.
The ClueWeb 09 is a large-scale collection of web documents that was collected in January and February 2009. The entire dataset consists of about one billion web pages in ten languages. For its tremendous size, the ClueWeb has been used in several tracks of the Text REtrieval Conference (TREC), most notably in the Web track. For distributed environment (in a different problem setting), ClueWeb has been used in [15]. It is desired to construct a new dataset based on ClueWeb for experiments in federated search.
Three Web tracks of TREC (from 2009 to 2011) have been using the ClueWeb so far. Each track has provided 50 queries based on which we build the new testbed. Within the full ClueWeb dataset, Wikipedia is the main contributor of relevant documents for Web track queries. The total size of Wikipedia is about 6 million documents, which is reasonable for creating a separate testbed. We extract all Wiki documents, and apply the same K-means algorithm that was used for creating the TREC4-Kmeans. We also select only 106 queries which contain at least one relevant Wikipedia document (out of the 150 provided queries) for training and testing. In the end, we constructed 100 information sources for the testbed ClueWeb-Wiki, with statistics provided in Table 2. The distribution of source sizes is also shown in

827

Figure 1. Most of the sources have less 70,000 documents, and there are 12 sources of more than 100,000 documents.
5.2 Experiment Configuration
Given a set of information sources, we assign each source a retrieval algorithm chosen from a set of: vector space TF.IDF with "ltc" weighting [21], a unigram statistical language model with linear smoothing (with smooth parameter as 0.5) [16] and Okapi [20] in a round robin manner. This choice is based on the fact that those models are commonly and widely used in information retrieval. Each information source is set to return at most 200 documents for each query. At the centralized sample database, we utilize five models: the three above models, the Inquery [5] and the Indri [28] algorithm. All retrieval algorithm implementations use the Lemur Toolkit [14]. We randomly select a set of queries for training, and used the other set for testing. For the TREC123, there are 50 training queries out of 100; those numbers of TREC4-Kmeans and ClueWeb-Wiki are 25 out of 50 and 50 out of 106.
We choose K, the number of latent groups, to be 3 in our main results. Some experimental results with different K values are also presented. For each information source, we sample at most 300 documents for creating the centralized sample database. For each query, we use ReDDE to select the top 5 sources for TREC123, TREC4-Kmeans and ClueWeb-Wiki.
Our metrics for the performance is the high-precision at document level, which is the percentage of the number of relevant documents in the final merged ranked list. Given that list, we measure the precision at top 5, 10, 15, 20 and 30 respectively. In next section, we will present our experimental results of all datasets.
6. EXPERIMENTAL RESULTS
6.1 High-precision Results
We now present the high-precision results on the above three testbeds. Tables 3-5 show the high-precision results on TREC123, TREC4-Kmeans and ClueWeb-Wiki respectively. The first column is our baseline using SAFE algorithm with Indri [18] as the single centralized retrieval algorithm. SAFE has been demonstrated to generate accurate and robust results compared with SSL and other results merging algorithms. We denote this method as SFI. The LR column presents the results using the logistic regression model to learn the combination weights of all centralized retrieval methods. The last column MoRM presents the results using the proposed mixture of retrieval algorithms. All precision results of LR and MoRM are compared with the baseline SFI using paired t-tests at level p < 0.05.
For TREC123, the performance of MoRM is significantly better than that of SFI. MoRM is also consistently better than the performance of logistics regression model. In general, both learning methods show improvements over the baseline method of one single feature. For TREC4-Kmeans, MoRM also outperforms SFI, although the differences are not significant as in TREC123. This can be explained as in TREC4-Kmeans, we only train on 25 queries, whereas in TREC123, we trained on 50 queries. These above results

Table 3: High-precision result on TREC123 with 300 sample documents and top 5 information sources selected for each query. A * denotes a significant difference at level p < 0.05 compared to the original SAFE algorithm using Indri as the only centralized retrieval algorithm.

Doc

TREC123

Rank SFI

LR

MoRM

@5 0.268 0.332 (+ 23.88 %) 0.344 (+ 28.36 %) *

@10 0.246 0.272 (+ 10.57 %) 0.304 (+ 23.58 %) *

@15 0.229 0.267 (+ 16.31 %) 0.279 (+ 21.54 %) *

@20 0.208 0.251 (+ 20.67 %) * 0.261 (+ 25.48 %) *

@30 0.208 0.229 (+ 9.95 %) 0.232 (+ 11.54 %)

Table 4: High-precision result on TREC4-Kmeans with 300 sample documents and top 5 information sources selected for each query. A * denotes a significant difference at level p < 0.05 compared to the original SAFE algorithm using Indri as the only centralized retrieval algorithm.

Doc Rank
@5 @10 @15 @20 @30

SFI 0.272 0.244 0.211 0.192 0.177

0.280 0.252 0.227 0.212 0.193

TREC4-Kmeans

LR

MoRM

(+ 2.94 %) 0.296 (+ 8.82 %)

(+ 3.28 %) 0.256 (+ 4.92 %)

(+ 7.59 %) 0.227 (+ 7.59 %)

(+ 10.42 %) 0.216 (+ 12.50 %)

(+ 9.02 %) 0.192 (+ 8.29 %)

Table 5: High-precision result on ClueWeb-Wiki with 300 sample documents and top 5 information sources selected for each query. A * denotes a significant difference at level p < 0.05 compared to the original SAFE algorithm using Indri as the only centralized retrieval algorithm.

Doc

ClueWeb-Wiki

Rank SFI

LR

MoRM

@5 0.168 0.182 (+ 8.46 %) 0.204 (+ 21.26 %)

@10 0.146 0.163 (+ 11.00 %) 0.173 (+ 18.31 %)

@15 0.139 0.164 (+ 17.95 %) 0.168 (+ 20.53 %)

@20 0.132 0.150 (+ 13.55 %) 0.153 (+ 15.59 %)

@30 0.111 0.121 (+ 9.67 %) 0.123 (+ 10.75 %)

have shown the advantage of using multiple centralized retrieval algorithms for learning comparable document scores, over the previous model that uses only one single centralized retrieval algorithm. It also demonstrates the advantage of using the mixture model of multiple sets of weights over the logistic regression model that uses only one single set of combination weights.
For the Wikipedia dataset based on ClueWeb, the proposed model also consistently outperforms SFI and LR. The differences however are not significant. Such a significance may be harder to achieve, since on average, this dataset contains less relevant documents per query than the other datasets, as shown in Table 2.
6.2 Experiments with Different Number of Latent Variables
In this section, we discuss the experimental results when the number of latent variable K changes. We only report

828

Precision Value Precision Value

0.36

0.21

K=1

K=1

K=3

0.2

K=3

0.34

K=5

K=5

K=10 0.19

K=10

0.32 0.18

0.3

0.17

0.28

0.16

0.15 0.26
0.14
0.24 0.13

0.22

0.12

5

10

15

20

25

30

5

10

15

20

30

Document Rank

Document Rank

(a) TREC123

(b) ClueWeb-Wiki

Figure 2: High-precision Results of TREC123 and ClueWeb-Wiki with Different Number of Latent Variables

TREC123 and ClueWeb-Wiki for these experiments, and try different configuration of K = {1, 3, 5, 10}. Similar pattern can be observed on the TREC4-Kmeans. K = 1 is actually equivalent to the logistic regression model. Figure 2 shows the results of this experiment. It can be seen that the mixture of retrieval algorithms model is quite consistent with a small range of K values. For ClueWeb-Wiki, the performance of the mixture model with K > 1 is at least equal or higher than that of the logistic regression. For TREC123, the performances with different values of K are also stable for most of the test levels.
7. CONCLUSION & FUTURE WORK
This paper proposes a novel method of mixture model with multiple centralized retrieval algorithms for result merging in federated search. Existing result merging algorithms do not fully address the issue of heterogeneity of information sources in federated search. Their arbitrary choices of a single centralized retrieval algorithm suffer from the fact that information sources are inherently different in source statistics, query processing techniques, and/or document retrieval algorithms. The proposed model attempts to combine various evidence from multiple centralized retrieval algorithms in a mixture model framework, in order to map source-specific document ranks to comparable scores for result merging. We have shown that a single set of combination weights of the evidence do not offer enough flexibility in dealing with such a heterogeneous environment. A mixture model that learns multiple sets of combination weights according to the clusters of sources proves to be a better choice. A set of experiments has been conducted with two traditional TREC datasets and a new dataset based on the ClueWeb. The empirical results in three datasets have demonstrated the effectiveness of the proposed mixture model with multiple centralized retrieval algorithms.
This model could be extended in many ways. For instance, we could add more flexibility to the model by cus-

tomizing the prior distribution k independently for each source, which means a source will be associated with a set of combination weights independent of the others. However, this model could require a larger training dataset to learn the parameters. A hybrid model where a cluster of similar sources independently uses multiple sets of weights is more feasible. The similarity between sources will play an important factor in creating those clusters. Another direction is to build a mixture model based on cluster of queries instead of cluster of sources, in which each query will trigger a different set of combination weights of all features. Furthermore, we can combine both of the above methods. It is also interesting to explore other types of evidence, such as the links between documents from different sources and incorporate them into the learning model.
8. ACKNOWLEDGMENTS
This work is partially supported by NSF research grants IIS-0746830, CNS- 1012208 and IIS-1017837. This work also partially supported by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.
9. REFERENCES
[1] J. Arguello, J. Callan, and F. Diaz. Classification-based resource selection. Proceeding of the 18th ACM conference on Information and knowledge management, pages 1277≠1286, 2009.
[2] M. Baillie and M. Carman. A multi-collection latent topic model for federated search. Information Retrieval, 14(4):390≠412, Aug. 2011.
[3] M. Bergman. The deep web: surfacing the hidden value. Technical report, 2001.
[4] J. Callan. Distributed information retrieval. Advances in Information Retrieval, pages 127≠150, 2000.
[5] J. Callan, W. B. Croft, and S. M. Harding. The inquery retrieval system. In Proceedings of the Third

829

International Conference on Database and Expert Systems Applications, 1992.
[6] N. Craswell, D. Hawking, and P. Thistlewaite. Merging results from isolated search engines. In Proceedings of the 10th Austrlasian Database Conference, 1999.
[7] A. P. Dempster, N. M. Laird, and D. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, 39(B):1≠38, 1977.
[8] R. Fletcher. Practical methods of optimization, volume 1. Wiley, 1987.
[9] L. Gravano, C.-C. K. Chang, H. Garcia-Molina, and A. Paepcke. Starts: Stanford proposal for internet meta-searching. In Proceedings of the ACM-SIGMOD International Conference on Management of Data (SIGMOD). ACM ACM ACM ACM, 1997.
[10] L. Gravano, H. Garcia-Molina, and A. Tomasic. Gloss: text-source discovery over the internet. ACM Transactions on Database Systems (TODS), 24(2):229≠264, 1999.
[11] J. Gross. Linear regression, volume 175. Springer Verlag, 2003.
[12] C. He, D. Hong, and L. Si. A weighted curve fitting method for result merging in federated search. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR '11, pages 1177≠1178, New York, NY, USA, 2011. ACM.
[13] http://lemurproject.org/clueweb09/. The clueweb09 dataset.
[14] http://www.lemurproject.org/. The lemur toolkit.
[15] A. Kulkarni and J. Callan. Document allocation policies for selective searching of distributed indexes. Proceedings of the 19th ACM international conference on Information and knowledge management, pages 449≠458, 2010.
[16] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 111≠119, 2001.
[17] W. Meng and C. Yu. Advanced metasearch engine technology. Synthesis Lectures on Data Management, 2(1):1≠129, 2010.
[18] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Information Processing and Management, 40(5):735≠750, 2004.
[19] Y. Rasolofo, F. Abbaci, and J. Savoy. Approaches to collection selection and results merging for distributed information retrieval. Proceedings of the tenth international conference on Information and knowledge management, pages 191≠198, 2001.
[20] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at trec-3. NIST SPECIAL PUBLICATION SP, pages 109≠109, 1995.

[21] G. Salton, E. Fox, and H. Wu. Extended boolean information retrieval. Communications of the ACM, 26(11):1022≠1036, 1983.
[22] M. Shokouhi. Central-rank-based collection selection in uncooperative distributed information retrieval. Advances in Information Retrieval, 2007.
[23] M. Shokouhi and L. Si. Federated search. 2011.
[24] M. Shokouhi and J. Zobel. Robust result merging using sample-based score estimates. ACM Transactions on Information Systems (TOIS), 27(3):1≠29, 2009.
[25] X. M. Shou and M. Sanderson. Experiments on data fusion using headline information. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '02, pages 413≠414, New York, NY, USA, 2002. ACM.
[26] L. Si and J. Callan. Relevant document distribution estimation method for resource selection. Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 298≠305, 2003.
[27] L. Si and J. Callan. A semisupervised learning method to merge search engine results. ACM Transactions on Information Systems (TOIS), 21(4):457≠491, 2003.
[28] T. Strohman, D. Metzler, H. Turtle, and C. W. B. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligence Analysis, 2004.
[29] P. Thomas. Server selection in distributed information retrieval: a survey. To appear in: Journal of Information Retrieval, 2012.
[30] M. Tsai, H. Chen, and Y. Wang. Learning a merge model for multilingual information retrieval. Information Processing & Management, 47(5):635≠646, 2011.
[31] Y. Wang and D. J. DeWitt. Computing pagerank in a distributed internet search system. In VLDB '04: Proceedings of the Thirtieth international conference on Very large data bases, pages 420≠431. VLDB Endowment, 2004.
[32] S. Wu, Y. Bi, and X. Zeng. The linear combination data fusion method in information retrieval. In Database and Expert Systems Applications, pages 219≠233. Springer, 2011.
[33] J. Xu and J. Callan. Effective retrieval with distributed collections. Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 112≠120, 1998.
[34] J. Xu and W. B. Croft. Cluster-based language models for distributed retrieval. Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 254≠261, 1999.

830

Reactive Index Replication for Distributed Search Engines

Flavio P. Junqueira
Yahoo! Research Barcelona, Spain
fpj@yahoo-inc.com

Vincent Leroy
Yahoo! Research Barcelona, Spain
leroy@yahoo-inc.com

Matthieu Morel
Yahoo! Research Barcelona, Spain
matthieu@yahoo-inc.com

ABSTRACT
Distributed search engines comprise multiple sites deployed across geographically distant regions, each site being specialized to serve the queries of local users. When a search site cannot accurately compute the results of a query, it must forward the query to other sites. This paper considers the problem of selecting the documents indexed by each site focusing on replication to increase the fraction of queries processed locally. We propose RIP, an algorithm for replicating documents and posting lists that is practical and has two important features. RIP evaluates user interests in an online fashion and uses only local data of a site. Being an online approach simplifies the operational complexity, while locality enables higher performance when processing queries and documents. The decision procedure, on top of being online and local, incorporates document popularity and user queries, which is critical when assuming a replication budget for each site. Having a replication budget reflects the hardware constraints of any given site. We evaluate RIP against the approach of replicating popular documents statically, and show that we achieve significant gains, while having the additional benefit of supporting incremental indexes.
Categories and Subject Descriptors
H.3.3 [Information Storage Systems]: Information Retrieval Systems
General Terms
Design, Experimentation, Performance
Keywords
Multi-site web search engine, distributed index, replication
1. INTRODUCTION
Distributed search engines aim at horizontal scalability for Web search [2, 6]. The search engine is distributed over
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

several search sites deployed in (smaller) data centers spread across the world. Each search site only indexes a fraction of the documents, and receives the queries of the users in its region. When a query cannot be answered locally, it is forwarded to the other sites in order to compute accurate responses [2, 8]. Distributed search engines share the workload among several sites. Thus, deploying an extra site in a new data center increases the capacity of the search engine.
Designing an efficient distributed search engine is a challenging task. Traditional search engine algorithms rely on all the resources being accessed within a single data center. However, in the case of distributed search, the index is split among several distant locations. Accessing data in another search site translates into a higher response time due to network latency. Ideally, a site processes most queries locally to offer low response time and avoid the cost of processing the query in several sites. To enable this property, a careful selection of the documents to index on each site becomes necessary to guarantee that most frequently requested documents can be retrieved locally. A trivial solution is of course to replicate all documents across all sites. This solution, however desirable from a latency perspective, induces a high utilization of computer resources. Consequently, an important goal is to reduce resource utilization while providing low latency to end users through local query processing.
Brefeld et al. use machine-learning techniques to assign each document to one or multiple sites [4]. Their approach, however, does not consider the popularity of documents or a resource budget for each site. The popularity of documents becomes important when having to selectively drop documents due to resource constraints. Following a different approach, Blanco et al. propose to assign each document to a single site, and later rely on user activity to proceed to documents replication [3]. Their method relies on terms distribution and cache invalidations of a document to predict at which search site it is most likely to be requested. However, they do not propose any replication strategy.
Upon receiving a query, a search site processes it against its local index and then estimates whether other sites may have better documents, in which case the query is forwarded. Several heuristics have been developed for estimating the quality of documents in different sites. They rely on some partial knowledge of the content of the other search sites, either per-term score thresholds [2], or the scores of previously processed queries [8]. These algorithms are conservative: they can generate false positives, but no false negatives. Hence, the results computed by the distributed search engines are always the same as the ones generated by a single-

831

Site Si

Crawler

MIi

SIi

FIi

Local query processing
Query

Master site
selection

full replication
Index replication service

partial replication
Distributed query processing

Query forwarding heuristic
Results

MIj

SIj

FIj

User

Site Sj

Figure 1: Architecture of a distributed search engine

site search engine. While these two approaches achieve a reasonable precision, they only target static indexes and require offline computation.
Contributions. We make the following contributions in this paper:
∑ We propose RIP (Reactive Indexing Protocol), a practical algorithm to selectively replicate documents and posting lists across sites in a distributed search engine. The algorithm makes decisions online and locally;
∑ RIP uses document popularity and user queries to select documents to replicate. Such information is important to ensure locality while respecting resource constraints;
∑ We evaluate the algorithm and compare it against a baseline that replicates documents statically, as well as a reactive documents replication algorithm. We show that RIP increases query locality by 23% while only increasing the index size of each site by 14%.
Roadmap. The remainder of this paper is organized as follows. We describe the architecture of the distributed search engine in Section 2. In Section 3 we present RIP, and evaluate its performance in Section 4. Finally, we review related work in Section 5 and conclude with Section 6.
2. ARCHITECTURE
We consider a distributed search engine comprising a set of search sites S deployed across geographically different regions. The collective of the sites forming the search engine indexes a document collection D. We present a global view of the architecture of the distributed search engine in Figure 1, and describe the elements composing it throughout this section.
2.1 Traditional search engine architecture
The architecture of a search engine is typically divided into three components. The crawler fetches documents from the Web and discovers new content by following hypertext links. The indexer processes D, the collection of documents, to generate an inverted index. For each term t present in the collection, the inverted index contains a posting list, i.e., a list of the documents that contain t. A popular technique for implementing an index is incremental indexing. Incremental indexing enables the addition, deletion, or update of indexed documents without fully regenerating the index. This fea-

ture is particularly important in the case of large scale Web search engines where the cost of regenerating the full index to update frequently modified Web pages (e.g. news) is prohibitive. Incremental indexes offer a good trade-off between the freshness of the documents and the processing performance. Hence, in this paper, we consider the case of an incremental indexer. The query processor receives user queries and processes them against the index. Historically, commercial Web search engines have preferred conjunctive query processing [14]: a document has to contain all the terms in the query to be in the result set. To obtain the result set, the query processor computes the intersection of the posting lists of the query terms, evaluating the scores of the documents using a ranking function. The k (typically 10) results with the highest scores are returned to the user.
2.2 Assignment of documents to sites
In a distributed search engine, each search site has its own index and processes the queries of the users in its region. It is therefore important to carefully select documents to index in each site to tailor the data structures of the sites for their users. For a short response time, a search site must be able to process most of the queries it receives using its local index alone. Indexing documents that are popular in a region enables locality. However, it is also important to limit the number of documents indexed at each site. The query processing latency in fact increases with the size of the index [5]. Furthermore, replicating a large fraction of the documents at each site limits the scalability of the search engine. In this work, we assume that each search site has a fixed index capacity, either due to limited resources, or arbitrarily chosen to reduce the processing time. We express this limit using number of postings, i.e., the sum of the length of all the posting lists in the index.
To the extent of our knowledge, two approaches have been developed to compute an assignment of documents to search sites. The first one relies on machine learning algorithms to analyze new documents upon their discovery and assigns them to one or more search sites [4]. The second one assigns each document to a single master site [3]. This assignment results in a minimal index, with each document indexed in a single location. The master site of a document is responsible for maintaining it in its index, which guarantees that the distributed search engine has the same recall as a centralized implementation. Our work builds up on the master selection approach [3]. We assume the existence of an algorithm that assigns each document to a single search site. Hence, the search site Si has a master index MIi which indexes all the documents Si is the master of. Typically, this index represents the majority of Si's index capacity.
The popularity of Web pages typically follows a power law: while most of the Web pages are unpopular, a few of them are requested very frequently. A Web page might present high locality, being popular in a single region, or be popular across many regions. Distributed search engines work better in a context where documents have a strong locality. Indeed, this means that each document only needs to be indexed by the search site located in the region where it is popular. Fortunately, a large fraction of the popular Web pages exhibit a high divergence in their popularity across regions [3]. Nevertheless, there are still documents which are popular across region boundaries and are requested by users

832

of different search sites. The master selection algorithm, ensures that each document is indexed by its master site.
In this paper, we propose RIP, a Reactive Indexing Protocol. RIP uses part of the remaining index capacity of each search site to replicate documents that are frequently requested locally, but were assigned to a different search site by the master selection algorithm. Contrary to the master selection algorithm, which only relies on the content of the document, RIP is reactive: it analyzes the behavior of the users at each site to dynamically adjust replication decisions. These fully replicated documents form the shadow index, denoted SIi for the site Si.
2.3 Distributed query processing
In a distributed search engine, a search site indexes locally only a fraction of the documents. To preserve the quality of results, a distributed search engine must generate the same results as a centralized implementation. A query submitted to a site Si must be evaluated on the full set of documents D, whether they are indexed locally (MIi and SIi) or not. So far, the main approach to executing queries in a distributed search engine relies upon query forwarding [2, 8]. When processing a query, a search site first computes results using its local index, and then relies on a forwarding heuristic to determine whether another site may be able to provide higher quality results. If there are such sites according to the evaluation of the heuristic, then the query is forwarded to the relevant search sites for further processing. Finally, the results are then merged and returned to the user.
The forwarding heuristic is conservative with respect to query forwarding, and it can generate false positives, but no false negatives. As already mentioned in Section 2.2, it is preferable to answer a query locally, since it reduces the response time. It is therefore important to devise an accurate forwarding heuristic to reduce the forwarding rate.
Existing forwarding heuristics [2, 8] leverage properties of the search engine ranking function to compute an upper bound on the score of documents which are not indexed locally. This ranking function s(d|q), presented on Figure 2, was introduced by Baeza-Yates et al. [2]. The score of a document d is computed by averaging partial scores over the terms of the query q. A document d has a quality score, expressed by f (d), and a relevance score for a term t, computed by g(d|t). The parameters wf and wg weight quality and relevance respectively. The partial score of a document d for a term t, expressed as r(d|t), is typically maintained in the posting lists of the index to improve query evaluation performance. Note that distributed search engines are compatible with more complex ranking functions, including positional features, machine learning and diversification. In these cases, the s(d|q) ranking function is used in the first phase of a two-phase ranking [9], which ensures that relevant documents will be known locally before the execution of the second phase of the ranking.
The heuristic we propose here supports the same ranking function. A site Si may partially replicate the posting lists of the master indexes of other sites in order to estimate the need to forward queries. This forwarding index is designated as FIi. The posting lists of forwarding indexes are ordered by partial score. For a given term t, Si replicates the list of documents that have the highest partial score r(d|t). A document whose master is Sj and that contains several

|q|

s(d|q)

=

wf f (d)

+

wg |q|

g(d|ti)

i=1

r (d |t) = wf f (d) + wgg(d|t)

|q|

1

s(d|q) = |q|

r(d|ti)

i=1

Figure 2: Ranking function

terms may only be indexed in one of the posting lists of FIi, provided its other partial scores are low.
2.4 Index structure
In this section, we summarize the different elements of the index of a search site. The index of a site Si is logically divided into three components: Master index MIi: It contains the documents that were assigned to Si by the master selection algorithm. Shadow index SIi: It contains documents that are fully replicated by Si. They were assigned to one of the other search sites by the master selection algorithm, but are replicated to improve the query processing locality. Forwarding index FIi: It contains partial information about documents assigned to the other search sites. For a given term t, the posting list associated to t in FIi contains the list of documents that have the highest partial scores for t. Documents having high partial scores are also likely to have a high popularity. Hence, SIi and FIi may overlap.
These logical indexes represent the data available to Si for processing queries. The posting lists of search engines are, in most cases, ordered by document ID. This allows high compression rates, and is particularly efficient for conjunctive query processing [14]. In this work, we do not make any assumption on the layout of MIi and SIi. However, we present FIi as an index sorted by impact. This means that the postings are ordered by partial scores. This assumption simplifies the description of the algorithm, but there is, in practice, no reason not to implement these three indexes as a single incremental index relying on document ID ordered posting lists.
3. REACTIVE INDEXING PROTOCOL
3.1 Problem definition
We consider the following problem. In a distributed search engine, each search site is assigned an index budget expressed as the maximum number of posting lists entries a site can accomodate. The collection of documents D is split across the search sites through an initial master index selection algorithm. The remaining index capacity of each site is freely used to replicate documents and posting lists of the other sites. A sequence of queries is submitted to each search site and the goal is to maximize the amount of queries that are answered locally, without query forwarding. Consequently, a search site updates its index as it processes queries. A search site modifies its index only between query executions, and it does so for a given query only after its results are returned to the user.
Let res(q) be the set of the k documents obtaining the highest scores for the query q according the the search en-

833

gine's ranking function s(d|q). A site Si has to fulfill two conditions to answer q locally. First, the documents of res(q) must all be indexed and copied locally. The search site needs a copy of the document data to generate the snippet presented on the results page, and also in the case that it uses using a two-phase ranking [9]. The search site also needs to be able to compute accurate scores for all the top-k results to display them properly ranked. This requirement can be expressed as follows:

d  res(q), d  MIi  d  SIi .

Second, Si should be able to determine, using local data structures, that no other document could potentially score higher than the lowest score of the results:

d  (D - res(q)), e  res(q), scBound (d |q)  s(e|q) ;

where scBound (d |q) is the function that computes an upper

bound on the score of the document d for the query q using

only local information, i.e. the information from MIi, SIi

and FIi. We estimate the future queries Qfi of Si using Qi, a recent
query stream received by Si. Thus, at any given point, we

are trying to maximize the locality of the queries in Qi to

increase the probability that future queries will be answered

locally. Let us focus on the first locality condition. The cost

of replicating a document is equivalent to the number of

posting list entries it requires in the index. To simplify the

problem, suppose that all documents contain the same num-

ber of terms and therefore have the same cost. The problem

we are trying to solve is a particular form of the knapsack

problem. The objects we are selecting are queries. The util-

ity of selecting a query is proportional to its frequency, while

its cost is equal to the indexing of the results, as well as the

partial information ensuring the quality of results.

Given that the search engine aims at serving the k best

results for each query, we could make the simplifying as-

sumption that all queries have the same cost: indexing k

documents. However, even in this case, the complexity of

the problem arises from the fact that many queries share re-

sults. Hence, the cost of a selection is not equal to the cost

of each query, as some documents would be counted several

times. The knapsack problem is N P -hard, but has greedy

heuristics that perform reasonably well. In particular, the

most common approach consists of selecting the objects in

a

decreasing

order

of

value cost

.

In

our

case,

the

cost

of

selecting

a query depends on the previously selected ones; hence the

costs should be re-evaluated at each step of the algorithm.

3.2 Practical approach
As presented in Section 3.1, a search site has to satisfy two conditions to answer a query locally. The first one is that the results should be indexed locally, and the second one is that the search site should have enough information to guarantee that no other document in D can score higher. The computation of an exact optimal solution is NP-hard, which leads us towards heuristic solutions. Moreover, we need to consider practical implementation constraints in the design of our algorithm.
This work targets search engines performing incremental indexing. The index of the search engine is regularly updated, and the algorithm must account for the presence of new documents. This eliminates the possibility of relying solely on an offline algorithm executed during the initial in-

dex generation. A practical solution should also use a minimal amount of computation and memory, so as to ensure that as many resources as possible are dedicated to query processing. Part of the computational cost of the problem presented in Section 3.1 arises from the fact that the benefit of replicating a document cannot be simply evaluated, it depends on the full selection of replicated documents. From this consideration, it would be tempting to devise a solution based on hypergraphs of documents and queries to model replication dependencies. However, given the scale of the document collections we consider, such data structures do not scale, both with respect to their memory consumption and the processing cost required to exploit them.
Inspired by previous work in Web caches [13], we propose a Reactive Indexing Protocol (RIP). Each search site engine monitors the queries of its users to gather local statistics about the frequency of terms and documents. Based on these observations, our algorithm evaluates the utility of replicating information. A search site Si may either replicate documents, to ensure that the results are copied locally, or fragments of the posting lists of other sites, to increase its knowledge of their document collection and make more accurate query forwarding predictions by computing tighter score bounds. As introduced in Section 2.4, Si indexes fully replicated documents in SIi, while the replicated fragments of posting lists form FIi.
3.3 Algorithm
Distributed search engines rely on query forwarding heuristics to determine whether a given query q should be evaluated on other search sites to improve the quality of the results. The role of the forwarding heuristic is to compute, for all documents d  D, a score upper bound scBound (d |q). If the top-k documents are either not fully replicated locally, or cannot be clearly identified, the search engine decides to forward the query to the other sites to guarantee the quality of results. In this work, we introduce a new query forwarding heuristic and RIP, its associated index replication algorithm.
3.3.1 Forwarding heuristic
The forwarding heuristic we propose stems from the NRA top-k processing algorithms [11], with a few adaptations to deal with incomplete posting lists. In NRA, posting lists are sorted by impact and processed from top to bottom. NRA maintains a sorted heap of potential top-k results with upper and lower bounds on their scores. These bounds are updated as the processing progresses down the posting lists. As soon as the upper bound of the (k + 1)th document is lower than the lower bound of the kth document, the topk results are identified and the algorithm terminates. In the worst case, NRA has to process the full posting lists, but, in most situations, it achieves significant performance gains and only processes a small fraction of the index. The forwarding heuristic performs a similar computation. It processes the query over the forwarding index FIi and computes upper bounds on scores. The posting lists of the forwarding index are only partial, but they are continuous. A posting list replicated by Si for a term t down to the score value v contains all the documents of D whose master is different from Si and whose partial score r(d|t) is higher than v. Therefore, for a given term, FIi provides either an exact partial score, or an upper bound equal to the score of the last posting list entry. While processing, the forwarding heuristic

834

t1
d238 - 24.5 d789 - 24.2 d555 - 23.1 d358 - 22.8

t2
d657 - 18.3 d745 - 17.9 d555 - 17.3 d618 - 17.0 d194 - 16.7

t3
d675 - 17.1 d348 - 16.2 d135 - 14.9

Figure 3: Forwarding heuristic on FIi

ignores the documents present in the shadow index SIi, as they are already evaluated by traditional query evaluation and are assigned a precise score.
We illustrate the forwarding algorithm with the example of Figure 3. The query of the user is "t1, t2, t3", and the figure displays the posting lists of FIi corresponding to those terms which the forwarding heuristic evaluates to decide whether the query should be forwarded. The top documents for t1 are replicated in SIi (in bold), so they do not need to be considered. The following document is d555, so we know its exact partial score for this term. This document is also present in the posting list of t2, so we will also find its exact partial score for t2 as the top-k execution progresses. However, d555 is not represented in the posting list of t3. The last known document of this posting list is d135. As a consequence, we use its partial score as an upper bound of d555's partial score for t3. Hence, the upper bound score computed for d555 is (23.1+17.3+14.9)/3=18.4. We can also compute a bound on the score of any document absent from these posting lists using the scores of the last entries (22.8, 16.7 and 14.9 in this example). Using FIi, the forwarding heuristic computes the highest possible score for a document that is not indexed locally and compares it with the score of local documents (MIi and SIi).
As an optimization, when a posting list is fully replicated (i.e. replicated down to the 0 score), the forwarding heuristic leverages the conjunctive properties of the ranking function. Any document that is absent from this posting list can be ignored, as it cannot be part of the results.
3.3.2 Replication principle
The replication algorithm works as follows. For each term t, a site maintains two replication thresholds, expressed in partial score values: the document replication threshold tdt and the postings replication threshold tpt. RIP reactively adjusts these thresholds using the activity of the local users to determine which documents and postings are replicated.
d  D, r(d|t)  tdt  master (d ) = i  d  SIi
d  D, r(d|t)  tpt  master (d ) = i  d  FIi
For the example described on Figure 3, tdt1 is 24.2, while tpt1 is 22.8. By lowering tdt, RIP decreases the highest scores associated to t for a non local document. Lowering tpt decreases the lowest score associated to t in FIi. Both these actions increase the information related to the term t and decrease the amount of query forwarding. However, their impact and cost can vary significantly. Fully replicating a document is costly, as it generates one posting entry per unique term in the document. On average, a Web page contains 250 unique terms [15], therefore replicating a document

is 250 times more costly than replicating a posting entry. Given that the differences in partial scores between entries are, in most cases, higher among high quality documents, fully replicating a document often has a higher positive impact on query forwarding. RIP's objective is to achieve a good balance between documents and postings replication to use the replication budget as efficiently as possible.
After each query execution, RIP analyses the query results to determine which data should be replicated to ensure that, in the future, this query could be processed locally. Let w be the lowest score of the last document returned as a result for the query t1. . . t|q|. If the query only contains one term, then the replication operation is trivial, and the algorithm determines that tdt1 should be w. However, if the query contains several terms, then the algorithm has to decide whether it should replicate documents or posting lists. The algorithm we propose relies on a parameter  to balance the replication between documents and postings.

tdt =  ◊ |q| ◊ w

(1 - )|q| ◊ w

tpt =

|q| - 1

Using the scoring function s(d|q), it is possible to verify that for all the documents present in a single posting list of FIi, the forwarding heuristic has enough data to compute a score upper bound at most equal to w:





1

t  q, |q| tdt +

tpu = w

uq-{t}

By definition, replicating document provides the corresponding postings, so tdt  tpt. As a consequence, given that |q|  2,   0.5. When  is low, RIP favors replicating documents, which increases the probability of having query results in the local index. However, the forwarding heuristic has less information to ensure that these local results are optimal. On the contrary, a high value of  provides a very accurate forwarding heuristic, but fewer replicated documents.
In practice, some of the documents are present in several posting lists. Hence, they have precise values for several terms, and their score estimations may exceed w. The results of the query, for instance, will be present in all the posting lists matching the query, and will generate scores higher than w. Similarly, other documents present in at least 2 posting lists, such as d555 in Figure 3, may have high upper bounds on their score and could trigger the query forwarding mechanism. When these documents are not part of the query results, query forwarding is unnecessary. In order to avoid these cases of false positives, RIP identifies these documents and fully indexes them in SIi.

3.3.3 Practical algorithm using blocks
RIP needs to estimate the amount of documents or postings a replication decision represents before deciding whether it should be applied or not. Furthermore, taking replication decisions at the level of a single posting may lead to unstable results and generate a high overhead.
Each search site estimates loosely the score distribution for each term by regularly probing the other search sites. This data structure is comprised of blocks, and is illustrated in Figure 4. A block constitutes a unit of replication identified by its index as well as its score bounds. This information

835

block index/size

t4

0/10 Upper = 15.7, Lower = 12.7

1/20 Upper = 12.7, Lower = 9.8

2/40

Upper = 9.8, Lower = 7.3

3/80

Upper = 7.3, Lower = 4.8

t5 Upper = 17.1, Lower = 15.3 Upper = 15.3, Lower = 13.7 Upper = 13.7, Lower = 6.4
Upper = 6.4, Lower = 1.8

Figure 4: Blocks replication (k = 10)

is not required to be perfect, and can be obtained through sampling. The first block has size k, and the size of the following blocks increases exponentially, using a power of 2.
We adapt RIP to apply the replication thresholds tdt and tpt on blocks of documents and postings instead of single elements. Figure 4 presents the computation of the thresholds for the query "t4, t5". The lowest score w = 8.5, and  = 0.6. RIP determines that tdt4 should be 0.6 ◊ 2 ◊ 8.5 = 10.2. Therefore, the first 2 blocks of documents should be replicated (in bold), and dtt4 becomes 9.8, once adjusted to the block limit. tpt5 is evaluated to be (1-0.6)◊2◊8.5/(2-1) = 6.8, which means that 3 blocks of postings should be replicated (in italics) and tpt5 is adjusted to 6.4. This operation is then repeated for tdt5 and tpt4 .
The usage of blocks brings several benefits to RIP. It materializes a small number of well defined replication bounds, which favors clear replication decisions while lowering the amount of memory required to maintain them. In addition, it ensures the continuity of replicated data. A replication decision caused by a given query may, as a side effect, trigger the replication of blocks that contain documents useful for other future queries.
3.3.4 Space management
RIP does not directly proceed to the replication of data. Instead, it maintains counters about the number of times a particular piece of information was determined to be useful. We refer to these counters as the temperature of the data. The higher the temperature, the more useful it is. The temperature values are used to determine which data should be replicated within the budget constraint. As explained previously, depending on the data type, the cost of the replication varies. Given that a document has on average 250 distinct terms, the cost of replicating a document is on average 250, the cost of replicating the documents of a block of size n is on average 250n, and the cost of replicating only the postings of this block is exactly n.
We rely on the cubic selection scheme [13] to decide which elements are selected for replication. This approach was initially designed to cache Web objects. Hence, in this context, the size of an object is always precisely known, and there are no dependencies between objects. In our case, the size of an object is first estimated, and then corrected when a replication decision is actually taken. For instance, RIP first assigns a cost of 250 to a document, and then corrects it upon receiving the content of the document. Similarly, evicting a block whose documents are replicated does not necessarily free 250n, as some of the documents may remain replicated due to other decisions (e.g. replicated blocks of other terms). We also ensure that the continuity of block replication is maintained. It is impossible to evict a block without evicting all the following blocks.

3.3.5 Dealing with incremental indexing
Web search engines are often designed to support incremental indexing. This feature is particularly used for popular Web pages that are frequently modified, such as news Websites. Replicating information in several locations poses the problem of data consistency. If a search site does not take into account a new version of a Web page, it will serve stale results to its users.
To ensure that index updates are propagated, a search site keeps track of which other sites replicate the documents it is the master of. In addition, for each term, it maintains the documents and postings replication thresholds of each one of the other sites. When the crawler sends a new version of a document to its master site Si, MIi is updated and all the sites replicating any stale data are notified.
4. EVALUATION
To evaluate RIP's efficiency, we simulate a distributed search engine configuration consisting of five search sites S = {S1 . . . S5}. We first compute optimal query results through a centralized search engine indexing all documents, and then evaluate each site's ability to generate these results using its master index and the data it replicates. Each search site is associated with a query log originating from neighboring countries and collected from the front-end of a commercial search engine. In total, we sample 7, 023, 102 consecutive queries, and split them chronologically into a training set and a testing set of equal size. The collection of documents consists of 31, 599, 910 Web pages randomly sampled from the index of the same search engine.
We rely on the distribution of terms in queries and documents to assign each document to a master site (KL-q feature), as in the work of Blanco et al. [3]. This process creates, for each site, the master index MI, and represents the initial assignment of documents to sites in our evaluation. In this work, the master index creation process is fixed, and these indexes remain constant throughout the experiments. We evaluate RIP's ability to generate SI and FI, and to reduce the query forwarding rate.
Commercial search engines often cache query results to avoid re-evaluating repeated queries. To make our experiment more realistic, we assume that the search engine implements a results cache with a Time-To-Live (TTL): cached results are only served if their TTL has not expired. We use a TTL value of 2 hours. For incremental indexes, it is possible for a results cache to return stale results, which happens when the result set does not reflect accurately the current state of the index. In our setup, the TTL of the cache is 2 hours. Such a TTL value is moderately aggressive: the staleness of the results will never exceed 2 hours.
4.1 Search results diversity
Blanco et al. [3] observe that most documents exhibit a high divergence of popularity among the different search sites. They are very popular in a given region, but are rarely requested at other search sites. We confirm this observation by evaluating how the popularity ranking of documents differs across search sites. We execute the training queries on the search engine and count how often each document is returned as part of the top-10 results.
In Figure 5, we present the similarity between the list of documents that are most popular at each individual search site, and the documents that are globally popular, among

836

Overlap

0.6 0.55
0.5

SSSSS23451

0.45

0.4

0.35

0.3

0.25

0.2

0

20

40

60

80

100

Popularity of documents (in %, from most popular to least popular)

Figure 5: Similarity between documents locally popular and documents globally popular

Proportion

1

query length before cache

query length after cache

0.8

cache hit rate

0.6

0.4

0.2

0

1

2

3

4

5+

Query length

Figure 7: Queries distribution and cache efficiency

Overlap

0.4 0.35
0.3 0.25
0.2

SSSSSSSSSS1112223341

/ / / / / / / / / /

SSSSSSSSSS3453454552

0.15

0.1

0.05

0

0

20

40

60

80

100

Popularity of documents (in %, from most popular to least popular)

Figure 6: Similarity between locally popular documents at 2 sites

all the search sites taken together. On average, the set of the x documents most popular at a given site and the set of the x documents most popular considering the activity of all sites globally only overlap by 40%. This observation clearly illustrates how users from different regions have diverse interests. This argues in favor of an index replication policy that relies on local user activity and optimizes the replicated data for each site individually.
The comparison of popular documents between pairs of sites, presented in Figure 6, shows even more differences. The pair of sites exhibiting the highest similarity is under 40%. This similarity is due to the language used in queries, as those two regions share the same regional language. Note that the similarity of the very few most popular documents is typically higher. This is due to very few documents being popular across different regions.
4.2 Cache impact
The results cache generates a hit when identical queries are submitted to the search engine within its TTL interval. Longer queries typically present lower frequency compared to short ones [1], and therefore the results cache affects them differently. In our experiments, the average hit rate of the results cache is 48.4%. Figure 7 illustrates the distribution of queries depending on their length. Queries of length 2 represent the largest fraction of the workload, followed by single term queries. As expected, the hit rate of the cache drops

as we increase the length of queries; long queries are more likely to be unique. Hence, the workload of a search engine is often dominated by the processing of longer queries. This observation significantly hardens the task of the forwarding heuristic. Indeed, long queries involve more posting lists, and generally have results whose partial scores are lower: they require more replicated data to be indexed locally.
4.3 Replication and query forwarding
We evaluate RIP and its query forwarding heuristic by running queries in our logs against the collection of documents. First, we warm up the replication algorithm and the cache using the training queries. Next, we process the testing queries to evaluate the amount of query forwarding. For these experiments, we use the forwarding heuristic of Section 3.3.1 and one of the following replication schemes: Static global documents replication (SDR): Using the training queries, we determine which documents are globally popular and replicate this static set across all sites [2, 8]. In this case, FI contains an upper bound per term for non replicated documents, which corresponds to the thresholds of Baeza-Yates et al. [2]. Reactive documents replication (RDR): Each search site reactively determines which documents are most frequently part of the results of their users and replicate the most popular ones. This setup computes a different set of replicated documents for each site to match the activities of their users. As with SDR, FI contains one bound per term, dynamically adjusted to reflect document replication. Reactive Indexing Protocol (RIP): Each site reactively replicates blocks of documents and posting lists, as well as individual documents when they generate false positives. This is the approach we describe in Section 3.3.3. Each site replicates data matching the needs of its users, while preserving the continuity of information in posting lists.
Overall performance.
We evaluate the three different replication schemes using different replication budgets and present the results in Figure 8. The budget is represented as the maximum number of documents replicated, and we assume, as explained in Section 3.3.2, that 250 individual posting list entries use the same space as a fully indexed document. The results clearly indicate that the two algorithms based on local information outperform the static documents replication us-

837

Proportion of queries executed locally

0.61 0.6
0.59 0.58 0.57 0.56 0.55 0.54 0.53 0.52 0.51
0.5 100

SDR RDR RIP

1000

10000

100000

Replication budget (in documents)

1e+06

Figure 8: Query locality wrt replication budget

Prop. ans. locally, ignoring cache hits

0.6 0.5 0.4 0.3 0.2 0.1
0 1

RDR (budget=10,000) RIP (budget=10,000) RDR (budget=1,000,000) RIP (budget=1,000,000)

2

3

4

5+

Query length

Figure 9: Query locality wrt query length

ing global statistics. Indeed, as observed in Section 4.1, the users of each site are interested in different documents.
For a low replication budget, below 100,000, we observe that simply replicating the results of the queries is more efficient than replicating blocks of documents and posting lists. However, as the budget increases, the blocks replication scheme clearly outperforms the replication of individual documents. This difference grows with the amount of space dedicated to replication. With a replication budget of 1,000,000 documents, each search site has an average indexing capacity of 7,119,982 documents (22.5% of the total collection), which represents an overhead of 14% over a setup without any replication. In this configuration, RIP raises the amount of queries processed locally by 23%, while RDR raises it by 13%. Note that the  parameter of RIP, used to compute replication thresholds, is set to 0.6. In practice, any value between 0.55 and 0.65 obtains good performance, above 59.7% with a budget of 1,000,000.
Detailed performance analysis.
We detail the performance of RDR and RIP in Figure 9. With a small replication budget, it is most efficient to focus replication on single term queries. They only require little replicated data to be answered locally, as the results are simply the documents with the highest scores for the query term. RDR performs well for these easy queries. Given that one-term queries are less likely to be unique, the temperature of their results increases over time and they are replicated. However, when the replication budget increases, it becomes more interesting to also replicate data for longer queries. The results show that RDR is unable to answer these queries, even with a large budget. The documents that are part of the results may be replicated. However, given that the corresponding posting lists are not replicated, the search engine is unable to ensure that the query results are optimal, and the forwarding heuristic returns a false positive. RIP can compute low thresholds, even for longer queries, and is able to answer locally over 10% of long queries by replicating continuous blocks of documents and postings.
Query replication cost analysis.
Figure 10 presents the position of the query results in the posting list blocks of RIP's forwarding index, depending on the query length. For a one term query, the result set comprises the documents with the highest partial scores for

Proportion of results (cdf)

1

0.9

query length = 2 query length = 3

0.8

query length = 4

query length = 5+

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

2

3

4

5

6

7

8

9 10

Posting list block number

Figure 10: Depth of results in posting lists

the term. Hence, they are all located in the first posting list block, which is a small amount of information for RIP to replicate to answer these queries correctly. However, as the length of the query increases, the matching documents are less frequent, due to the conjunctive nature of the query processing. As a consequence, they are located in deeper blocks, and require more replicated information to enable local processing. For example, 67% of the results of 5-term queries are located in the 10th block. Given that the size of blocks is a power of two, this data is costly to replicate, making forwarding more frequent for long queries.
New queries.
The replication algorithms rely on previous queries to compute a replication scheme and increase the probability of answering future queries locally. When a query is repeated, it can be answered by the results cache, if it falls within the TTL, or by the data replicated upon the first occurrence of the query. New queries however are more challenging. We examine the query processing locality for new queries on Figure 11, with a replication budget of 1,000,000.
SDR is particularly efficient at processing new one-term queries locally, since it benefits from document popularity information from all search sites. A query that is processed for the first time in a site might have been present at another site during the training period. Consequently, the static replication has included it in the computation of the list of replicated documents.

838

Proportion answered locally True Positives

0.5 0.45
0.4 0.35
0.3 0.25
0.2 0.15
0.1 0.05
0 1

SDR RDR RIP

1 SDR RDR
0.8 RIP

0.6

0.4

0.2

2

3

4

Query length

0

5+

0

0.2

0.4

0.6

0.8

1

False Positives

Figure 11: Query forwarding rate for new queries

Figure 12: Forwarding ROC curve

RDR has higher overall performance than SDR, since it targets the set of replicated documents for each site. However, since it only relies on the queries processed at a given site to build the list of replicated documents, the performance for new queries is low. New queries can only be processed locally if their data has been replicated due to previous distinct queries executed at this site. As RDR only replicates the documents in result sets, it is unlikely that the site has replicated all the results of the new query and the data necessary to ensure that local results are sufficient.
RIP performs well for new queries. Although it only relies on local knowledge, as it is the case for RDR, the block replication pattern favors new queries. Instead of precisely replicating the documents answering a particular query, RIP transfers blocks of data, which contain additional documents and postings related to the query terms. When a new query arrives, the algorithm is more likely to have replicated a sufficient amount of information for each of the query terms, which increases the probability of processing it locally.
False positives analysis.
Our forwarding heuristic always forwards a query if there another search site may improve its results. As a consequence, the forwarding heuristic does not generate false negatives. For some queries, the search site has the query results in its index but forwards the query nevertheless, because it cannot prove that those are the best results. These cases constitute false positives, as forwarding the query does not modify its results. Considering a setup with a replication budget of 1,000,000, the proportion of false positives among the forwarding decisions is 51%, 53% and 46% for SDR, RDR, and RIP respectively.
Allowing the search engine to return non-optimal results by allowing false negatives reduces the query forwarding rate. The forwarding heuristic computes a score that corresponds to the limits of its knowledge. We use this score, as well as the fact that long queries are more difficult to predict, to rank forwarding decisions. We display, on Figure 12, the distribution of true and false positives (ROC curve [12]). As one-term queries only involve one posting list, making it impossible to generate a false positive, we ignore them in this experiment. The remaining query lengths generate distinctive curve fragments. These fragments remain relatively close to a diagonal, which means that separating true and false positives using solely the knowledge score and query

length is difficult. Indeed, scores can vary significantly depending on the query term, and cannot be easily compared. Note that the curve representing RIP is bellow the one of the other algorithms. This is because RIP presents an overall higher performance, and therefore forwards fewer queries. The remaining false positives are therefore harder to detect.
5. RELATED WORK
The problem of assigning documents to sites in distributed search engine has been studied in previous work, and two main approaches have been developed to assign documents to sites. Baeza-Yates et al. propose to replicate a set of global, high quality documents to all search sites [2]; Cambazoglu et al. follow the same approach [8]. As shown in Section 4.2, the documents users are interested in significantly differ across regions. Thus, it is more efficient to perform fine grain replication, and select different replicated documents for each search site. Our experiments presented in Section 4.3 show that per-site replication algorithms systematically outperform global replication decisions.
Brefeld et al. propose to use machine-learning techniques to statically assign documents to search sites [4]. This approach optimizes the replication of documents for each site, but it relies on attributes, such as the language of the document, which are weakly correlated with its popularity. Contrary to this approach, the algorithm we propose is reactive, and only replicates documents when it observes that users actually request them. The algorithm we propose also explicitly prioritizes pieces of replicated data depending on their size and temperature. The approach of Brefeld et al. relies on an algorithm that simply assigns documents to search sites, and it does not enable an operator to vary the indexing capacity.
Existing query forwarding algorithms rely on the computation of upper bounds to estimate the score of documents that are not indexed locally. Baeza-Yates et al. [2] compute a bound for each term and each search site. Cambazoglu et al. [8] refine this approach and show that maintaining bounds on pairs of terms more precise estimates of scores and can reduce the amount of query forwarding. In the case of a static index, these bounds can easily be computed during the generation of the index, and do not vary at runtime. However, when the index and the set of replicated documents are dynamically selected, these bounds need to be updated. This process can become particularly costly when

839

each site keeps track of many bounds, which is the case in the algorithm of Cambazoglu et al. [8]. In that case, one possibility is to maintain these bounds lazily using a sliding window scheme. Yet, it reduces the effectiveness of the results cache, since results computed using these bounds will be time-stamped with the oldest bound used during their computation. The approach we propose explicitly maintains two bounds per term, which is a reasonable trade-off between their accuracy and the maintenance cost.
The main difference between the algorithms we propose and previous work [7, 8] is the interaction between the replication algorithm and the forwarding heuristic. While other algorithms focus on replicating popular documents, our approach also maintains single posting lists elements to ensure the continuity of the information on the scores for a given term. As there is no "hole" in the forwarding index, the upper bounds for each term are lower. In addition, by replicating postings further, we efficiently lower the bounds computed for longer queries. Overall, this significantly reduces the amount of query forwarding.
Ding and Suel [10] developed an algorithm for fast top-k processing on document ID-ordered posting lists. By maintaining upper bounds on the partial scores of compressed posting list segments, they significantly reduce both the amount of computation and the processing time. Our approach for block replication could be adapted to replicate document ID-ordered blocks using these thresholds. However, this would lead to a significant overhead for storing the index, since documents obtaining low scores would also be replicated if they belong to a segment with a high threshold. Consequently, we opted for strictly following the order of partial scores to replicate information.
6. CONCLUSION
We propose RIP, a Reactive Indexing Protocol for distributed search engines. With RIP, the search engine initially indexes each document on a single master site, and monitors the local user activity of each site to generate an index replication scheme. Our scheme replicates documents and fragments of posting lists. RIP enables a significant reduction of the amount of query forwarding between search sites, and consequently, of query processing latency. We show experimentally that, by making local decisions, RIP significantly outperforms previous replication strategies based on global information: in a 5-site setup, when each site has an index capacity of 22.5% of the documents collection, 60% of the queries are processed locally. We also show that replicating fragments of posting lists allows the query forwarding heuristic to compute lower score thresholds on unknown documents, which increases performance. Finally, RIP has by design the additional benefit of being an online approach, supporting both incremental indexing and a precise index capacity configuration, which simplifies operations in a production environment.
Acknowledgments
This work has been partially supported by the COAST project (ICT-248036), funded by the European Community. The authors have been also supported by the INNCORPORA Torres Quevedo Program from the Spanish Ministry of Science and Innovation, co-funded by the European Social Fund.

7. REFERENCES
[1] R. Baeza-Yates, A. Gionis, F. Junqueira, V. Murdock, V. Plachouras, and F. Silvestri. The impact of caching on search engines. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 183≠190, 2007.
[2] R. Baeza-Yates, A. Gionis, F. Junqueira, V. Plachouras, and L. Telloli. On the feasibility of multi-site web search engines. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, pages 425≠434, 2009.
[3] R. Blanco, B. B. Cambazoglu, F. P. Junqueira, I. Kelly, and V. Leroy. Assigning documents to master sites in distributed search. In Proceedings of the 20th ACM Conference on Information and Knowledge Management, pages 67≠76, 2011.
[4] U. Brefeld, B. B. Cambazoglu, and F. P. Junqueira. Document assignment in multi-site search engines. In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, pages 575≠584, 2011.
[5] E. Brewer. Lessons from giant-scale services. Internet Computing, IEEE, 5(4):46≠55, 2001.
[6] B. B. Cambazoglu, V. Plachouras, and R. Baeza-Yates. Quantifying performance and quality gains in distributed web search engines. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 411≠418, 2009.
[7] B. B. Cambazoglu, V. Plachouras, F. Junqueira, and L. Telloli. On the feasibility of geographically distributed web crawling. In Proceedings of the 3rd International Conference on Scalable Information Systems, pages 31:1≠31:10, 2008.
[8] B. B. Cambazoglu, E. Varol, E. Kayaaslan, C. Aykanat, and R. Baeza-Yates. Query forwarding in geographically distributed search engines. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 90≠97, 2010.
[9] B. B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt. Early exit optimizations for additive machine learned ranking systems. In Proceedings of the third ACM international conference on Web search and data mining, pages 411≠420, 2010.
[10] S. Ding and T. Suel. Faster top-k document retrieval using block-max indexes. In Proceedings of the 34nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 993≠1002, 2011.
[11] I. Ilyas, G. Beskales, and M. Soliman. A survey of top-k query processing techniques in relational database systems. ACM Computing Surveys (CSUR), 40(4):11, 2008.
[12] F. J. Provost, T. Fawcett, and R. Kohavi. The case against accuracy estimation for comparing induction algorithms. In ICML, pages 445≠453, 1998.
[13] I. Tatarinov. An efficient LFU-like policy for Web caches. Technical report, Computer Science Department, North Dakota State University, 1998.
[14] S. Tatikonda, B. Cambazoglu, and F. Junqueira. Posting list intersection on multicore architectures. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information, pages 963≠972, 2011.
[15] J. Zhang and T. Suel. Optimized inverted list assignment in distributed search engine architectures. In Parallel and Distributed Processing Symposium, 2007. IPDPS 2007. IEEE International, pages 1≠10. IEEE, 2007.

840

Personalized Diversification of Search Results
David Vallet and Pablo Castells
Universidad AutÛnoma de Madrid Escuela PolitÈcnica Superior, Departamento de IngenierÌa Inform·tica
{david.vallet, pablo.castells}@uam.es

ABSTRACT
Search personalization and diversification are often seen as opposing alternatives to cope with query uncertainty, where, given an ambiguous query, it is either preferable to adapt the search result to a specific aspect that may interest the user (personalization) or to regard multiple aspects in order to maximize the probability that some query aspect is relevant to the user (diversification). In this work, we question this antagonistic view, and hypothesize that these two directions may in fact be effectively combined and enhance each other. We research the introduction of the user as an explicit random variable in state of the art diversification methods, thus developing a generalized framework for personalized diversification. In order to evaluate our hypothesis, we conduct an evaluation with real users using crowdsourcing services. The obtained results suggest that the combination of personalization and diversification achieves competitive performance, improving the baseline, plain personalization, and plain diversification approaches in terms of both diversity and accuracy measures.
Categories and Subject Descriptors
H.3.3 Information Search and Retrieval ≠ retrieval models, information filtering.
General Terms
Algorithms, Experimentation, Experimentation, Human Factors.
Keywords
Diversity, personalization, search.
1. INTRODUCTION
Maximizing the total returned relevance in response to an information need has been traditionally the main fundamental principle underlying research and development in the Information Retrieval (IR) field. More recently, strands of research in the community considered other angles of the practical effectiveness of the output of retrieval systems, and the role that diversity, in particular, plays along with relevance in delivering effective value to the users of such systems [5,6,22]. The value of IR diversity is motivated by the uncertainty involved in a single query as the only evidence of the user information need [1,7,15,16]. The assumption is that different users may mean slightly (or even quite) different things by the same query expression. In the absence of further knowledge or observations about the actual user need beyond the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08...$15.00.

explicit query, the goal of the diversity approach is to satisfy as many users as possible with a single result set. More specifically, diversification aims to minimize the number of totally unsatisfied users, trading degrees of satisfaction in exchange for increasing the size of the ≠at least minimally≠ satisfied population. The implicit assumption is that the loss involved in zero vs. small utility is far more significant than the loss involved between degrees of satisfaction. In other words, relevance follows a diminishing marginal returns pattern, where the gain obtained from increased relevance decreases fast with the number of relevant documents [14,21].
This view contrasts with the principles behind personalization. IR personalization takes the same starting point and addresses the same problem as diversity, where a sole query is viewed as an insufficient expression of the full and precise user need. But in contrast with diversification, the personalization approach strives to get further knowledge beyond the query, so as to explicitly tackle and overcome ≠to the degree this is possible≠ the initial uncertainty [9,11,12,18]. The uncertainty is reduced by extracting and exploiting further information from a wide variety of observations and resources, such as session feedback from the user, longterm records of user behavior, user stereotypes, social links, and other sources for user context and interest modeling. While in the diversity approach the system accepts and adapts its behavior to a situation of uncertainty, personalization tries to change this situation by enhancing the system knowledge about the user need. Rather than aiming to satisfy as many users as possible, personalization aims to build, in a way, a sense of who the user is, and maximize the satisfaction for the specific sensed user.
From these considerations, it might seem natural to see diversity and personalization as contradictory approaches. One might understand that by opting for diversity, the adaptation of results to specific users has been implicitly given up on. Conversely, if the query uncertainty is reduced by grasping the user context, the reason for diversification may have been done away with to some extent. In this paper we contend, however, that diversity and personalization are not necessarily incompatible or mutually exclusive goals, but on the contrary they may complement and in fact enhance each other.
Our research is motivated by the understanding that personalization is a process that involves itself a great deal of uncertainty, considerably higher, in fact, than ad-hoc search. The observation of implicit evidence of interest from users involves indeed much higher degrees of ambiguity and incompleteness than regular queries. The interpretation of the input for personalization is an explicit and inherent ≠and considerably difficult≠ part of the personalization task. The same observed contextual cues may fit several plausible interpretations whereby, by the same rationale as for search diversification, diversifying the system's assumptions may help minimize the ratio of severe failures in guessing the implicit user interests. Moreover, the observations of user behav-

841

ior and preferences available to the system usually cover a small fraction of the full range of user interests, whereby context representations are far more incomplete than queries as expressions of user needs. Furthermore, the same as search diversity considers notions of aspects and facets in user queries and needs, user interests are typically heterogeneous, whereby the consideration of user profile aspects for diversification can be as natural as is the introduction of query aspects.
In this paper we develop a generalization of existing diversification approaches, by adding a personalization component into them. Specifically, we introduce a user random variable in probabilistic diversification models previously developed in the literature on IR diversity [1,16]. We describe in detail a full approach to develop the resulting framework into computable components. We report experiments with real users where our framework is compared to partial approaches that apply either personalization or diversification alone. We also compare our approach to a previous approximation to diversity and personalization combined [13]. Our approach compares favorably to all such alternatives, thus providing an empiric validation of our hypothesis.
The rest of the paper is organized as follows. In section 2, we further discuss the possible complementarity between diversity and personalization. In section 3 we propose the new personalized diversity models. In section 4 we present a possible framework instantiation of the model. In section 5, we describe the evaluation framework. Experimental results are presented in section 6, followed by a brief discussion of related work in section 7. Finally, we conclude in section 8 and indicate possible future directions of the presented work.
2. DIVERSITY AND PERSONALIZATION, AN INTEGRATED VIEW
The vision of combining diversity and personalization opens a rich area for research, barely explored to date. Diversity and personalization are complementary and can play together in different ways. The likes and preferences of people typically comprise quite diverse areas of interest, therefore not all of one's preferences should come into play in a specific retrieval task. The activation of the right preference area is critical for the performance of a personalization technique. Areas of user interest are often related to different contexts (work, leisure, time, tasks, situations, etc.). Choosing the wrong area is the typical cause of personalization intrusiveness ≠ the Achilles' heel of personalization: the inferred preferences may be right, but they are applied at the wrong time.
Figure 1. Diversity and personalization as complementary dimensions.
Diversity and personalization may interact and be combined in different ways to enhance each other. For instance, the personalization system may use a diversified selection of user interests, to cope with the uncertainty about which are most pertinent to the current context. Likewise, personalization can improve the effectiveness of aspect weighting in diversification, by favoring query

interpretations which are predicted to be more related to each specific user. Besides interacting with each other, personalization and diversification can of course be directly applied to the search system. Which combination of the three components (diversity, personalization, and search system) is more appropriate may depend on a number of factors, including the characteristics of the query, the system knowledge about the user context, and relations between both. For instance, to the extent that user preferences are unrelated to the query, it may be advisable to apply them to the diversification component, and to little or no extent to the accuracy-seeking component. And so forth.
As a noteworthy example of the combination of diversity and personalization, diversity has been an actively researched topic in the Recommender Systems field in the last decade (see e.g. [19,20,23,24]). Recommendation is a genuine personalized retrieval task, but interestingly, one in which the explicitly query is absent, and only implicit evidence of user interests are available to the retrieval system. To this respect, our research vision is new in relation to prior work in that we deal with the problem of handling user preferences, the diversity dimension, and a query, altogether. Also interestingly, recommendation diversity has only been researched by similarity-based methods, devoid of an explicit representation of user intents (with the exception of [23]). In contrast, we shall consider an explicitly intent-oriented approach.
Figure 1 illustrates the complementarity of diversity and personalization in query-based search. Personalization is useful to focus the scope of diversification within a closer scope to general user interests (area 1 in the figure). Still, a fraction of user-neutral diversity beyond this may be appropriate (area 5). This fraction can be made variable and dependent on the degree of uncertainty in the system's knowledge about user preferences. Similarly, depending on this uncertainty, the diversification of user preferences should be more appropriate or less ≠e.g. diversity of personalization is less appropriate if the area 1 in the figure is much larger than the area labeled 2, and it is more beneficial in the opposite situation.
3. PERSONALIZED DIVERSITY
The approach we investigate here consists of the introduction of the user as an explicit random variable in the diversification models. We take as starting point the original formulations of two representative intent-oriented diversification algorithms, namely IA-Select [1] and xQuAD [15,16], characterized by using an explicit representation of query intents for diversification. Other diversification schemes not explicitly using aquery aspect space [5,6,21,22] might be generalized as well towards personalized versions, which we envision as future work.
As a starting point and general principle in our approach, a user random variable is introduced in every model component as a complement of the query and aspect variables in the representation of user needs. In practical terms, with respect to the original non-personalized methods, the resulting expressions give rise to new and/or additional model terms requiring handling and estimation, as we shall see. We shall deal with and provide approaches for them. On the other hand, the user-based formulation provides for considering independence assumptions at different points, which result in different variants of the framework, standing for different angles on personalized diversification, as we shall see.
3.1 Personalized IA-Select
The IA-Select and xQuAD diversification algorithms greedily rerank search results by maximizing an objective function. Based on the description in [1], the objective function of IA-Select can

842

be expressed as:

 =  ||| 1 - ||





where  is the incremental subset of diversified documents from

the original result set. We generalize this formulation to a person-

alized version by introducing the user random variable:

,  =

=  |, |, |,  1 - |, |, 





In this expression we have three main components involved:

∑ A personalized search system: |, . ∑ The personalized query aspect distribution: |, .
∑ The personalized aspect distribution over documents: |, .

The first component represents a baseline personalized search system. If we only have a non-personalized system, we may develop this term using Bayes' rule as:

|, | ||

|,  =

|

  || =

|| =  || = ,  || (1)

where we have assumed  and  are conditionally independent

given a document, and we assume a uniform document prior. The

expression is thus reduced to the combination of a nonpersonalized search system, represented by | ≠the same as

in the original IA-Select≠, and a pure personalization factor represented by |. The latter factor captures how much user  is

predicted to like document  (regardless of a specific search task),

expressed as how likely it is to observe the document given the

user. This is what a personalization system typically provides for,

based on some mechanism for extracting or learning user prefer-

ences. We shall provide further details for a particular implemen-

tation of the baseline search and personalization components in

our experiments in section 4.

Now the aspect-document distribution component of personalized IA-Select can be developed as:

|, | ||

|,  =

|

  || =

|| =  || = ,  || (2)

where we assume a uniform aspect prior, and conditional inde-

pendence between documents and users given a query aspect.

A natural way to estimate the personalized aspect distribution | is by marginalization over the set of documents:

| =  |, |   ||





where we assume conditional independence between aspects and users given a document.

Finally, for the personalized query aspect distribution |,  several derivations are possible. A convenient one is to develop |,  by marginalizing over the set of documents, because it allows taking advantage of the computation of the two previous top-level components in equations 1 and 2:

|,  =  |, , |,  ~  |, |,  (3)





where we assume the conditional independence of query aspects
and queries given a user and a document. We may also consider |,   | as a simplified option (assuming conditional independence of aspects and users given a document), using the approximations to | discussed in the next section. In appendix A, we offer alternative developments of |, , which might be more suitable to specific frameworks.

3.2 Personalized xQuAD
The objective function of the xQuAD algorithm is formulated in [16] as:
 = 1 - | +  , | =

= 1 -  | +   || 1 - |





In the personalized version of this algorithm the objective function is thus generalized to:

,  = 1 -  |, 

+   |, |,  1 - |,  (4)





This expression involves three main components:

∑ The personalized search system: |, . ∑ The personalized query aspect distribution: |, .
∑ The personalized, aspect-dependent document distribution: |, .

The first two terms were already handled in the previous section (equations 1 and 3). Using Bayes' theorem and equation 2, it can be seen that the third term can be approximated by:
|| |,    || = ,  ||
where ≠as in equation 2≠ we are assuming documents and users are conditionally independent given a query aspect. Again, the estimation of | admits different approaches depending on the available data, an example of which we develop in our experiments in section 4.
Finally, the term | can be estimated in different ways depending on the nature of the query aspect space and available observations. For instance, if  is an Open Directory Project (ODP) category as in [1,14], we may estimate | by a text classification method as the probability that document  belongs to class . Other estimation approaches can be applied for query aspects of a different nature. For instance, | can be computed by calling the baseline search system on  and , as far as  belongs to the space of queries ≠this is the case in [16], where  are related and suggested queries from a search engine.

3.3 Discussion
The personalized versions of IA-Select and xQuAD provide a framework that supports different views on the combination between diversity and personalization. For instance, the summation  |, |,  in xQuAD can be seen as a marginalization over query aspects for a personalized search model |,  (assuming conditional independence of documents and queries given a user and an aspect), that is, a means to diversify user preferences over a query aspect space. This can be seen as a means to tackle the uncertainty about implicit user interests by a diversification strategy, as motivated earlier in section 2.
Conversely, |,  can be seen as an enhancement in the estimation of the aspect distribution | for diversification, by

843

exploiting available information about user preferences for aspects, in order to improve the effect of diversity by making it more user-specific. Furthermore, |, , in the novelty component, enhances the redundancy model by personalizing the utility estimation of seen documents. With slight variations in the formulation, analogous effects can be identified in IA-Select.
On the other hand, the framework flexibly supports different configurations by selectively removing the user variable (i.e. adding user-independence assumptions) from some of its components. This selective removal results into different combinations and views on personalization and diversity. For instance, if we remove personalization from the external baseline system 1 -  |,  in xQuAD, the result is a personalized diversification of a nonpersonalized search system. We may do the opposite in order to have a non-personalized diversification of a personalized retrieval system. At a finer granularity, we may combine a personalized aspect distribution |,  with a non-personalized redundancy component |. And so forth. The variations could even be dynamic and selective, depending e.g. on the reliability of the system knowledge about implicit user interests, the degree of relatedness between recorded preferences and the query at hand, the vagueness of the query, and so forth.
In our experiments, we compare five framework configurations, as we shall see: a) fully personalized diversification, b) nonpersonalized diversification, c) non-diversified personalization, d) a plain (non-personalized, non-diversified) search system baseline, and e) personalization after diversification, the latter in order to compare our framework to a personalized diversification approach reported in prior work [13].

4. FRAMEWORK INSTANTIATION
In this section we introduce a particular instantiation of the proposed personalization and diversification framework. Instantiating our model involves providing estimates for: 1) the personalization component |; 2) the diversity component |; and the document relevance component |. In the following section we offer insight on how to estimate the query relevance component, based on an external search engine, and the personalization component, from a weighted term-based representation of documents and user profiles. In section 4.2, we present estimations more specific to our framework: a social-based representation of documents and users, and the calculation of the diversification component. Finally, section 4.3 addresses a common problem that can arise from the use of external sources to estimate some components, and its implication on the xQuAD approach.

4.1 Model estimations
Similar to e.g. [4], the document generation model can be approximated from a baseline retrieval function by normalizing the score by the sum of all scores of the top k documents being diversified:

, 

|   , 

(5)

where  is the set of documents1to be diversified for query . In our implementation we make use of an external Web search sys-

tem, estimating ,  as a rank-sim normalization [10] of the

baseline retrieval function, that is ,  = 1 - , /, where ,  is the position of document  in the order induced

by the retrieval system scores ,  for   .

1 Delicious, Discover yourself: http://www.delicious.com/

The user preference model described by | can be obtained in many ways, depending on the available document features and user behavior observations. For instance, for text retrieval, we may marginalize | over words:

| =  |, |



   ||/

(6)


where we assume conditional independence of words and users given a document. The following estimates of the resulting distributions were tested in our experiments:

,  |   , 

,  |   , 





 ,    , 

1   ||

where ,  is the term count of term  in , ,  is the term count in the user profile representation, and  is the document collection.

Alternatively, we can estimate the user preference model by an adaptation of the BM25 probabilistic model, similar to the approach presented in [18]:

|

=


:,



,

,  + 

 1

 + 1 -  + avg||||

(7)

where  is the inverse user frequency of term  in the set of users, and || is the size of the user profile calculated as  , . We set  and  to the standard values of 0.75 and 2, respectively. In the following section we show how to imple-

ment this estimate with user and document profiles based on

social annotations.

4.2 Framework-specific estimations
We propose now an instantiation of the personalized and diversification components by exploiting two sources of information. As a source for document and user representations, we use the social bookmarking site Delicious1, which allows the collective bookmarking of Web pages by user generated tags. This collection of tag-user and tag-document relations, commonly named folksonomy, is used in our model to compute the values for ,  and , , where in this case  represents a tag in the folksonomy and the frequency values are, respectively, the number of times a user used the tag in their profile bookmark annotations, and the number of times a tag was used (by any user) to annotate a document. The inverse frequency of a tag,  can be computed by counting how many users have a specific tag in their profile. For the latter calculation, we used a small training collection of 3,000 users, randomly sampled from Delicious. The advantages of using a source of this nature for document and user profile representation is threefold: first, it is a simple solution to having user and document profiles represented in the same term space model; second, folksonomies offer a more concise way of representing content, facilitating the computation of our model; and third, it facilitates the evaluation of our approaches, by providing us with a publicly-available user profiling resource, sparing the need for sophisticated profile learning techniques.
Another key element for our model is the estimation of the diversity component |, which should capture how related a category is to a document. In our experiments we shall follow a content-based approach, by classifying Web pages into the ODP

844

Figure 2. Process of user topic creation, annotation, and evaluation

taxonomy. We first considered a Rocchio-based classification approach, as used by Agrawal et al [1], but after close examination and testing on a development collection, we found it too imprecise to be used as an estimator2. As an alternative, we made use of the Textwise3 ODP classification service which yielded acceptable results on our development collection. The Textwise API returns up to three possible ODP classifications for a document, ranked by a score in 0,1 that reflects the degree of confidence on the classification. We used directly this value as our estimate of |, after normalization
4.3 Component Normalization
Both the original and the personalized version of the xQuAD scheme use a linear combination of two components, one seeking to procure relevance, and the other seeking diversity (see equation 4). The influence of these two factors can be adjusted by varying the  parameter. In order for this linear combination to make proper sense, the two components should be comparable ≠a case of the classic rank fusion problem, which requires a normalization step [10]. Our framework instantiation estimates involve heterogeneous sources and systems, such as an external baseline search system, social annotations, and an external classification service. The use of such third-party tools and resources makes normalization all the more necessary. For this purpose we use a distributionbased normalization approach described by Fernandez et al [8]. Normalization is applied on the two components (relevance and diversity) that are linearly combined, both in the original and the personalized version of xQuAD. The objective function is thus modified to:
,   1   |, 

   |, |,  1  |, 





where  and  are the score normalization functions. The normalizers use a histogram of output values from the functions to

be normalized, sampling from very simple training data (e.g. no

relevance judgments required). The reader can refer to [8] for

more details on the normalization method.

5. EVALUATION FRAMEWORK
A proper evaluation of personalization techniques is difficult to achieve with offline data only, and in our case indeed requires relevance judgments from real users and potentially real Web search topics, as well as the availability of some form of user

profile representation. We used a crowdsourcing service for this purpose, requiring users of this service ≠known as workers≠ to have a social profile in Delicious in order to evaluate a number of search topics. We provide further details on this in section 5.1. 23
We first investigated creating a set of fixed ambiguous topics using Wikipedia's disambiguation pages, which indicate concepts or topics that may have multiple meanings. This methodology was used in previous evaluation schemes in the literature [14,15]. However, after an initial analysis of the worker's feedback, we found that workers were sometimes unfamiliar with the selected ambiguous topics, and were as a consequence unable to provide consistent relevance judgments. As a solution to this problem, we investigated an automatic topic creation methodology, which uses the social profile of the worker in order to find test topics known to the worker. In order to extract topics from the Delicious profile, we adapted to our online evaluation setup an offline evaluation technique initially suggested by Vallet et al. [18].
This topic generation technique creates a test topic from a candidate Web page bookmark, randomly chosen from the worker's Delicious profile. Given the candidate bookmark, a query topic is created, using as keywords the top K most popular annotations (tags) assigned by all users to the same bookmark, thus obtaining a keyword based topic representation related to the bookmark. We ensured that the topics were generated by somewhat popular bookmarks by only generating topics from documents that had annotations from at least five different users.
By following the above process there is a much higher chance that the evaluation topic is known by the user (as it is related to the document saved in his profile). When analyzing the workers' feedback, we found a much more positive response to this type of profile generated topics, in terms of both knowledge of the topic, and user engagement in the evaluation process.
The complete topic and result generation process is depicted in Figure 2. The first step (1) is to generate a topic suitable for the user to evaluate, following the process described above. By setting the number K of popular tags used in the topic generation process, we were able to generate somewhat ambiguous topics, since a larger number of keywords would, in general, generate more specific topics. Given our focus on diversification and
2 Other alternatives such a NaÔve Bayes classifier or a probabilistic estimator were considered, obtaining similar results.
3 Textwise LLC: http://textwise.com

845

Figure 3. Snapshot of the interactive evaluation interface. Evaluation guidelines have been removed due to space restrictions

personalization on ambiguous topics, we randomly generated an equal amount of topics of size K = 1 and K = 2, thus focusing on ambiguous topics, and expecting that the latter size, produces, on average, more specific topics. Section 6 further investigates the effect of distinct topic query sizes. The generated query topic was then sent to the Yahoo! Search API4, from which the top N results were obtained. In our experiments, N was set to 300. We set the location parameter of the search API to the United States, as workers were selected from this region.
In step (2), we extract both social and classification information from each of the documents in the result set. The social annotations were extracted directly from Delicious, by collecting the top 100 most recent annotations given by users. The document topic classification is obtained using the Textwise URL classification service, which returns up to the top 3 ODP most likely categories for each document. The ODP classes were cut down to level 3 in the ODP taxonomy ( Top / Level 1 / Level 2 / Level 3). This annotation extraction and document classification process provides the information needed to compute the personalization and diversification estimates of our studied approaches, respectively. It is worth noting that these two services did not provide a complete coverage over the search results. In particular, the Delicious coverage was 57.5% over the top 300 results and went up to a 69.5% coverage over the top 20 results. Textwise had a coverage of 94.7% and returned an average of ~2 categories per document. In order to cope with the incomplete coverage of Delicious, we added a smoothing factor to the computation of |,  =  |,  + 1 - , with  = 0.5 , so that documents out of coverage would not get a strict zero score.
In step (3), the output of each evaluated approach is used to rerank the Web search results according to personalization and/or diversification features extracted in step 2. The top P documents of each reranking technique are aggregated into a single evaluation list, which is then randomly shuffled and passed onto the evaluation framework. In our experiments, we evaluated the top P = 5 results of each algorithm.
5.1 Crowdsourced Evaluation
Crowdsourcing services (e.g. Amazon mechanical turk5, Crowdflower6) are recently proving to be a valuable tool to optimize the necessary resources to evaluate a wide range of information retrieval tasks [2,3]. These evaluations are based on relevance judgments provided by users of the crowdsourcing service.

Alonso and Baeza proposed a methodology to obtain relevance assessments for TREC related topics [2] using such crowdsourcing services. In their study, it was concluded that workers can provide relevance judgements with comparable quality to those provided by expert assessors. 456
Our evaluation framework, however, has to additionally consider the personalization and diversification factors involved in our research. Regarding diversification, Agrawal et al. [1] required workers to manually classify results over a predefined set of ODP categories, representing the subtopics that could possibly be related to a search query. The goal was to obtain a result/topic relevance assessment that could be used to evaluate a diversification approach in terms of both accuracy and diversification metrics. In our setup, in addition to this type of assessment, we also needed to obtain a personal relevance assessment, i.e., to which degree a search result is preferred subjectively by the assessor. This value allows us to measure the accuracy of the personalization techniques with respect to the real interest of the user. Our evaluation framework is based on prior early work [17], which focused on the acquisition of personal assessments from workers, and has been adapted to include diversification factors. The main difference between our evaluation methodology and the one presented by Alonso and Baeza [2,3] is that our assessments are subjective to the worker, thus relevance assessments cannot be compared across workers, as they evaluate specific topics related to their profile, according to their personal interests and their own notion of relevance to the topic.
In prevention for malicious workers, we implemented a simple technique based on the injection of irrelevant results in some of the evaluated topics, which were then used as quality assessment topics to detect workers that were answering questions randomly. This would not detect workers that mark all results as irrelevant, but a close inspection of the gathered results did not show this behaviour.
We also had a number of prerequisites to access the evaluation interface: 1) as mentioned earlier, workers had to have a valid Delicious account;
4 Yahoo! BOSS API: http://developer.yahoo.com/search/boss/
5 Amazon mechanical turk: http://www.mturk.com/
6 Crowdflower: http://crowdflower.com/

846

Table 1: Diversity metric values for the evaluated approaches. Values in bold indicate the best for each metric. Underlined values indicate a statistically significant difference with respect to the baseline, double underlined values indicate, in addition, a statistical significance with respect xQuAD (Wilcoxon, p < 0.05).

Topic

Topic relevance Topic (K=1)

Topic (K=2)

User relevance

User

User (K=1)

User (K=2)

ERR-IA@5 -nDCG@5 S-recall@5 ERR-IA@5 -nDCG@5 S-recall@5 ERR-IA@5 -nDCG@5 S-recall@5 ERR-IA@5 -nDCG@5 S-recall@5 ERR-IA@5 -nDCG@5 S-recall@5 ERR-IA@5 -nDCG@5 S-recall@5

Baseline IA-Select xQuAD PersBM25 xQuADBM25 PIA-Select PIA-SelectBM25 PxQuAD PxQuADBM25

0.274 0.787 0.500 0.268 0.796 0.493 0.281 0.779 0.262 0.758 0.463 0.251 0.748 0.451 0.274 0.769 0.275 0.793 0.510 0.272 0.806 0.509 0.279 0.778 0.267 0.778 0.486 0.266 0.794 0.490 0.267 0.758 0.272 0.794 0.516 0.266 0.798 0.509 0.279 0.788 0.251 0.742 0.489 0.231 0.723 0.478 0.276 0.767 0.279 0.803 0.543 0.269 0.793 0.515 0.291 0.818 0.275 0.796 0.503 0.268 0.805 0.501 0.285 0.788 0.281 0.810 0.519 0.275 0.818 0.519 0.288 0.802

0.513 0.254 0.626 0.475 0.240 0.592 0.460 0.273 0.670 0.498 0.475 0.237 0.582 0.426 0.211 0.532 0.398 0.268 0.642 0.458 0.511 0.257 0.624 0.478 0.244 0.584 0.459 0.273 0.673 0.502 0.481 0.262 0.646 0.483 0.261 0.631 0.481 0.262 0.664 0.486 0.526 0.258 0.631 0.492 0.244 0.597 0.470 0.273 0.671 0.520 0.503 0.239 0.593 0.481 0.219 0.546 0.457 0.263 0.650 0.512 0.579 0.273 0.656 0.521 0.260 0.613 0.489 0.290 0.710 0.562 0.510 0.268 0.649 0.489 0.255 0.620 0.478 0.286 0.687 0.506 0.520 0.267 0.658 0.496 0.249 0.612 0.460 0.290 0.716 0.542

2) the Delicious account was validated by asking the worker to add a random bookmark to their public profile; 3) the Delicious profile had to be at least 3 months old, contain at least 30 bookmarks, and the 30th most recent bookmark had to be at least 15 days old. In general, the use of the quality assessment topics and our prerequisites for accessing the evaluation interface were enough to avoid misbehaving workers.
Figure 3 shows a snapshot of the evaluation interface. The aggregated output of the evaluated approaches is presented as a search result list to the worker, who has to provide individual assessments by answering three questions:
∑ Q1 (user): a 4-grade scale assessment on how relevant is the
result to the user's interests.
∑ Q2 (topic): a 4-grade scale assessment on how relevant is the
result to the evaluated topic.
∑ Q3 (subtopic): workers assign each result to a specific subtop-
ic related to the evaluated topic.
Q1 provides for measuring the accuracy of the evaluated approaches with respect to the user interest. Q2 is used to evaluate how relevant is the result to the overall search topic ≠a successful reordering technique will place results high that are assessed as both relevant to the topic and to the user's interests. Finally, Q3 provides a subjective classification of the search result to the possible subtopics related to the search topic. The interface is designed to encourage the reuse of introduced subtopics, so as to facilitate workers to provide a more concise categorization of search results. A tutorial was included in the topic description, which was helpful for workers to understand the concepts of subtopics and how this assignment is performed. We decided to take a different approach from Agrawal et al. [1], who used predefined ODP subtopics for result categorization, for the following reasons: 1) we felt that some ODP categories are difficult to understand, often not self-explanatory, and thus can be confusing for workers for annotation; 2) candidate ODP categories need to be automatically selected by the ODP classifier, which could cause difficulties for workers in case of classification errors. In Agrawal et al.'s study this problem was alleviated by having more than one worker evaluating a single topic, but in our personalized setup

topics can only be evaluated by a single worker. After analyzing the workers' assessments and open comments feedback, they seemed to be comfortable with providing their own classification scheme. It is also worth noting that our query generation process produced topics that were known to the worker.
The assessment collection process spanned over a period of four weeks. During this period, we were able to collect information from 35 users that satisfied our prerequisites, collecting assessments for a total amount of 180 topics (median of 4 topics per user) and 3,800 individual results. The user profile and relevance assessment information has been anonymized and made publicly available at http://ir.ii.uam.es/~david/persdivers/.
6. EXPERIMENT RESULTS
Nine different approaches were evaluated with the approach described in the previous section: the baseline system, namely the original ranking returned by the Web retrieval engine; two state of the art search diversification approaches, IA-Select [1] and xQuAD [16]; a plain (in the sense of "not diversified") personalized search approach based on social tagging profiles and BM25, presented in [18] (PersBM25); a two stage diversification and personalization approach, suggested by Radlinski and Dumais in [13] which, in our implementation, first applies the xQuAD algorithm and then the PersBM25 technique (xQuADBM25); two different personalized versions of IA-Select, one with a probabilistic calculation of | ≠equation 6≠ (PIA-Select), and the other using BM25 (equation 7) as an alternative calculation (PIA-SelectBM25), as described in section 4.1; two different personalized versions of xQuAD, differing on the same alternative as in PIA-Select for the calculation of |.
In order to evaluate for diversity, we use three well known metrics: the intent aware version of expected reciprocal rank (ERRIA), -nDCG [7], and subtopic recall (S-recall) [22]. For accuracy, we use nDCG and precision. User assessments were graded from one to four, where a value greater than one was assumed as relevant. The nDCG metric made use of the graded values for each of these approaches, Since users only evaluated the top 5 results of each of these approaches, all metrics take this cut off point.

847

Table 2: Accuracy metrics for evaluated approaches. Values in bold indicate the best performing values. Underlined values indicate a statistically significant difference with respect to the baseline, double underlined values indicate, in addition, a statistical significance with respect to xQuAD (by Wilcoxon, p < 0.05 in both cases).

Topic

Topic relevance Topic (K=1) Topic (K=2)

User relevance

User

User (K=1) User (K=2) F(Topic,User)

nDCG@5 P@ 5 nDCG@5 P@ 5 nDCG@5 P@ 5 nDCG@5 P@ 5 nDCG@5 P@ 5 nDCG@5 P@ 5 nDCG@5 P@ 5

Baseline IA-Select xQuAD PersBM25 xQuADBM25 PIA-Select PIA-SelectBM25 PxQuAD PxQuADBM25

0.393 0.363 0.381 0.388 0.375 0.331 0.363 0.395 0.405

0.918 0.911 0.911 0.933 0.924 0.861 0.892 0.930 0.939

0.401 0.367 0.390 0.410 0.390 0.315 0.369 0.408 0.414

0.914 0.905 0.907 0.947 0.931 0.840 0.897 0.928 0.943

0.385 0.923 0.330 0.359 0.917 0.294 0.370 0.915 0.320 0.359 0.915 0.357 0.357 0.915 0.334 0.350 0.885 0.306 0.356 0.885 0.345 0.379 0.931 0.337 0.394 0.933 0.361

0.702 0.664 0.670 0.746 0.688 0.652 0.670 0.714 0.730

0.318 0.277 0.310 0.373 0.340 0.292 0.350 0.334 0.354

0.657 0.617 0.611 0.714 0.642 0.590 0.625 0.674 0.672

0.344 0.755 0.314 0.720 0.332 0.741 0.336 0.784 0.327 0.741 0.323 0.725 0.340 0.723 0.341 0.760 0.371 0.800

0.359 0.325 0.348 0.372 0.354 0.318 0.354 0.364 0.382

0.796 0.768 0.772 0.829 0.789 0.742 0.766 0.807 0.821

We compute the metrics in two ways: 1) with the assessments given by workers when replying to Q2 (see section 5.1) where they judge how relevant was the result to the evaluated topic; and 2) metrics that make use of the assessments given by workers when replying to Q1, where they judge how relevant is the result to their personal interests. We also provide a breakdown of the obtained results into topics with single keyword queries (K = 1) and topics with two keywords (K = 2). The reason is that we expect, on average, the former to represent more ambiguous topics than the latter.
6.1 Diversity Metrics
Table 1 shows the obtained results regarding diversity metrics from which we can conclude the following. In terms of topic relevance (Table 1 left), most approaches, except PIA-Select, are on par with the commercial search engine baseline. PxQuADBM25 has a significantly better performance than the baseline and plain diversification approaches in terms of ERR-IA and -nDCG@5. In terms of user relevance (Table 1 right) the improvement of personalized diversity over the baseline and plain diversification is increased, indicating that the personalization factors have a positive effect on the overall view of relevance with respect to the user preference and the diversity of results preferred by the user.
Surprisingly, the plain personalization approach did not show a significant decrease in the diversification values with respect to the baseline, although the obtained values are smaller than the diversification approaches. We hypothesize that this is due to this approach taking into account the original ordering of the search results, given by the baseline, which already performs well in terms of diversity. Thus, this diversification approach resembles the two stage diversification and personalization approach implemented in xQuADBM25 [13] in terms of diversity values.
Regarding query size, the best performing version of IA-Select, PIA-SelectBM25, seems to provide better results with topic queries of size K = 2 (more specific queries), whereas for K = 1, the personalized versions of xQuAD ≠PxQuADBM25 and PxQuAD≠ seem to perform better regarding topic and user relevance, respectively.
In summary, a general overview of the results indicate that the best performing techniques in terms of diversity metrics are three of our proposed personalized diversification approaches: PIASelectBM25, PxQuAD, and PxQuADBM25. The latter has a statisti-

cally significant difference with respect to the baseline both in terms of topic and user relevance. In subtopic recall, the PIASelectBM25 approach is the best performing one, as also in terms of topic relevance on more specific topics (K = 2). It is worth noting that all of these approaches outperform significantly the plain personalized approach, PersBM25, both in topic and user relevance. The other personalized diversification approach, PIASelect, is not able to improve the baseline. This might be due to a negative effect of the probabilistic estimate of the personalized factor on the overall behavior of the PIA-Select algorithm. However, the other variation, PIA-SelectBM25, which uses BM25 as personalized estimation factor, appears to perform much better. When compared with the best performing full diversification approach, xQuAD, we can observe that either PxQuADBM2 or PIA-SelectBM25 achieve statistically significant results in most of the diversity metrics.
6.2 Accuracy
Table 2 shows the accuracy metrics for the different evaluated approaches, from which we may observe the following. In terms of topic relevance (Table 2 left), values are in general close to the baseline: P@5 values are high for all approaches, whereas the nDCG@5 values, which make use of the graded relevance, are more diverse. We see the expected decrease in performance as measured by nDCG values for the plain diversification approaches (IA-Select, xQuAD). It comes as somewhat unexpected that the best value is obtained by a personalized diversification approach, PxQuADBM25, which is supposed to affect diversity negatively, but that even reaches statistical significance with respect to the baseline. The results obtained by this approach seem to be the highest in almost all metrics regarding topic relevance and queries with size K = 1 and K = 2. These results suggest that there is a further benefit from adding personalized factors to the diversification approach. However, the other personalized diversification approaches do not appear to outperform the baseline, so no definitive conclusion can be drawn from this result.
In terms of user relevance (Table 2 right), there appears to be two clear dominant approaches: PxQuADBM25 and PersBM25. The personalized diversification approach has the best performance in terms of nDCG, outperforming the baseline. It achieves a 9.5% improvement over the baseline, which is statistically significant.

848

The performance of the plain personalization approach, PersBM25, is on par with the latter, obtaining also a statistical significance with respect to the baseline. It seems that the main difference between these two approaches is that the performance of the PersBM25 approach in more ambiguous queries (K = 2) is slightly better than the PxQuADBM25 approach, while on less ambiguous queries (K = 2) the latter has better performance.
The last two columns of Table 2 show the F-measure values regarding both the user and topic relevance, in order to assess the combined performance of the approaches taking into account both topic and user relevance. The best values are obtained for the plain personalization approach PersBM25 in terms of P@5 and the diversified personalization approach PxQuADBM25 in terms of nDCG (graded relevance). Both of these approaches achieve statistically significant results with respect to the baseline and the best performing diversity approach, xQuAD.
In summary, while the plain personalization approach, PersBM25, appears to be on par with our best personalized diversification approach PxQuADBM25, in terms of relevance to the user, the former underperforms the baseline in terms of relevance to the topic, while the latter, surprisingly, improves the baseline to this regard, with statistical significance.
6.3 Experiment conclusions
In terms of overall performance, the personalized diversification approach PxQuADBM25 fares best overall: it has the best results in terms of topic-based diversity and is competitive in terms of userbased diversity. Additionally, it has the best accuracy values, together with the plain personalization approach. In most of the obtained values, this approach improves significantly the baseline. Hence, the PxQuADBM25 approach appears to be the best approach to balance both diversification and personalization factors, resulting in competitive values in terms of both diversity and accuracy, with respect to both topic and user relevance.
7. RELATED WORK
To the best of our knowledge, only Radlinski and Dumais [13] have previously investigated a possible combination of personalization and diversification approaches in Web search. They present a two-stage approach: given an initial user query, the search results are first diversified by executing related sub-queries, taken from query reformulations. In a second step, results are personalized using the user profile. Their hypothesis is that by diversifying results there is a higher chance of encountering documents that satisfy the user's interests. In our evaluation, we included a technique based on their two stage approach (xQuADBM25). In our experiments this approach did not achieve significant improvements over plain personalization, and it was outperformed by our personalized diversification approaches. A possible explanation is that the basic personalized approach was applied to the search baseline, which already had competitive diversity performance. Another explanation would be that their approach may further benefit from diversification techniques based on sub-queries, which were not tested in our experiments.
In our work, we exploit social annotation profiles in order to apply personalization factors related to a Web search. The use of a source of information of this type for Web personalization was first suggested by Noll and Meiner [12], who evaluated with real users the use of Delicious user profiles to personalize search, proving the effectiveness of the approach with a simple personalization model. Vallet et al. [18] presented an evaluation framework allowing for offline evaluation of Web search personalization approaches based on social annotations. They tested a number of

approaches, including Noll and Meiner's, concluding that the best performing approaches were based either on the BM25 probabilistic model or a simple scalar product. From this work we have adapted the proposed BM25 model as a personalized estimation factor. Our evaluation framework is based on their offline evaluation approach, which has been adapted here to allow for an online evaluation of personalization and diversification methods with real users.
Han et al. [9] presented an improvement over Noll and Meiner [12] and Vallet et al. [18], in which user and document tags are related to the ODP taxonomy. The taxonomy is then exploited to expand tags to related concepts. The authors proposed to use the user's search query to activate concepts within the ODP taxonomy, performing a sort of "contextualization" of user preferences. These preferences are then used to compute the personalization factor to be applied on the search output. Their evaluation, which used the framework presented by Vallet et al., indicated an improvement over the state of the art approaches. In our work, we treated the ODP and tag information as independent sources of information. In future work we will analyze whether relating tags to ODP concepts has a positive impact on personalized diversification approaches.
8. CONCLUSIONS AND FUTURE WORK
In this work we have presented a number of approaches that combine both personalization and diversification components. To this end, we have investigated the introduction of the user as an explicit random variable in two state of the art diversification models: IA-Select [1] and xQuAD [15,16]. Our experiments show that there is a clear benefit in introducing this variable, achieving statistically significant improvements over the baselines that range between 3%-11% in terms accuracy values, and between 3%-8% in terms of diversity values.
These results lead us to conclude that diversification has a case even in the presence of personalization. Personalization in diversity can be seen in two ≠to some degree equivalent≠ ways: a) concentrating the extent of diversification in the areas of user interests, and b) seeking relevance to as diverse sides of user preferences as possible. We identify three situations in which our approaches have a potential benefit: 1) user preferences are not necessarily relevant for the query at hand, therefore they may remove uncertainty from it to very variable degrees (from a little to none at all); 2) the system knowledge about user preferences is incomplete (restricted to the observed user activity within the system) and imprecise (evidence of preferences can be implicit, indirect, ambiguous), whereby diversifying the effect of personalization reduces its involved risk; 3) user preferences are themselves diverse, therefore for lack of knowledge about which are most relevant in a given context, it is appropriate to diversify the relevance to the different preference aspects. A similar case as is made for diversification on query intents can thus be made for preference aspects.
We have presented different alternatives to estimate the components that arise from the formal development of our scheme, and we evaluated a number of them. Future work may include further investigation on these alternatives. We have tested two estimation approaches for the user/document and document/category relations, based on social annotations (Delicious) and ODP classification (Textwise), respectively. Other ways of approximating these values could be investigated, such as other sources of social profiles (e.g. Facebook, Twitter) or other sources of user profile information (e.g. query and browsing history). Other alternatives for document classification (e.g. clustering, or query reformulations [16]) might be

849

explored as well. A follow-up evaluation could also take into consideration different values of the combination parameter in xQuAD, which controls the strength of the personalized diversity component on the personalized variation of xQuAD.
The vision of combining diversity and personalization indeed opens a rich array of possibilities. Diversity and personalization can be combined in different ways, some of which we have investigated here, beyond which we see wide room for further research.
9. ACKNOWLEDGMENTS
This work was supported by the national Spanish projects TIN2011-28538-C02-01 and S2009TIC-1542.
10. REFERENCES
[1] Agrawal, R., Gollapudi, S., Halverson, A., and Ieong, S. Diversifying search results. In WSDM'09, pages 5-14, 2009.
[2] Alonso, O. and Baeza-Yates, R. Design and implementation of relevance assessments using crowdsourcing. In ECIR `11, pages 153-164, 2011.
[3] Alonso, O. and Mizzaro, S. Can we get rid of TREC assessors? using mechanical turk for relevance assessment. In SIGIR'09 Workshop on The Future of IR Evaluation, 2009
[4] Bache, R., Baillie, M., and Crestani, F. Language models, probability of relevance and relevance likelihood. In CIKM'07, pages 853-856, 2007.
[5] Carbonell, J. G. and Goldstein, J. The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. In SIGIR'98, pages, 335-336, 1998.
[6] Chen, H. and Karger, D. R. Less is More. In SIGIR'06, pages 429-436, 2006.
[7] Clarke, C. L. A., Kolla, M., Cormack, G. V., Vechtomova, O., Ashkan, A., B¸ttcher, S., and MacKinnon, I. Novelty and diversity in information retrieval evaluation. In SIGIR'08, pages 659-666, 2008.
[8] Fern·ndez, M., Vallet, D., and Castells, P. Probabilistic score normalization for rank aggregation. In ECIR'06, pages 553556, 2006.
[9] Han, X., Shen, Z., Miao, C., and Luo, X. Folksonomy-Based ontological user interest profile modeling and its application in personalized search. In Active Media Technology, LNCS, vol 6335 pages 34-46.
[10] Lee, J. H.. Analyses of multiple evidence combination. In SIGIR '97, pages 267-276, 1997.
[11] Micarelli, A., Gasparetti, F., Sciarrone, F., and Gauch, S. Personalized search on the World Wide Web. The Adaptive Web, pages 195-230, 2007.
[12] Noll, M. G., Meinel, C. Web search personalization via social bookmarking and tagging. In ISWC'07. pages 367380, 2007.
[13] Radlinski, F. and Dumais, S. Improving personalized web search using result diversification. In SIGIR'06, pages 691692, 2006.
[14] Rafiei, D., Bharat, K., and Shukla, A. Diversifying web search results. In WWW'10, pages 781-790, 2010.
[15] Santos, R., Peng, J., Macdonald, C., and Ounis, I. Explicit search result diversification through sub-queries. In ECIR `10, pages 87-99, 2010.

[16] Santos, R. L. T., Macdonald, C., and Ounis, I. Exploiting query reformulations for web search result diversification. In WWW'10, pages 881-890, 2010.
[17] Vallet, D. Crowdsourced Evaluation of Personalization and Diversification Techniques in Web Search. In CIR' 11 Workshop (SIGIR 2011), 2011.
[18] Vallet, D., Cantador, I., and Jose, J. Personalizing web search with Folksonomy-Based user and document profiles advances in information retrieval. In ECIR'10, pages 420-431, 2010.
[19] Vargas, S. and Castells, P. Rank and Relevance in Novelty and Diversity Metrics for Recommender Systems. In RecSys'11, pages 109-116, 2011.
[20] Vargas, S., Castells, P., and Vallet, D. Intent-Oriented Diversity in Recommender Systems. In SIGIR'11, pages 12111212, 2011.
[21] Wang, J. and Zhu, J. Portfolio theory of information retrieval. In SIGIR'09, pages 115-122, 2009.
[22] Zhai, C., Cohen, W. W., and Lafferty, J. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. In SIGIR'03, pages 10-17, 2003.
[23] Zhang, M. and Hurley, N. Avoiding Monotony: Improving the Diversity of Recommendation Lists. In RecSys'08, pages 123-130, 2008.
[24] Ziegler, C-N., McNee, S. M., Konstan, J. A., and Lausen, G. Improving recommendation lists through topic diversification. In WWW'05, pages 22-32, 2005

Appendix A. PERSONAIZED QUERY AS-
PECT DISTRIBUTION
In this appendix we offer alternative estimations for the personalized query aspect distribution, |, , component:

1.

|,

~

||  ||

=

||  ||

assuming a uniform aspect prior, and conditional independence between queries and users given an aspect. | can be estimated as:
| =  |, |.

2.

|,

~

||  ||

=

||  ||/

assuming conditional independence between queries and users given an aspect. Assuming a small aspect overlap (nr. of aspects per document) the ratio of category and document priors can be calculated as
/ ~ |  | is covered by | where  could be interpreted as the set of documents being reranked, or the whole collection. For higher aspect overlap, all document-aspect pairs should be taken into account, and normalized by the number of pairs.

3. |,  =  |, , |,  ~

 |, |, , and |, ~ ||||,

assuming conditional independence of aspects and queries

given a document, and a uniform aspect prior; or

|,

~

|| |

without

the

latter

assumption.

850

AspecTiles: Tile-based Visualization of Diversified Web Search Results


Mayu Iwata
Microsoft Research Asia, China
Osaka University, Japan
iwata.mayu@ist.osakau.ac.jp
Yu Chen
Microsoft Research Asia, China
Yu.Chen@microsoft.com

Tetsuya Sakai
Microsoft Research Asia, China
tetsuyasakai@acm.org
Yi Liu
Microsoft Research Asia, China
lewisliu@microsoft.com

Takehiro Yamamoto
Microsoft Research Asia, China
Kyoto University, Japan
tyamamot@dl.kuis.kyotou.ac.jp
Ji-Rong Wen
Microsoft Research Asia, China
jrwen@microsoft.com

ABSTRACT
A diversified search result for an underspecified query generally contains web pages in which there are answers that are relevant to different aspects of the query. In order to help the user locate such relevant answers, we propose a simple extension to the standard Search Engine Result Page (SERP) interface, called AspecTiles. In addition to presenting a ranked list of URLs with their titles and snippets, AspecTiles visualizes the relevance degree of a document to each aspect by means of colored squares ("tiles"). To compare AspecTiles with the standard SERP interface in terms of usefulness, we conducted a user study involving 30 search tasks designed based on the TREC web diversity task topics as well as 32 participants. Our results show that AspecTiles has some advantages in terms of search performance, user behavior, and user satisfaction. First, AspecTiles enables the user to gather relevant information significantly more efficiently than the standard SERP interface for tasks where the user considers several different aspects of the query to be important at the same time (multi-aspect tasks). Second, AspecTiles affects the user's information seeking behavior: with this interface, we observed significantly fewer query reformulations, shorter queries and deeper examinations of ranked lists in multiaspect tasks. Third, participants of our user study found AspecTiles significantly more useful for finding relevant information and easy to use than the standard SERP interface. These results suggest that simple interfaces like AspecTiles can enhance the search performance and search experience of the user when their queries are underspecified.
This research was conducted while the first author was an intern at Microsoft Research Asia.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]; H.5.2 [Interfaces]: [Evaluation/methodology]
General Terms
Experimentation, Human Factors
Keywords
Diversified Search, Search Result, Visualization, User Study
1. INTRODUCTION
Search queries are often ambiguous ("office" may mean a workplace or a Microsoft product) or underspecified ("harry potter" may mean the books, the films or the main character) [7]. Given such queries, search result diversification aims to cover different search intents of the query in a single ranked list of retrieved documents [1]. We are particularly interested in underspecified queries as every query is arguably underspecified to some extent unless it is a perfect representation of the underlying information need. One possible way to explicitly represent these different intents would be to use information types, which we refer to as aspects1. For example, desired aspects of an underspecified query "elliptical trainer" (a stationary exercise machine) may include product names, benefits and reviews. The present study concerns what kind of interface would be effective for presenting a diversified ranked list of results obtained in response to such a query.
An underspecified query may possibly represent an information need for a specific aspect of a topic or for various aspects of a topic. For example, with the aforementioned "elliptical trainer" query, the user may be looking for product names of the elliptical trainer, or he may be looking for not only product names, but also benefits and reviews of elliptical trainers. Thus, we make a distinction between single-aspect tasks and multi-aspect tasks in our study. This is because the effectiveness of a search result presentation interface may depend on the variety of aspects that the user desires. Another possible viewpoint for categorizing search tasks would be the number of relevant answers the user requires (e.g. home page findings vs. recall-oriented search) [3, 17], but this study primarily concerns
1This usage of "aspect" is different from TREC, where it was used synonymously with "instance." [11, 12]

85

recall-oriented search where several answers that match a certain aspect of the query need to be collected by the user.
To help the user efficiently and effectively locate retrieved relevant answers that match the desired aspects within the Search Engine Result Page (SERP), we propose a simple extension to the standard SERP interface, which we call AspecTiles. Figure 1 shows a screenshot of the AspecTiles interface. As can be seen, AspecTiles visualizes which of the ranked web pages are relevant to which aspects by means of colored squares ("tiles") that are shown to the left of each document in the SERP. Moreover, the color represents the degree of relevance to an aspect. We believe that this line of research is important but missing in current search result diversification studies: even though most diversification systems rely on explicit identification of different search intents, a standard SERP interface does not tell the user which document is likely to be relevant to which of the intents, and to what degree. We hypothesized that a more informative interface such as AspecTiles will help web search engine users.
In this study, we conduct a user study involving 30 search tasks designed based on the TREC web diversity task topics as well as 32 participants to address the following Research Questions:
∑ RQ1: Does AspecTiles enable users to gather relevant information more effectively and efficiently when compared to the standard SERP interface?
∑ RQ2: Does AspecTiles affect the user's information seeking behavior? In what way?
RQ1 is the central question addressed in this paper. Compared to the standard SERP interface, AspecTiles provides the additional information of which documents are relevant to which aspects and to what degree. Thus, we expect the user to quickly scan the ranked list and locate relevant documents that match the desired aspects. Moreover, we would like to clarify under what conditions AspecTiles can be beneficial. For example, it is possible that AspecTiles may be more useful for multi-aspect tasks than for single-aspect tasks, as it can highlight the difference between a document that covers multiple aspects and one that covers only one aspect. Moreover, other factors such as the number of tiles displayed, the quality of the aspects represented as tiles and the accuracy of the estimated degree of relevance represented by the tile color may affect the outcome.
While RQ1 concerns user performance, RQ2 concerns user behavior. For example, we can hypothesize that, if the displayed aspects match well with the user's desired aspects, then the user can locate desired information without reformulating his query many times. Another possible hypothesis would be that, since AspecTiles enables the user to skip documents that are seemingly irrelevant to the desired aspects while scanning the ranked list, it may guide the user to documents that lie deeper in the ranked list.
The remainder of this paper is organized as follows. Section 2 discusses related work. Section 3 describes our experimental design for evaluating AspecTiles, and Section 4 presents our results. Finally, Section 5 concludes this paper and discusses future work.
2. RELATED WORK
Information visualization takes advantage of the user's visual information processing abilities by generating graphical representations of data or concepts for presenting textual information [21], and is known to be useful for quantitative comparisons [15]. Thus, many search result visualization methods have been proposed to provide the user with a quick overview of the retrieved documents.

According to Turetken and Sharda [20], web documents can be visualized based on content (e.g., title and term frequency) [4, 9, 10, 14, 16, 23], connectivity (e.g., number of incoming and outgoing links), and others (e.g., metadata like document size and the web domain) [4, 10]. Our AspecTiles falls into the first category, as it visualizes the relevance of each retrieved web page with respect to each displayed aspect.
The TileBar interface proposed by Hearst [9] and the simpler HotMap interface proposed by Hoeber and Yang [14] are visually somewhat similar to AspecTiles. In TileBar, each retrieved document is represented by a rectangle, where the query terms are arranged vertically and the document length is represented horizontally. For each query term, its term frequency within each section of a document is represented in grayscale. In HotMap, a row of squares are shown to the left of each retrieved document, where each square represents a particular query term and its withindocument term frequency. The user experiments by Hoeber and Yang suggested that HotMap is useful for effective and efficient information gathering. Our AspecTiles is different from TileBar and HotMap in that (a) the row of squares (which we call tiles2) represent possible aspects or intents of the query rather than the individual query terms of that query; and (b) the color of each tile represents the degree of relevance of the document to that particular aspect rather than term frequency. Thus, AspecTiles requires the backend search engine to provide a set of possible aspects given a query, and estimated per-aspect relevance scores. To our knowledge, our work is the first to study this kind of interface for the purpose of presenting diversified search results.
There are other ways to visualize search engine results, such as glyph-based and graph-based methods. Glyph-based approaches include the work by Heimonen and Jhaveri [10] who used documentshaped icons, and the work by Chau [4] who used a flower metaphor for representing statistics such as term frequency and document length. Graph-based approaches include bar charts of Reiterer et al. [16], and the radar charts of Yamamoto and Tanaka [23]. The radar charts were used for visualizing a fixed set of credibility criteria (accuracy, objectivity, authority, currency, and coverage). None of these studies was to do with multiple intents/aspects and search result diversification.
Brandt et al. [2] describe a method for dynamically presenting diversified search results. The main idea is to present a tree of web pages and dynamically recomputing it based on the nodes expanded by the user instead of showing a flat static list. While this rich presentation approach may have benefits, it is radically different from the standard SERP, and may be difficult to prevail. Moreover, its usability (e.g. the cognitive load of making the user backtrack within a tree, that of rearranging documents that the user has seen or has yet to see, etc.) is unknown. In contrast, AspecTiles lets the user go through a list just like traditional SERP, and is arguably easier for existing web search engines to adopt.
Our choice of using simple tiles (i.e. colored squares) for representing per-aspect relevance of retrieved documents is based on findings from previous research in user interface studies including the aforementioned ones. In particular, the user evaluation of HotMap [14] suggested that the simple interface offers effective and efficient information access as well as high user satisfaction. While HotMap users may quickly skip documents that are nonrelevant to the query and locate relevant documents with high query term frequencies, AspecTiles users may quickly skip documents that are nonrelevant to the desired aspects of the query and even locate relevant documents that cover multiple desired aspects. Fur-
2Note that Hearst's "tiles" refer to nonoverlapping text segments [9].

86

Figure 1: Screenshot of AspecTiles with five tiles (elliptical trainer).

Figure 2: Screenshot of AspecTiles with three tiles (elliptical trainer).

thermore, Xiang et al. [22] compared a tree view and a list view to visualize criminal data relations, and reported that users may find it difficult to use unfamiliar interfaces like the tree view. Chen and Yu [5] conducted a meta-analysis of six information visualization usability studies for IR and also reported that users perform better with simple visual interfaces than with complex ones.
Regarding the underlying search engine that SERP interfaces require, a number of effective search result diversification algorithms have been proposed recently [1, 8, 18]. Any of these algorithms could be used with our AspecTiles presentation interface, provided that an explicit set of aspects of the given query and per-aspect relevance scores for each retrieved document are available. In this study, we use as input the diversified web search system described by Dou et al. [8]. This system was a top performer in the Japanese Document Ranking (search result diversification) subtask of the recent NTCIR-9 INTENT task [19].
3. EXPERIMENTAL DESIGN
This section describes a user study that we designed to address the research questions mentioned in Section 1: Is AspecTiles more effective and efficient than the standard SERP? How does it affect the user's search behavior?
3.1 Interfaces
We compared three prototype interfaces in our user experiments: AspecTiles with five tiles, AspecTiles with three tiles and a baseline interface with no tiles. The number of tiles refers to the initial setting ≠ participants were allowed to change the number of displayed tiles during their search tasks, as we shall explain later. We initially displayed either five or three tiles because (a) too many tiles would mean a wide left margin for the SERP and therefore waste of space; and (b) showing too much information may have adverse effects on the user performance and satisfaction. Note, for example, that current web search engines typically show no more than eight query suggestions at a time.
Figures 1, 2 and 3 show screenshots of our three interfaces for the query "elliptical trainer." All interfaces show the title, snippet and URL of each ranked document. In addition, the two AspecTiles interfaces show (a) aspect labels (e.g. "reviews", "best machines",

"exercise" etc.) at the top of SERP; (b) a colored tile for each displayed aspect, whose color represents the per-aspect relevance of the document in five-point scale (dark blue means highly relevant and light blue means marginally relevant); and (c) a popup text when the mouse is placed over a tile, which reminds the user of the aspect label. In addition, the user can freely adjust the number of displayed aspects to any number between zero and eight during the experiment, by clicking the Add or the Remove button. For example, if a user of the three-tile AspecTiles interface finds that none of the displayed aspects is useful, he may choose to click Add to show more aspects, some of which may turn out be relevant to his needs. Whereas, the baseline interface is very similar to popular search engines such as Google and Bing.
The three experimental interfaces share the same underlying diversification algorithm. When a query is issued, its top 300 search results and query suggestions are obtained using Bing API3. In our study, we treated these query suggestions as the aspects to be displayed on the interface, as they are easy to obtain, reasonably accurate and were effective for search result diversification with Dou et al.'s algorithm [8] at the NTCIR-9 INTENT task [19]. To save space, the aforementioned aspect labels were obtained after removing all query terms from each query suggestion; whereas, the original query suggestion string was shown in the popup text (e.g. "reviews" for the former and "elliptical trainer reviews" for the latter). A more sophisticated method that is dedicated to finding accurate aspects (information types) for AspecTiles deserves to be explored, but this is beyond the scope of our study: we focus on the usefulness of AspecTiles in a realistic situation where the displayed aspects do not perfectly overlap with the desired aspects.
Using the aspects thus obtained, we used the algorithm by Dou et al. to compute per-aspect relevance scores and to rerank the SERP obtained by Bing API. Our future plan includes letting the user click the tiles (as well as the aspect labels) on the AspecTiles interface and thereby dynamically rerank documents, but this is beyond the scope of the present study. Our current focus is to examine the effect of showing the tiles to the user rather than that of interacting with the tiles as well as with the aspect labels (query suggestions).
3http://www.bing.com/toolbox/bingdeveloper/

87

Figure 3: Screenshot of the baseline interface (elliptical trainer).
3.2 Tasks
As we hypothesized that AspecTiles may be more useful for multi-aspect tasks (e.g. Find product names, benefits and reviews of the elliptical trainer) than for single-aspect tasks (e.g. What are the benefits of an elliptical trainer?), we devised 15 multi-aspect tasks and 15 single-aspect tasks shown in Table 1. For this purpose, we carefully examined the TREC 2009 web diversity task topics and selected 15 faceted (i.e. underspecified) topics that had informational subtopics [6], so that each selected topic could be used as a resource for creating exactly one multi-aspect task and exactly one single-aspect task 4. Figure 4 shows a selected topic: it contains a query field, a description field and several informational subtopics. From this topic, we created a multi-aspect task: "Find information about elliptical trainer machines. (reviews, benefits, product names)." The words in parenthesis correspond to the desired aspects, i.e., the types of information that the participants are asked to collect. In this example, these desired aspects correspond to informational subtopics 1, 3 and 4: for every multi-aspect task, we selected exactly three desired aspects. When more than three informational subtopics were available, we carefully selected three desired aspects from them so that the overlap between the selected aspects and the displayed aspects (i.e. query suggestions) are neither too high nor too low. As we mentioned earlier, we wanted to examine the realistic situation where the displayed aspects are of reasonable accuracy. In fact, the average number of informational subtopics per faceted topic was 3.8, so we used up most of the informational subtopics in the faceted topics. On the other hand, from the same topic, we created one single-aspect task: "What are the benefits of an elliptical trainer compared to other fitness machines?" Note that this is exactly subtopic 3. In summary, we prepared 15 multi-aspect tasks that require participants to look for exactly three aspects (e.g. reviews, benefits and product names), and 15 single-aspect tasks that require them to look for exactly one aspect (e.g. benefits).
4TREC diversity topics also have navigational subtopics, which we did not utilize as our focus was on collecting multiple answers that are relevant to each aspect.

Figure 4: A sample topic from the TREC 2009 web diversity task.
3.3 Procedure
We hired 32 participants for our user study. 21 of them were male and 11 were female; 24 were students in computer science; four were researchers in computer science and the other four were desk workers. The average age of the participants was 26.18 years, with a standard deviation of 4.34.
Then we formed an experimental design similar to that of the TREC interactive track [13]. Each participant performed a total of six tasks (three multi-aspect and three single-aspect): two using the five-tile AspecTiles, two using the three-tile AspecTiles and the other two using the baseline. The choice and the order of systems and tasks were randomized for each participant.
Prior to the actual user experiment, each participant went through a training task to become familiar with the interfaces and the tasks. Each training session took approximately ten minutes.
For each of the six tasks, the participants were provided with a printed task description document that contained the initial query (e.g. "elliptical trainer"), a short explanation of the topic (e.g. "a stationary exercise machine"), (for multi-aspect tasks only) the three desired aspects (e.g. "reviews," "benefits" and "product names.") and some additional information if required. For the elliptical trainer example, the short explanation indicated that it is a kind of stationary exercise machine (description obtained from Wikipedia) and a sample picture of an elliptical trainer was shown to the participants as additional information. This is because the tasks that originate from TREC are not real information needs of the participants, and therefore the participants were not necessarily familiar with all of the tasks.
In each task session, the participant used one of the aforementioned three interfaces to collect as many answers as possible within ten minutes (for single-aspect tasks) or fifteen minutes (for multiaspect tasks). An answer is a relevant instance that matches one of the desired aspects (e.g. a particular product name of elliptical trainer). When the participant clicked on a URL within the interface, a special landing page window popped up, from which he could extract arbitrary part of the HTML by a mouse drag and bookmark it. A single bookmark can contain multiple text segments from the same webpage, as well as images and links. A single relevant bookmark may contain one or multiple answers, and may even cover multiple desired aspects. For example, one bookmark may contain a product name of elliptical trainer, its review as well as its picture. Note that we used a special browsing interface to facilitate collection of answers rather than a standard web browser.

88

Topic elliptical trainer disneyland hotel getting organized
dinosaurs poker tournaments
volvo rick warren
diversity starbucks diabetes education
atari cell phones
hoboken orange county convention center the secret garden

Table 1: Task list. S: single-aspect task, M: multi-aspect task (three desired aspects)
S: What are the benefits of an elliptical trainer compared to other fitness machines? M: Find information about elliptical trainer machines. (reviews, benefits, product names) S: What hotels are near Disneyland? M: Find information about hotels near Disneyland Resort in California. (packages, reviews, coupons) S: Find catalogs of office supplies for organization and decluttering. M: Find tips, resources, supplies for getting organized and reducing clutter. (tips, resources, supplies) S: I'm looking for free pictures of dinosaurs. M: I want to find information about and pictures of dinosaurs. (pictures, names, games) S: Find books on tournament poker playing. M: I want to find information about live and online poker tournaments. (schedules, game names, books) S: Where can I find Volvo semi trucks for sale (new or used)? M: I'm looking for information on Volvo cars and trucks. (reviews, dealers, product names) S: I want to see articles and web pages about the controversy over Rick Warren's invocation at the Obama inauguration. M: I'm looking for information on Rick Warren, the evangelical minister. (biography, articles, books) S: What is cultural diversity? What is prejudice? M: Find information on diversity, both culturally and in the workplace. (management methods, definition, poems) S: Find the menu from Starbucks, with prices. M: Find information about the coffee company Starbucks. (menu, calories, coupons) S: Find free diabetes education materials such as videos, pamphlets, and books. M: I'm looking for online resources to learn and teach others about diabetes. (materials, diet, classes) S: I want to read about the history of the Atari 2600 and other Atari game consoles. M: Find information about Atari, its game consoles and games. (history, classic games, arcade games) S: What cell phone companies offer Motorola phones? M: Find information about cell phones and cellular service providers. (provider names, prices, reviews) S: Find restaurants in Hoboken. M: Find information on the city of Hoboken, New Jersey. (restaurants, history, real estate) S: What hotels are near the Orange County Convention Center? M: Looking for information about the Orange County Convention Center in Orlando, Florida. (event schedules, hotels, restaurants) S: Find reviews of the various TV and movie adaptations of The Secret Garden. M: Find information and reviews about The Secret Garden book by Frances Hodgson Burnett as well as movies and the musical based on it. (reviews, summary, music)

But this condition was the same across the three interfaces that we wanted to compare.
The query field of the original TREC topic served as the initial query for each task, so that each participant began his task by looking at a diversified search result. However, participants were also allowed to freely reformulate the query. As was mentioned earlier, they were allowed to change the number of displayed aspects to an arbitrary number between zero and eight in the AspecTiles interfaces during the experiment.
Participants were also allowed to finish their task sessions prior to the time limit if they felt that their collected answers were complete. After each task session, they were asked to fill out a post-task questionnaire. After completing all of the six task sessions, they were asked to fill out an exit questionnaire. The entire procedure for one participant took approximately one hour.
4. RESULTS
Section 4.1 discusses the results for RQ1 (search performance); 4.2 discusses the results for RQ2 (search behavior); and 4.3 discusses the questionnaire results.
4.1 Search performance
4.1.1 Bookmark relevance assessment
Through 192 task sessions (32 participants, each with 6 tasks), a total of 1,262 bookmarks were obtained. In order to evaluate user performance for these task sessions, three annotators independently assessed all of these bookmarks. Figure 5 is a screenshot of the assessment tool we developed for that purpose: the left panel shows a particular bookmark (a part of an HTML file) to be as-

sessed with radio buttons for selecting a relevance grade ("highly relevant," "somewhat relevant" or "not relevant"). For multi-aspect tasks, if a bookmark was judged relevant, then the assessors were also asked to check on one or more of the desired aspects that the bookmark contained. The inter-assessor agreement was quite high (Fleiss' kappa: 0.666). Based on the three assessment sets, we constructed two ground-truths: strict and lenient sets. In the former, the relevance grade of each bookmark reflects the lowest relevance grade for that bookmark among the three assessors, and the matched aspects of that bookmark are obtained by taking the intersection of the three assessors' selected aspects. In the latter, the relevance grade of each bookmark reflects the highest relevance grade for that bookmark among the three assessors, and the matched aspects of that bookmark are obtained by taking the union of the three assessors' selected aspects. Thus, the strict set reflects the view: "nothing is relevant unless all three assessors say that it is relevant" while the lenient set reflects the view: "anything is relevant if at least one assessor says that it is relevant."
4.1.2 Effectiveness and Efficiency
Using the strict and lenient sets of ground truths, we compared the user effectiveness and efficiency for AspecTiles and for the baseline interface. For each task session, let n be the number of relevant bookmarks found by a particular user. Then precision is n divided by the number of bookmarks found by that user for that task session, and recall is n divided by the total number of known relevant bookmarks for that task session. As measures of search efficiency, we also measured the time taken to obtain the first relevant bookmark ("Time-FirstRel"), the number of document clicks before the first relevant bookmark was obtained

89

Figure 5: Screenshot of the assessment tool. The left panel shows a bookmark (containing a picture, a product name and its review) and the right panel shows the entire page from which the bookmark was extracted.
("DocClick-FirstRel"), the average time between obtaining a relevant bookmark and obtaining the next relevant bookmark ("TimeTwoRel"), and the average number of document clicks between obtaining a relevant bookmark and obtaining the next relevant bookmark ("DocClick-TwoRel"). Clearly, all of these statistics should be low for an ideally efficient interface. For multi-aspect tasks that require three aspects, we also compute aspect coverage, i.e. the fraction of desired aspects covered by the user's bookmarks (which can range from 0/3 to 3/3).
Tables 2 and 3 show the user effectiveness and efficiency results with the strict and lenient sets, respectively. For both task types (single-aspect and multi-aspect), we conducted a two-factor (interface and task) ANOVA at p  0.05 for all types of measurements. For single-aspect tasks, only the task effect was significant for Precision, Time-FirstRel, DocClick-FirstRel, Time-TwoRel, and DocClick-TwoRel; none of the effects was significant for Recall. For multi-aspect tasks, only the task effect was significant for Precision and Aspect coverage, and only the system effect was significant for Recall, Time-FirstRel, DocClick-FirstRel, Time-TwoRel, and DocClick-TwoRel. If a significant interaction was found, we also conducted a Bonferroni post-hoc test at p  0.05 between two interfaces. In the tables, statistically significant differences with the baseline are shown in bold.
In multi-aspect tasks, it can be observed that the five-tile AspecTiles demonstrated significantly higher recall than the baseline. Moreover, although not statistically significant, it can be observed that Precision and Aspect Coverage values also tend to be higher with AspecTiles. On the other hand, it can be observed that AspecTiles is significantly better than the baseline in terms of TimeFirstRel, DocClick-FirstRel, Time-TwoRel and DocClick-TwoRel. The latter two statistics in particular show that even the three-tile AspecTiles is advantageous over the baseline. Thus, we can conclude that, for multi-aspect tasks, AspecTiles lets the user collect relevant pieces of information in shorter time using fewer clicks than the baseline, while keeping the user effectiveness at least comparable. In addition, note that, for multi-aspect tasks, the measurements for five-tile AspecTiles are consistently better than those for three-tile AspecTiles. Although the differences are not statistically

Table 2: Effectiveness and efficiency results (strict). The aver-

age of all 32 task sessions is reported, with the standard deviation in parenthesis. Significant differences with the baseline at p  0.05 according to the Bonferroni post-hoc test are shown in bold.

Task

Baseline Three-tile Five-tile

type

AspecTiles AspecTiles

Single Precision 0.498

0.496

0.469

(0.387) (0.409)

(0.395)

Recall

0.143 (0.127)

0.154 (0.193)

0.152 (0.145)

Time

187.5

193.4

183.8

-FirstRel (96.8) DocClick 2.500

(108.4) 1.655

(100.9) 2.258

-FirstRel (2.502) (1.738)

(2.065)

Time

99.8

-TwoRel (63.6)

89.3 (70.1)

113.4 (76.9)

DocClick 2.631

2.234

2.997

-TwoRel (1.348) (1.061)

(1.742)

Multi Precision 0.684

0.685

0.717

(0.248) (0.252)

(0.232)

Recall

0.130 (0.088)

0.184 (0.178)

0.214 (0.204)

Aspect 0.644

0.741

0.759

Coverage (0.315)

Time

166.9

(0.233) 130.3

(0.250) 118.0

-FirstRel (117.0) (92.7)

(120.6)

DocClick -FirstRel
Time

3.345 (3.199) 115.5

1.963 (1.055)
82.4

1.714 (0.897)
69.3

-TwoRel (77.1) DocClick 2.808

(77.0) 2.087

(48.4) 1.997

-TwoRel (1.607) (1.270)

(0.598)

significant in our experiment, this suggests that displaying tiles is beneficial.
For single-aspect tasks, we cannot claim that AspecTiles is effective as there are no significant differences observed. We attribute this mainly to the fact that for single-aspect tasks, the chance of finding the desired aspect within the displayed aspects is smaller compared to multi-aspect tasks. For example, for both the singleaspect task "What cell phone companies offer Motorola phones?" and the multi-aspect task "Find information about cell phones and cellular service providers. (provider names, prices, reviews)," the initial query automatically issued by the system was "cell phones." As a result, in both tasks, the initially displayed aspects (obtained from query suggestions in response to "cell phones") were "providers, verizon, sprint" in three-tile AspecTiles, and additionally "prepaid, cheap" in five-tile AspecTiles. While the displayed aspects such as "providers" and "cheap" are probably directly useful for the multi-aspect task as it requires "provider names" and "prices," they are probably not useful for the particular single-aspect task as "motorola" is not included in them. We manually counted the number of initially displayed aspects that are actually relevant to the desired aspects for all task sessions: it was 0.683 for singleaspect tasks and 1.113 for multi-aspect tasks on average. The next section investigates this effect of quality of the displayed aspects in more detail.

90

Table 3: Effectiveness and efficiency results (lenient). The aver-

age of all 32 task sessions is reported, with the standard deviation in parenthesis. Significant differences with the baseline at p  0.05 according to the Bonferroni post-hoc test are shown in bold.

Task

Baseline Three-tile Five-tile

type

AspecTiles AspecTiles

Single Precision 0.615

0.581

0.649

(0.425) (0.441)

(0.383)

Recall

0.136 (0.107)

0.148 (0.151)

0.185 (0.149)

Time

176.2

183.9

155.8

-FirstRel (100.0) DocClick 2.188

(105.3) 2.207

(87.7) 1.871

-FirstRel (2.086) (2.555)

(1.408)

Time

94.1

-TwoRel (65.4)

107.9 (91.2)

90.8 (54.1)

DocClick 2.461

2.758

2.285

-TwoRel (1.354) (2.328)

(1.170)

Multi Precision 0.772

0.802

0.849

(0.245) (0.285)

(0.221)

Recall

0.127 (0.081)

0.173 (0.103)

0.213 (0.194)

Aspect 0.733

0.828

0.828

Coverage (0.296)

Time

165.3

(0.262) 124.3

(0.246) 114.0

-FirstRel (117.4) (87.8)

(118.0)

DocClick -FirstRel
Time

3.333 (3.188) 110.6

1.926 (1.072)
72.9

1.643 (0.870)
66.6

-TwoRel (76.0) DocClick 2.749

(62.5) 1.987

(47.3) 1.920

-TwoRel (1.615) (1.211)

(0.564)

4.1.3 Effect of Quality of Displayed Aspects and Relevance Estimation Accuracy
AspecTiles requires as input (a) a set of aspects to be displayed in response to a query; and (b) per-aspect relevance score for each retrieved document. In this study, we chose to use query suggestions for (a), and a state-of-the-art diversified web search system [8] for (b), so that we can study the advantages of AspecTiles in a practical situation. Thus, in our study, both the quality of the displayed aspects and the estimated per-aspect relevance score (shown as colors of the tiles) are far from perfect. In this section, we investigate their effects on user effectiveness.
To investigate the effect of displayed aspect quality on user effectiveness, we computed precision, recall and (for multi-aspect tasks only) aspect coverage of the bookmarks at the query level rather than the task session level as was done in Section 4.1.2. This is because participants can issue multiple queries during a task session, and the displayed aspects (and therefore their quality) change accordingly within the task session. For each issued query, let n be the number of relevant bookmarks found by a particular user for that query. Then per-query precision is n divided by the number of bookmarks found by that user for that query, per-query recall is n divided by the total number of known relevant bookmarks for that query. We also compute per-query aspect coverage, i.e. the fraction of desired aspects covered by the user's bookmarks for that query. The quality of each set of displayed aspects was computed automatically: if a displayed aspect had at least one overlapping word with the task sentence, it was regarded as relevant; the qual-

Table 4: Spearman's rank correlation between user effectiveness (strict) and the quality of displayed aspects / relevance estimation accuracy.

Task

Displayed Relevance

type

aspect estimation

quality accuracy

Single Multi

Precision Recall
Precision Recall
Aspect Coverage

0.334 0.300 0.434 0.324 0.369

0.496 0.490 0.506 0.516 0.506

ity of the displayed aspects set was defined as the fraction of such relevant displayed aspects.
On the other hand, to investigate the effect of relevance estimation accuracy, we computed precision, recall and (for multi-aspect tasks only) aspect coverage of the bookmarks for every clicked document. This is because the quality of estimated per-aspect relevance score for each clicked document change accordingly within the task session. For each clicked document, let n be the number of relevant bookmarks found by a particular user on that document. Then perdocument precision is n divided by the number of bookmarks found by that user within a particular document, per-document recall is n divided by the total number of known relevant bookmarks within that document. We also compute per-document aspect coverage, i.e. the fraction of desired aspects covered by the user's bookmarks within that document. For example, if two bookmarks were saved from a clicked document and only one of them was relevant, the precision for this document is 0.5. For computing these metrics, the first author of this paper manually examined all 1,483 clicked documents obtained in our experiments, and identified false alarms: a false alarm is a document-aspect pair that AspectTiles presented as relevant even though in fact the document content suggested otherwise. (Whereas, she did not check for misses: relevant documentaspect pairs which AspecTiles failed to present as relevant.)
Tables 4 and 5 show the Spearman's rank correlation values between user effectiveness and the aspect quality / relevance estimation accuracy with the strict and lenient sets, respectively. Recall that the user effectiveness values were computed per query for the displayed aspect quality and per document for the relevance estimation accuracy. It can be observed that user effectiveness (precision, recall and aspect coverage) is indeed correlated with the quality of displayed aspects and with the accuracy of estimated per-aspect relevance scores. The correlation is especially high for the latter. These results suggest that AspecTiles would be even more effective if the underlying search system can provide more appropriate selection of aspects to be displayed, as well as accurate per-aspect relevance scores. Note that showing innacurate relevance scores (in the form of colors) may seriously mislead the user: if the system fails to indicate that a document is relevant from a certain aspect, the user may skip the document; conversely, if the system shows a false positive for a certain aspect, the user may be forced to visit a document that is not useful to him.
4.2 Search Behavior
Now we discuss RQ2: how does AspecTiles affect the user's information seeking behavior? For this purpose, we recorded the participants's every operation performed on the three interfaces during the aforementioned experiments.

91

Table 5: Spearman's rank correlation between user effectiveness (lenient) and the quality of displayed aspects / relevance estimation accuracy.

Task

Displayed Relevance

type

aspect estimation

quality accuracy

Single Multi

Precision Recall
Precision Recall
Aspect Coverage

0.320 0.287 0.411 0.392 0.327

0.522 0.506 0.555 0.542 0.523

4.2.1 Search Behavior Measurements
Table 6 summarizes the number of query reformulations, the average query lengths (including the initial query and the queries reformulated by the participants), the average rank of clicked documents ("Rank-All"), the average rank of clicked documents that contain at least one relevant bookmark in the strict set ("Rank-Rel (strict)"), and the average rank of clicked documents that contain at least one relevant bookmark in the lenient set ("Rank-Rel (lenient)"). As before, for both task types (single-aspect and multiaspect), we conducted a two-factor (interface and task) ANOVA at p  0.05 for all types of measurements. For multi-aspect tasks, the interface effect was significant for all measures; for singleaspect tasks, none of the effects was significant. We thus conducted a Bonferroni post-hoc test at p  0.05 between two interfaces for multi-aspect tasks; significant differences with the baseline are shown in bold. It can be observed that, when the participants used AspecTiles for multi-aspect tasks, they performed significantly fewer query reformulations, issued shorter queries, and clicked documents that are deeper in the ranked list, compared to the baseline interface. Though not statistically significant, similar trends can be observed for single-aspect tasks, except for the query length. We also examined other measurements such as the number of clicked documents and the time to complete the task session, but did not find any clear trends.
The above significant differences between AspecTiles and the baseline interface are quite intuitive. With AspecTiles, users perform fewer query reformulations probably because the displayed aspects serve as an alternative to forming new queries. For example, if the displayed aspects represent more specific information needs than the current query, then the user may not have to add a new query term to the current query to explicitly specialize his need. This is also reflected in the significant differences in the average query length. Moreover, with AspecTiles, users dig deeper in the ranked lists, probably because AspecTiles lets the user quickly skip documents that appear to be irrelevant to the desired aspects. That is, provided that the choice of displayed aspects and the peraspect relevance shown in colors are both sufficiently accurate, AspecTiles may be able to let the user reach the desired documents efficiently. Moreover, note that five-tile AspecTiles outperforms three-tile AspecTiles in all of these measurements, though the differences are not statistically significant.
4.2.2 Search Behavior Sample
Section 4.2.1 discussed the summary statistics that represent user behavior. We now discuss the actual operation sequences of the participants. Figure 6 shows some typical user operations handpicked from our multi-aspect tasks; similar data for single-aspect tasks are omitted due to lack of space. Each box represents a particular user operation on the interface, and a row of boxes represent a

Table 6: Number of query reformulations, query length, and

the average rank of clicked documents. The average and interquartile range values are reported. Significant differences with the baseline at p  0.05 according to the Bonferroni posthoc test are shown in bold.

Task

Baseline Three-tile Five-tile

type

AspecTiles AspecTiles

Single Query 2.313

1.724

1.548

reform (0-8)

(0-5)

(0-5)

Query length

2.585 (1-7)

2.532 (1-5)

2.646 (1-8)

Rank-All 7.502

8.899

8.285

(1-46) Rank-Rel 6.968

(1-49) 10.025

(1-47) 8.511

(strict) (1-27)

(1-39)

(1-47)

Rank-Rel 7.132 (lenient) (1-27)

9.929 (1-39)

9.155 (1-47)

Multi Query 3.900

2.370

1.552

reform Query

(0-13) 2.741

(0-10) 2.347

(0-9) 2.189

length (1-7)

(1-5)

(1-5)

Rank-All Rank-Rel

7.229 (1-70) 7.294

10.785 (1-81) 12.426

13.152 (1-95) 13.725

(strict) (1-40) Rank-Rel 7.282

(1-61) 12.108

(1-86) 13.347

(lenient) (1-40)

(1-61)

(1-86)

particular participant's task session over a timeline. The operations shown are: "select a highly/somewhat relevant bookmark of the lenient set" (number of aspects covered by the bookmark is shown); "select a Nonrelevant answer of the lenient set" (indicated by N)); "reformulating a Query" (Q); "click a Document" (D); "put mouse over a tile to show the Popup label" (P); and "increase number of Tiles" (T).
Figure 6 confirms some of our aforementioned findings more qualitatively, and offers some additional insights into user behavior. For example, by comparing the rows of five-tile AspecTiles with those of the baseline, it can be observed not only that the latter contains more query reformulation operations (Q's), but that participants often reformulated queries at the very beginning of the search interactions with the baseline interface (See lines (1) and (4)). With the baseline interface, the number of sessions beginning with an immediate query reformulation was 17 for single-aspect tasks and 12 for multi-aspect tasks; in contrast, with five-tile AspecTiles, the same statistic was 8 for single-aspect tasks and 4 for multi-aspect tasks. Moreover, it can be observed that the number of clicked documents before finding the first relevant bookmark (D's to the left of the first square with a number) tends to be smaller with AspecTiles, which is again in line with our quantitative results. Furthermore, it can be observed that, with AspecTiles, participants tend to find a larger number of relevant bookmarks and relevant aspects, although these trends were not statistically significant in our quantitative results.
Next, we discuss typical usage patterns of AspecTiles, which we have observed physically during the user experiments and also from the operation logs that we analyzed postmortem. An AspecTile user typically begins a session by checking the aspect labels of the initial search result. If the displayed aspects seem appropriate, he utilizes the tiles, i.e. place a mouse over a tile and see the aspect label as a popup text, and clicks the document. In Figure 6, this can

92

Figure 6: The participants' operation sequences in nine task sessions. The operations shown are: "select a highly/somewhat relevant bookmark of the lenient set" (number of aspects covered by the bookmark is shown); "select a Nonrelevant answer of the lenient set" (indicated by N)); "reformulating a Query" (Q); "click a Document" (D); "put mouse over a tile to show the Popup label" P; "increase number of Tiles" T.

be seen as a repetition of P's and D's (See lines (2), (3) and (6)). On the other hand, if the displayed aspects look unsatisfactory, the user often increases tiles and reformulate queries, as represented by T's and Q's in the figure (See lines (5) and (8)). For example, line (8) represents a multi-aspect task for "I'm looking for information on Rick Warren, the evangelical minister. (biography, articles, books)," where the initially displayed aspect labels were "controversy, obama, sermons." As the user judged that these displayed aspects did not directly match the three desired aspects, he immediately increased the number of displayed aspects (two T's at the beginning of the session). Again, this example suggests that the quality of the displayed aspects is important. Moreover, Figure 6 line (3) contains many nonrelevant bookmarks (N's), and this was because the document clicked by the user had a high estimated relevance to the aspect "(atari) classic games" even though it in fact discussed many other types of games besides Atari. Thus the user was misled by the color displayed by AspecTiles. Again, this highlights the importance of providing accurate relevance per-aspect estimates to the user.
As an additional and inconclusive remark, it appeared that while participants with high computer skills (e.g. researchers) tended to actively reformulate queries without referring to the tiles, those with relatively lower computer skills tended to rely on the tiles. While it is possible that AspecTiles may be more helpful to the latter class of users, we will have to design a new experiment with a larger set of participants to pursue this new research question.
4.3 User Satisfaction
Finally, we discuss the results of the exit questionnaire, which we summarized in Table 7. The participant's answer to each question was on a five point scale: from 1 (strongly disagree) to 5 (strongly agree). The Mann-Whitney's U test at p  0.05 was used to compare AspecTiles with the baseline. Note that only Question 6 was asked separately for three-tile and five-tile AspecTiles. It can be observed that the participants found AspecTiles useful for finding relevant information, easy to use and that they are willing to use it again, compared to the baseline (Q1-Q3). Moreover, participants seemed to prefer the five-tile version to the three-tile version (Q6), although the difference is not statistically significant. Recall that the user effectiveness was indeed generally higher with five-tile AspecTiles than with the three-tile version.
We also received some additional feedback from participants. Five participants remarked that they wanted to select or create their own aspects to display; other five remarked that reranking the search result by a selected aspect would be very useful. In summary, As-

pecTiles was generally popular with the participants, and some of them wanted even more advanced features in it.
For the results of the post-task questionnaire, we asked the participants to answer four questions: "Are you familiar with this topic?," "Was it easy to do the search on this topic?," "Are you satisfied with your search results?" and "Did you have enough time to do an effective search?," but we could not find any clear trends among the three interfaces.
5. CONCLUSIONS
In this study, we proposed a simple extension to the standard SERP interface for presenting a diversified search result for an underspecified query. AspecTiles shows a row of tiles to the left of each retrieved document, where each tile represents a particular aspect of the query and the per-aspect relevance degree of that document. To compare AspecTiles with the standard SERP interface in terms of usefulness, we conducted a user study involving 30 search tasks designed based on the TREC web diversity task topics as well as 32 participants. Our main findings are:
∑ In multi-aspect tasks, participants were significantly more efficient with AspecTiles than with the baseline interface, in terms of time to find the first relevant bookmark, number of documents clicked before finding the first relevant bookmark, the average time between finding two relevant bookmarks, and the average number of document clicks between finding two relevant bookmarks. On the other hand, the search effectiveness with AspecTiles was at least as high as that with the baseline: in fact, the five-tile AspecTiles significantly outperformed the baseline in terms of recall as well.
∑ In multi-aspect tasks, AspecTiles users used significantly fewer query reformulations, shorter queries, and dug deeper in the ranked list, compared to users of the baseline interface.
∑ Participants of our user study found AspecTiles significantly more useful for finding relevant information, easier to use, and were more willing to use it again than the baseline interface.
These results suggest that simple interfaces like AspecTiles can enhance the search performance and search experience of the user when their queries are underspecified. While we did not obtain statistically significant results for single-aspect tasks, the general trend suggests that AspecTiles may help these cases as well.

93

Table 7: Exit questionnaire results. The average and standard deviation values are reported. Significant differences with the baseline at p  0.05 according to the Mann-Whitney's U test are shown in bold.

Questions
Q1: Did you find the interface useful for finding relevant information? Q2: Was it easy to use the interface? Q3: Do you want to use the interface again? Q4: Was the color information intuitive? Q5: Were the aspect labels appropriate? Q6: Was the number of displayed aspects appropriate?

Baseline
2.875 (0.941) 3.156 (0.919) 2.875 (0.906)
-

Three-tile AspecTiles Five-tile AspecTiles

3.718 (0.581)

3.562 (0.800)

3.593 (0.837)

3.218 (1.099)

3.812 (0.997)

3.437 (0.877)

3.562 (0.913)

As our future work, we plan to enable the aspect-based reranking feature that was mentioned in Section 4.3, to enable both more efficient and more exploratory search. Moreover, as our results show that the quality of displayed aspects and the accuracy of per-aspect relevance estimates are important for maximizing the benefit of AspecTiles, we plan to improve the backend search system. Finally, as we discussed in Section 4.2.2, we would like to identify the class of users for which interfaces like AspecTiles would be most helpful.
6. ADDITIONAL AUTHORS
Shojiro Nishio (Osaka University, Japan, email: nishio@ist.osaka-u.ac.jp)
7. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Leong. Diversifying Search Results, In Proc. of WSDM 2009, pages 5≠14, 2009.
[2] C. Brandt, T. Joachims, Y. Yue, and J. Bank. Dynamic Ranked Retrieval, In Proc. of WSDM 2011, pages 247≠256, 2011.
[3] A. Broder. A Taxonomy of Web Search, ACM SIGIR Forum, 36(2):3≠10, 2002.
[4] M. Chau. Visualizing Web Search Results Using Glyphs: Design and Evaluation of a Flower Metaphor, ACM Transactions on Management Information Systems, Vol. 2, No. 1, Article 2, 2011.
[5] C. Chen and Y. Yu. Empirical Studies of Information Visualization: A Meta-Analysis, Int'l Journal of Human-Computer Studies, 53(5):851≠866, 2000.
[6] C. L. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 Web Track, In Proc. of TREC 2009, 2010.
[7] C. L. Clarke, M. Kolla, and O. Vechtomova. An Effectiveness Measure for Ambiguous and Underspecified Queries, In Proc. of ICTIR 2009, pages 188≠199, 2009.
[8] Z. Dou, S. Hu, K. Chen, R. Song, and J. -R. Wen. Multi-Dimensional Search Result Diversification, In Proc. of WSDM 2011, pages 475≠484, 2011.
[9] M. A. Hearst. TileBars: Visualization of Term Distribution Information in Full Text Information Access, In Proc. of CHI 1995, pages 59≠66, 1995.
[10] T. Heimonen and N. Jhaveri. Visualizing Query Occurrence in Search Result Lists, In Proc. of IV 2005, pages 877≠882, 2005.
[11] W. Hersh, A. M. Cohen, P. Roberts, and H. K. Rekapalli. TREC 2006 Genomics Track Overview, In Proc. of TREC 2006, pages 52≠78, 2006.
[12] W. Hersh and P. Over. Interactivity at the Text Retrieval

Conference (TREC), Information Processing and Management, 37(3):365≠367, 2001. [13] W. Hersh and P. Over. TREC-9 Interactive Track Report, In Proc. of TREC-9, pages 41≠50, 1999. [14] O. Hoeber and X. D. Yang. A Comparative User Study of Web Search Interfaces: HotMap, Concept Highlighter, and Google, In Proc. of WI 2006, pages 866≠874, 2006. [15] J. Mackinlay. Automating the Design of Graphical Presentations of Relational Information, ACM Transactions on Graphics, 5(2):110≠141, 1986. [16] H. Reiterer, G. Tullius. and T. M. Mann. INSYDER: A Content-based Visual-Information-Seeking System for the Web, Int'l Journal on Digital Libraries, 5(1):25≠41, 2005. [17] T. Sakai. Evaluation with Informational and Navigational Intents, In Proc. of WWW 2012, pages 499≠508, 2012. [18] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting Query Reformulations for Web Search Result Diversification, In Proc. of WWW 2010, pages 881≠890, 2010. [19] R. Song, M. Zhang, T. Sakai, M. P. Kato, Y. Liu, M. Sugimoto, Q. Wang, and N. Orii. Overview of the NTCIR-9 INTENT Task, In Proc. of NTCIR-9, pages 82≠105, 2011. [20] O. Turetken and R. Sharda. Visualization of Web Spaces: State of the Art and Future Directions, ACM SIGMIS Database, 38(3):51≠81, 2007. [21] C. Ware. Information Visualization: Perception for Design, Morgan Kaufmann, 2000. [22] Y. Xiang, M. Chau, H. Atabakhsh, and H. Chen. Visualizing Criminal Relationships: Comparison of a Hyperbolic Tree and a Hierarchical List, Decision Support Systems, 41(1):69≠83, 2005. [23] Y. Yamamoto and K. Tanaka. Enhancing Credibility Judgment of Web Search Results, In Proc. of CHI 2011, pages 1235≠1244, 2011.

94

Combining Implicit and Explicit Topic Representations for Result Diversification

Jiyin He J.He@cwi.nl

Vera Hollink V.Hollink@cwi.nl
Centrum Wiskunde en Informatica Science Park 123, 1098XG Amsterdam, the Netherlands

Arjen de Vries arjen.de.vries@cwi.nl

ABSTRACT
Result diversification deals with ambiguous or multi-faceted queries by providing documents that cover as many subtopics of a query as possible. Various approaches to subtopic modeling have been proposed. Subtopics have been extracted internally, e.g., from retrieved documents, and externally, e.g., from Web resources such as query logs. Internally modeled subtopics are often implicitly represented, e.g., as latent topics, while externally modeled subtopics are often explicitly represented, e.g., as reformulated queries.
We propose a framework that: i) combines both implicitly and explicitly represented subtopics; and ii) allows flexible combination of multiple external resources in a transparent and unified manner. Specifically, we use a random walk based approach to estimate the similarities of the explicit subtopics mined from a number of heterogeneous resources: click logs, anchor text, and web n-grams. We then use these similarities to regularize the latent topics extracted from the top-ranked documents, i.e., the internal (implicit) subtopics. Empirical results show that regularization with explicit subtopics extracted from the right resource leads to improved diversification results, indicating that the proposed regularization with (explicit) external resources forms better (implicit) topic models. Click logs and anchor text are shown to be more effective resources than web n-grams under current experimental settings. Combining resources does not always lead to better results, but achieves a robust performance. This robustness is important for two reasons: it cannot be predicted which resources will be most effective for a given query, and it is not yet known how to reliably determine the optimal model parameters for building implicit topic models.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval
General Terms
Algorithms, Experimentation
Keywords
Multi-source, Subtopics, Result diversification, Random walk
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08...$15.00.

1. INTRODUCTION
Queries in Web search are often short and underspecified [2]. For example, the query "python" may refer to a snake as well as a programming language, while the programming language "python" covers a wide range of subtopics, such as tutorials, documentations, and downloads. Without knowing the users' actual intent, result diversification deals with such queries by providing documents that cover as many subtopics of a query as possible, so that the average user's "unhappiness" is minimized [43].
Result diversification has been intensively studied and many diversification algorithms have been proposed [1, 7, 8, 21, 34, 35, 43]. This paper focuses on the discovery of subtopics, also referred to as query intent, facets, and sub-queries in the literature. Subtopics have been extracted internally, e.g., from documents retrieved in response to the query [7, 8, 26, 29], and externally, e.g., from Web resources such as query logs [1, 20, 21, 33, 35]. Internally modelled subtopics are often implicitly represented, e.g., as latent topics or document clusters, while externally modelled subtopics are often explicitly represented, e.g., as reformulated queries1. In particular, there has been much work that exploits explicit subtopics using various types of Web resources. Some take suggested queries from commercial search engines as subtopics of a query [35]; others mine the subtopics from resources such as taxonomies [1, 22], query logs[21, 33], anchor text and Web ngrams [20].
While both implicit and explicit subtopics extracted from various Web resources have shown their effectiveness in helping retrieving diverse search results, both topic representations have their limitations. Often, the process of extracting subtopics for a query in a Web resource is focused on finding the subtopics that are relevant to the query, while the relation among the extracted subtopics are not well preserved. For instance, all subtopics discovered in Web resources, e.g., all reformulations of a query, are treated as independent subtopics, regardless the fact that some may be synonymous [21, 35]. In some studies additional steps such as clustering were taken in order to avoid redundancy and find better defined topics [20, 33]. On the other hand, implicitly represented subtopics such as topic models or clusters constructed from document content convey much information about the relations or the structure of the topics present in the documents. Yet, in this case the infor-
1 Notice that in the context of result diversification, "implicit" sometimes refers to diversification methods such as MMR where no actual topics are modeled and only document similarities are used. In this paper, from a topic modeling perspective, we refer to topic models/clusters as implicit topic representations, since no "labels" that indicate the content of the topics are assigned, and refer to subtopics represented with external entities such as query suggestions, anchor text, as explicit topic representations, as such entities present the content of the topic explicitly [25].

851

mation that can be used is limited to the content of the document. Therefore we are interested in the following question:
∑ Can we make use of the information from both implicitly and explicitly represented subtopics?
Various resources were shown to provide useful information in subtopic mining, sometimes complementary to each other. Click logs have for example been shown an effective resource for subtopic mining in a number of studies [21, 33], but their availability are mainly limited to commercial web engines. Anchor text has been suggested a good proxy for query information, but it is not clear if one should be preferred over the other. Web ngrams are freely available, but may not provide as high quality subtopics for a query as click logs, partially because of the noise introduced by phrases not relevant to the topic of interest. Given a variety of resources, we are interested in a second question:
∑ Can we combine multiple external sources to help subtopic modeling?
We seek a principled way to model the subtopics of a query using multiple sources, including implicitly represented topics extracted from document content, as well as explicitly represented subtopics mined from various Web resources. While combining subtopics from multiple sources can be useful, it is non-trivial. Different resources provide evidence that a subtopic is relevant to a query from different perspectives, which leads to different types of measurements to quantify the similarity between subtopics. We propose a graph based approach to combine explicit subtopics from Web resources. More specifically, we construct local graphs over the subtopics extracted from each single resource. The local graphs are interconnected through subtopics that appear in multiple resources. A measure of subtopic similarity within each local graph is converted into random walk transition probabilities, to obtain a probabilistic framework. Subtopics from different sources are combined in a random walk over the resulting graph, that integrates the various external resources. The result of the random walk defines a probability distribution that encodes the similarity between all subtopics from all resources included in the graph. Given the similarities in the explicit subtopic graphs, we exploit these similarities for regularization during the construction of implicitly defined subtopics from the search result list. Using regularized topic models, the similarities among the explicit subtopics serve as additional constraints, that need to be satisfied when constructing implicit subtopics using document content.
Our contribution is two-fold. First, we propose a framework that: i) combines both implicitly and explicitly represented subtopics in a principled way; and ii) allows flexible combination of multiple external resources, in a transparent and unified manner. With respect to the proposed framework, an in-depth investigation on its effectiveness as well as limitations are provided. Second, using this framework, we compare the usefulness of various resources (and their combinations) for identifying diversification subtopics. Our findings provide additional evidence for findings in earlier studies on similar themes, and lay a foundation for future work on selecting useful resources for topic modeling.
The rest of the paper is organized as follows. In Section 2, we discuss the related work on subtopic modeling in result diversification, as well as work related to the techniques employed in this paper. In Section 3, we introduce our proposed framework, referred to as Multi-source subtopics (MSS), to combine explicit subtopics from multiple sources with the implicit ones derived from the initial search result list. In Section 4, we discuss the different types of resources considered for subtopic modeling. We then describe

the experiments designed to empirically study the properties of the proposed framework in Section 5, followed by a discussion on the experimental results and their implications in Section 6. Section 7 concludes the paper.
2. RELATED WORK
Search result diversification has a long history [5, 7, 10, 23, 37, 43] and a range of diversification approaches have been proposed previously. Maximum Marginal Relevance (MMR) [7], an early representative diversification method, aims to balance the relevance and diversity of a ranked list. A probabilistic version of MMR has been proposed by Zhai et al. [43], as part of their risk minimization framework. Zhu et al. [44] proposed a ranking method based on random walks in an absorbing Markov chain. By turning ranked items into absorbing states, the method prevents redundant items from receiving a high rank. Yue and Joachims [42] studied a learning algorithm based on a structural Support Vector Machine (SVM) that identifies diverse subsets in a set of documents. Carterette and Chandar [8] proposed a probabilistic model for faceted retrieval, where the facets of a query are modeled with Latent Dirichlet Allocation (LDA) [4], one of the state-of-the-art topic modeling approaches. In most of these studies, the focus has been on the development of ranking algorithms to produce ranked lists that convey both relevant and diverse information about a query. Only limited effort has been made in mining high quality subtopics for a query, and subtopics were usually modeled based on document content.
Recently however, in the context of Web retrieval, a renewed interest in result diversification has emerged. Representative approaches include Intent Aware select (IA-select) [1] and explicit query aspect diversification (xQuAD) [35]. Common to these approaches is the importance of modeling both the subtopics of a query and the relevance of a document with respect to the subtopics of a query. Both works exploited Web resources for subtopic mining. In [1], categories in the Open Directory Project (ODP) taxonomy were used as candidate subtopics, and in [35], query suggestions from Web search engines were used as related subtopics of a query. Recently, Dou et al. [21] has first proposed to combine subtopics mined from multiple sources, including query logs, anchor text, document clusters, and web sites of search results. A common assumption among studies where Web resources are used for subtopic extraction is that the extracted subtopics would be independent. However, these assumptions are most likely not true. Anchor text has for instance been shown to be an effective substitute for query logs in generating query reformulations [19], which indicates that these two resources should be correlated. Moreover, within a single resource such as query logs, various reformulations of a query may be synonyms that refer to the same concept. Our approach does not require to assume the independence among mined subtopics. Conversely, we focus on modeling specifically the similarity or relatedness among the subtopics. In our experiments, we employ IA-select as the primary diversification method (see Section 5.3), to indirectly evaluate the effectiveness of using our approach to model subtopics from multiple sources.
A closely related line of research focuses on mining subtopics of a query from various Web resources. Radlinski et al. [33] infer query aspects from query logs, using clicks and session information to model the relations between queries. Dang et al. [20] infer query intent from anchor text and Web ngrams. In both studies, clustering has been applied to the extracted aspects or intents, so that topically redundant or similar entities (queries, anchor text, or Web ngrams) are grouped together. In this paper, we use the same types of Web resources for subtopic mining, and compare their effectiveness in the context of result diversification; although in terms of a query

852

log resource, our data set can only be seen as a very small subset of the log used in [33]. Instead of applying clustering, we incorporate the obtained similarities between the extracted subtopics into the topic models constructed using document content. This way we incorporate the similarity between extracted subtopics into the similarity between documents.
To compute the similarity between subtopics extracted from different Web resource, we construct graphs over these subtopics and compute their associations using Markov random walks. Similar approaches have been studied in a wide range of topic in the context of query log analysis. For example, Craswell and Szummer [16] studied the usage of Markov random walks over the bipartite graph constructed from click logs for document ranking. Fuxman et al. [22] developed a random walk algorithm for keyword generation from query click graphs. This approach was also adopted in [1] to extract subtopics related to a query from the ODP taxonomy. Ma et al. [30] proposed a query suggestion approach based on Markov random walk and hitting time on the query-URL bipartite graph, aimed at suggesting both semantically relevant and diverse queries to Web users. A recent work that is closely related to ours is the Multi-view random walk (MVR) approach proposed by Cui et al. [18], which aims to combine knowledge from different views of an object in a random walk process. We discuss the differences between their work and ours in detail in Section 3.
To balance graph-based and textual similarity between documents, we use regularized topic models [6, 24]. A similar approach was taken in Guo et al. [24], where queries were enriched with document snippets to improve a query similarity model; we however enrich the document content with queries to construct a document similarity model.
3. MULTI-SOURCE SUBTOPICS
3.1 Modeling relation among explicitly represented topics from Web resources
We now detail how we use a Markov random walk based approach to compute the relations between the explicit subtopics of a query in different types of resources. Let R = {ri}Ni=1 be a resource that contains N subtopics related to a query q. Consider for instance a query log that contains N reformulations of q. For a set of resources R = {Rg}, we construct a network consisting of subgraphs on multiple parallel `planes', as shown in Figure 1. First, for each Rg, we construct a weighted graph Gg = {Eg; Rg}, where the nodes Rg corresponding to the subtopics in resource g lie on a single plane, and the edges Eg are weighted by the similarity w(ri, rj) between the two nodes. For now, we abstract from the specific way the similarity is defined within a plane, aiming for a method that can be applied generically; details of the graphs created for specific resources are deferred to Section 4. In the second step, we interconnect nodes from different planes Ga and Gb with an extra set of edges, where each edge connects the nodes ri  Ga, rj  Gb that we consider equivalent. Throughout this paper, we consider ri and rj equivalent whenever they have exact matching text representations. While it would be interesting to include non-exact matches, we leave this for future investigation.
For each resource considered, the similarity scores computed may encode different semantics, such that they may not be compared directly across different planes. This is the primary reason why we are not in favor of constructing a single graph that mixes up subtopics from different resources. To resolve this problem, we first transform the similarity scores into 1-step random walk transition probabilities. The only assumption behind the resulting transition probability is that the chance to travel from one subtopic to

another in a random walk step should depend on the similarity between these subtopics, i.e., the more similar two subtopics are, the more likely the transition from one subtopic to the other can happen.
By applying a t-step random walk over the graphs, we expect to find new associations, in particular between subtopics that were not directly connected in a single graph. Further, the random walk sums up the transition probability of all paths of length t between two nodes. The transition probability will be high if there exist many paths from one node to the other. The higher the transition probability the more similar or closely related the two nodes are. During the random walk, the underlying clustering structure of the nodes emerges [36]. If such a clustering structure exists among the nodes, with longer walks, the probability between nodes within a cluster converges to a value higher than that with any node outside the cluster [16]. The probability distribution that arises can therefore be interpreted as a similarity measure that captures the relations between the subtopics in our graphs.
We use p1(rj|ri) to denote the 1-step transition probability from ri to rj within a single plane, which is computed as

p1(rj|ri) = w(i, j)/ w(i, j).

(1)

j

We refer to the nodes connected to nodes in other planes as
"teleport points" and the transition from one plane to another as a "between-plane teleport". We use p1 (rj|ri) to denote the 1-step between plane transition probability from ri to rj. While it is possi-
ble to assign a different value for each pair of nodes, e.g., based on
additional evidence or the performance on training material, in this
paper we only study the simplified situation assuming each plane has a fixed p1 . p1 can be viewed as the probability that a plane is chosen as the destination of a teleporting. For simplicity, we use
g to denote this probability for plane g. In summary, we have

p1 (rj |ri) =

0 g

if ri is not a teleporting point, otherwise, where rj  Gg.

(2)

Piecing together the within-plane and between-plane transition probability, while conducting the random walk, the 1-step transition probability from ri to rj is computed as follows.

p1(rj |ri) =

p1(rj |ri) p1 (rt|ri)p1(rj |rt)

if ri, rj  Gg, otherwise,

(3)

where rt is the teleport point in the plane of rj connected with ri. A t-step walk from ri to rj is defined recursively as

pt(rj |ri) = p1(rj |rk)pt-1(rk|ri),

(4)

k

where rk's are the intermediate nodes that directly connect to rj. At first sight, our proposed representation may look similar to
the multi-view random walk (MVR) model [18]. In MVR however, each plane represents a different view of the exact same set of objects; i.e., each object within the network has a corresponding node at each plane. In our setup, this correspondence is not required, and therefore a phrase occuring in, for example, an anchor text collection does not necessarily have to occur in a query log as well. Not requiring such a correspondence allows for a greater diversity of resources used. Also, MVR defines a 1-step cross-view transition only between corresponding nodes. In principle, although we have not yet studied this alternative setting, our representation allows to consider also a between-plane 1-step transition from a node in one plane to multiple nodes in the other plane, i.e., setting the p1 (j|i) with non-zero values to each pair of nodes i and j.

853

Figure 1: An illustration of the mutli-source subtopic network. Solid lines indicate within plane transitions, and dashed lines indicate the between plane teleporting. Each of the nodes I, K, J, L, M, are possible intents for the same query. Each plane represents intents generated from a single data source.

3.2 Combining implicit and explicit subtopics
Topic models, such as the probabilistic latent semantic analysis (pLSA) [28], are commonly used to model underlying topics associated with a set of documents. The extracted topics are implicitly represented by a set of word distributions. For a query q, we apply pLSA on the set of retrieved documents D = {di}M i=1 to obtain the implicit subtopics associated with q.
Now that we have described our approach to model the relations between subtopics extracted from multiple resources, the next question is: how can we combine the relations between the explicit subtopics with the implicit subtopics? Notice that for diversification, the ultimate goal is to generate a ranked list of documents that are both relevant to the query and contain diverse content. Therefore it is necessary to associate the mined subtopics to the documents to be ranked, and convert the relations between subtopics to the relations between documents.
We first translate similarities between explicit subtopics to the similarities between documents as follows.

p(di|dj ) = p(di|rk)pt(rk|rl)p(rl|dj )

(5)

k,l

To obtain p(r|d), we use r as a query and compute the query likelihood score as defined in a language modeling retrieval model [32]. Then following the Bayes' rule, p(d|r) is computed as

p(di|rk) = p(rk|di)p(di)/p(rk),

(6)

where p(rk) is computed by marginalization as diD p(rk|di), and p(di) as 1/|D|, assuming a uniform distribution of d  D.
Given these preliminaries, we now combine the similarities between documents, obtained from the explicit subtopics, with the implicit topics. Hereto, we apply Laplacian pLSA [6] (also referred to as regularized topic models [24]), using the document similarities given by Eq. 5 to regularize the implicit topic model. Laplacian pLSA employs a generalized version of EM to maximize the regularized log-likelihood of the topic model, L:

L = L-1 2

(P (zk|di) - P (zk|dj ))2p(di|dj ) (7)

k i,j

Here, L is the log-likelihood of the implicit topic model as maximized by pLSA. P (zk|di) is the probability of topic zk given document di.  is a parameter that controls the amount of regularization from external resources.
By maximizing the regularized log-likelihood, Laplacian pLSA (softly) assigns documents to the same cluster if they 1) share many terms and 2) belong to the same explicit subtopics.  allows us to

balance these two requirements and combine both implicit and explicit representations of query subtopics in a unified and principled manner. Notice that when no explicit subtopics can be found for a query, the regularized pLSA is reduced to the normal pLSA.

4. RESOURCES
We now describe the implementation of each plane of the network. In this paper, we consider three resources: click logs, anchor text and Web ngrams (summarized in Table 1).

Graph GC GA GN

Nodes search queries anchor texts web ngrams

Edge weights coclicked documents coocurrence in text passages coocurrence in text passages

Table 1: Used resources

4.1 Click logs
Clickthrough data has proven to provide useful information for identifying search intents, or, related topics of queries [24, 33]. In particular, if two queries share many co-clicked documents, it is likely that they convey similar topical information [3, 39].
We construct a graph GC from click logs, using logged queries as nodes, following a rather standard approach of using the total number of co-clicked documents to set the edge weights for random walk (see e.g. [16]). Since our goal is to find subtopics related to a query, unlike in many other studies where a global graph is constructed over all logged queries, we construct local graphs for a given query. The advantage of using local graphs is two-fold: i) it is computationally efficient, and ii) queries that are not relevant to the original query are effectively pruned. In order to collect related subtopics for a query q, we find the queries that share co-clicked documents with q, and for each of these queries further find its coclicked queries. We then filter out queries that has a 1-step (or 2step in the latter case) transition probability less than 0.01 starting from q in order to prevent popular queries such as "yahoo" from connecting to many weakly related queries.
4.2 Anchor texts
Anchor text has proven to be another effective feature for various search tasks, e.g. [19]. We use the anchor texts from the ClueWeb09 collection [27] to construct an anchor text graph GA, using the method described by Dang et al. [20]. As preprocessing, we remove all anchors that are connected to only one URL. For a given query, we collect the N (N = 100 in our experiments) most frequently occurring anchors that contain all of the query terms. These anchors become the nodes of the graph. The edge weights are computed according to the method that proved most effective in the study of Dang et al. [20]: on the basis of the number of times the anchor phrases co-occur in text passages in the collection.
4.3 Web n-grams
Following Dang et al. [20], we collect Web n-grams using the Microsoft Bing N-gram [38] service to build an n-gram graph GN . For each query q, we retrieve the M (M = 1000) terms t with the highest probability of seeing t after query q in the body text of the web pages indexed by Bing, pngram(t|q). As the N-gram service provides n-grams up to length 5, this graph can be constructed for queries consisting of 4 or less terms. Dang et al. [20] use the N terms with the highest pngram(t|q). However, these often include common terms that are not specific to the queries. Therefore, we

854

Sample subtopics GC
anti spy microsoft spyware antispyware microsoft beta windows defender GCA
space defender 1 0 defender industries microsoft beta a public defender tri state defender

subtopic windows defender windows defender windows defender windows defender microsoft antispyware
subtopic star defender 4 defender industries inc windows defender public defender chicago defender

score .2261 .2262 .2265 .2260 .1218
score .1266 .2055 .1062 .1160 .1035

Top 3 related subtopics

subtopic

score subtopic

microsoft antispyware .1208 defender

microsoft antispyware .1208 defender

microsoft antispyware .1207 defender

microsoft antispyware .1209 defender

defender

.1141 antispyware

subtopic

score subtopic

star defender 3

.1266 star defender 2

defender

.1197 windows defender

microsoft defender

.0555 microsoft s windows defender

public defender's office .1040 office of the public defender

the chicago defender .1035 national legal aid defender association

score .1122 .1121 .1121 .1120 .0995
score .1266 .0462 .0538 .1040 .0352

Table 2: A random sample of 5 subtopics related to the query "defender" from GC and GCA and the top 3 subtopics related to each of the sample subtopics. The scores are the result of a 5-step random walk on the corresponding graphs.

compute for each retrieved term the lift [41]:

lift (t, q) = pngram(t|q)/pngram(t)

(8)

where pngram(t) is the a priori probability of seeing term t. The
N (N = 100) n-grams (= query + term) with the highest lift are
included as nodes in the graph. Following Dang et al. [20], the weights of the edges are computed in the same way as in GA.

4.4 An example of Multi-source subtopics
Having introduced the MSS framework and the target Web resources, now let us see an example that illustrates the result of the random walks on the subtopic graphs.
Table 2 shows an example result of a 5-step random walk on two different graphs, GC and GCA, i.e., the graphs constructed using click logs, and the combination of click logs and anchor text, respectively. We take the query "defender" and randomly sample 5 subtopics related to this query from each graph. For each sampled subtopic, we list the 3 most similar subtopics (from left to right), where the similarity is measured by the transition probabilities starting from the sampled subtopics as a result of the random walk. In this example, GC contains 21 subtopics, and GCA contains 118 subtopics. We see that all the subtopics sampled from GC are closely related to the security software "windows defender", which indicates that this is a dominant topic in GC . The sample drawn from GCA, on the other hand, covers diverse topics: computer games (space defender, star defender, etc., ), defender industries, windows defender, public defender, and subtopics about the newspapers "tri-state defender" and "chicago defender". On top of that, if we take a close look at the resulting similarity scores, we see that these scores effectively reflect the semantic relatedness between the subtopics. For example, "defender industries" is closely related to "defender industries inc", as both can be interpreted as the marine and boat supply company "Defender Industries Inc."; it is loosely related to "defender", as it is the original query, which is vague and can be interpreted as anything; it is not very likely to be related to "windows defender", and correspondingly, a very weak relation is indicated by the similarity score.

5. EXPERIMENTS
5.1 Research questions and experimental setup
In this section, we empirically investigate the properties of the proposed framework and its impact on result diversification.
Our framework combines different types of resources at two levels. First, it combines the implicitly and explicitly represented

subtopics by constructing regularized topic models. It is therefore natural to investigate the following research question.
RQ1 Does regularization with subtopics extracted from external resources help to form better topic models?
Although the question asks whether "better topic models" are formed, it is difficult to access the quality of topic models directly, due to the subjective nature of topics. Instead, by applying the resulting topic models to diversification, we indirectly assess their quality by examining the diversification performance. It is reasonable to assume that better topic models lead to better diversification results.
Further, the framework combines explicit subtopics from multiple external resources, which leads to the following two questions.
RQ2 How do various subtopics from external resources and their combinations compare in terms of diversification performance?
RQ3 Are combinations of subtopics from different external resources more effective in terms of diversification performance than that of single resources?
Particularly, previous studies suggest that anchor text is an effective replacement for query logs (which are often not publicly available), e.g., for query reformulation [19] and inferring query subtopics [20]. Comparison of the two resources within our framework can provide supplementary evidence and validate these findings.
Note that our framework outputs regularized topic models of a query, i.e., an implicit topic representation. Like any topic model based approach, LapPLSA (Laplacian pLSA) depends on a prefixed parameter, the number of topics K. There is no easy solution to find the optimal K without prior knowledge or sufficient training data. In our case, neither is available. However, in reality, "neither is available" is often the practice. Hence, it is useful to investigate the robustness of the framework when K is not optimal.
RQ4 How sensitive is the performance of diversification based on LapPLSA to the choice of K?
5.2 Data
We use the Clueweb09 setB dataset and the topic sets released at TREC2009-2011 Web Track diversity task as our test collection [12≠14]. Since the topic sets are designed to be different, e.g., the 2011 topic set is expected to contain "tougher" queries [14] than the others, we experiment with topics from each set separately.
The following data are used as the Web resources that provide query subtopic information. We use the MSN query log [17] to construct GC . Notice that this log is fairly limited: it contains 15 million queries from US users, sampled over 1 month. Following [20],

855

Graph

Coverage

1-50 51-100 101-150

GC

39 37

21

GA

48 47

25

GN

48 45

34

GCA

48

48

31

GCN

50

48

39

GAN

50

48

39

GCAN 50

48

39

Table 3: Coverage of Web resources over the TREC topic sets

we use the Microsoft Ngram service to construct GN , and anchor texts extracted from Clueweb09 dataset to construct GA[27].
Table 3 shows the coverage of the Web resources on the TREC topics. A TREC topic is "covered" by a resource if the corresponding graph of the query is not empty. We see that in terms of individual resources, click logs has the lowest coverage on all three topic sets. In terms of topic sets, the set from TREC 2011 is obviously poorly covered. Combining multiple resources increases the coverage over all three sets, but still, not all topics are covered.
5.3 Diversification method
After pilot experiments with a number of state-of-the-art diversification methods, including IA-select [1], xQuAD[35] and MMR [7]2, we decide to focus on IA-select. Results between IA-select and xQuAD are comparable, while MMR has a lower performance. The advantage of IA-select is that it has no extra parameters to be tuned: both xQuAD and MMR have an extra parameter that linearly combines relevance and diversity. IA-select provides a more transparent way to analyze the behavior of our framework.
Given a set of candidate documents and a set of subtopics related to a query q, IA-select [1] selects a document d to be included in the ranked list base on: the relevance of d to q, and the probability that d covers subtopics that all previously selected documents failed to do so. The key elements used in the algorithm can be reduced to the following quantities: i) V (d|q, z), the probability that d is relevant to q when the intended subtopic is z; and ii) p(z|q), the probability that subtopic z is related to q.
The first quantity is determined by the retrieval score of d given q, weighted by the likelihood that d covers subtopic z, i.e., p(d|z), which can be derived from the resulting topic models.
We compute p(z|q) as follows.

p(z|q) = p(z|d)p(d|q),

(9)

d

where p(z|d) is given by the topic models, and

p(d|q) = p(d|r)p(r|q).

(10)

r

where r are the nodes in the graphs; p(d|r) is calculated using Eq. 6 and p(r|q) is given by the result of random walks. Since our graphs were constructed locally using the original query q as the starting point, all graphs will at least contain the original query as a node. If the external resources do not contain any subtopics related to q, Eq. 10 reduces to the relevance score of d to the original query q.
We use IA-select to re-rank a pre-retrieved set of documents. To create the baseline ranked list, we use the Indri toolkit that imple-

2Unlike IA-select and xQuAD, MMR does not actually require subtopics being modeled. Here, when computing the similarities between documents, documents are represented by topic distributions in stead of term distributions.

Figure 2: Change of probability distributions during random walks. X-axis shows the number of steps; Y-axis shows the KLdivergence between the states before and after a 1-step walk.
ments the retrieval model based on the language modeling framework [31]. We use the spam list released at TREC Web Track [15] for spam filtering, setting the spam threshold to 70%. The top ranked 100 documents are used to construct topic models.
5.4 Parameter settings
Our proposed framework comes with a number of parameters. For random walks, two parameters are involved, namely , the between plane transition probabilities, and t, the number of steps for the random walks3. For LapPLSA, we have: the regularization parameter , and the number of topics K.
As discussed in Section 3.1,  is practically the probability that a plane is chosen as the destination of a teleporting. As we do not have evidence or prior knowledge about whether one plane should be preferred over others, we simply assume a uniform distribution over all planes: g = 1/|g|, where |g| is the total number of planes.
Since all our graphs are fairly small due to the fact that they are constructed locally based on subtopics that are closely related to a given query, random walks on these graphs converge very quickly. With a few preliminary experiments, we find that within 5 steps, the changes of the probability distributions over most graphs drop to a level that is almost negligible. Therefore we set t to 5 steps. A typical example is shown in Figure 2, where we use KL-divergence to measure the change of probability distributions over a graph. The graph consists of subtopics from click logs, anchor text and Web ngrams. The box plot is constructed over 150 TREC topics.
Regularization parameter  can take the value of non-negative real numbers; Researchers have usually set its value empirically [6, 24]. Preliminary experiments with our method suggest that the endto-end diversification results are relatively stable when varying  within a range around 10, although fine tuning  can indeed lead to improved results for a specific experiment. We decide to set  to a fixed value that generates reasonable diversification results, using  = 10 in all our experiments. Finally, note that  = 0 makes LapPLSA equivalent to pLSA without regularization.
While results are relatively stable with respect to , we find that the performance of diversification with topic models is rather sensitive to the parameter K. In Section 6, we will discuss the impact of K on the diversification results using our framework.
5.5 Evaluation metrics
We evaluate the diversification result in terms of -nDCG@20 [11] and ERR-IA@20 [9], which are primary evaluation metrics at TREC Web Track for the diversity task [14].  is set to 0.5 for -nDCG@20. For significance testing, we use the Wilcoxon sign rank test. When
3The implementation of the MSS random walks can be downloaded at http://code.google.com/p/mss-rw/

856

reporting results, we use ( ) to indicate a significant difference with p-value<.05 (.01).
6. RESULTS AND DISCUSSION
We now discuss the experimental results and their indications to the answers to our research questions. In the following discussion, we will first discuss the behavior of LapPLSA regularized by subtopics from different types of external resources in terms of the general trends shown by the results. We then zoom in to a few specific settings of K and examine the effectiveness of the proposed framework, e.g., highlighting the significance of the observed differences between approaches. Although the number of topics K is an important parameter that has an impact on the diversification result, we do not attempt to optimize it. We do investigate the sensitivity of our results to the estimation of K, when discussing RQ4.
6.1 Single resources
We start with the performance of LapPLSA using single resources. Figure 3 shows the result of IA-select using topic models constructed with the following methods: pLSA without regularization and LapPLSA regularized by similarity matrices generated using click logs, anchor text, and Web ngrams, i.e., LapPLSA_C, LapPLSA_A, and LapPLSA_N, respectively. "Baseline" refers to the run without diversification.
First, we see that both pLSA and LapPLSA (with different resources) can outperform the baseline. Compared to pLSA, LapPLSA shows more robust performance: diversification with pLSA can underperform the baseline given an improperly set K, while diversification with LapPLSA regularized by the subtopics from an external resource in general outperforms the baseline irrespective of the choice of K. The only exception is the case where K = 2, which is presumably not a sensible choice for K.
Second, judging from Figure 3, the effectiveness of each resource differs on different topic sets. It is noticeable that on topic set 1-50, click logs remarkably outperform the other two resources across all settings of K. A possible explanation is that this topic set is derived from query logs of commercial search engines [12], and therefore the click logs have a relatively high coverage and turn out to be an effective resource for these topics. On topic set 51-100, click logs and anchor text show comparable performance, while Web ngrams are occasionally effective (given a specific settings of K). On topic set 101-150, anchor text generally outperforms click logs. This may due to the fact that the click logs have a very low (< 50%) coverage on this topic set, and that the topic set is rather recent (created in 2011) while the click logs were created in 2006, which may lead to further sparseness: e.g., on average, GA has 17.1 nodes per query, while GC only has 7.6 nodes per query on this topic set. In general, click logs and anchor text seem to be more valuable resources for regularization compared to Web ngrams, across different settings of K. Notice that the Web ngrams are primarily derived from document content, so perhaps their lower effectiveness can be explained by lower influence on pLSA, which also uses document content. To some extent, we can consider the Web ngrams more similar to the document content than click logs and anchor text.
As expected, the diversification results of IA-select based on both pLSA and on LapPLSA are sensitive to the change of the parameter K. In particular, there is no clear correlation between the number of clusters and the end-to-end diversification performance, which further suggests the difficulty of finding an optimal K (that would fit for a set of queries). For more detailed analysis on the parameter K, see Section 6.4.

6.2 Combining multiple resources
Figure 4(a)-4(c) show the result of combining subtopics from the two relatively more effective resources, namely click logs(GC ) and anchor text (GA). While we only show the combination of these two resources, a similar trend can be observed in the other combinations. We see that combining resources does not always lead to improved diversification results over that of the single resources. Such improvement only happens in a few cases where K is likely to be in an optimal setting for the combined resource GCA, i.e., K = 10 in Figure 4(b) and K = 6 in Figure 4(c). In general, the combination of different resources leads to an "averaged" performance compared to the individual resources that are combined.
This observation is reasonable. Intuitively, combining resources on the one hand increases the coverage of subtopics, e.g., as shown in Table 3, but on the other hand may introduce noise, e.g., if one of the resources contains low quality subtopics. Recall that when constructing the local graphs, their nodes (i.e., subtopics) were selected based on their relation to the original query. In this stage, the relevance of the nodes with respect to the original query was taken into account. However, when combining resources, i.e., during the random walk stage, the goal was to measure the similarities among nodes across resources and no further pruning was conducted. That is, all entries in different graphs were assumed to be equally good. Given our observations on the combined result, a natural step for future work would prune further to prevent low quality resources from deteriorating high quality resources.
6.3 Zoom in on specific settings of K
Now, let us zoom in to the performance of diversification under specific settings of K. Specifically, we aim to examine the answers to the following questions: 1) are the differences between the diversified result and the baseline significant? and 2) are the differences between the LapPLSA regularized by subtopics from different external resources and the non regularized pLSA significant? For the first question, we consider the setting when K is set to the optimal value for a given topic set. For the second question, since both pLSA and LapPLSA are sensitive to K, we set K to the optimal value for each query, so that the impact of K is reduced to the minimum. Note that here by saying "optimal", we mean the best K in the range of [2, 10] in terms of -nDCG@20.
Table 4 and Table 5 show the results with respect to the first and the second questions, respectively.
From Table 4 we see that with a proper setting of K, in most cases, diversification with pLSA as well as with LapPLSA significantly outperforms the baseline. Again, the effectiveness of each individual resource differs for the three topic sets. For topics 1-50, regularization with click logs is most effective, while regularization with anchor text and with Web ngrams fails to have significant improvement over the baseline. On topic set 51-100, while pLSA based diversification does not result in significant improvement over the baseline in terms of both ERR-IA@20 and -nDCG@20, all LapPLSA based runs show significant improvement in terms of at least one of the two measures. On topic set 101150, only the run with LapPLSA using click logs does not achieve significant improvement over the baseline, all others do.
Table 5 shows that LapPLSA regularized with subtopics from external resources does not always lead to significant improvement over pLSA. However, on each of the three topic sets, at least one resource exists that, when used for regularization, outperforms pLSA significantly. Again, regularization with click logs is shown to be most effective, on topic set 1-50. On topic set 51-100, except click logs, regularization with one of the resources or their combinations results in significant improvement over the non-regularized pLSA.

857

(a) Topics 1-50

(b) Topics 51-100

(c) Topics 101-150

Figure 3: Result of diversification in terms of -nDCG@20 with topic models constructed using single resources. IA-select is used as the diversification method. The x-axis represents the value of K. The y-axis represents the -nDCG@20 scores.

(a) Topics 1-50

(b) Topics 51-100

(c) Topics 101-150

Figure 4: Result of diversification in terms of -nDCG@20 with topic models constructed using multiple resources. IA-select is used as the diversification method. The x-axis represents the value of K. The y-axis represents the -nDCG@20 scores.

Topics 1-50

Topics 51-100 Topics 101-150

Method K E-IA@20 nD@20 K E-IA@20 nD@20 K E-IA@20 nD@20

NoDiv ≠ .130 .215 ≠ .161 .246 ≠ .354 .456

pLSA 5 .136 .222 9 .175 .259 6 .365 .472 GC 5 .149 .236 10 .184 .273 8 .382 .479 GA 7 .138 .227 7 .182 .266 7 .386 .486 GN 5 .138 .225 10 .186 .272 7 .379 .478 GCA 7 .139 .229 10 .186 .276 6 .385 .486 GCN 10 .145 .228 7 .182 .267 6 .377 .481 GAN 10 .138 .224 10 .187 .270 7 .376 .477 GCAN 7 .135 .224 10 .187 .274 8 .386 .482

Table 4: Diversification result with pLSA and LapPLSA regularized by different external resources and their combinations. All runs are compared to the baseline NoDiv. Boldface indicates the highest score among all runs.

Topics 1-50

Topics 51-100 Topics 101-150

Method E-IA@20 nD@20 E-IA@20 nD@20 E-IA@20 nD@20

pLSA .149

.234

.193

.276

.397

.499

GC .164

.257

.201

.293

.410

.511

GA .147

.240

.200

.290

.410

.509

GN .145

.235

.201

.287

.409

.507

GCA .151

.244

.207

.299

.410

.510

GCN .153

.241

.200

.289

.402

.505

GAN .144

.234

.196

.283

.395

.499

GCAN .148

.240

.198

.289

.413

.512

Table 5: Comparing LapPLSA and pLSA. All runs are compared to pLSA. All the scores are significantly greater compared to the baseline NoDiv in Table 4. Boldface indicates the highest score among all runs.

In fact, the performance of regularization with click logs is still decent; testing for significance of the difference between run GC and run pLSA has a p-value of 0.077 for ERR-IA@20 and 0.059 for -nDCG@20. The TREC 2011 topic set seems the most difficult one. Regularization with most resources or their combinations does not lead to significant improvement over the pLSA run. The only exception is the combination of the click logs and the Web ngrams. This result is to some extent consistent with statement in the TREC Web Track guideline that the topic set "introduces "tougher" topics, ...they can rely less on click/anchor information, and popularity signals like PageRank."[14]. Combining all three resources seems to be a relatively safe choice: it improves significantly over the pLSA run on two out of the three topic sets, and on the third topic set, although the difference is not statistically significant (with a

p-value of 0.1 for ERR-IA@20 and 0.054 for -nDCG@20), the highest absolute score is achieved across all settings on this set.
6.4 A robustness analysis on the parameter K
From previous experiments, we have seen that the number of topics K is an important parameter, whose optimal value is difficult to predict. Further, we also see in Figure 3 and Figure 4 that across different settings of K, in most cases the averaged performance of LapPLSA exceeds that of pLSA. Given this observation, we are interested in the question: is regularized pLSA likely to outperform non-regularized pLSA no matter the value of K we select?
The above question can be reformulated as follows. Assume we have two samples of diversification results in terms of -nDCG@20. Sample 1 is the result of diversification using pLSA for varying K, and sample 2 is the result of diversification using LapPLSA regu-

858

Resource
GC GA GN GCA GCN GAN GCAN

1-50 W p-value 122 .0005 118 .0028 112 .0188 121 .0007 116 .0056 108 .0503 109 .0400

51-100 W p-value 117 .0040 113 .0141 101 .1902 118 .0028 109 .0400 110 .0314 112 .0188

101-150 W p-value 107 .0625 123 .0003 109 .0400 114 .0106 118 .0028 113 .0142 119 .0019

Table 6: Comparing performance of LapPLSA and pLSA over random K's. Boldface indicates that the W value of a combined resource is equal or above the lowest W of the single resources that are combined.

larized by certain external subtopic resource, also for varying K. If we randomly pick a score from each sample, how probable does the score from sample 2 exceed the score from sample 1? This can be tested with a Wilcoxon ranksum test [40]4.
Table 6 shows the result. Each sample contains 9 observations, i.e., for K = 2, ..., 10. W is the rank sum statistics , where a larger W indicates a more extreme difference between the two samples. Two observations stand out. First, in all cases but three(GAN on topics 1-50, GN on topic 51-100, and GC on 101-150), the differences between pLSA and LapPLSA are significant with a p-value < 0.05. That is, with a random setting of K, LapPLSA regularized with external resources tends to outperform non-regularized pLSA. Second, in most cases, the W value of those combined resources are in between (occasionally above) the resources that are combined. This is consistent with the observation made in Section 6.2. Further, compared to GC and GA, GN has a relatively lower W on all three topic sets, which suggests that with a random K, LapPLSA regularized with GN is less likely to improve over pLSA compared to GA and GC .
6.5 Summary
We conclude the experimental analysis by relating our findings to the research questions formulated before (See Section 5.1).
With respect to RQ1, we find that LapPLSA regularized with explicit subtopics extracted from good resources improves diversification results, which indicates that better topic models are formed.
With respect to RQ2, we find that different resources are effective on different topic sets. Futher, based on the observation in Figure 3 and the results discussed in Section 6.4, we conclude that the effectiveness of Web ngrams is the least robust, or more sensitive to the setting of K compared to anchor text and click logs. As mentioned before, we suspect that the Web ngrams are more similar to the document content than truly external resources, which could explain the observed difference.
For RQ3, we find that combining resources does not always improve the diversification result. The combined resource usually results in a diversification performance in between that of the individual resources combined.
In terms of RQ4, we find that LapPLSA regularized with explicit subtopics tends to outperform the non-regularized pLSA for cases where we do not optimize the setting of K, and simply choose it at random from a reasonable range. We therefore conclude that
4This includes the assumption that diversification results are independent from each other with respect to different K's. We believe this assumption is sensible, especially given the observation that there is no clear pattern on the change of diversification performance with respect to the change of K.

Topics

1-50

51-100

101-150

Method K  E-IA@20 nD@20 K  E-IA@20 nD@20 K  E-IA@20 nD@20

pLSA 5 .4 .136 .223 3 .1 .179 .258 5 .5 .368 .471 GC 10 .1 .167 .256 10 .1 .195 .286 8 .2 .393 .486 GA 3 .1 .145 .233 9 .1 .195 .286 7 .3 .396 .490 GN 9 .1 .154 .240 10 .4 .184 .271 10 .3 .384 .482 GCA 5 .1 .145 .240 7 .1 .202 .295 9 .3 .397 .491 GCN 7 .1 .152 .243 7 .2 .187 .277 10 .3 .385 .485

GAN 5 .1 .142 .233 10 .1 .192 .283 10 .2 .390 .485 GCAN 3 .1 .148 .236 10 .2 .188 .279 8 .2 .392 .487

Table 7: Performance of xQuAD with pLSA and LapPLSA regularized by different external resources and their combinations.  and K are optimized with respect to each topic set.

regularized pLSA has the advantage that it provides a more robust performance in practice (where we will not know the optimal K).
Finally, for completeness, we include the performance of xQuAD in Table 7. With a proper setting of  and K, xQuAD shows better diversification performance compared to the results of IA-select in Table 4 in most cases. Same as with IA-select, regularization helps in generating better diversification results with xQuAD. Moreover, the usefulness of different resources and their combinations show similar trend compared to that of IA-select.
7. CONCLUSION
We proposed Multi-source Subtopics (MSS), a framework for subtopic modeling that i) uses a random walk based approach to combine and estimate the similarity between subtopics extracted from multiple Web sources, and ii) uses the obtained similarity relations to regularize topic models constructed using document content. MSS combines subtopics from multiple resources transparently, unifying implicit and explicit representations of subtopics.
We have demonstrated how the application of this framework in the context of search result diversification allows us to flexibly combine, analyze and compare subtopics extracted from different resources. Empirical results show that topic models regularized by the topical information extracted from external resources lead to improved and more robust diversification results, supporting our claim that better topic models are formed. Among the Web resources employed in our experiments, anchor text and click logs were shown to be generally more effective than Web ngrams. However, the effectiveness of a resource also depends on the properties of a specific query, and especially whether a resource contains subtopic information related to a query at all. Combining multiple resources could alleviate lack of coverage, but in our current setup leads to a diversification performance in between that of the resources that are combined.
A number of directions are left to be explored in the future. First of all, there is room to improve our method by exploring how to weigh resources by their (expected) quality for a given query. More resources can be analyzed, for example derived from social bookmarking sites, and more sophisticated ways of parameter optimization should be considered. Second, a component wise evaluation may improve our understanding beyond observations on the diversification pipeline as a whole. Hereto, we need to develop a method to evaluate the quality of the constructed subtopics directly through human assessment. This may be especially useful to improve our understanding of the features that make an external resource a good one for a given query. Finally, perhaps the strength of the external resources is not yet exploited in full when we only use them to regularize pLSA. An alternative could be to integrate implicit topics as nodes in the multi-plane random walks, and thus treat both types of topic representations more equally.

859

Acknowledgements
This research was funded by European Commission FP7 grant 257024, in the Fish4Knowledge project
8. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In WSDM '09, pages 5≠14, 2009.
[2] J. Allan and H. Raghavan. Using part-of-speech patterns to reduce query ambiguity. In SIGIR '02, pages 307≠314, 2002.
[3] D. Beeferman and A. Berger. Agglomerative clustering of a search engine query log. In KDD'00, pages 407≠416, 2000.
[4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res., 3:993≠1022, 2003.
[5] B. R. Boyce. Beyond topicality: A two stage view of relevance and the retrieval process. Information Processing & Management, 18(3):105≠109, 1982.
[6] D. Cai, Q. Mei, J. Han, and C. Zhai. Modeling hidden topics on document manifold. In CIKM '08, pages 911≠920, 2008.
[7] J. Carbonell and J. Goldstein. The use of MMR, diversitybased reranking for reordering documents and producing summaries. In SIGIR '98, pages 335≠336, 1998.
[8] B. Carterette and P. Chandar. Probabilistic models of ranking novel documents for faceted topic retrieval. In CIKM'09, pages 1287≠1296, 2009.
[9] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In CIKM'09, pages 621≠ 630, 2009.
[10] H. Chen and D. R. Karger. Less is more: probabilistic models for retrieving fewer relevant documents. In SIGIR '06, pages 429≠436, 2006.
[11] C. Clarke, M. Kolla, G. Cormack, O. Vechtomova, A. Ashkan, S. B¸ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR'08, pages 659≠666, 2008.
[12] C. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 web track. In TREC'09, 2009.
[13] C. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2010 web track. In TREC'10, 2010.
[14] C. Clarke, N. Craswell, and E. Soboroff, I.and Voorhees. Overview of the TREC 2011 web track. In TREC'11, 2011.
[15] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information Retrieval, 14(5):441≠465, 2011.
[16] N. Craswell and M. Szummer. Random walks on the click graph. In SIGIR'07, pages 239≠246, 2007.
[17] N. Craswell, R. Jones, G. Dupret, and E. Viegas, editors. WSCD'09, 2009.
[18] J. Cui, H. Liu, J. Yan, L. Ji, R. Jin, J. He, Y. Gu, Z. Chen, and X. Du. Multi-view random walk framework for search task discovery from click-through log. In CIKM '11, pages 135≠140, 2011.
[19] V. Dang and B. W. Croft. Query reformulation using anchor text. In WSDM '10, pages 41≠50, 2010.
[20] V. Dang, X. Xue, and B. Croft. Inferring query aspects from reformulations using clustering. In CIKM '11, 2011.
[21] Z. Dou, S. Hu, K. Chen, R. Song, and J.-R. Wen. Multidimensional search result diversification. In WSDM'11, pages 475≠484, 2011.
[22] A. Fuxman, P. Tsaparas, K. Achan, and R. Agrawal. Using

the wisdom of the crowds for keyword generation. In WWW '08, pages 61≠70, 2008. [23] W. Goffman. A searching procedure for information retrieval. Information Storage and Retrieval, 2(2):73≠78, 1964. [24] J. Guo, X. Cheng, G. Xu, and X. Zhu. Intent-aware query similarity. In CIKM'11, pages 259≠268, 2011. [25] J. He. Exploring topic structure: Coherence, Diversity and Relatedness. PhD thesis, University of Amsterdam, 2011. [26] J. He, E. Meij, and M. de Rijke. Result diversification based on query-specific cluster ranking. J. Am. Soc. Inf. Sci. Technol., 62(3):550≠571, 2011. [27] D. Hiemstra and C. Hauff. MIREX: MapReduce information retrieval experiments. Technical Report TR-CTIT-10-15, University of Twente, 2010. [28] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR'99, pages 50≠57, 1999. [29] Z. Li, F. Chen, Q. Xing, J. Miao, Y. Xue, T. Zhu, B. Zhou, R. Cen, Y. Liu, M. Zhang, Y. Jin, and S. Ma. Thuir at trec 2009 web track: Finding relevant and diverse results for large scale web search. In TREC, 2009. [30] H. Ma, M. R. Lyu, and I. King. Diversifying query suggestion results. In AAAI'10, 2010. [31] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Inf. Process. Manage., 40:735≠750, September 2004. [32] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR '98, pages 275≠281, 1998. [33] F. Radlinski, M. Szummer, and N. Craswell. Inferring query intent from reformulations and clicks. In WWW '10, pages 1171≠1172, 2010. [34] D. Rafiei, K. Bharat, and A. Shukla. Diversifying web search results. In WWW '10, pages 781≠790, 2010. [35] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In WWW'10, pages 881≠890, 2010. [36] N. Tishby and N. Slonim. Data clustering by markovian relaxation and the information bottleneck method. In NIPS, pages 640≠646, 2000. [37] J. Wang and J. Zhu. Portfolio theory of information retrieval. In SIGIR '09, pages 115≠122, 2009. [38] K. Wang, C. Thrasher, E. Viegas, X. Li, and B.-j. P. Hsu. An overview of microsoft web n-gram corpus and applications. In NAACL HLT'10, pages 45≠48, 2010. [39] J.-R. Wen, J.-Y. Nie, and H.-J. Zhang. Clustering user queries of a search engine. In WWW '01, pages 162≠168, 2001. [40] F. Wilcoxon. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80≠83, 1945. [41] I. H. Witten and E. Frank. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann, 1st edition, 1999. [42] Y. Yue and T. Joachims. Predicting diverse subsets using structural SVMs. In ICML '08, pages 1224≠1231, 2008. [43] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR '03, pages 10≠17, 2003. [44] X. Zhu, A. B. Goldberg, J. Van, and G. D. Andrzejewski. Improving diversity in ranking using absorbing random walks. Technical report, University of Washington, 2007.

860

Using Preference Judgments for Novel Document Retrieval
Praveen Chandar and Ben Carterette {pcr,carteret}@udel.edu
Department of Computer and Information Sciences University of Delaware
Newark, DE, USA 19716

ABSTRACT
There has been considerable interest in incorporating diversity in search results to account for redundancy and the space of possible user needs. Most work on this problem is based on subtopics: diversity rankers score documents against a set of hypothesized subtopics, and diversity rankings are evaluated by assigning a value to each ranked document based on the number of novel (and redundant) subtopics it is relevant to. This can be seen as modeling a user who is always interested in seeing more novel subtopics, with progressively decreasing interest in seeing the same subtopic multiple times. We put this model to test: if it is correct, then users, when given a choice, should prefer to see a document that has more value to the evaluation. We formulate some specific hypotheses from this model and test them with actual users in a novel preference-based design in which users express a preference for document A or document B given document C. We argue that while the user study shows the subtopic model is good, there are many other factors apart from novelty and redundancy that may be influencing user preferences. From this, we introduce a new framework to construct an ideal diversity ranking using only preference judgments, with no explicit subtopic judgments whatsoever.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]
Keywords: diversity, user study, preference judgments
1. INTRODUCTION
Research on novelty and diversity aims to improve the effectiveness of search engines by providing results that serve a range of possible user intents for the given query. These problems have been the subject of much interest in IR and web search recently, including the focus of a TREC task1. Batch effectiveness evaluation of retrieval systems serves sev-
1TREC 2009-2011 Web Track Diversity task focuses on novelty and diversity in search results.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

eral important purposes: first, giving developers and researchers a measurable objective; second, allowing for failure analysis and troubleshooting; and third, trying to estimate how useful search results will be to users. For the last of these, it is helpful to think of the evaluation measure and relevance judgments as a model of user utility. Measures like precision and recall can be seen as modeling utility in terms of the proportion of retrieved documents that are relevant and the proportion of relevant documents retrieved; measures like discounted cumulative gain or expected reciprocal rank offer a more refined model of user utility that incorporates graded judgments and rank-based discounts.
None of these models capture novelty or redundancy in ranked results. All of them will reward a system for retrieving the same relevant document 10 times in a row, or 10 relevant documents that are only superficially different from each other; while that may be useful for knowing whether retrieval features are working correctly, it is not likely to be very useful to a user. Diversity evaluation attempts to model novelty and redundancy in ranked results so as to give a more precise model of user utility.
Current diversity evaluation measures in the literature require judgments of relevance to subtopics (also called aspects, facets, or nuggets) of a topic. For example, judgments for the query Windows would include binary relevance judgments for each document for the subtopics window panes, windows operating system, etc. These subtopic judgments are used to determine whether a document is redundant with a previously-ranked document, or whether it contains some new information that a user might find interesting, or whether it is relevant to an alternative intent and perhaps not useful to this user but still useful enough to a different user. Measures like -nDCG, ERR-IA, subtopic recall, and D-measures [?, ?, ?, ?] all use this same basic model, assigning more value to a document with more novel subtopics and less value to one with more redundant subtopics.
Like any model, the subtopic model surely has shortcomings. Novelty and redundancy are certainly not the only reasons a user might prefer one relevant document over another. Apart from a study by Sanderson et al. that showed that user preferences for rankings correlate with -nDCG [?], there has not been much work on validating this model against real user preferences. And if the model does not track user preferences, then it is hard to justify its continued use: it conflates various aspects of effectiveness in such a way that, if used as an objective function, it can be difficult to understand the precise effect of change in the ranker.

861

Fortunately these measures produce directly-testable hypotheses about user preferences. In this work we describe a novel user study to test these hypotheses. In Section 2 we start by describing the diversity retrieval problem in more detail and define the model more precisely. In Section 3 we present a user study, including a crowdsourced design; we show that while the model is not perfect, it is certainly not invalid. Section 4 builds on this by presenting a preferencebased method for determining a diversity-aware ranking of documents. We conclude in Section 5.
2. NOVELTY RANKING TASK
Consider a user that has an unambiguous but broad information need and goes to a search engine to help satisfy it. This user will input a query and then see a ranked list of results, some of which will be relevant and some of which will not. The user will presumably click the relevant results to view and absorb the information they contain. Ideally, each relevant result would provide some new information that was not provided by previous relevant results; in other words, the relevant results would not be redundant with each other. The idea is that, each time a user clicks on a new relevant document, the amount of knowledge a user gains must be maximized by the novel content in the document.
The goal of ranking documents with novelty is to ensure that each relevant document a user sees as they progress down a ranked list provides new, non-redundant information that will help them satisfy their need. This means that a ranking of documents cannot be based solely on the probability of relevance; the novelty of a document depends to no small degree on the documents that have been ranked above it. Similarly, evaluation of these results cannot be based solely on binary or even graded relevance judgments, since these judgments are made to individual documents independently of all the other documents that might have been ranked. Part of studying the task is defining evaluation measures that can model redundancy and novelty.
2.1 Relationship With Other Tasks
The novelty task has similarities with some existing tasks such as the diversity task studied as part of the TREC Web track. Diversity aims at retrieving a subset of documents that has the maximum coverage of subtopics with the assumption that different users may be interested in different subtopics. In novelty ranking, the goal is to provide a set of documents for a single topic from which the user can get as much information as possible for that particular topic. We assume all users are interested in all of the subtopics, like the standard ad hoc assumption that all users are interested in all of the relevant material.
2.2 Intrinsic vs Extrinsic Diversity
Researchers in the past have identified two types of diversity: extrinsic and intrinsic [?]. Extrinsic diversity addresses the uncertainty in an ambiguous query where the intent is unclear and is best served by a ranking of documents covering several intents. Intrinsic diversity can be described as diversification that focuses on reducing redundancy and providing novel information for an unambiguous but still underspecified information need. In our work, we focus on intrinsic diversity which we refer to as novelty ranking, as we believe it will be easier for assessors to express preferences when there is no ambiguity of intent.

2.3 Evaluation
Evaluation measures for novelty and diversity must account for both relevance and novelty in the result set. It is important that redundancy caused by documents containing previously retrieved subtopics be penalized and documents containing novel information be rewarded. Most evaluation measures solve this problem by requiring that the subtopics for a query be known and that documents have been judged with respect to subtopics.

2.3.1 Existing Evaluation Measures
Subtopic recall. Subtopic recall measures the number of unique subtopics retrieved at a given rank [?]. Given that a query q has m subtopics, the subtopic recall at rank k is given by the ratio of number of unique subtopics contained by the subset of document up to rank k to the total number of subtopics m.

k i=1

subtopics(di)

S-recall@k =

(1)

m

-nDCG scores a result set by rewarding newly found subtopics

and penalizing redundant subtopics. In order to calculate -

nDCG we must first compute the gain vector [?]. The gain

vector is computed by summing over subtopics appearing in

the document at rank k:

m

G[i] = (1 - )cj,i-1

(2)

j=1

where cj,i is the number of times subtopic j has appeared in documents up to (and including) rank i. Once the gain vector is computed, a discount is applied at each rank to penalize documents as the rank decreases. The most commonly used discount function is the log2(1 + i), although other discount functions are possible. The discounted cumulative gain is given by

k

G[i]

DCG@k =

(3)

i=1 log2(1 + i)

-DCG must be normalized to compare the scores against various topics. This is done by finding an "ideal" ranking that maximizes -DCG, which can be done using a greedy algorithm. The ratio of -DCG to that ideal gives -nDCG.

Intent-aware family. Agrawal et al. studied the problem of answering ambiguous web queries, which is similar to the subtopic retrieval problem [?]. The focus of their evaluation measure is to measure the coverage of each intent separately for each query and combine them with a probability distribution of the user intents. They call this the intent-aware family of measures. It can be used with most of the traditional measures for evaluations such as precision@k, MAP, nDCG, and so on.

ERR-IA. Expected Reciprocal Rank (ERR) is a measure based on "diminishing returns" for relevant documents [?]. According to this measure, the contribution of each document is based on the relevance of documents ranked above it. The discount function is therefore not just dependent on the rank but also on relevance of previously ranked documents. A weighted average of the ERR measures for each interpretation would give the intent-aware version of ERR [?].

862

D-Measure. The D and the D# measures described by Sakai et al. [?] aims to combine two properties into a single evaluation measure. The first property is to retrieval documents covering as many intents as possible and the second is to rank documents relevant to more popular intents higher than documents relevant to less popular intents.
2.3.2 Principles of Existing Evaluation Measures
All of these measures estimate effectiveness of a system's ranking by iterating over the ranking, rewarding relevant documents containing a unseen subtopic(s) and penalizing relevant documents containing subtopic(s) seen before in the ranking. They are all based on a few principles in general:
1. A document with more unseen subtopics is worth more than a document with fewer unseen subtopics;
2. A document with both unseen and already-seen subtopics is worth more than a document with only the same unseen subtopic;
3. A document with unseen subtopics is worth more than a document with only redundant subtopics.
One of our goals with this work is to test whether these principles hold for real users.
2.4 Data
Our analysis was conducted primarily on the Newswire data created by Allan et al. [?] to investigate the relationship between system performance and human performance on a subtopic retrieval task. The data consists of 61 topics, each with a short (3-6 word) query, and judgments of relevance to documents in a subset of the TDT5 corpus. The Newswire data includes relevance judgments for the top 130 documents retrieved by a query-likelihood language model for the short query for each query. The judgments consists of binary relevance judgments for each document, and for each relevant document, a list of subtopics contained in that document. This data reflects an intrinsic diversity task and is therefore most appropriate to this work.
3. FACTORS INFLUENCING USER PREFERENCES
In Section 2.3.2, we identified some principles on which the evaluations for diversity are based on. In this section we tests if these principles hold for real users and further study in detail the role of subtopics in influencing user preference. Although in practice the same evaluation measures are used for both intrinsic and extrinsic diversity, our focus is on intrinsic diversity as it is easier for assessors to understand the concept of relevance when there is no ambiguity of intent. We explore the factors that influence user preference for novelty ranking using a preference based framework.
3.1 Triplet Framework
The idea of pairwise preference judgments is relatively new in the IR literature, having been introduced by Rorvig in 1990 [?] but not subject to empirical study until the past several years [?, ?]. Comparison studies between absolute and preference judgments show that preference judgments can often be made faster than graded judgments, with better agreement between assessors (and more consistency with individual assessors) [?]. Also with preferences tassessors can make much finer distinctions between documents.

We propose a preference-based framework to study novelty consisting of a set up in which three relevant documents that we refer to as a triplet are displayed such that one of them appears at the top and the other two are displayed as a pair below the top document. We will use DT , DL, and DR to denote the top, left, and right documents respectively, and a triplet as DL, DR|DT . An assessor shown such a triplet would be asked to choose which of DL or DR they would prefer to see as the second document in a ranking given that DT is first, or in other words, they would express a preference for DL or DR conditional on DT . For the purpose of this study we will assume we have relevance judgments to a topic, and for each relevant document, binary judgments of relevance to a set of subtopics. Thus we can represent a document as the set of subtopics it has been judged relevant to, e.g. Di = {Sj, Sk} means document i is relevant to subtopics j and k. Varying the number of subtopics in top, left and right documents yields specific hypotheses about preferences for novelty over redundancy.
3.2 Hypotheses
The triplet framework allows us to collect judgments for novelty based on preferences and also enables us to test various hypotheses. As discussed above, varying the number of subtopics in DT , DL and DR it is possible to enumerate various hypotheses concerning the effect of subtopics in a document. We define two types of hypotheses; one very specific with respect to subtopic counts and redundancy, and the other more general.
Hypothesis Set 1 : First we propose the simplest possible hypotheses that capture the three principles above. We will denote a preference between two documents using , e.g. DL DR means document DL is preferred to document DR. Then the three hypotheses stated formally are:
H1: if DL, DR|DT = {S2}, {S1}|{S1} , then DL DR (novelty is better than redundancy)
H2: if DL, DR|DT = {S1, S2}, {S2}|{S1} , then DL DR (novelty+redundancy is better than novelty alone)
H3: if DL, DR|DT = {S2, S3}, {S2}|{S1} , then DL DR (novelty+novelty is better than novelty alone)
Hypothesis Set 2 : Here we define a class of hypotheses in which the number of subtopics contained in each document in a triplet is categorized by relative quantity. We identify six variables based on number of subtopics that almost completely describe the novelty and redundancy present in the triplet. The six variable are as follows:
1. T n - Number of subtopics in DT ; 2. N Ln - Number of subtopic in DL not present in DT ; 3. N Rn - Number of subtopic in DR not present in DT ; 4. Sn - Number of subtopics shared between DL and DR; 5. RLn - Number of subtopics in DL and present in DT ; 6. RRn - Number of subtopics in DR and present in DT .
The number of subtopics for each of the six variables are categorized as low or high. The six variables enable us to test the effect of novelty and redundancy w.r.t the number of subtopics in a triplet. The variables N Ln and N Rn focus on novelty whereas RLn and RRn focuses on redundancy. For instance, by varying N Ln and N Rn and holding the other variables constant, it is possible to test the effect of the relative quantity of novel subtopics in a document.

863

3.3 Experimental Design
In this section we describe the experimental design used to test the hypotheses defined above. Notice that the defined hypotheses are based on the number of subtopics contained in the documents and they fit into the triplet framework which requires conditional preference judgments. Therefore, to test our hypotheses, two kinds of judgments were needed: subtopic level judgments and conditional preference judgments. The subtopic level judgments were obtained from the data described in Section 2.4. Conditional preference judgments were collected using crowd sourcing as it is a fast, easy and a low cost way of collection user judgments [?].
We used Amazon Mechanical Turk (AMT) [?]; an online labor marketplace to collect user judgments. AMT works as follow: requestor create a group of Human Intelligence Task (HITs) with various constraints and worker from the marketplace works on these task to complete the task. In this work we, use a design similar to the one used by Chandar and Cartertte [?]. Designing a user study using AMT involves deciding on the HIT layout and HIT properties.
3.3.1 HIT Layout
In order to collect user judgments for our hypotheses using AMT, we had to organize the triplets satisfying a given hypothesis into HITs. Each HIT consisted of the following (in order of display): a set of instructions about the task, original keyword query, topic description, five preference triplets, and a comment field allowing worker to provide feedback. A brief description about each element is given below:
Guidelines: Workers were provided with a set of instructions and guidelines prior to judging. Guidelines specified that workers should assume that everything they know about the topic is in the top document and are trying to find a document that would be most useful for learning more about the topic. Guidelines did not mention anything about subtopics, or even novelty/redundancy except as examples of properties assessors might take into account in their preferences (along with recency, ease of reading, and relevance).
Query text and topic description: The query text described the topic in a few words (we used the topic "titles" in the traditional TREC jargon) and topic description provided a more verbose and informative description about the topic. Again, there was no mention of explicit subtopics.
Preference triplet: Figure 1 shows an example preference triplet with the query text and topic description. Each preference triplet consists of three documents, all of which were relevant to the topic and the document were picked randomly from the data described in Section 2.4 to meet the constraints of a given hypothesis. One document appeared at the top followed by two documents below it, the triplets were chosen randomly such that the hypothesis constraints were satisfied. A HIT consisted of five preference triplets belonging to the same query shown one below the other.
The triplets were followed by a preference option for the workers to indicate which of the two documents they preferred. The workers were asked to pick the document from the lower two that provided the most new information, assuming that all the information they know about the topic is in the top document. They could express a preference based on whatever criteria they liked; we listed some examples in the guidelines. Note that we do not show them any subtopics, nor do we ask them to try to determine subtopics and make a preference based on that.

Comments Field was provided at the end, so that the workers could to provide a common feedback for all the five triplets, if they chose to do so.
3.3.2 HIT Properties
Workers are paid for each HIT they complete and picking an appropriate amount for each task is always tricky. In our study, workers were paid $0.80 for each completed HIT. Also each HIT had a time limit of three hours before which the HIT had to be completed. While the actual task might not take three hours to complete; the extra time allows them to take breaks if needed, since the workers had to read fifteen documents per HIT. We had five separate workers judge each HIT for the our first set of hypotheses and three separate workers judge each HIT for the our second set of hypotheses.
3.3.3 Triplets
Triplets were generated by randomly picking the three relevant documents for a given query and representing them as subtopic(s). Triplets for the first set of hypotheses in Section 3.2 were considered such that the constraints are satisfied for each hypothesis. For example, for hypothesis H1 given a query x the triplet would consist of DT containing only the subtopic S1 and DL containing the subtopics S1 and S2. Six queries were used to test the first set of hypotheses with four triplets for each query.
The triplets were generated in a similar way for the second set of hypotheses but the constraints for each hypothesis were based on the six variables described in 3.2. For example, a triplet with a variable setting of T n = High, Sn = High, N Ln = High and N Rn = High would contain 5 or more subtopics in the top document DT and 3 or more subtopics in the left and right documents (DL and DR) such that there are 1 or more subtopics shared between DL and DR. The details of the number of subtopics for each categories of high and low levels for each variables are provided in the Table 1. Eight queries were used to test the second set of hypotheses with four different triplets for each query.
3.3.4 Quality Control
There are two major concerns in collecting judgments through crowdsourcing platform such as AMT. One is "Do the workers really understand the task?" and the other is "Are they making faithful effort to do the work or clicking randomly?". We address these concerns using three techniques: majority vote, trap questions, and qualifications.
Majority vote: Since novelty judgments to be made by the workers are subjective and it is possible some workers are clicking randomly, having more than one person judge a triplet is common practice to improve the quality of judgments. In our study, each HIT was judged by 5 or 3 different workers (depending on hypothesis set). We look at the individual preferences as well as the majority preference.
Trap questions: Triplets for which answers are obvious were included to assess the validity of the results. We included two kinds of trap questions: "non-relevant document trap" and "identical document trap". For the former, one of the bottom two documents was not relevant to the topic and should never be preferred. For the latter, the top document and one of the bottom two documents were the same. The workers were expected to pick the non-identical document as it provides novel information. One of the five triplets in a HIT was a trap and the type was chosen randomly.

864

Figure 1: Screenshot of the preference triple along with the query text and description.

variable
Tn Sn NLn NRn RLn RRn

number of subtopics

low

high

1-4

5-9

0

1-2

0-2

3-6

0-2

3-6

0

1-2

0

1-2

Table 1: Number of subtopics corresponding to the high and low categories for each variable in our second set of hypotheses.

Qualifications: It is possible to qualify workers before they are allowed to work on your HITs in AMT. Worker qualifications can be determined based on historical performance such as percentage of approved HITs. Also, worker's qualification can be based on a short questionnaire or a test. The two qualifications used in are study are explained below:
1. Approval rate: HITs can be restricted to workers with an overall minimum percentage of approval. It is commonly used for improving accuracy and reducing spammer from working on your task. An overall approval rate of 95% was required to work on our HITs.
2. Qualification test: Qualification tests can be used to ensure that workers have the required skill and knowledge to perform the task. In our case, workers had to be trained to look for documents that provide novel information given the top document. We created a qualification test having the same design layout as the actual task but had only three triplets. Two of the three triplets were identical document traps and the other was a non-relevant trap with instructions for each triplet aiding in making a preference.
3.4 Results and Analysis
Judgments for a total of 60 triplets (out of which 12 triplets were traps) were obtained for the hypothesis set 1. Since we had each triplet assessed by five separate assessors, a total of 300 judgments were collected out of which 60 were traps. We had 39 unique workers (identified by worker ID) on AMT judge these triplets across six topics.
Table 2 shows results for H1. It turns out that there is no clear preference for either redundant or novel documents

H1 topic childhood obesity terrorism indonesia earthquakes weapons for urban fighting total

all prefs same new
6 14 8 12 15 5 15 5 44 36

consensus same new
13 13 31 31 88

Table 2: Results for H1: that novelty is preferred to redundancy. The "all prefs" columns give the number of preferences for the redundant and the novel document for all assessors. The "consensus" columns take a majority vote for each triplet and report the resulting number of preferences.

H2 topic kerry endorsement childhood obesity terrorism indonesia libya sanctions total

all prefs

new same+new

9

11

4

16

13

7

4

16

30

50

consensus

new same+new

2

2

0

4

4

0

0

4

6

10

Table 3: Results for H2: that novelty and redundancy together are preferred to novelty alone. The "all prefs" columns give the number of preferences for the redundant+novel document and the novel document for all assessors. The "consensus" columns take a majority vote for each triplet and report the resulting number of preferences.

for the four queries. For two of our queries assessors tended to prefer the novel choice; for the other two they tended to prefer the redundant choice. When we use majority vote to determine a consensus for each triplet, we find that the outcomes are exactly equal. Thus while we cannot reject H1, we have to admit that if it holds it is much less strong than we expected.
Table 3 shows a clearer (but still not transparent) preference for H2, novelty and redundancy together over novelty alone. Over all assessors and all triplets, the preference is significant by a binomial test (50 successes out of 80 trials; p < 0.05). Still, there is one query ("john kerry endorsement") for which the difference is insubstantial, and one that has the opposite result ("terrorism indonesia"). The latter

865

H3 topic kerry endorsement childhood obesity terrorism indonesia libya sanctions total

all prefs

new new+new

9

11

3

17

2

18

8

12

22

58

consensus

new new+new

1

3

0

4

0

4

1

3

2

14

Table 4: Results for H3: that two novel subtopics are preferred to one. The "all prefs" columns give the number of preferences for the novel+novel document and the novel document for all assessors. The "consensus" columns take a majority vote for each triplet and report the resulting number of preferences.

topic earthquakes terry nichols guilt evidence medicare drug coverage oil producing countries no child left behind european union member german headscarf court ohio highway shooting
total

high low 76 - 20 (79%) 75 - 21 (78%) 73 - 23 (76%) 65 - 31 (68%) 62 - 34 (65%) 61 - 35 (64%) 59 - 37 (61%) 51 - 45 (53%) 522 - 246 (68%)

left right 96 - 96 (50%) 100 - 92 (52%) 86 - 106 (45%) 89 - 103 (46%) 81 - 111 (42%) 103 - 89 (54%) 84 - 108 (44%) 104 - 88 (54%) 743 - 793 (48%)

Table 5: Results of preference judgments by the number of new subtopics in DL, DR over DT (variables N Ln, N Rn). Counts are aggregated over all values of T n, Sn per query. The first column gives preference counts for the document with more new subtopics over the document with fewer when N Ln N Rn. The second column is the baseline, giving counts for preferences for left over right.

case is particularly interesting because it is the opposite of what we would expect after seeing the results in Table 2: given that assessors preferred redundant documents to novel documents for that query, why would they prefer novel documents to documents with both novelty and redundancy?
Table 4, with results for H3, is the strongest positive result: a clear preference for documents with two new subtopics over documents with just one. In this case both results are significant (58 successes out of 80 trials and p < 0.0001 over all triplets and all assessors; 14 successes out of 16 trials and p < 0.01 for majority voting). Nevertheless, there are still queries for which the preference is weak.
Based on this, it seems like novelty + novelty > novelty, novelty+redundancy  novelty, but not novelty  redundancy.
There were a total of 640 triplets (out of which 128 triplets were traps) for the second part of our study. Each of these triplets were judged by three separate assessors, thus a total of 1920 judgments were made out of which 384 were traps. And for this study we had 38 unique workers (identified by worker ID) on AMT working on our triplets. Some of these workers had worked on the first study as well. Almost 70% of the judgments were completed by 15% of the workers and about 93% of the irrelevant traps were passed by the workers. This power law distribution for our task has been observed earlier for other tasks as well [?], we hope to investigate on this issue in the future.
Triplets were generated by controlling four variable: T n, Sn, N Ln and N Rn, we obtained sixteen unique settings for the four variable combination as each of the four variables

were categorized into low and high with equal number of triplet in each setting. This allowed us to perform ANOVA such that the number of new subtopics in the left or right document was the primary predictor of preference, with the number of subtopics in the four variables as the secondary predictors. ANOVA indicated that there is a lot of residual variance, suggesting there are various factors influencing preferences that we have not included in the model.
Table 5 analyzes preferences for more new subtopics in DL or DR over fewer new subtopics (variables N Ln and N Rn) by topic. We looked at four cases: the first two (N Ln high, N Rn low; N Ln low, N Rn high) can tell us whether users prefer to see more new subtopics over fewer, while the second (N Ln high, N Rn high; N Ln low, N Ln low) along with the first two give us a baseline preference for left over right. While we would expect the baseline preference to be 50% (since which document appears on the left versus right is randomized), there may be other unmodeled factors that cause it to be more or less than 50%, so it is useful to compare to this baseline.
It is clear from this table that users as a group prefer to see more new subtopics, just as we saw in the results for H3 above. Still, there are individual queries for which that preference is not strong, especially when compared to the baseline (e.g. the "Ohio highway shooting" topic), and even when the preference is strong in aggregate there are cases where they do not hold.
There is some effect due to the number of subtopics in DT , with preferences for more new subtopics stronger when T n is low. When it is low, the preference for high versus low is 271 to 113 (70%) against a baseline preference for left over right of 347 to 421 (45%)2. When T n is high, the preference for high versus low is 251 to 133 (65%) against a baseline of 396 to 372 (52%). We conjecture that when the top document already has a lot of information about the topic, there is a little less reason to prefer either left or right regardless of how many subtopics they contain.
There is not much effect due to the number of shared subtopics between DL and DR. When Sn is low, the preference for more new subtopics over fewer is 268 to 116 (70%) against a baseline of 370 to 398 (48%); when it is high, the preference for more new is 254 to 130 (66%) against a baseline of 373 to 395 (49%). This may be because fewer shared subtopics makes it easier to express a preference. However, the effect is too small to draw any firm conclusion.
There is interesting interaction between the number of new subtopics and the number of redundant subtopics in DL and DR. When one has a high number of new subtopics and the other has a low number of new subtopics, number of redundant subtopics seems to influence the strength of preference for the one with more new subtopics: if there are more redundant subtopics along with the new subtopics, the preference is 118 to 44 (73%), but when there are fewer redundant subtopics with more new subtopics and more redundant subtopics with fewer new subtopics, the preference is even at 51 to 51 (50%). This suggests again that users like redundancy, sometimes enough to overcome a lack of novelty. However we must note that data here is sparse, also the two variables RRn and RLn were not the ones that we controlled for in our experiment.
2We presume that the greater-than-expected preference for the right document is just due to random chance.

866

Topic childhood obesity weapons for urban fighting kerry endorsement libya sanctions earthquake terrorism indonesia
Mean

Agreement 0.71 0.92 0.58 0.62 0.72 0.71 0.69

No. triplets 15 5 10 10 5 15 60

Table 6: Interassessor agreement scores for each topic for the first study.

Topic oil producing countries terry nichols guilt evidence no child left behind german headscarf court medicare drug coverage earthquakes european union member ohio highway shooting
Mean

Agreement 0.63 0.72 0.61 0.57 0.66 0.65 0.59 0.59 0.63

No. triplets 80 80 80 80 80 80 80 80 640

Table 7: Interassessor agreement scores for each topic for the second study.

3.5 Interassessor Agreement
As described above, each triplet was judged by five different workers for the first study (hypotheses set 1) and by three workers for the second study (hypotheses set 2). We calculated an inter-assessor agreement score for each triplet for the first study as follows. The judgments were considered as 10 pairs of answers given for a single triplet, adding 1 points to the score if the two workers agreed (complete agreement); and adding nothing if they judged different documents (no agreement). The perfect agreement would sum up 10 points, so we divided the score obtained by 10 and normalized from 0 (no agreement at all) to 1 (perfect agreement). Mean agreement for the first study is given in Table 6 for each query. Overall a high mean agreement of 0.7 was found across all triplets and the scores are close to the agreement observed previously [?]. Since the mean agreement was quite high for the first study, it encouraged us to reduce the number of workers for each triplet and increase the number of queries for the second study. Similar mean agreement can be seen for the second study in Table 7.
3.6 Possible Confounding Effects in Display
The way the hits were displayed may introduce some confounding effects, possibly causing assessors to choose documents for reasons other than novelty or redundancy. We investigated two such effects: Document length A preference towards shorter documents was observed in general, though the preference gets weaker over the three hypotheses. For H1, assessors preferred the shorter document in 79% of triplets. For H2, that decreased to 71% of triplets, and for H3 it dropped steeply to only 44%. However, it is also true that the mean difference in length for the pair of documents they were choosing between was greatest for H1 triplets and least for H3 triplets (H1:158 terms, H2:126 terms, H3:47 terms). Therefore its safe to conclude there seems to be a preference towards shorter documents.

Highlighted terms It turns out that assessors tended to prefer the document with fewer highlighted query terms. For H1, assessors preferred the document with more query terms only 35% of the time. For H2 that drops to 13%, and for H3 it comes back up to 29%. The mean difference in number of query term occurrences is quite low, only on the order of one additional occurrence on average for H1 and H3 documents, and only 0.2 additional occurrences for H2. While the effect is significant, it seems unlikely that assessors can pick up on such small differences. We think the effect is more likely due to the distribution of subtopics in documents.
3.7 Additional Investigation
While the results suggest that the number of subtopics influences user preferences, it is also clear that from the analysis that other factors are affecting preferences. The results from H1 and the weaker preference in H2 were not what we expected. We investigated this more by looking at a number of triplets ourselves and identifying some new hypotheses about why assessors were making the preferences they were. From looking at triplets for the "earthquakes" topic, we identified three possible reasons for preferring a document with a redundant subtopic:
∑ it updates or corrects information in the top document; ∑ it significantly expands on the information in the top
document; ∑ despite having a novel subtopic, the other choice pro-
vides little information of value.
This suggests to us that there are other factors that affect user preferences, in particular recency, completeness, and value. It may also suggest that there are implicit subtopics (at finer levels of granularity) that the original assessors did not identify, but that make a difference in preferences. None of this is surprising, but there is currently no evaluation paradigm of note that take all of these factors into account in a holistic way. Preference judgments can, and this analysis suggests additional hypotheses for testing with preferences.
4. PREFERENCE JUDGMENTS FOR AN IDEAL NOVELTY RANKING
The user study shows that although users tend to prefer documents containing more novel subtopics, it is also evident that factors other than subtopics play a vital role. The study also shows that the presence of subtopic in a document is taken into account implicitly and preferences are based not only on the number of subtopics but also on several other factors that include subtopic importance, relevance of the subtopic, readability of the document, etc. In this section, we propose an approach that attempts to capture these factors implicitly using a preference based framework to form a full ranking of documents with novelty as an implicit quality.
Our approach involves a series of sets of preference comparisons. Each set is essentially a comparison sort algorithm, with the comparison function a simple preference conditional on information contained in top-ranked documents from prior sets of comparisons, generalizing the triplet framework we introduced above.
The first set of preferences is meant to produce a relevance ranking: given a choice between two documents, assessors select the one they prefer, with topical relevance being the primary consideration in the judgment. Once these comparisons are done for all pairs, it is possible to obtain the best or

867

"most relevant" document, i.e. the most preferred document based on the number of times a document was selected.
For the second set of preferences, the assessor needs to consider the novelty of information in the document along with relevance. This leads to exactly the triplet framework we used previously. For this second set, the assessor will see the top-ranked document from the previous set as DT , then pick from two documents DL, DR conditional on that.
The sequence continues by adding more documents to the top. For the third set, the comparison involves information in two previously ranked documents along with a pair of documents; for the fourth, it involves information in three previously ranked documents along with a pair. This continues to the final set, in which there are only two documents to compare conditional on n - 2 previous top documents.
When complete, the most preferred document in the first set takes rank 1, the most preferred document in the second set takes rank 2, and so on. Observe that the first set of judgments correspond to relevance judgments and sets 2 through n - 1 correspond to novelty.
This method asks for a very large number of preferences: if fully judged, there would be O(n2) preferences in the first set, O((n - 1)2) in the second, and so on, for a total of O(n3) judgments, which is almost certainly infeasible. We hypothesize that the first two sets of preferences (one for relevance and one for novelty) will provide a near-optimal approximation to the full set and if judgments are transitive (that is, if document A is preferred to B and B is preferred to C, then A should be preferred to C as well), the number of judgements needed can be reduced drastically. We will test both of these hypotheses below.
4.1 Experimental Design
As described above, we asked assessors to make the first two sets of judgments for each topic. The first set of judgments attempts to rank documents by relevance to the topic; intuitively, these judgments could be used to find the most relevant document in the ranked list: that which is preferred to everything else (assuming judgments are transitive) is most relevant. The second set of judgments attempts to rank the remaining documents by the degree of novelty they provide given that we know the document that is ranked at position one from the first set.
For this experiment we elected not to use MTurk. We wanted a single assessor to do all the preferences for a single topic, first so they would be able to build a familiarity with the topic as they judge, and second so we could assess their self-consistency. Thus we asked students at our institution to participate in the study. These students are mostly in computer science, mostly studying NLP and language technologies. Like the workers in the previous section, they were not given explicit instruction regarding subtopics; they were only asked to express a preference. We had 6 assessors complete preferences for at least one topic.
We designed two new web interfaces running on a local server to be used by assessors to collect preferences for both relevance and novelty, the first two sets of preferences described above. Common elements in both interfaces are the original keyword query, topic description, article texts (with query keywords highlighted), preference buttons for indicating which of the two documents the assessor prefers, a progress bar with a rough estimate of the percentage of preferences completed, and a comment field allowing them

to say why they made their choice (if they wish). Elements specific to each experiment are described in more detail in the respective sections below.
For this study, we asked assessors to judge all pairs of documents in the first two sets. Topics were chosen from the data described in Section 2.4. We wanted to include all known relevant documents for the topic in the preference experiment. Since we were asking assessors for all pairs, we limited our selection to topics with a relatively small number of relevant documents. We then added a randomlyselected set of nonrelevant documents from among the topranked documents for the topic. We kept the total number of preferences in an experiment to less than 200.
The first two documents shown to an assessor were chosen randomly from the set of all documents to be ranked. After that, whichever document the assessor preferred remained fixed in the interface; only the other document changed. This way the assessor only had to read one new document after each judgment, just as they would in normal singledocument assessing. Furthermore after the first O(n) judgments we know the top-ranked document for the current set, and thus if transitivity holds it follows that we only need a linear number of preferences at each set.
4.1.1 First Level Judgments: Relevance Preferences
In the first set of judgments, the assessor was shown two documents (news articles) and a statement of an information need (a topic); the task was to pick the most preferred document using the "prefer left" or "prefer right" buttons. A screenshot of the first level judgments is shown in Figure 2
The assessor was provided with a set of instructions and guidelines prior to judging. The guidelines specified that the assessor should assume they know nothing about the topic and are trying to find documents that are topically relevant, that is, that provide some information about it. If a document contains no topical information, the assessor could judge it "not relevant"; if they do so, the system will assume they prefer every other document to that one and remove it from this set as well as all subsequent sets so it will not be seen in future comparisons. Assessors could also judge "both not relevant" to remove both from the set and see a new pair. These buttons can make the task easier by reducing the total number of preference judgments the assessors need to make.
If both documents were topically relevant, the assessor could express a preference based on whatever criteria they liked. Some suggestions included in the guidelines were: one document is more focused on the topic than the other; one document has more information about the topic than the other; one document has more detailed information than the other; one document is easier to read than the other. Assessors could exit for a break as long as they liked and return at the point where they stopped. A progress indicator let them know roughly how close they were to the end
4.1.2 Second Level Judgments: Novelty Preferences
For the second set of preferences, the assessor was shown three documents and a statement of an information need (a topic); the task was to pick the most useful document from two of the three to learn more about the topic given what is presented in the third.
The interface for the second level judgment was very similar to the triplet layout shown is Figure 1. One document

868

Figure 2: Screenshot of the preference collection interface for relevance preferences.
appeared at the top of the screen; this was the most preferred document as identified by the assessor after the first set of preferences. The assessors were asked to pick a document from a pair of documents (appearing below the top document) that provided the most novel information given that they know all the information in the top document.
Guidelines specified that the assessor should pretend that the top document is the entirety of what they know about the topic, and their goal is now to find the best document for learning more about the topic. Beyond that, they could express a preference based on whatever criteria they liked, including those listed above.
There are no nonrelevant judgment buttons in this interface. Any document that was judged nonrelevant in the first set of preferences will not be seen in this set. Anything that was relevant in the first set is assumed to still be relevant; if a relevant documents provides no new relevant information, we assume the assessor's preferences will result in that document being ranked near the bottom of this set.
4.2 Experimental Analysis
As described above, we conducted novelty preference judging with five topics from the data described in Section 2.4. On average, 16.8 documents were judged for each topic. A total of 605 pairs were judged for 4 topics by 6 assessors for experiment levels 1 and 2. We compared these judgments to the original subtopic-based judgments in the data.
Agreement on Relevance. To assess agreement on the relevance of a document, we assume that any document not explicitly judged not relevant must be relevant. We consider the original relevance judgments derived from the subtopic judgments as the ground truth and assess the performance of our assessors relative to that. Table 9 shows the confusion matrix between assessors making preference judgments and the original assessors making subtopic judgments; broad agreement on the two classes is 71%. Preference assessors identified 76% of the relevant documents that the original assessors found, and 60% of the documents judged relevant by at least one assessor were judged relevant by both. This is a high level of agreement for IR tasks; compare to the 40% agreement on relevance reported by Voorhees [?].

topic
OPEC actions OPEC actions - Alternate childhood obesity childhood obesity - Alternate suicide bombers teens women foreign students visa restrictions

Rank Correlation Level 1 Level 2 0.563 0.534 0.568 0.377 0.467 0.264 0.403 0.394 0.320 0.200 0.532 0.030

Table 8: Kendall's  correlations between rankings from real preference judgments and rankings from simulated preference judgments (for the relevance ranking (level 1) and the novelty ranking (level 2)).

Preference Judgments
Relevant Non-Relevant

Subtopic Judgments

Relevant Non-Relevant

58

20

18

37

Table 9: Confusion matrix for relevance judgments derived from the subtopic judgments in the original Newswire collection and derived from our preference judgments (all queries aggregated).

Rank Correlation. Another way to compare preference judgments to the original subtopic judgments is by using both to construct a ranking of documents, then computing a rank correlation statistic between the two rankings. The subtopic judgments included in the Newswire data were obtained by assessors explicitly labeling subtopics for each relevant document. We use the subtopic information to simulate preference judgments that might have been obtained via our experiment. For the first set, we always prefer the document with the greatest number of subtopics. (Except in the case of a tie, when we prefer a random document.) For the second set, the top-ranked document from the first set becomes the "top document", and then for each pair we prefer the document that contains the greatest number of subtopics that are not in that top-ranked document. The final ranking has the most-preferred document from the first set of preferences at rank 1 followed by the ranking obtained from the second set of preferences.
Kendall's  rank correlation for each topic for both level 1 and level 2 preference judgments is shown in Table 8. Kendall's  ranges from -1 (lists are reversed) to 1 (lists are exactly the same), with 0 indicating a random reordering. The values we observe are positive and statistically significant (except for level 2 judgments for topic foreign students visa restrictions). Kendall's  is based on pairwise swaps, and thus can be converted into agreement on pairwise preferences by adding 1 and dividing by 2. When doing this we see that agreement is again high for the relevance ranking, and also high for the novelty ranking, well over the 40% observed by Voorhees (except for topic foreign students visa restrictions). We believe this validates our second set of preferences, though certainly the question is not closed.
4.2.1 Transitivity in Preference Judgments
One issue in using our preference judgments for novelty is that the number of pairwise judgments increases quickly with number of documents. Increase in number of judgments means increase in assessor time, but if the assessors are consistent i.e. if their judgments are transitive, then we can

869

srecall@20 0.70 0.75 0.80 0.85 0.90 0.95 1.00









































1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Levels
Figure 3: S-recall increases as we simulate deeper levels of preference judgments, but the first set of novelty preferences (level 2) gives an increase that nearly exceeds all subsequent levels combined.
reduce the number of preferences from O(n2) to O(n log n) at each level; furthermore, since we really only need the "best" document at each level, transitivity would allow us to reduce the number of preferences to O(n) at each level.
We performed experiments to check for transitivity in the novelty task by looking at triplets of documents. A triplet of documents i, j, k is transitive if and only if i is preferred to j, j is preferred to k, and i is preferred to k. The ratio of number of triplets found to be transitive to the total number of triplets give a measure of transitivity in the preference judgments. On average transitivity holds for 98% across all queries with each query being transitive 96% of the time. This suggests that the assessors are highly consistent in their judgments; thus using a sorting algorithm with minimum information loss could further reduce the number of judgments required. It also suggests that whatever other features of documents (apart from topical relevance and novelty) the assessors are using in their decision process, they are consistent in their use of those features.
4.2.2 How many levels of judgments are needed?
In this section we show that the first two sets of preferences, i.e. experiments 1 and 2, are approximately sufficient to produce an optimal ranking. We again use preferences simulated from subtopic judgments: a relevance ranking is found by always preferring the document with more subtopics ("level 1"); a first approximation to a novelty ranking is found by always preferring the document with the most subtopics that are not in the top document ("level 2"); a second approximation by always preferring the document with the most subtopics that are not in the first two documents ("level 3"); and so on up to level 20.
Figure 3 shows the S-recall scores increasing as the number of preference sets increases. Clearly the increase in S-recall from level 1 to level 2 is the largest, nearly exceeding the total increase obtained from all subsequent levels put together. This suggests that the first approximation novelty ranking is likely to be sufficient; this has the benefit of reducing the amount of assessor effort needed to produce the data.
5. CONCLUSION AND FUTURE WORK
We have taken initial steps into investigating the use of preference judgments for novelty ranking tasks. We have proposed a novel framework for obtaining preference judgments for the novelty task and explicated the pros and cons of using preference judgments. Preliminary results for com-

paring explicit subtopic labels with preference judgments suggest that preference judgments can give similar information about both relevance and novelty as the subtopic judgments that are typically used.
Based on this, we proposed a preference-based approach to obtaining a full ranking for relevance, novelty, and all other factors that contribute to user preferences. We showed that rankings obtained in this way correlate well to rankings based on subtopic judgments, and since assessors are highly self-consistent, probably capturing a great deal of other information as well. Of course, if subtopic judgments were replaced with preferences, we would need a new set of evaluation measures. This clearly is a direction for future work.
6. REFERENCES [1] Amazon mechanical turk. http://www.mturk.com. [2] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proc. of WSDM, pages 5≠14, 2009. [3] J. Allan, B. Carterette, and J. Lewis. When will information retrieval be "good enough"? In Proc. of SIGIR, pages 433≠440, 2005. [4] O. Alonso, D. Rose, and B. Stewart. Crowdsourcing for relevance evaluation. In ACM SIGIR Forum, number 2, pages 9≠15, Nov. 2008. [5] J. Arguello, F. Diaz, J. Callan, and B. Carterette. A methodology for evaluating aggregated search results. In Proceedings of the 33rd European Conference on Information Retrieval (ECIR, 2011. [6] B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais. Here or there: preference judgments for relevance. In Proceedings of the IR research, 30th European conference on Advances in information retrieval, Proceedings of ECIR, pages 16≠27, 2008. [7] P. Chandar and B. Carterette. What qualities do users prefer in diversity ranking. In Proceedings of the 2nd Workshop on Diversity in Document Retrieval, 2012. [8] O. Chapelle, S. Ji, C. Liao, E. Velipasaoglu, L. Lai, and S.-L. Wu. Intent-based diversification of web search results. Information Retrieval, pages 1≠21, 2011. [9] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceeding of CIKM, pages 621≠630, 2009. [10] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova,
A. Ashkan, S. Bu®ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of SIGIR, pages 659≠666, 2008. [11] T. Moore and R. Clayton. Evaluating the wisdom of crowds in assessing phishing websites. In In 12th International Financial Cryptography and Data Security Conference, pages 16≠30. Springer-Verlag, 2008. [12] F. Radlinski, P. N. Bennett, B. Carterette, and T. Joachims. Redundancy, diversity and interdependent document relevance. SIGIR Forum, 43:46≠52, Dec 2009. [13] M. E. Rorvig. The simple scalability of documents. JASIS, 41(8):590≠598, 1990. [14] T. Sakai, N. Craswell, R. Song, S. Robertson, Z. Dou, and C. Y. Lin. Simple evaluation metrics for diversified search results. In Proc. EVIA, 2010. [15] M. Sanderson, M. L. Paramita, P. Clough, and E. Kanoulas. Do user preferences and evaluation measures line up? In Proceedings of SIGIR, pages 555≠562, 2010. [16] E. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. In Proceedings of SIGIR, pages 315≠323, 1998. [17] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proceedings of SIGIR, pages 10≠17, 2003.

870

Quality through Flow and Immersion: Gamifying Crowdsourced Relevance Assessments

Carsten Eickhoff
Delft University of Technology Netherlands
c.eickhoff@tudelft.nl

Christopher G. Harris Padmini Srinivasan
The University of Iowa USA
{christopherharris,padminisrinivasan}@uiowa.edu

Arjen P. de Vries
Centrum Wiskunde & Informatica Netherlands
arjen@acm.org

ABSTRACT
Crowdsourcing is a market of steadily-growing importance upon which both academia and industry increasingly rely. However, this market appears to be inherently infested with a significant share of malicious workers who try to maximise their profits through cheating or sloppiness. This serves to undermine the very merits crowdsourcing has come to represent. Based on previous experience as well as psychological insights, we propose the use of a game in order to attract and retain a larger share of reliable workers to frequentlyrequested crowdsourcing tasks such as relevance assessments and clustering. In a large-scale comparative study conducted using recent TREC data, we investigate the performance of traditional HIT designs and a game-based alternative that is able to achieve high quality at significantly lower pay rates, facing fewer malicious submissions.
Categories and Subject Descriptors
H.1.2 [Models and Principles]: User / Machine Systems --Human Factors; H.1.2 [Models and Principles]: User / Machine Systems --Human information processing; H.5.2 [Information Interfaces & Presentation]: User Interfaces
Keywords
Crowdsourcing, Gamification, Serious Games, Relevance Assessments, Clustering
1. INTRODUCTION
In the course of the past 5 years, crowdsourcing has advanced from a niche phenomenon to becoming an accepted solution to a wide range of data acquisition challenges [10]. It has been used in the training and test phases of a great
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

number of scientific projects and is firmly integrated into numerous evaluation and data acquisition schemes in academia and industry. One problem, however, seems to be inherent to the field; a significant share of the annotations created on crowdsourcing platforms are fraudsters' attempts to cheat the HIT (Human Intelligence Task) provider into paying them without having properly worked on the HIT. As a consequence, almost every scientific publication that employs crowdsourcing for data acquisition details the authors' tailor-made defense scheme against cheaters and a rising number of publications is exclusively dedicated to the task of detecting these individuals. The range of commonlyobserved measures taken includes asking redundant questions to rely on an aggregate of several workers rather than the decisions of just one individual [33], using held-out data to compare with, asking plausibility questions and many more sophisticated methods [16, 13]. In many time-insensitive applications, HIT providers restrict the crowd of workers to certain nationalities (a typical example would be US workers only) who they trust will provide higher quality results. Although the approach is widely accepted and has been shown to significantly reduce the number of spam submissions, we believe that this uptake may be treating symptoms rather than the actual underlying cause. Rather than attributing the confirmed performance differences between the inhabitants of different countries to their nationality, we hypothesize that there are 2 major types of workers with fundamentally different motivations for offering their workforce on a crowdsourcing platform: (1) Money-driven workers are motivated by the financial reward that the HIT promises. (2) Entertainmentdriven workers primarily seek diversion but readily accept the financial incentives as an additional stimulus. We are convinced that the affiliation (or proximity) to one of those fundamental worker types can have a significant impact on the amount of attention paid to the task at hand, and, subsequently, on the resulting annotation quality. We realize, that money-driven workers are by no means bound to deliver bad quality; however, they appear to be frequently tempted into sloppiness by the prospect of a higher time efficiency and therefore stronger satisfaction of their main motivation. Entertainment-driven workers, on the other hand, appear to work a HIT more faithfully and thoroughly and regard the financial reward as a welcome bonus. They typically do not indulge in simple, repetitive or boring tasks. We propose to more strongly focus on entertainment-driven work-

871

ers by phrasing crowdsourcing problems in an entertaining and engaging way: As games. Csikszentmihalyi's theory of flow [12], a state of maximal immersion and concentration at which optimal intrinsic motivation, enjoyment and high task performance are achieved, further encouraged our design. We intend to increase the degree of satisfaction entertainmentdriven workers experience. This can lead to (a) higher result quality, (b) quicker batch processing rates, (c) lower overall cheater rates, (d) better cost efficiency. An additional incentive for delivering high-quality results in a game scenario would be the element of competition and social standing among players. Taking into account recent behavioural analyses of online communities and games [24], entertainment seekers can be expected to put considerable dedication into producing high-quality results to earn more points in a game to progress into higher difficulty levels or a rank on the high score leaderboard. The novel contributions of this work are 5-fold: (1) We describe a game-based approach to collecting document relevance assessments in both theory and design. (2) Based on NIST-created TREC data, we conduct a large-scale comparative evaluation to determine the merit of the proposed method over state-of-the-art relevance assessment crowdsourcing paradigms. (3) Venturing beyond "hard" quality indicators such as precision, cost-efficiency or annotation speed, we discuss a wide range of socio-economical factors such as demographics and alternative incentives to enhance a fundamental understanding of worker motivation. (4) In a separate study, we demonstrate the generalizability of the proposed game to other tasks on the example of noisy image classification. (5) We create a corpus of relevance assessments of considerable size and quality that we disclose to the research community. The remainder of this work is structured as follows: Section 2 describes the state of the art of crowdsourcing for document relevance assessments as well as in the domain of games with a purpose (GWAP). Section 3 introduces the theoretical considerations and design decisions that guide our annotation game. In Section 4, we describe the setup as well as the results of a large scale study conducted on several commercial crowdsourcing platforms in order to determine the usefulness of the proposed method. Section 5 demonstrates the generalizability of our method on the task of image classification. Section 6 is dedicated to a discussion of the key insights gained in the course of this work, before concluding in Section 7.
2. RELATED WORK
This section introduces related approaches from three different research areas at the intersection of which this work is situated, namely, relevance assessment, crowdsourcing and games with a purpose. Document relevance assessments have been playing a central role in IR system design and evaluation since the early Cranfield experiments [42]. Explicit judgements of (the degree of) relevance between a document and a given topic are used as a proxy of user satisfaction. Based on such test collections, the results of retrieval systems can be compared without undergoing numerous iterations of user studies. As a leading actor in IR evaluation and benchmarking, NIST's Text Retrieval Conference (TREC) [43] looks back on two decades of assessing document relevance. In order to be

suitable for the evaluation of state-of-the-art Web-scale systems, the requirements in terms of size, topical coverage, diversity and recency that the research community imposes on evaluation corpora have been steadily rising. As a consequence, the creation and curation of such resources becomes more expensive. To further ensure the scalability of test collection-based system evaluation, considerable effort has been invested into designing robust performance measures [6], selecting the right documents for evaluation [8] and inferring judgements from user interaction logs [19]. Crowdsourcing represents an alternative means of collecting and annotating large-scale data sets. By employing a large group of individuals, paid at transaction level, tasks of considerable size can be completed in a timely and affordable manner. Document relevance assessments have been shown to be a task that can reliably be fulfilled by crowd workers [4, 22, 15]. One of the fundamental challenges in crowdsourcing is overcoming malicious and sloppy submissions. Many effective schemes exist, ranging from aggregating the results of independent workers to the use of honey pot questions [17, 18, 14]. Marshall et al. discuss the importance of engaging HIT design on result quality [28]. Recently, several scientific workshops have been dedicated to pursuing how to use crowdsourcing effectively and efficiently [10, 26]. Most notably, TREC 2011 for the first time offered a dedicated crowdsourcing track [25] addressing the crowdsourced collection of document relevance assessments. In this work, we adopt the evaluation scheme and data set used there. The majority of crowdsourced tasks are plain surveys, relevance assessments or data collection assignments that require human intelligence but very little creativity or skill. An advance into bringing together the communities of online games and crowdsourcing is being made by the platform Gambit [1], that lets players complete HITs in exchange for virtual currency in their online gaming world. This combination, however, does not change the nature of the actual HIT carried out, beyond the fact that the plain HIT form is embedded into a game environment. Instead, we propose using an actual game to leverage worker judgements. A number of techniques have been designed to make participation in human computation efforts as engaging as possible. Perhaps the most effective technique among these is a genre of serious games called games with a purpose [38] which have been developed with the focus of efficient and entertaining transformation of research data collection into game mechanics. By equating player success in the game with providing quality inputs, the idea is to extract higherquality data than is currently done with dull repetitive tasks such as surveys. More than half a billion people worldwide play online games for at least an hour a day ≠ and 183 million in the US alone [29]. The average American, for example, has played 10,000 hours of video games by the age of 21 [31]. Channeling some of this human effort to gather data has shown considerable promise. People engage in these GWAPs for the enjoyment factor, not with the objective of performing work. Successful GWAPs include the ESP Game [37], which solicits meaningful, accurate image labels as the underlying objective; Peekaboom [41], which locates objects within images; Phetch [39], which annotates images with descriptive paragraphs; and Verbosity [40], which collects common-sense facts in order to train reasoning algorithms. The typical research objective of these GWAPs is to have two randomly-selected players individually assign mutually-

872

agreed document labels, with the game mechanics designed to reward uncommon labels. In contrast, the game mechanics of our proposed game is to encourage and reward consensus labeling. The Pagehunt game presented players with a web page and asked them to formulate a query that would retrieve the given page in the top ranks on a popular search engine to investigate the findability of web pages [27]. While task-specific games have been shown to be engaging means of harnessing the players' intelligence for a certain research goal, there has not been a formal investigation of the merits of game-based HITs over conventional ones. Additionally, current GWAPs are typically highly tailored towards a certain (often niche) problem at hand and do not lend themselves for application across domains. We will demonstrate the generalizability of our approach in Section 5.
3. METHODOLOGY
In this section, we will introduce the annotation game as well as the necessary pre- and post-processing steps in order to acquire standard topic/document relevance assessments from it. Careful attention will be paid to highlighting motivational aspects that aim to replace HIT payment by entertainment as a central incentive.
3.1 Game Design
The central concept of our proposed game is to require players to relate items to each other. In order to preserve its general applicability, we tried to make as few as possible assumptions about the nature of those items. The game shows n = 4 concept buckets b1 . . . bn at the bottom of the screen. From the top, a single item i slides to the bottom, and has to be directed into one of the buckets by the player. Doing so expresses a relationship between i and bj . Additional information about i can be found in an info box in the top left corner. In the case of document relevance assessments, the concept buckets bj display topic titles, item i is a keyword from a document and the info box displays the context in which the keyword appears in that document. Figure 3.1 shows a screenshot of our game. A live version can be found online1. For each assigned relation between i and bj the player is awarded a number of points. The score in points is based on the degree of agreement with other players. In addition to this scheme, the game shows a tree that grows a leaf for every judgement that consents with the majority decision. A full tree awards bonus points. In this way, we reward continuous attention to the game and the task. As a final element, the game is divided into rounds of 10 judgements each. After each round, the speed with which item i moves is increased, making the task more challenging to create additional motivation for paying close attention. After up to 5 rounds (the player can leave the game at any point in time before that) the game ends and the achieved points as well as the player's position in our highscore leaderboard are shown. Together with the log-in concept, this aims to encourage replaying as people want to advance into the higher ranks of the leaderboard.
3.2 Data Pre-processing
Previously, we described the mechanics and underlying assumptions of the game. Now, we will detail how to set
1http://www.geann.org

Figure 1: A screenshot of the annotation game in which the keyword "Eiffel Tower" has to be related to one of a number of concepts.

it up with TREC data for relevance assessments. All rel-
evant game information is stored in a relational database from which items, concepts and additional information are drawn and into which user judgements are stored, subsequently. We assume a list of pairs p consisting of a query q and a document d for which we will collect user judgements. For every d, we extract the textual web page content and break it up into a set S of sentences sd,1...sd,|S| using the LingPipe library [5]. In the following step, S is reordered by a ranking function r(s), based on decreasing mean inverse document frequency (idf ) of sentences s with a number of |s| constituent terms tn. |C| denotes the number of documents in the collection and df (t) is the number of documents containing term t. In this way, we promote the most salient and informative sentences in the document. The underlying idf (t) statistics for this step are computed collection-wide.

idf

(t)

=

|C| df (t) +

1

r(s)

=

1 |s|

|s|
idf (tn)
n=1

Finally, the k highest-ranking sentences (those with the highest scores of r(s)) are selected and used in the game. The concrete setting of k depends on the size of document d and was set to 0.1|S| in order to account for different document lengths. Higher settings of k result in a higher judgement density per document. For each of the selected sentences, we extract the single highest-idf term tn as sliding keyword i, and the full sentence as context information to be shown in the top left corner. The concept buckets b include the original query q, two randomly selected topics from the database, as well as an "other" option to account for none of the offered concepts being related to i. The buckets are shown in random order to prevent selection biases.

3.3 Assessment Aggregation
As a final step, we have to transform the players' conceptual associations into document-wide relevance judgements. Each player annotation a can be understood as a quintuple a = (pa, ia, sa, ca, ra) in which player p associated item i occurring in context sentence s with concept c in round r of the game.

873

In a first step, we map all associations a to relevance votes. We interpret associations of any s  d to the concept of the original query q as a player's binary relevance vote vp,s,q,r between sentence s and query q as described in Equation 1.

vp,s,q,r =

1 0

if ca = q else

(1)

In order to account for wrong associations and diversity in personal preference, we aggregate a global sentence-level vote vs,q across all players p. As the game speeds up in higher rounds, players have less time available for making the relevance decision. In a preliminary inspection of annotation results, we noticed significant drops of accuracy across subsequent rounds of the game. In order to account for this effect, we introduce a weighting parameter r representing the confidence that we put into judgements originating from round r of the game being correct. For simplicity's sake, we reduce the confidence by 0.05 per round after the first one. Alternative strategies could for example include learning this parameter as a maximum likelihood estimate across previous observations. Equation 2 details the aggregation to a global sentence-level vote vs,q across the set of players Ps,q that had encountered the combination of sentence s and query q.

1

vs,q

=

|Ps,q |

r vpi ,s,q,r
pi Ps,q

(2)

Finally, we aggregate across all sentence-level votes vs,q of a document d in order to get one global page-wide judge-

ment that is comparable to well-known (e.g., NIST-created)

annotations. Equation 3 outlines this process formally. It

should be noted, that omission of this third step may, given

the application at hand, be beneficial for the evaluation of

tasks such as passage-level retrieval or automatic document

summarization.

1

vd,q = |D|

vsi ,q

(3)

si D

4. EXPERIMENTATION

4.1 Research Directions
In this section, we describe the setup and results of a large-scale experiment conducted on several major commercial crowdsourcing platforms. Our performance comparison of traditional and game-based HITs will be guided by the following 7 fundamental directions:

Quality. How does the result quality of game-based crowdsourcing HITs relate to that of traditional ones given the same underlying crowd of workers and comparable financial means? We evaluate result quality in terms of agreement with gold standard NIST labels as well as with consensus labels across all participanting groups of the TREC 2011 Crowdsourcing Track [25].
Efficiency. Are game-based HITs more popular, resulting in quicker HIT uptake and completion than conventional ones? We investigate time to completion (for a given batch size) as well as the duration per document and per single vote.

Incentives. How much is fun worth? We investigate the influence of incentives such as fun & social prestige vs. monetary rewards on the uptake rate of HITs. Do users prefer entertaining HIT versions even though they pay less?
Consistency. Does our game encourage a stronger task focus, resulting in better within-annotator consistency? We investigate this dimension using a fixed set of workers (of varying reliability and quality levels) who are exposed to re-occurring HIT questions in a game-based or conventional setting to measure their self-agreement as an estimate of consistency and alertness.
Robustness. Does the share of (alleged) cheaters attracted to our game / attracted to conventional HITs differ? Independent of the overall result quality, the observed cheater rate is a surrogate of how reliable results are and how much sophistication a HIT should dedicate to fraud protection.
Population. Does the use of games lead to a different crowd composition? Offering game-based and conventional HITs, we collect surveys to investigate whether gamebased HITs attract different kinds of workers.
Location. State-of-the-art crowdsourcing approaches frequently filter their crowd by nationality in order to improve result quality. We investigate whether there are indeed geographical preferences for game-based or conventional HITs and whether those can be related to the crowd composition in those areas.
4.2 Experimental Setup
In this comparative study, we replicate the setting that was proposed in the TREC 2011 Crowdsourcing Track assessment task [25]. A total of 3200 topic/document pairs (30 distinct topics, 3195 unique documents) were judged for relevance. The documents are part of the ClueWeb09 collection [7], and the topics originate from the TREC 2009 Million Query Track [9]. A comprehensive list of all topics and document identifiers are available from the TREC 2011 Crowdsourcing Track home page2. We contrasted the performance and characteristics of our proposed gamified HIT (Section 4.2.2) with those of a traditional one (Section 4.2.1). To attribute for assessment mistakes and personal preference, we collected judgements from at least 3 individual workers per topic/document pair in both settings. All HITs were run in temporal isolation (No more than 1 batch at any given time) to limit mutual effects between the tasks. In the following, we describe the respective task designs in detail.
4.2.1 Traditional HIT
As a performance baseline, we designed a state-of-the-art relevance assessment HIT. Its design follows accepted insights from previous work as detailed in the following. In order to limit the number of context changes, the document is shown in-line on the platform's HIT form as proposed by Kazai [20]. In this way, no distracting opening and closing of windows or browser tabs is required. To further enhance the task, we highlight every occurrence of query terms in
2https://sites.google.com/site/treccrowd2011/

874

the document. This technique was reported to be beneficial by several previous approaches, e.g., [36]. Finally, in order to deal with malicious submissions, we measure agreement with NIST gold standard pairs of known relevance. Workers who disagree on more than 50% of the gold labels are rejected from the judgement pool. In the HIT instructions, we briefly introduce the available relevance categories. The definition of relevance was introduced according to the TREC guidelines [25]. The HIT form contains 2 questions:
1. Please indicate the relevance of the shown document towards the topic "<T>".
2. Do you have any remarks, ideas or general feedback regarding this HIT that you would like to share?
For each HIT, the place holder <T> is replaced by the current topic. Offering the possibility for worker feedback has been frequently reported to improve task quality and track down bugs or design flaws quickly [3]. The HIT was offered at a pay rate of 2 US cents per topic/document pair assessments; a reward level previously found adequate given the task [2].
4.2.2 Gamified HIT
The central piece of our proposed gamified version of the relevance assessment HIT is the annotation game that was described in Section 3.1. Instead of having the workers complete tasks locally on the crowdsourcing platform, the technical requirements of our game demanded running it off-site on a dedicated server. In order to verify task completion, workers are provided with a confirmation token after playing one round of the game (10 term associations). Back on the crowdsourcing platform, they enter this token in order to get paid. As a consequence, the actual HIT contained only a brief instruction to the off-site process and two input fields:
1. Please enter the confirmation token you obtained after completing one round of the game.
2. Do you have any remarks, ideas or general feedback regarding this HIT that you would like to share?
Again, we solicit worker feedback. The HIT was offered at a pay rate of 2 US cents for one round (10 term associations) of the game.
4.3 Evaluation
All experiments described in this section were conducted between December 2011 and February 2012 on two crowdsourcing platforms: Amazon Mechanical Turk [35] as well as all available channels on CrowdFlower [11]. Initial evaluation did not show any significant differences in the work delivered by workers from different platforms. We will therefore not split the pool of submissions along this dimension. In total, 795 unique workers created 105,221 relevance judgements via our game. Additionally, 3000 traditional relevance judgements were collected for comparison. In total, we invested $90 to collect a volume of 108,221 annotations across the two compared experimental conditions. Together with the TREC 2011 Crowdsourcing Track annotations and

Table 1: Annotation quality. HIT type Accuracy (NIST) Accuracy (TREC-CS)

Conventional

0.73

0.74

TREC-CS

0.79

1.0

Game (plain)

0.65

0.75

Game (sent)

0.77

0.87

Game (doc)

0.82

0.93

Table 2: Annotation quality as a function of the game round in which judgements were issued.

Round Accuracy (NIST) Accuracy (TREC-CS)

1

0.72

0.81

2

0.67

0.77

3

0.62

0.73

4

0.60

0.69

5

0.54

0.65

the original NIST labels, this makes the T11Crowd subset of ClueWeb09 one of the most densely-annotated Web resources known to us. To enable reproducibility of our insights and to further general crowdsourcing research, the complete set of our judgements and the game itself are available to the research community3.
4.3.1 Quality
As a starting point to our performance evaluation of gamebased crowdsourcing of relevance assessments, we investigate the quality of the collected labels. Table 1 details the performance of our game in terms of overlap with gold standard NIST labels as well as the global consensus across all TREC 2011 Crowdsourcing Track participants (TREC-CS). We can note that already the conventional HIT delivers high result quality. Ratios between 65% and 75% are often considered good rules-of-thumb for the expected agreement of faithful human judges given a relevance assessment task [44]. TREC consensus labels show a high overlap with NIST annotator decisions. The third row in Table 1 shows the performance of direct unaggregated sentence-level votes from our game as described in Equation 1. While agreement with the TREC crowd is already substantial, the overlap with high-quality NIST labels lags behind. As we aggregate across multiple workers' annotations of the same sentence (Equation 2) and, finally, across all sentences extracted from the same document (Equation 3), the performance rises significantly, outperforming all compared methods. We used a Wilcoxon signed rank test at  < 0.05-level to test significance of results.
In order to confirm the usefulness of our assumption from Section 3 concerning the decline of label quality as the game speeds up and the player has less time to make decisions, we evaluated annotation performance of raw labels according to the round in which they were issued. Table 2 shows a near-linear decline in agreement of plain game scores with TREC consensus as the game progresses. Agreement with NIST scores also consistently shrinks from round to round.
Finally, previous work on prediction in crowdsourcing systems demonstrates that reliability of the average predicted scores by the crowd improves as the size of the crowd in-
3http://sourceforge.net/projects/geann/

875

Table 3: Annotation efficiency.

Conventional Game-based

t per vote t per doc

40.1 sec 40.1 sec

5.2 sec 27.8 sec

Uptake (votes per hour)

95.2

352.1

Accuracy 0.65 0.70 0.75 0.80 0.85 0.90

Consensus NIST

0

5

10

15

20

25

30

35

Number of votes per pair

Figure 2: Quality as a function of votes per pair.
creases [34, 30]. The benefit of our game-based HIT is its popularity that allows us to collect more judgements per topic / document pair than traditional HITs. On average, each topic / document pair in the collection received 32 unique user judgements at the sentence level (some of which may originate from the same user as she or he rates different passages of the same document). Figure 2 shows how annotation quality develops as we add more judgements per pair. After initial fluctuation, as single votes have great influence on the overall decision, accuracy consistently improves as we add more votes per pair. The effect levels out as we approach the upper performance limit.
4.3.2 Efficiency
The second official performance indicator besides label quality in the TREC 2011 Crowdsourcing Track was the time necessary to collect the required judgements. For many use cases in human computation, low latencies are essential. The particular nature of the game imposed a time limit on players within which they had to make their decisions. As we detailed in the previous section, aggregation across users, weighting votes according to the difficulty level under which they were created, ensured competitive result quality. At the same time, however, a sequence of, individually quick, concept matchings enables workers to be more efficient and motivated than in conventional settings. Table 3 shows how conventional HITs take slightly longer to judge documents even when aggregating the duration of all passage-level votes in the game-based setting. Taking into account the significantly higher uptake rate (number of judgements issued per hour) of the game HITs, this serves for a considerably more efficient batch processing. Especially in conjunction with the previous section's findings of high degrees of redundancy serving for better result quality, high uptake rates become crucial as they allow for timely, yet accurate decisions.

Table 4: Game-based assessment behaviour.

Criterion

Observation

Games with 2+ rounds Rounds per game

70.9% 3.5

Players with 2+ games Games per player

79.5% 4.36

Time between games

7.4 hrs

4.3.3 Incentives
The third and final evaluation criterion employed for TREC 2011 was the cost involved in the collection of relevance labels. With our game-based approach, we aim to, at least partially, replace the financial reward of the HIT with entertainment as an alternative motivation. In this section, we will investigate to which degree this change in incentives can be observed in worker behaviour. In order to be paid via the crowdsourcing platform, workers had to complete at least one round (10 concept matchings) of our game. At that point the required confirmation token was displayed to them and they could return to the platform in order to claim their payment. However, the game offered an additional 4 levels to be played. From a purely monetary-driven perspective there would be no reason for continuing to play at that point. As we can see in Table 4, however, over 70% of games are played beyond the first round. This essentially results in crowdsourcing workers creating judgements free of charge because they enjoy the game experience. Additionally, we can observe players to return to the game after a number of hours to play again and improve their score and their resulting position on the leader board. Subsequent visits often happen directly to the game page, without being redirected from (and paid through) the crowdsourcing platform. Almost 80% of all players (633 out of 795) return after their first round played, with an average time gap of 7.4 hours between games. For regular HITs, we observed a return rate of only 23%.
When inspecting the concrete distribution of judgements across workers, as shown in Figure 3, we see this trend continued. Crowdsourcing tasks often tend to exhibit Power-law distributions of work over unique workers with some strong performers and a long tail of casual workers who only submit single HITs. Here, however, we notice a strong center group of medium-frequency players. We hypothesise that replacing the workers' extrinsic motivation ("do the HIT to earn money") by an intrinsic one ("let's have some fun"), causes these tendencies.
This has a number of noteworthy consequences: (1) We can attract workers to a HIT at a comparatively low pay rate. Even without playing beyond the first round, 2 US cents for 10 concept associations would roughly result in a prospective hourly pay of $1.20. (2) Furthermore, as most workers continue playing, additional annotations are created with no expectation of financial compensation. (3) Drawn by the competitive aspect of the game, workers re-

876

Frequency

0

50

100 150 200

0 100 200 300 400 500 600 Number of judgements per player

Figure 3: Distribution of judgements across users.

Table 5: Effective annotation cost. Conventional Game-based

Cost per doc

$0.06

$0.0004

Whole corpus

$192

$1.28

Effective hourly rate

$1.80

$0.18

turn and create even more unpaid assessments. As a consequence, the overall amount of money invested into the game-based collection of more than 100,000 sentence-level relevance judgements was $27.74. This includes all administrative fees charged by the crowdsourcing platforms. In comparison, the participants to the TREC 2011 Crowdsourcing Track reported overall costs of $50 - $100 for the collection of significantly fewer labels.
Table 5 shows a final cost comparison of the conventional and game-based versions of the inspected relevance assessment HIT. While all indicators of cost efficiency from a worker's perspective clearly speak for choosing the conventional, better-paying HIT, the previously described figures of HIT uptake rates as well as the high number of alternative HITs available at all times on large-scale platforms such as AMT, indicate, that we reach workers who consider the entertainment potential of a HIT before choosing it. If we consider all judgements made in rounds after the first one and all judgements from revisits that were not paid for on the crowdsourcing platform as free-of-charge judgements, we arrive at a share of 83.7% of all labels having been created free of charge. Additionally, a number of players (39 out of 795) accessed the game without being prompted (and paid) by a crowdsourcing platform. These players were recruited from the authors' professional and private networks or word of mouth of other players. We could not find significant differences in the judgement quality or volume created by this group of players. The invested amount of money can be seen as advertisement costs rather than actual payments. In a traditional setting, collecting the same annotation density would have cost $2104.

4.3.4 Consistency
Following Csikszentmihalyi's theory of Flow [12], a state of deep immersion is a good foundation for high performance independent of the concrete task at hand. With our game-based HIT, we aimed to exploit this observation in order to create greater task focus than workers typically achieve on conventional HIT types. The previously shown result quality figures support this hypothesis. As an additional performance indicator, we will measure the workers judgement consistency. Faced with the same passage of text and choice of concepts multiple times, a situationaware worker is expected to display a high degree of intraannotator agreement. In the course of our judgement collection, we showed identical assignments to workers 837 times and observed an intra-annotator agreement of 69.8%. We set up a dedicated crowdsourcing experiment in which a portion of the offered topic / document pairs re-occurred. The HIT was set up following the scheme described in Section 4.2.1. Across 500 redundantly issued assignments, we observed an intra-annotator agreement of only 61.3%, a significantly lower ratio (determined using Wilcoxon signed rank test at  < 0.05) than in the game-based setting. While the game setting resulted in higher consistency than usual crowdsourcing schemes, we could not match the consistency Scholer et al. [32] report for professional assessors as for example employed by NIST.
4.3.5 Robustness
Cheating, spamming and low-quality submissions are wellknown and frequently-observed incidents on commercial crowdsourcing platforms. Previously, we demonstrated convincing result quality of gamified document relevance assessments when labels are aggregated across a sufficiently large number of workers. Since our approach appeals more to the entertainment-seeking rather than money-driven workers, we did not include a dedicated cheat detection scheme as would often be considered necessary in state-of-the-art HITs. However, we realise that the observed cheat rate in an assignment can serve as a surrogate for the confidence and reliability of the overall results. To this end, we measure the observed proportion of cheat submissions to our game as well as to the conventional HIT version. Eickhoff et al. [14] suggest categorizing workers who disagree with the majority decision in more than half of all cases as cheaters. In order to deliver a conservative estimate of the rate of cheat submissions, we tighten their definition and consider a worker as cheating if at least 67% of their submissions disagree with the majority vote. This scheme was applied to both, the conventional HIT as well as the gamified version. In the game-based case, we additionally flagged all submissions as cheat that tried using forged confirmation tokens. Overall, this resulted in a share of 13.5% of the conventional HIT's judgements being considered cheated. For the gamebased version, the percentage was a significantly lower 2.3%. This finding conforms with [13], who observed innovative, creative tasks being less likely to be cheated on.
4.3.6 Population
In this work, we did not make use of any form of a priori filtering the pool of workers eligible to access our HITs. We hypothesise, however, that HIT type, financial reward and task phrasing influence the underlying crowd that decides to work on a given assignment. To better understand the com-

877

Table 6: Composition of the crowd Conventional Game-based

Preference Female

39%

37%

47%

35%

Age Univ. degree

34

27

46%

62%

Income

$20k

$45k

English Native Speaker

24%

25%

position of the group of commercial crowdsourcing workers that are interested in games, we accompanied parts of our HITs by surveys in which we asked for high-level participant demographics and their preference for either the conventional or the game-based HIT. Table 6 shows an overview of several salient outcomes of the survey. The split in decisions was roughly equal, with 24% of workers not indicating clear preferences. The entertainment-seeking worker is on average several years younger, more likely to hold a university degree and will typically earn a higher salary. Finally, women were found to be significantly less interested in games than their male co-workers. This conforms with general observations about gender differences made for example by [23]. A worker's language background did not influence his or her likelihood to prefer games.
4.3.7 Location
Many commercial crowdsourcing schemes report performance gains when filtering the crowd by nationality. Due to different expected levels of education, language skills or cultural properties, such steps may influence result quality. As a final dimension of our investigation of games for use on commercial crowdsourcing platforms, we will inspect whether worker origin has an influence on result quality. From our survey, we found Indian workers, with a share of 60%, to be the dominant group in both settings. US workers were consistently the runners-up with a proportion of approximately 25%. There was no significant difference in the likelihood to prefer games over conventional HITs between countries. Finally, when inspecting result quality from our game, again, no difference in performance or likelihood to cheat could be found. This suggests that filtering workers by nationality may not be ideal. In fact, the underlying worker motivation and HIT type preference may be assumed to have a far greater impact on observed uptake, performance and trustworthiness.
5. IMAGE CLASSIFICATION
In the previous sections, we described and evaluated the performance of the proposed crowdsourcing-powered annotation game for the task of TREC-style document relevance assessments. To demonstrate the generalization potential of the described concept-matching method, we applied the same game in an image classification pilot. In the course of the Fish4Knowledge project (http://www. fish4knowledge.eu/), several underwater cameras have been placed in selected locations in south-east Asian coral reefs. The continuous recordings are supposed to further knowledge about behaviour, frequency and migration patterns of the resident tropical fish species. A key step to coping with the large amounts of image data produced by these cam-

Figure 4: GeAnn applied for image grouping.
eras is a reliable automatic species classification. In order to train such systems, numerous training examples are required. While the project employs a team of marine biologists, their greater expertise is costly. Using our annotation game, we crowdsource the task of classifying the encountered species. Instead of relating keywords to TREC topics, the objective is now to match a shot of the underwater camera (often low quality) to high-quality examples of resident fish species. By initializing the underlying database with images rather than textual items, no changes to the actual game were necessary. Figure 4 shows a screenshot of this alternative game setting.
Our feasibility study encompassed 190 unique underwater camera shots for which known gold standard labels created by the marine biologists existed. Each biologist had classified all images, allowing us to contrast crowd agreement with expert agreement. The HIT was offered in January and February 2012 at the same pay rate (2 US cents per round of 10 associations) as the text-based version. Table 7 shows the results of the experiment in which the degree of agreement with the majority of experts as well as the crowd's inter-annotator agreement are detailed. We can see high agreement with expert labels as well as substantial agreement among workers. The popularity (qualitatively perceived through worker feedback) and uptake rate of this HIT even slightly exceeded those of the game-based one for document relevance assessments. Several workers had mentioned difficulties reading the moving text fragments in the short time available. With images, this does not appear to be an issue. Methods like this could play an essential role either in the creation of training and evaluation data necessary for the assessment of automatic classification quality, or as part of a hybrid human-computer classification in which automatic methods narrow down the range of potential species before human annotators select the most likely species from the pre-selection. It should be noted, however, that the domain experts are by no means obsolete in this setting. While they annotated fish images simply based on their knowledge of the resident species, players of our game only had to select one out of a range of 4 species by similarity.
6. DISCUSSION
In the previous section, we found convincing results across all inspected performance dimensions, supporting the bene-

878

Table 7: Image classification performance Agreement (exp.) Inter-annot. Agreement

Experts

0.82

-

Game

0.75

0.68

fit of offering alternative incentives besides the pure financial reward. In this section, we discuss a number of observations and insights that were not yet fully covered by the evaluation.
Firstly, considering the fact that the round concept of the game appears to invite workers to create assessments without payment (by playing on after having received the confirmation token), it is not obvious why we should limit the game to a fixed number of rounds. In the present setting, a game inevitably ends after the fifth round. One might argue that a higher number of rounds or even an open-ended concept would result in even greater cost efficiency. In fact, the opposite seems to be the case. In an initial version of the game, there was no upper limit to the number of rounds per game. As a consequence, some players were frustrated, as the only way to "finish" the game would be to either lose or give up. This resulted in fewer returning players. Additionally, the quality of annotations resulting from higher rounds was highly arguable as the objective of the game became mainly surviving through as many rounds of fast-dropping items as possible, rather than making sensible assessments. In the new, limited, setting, the clear objective is to do as well as possible in 5 rounds. Players who want to improve their score beyond this point have to return and start a new game.
A second key observation to be made is the fact that while we evaluate against the performance of NIST assessors and TREC participants, the tasks our workers face is a significantly different one. In the game, no worker gets to see full textual documents or is even told that the objective is to determine topical relevance of web resources towards given information needs. We deliberately aimed for such a loose coupling between game and task as we wanted to keep the game experience entertaining without the "aftertaste" of working. It is interesting that mere conceptual matching correlates well with actual relevance assessments. Also, in the data pre-processing phase, we do not extract sentences based on query terms but rather focus on pure idf figures as described in Section 3.2. In this way, we manage to capture the general gist of a document without artificially biasing it towards the topic.
Finally, the key insight gained from this work was the substantial benefit achieved by offering an alternative incentive to workers. Most of the interesting properties observed in the gamified system, such as workers producing free labels, would not have happened otherwise. This is, however, not necessarily limited to gamified tasks. In this paper we used games as one possible means of showing how a particular incentive (money) can be replaced with another one (entertainment). By doing so, we focus on a certain type of worker, entertainment-seekers, the existence of which we hypothesised based on previous experience with crowdsourcing. We are convinced that a better understanding of worker types and their specific intrinsic motivations is essential in driving the boundaries of current crowdsourcing quality. Kazai et al. [21] proposed an interesting classification of

workers into several categories. In their work, a number of performance-based worker types, including e.g., spammers, sloppy and competent workers are described. We believe, that more general worker models which also encompass aspects such as worker motivation capability and interest in certain HIT types, etc. can be of significant benefit for the field. Very similar to the task of advertisement placement, a worker whose motivations we understand, can be targeted with better-suited precisely-tailored HIT types.
The common example of worker filtering by nationality illustrates the practical need for a better understanding of worker motivation. This practice is not only of dubious ethical value, it may additionally address symptoms rather than causes. The original objective of identifying and rejecting such workers that try to game the task and get paid without actually working is often hard to fulfil. Filtering by nationality is straightforward to achieve, but also (at best) only correlated with reliability. This bears the significant risk of artificially thinning the pool of available workers. This work (e.g., Tables 4 and 6), demonstrates that in an entirely unfiltered environment no significant national differences in quality, cheat rates, etc. could be found when focussing on the desired worker type. In this way, we retain a large work force but, by task design, discourage undesired worker types from taking up our work in the first place.
Looking out towards future changes to the game based on lessons learned in this work, we aim for including yet another incentive besides entertainment. The leaderboard concept of the current game tries to spark competition between players and has a moderate success at doing so. However, the workers do not know each other. In a reputation-aware environment, such as a social network, this effect can be expected to have a far greater impact. Having the ability to compare scores and to compete in a direct multi-player game with their friends will create much more compelling incentives for (a) performing well in each game, (b) continuing to play, (c) returning for subsequent matches and (d) recommending the game to their circle of friends. We believe that exploiting these aspects by integrating social reputation into crowdsourcing will create many interesting applications.
7. CONCLUSION
In this work, we demonstrated the benefits of a gamebased approach for collecting relevance assessments, exploiting insights from the field of serious games for application in commercial large-scale crowdsourcing. After a description of key design criteria, we evaluated the proposed scheme following the setup of the TREC 2011 Crowdsourcing Track. We achieve confirm high result quality, matching the performance levels of the best TREC participants for the same task at a fraction of the invested cost, while attracting fewer cheaters. In a dedicated experiment, we showed the generalizability of the proposed game, that, without any changes, was applied for the task of image classification and clustering. In summary, we are convinced that alternative incentives besides the actual financial HIT reward can positively influence the outcome of crowdsourced data collection and annotation campaigns. As a tangible outcome of this work, a large-scale set of relevance judgements towards the T11Crowd subset of Clueweb was created and is available to the research community. Furthermore, the described game

879

can be accessed and deployed as an open source project4. Future work will further exploit the community aspect to increase the motivation for playing. This could be done by, e.g., introducing a multiplayer mode in which several players are in direct competition or by integrating the game into an identity and reputation-aware environment such as social networks or virtual worlds. More fundamentally, this work has demonstrated the benefit of addressing the specific preferences of entertainment-seeking workers. In the future, however, we should investigate formal worker models of worker motivation and capability to enable an optimal work distribution and representation for arbitrary worker types.
Acknowledgements
We would like to thank Jiyin He and the Fish4Knowledge project for providing us with the fish images and expert judgements.
8. REFERENCES [1] Gambit - Payment Solutions for Virtual Currency. http:// www.getgambit.com/, 2011. [2] O. Alonso and R. Baeza-Yates. Design and implementation of relevance assessments using crowdsourcing. In ECIR, 2011. [3] O. Alonso and M. Lease. Crowdsourcing 101: putting the WSDM of crowds to work for you. In WSDM, 2011. [4] O. Alonso, D.E. Rose, and B. Stewart. Crowdsourcing for relevance evaluation. In ACM SIGIR Forum, 2008. [5] B. Baldwin and B. Carpenter. LingPipe. Available from World Wide Web: http://alias-i. com/lingpipe, 2003. [6] C. Buckley and E.M. Voorhees. Retrieval evaluation with incomplete information. In SIGIR 2004. ACM. [7] J. Callan, M. Hoy, C. Yoo, and L. Zhao. Clueweb09 data set. http: // boston. lti. cs. cmu. edu , 2009. [8] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 268≠275. ACM, 2006. [9] B. Carterette, V. Pavlu, H. Fang, and E. Kanoulas. Million Query track 2009 overview. In Proceedings of TREC, volume 9, 2009. [10] V.R. Carvalho, M. Lease, and E. Yilmaz. Crowdsourcing
for search evaluation. In ACM SIGIR Forum, 2011. [11] CrowdFlower. Crowdsourcing - Labor on demand. http://
crowdflower.com/, 2012. [12] M. Csikszentmihalyi. Flow: The psychology of optimal
experience. Harper Perennial, 1991. [13] C. Eickhoff and A. de Vries. How crowdsourcable is your
task. In WSDM Workshop on Crowdsourcing for Search and Data Mining (CSDM), pages 11≠14, 2011. [14] C. Eickhoff and A.P. de Vries. Increasing Cheat Robustness of Crowdsourcing Tasks. Information Retrieval To appear, 2012. [15] C. Grady and M. Lease. Crowdsourcing document relevance assessment with Mechanical Turk. In NAACL HLT Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk, 2010. [16] C. Harris. You're Hired! An Examination of Crowdsourcing Incentive Models in Human Resource Tasks. In WSDM Workshop on Crowdsourcing for Search and Data Mining (CSDM), pages 15≠18, 2011. [17] M. Hirth, T. Hoﬂfeld, and P. Tran-Gia. Cheat-Detection Mechanisms for Crowdsourcing. University of Wu®rzburg, Tech. Rep, 2010.
4http://sourceforge.net/projects/geann/

[18] P. Ipeirotis. Crowdsourcing using Mechanical Turk: quality management and scalability. In Proceedings of the 8th International Workshop on Information Integration on the Web: in conjunction with WWW 2011. ACM, 2011.
[19] T. Joachims. Optimizing search engines using clickthrough data. In SIGKDD, 2002.
[20] G. Kazai. In search of quality in crowdsourcing for search engine evaluation. ECIR, 2011.
[21] G. Kazai, J. Kamps, and N. Milic-Frayling. Worker Types and Personality Traits in Crowdsourcing Relevance Labels. 2011.
[22] G. Kazai and N. Milic-Frayling. On the evaluation of the quality of relevance assessments collected through crowdsourcing. In SIGIR 2009 Workshop on the Future of IR Evaluation, 2009.
[23] C.H. Ko, J.Y. Yen, C.C. Chen, S.H. Chen, and C.F. Yen. Gender differences and related factors affecting online gaming addiction among taiwanese adolescents. The Journal of nervous and mental disease, 2005.
[24] J. Lampel and A. Bhalla. The role of status seeking in online communities: Giving the gift of experience. Journal of Computer-Mediated Communication, 2007.
[25] M. Lease and G. Kazai. Overview of the TREC 2011 Crowdsourcing Track (Conference Notebook). 2011.
[26] M. Lease and E. Yilmaz. Crowdsourcing for information retrieval. In ACM SIGIR Forum, number 2. ACM, 2012.
[27] H. Ma, R. Chandrasekar, C. Quirk, and A. Gupta. Improving search engines using human computation games. In CIKM 2009.
[28] C.C. Marshall and F.M. Shipman. The ownership and reuse of visual media. JCDL, 2011.
[29] J. McGonigal. Reality is broken: Why games make us better and how they can change the world. Penguin Pr, 2011.
[30] David Pennock. The Wisdom of the Probability Sports Crowd. http://blog.oddhead.com/2007/01/04/ the-wisdom-of-the-probabilitysports-crowd/, 2007.
[31] C. Richards. Teach the world to twitch: An interview with Marc Prensky, CEO and founder Games2train. com. Futurelab, 2003.
[32] F. Scholer, A. Turpin, and M. Sanderson. Quantifying test collection quality based on the consistency of relevance judgements. In SIGIR 2011.
[33] R. Snow, B. O'Connor, D. Jurafsky, and A.Y. Ng. Cheap and fast--but is it good?: evaluating non-expert annotations for natural language tasks. In EMNLP, 2008.
[34] J. Surowiecki, M.P. Silverman, et al. The wisdom of crowds. American Journal of Physics, 2007.
[35] Amazon Mechanical Turk. Artificial Artificial Intelligence. http://mturk.com, 2012.
[36] J. Urbano, M. Marrero, D. Mart¥in, J. Morato, K. Robles, and J. Llor¥ens. The University Carlos III of Madrid at TREC 2011 Crowdsourcing Track: Notebook Paper. 2011.
[37] L. von Ahn and L. Dabbish. Labeling images with a computer game. In SIGCHI, 2004.
[38] L. von Ahn and L. Dabbish. Designing games with a purpose. Communications of the ACM, 2008.
[39] L. von Ahn, S. Ginosar, M. Kedia, and M. Blum. Improving image search with phetch. In ICASSP, 2007.
[40] L. von Ahn, M. Kedia, and M. Blum. Verbosity: a game for collecting common-sense facts. In SIGCHI 2006.
[41] L. von Ahn, R. Liu, and M. Blum. Peekaboom: a game for locating objects in images. In SIGCHI 2006.
[42] E. Voorhees. The philosophy of information retrieval evaluation. In Evaluation of cross-language information retrieval systems, 2002.
[43] E. Voorhees, D.K. Harman, National Institute of Standards, and Technology (US). TREC: Experiment and evaluation in information retrieval. MIT press USA, 2005.
[44] P. Welinder, S. Branson, S. Belongie, and P. Perona. The multidimensional wisdom of crowds. In NIPS, 2010.

880

An IR-based Evaluation Framework for Web Search Query Segmentation

Rishiraj Saha Roy and Niloy Ganguly
Indian Institute of Technology Kharagpur Kharagpur, West Bengal, India - 721302.
{rishiraj, niloy}@cse.iitkgp.ernet.in
ABSTRACT
This paper presents the first evaluation framework for Web search query segmentation based directly on IR performance. In the past, segmentation strategies were mainly validated against manual annotations. Our work shows that the goodness of a segmentation algorithm as judged through evaluation against a handful of human annotated segmentations hardly reflects its effectiveness in an IR-based setup. In fact, state-of the-art algorithms are shown to perform as good as, and sometimes even better than human annotations ≠ a fact masked by previous validations. The proposed framework also provides us an objective understanding of the gap between the present best and the best possible segmentation algorithm. We draw these conclusions based on an extensive evaluation of six segmentation strategies, including three most recent algorithms, vis-`a-vis segmentations from three human annotators. The evaluation framework also gives insights about which segments should be necessarily detected by an algorithm for achieving the best retrieval results. The meticulously constructed dataset used in our experiments has been made public for use by the research community.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query formulation, Retrieval models
General Terms
Measurement, Experimentation, Human Factors
Keywords
Query segmentation, IR evaluation, Evaluation framework, Test collections, Manual annotation
1. INTRODUCTION
Query segmentation is the process of dividing a query into individual semantic units [3]. For example, the query
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08... $15.00.

Monojit Choudhury and Srivatsan Laxman
Microsoft Research India Bangalore, Karnataka, India - 560025.
{monojitc, slaxman}@microsoft.com
singular value decomposition online demo can be broken into singular value decomposition and online demo. All documents containing the individual terms singular, value and decomposition are not necessarily relevant for this query. Rather, one can almost always expect to find the segment singular value decomposition in the relevant documents. In contrast, although online demo is a segment, finding the phrase or some variant of it may not affect the relevance of the document. Hence, the potential of query segmentation goes beyond the detection of multiword named entities. Rather, segmentation leads to a better understanding of the query and is crucial to the search engine for improving Information Retrieval (IR) performance.
There is broad consensus in the literature that query segmentation can lead to better retrieval performance [2, 3, 7, 9, 13]. However, most automatic segmentation techniques [3, 4, 7, 9, 13, 15] have so far been evaluated only against a small set of 500 queries segmented by human annotators. Such an approach implicitly assumes that a segmentation technique that scores better against human annotations will also automatically lead to better IR performance. We challenge this approach on multiple counts. First, there has been no systematic study that establishes the quality of human segmentations in the context of IR performance. Second, grammatical structure in queries is not as well-understood as natural language sentences where human annotations have proved useful for training and testing of various Natural Language Processing (NLP) tools. This leads to considerable inter-annotator disagreement when humans segment search queries. Third, good quality human annotations for segmentation can be difficult and expensive to obtain for a large set of test queries. Thus, there is a need for a more direct IR-based evaluation framework for assessing query segmentation algorithms. This is the central motivation of the present work.
We propose an IR-based evaluation framework for query segmentation that requires only human relevance judgments (RJs) for query-URL pairs for computing the performance of a segmentation algorithm ≠ such relevance judgments are anyway needed for training and testing of any IR engine. A fundamental problem in designing an IR-based evaluation framework for segmentation algorithms is to decouple the effect of segmentation accuracy from the way segmentation is used for IR. This is because a query segmentation algorithm breaks the input query into, typically, a non-overlapping sequence of words (segments), but it does not prescribe how these segments should be used during the retrieval and ranking of the documents for that query. We resolve this problem

881

by providing a formal model of query expansion for a given segmentation; the various queries obtained can then be issued to any standard IR engine, which we assume to be a black box.
We conduct extensive experiments within our framework to understand the performance of several state-of-the-art query segmentation schemes [7, 9, 11] and segmentations by three human annotators. Our experiments reveal several interesting facts such as: (a) Segmentation is actively useful in improving IR performance, even though submitting all segments (detected by an algorithm) in double quotes to the IR engine degrades performance; (b) All segmentation strategies, including human segmentations, are yet to reach the best achievable limits in IR performance; (c) In terms of IR metrics, some of the segmentation algorithms perform as good as the best human annotator and better than the average/worst human annotator; (d) Current match-based metrics for comparing query segmentation against human annotations are only weakly correlated with the IR-based metrics, and cannot be used as a proxy for IR performance; and (e) There is scope for improvement for the matching metrics that compare segmentations against human annotations by differentially penalizing the straddling, splitting and joining of reference segments. In short, the proposed evaluation framework not only provides a formal way to compare segmentation algorithms and estimate their effectiveness in IR, but also helps us to understand the gaps in human annotation-based evaluation. The framework also provides valuable insights regarding the segmentations that can be used for improvement of the algorithms.
The rest of the paper is organized as follows. Sec. 2 introduces our evaluation framework and its design philosophy. Sec. 3 presents the dataset and the segmentation algorithms compared on our framework. Sec. 4 discusses the experimental results and insights derived from them. In Sec. 5, we discuss a few related issues, and the next section (Sec. 6) gives a brief background of past approaches to evaluate query segmentation and their limitations. We conclude by summarizing our contributions and suggesting future work in Sec. 7.
2. THE EVALUATION FRAMEWORK
In this section we present a framework for the evaluation of query segmentation algorithms based on IR performance. Let q denote a search query and let sq = sq1 , . . . , sqn denote a segmentation of q such that a simple concatenation of the n segments equals q, i.e., we have q = (sq1 + ∑ ∑ ∑ + sqn), where + represents the concatenation operator. We are given a segmentation algorithm A and the task is to evaluate its performance. We require the following resources:
1. A test set Q of unquoted search queries.
2. A set U of documents (or URLs) out of which search results will be retrieved.
3. Relevance judgments r(q, u) for query-URL pairs (q, u)  Q ◊ U. The set of all relevance judgments are collectively denoted by R.
4. An IR engine that supports quoted queries as input.
The resources needed by our evaluation framework are essentially the same as those needed for the training and testing of a standard IR engine, namely, queries, a document corpus and set of relevance judgments. Akin to the

Table 1: Example of generation of quoted versions for a segmented query.

Segmented query we are | the people | song lyrics

Quoted versions
we are the people song lyrics we are the people "song lyrics" we are "the people" song lyrics we are "the people" "song lyrics" "we are" the people song lyrics "we are" the people "song lyrics" "we are" "the people" song lyrics "we are" "the people" "song lyrics"

training examples required for an IR engine, we only require relevance judgments for a small and appropriate subset of Q ◊ U (each query needs only the documents in its own pool to be judged) [14].
It is useful to separate the evaluation of segmentation performance, from the question of how to best exploit the segments to retrieve the most relevant documents. From an IR perspective, a natural interpretation of a segment could be that it consists of words that must appear together, in the same order, in documents where the segment is deemed to match [3]. This can be referred to as ordered contiguity matching. While this can be easily enforced in modern IR engines through use of double quotes around segments, we observe that not all segments must be used this way (see [10] for related ideas and experiments in a different context). Some segments may admit more general matching criteria, such as unordered or intruded contiguity (e.g., a segment a b may be allowed to match b a or a c b in the document). The case of unordered intruded matching may be restricted under linguistic dependence assumptions (e.g., a b can match a of b or b in a). Finally, some segments may even play non-matching roles (e.g., when the segment specifies user intent, like how to and where is). Thus, there may be several different ways to exploit the segments discovered by a segmentation algorithm. Even within the same query, different segments may need to be treated differently. For instance, in the query cannot view | word files | windows 7, the first one might be matched using intruded ordered occurrence (cannot properly view), the second segment may be matched under a linguistic dependency model (files in word) and the last one under ordered contiguity.
Intruded contiguity and linguistic dependency may be difficult to implement for the broad class of general Web search queries. Identifying how the various segments of a query should be ideally matched in the document is quite a challenging and unsolved research problem. On the other hand, an exhaustive expansion scheme, where every segment is expanded in every possible way, is computationally expensive and might introduce noise. Moreover, current commercial IR engines do not support any syntax to specify linguistic dependence or intruded or unordered occurrence based matching. Hence, in order to keep the evaluation framework in line with the current IR systems, we focus on ordered contiguity matching which is easily implemented through the use of double quotes around segments. However, we note that the philosophy of the framework does not change with increased sophistication in the retrieval system ≠ only the expansion sets for the queries have to be appropriately modified.
We propose an evaluation framework for segmentation algorithms that generates all possible quoted versions of a

882

segmented query (see Table 1) and submits each quoted version to the IR engine. The corresponding ranked lists of retrieved documents are then assessed against relevance judgments available for the query-URL pairs. The IR quality of the best-performing quoted version is used to measure performance of the segmentation algorithm. We now formally specify our evaluation framework that computes what we call a Quoted Version Retrieval Score (QVRS) for the segmentation algorithm given the test set Q of queries, the document pool U and the relevance judgments R for queryURL pairs.

Quoted query version generation

Let the segmentation output by algorithm A be denoted by

A(q) = sq = sq1 , . . . , sqn . We generate all possible quoted

versions of the query q based on the segments in A(q). In

particular, on any of

we the

define A0(q) = (sq1 + segments, A1(q) =

∑ ∑ ∑ + sqn) (sq1 + ∑ ∑

with no quotes ∑ + "sqn") with

quotes only around the last segment sqn, and so on. Since

there are n segments in A(q), this process will generate 2n

versions of the query, Ai(q), i = 0, . . . , 2n - 1. We note that

if bi = (bi1, . . . , bin) be the n-bit binary representation of i,

then Ai(q) will apply quotes to the jth segment sqj We deduplicate this set, because {Ai(q) : i = 0, .

iff bij . . , 2n

= -

1. 1}

can contain multiple versions that essentially represent the

same quoted query version (when single words are inside

quotes). For example, the query versions "harry potter"

"game" and "harry potter" game are equivalent in terms

of the input semantics of an IR engine. The resulting set of

unique quoted query versions is denoted QA(q).

Document retrieval using IR engine
For each Ai(q)  QA(q) we use the IR engine to retrieve a ranked list Oi of documents out of the document pool U that matched the given quoted query version Ai(q). The number of documents retrieved in each case depends on the IR metrics we will want to use to assess the quality of retrieval. For example, to compute an IR metric at the top k positions, we would require that at least k documents be retrieved from the pool.

Measuring retrieval against relevance judgments

Since we have relevance judgments (R) for query-URL pairs in Q◊U, we can now compute IR metrics such as normalized Discounted Cumulative Gain (nDCG), Mean Average Precision (MAP) or Mean Reciprocal Rank (MRR) to measure the quality of the retrieved ranked list Oi for query q. We use @k variants of each of these measures which are defined to be the usual metrics computed after examining only the top-k positions. For example, we can compute nDCG@k for query q and retrieved document-list Oi using the following formula:

nDCG@k(q, Oi

,

R)

=

r(q, Oi1)

+

k j=2

r(q, Oij ) log2 j

(1)

where Oij, j = 1, . . . , k, denotes the jth document in the ranked-list Oi and r(q, Oij) denotes the associated relevance judgment from R.

Oracle score using best quoted query version
Different quoted query versions Ai(q) (all derived from the same basic segmentation A(q) output by the segmentation

algorithm A) retrieve different ranked lists of documents Oi. As discussed earlier, automatic apriori selection of a good (or
the best) quoted query version is a difficult problem. While
different strategies may be used to select a quoted query
version, we would like our evaluation of the segmentation algorithm A to be agnostic of the version-selection step. To this end, we select the best-performing Ai(q) from the entire set QA(q) of query versions generated and use it to define our oracle score for q and A under the chosen IR metric [8]. For example, the oracle score for nDCG@k is as defined
below:

nDCG@k(q, A) = max nDCG@k(q, Oi , R) (2) Ai (q)QA (q)
where Oi denotes the ranked list of documents retrieved by the IR engine when presented with Ai(q) as the input. We note that QA(q) always contains the original unsegmented version of the query. We refer to such an ∑(∑, ∑) as the Oracle.
This forms the basis of our evaluation framework. We note that there can also be other ways to define this oracle score. For example, instead of seeking the best IR performance possible across the different query versions, we could also seek the minimum performance achievable by A irrespective of what version-selection strategy is adopted. This would give us a lower bound on the performance of the segmentation algorithm. However, the main drawback of this approach is that the minimum performance is almost always achieved by the fully quoted version (where every segment is in double quotes) (see Table 7). Such a lower bound would not be useful in assessing the comparative performance of segmentation algorithms.

QVRS computation

Once the oracle scores are obtained for all queries in the test set Q, we can compute the average oracle score achieved by A. We refer to this as the Quoted Version Retrieval Score (QVRS) of A with respect to test set Q, document pool U and relevance judgments R. For example, using the oracle with the nDCG@k metric, we can define the QVRS score as
follows:

QV RS(Q, A, nDCG@k) =

1 |Q|

nDCG@k(q, A) (3)

qQ

Similar QVRS scores can be computed using other IR metrics such as MAP@k and MRR@k. In our experiments section, we report results using nDCG@k, MAP@k, and MRR@k, for k = 5 and k = 10 as most Web users examine only the first five or ten search results.

3. DATASET AND ALGORITHMS
In this section, we describe the dataset used and briefly introduce the algorithms compared on our framework.
3.1 Test set of queries (Q)
We selected a random subset of 500 queries from a slice of the query logs of Bing Australia1 containing 16.7 million queries issued over a period of one month (May 2010). We used the following criteria to filter the logs before extracting a random sample: (1) Exclude queries with non-ASCII characters, (2) Exclude queries that occurred fewer than 5 times
1 http://www.bing.com/?cc=au

883

in the logs (rarer queries often contained spelling errors), and (3) Restrict query lengths to between five and eight words. Shorter queries rarely contain multiple multiword segments, and when they do, they are mostly named entities that can be easily detected using dictionaries. Moreover, traditional search engines usually give satisfactory results for short queries. On the other hand, queries longer than eight words (only 3.24% of all queries in our log) are usually error messages, complete NL sentences or song lyrics, that need to be addressed separately.
We denote this set of 500 queries by Q, the test set of unsegmented queries needed for all our evaluation experiments. The average length of queries in Q (our dataset) is 5.29 words. The average query length was 4.31 words in the Bergsma and Wang 2007 Corpus2 (henceforth, BWC07) [3]. Each of these 500 queries were independently segmented by three human annotators (who issue around 20-30 search queries per day) who were asked to mark a contiguous chunk of words in a query as a segment if they thought that these words together formed a coherent semantic unit. The annotators were free to refer to other resources and Web search engines during the annotation process, especially for understanding the query and its possible context(s). We shall refer to the three sets of annotations (and also the corresponding annotators) as HA, HB and HC .
It is important to mention that the queries in Q have some amount of word level overlap, even though all the queries have very distinct information needs. Thus, a document retrieved from the pool might exhibit good term level match for more than one query in Q. This makes our corpus an interesting testbed for experimenting with different retrieval systems. There are existing datasets, including BWC07, that could have been used for this study. However, refer to Sec. 5.1 for an account of why building this new dataset was crucial for our research.
3.2 Document pool (U) and RJs (R)
Each query in Q was segmented using all the nine segmentation strategies considered in our study (six algorithms and three humans). For every segmentation, all possible quoted versions were generated (total 4, 746) and then submitted to the Bing API3 and the top ten documents were retrieved. We then deduplicated these URLs to obtain 14, 171 unique URLs, forming U . On an average, adding the 9th strategy to a group of the remaining eight resulted in about one new quoted version for every two queries. These new versions may or may not introduce new documents to the pool. We observed that for 71.4% of the queries there is less than 50% overlap between the top ten URLs retrieved for the different quoted versions. This indicates that different ways of quoting the segments in a query does make a difference in the search results. By varying the pooling depth (ten in our case), one can roughly control the number of relevant and non-relevant documents entering the collection.
For each query-URL pair, where the URL has been retrieved for at least one of the quoted versions of the query (approx. 28 per query), we obtained three independent sets of relevance judgments from human users. These users were different from annotators HA, HB and HC who marked the segmentations, but having similar familiarity with search systems. For each query, the corresponding set of URLs was
2 http://bit.ly/xoyT2c 3 http://msdn.microsoft.com/en- us/library/dd251056.aspx

Table 2: Segmentation algorithms compared on our framework.

Algorithm
Li et al. [9] Hagen et al. [7] Mishra et al. [11]
[11] + Wiki PMI-W [7] PMI-Q [11]

Training data
Click data, Web n-gram probabilities Web n-gram frequencies, Wikipedia titles Query logs Query logs, Wikipedia titles Web n-gram probabilities (used as baseline) Query logs (used as baseline)

shown to the users after deduplication and randomization (to prevent position bias for top results), and asked to mark whether the URL was irrelevant (score = 0), partially relevant (score = 1) or highly relevant (score = 2) to the query. We then computed the average rating for each query-URL pair (the entire set forming R), which has been used for subsequent nDCG, MAP and MRR computations. Please refer to Table 8 in Sec. 5.3 for inter-annotator agreement figures and other related discussions.
3.3 Segmentation algorithms
Table 2 lists the six segmentation algorithms that have been studied in this work. Li et al. [9] use the expectation maximization algorithm to arrive at the most probable segmentation, while Hagen et al. [7] show a simple frequencybased method produces a performance comparable to the state-of-the-art. The technique in Mishra et al. [11] uses only query logs for segmenting queries. In our experiments, we observed that the performance of Mishra et al. [11] can be improved if we used Wikipedia titles. We refer to this as "[11] + Wiki" in our experiments (see Appendix A for details). The Point-wise Mutual Information (PMI)-based algorithms are used as baselines. The thresholds for PMI-W and PMI-Q were chosen to be 8.141 and 0.156 respectively, that maximized the Seg-F (see Sec. 4.2) on our development set.
3.4 Public release of data
The test set of search queries along with their manual and some of the algorithmic segmentations, the theoretical best segmentation output that can serve as an evaluation benchmark (BQVBF in Sec. 4.1), and the list of URLs whose contents serve as our document corpus is available for public use4. The relevance judgments for the query-URL pairs have also been made public which will enable the community to use this dataset for evaluation of any new segmentation algorithm.
4. EXPERIMENTS AND OBSERVATIONS
In this section we present experiments, results and the key inferences made from them.
4.1 IR Experiments
For the retrieval-based evaluation experiments, we use the Lucene5 text retrieval system, which is publicly available as a code library. In its default configuration, Lucene does not perform any automatic query segmentation, which is very important for examining the effectiveness of segmentation algorithms in an IR-based scheme. Double quotes
4 http://cse.iitkgp.ac.in/resgrp/cnerg/qa/querysegmentation.html 5 http://lucene.apache.org/java/docs/index.html

884

Table 3: Results of IR-based evaluation of segmentation algorithms using Lucene (mean oracle scores).

Metric

Unseg. [9] query

[7]

[11] [11] + PMI-W PMI-Q HA HB HC BQVBF

Wiki

nDCG@5 0.688 0.752* 0.763* 0.745 0.767* nDCG@10 0.701 0.756* 0.767* 0.751 0.768*

0.691 0.704

0.766* 0.770 0.768 0.759 0.825 0.767* 0.770 0.768 0.763 0.832

MAP@5 MAP@10

0.882 0.930* 0.942* 0.930* 0.945* 0.865 0.910* 0.921* 0.910* 0.923*

0.884 0.867

0.932* 0.944 0.942 0.936 0.958 0.912* 0.923 0.921 0.916 0.944

MRR@5 MRR@10

0.538 0.632* 0.649* 0.609 0.650* 0.549 0.640* 0.658* 0.619 0.658*

0.543 0.555

0.648* 0.656 0.648 0.632 0.656* 0.665 0.656 0.640

0.711 0.717

The highest value in a row (excluding the BQVBF column) and those with no statistically significant difference with the highest value are marked in boldface. The values for algorithms that perform better than or have no statistically significant difference with the minimum of the
human segmentations are marked with *. The paired t-test was performed and the null hypothesis was rejected if the p-value was less than 0.05.

Table 4: Matching metrics for different segmentation algorithms and human annotations with BQVBF as reference.

Metric Unseg. [9] query

[7]

[11] [11] + PMI-W PMI-Q HA HB HC BQVBF

Wiki

Qry-Acc Seg-Prec Seg-Rec
Seg-F Seg-Acc

0.044 0.226* 0.325* 0.267* 0.470

0.056 0.176* 0.166* 0.171* 0.624

0.082* 0.189* 0.162* 0.174* 0.661*

0.058 0.206* 0.210* 0.208* 0.601

0.094* 0.203* 0.174* 0.187* 0.667*

0.046 0.229* 0.323* 0.268* 0.474

0.104* 0.218* 0.196* 0.206* 0.660*

0.086 0.176 0.144 0.158 0.675

0.074 0.166 0.133 0.148 0.675

0.064 0.178 0.154 0.165 0.663

1.000 1.000 1.000 1.000 1.000

The highest value in a row (excluding the BQVBF column) and those with no statistically significant difference with the highest value are marked in boldface. The values for algorithms that perform better than or have no statistically significant difference with the minimum of the
human segmentations are marked with *. The paired t-test was performed and the null hypothesis was rejected if the p-value was less than 0.05.

can be used in a query to force Lucene to match the quoted phrase (in Lucene terms) exactly in the documents. Starting with the segmentations output by each of the six algorithms as well as the three human annotations, we generated all possible quoted query versions, which resulted in a total of 4, 746 versions for the 500 queries. In the notation of Sec. 2, this corresponds to generating QA(q) for each segmentation method A (including one for each human segmentation) and for every query q  Q. These quoted versions were then passed through Lucene to retrieve documents from the pool. For each segmentation scheme, we then use the oracle described in Sec. 2 to obtain the query version yielding the best result (as determined by the IR metrics ≠ nDCG, MAP and MRR computed according to the human relevance judgments). These oracle scores are then averaged over the query set to give us the QVRS measures.
The results are summarized in Table 3. Different rows represent the different IR metrics that were used and columns correspond to different segmentation strategies. The second column (marked "Unseg. Query") refers to the original unsegmented query. This can be assumed to be generated by a trivial segmentation strategy where each word is always a separate segment. Columns 3-8 denote the six different segmentation algorithms and 9-11 (marked HA, HB and HC ) represent the human segmentations. The last column represents the performance of the best quoted versions (denoted by BQVBF in table) of the queries which are computed by brute force, i.e. an exhaustive search over all possible ways of quoting the parts of a query (2l-1 possible quoted versions for an l-word query) irrespective of any segmentation algorithm. The results are reported for two sizes of retrieved

URL lists (k), namely five and ten. Since we needed to convert our graded relevance judgments to binary values for computing MAP@k, URLs with ratings of 1 and 2 were considered as relevant (responsible for the generally high values) and those with 0 as irrelevant. For MRR, only URLs with ratings of 2 were considered as relevant.
The first observation we make from the results is that human as well as all algorithmic segmentation schemes consistently outperform unsegmented queries for all IR metrics. Second, we observe that the performance of some segmentation algorithms are comparable and sometime even marginally better than some of the human annotators. Finally, we observe that there is considerable scope for improving IR performance through better segmentation (all values less than BQVBF ). The inferences from these observations are stated later in this section.
4.2 Performance under traditional matching metrics
In the next set of experiments we study the utility of traditional matching metrics that are used to evaluate query segmentation algorithms against a gold standard of human segmented queries (henceforth referred to as the reference segmentation). These metrics are listed below [7]:
1. Query accuracy (Qry-Acc): The fraction of queries where the output matches exactly with the reference segmentation.
2. Segment precision (Seg-Prec): The ratio of the number of segments that overlap in the output and reference segmentations to the number of output segments, averaged across all queries in the test set.

885

Table 5: Performance of PMI-Q and [9] with respect to matching (mean of comparisons with HA, HB and HC as references) and IR metrics.

Metric
PMI-Q [9]

nDCG@10
0.767 0.756

MAP@10 MRR@10 Qry-Acc Seg-Prec Seg-Rec

0.912 0.910

0.656 0.640

0.341 0.375

0.448 0.524

0.487 0.588

The highest values in a column are marked in boldface.

Seg-F
0.467 0.554

Seg-Acc
0.810 0.810

3. Segment recall (Seg-Rec): The ratio of the number of segments that overlap in the output and reference segmentations to the number of reference segments, averaged across all queries in the test set.
4. Segment F-score (Seg-F ): The harmonic mean of Seg-Prec and Seg-Rec.
5. Segmentation accuracy (Seg-Acc): The ratio of correctly predicted boundaries and non-boundaries in the output segmentation with respect to the reference, averaged across all queries in the test set.
We computed the matching metrics for various segmentation algorithms against HA, HB and HC . According to these metrics, "Mishra et al. [11] + Wiki" turns out to be the best algorithm which agrees with the results of IR evaluation. However, the average Kendall-Tau rank correlation coefficient6 between the ranks of the strategies as obtained from the IR metrics (Table 3) and the matching metrics was only 0.75. This indicates that matching metrics are not perfect predictors for IR performance. In fact, we discovered some costly flaws in the relative ranking produced by matching metrics. One such case was rank inversions between Li et al. [9] and PMI-Q. The relevant results are shown in Table 5, which demonstrate that while PMI-Q consistently performs better than Li et al. [9] under IR-based measures, the opposite inference would have been drawn if we had used any of the matching metrics.
In Bergsma and Wang [3], human annotators were asked to segment queries such that segments matched exactly in the relevant documents. This essentially corresponds to determining the best quoted versions for the query. Thus, it would be interesting to study how traditional matching metrics would perform if the humans actually marked the best quoted versions. In order to evaluate this, we used the matching metrics to compare the segmentation outputs by the algorithms and human annotations against BQVBF . The corresponding results are quoted in Table 4. The results show that matching metrics are very poor indicators of IR performance with respect to the BQVBF . For example, for three out of the five matching metrics, the unsegmented query is ranked the best. This shows that even if human annotators managed to correctly guess the best quoted versions, the matching metrics would fail to estimate the correct relative rankings of the segmentation algorithms with respect to IR performance. This fact is also borne out in the Kendall-Tau rank correlation coefficients reported in Table 6. Another interesting observation from these experiments is that Seg-Acc emerges as the best matching metric with respect to IR performance, although its correlation coefficient is still much below one.
6This coefficient is 1 when there is perfect concordance between the rankings, and -1 if the trends are reversed.

Table 6: Kendall-Tau coefficients between IR and matching metrics with BQVBF as reference for the latter.

Metric Qry-Acc Seg-Prec Seg-Rec Seg-F Seg-Acc

nDCG@10 MAP@10 MRR@10

0.432 0.322 0.395

-0.854 -0.887 -0.782

-0.886 -0.920 -0.814

-0.854 -0.887 -0.782

0.674 0.750 0.598

The highest value in a row is marked in boldface.

4.3 Inferences
Segmentation is helpful for IR. By definition, ∑(∑, ∑) (i.e., the oracle) values for every IR metric for any segmentation scheme are at least as large as the corresponding values for the unsegmented query. Nevertheless, for every IR metrics, we observe significant performance benefits for all the human and algorithmic segmentations (except for PMI-W) over the unsegmented query. This indicates that segmentation is indeed helpful for boosting IR performance. Thus, our results validate the prevailing notion and some of the earlier observations [2, 9] that segmentation can help improve IR.
Human segmentations are a good proxy, but not a true gold standard. Our results indicate that human segmentations perform reasonably well in IR metrics. The best of the human annotators beats all the segmentation algorithms, on almost all the metrics. Therefore, evaluation against human annotations can indeed be considered as the second best alternative to an IR-based evaluation (though see below for criticisms of current matching metrics). However, if the objective is to improve IR performance, then human annotations cannot be considered a true gold standard. There are at least three reasons for this:
First, in terms of IR metrics, some of the state-of-the-art segmentation algorithms are performing as well as human segmentations (no statistically significant difference). Thus, further optimization of the matching metrics against human annotations is not going to improve the IR performance of the segmentation algorithms. Thus, evaluation on human annotations might become a limiting factor for the current segmentation algorithms.
Second, the IR performance of the best quoted version of the queries derived through our framework is significantly better than that of human annotations (last column, Table 3). This means that humans fail to predict the correct boundaries in many instances. Thus, there is scope for improvement for human annotations.
Third, IR performance of at least one of the three human annotators (HC ) is worse than some of the algorithms studied. In other words, while some annotators (such as HA) are good at guessing the "correct" segment boundaries that will help IR, not all annotators can do it well. Therefore, unless

886

Figure 1: Distribution of multiword segments in queries across segmentation strategies.
the annotators are chosen and guided properly, one cannot guarantee the quality of annotated data for query segmentation. If the queries in the test set have multiple intents, this issue becomes an even bigger concern.
Matching metrics are misleading. As discussed earlier and demonstrated by Tables 4 and 6, the matching metrics provide unreliable ranking of the segmentation algorithms even when applied against a true gold standard, BQVBF , that by definition maximizes IR performance. This counter-intuitive observation can be explained in two ways. Either the matching metrics or the IR metrics (or probably both) are misleading. Given that IR metrics are well-tested and generally assumed to be acceptable, we are forced to conclude that the matching metrics do not really reflect the quality of a segmentation with respect to a gold standard. Indeed, this can be illustrated by a simple example.
Example. Let us consider the query the looney toons show cartoon network, whose best quoted version turns out to be "the looney toons show" "cartoon network". The underlying segmentation that can give rise to this and therefore can be assumed to be the reference is: Ref: the looney toons show | cartoon network The segmentations (1) the looney | toons show | cartoon | network (2) the | looney | toons show cartoon | network are equally bad if one considers the matching metrics of QryAcc, Seg-Prec, Seg-Rec and Seg-F (all values being zero) with respect to the reference segmentation. Seg-Acc values for the two segmentations are 3/5 and 1/5 respectively. However, the BQV for (1) ("the looney" "toons show" cartoon network) fetches better pages than the BQV of (2) (the looney toons show cartoon network). So the segmentation (2) provides no IR benefit over the unsegmented query and hence performs worse than (1) on IR metrics. However, the matching metrics, except for Seg-Acc to some extent, fail to capture this difference between the segmentations.
Distribution of multiword segments across queries gives insights about effectiveness of strategy. The limitation of the matching metrics can also be understood from the following analysis of the multiword segments in the queries. Fig. 1 shows the distribution of queries having a specific number of multiword segments (for example, 1 in the legend indicates the proportion of queries having one multiword segment) when segmented according to the various

strategies. We note that for Hagen et al. [7], HB, HA and "Mishra et al. [11] + Wiki", almost all of the queries have two multiword segments. For HC , Li et al. [9], PMI-Q and Mishra et al. [11], the proportion of queries that have only one multiword segment increases. Finally, PMI-W has almost negligible queries with a multiword segment. BQVBF is different from all of them and has a majority of queries with one multiword segment. Now given that the first group generally does the best in IR, followed by the second, we can say that out of the two multiword segments marked by these strategies, only one needs to be quoted. PMI-W as well as unsegmented queries are bad because these schemes cannot detect the one crucial multiword segment quoting which improves the performance. Nevertheless, these schemes do well for matching metrics against BQVBF because both have a large number of single word segments. Clearly this is not helpful for IR. Finally, Mishra et al. [11] performs poorly despite being able to identify a multiword segment in most of the cases because it is not identifying the one that is important for IR.
Hence, the matching metrics are misleading due to two reasons. First, they do not take into account that splitting a useful segment (i.e., a segment which should be quoted to improve IR performance) is less harmful than joining two unrelated segments. Second, matching metrics are, by definition, agnostic to which segments are useful for IR. Therefore, they might unnecessarily penalize a segmentation for not agreeing on the segments which should not be quoted, but are present in the reference human segmentation. While the latter is an inherent problem with any evaluation against manually segmented datasets, the former can be resolved by introducing a new matching metric that differentially penalizes splitting and joining of segments. This is an important and interesting research problem that we would like to address in the future. However, we would like to emphasize here that with the IR system expected to grow in complexity in the future (supporting more flexible matching criteria), the need for an IR-based evaluation like ours' becomes imperative.
Based on our new evaluation framework and corresponding experiments, we observe that "Mishra et al. [11] + Wiki" has the best performance. Nevertheless, the algorithms are trained and tested on different datasets, and therefore, a comparison amongst the algorithms might not be entirely fair. This is not a drawback of the framework and can be circumvented by appropriately tuning all the algorithms on similar datasets. However, the objective of the current work is not to compare segmentation algorithms; rather, it is to introduce the evaluation framework, gain insights from the experiments and highlight the drawbacks of human segmentation-based evaluation.
5. RELATED ISSUES
In this section, we will briefly discuss a few related issues that are essential for understanding certain design choices and decisions made during the course of this research.
5.1 Motivation for a new dataset
TREC data has been a popular choice for conducting IRbased experiments throughout the past decade. Since there is no track specifically geared towards query segmentation, the queries and qrels (query-relevance sets) from the ad hoc retrieval task for the Web Track would seem the most rele-

887

Table 7: IR-based evaluation using Bing API.

Metric

Unseg. All quoted for Oracle for query [11] + Wiki [11] + Wiki

nDCG@10 MAP@10 MRR@10

0.882 0.366 0.541

0.823 0.352 0.515

0.989* 0.410* 0.572*

The highest value in a row is marked bold. Statistically significant (p < 0.05 for paired t-test) improvement over the unsegmented query is marked with *.

vant to our work. However, 74% of the 50 queries in the 2010 Web track ad hoc task had less than three words. Also, when these 50 queries were segmented using the six algorithms, half of the queries did not have a multiword segment. As discussed earlier, query segmentation is useful but not necessarily for all types of queries. The benefit of segmentation may be observed only when there are multiple multiword segments in the queries. The TREC Million Query Track, last held in 2009, has a much larger set of 40, 000 queries, with a better coverage of longer queries. But since the goal of the track is to test the hypothesis that a test collection built from several incompletely judged topics is a better tool than a collection built using traditional TREC pooling, there are only about 35, 000 query-document relevance judgments for the 40, 000 queries. Such a sparse qrels is not suitable here ≠ incomplete assessments, especially for documents near the top ranks, could cause crucial errors in system comparisons. Yet another option could have been to use BWC07 as Qand create the corresponding U and R. However, this query set is known to suffer from several drawbacks [7]. A new dataset for query segmentation7 containing manual segment markups collected through crowdsourcing has been recently made publicly available (after we had completed construction of our set) by Hagen et al. [7], but it lacks query-document relevance judgments. These factors motivated us to create a new dataset suitable for our framework, which has been made publicly available (see Sec. 3.4).
5.2 Retrieval using Bing
Bing is a large-scale commercial Web search engine that provides an API service. Instead of Lucene, which is too simplistic, we could have used Bing as the IR engine in our framework. However, such a choice suffers from two drawbacks. First, Bing might already be segmenting the query with its own algorithm as a preprocessing step. Second, there is a serious replicability issue. The document pool that Bing uses, i.e. the Web, changes dynamically with documents added and removed from the pool on a regular basis. This makes it difficult to publish a static gold standard dataset with relevance judgments for all appropriate queryURL pairs that the Bing API may retrieve even for the same set of queries. In view of this, the main results were reported in this paper using the Lucene text retrieval system.
However, since we used Bing API to construct U and corresponding R, we have the evaluation statistics using the Bing API as well. For paucity of space, in Table 7 we only present the results for nDCG@10, MRR@10 and MAP@10 for "Mishra et al. [11] + Wiki". The table reports results for three quoted version-selection strategies: (i) Unsegmented query only (equivalent to each word being within quotes) (ii)
7 http://bit.ly/xIhSur

Table 8: Inter-annotator agreement on features as observed from our experiments.

Feature Pair 1 Pair 2 Pair 3 Mean

Qry-Acc Seg-Prec Seg-Rec
Seg-F Seg-Acc

0.728 0.750 0.756 0.753 0.911

0.644 0.732 0.775 0.753 0.914

0.534 0.632 0.671 0.651 0.872

0.635 0.705 0.734 0.719 0.899

Rel. judg. 0.962 0.959 0.969 0.963
For relevance judgments, only pairs of (0, 2) and (2, 0) were considered disagreements.

All segments quoted and (iii) QVRS (oracle for "Mishra et al. [11] + Wiki"). For all the three metrics, QVRS is statistically significantly higher than results for the unsegmented query. Thus, segmentation can play an important role towards improving IR performance of the search engine. We note that the strategy of quoting all the segments is, in fact, detrimental to IR performance. This emphasizes the point that how the segments should be matched in the documents is a very important research challenge. Instead of quoting all the segments, our proposal here is to assume an oracle that will suggest which segments to quote and which are to be left unquoted for the best IR performance. Philosophically, this is a major departure from the previous ideas of using quoted segments, because re-issuing a query by quoting all the segments implies segmentation as a way to generate a fully quoted version of the query (all segments in double quotes). This definition severely limits the scope of segmentation, which ideally should be thought of as a step forward better query understanding.
5.3 Inter-annotator agreement
Inter-annotator agreement (IAA) is an important indicator for reliability of manually created data. Table 8 reports the pairwise IAA statistics for HA, HB and HC . Since there are no universally accepted metrics for IAA, we report the values of the five matching metrics when one of the annotations (say HA) is assumed to be the reference and the remaining pair (HB and HC ) is evaluated against it (average reported). As is evident from the table, the values of all the metrics, except for Seg-Acc, is less than 0.78 (similar values reported in [13]), which indicates a rather low IAA. The value for Seg-Acc is close to 0.9, which to the contrary, indicates reasonably high IAA (as in [13]). The last row of Table 8 reports the IAA for the three sets of relevance judgments (therefore, the actual pairs for this column are different from that of the other rows). The agreement in this case is quite high.
There might be several reasons for low IAA for segmentation, such as lack of proper guidelines and/or an inherent inability of human annotators to mark the correct segments of a query. Low IAA raises serious doubts about the reliability of human annotations for query segmentation. On the other hand, high IAA for relevance judgments naturally makes these annotations much more reliable for any evaluation, and strengthens the case for our IR-based evaluation framework which only relies on relevance judgments. We note that ideally, relevance judgments should be obtained from the user who has issued the query. This has been re-

888

ferred to as gold annotations, as opposed to silver or bronze annotations which are obtained from expert and non-expert annotators respectively who have not issued the query [1]. Gold annotations are preferable over silver or bronze ones due to relatively higher IAA. Our annotations are silver standard, though very high IAA essentially indicates that they might be as reliable as gold standard. The high IAA might be due to the unambiguous nature of the queries.
6. RELATED WORK
Since its inception in 2003 [12], many algorithms have been proposed for automatic segmentation of Web queries. The approaches vary from purely supervised [3] to fully unsupervised [7, 11] machine learning techniques. They differ widely in terms of resources usage (Table 2) and the underlying algorithmic techniques (e.g., expectation maximization [13] and eigenspace similarity [15]).
6.1 Evaluation on manual annotations
Despite the diversity in approaches to the task, till date there has been only one standard approach for evaluation of query segmentation algorithms, which is to compare the machine output against a set of queries segmented by humans [3, 4, 7, 9, 11, 13, 15]. The basic assumption underlying this evaluation scheme is that humans are capable of segmenting a query in a "correct" or "the best possible" way, which, if exploited appropriately, will result in maximum benefits in IR performance. This is probably motivated by the extensive use of human judgments and annotations as the gold standard in the field of NLP (e.g., parts-ofspeech labeling, phrase boundary identification, etc.). However, this idea has several shortcomings, as pointed out in Sec. 4.3. Among those who validate query segmentation against human-labeled data, most [3, 4, 6, 7, 9, 13, 15] report accuracies on BWC07 [3]. The popularity of the BWC07 dataset is partly because it was one of the first human annotated datasets created for query segmentation, and partly because it is the only publicly available dataset of its kind. While BWC07 has provided a common benchmark for comparing various query segmentation algorithms, there are several limitations of this specific dataset. BWC07 only contains noun phrase queries and there is a non-trivial amount of noise in the annotations. See [7] for a detailed criticism of this dataset.
6.2 IR-based evaluation
There has been only a handful of studies that explore some initial ideas about IR-based evaluation [2, 7, 9] for query segmentation. Bendersky et al. [2] were the first to study the effects of segmentation from an IR perspective. They wanted to see if retrieval quality could be improved by incorporating knowledge of query chunks into an MRF-based retrieval system [10]. Their experiments on different TREC collections using popular IR metrics like MAP indicate that query segmentation can indeed boost IR performance. Li et al. [9] examined the usefulness of query segmentation when built into language models for retrieval, in a Web search setting. However, none of these studies propose an objective IR-based evaluation framework for query segmentation. Their scope is limited to the demonstration of one particular strategy for exploiting segmentations for improving IR, instead of evaluating and comparing a set of algorithms.

As an excursus to their main work, Hagen et al. [7] examined if submitting fully quoted queries (generated from algorithm outputs) results in fetching better pages by the search engines. They study the top fifty retrieved documents when the following versions of the queries ≠ unsegmented, manually quoted, quoted by the technique in Bergsma and Wang [3], and by their own method ≠ are submitted to Bing. Assuming the pages retrieved by manual quotation as relevant, it was observed that the technique in Bergsma and Wang [3] achieves the highest average recall. However, the authors also state that such an assumption need not hold good in reality and emphasized the need for an in-depth retrieval-based evaluation.
We would like to emphasize here that the aim of a segmentation technique is not to come up with the best quoted version of a query. While some past works have explicitly or implicitly assumed this definition, there are also other works that view segmentation as a purely structural analysis of a query that identifies chunks or sequences of words that are semantically connected as a unit [9, 11]. By quoting all the segments we would be penalizing the latter philosophy of segmentation, which is a more productive and practically useful view.
There have been a few studies on detection of noun phrases from queries [5, 16]. This task is similar to query segmentation in the sense that the phrase can be considered as a single unit in the query. Zhang et al. [16] has shown that such phrase detection schemes can actually help in retrieval, and therefore, is along the lines of the philosophy of the present evaluation framework. Nevertheless, as far as we know, this is the first time that a formal conceptual framework for an IR-based evaluation of query segmentation has been proposed. Our study, also for the first time, compares the effectiveness of human segmentation and related matching metrics to an IR-based evaluation.
7. CONCLUSIONS AND FUTURE WORK
End-user of query segmentation is the retrieval engine; hence, it is essential that any segmentation algorithm should be evaluated in an IR-based framework. In this research, we overcome several conceptual challenges to design and implement the first such scheme of evaluation for query segmentation. Using a carefully selected query test set and a group of segmentation strategies, we show that it is possible to have a fair comparison of the relative goodness of each strategy as measured by standard IR metrics. The proposed framework uses resources which are essential for any IR system evaluation, and hence does not require any special input. Our entire dataset ≠ complete with queries, segmentation outputs and relevance judgments ≠ has also been made publicly available to facilitate further research by the community.
Moreover, we gain several useful and non-intuitive insights from the evaluation experiments. Most importantly, we show that human notions of query segments may not be the best for maximizing retrieval performance, and treating them as the gold standard limits the scope for improvement for an algorithm. Also, the matching metrics extensively used till date for comparing against gold standard segmentations can often be misleading. We would like to emphasize that in the future, the focus of IR will mostly shift to tail queries. In such a scenario, an IR-based evaluation scheme gains relevance because validation against a fixed set of gold

889

standard segmentation may often lead to overfitting of the algorithms without yielding any real benefit.
A hypothetical oracle has been shown to be quite useful, but we realize that it will be a much bigger contribution to the community if we could implement a context-aware oracle that can actually tell the search engine which version of a segmented query should be chosen at runtime.
8. ACKNOWLEDGMENTS
We would like to thank Bo-June (Paul) Hsu and Kuansan Wang (Microsoft Research, Redmond), for providing us with the code for Li et al. [9]. We also thank Matthias Hagen (Bauhaus Universit®at Weimar), for providing us with the segmentation output of Hagen et al. [7] on our test set at a very short notice. The first author was supported by Microsoft Corporation and Microsoft Research India under the Microsoft Research India PhD Fellowship Award.
9. REFERENCES
[1] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P. de Vries, and E. Yilmaz. Relevance assessment: are judges exchangeable and does it matter. In SIGIR '08, pages 667≠674. ACM, 2008.
[2] M. Bendersky, W. B. Croft, and D. A. Smith. Two-stage query segmentation for information retrieval. In SIGIR '09, pages 810≠811. ACM, 2009.
[3] S. Bergsma and Q. I. Wang. Learning noun phrase query segmentation. In EMNLP-CoNLL'07, pages 819≠826, 2007.
[4] D. J. Brenes, D. Gayo-Avello, and R. Garcia. On the fly query segmentation using snippets. In CERI '10, pages 259≠266, 2010.
[5] A. L. da Costa Carvalho, E. S. de Moura, and P. Calado. Using statistical features to find phrasal terms in text collections. JIDM, 1(3):583≠597, 2010.
[6] M. Hagen, M. Potthast, B. Stein, and C. Br®autigam. The power of naive query segmentation. In SIGIR '10, pages 797≠798. ACM, 2010.
[7] M. Hagen, M. Potthast, B. Stein, and C. Br®autigam. Query segmentation revisited. In WWW '11, pages 97≠106, 2011.
[8] M. Lease, J. Allan, and W. B. Croft. Regression rank: Learning to meet the opportunity of descriptive queries. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR '09, pages 90≠101, Berlin, Heidelberg, 2009. Springer-Verlag.
[9] Y. Li, B.-J. P. Hsu, C. Zhai, and K. Wang. Unsupervised query segmentation using clickthrough for information retrieval. In SIGIR '11, pages 285≠294. ACM, 2011.
[10] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In SIGIR'05, pages 472≠479, 2005.
[11] N. Mishra, R. Saha Roy, N. Ganguly, S. Laxman, and M. Choudhury. Unsupervised query segmentation using only query logs. In WWW '11, pages 91≠92. ACM, 2011.
[12] K. M. Risvik, T. Mikolajewski, and P. Boros. Query segmentation for web search. In WWW (Posters), 2003.

[13] B. Tan and F. Peng. Unsupervised query segmentation using generative language models and wikipedia. In WWW '08, pages 347≠356. ACM, 2008.
[14] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Inf. Process. Manage., 36:697≠716, September 2000.
[15] C. Zhang, N. Sun, X. Hu, T. Huang, and T.-S. Chua. Query segmentation based on eigenspace similarity. In ACL/AFNLP (Short Papers)'09, pages 185≠188, 2009.
[16] W. Zhang, S. Liu, C. Yu, C. Sun, F. Liu, and W. Meng. Recognition and classification of noun phrases in queries for effective retrieval. In CIKM '07, pages 711≠720. ACM, 2007.
APPENDIX A: WIKI-BOOST
Algorithm 1 Wiki-Boost(Q , W )
1: W   2: for all w  W do 3: w  Seg-P hase-1(w) 4: W  W  w 5: end for 6: W -scores   7: for all w  W do 8: w -score  P M I(w ) based on Q 9: W -scores  W -scores  w -score 10: end for 11: U -scores   12: for all unique unigrams u  Q do 13: u-score  probability(u) in Q 14: U -scores  U -scores  u-score 15: end for 16: W -scores  W -scores  U -scores 17: return W -scores
In this appendix, we explain how to augment the output of an n-gram score aggregation based segmentation algorithm with Wikipedia titles8. Input to Wiki-Boost is a list of queries Q already segmented by the algorithm in Mishra et al. [11] (or any algorithm that meets the above criterion) (say, Seg-Phase-1) and W , the list of all stemmed Wikipedia titles (4, 508, 386 entries after removing one-word entries and those with non-ASCII characters). We compute the PMIscore of an n-segment Wikipedia title w (segmented by SegPhase-1) by taking the higher of the PMI scores of the first (n -1) segments with the last segment and the first segment and the last (n-1) segments. The frequencies of all n-grams are computed from Q . Scores for unigrams are defined to be their probabilities of occurrence. Thus, the output of the Wiki-Boost is a list of PMI-scores for each Wikipedia title in W .
Following this, we use a second segmentation strategy (say, Seg-Phase-2) that takes as input q (the query q segmented by Seg-Phase-1) and tries to further join the segments of q such that the product of scores of the candidate output segments, computed based on the output of WikiBoost, is maximized. A dynamic programming approach is found to be helpful in searching over all possible segmentations in Seg-Phase-2. The output of Seg-Phase-2 is the final segmentation output.
8http://dumps.wikimedia.org/enwiki/latest/, accessed April 6, 2011

890

On Per-topic Variance in IR Evaluation

Stephen E. Robertson
Microsoft Research 7 JJ Thomson Avenue Cambridge CB3 0FB, UK
stephenerobertson@hotmail.co.uk

Evangelos Kanoulas
Information School University of Sheffield
Sheffield, UK
ekanoulas@gmail.com

ABSTRACT
We explore the notion, put forward by Cormack & Lynam and Robertson, that we should consider a document collection used for Cranfield-style experiments as a sample from some larger population of documents. In this view, any pertopic metric (such as average precision) should be regarded as an estimate of that metric's true value for that topic in the full population, and therefore as carrying its own per-topic variance or estimate precision or noise. As in the two mentioned papers, we explore this notion by simulating other samples from the same large population. We investigate different ways of performing this simulation. One use of this analysis is to refine the notion of statistical significance of a difference between two systems (in most such analyses, each per-topic measurement is treated as equally precise). We propose a mixed-effects model method to measure significance, and compare it experimentally with the traditional t-test.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]
General Terms: Experimentation, Measurement, Performance
Keywords: information retrieval, evaluation, statistical precision, significance testing, mixed-effects model, simulation
1. INTRODUCTION
In two recent papers, by Cormack and Lynam [3] and Robertson [9], it has been suggested that in Cranfield-style test-collection-based IR evaluations, we should in general consider the document collection used as a sample from a population (in addition to considering the set of topics or queries thus). Such a view suggests (among other things) that each per-topic measurement (such as of Average Precision) should be considered as carrying its own statistical precision, or noise, and that this noise might be different for every topic. This potentially complicates the issue of establishing statistical significance in IR tests. Looked at in a
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

more global way, the set of measurements that we take in a test, averaged over a set of topics, are the result not of a simple sampling of queries/topics, but of the interaction between a sampling of queries/topics and a separate sampling of documents. In [3] this question was investigated in an empirical way, via bootstrap experiments on experimental results; [9] was concerned with more theoretical issues involving the nature of IR evaluation metrics, and appealed only to very limited simulation experiments based on simple models. In some sense the present paper takes a position between these two, with experiments based on a variety of models starting from bootstrapping from empirical results.
The primary question being investigated in this paper is the question of how we should think about the reliability of individual per-topic measurements. The question is tricky, and the paper does not arrive at any firm conclusions. However, it is hoped to shed some light on the question. Specifically, we consider tests of statistical significance for differences between systems. An approach that can make use of per-topic noise is the mixed-effects models. We propose and investigate a linear mixed-effects models approach to significance.
2. BACKGROUND
2.1 Cormack & Lynam
In this paper [3], a TREC collection of documents was split randomly into two parts. For each topic, the evaluation result (in the form of the value of a metric M such as average precision, AP ) in first half of the collection was taken as predicting the value of M in the second half. In order to estimate the variance (or statistical precision) associated with this prediction, the authors took repeated bootstrap samples from the first half of the collection, and re-measured M on each sample. The distribution of M values over samples (single topic, single original set of results) provides an indication of the statistical precision required. They concluded that this procedure does indeed adequately model the variance inherent in an AP estimate for an individual topic in a document collection of a certain size, regarded as a sample from an infinite collection of potential documents (so that the split collection is seen as two independent samples from this population).
Certain technical issues arise: first, it is better to use M = logit(AP ) rather than M = AP , as it has better distributional properties; second, in the bootstrap sampling it is necessary to treat very small values of R, the total number

891

of relevant documents in the observed collection, in special ways. These are discussed further below.
2.2 Robertson
In this paper [9], different metrics M are investigated theoretically, with a view to establishing whether it makes sense to pose the infinite-document-population question in terms of each metric. Some metrics turn out not to be understandable in these terms; a metric as defined on a given finite population may not be generalisable to an infinite population. In particular, for example, there appears to be no equivalent to NDCG defined on an infinite population. AP , however, can be so generalised.
Even if it is generalisable, we should ask the question: is the metric value observed on the given finite population a good estimate of its `true' value in the infinite population? Some limited simulation experiments, using some previously proposed models of score distributions for relevant and nonrelevant documents, are made in order to investigate this question. It appears from both theoretical arguments and these simulation experiments that some such metrics may provide biased estimates of their true population values. AP seems to be one of these: AP observed on a given finite document population is likely to be an over-estimate of `true' AP on the infinite population of which the observed finite population is a sample. The reason postulated for the bias in AP is as follows. The infinite population equivalent is defined as a particular form of integral over (area under) the infinite-population recall-precision curve, which we expect to be smooth. However, in estimating it for an observed sample, we choose to measure the height of the recall-precision curve only at the points in the ranking where relevant documents occur, i.e. exactly at the peaks of the noisy sample curve.
2.3 Topics and documents
The usual approach to system evaluation in IR experiments is to choose a metric defined on the results of searching on an individual topic or query, and take an average of this metric over the set of topics, for the system concerned (there are many such metrics in common use, including AP and NDCG). The usual approach to statistical significance is to consider the results for the individual topics as the (indivisible) units of measurement. So suppose, for example, that the measure is RPrec and the significance test is the sign test, the fact that system A performs better than system B by RPrec on topic 1 counts as a definitive result. The significance question arises only at the aggregation-overtopics level (`does system A perform better than system B on significantly more topics?'). We do not ask the question as to whether or in what sense the individual topic 1 result was significant. This approach effectively assumes that the topics were sampled from some population of (possible or actual) topics, but that the document collection is fixed in stone for all time.
Stated like this, it must be clear that this (the document collection part) is a rather drastic assumption. In fact it is in part mitigated by the general view in IR research that a result is only good if it works on multiple test collections. But this is a rather crude approach to the problem. It would be good to have a much better understanding of what each individual topic result is telling us, and of how best to draw general conclusions from the topic sample ◊ document sam-

ple results that we have. A start in exploring these issues is made in the two papers cited above; the present paper attempts to take this analysis a step further.
A basic premise of the approach taken in the two cited papers is that the population of documents looks different from the point of view of each separate topic ≠ in particular, with regard to relevance. The simulations in both papers are based on modelling the document population separately for each topic. An alternative approach would be to model the population of documents independently of the topics, and then model the interaction of each topic with that population. Such an approach would be interesting indeed but very hard. It would also have to model the interaction of each system with the population, separately applied to each topic. The big advantage of the per-topic model of the population is that both relevance and system responses (e.g. scores) can be taken as given. Furthermore, it serves to emphasise an important aspect of the document sample view of IR experiments. While the entire document collection is typically very large (from which one might be tempted to conclude that statistical precision issues do not arise), from the point of view of each topic we see a (typically much smaller) sets of relevant or retrieved documents, on which all the measures depend. Precision at cutoff 5, for example, should be seen as a very imprecise measurement (in the statistical sense) for a single topic.
2.4 Variation between topics
It is well understood in the IR research community that there is a great deal of variation between topics. For example, the analysis of variance in the paper by Banks et al. [1] finds the topic effect to be greater than the system effect (as well as a significant interaction effect). The Robust track at TREC was devoted to investigating hard topics, in the recognition that there are significant numbers of topics that appear to be generally hard for systems to perform well on.
If it is accepted that the document collection should be regarded as sampled in some way from a population, then as indicated above, each per-topic measurement has some builtin error of estimation because the measurement is based on the sample rather than the population. One of the aims of this paper is to get a handle on this estimation error. In particular, we ask the question: does this estimation error vary substantially between topics? If this variation is substantial, should this affect how we analyse test results?
2.5 Scores and simulations
The method of simulation used in [3] is based on bootstrap ideas. We start with a given system and topic and the corresponding set of results (documents retrieved and ranked) from a real test collection of documents. Then we model the process of drawing other samples from the notional infinite population of documents by repeated sampling with replacement from these actual documents. The method used in [9] is a parametric approach based on score distributions. On the back of previous work on fitting distributions to the scores of relevant and non-relevant documents respectively, these distributions are then taken as representing the notional population of documents. We can repeatedly sample from the distributions rather than from any specific documents.
It is clear that the score-distribution approach implies a smoothing effect which the bootstrap method does not have.

892

Modelling the population as consisting only of instances exactly like the ones we have already seen seems to go against at least part of the basic idea of samples and populations ≠ that any sample we take consists only of examples. We should in some sense expect the population to include other kinds of examples, not represented directly in the sample we have. On the other hand, the use of score distributions typically involves strong assumptions about the nature of the distributions, and also involves some dependence on the particular scoring methods used by systems. Systems differ greatly not only in the kinds of scores they produce, but also in whether they produce a consistent set of scores at all. For example, some systems may do an implicit AND on the query terms, and then use a scoring function which is defined only within the AND set. Others may rank by rule or on the basis of local comparisons which do not result in an overall score. These facts clearly limit the scope of score-distribution modelling ≠ as against the advantage of smoothness.
In this paper, accepting the limitations of the score distribution approach, we nevertheless use it to investigate the smoothness issues. That is, we try to characterise how assuming a smoother population might affect what we can infer from the sample. But we look at a range of scoredistribution methods, of which one extreme (no smoothing) corresponds to the Cormack and Lynam bootstrap method. Thus taking the scores of the retrieved real documents as given and performing bootstrap samples on them is equivalent to bootstrapping from the documents themselves1. Then we introduce various levels of smoothing, from minimal Gaussian noise to fully-fledged parametric distribution fitting.
An alternative method not pursued in this paper might be to smooth by introducing noise into the documents themselves. Some such methods have been used in the context of query performance prediction [11].
3. MOTIVATING HYPOTHESIS
To restate the hypothesis that motivates this paper, as well as the two earlier ones:
A test document collection should be thought of as a sample from some hypothetical universe of possible documents.
As is common in statistical analysis of any experimental results, we assume that the sample that we have is in some way representative of the universe, which we take to be large or infinite. The primary object of statistical analysis is to draw inferences about this universe. This is not to say that the test collection is representative of other real documents in the world, but that such a view is a prerequisite for basic statistical understanding. If we do not know how to draw inferences correctly for this hypothetical universe, then we certainly do not know how to draw inferences about the real world.
The primary concern of this paper is with the variance (error) associated with each observation that we make (a given metric on a given system-topic pair). Since we cannot normally make multiple repeated observations on different test
1We note that the assumption that a system would always give the same score to a given document, even in the context of a varying collection, is debatable: there is some discussion in [4]. We do not pursue this question in the present paper.

collections sampled from the same universe, the only way we can discover how such repetitions might be distributed is by simulation.
4. DATA AND METRICS
The document collection used in the experiments reported here is the one contained in TREC Disk 4 and 5, excluding the Congressional Record sub-collection, together with the 50 topics used for the TREC 8 Ad Hoc track. The system runs used are a new set, similar to that used in [6], consisting of a series of 44 runs on the Terrier system. Four different weighting models were used (BM25, the Hiemstra language model, PL2 and TF*IDF), and eleven different query versions (title, title+description, title+description+narrative, and eight query expansion runs using pseudo relevance feedback, expanding the query by between 4 and 512 terms). This set may be characterised in various ways: (a) all the systems (models) generate fairly traditional forms of scores; (b) all runs use the same initial parsing (including Porter stemming and stopword removal); (c) the collection of runs constitutes a systematic matrix of certain variables (weighting schemes and forms of query); (d) all other system variables are held constant.
In the event, four pairs of runs gave the exact same AP values and thus only one run from each pair was used in the experiments while the other was discarded. Thus the analysis below is based on 40 runs.
The primary metric used in this analysis is logit(AP ), where AP is average precision. As indicated above, following [3], the distributional properties of logit(AP ) seem to be better than those of AP ; in particular, because (theoretical) range of logit(AP ) is (-, ), it makes sense to model distributions of repeated logit(AP ) measurements as Gaussian; attempting to do the same with AP measurements results in some logical inconsistencies. In [3], some evidence is presented that such distributions of logit(AP ) measurements are indeed approximately Gaussian. However, it should be noted that this does not resolve the issue raised in [9], that of the bias in estimation from finite samples, which applies to logit(AP ) as well as to AP . The use of logit necessitates some smoothing, to avoid infinities at either end ≠ we follow previous practice in using an epsilon correction. However, we note that a new more heavily smoothed version of logit(AP ), yaAP, is proposed in [8]. This has better distributional properties than logit(AP ) itself, and may therefore be a better candidate metric for the work reported here; however, we have not yet rerun the analyses with yaAP.
The 44 runs have MAPs ranging from 0.273 to 0.131, with a significant bunching to the top end (31 of the runs have M AP > 0.24). If instead of taking the mean of AP , we take the mean of logit(AP ) and then convert it back to the AP scale by reversing the transformation, we see a different ranking of runs and rather lower `AP ' values, ranging from 0.188 to 0.034 (this is consistent with previous reported results using GMAP). Bunching of runs is not quite so strong but still evident.
5. SIMULATION MODELS
5.1 General simulation principles
In general, we take the results from each topic and system run (the scores of the top 1000 ranked documents), split into

893

relevant and non-relevant. This is taken as the results from searching a sample from some (assumed large or infinite) population of documents, and we wish to simulate what the search results from other samples from this same large population of documents would look like. Thus we need to infer a model of the whole population of documents and resample from it. We avoid sampling from the entire supposed population of documents (which would be intractable) by sampling separately for the results of each topic. What follows is a brief description of the method, which is similar to that used in [3], except that it separates the sampling of relevant and non-relevant documents/scores, in order to allow various smoothing methods. It involves a number of approximations that deserve more discussion than is possible in the present paper.
Suppose that the size of the original test collection is N , this topic has R known relevant documents, of which r are retrieved in the top 1000 by this system run. For the purposes of the present experiment, we will assume that all samples from the large population of documents are the same size, N . This assumption is not used directly, but is implicit in what follows. We first take a stochastic decision about how many relevant to include in our sample. This theoretically has two steps: deciding how many reldocs should be in the entire sampled collection, and deciding how many should be in the top 1000 retrieved set. The first step can be based on the observed generality of the topic (proportion of the original test collection that is relevant, R/N ), and the second can be based similarly on the proportion of these retrieved by the system (r/R). In practice we combine these two steps, and take a Poisson approximation because R/N at least is assumed to be small. So we take a sample from a Poisson distribution with mean r, to define the number rs of relevant retrieved for this topic in this simulation run. We ignore instances where this number is zero (further discussion below).
Next we take rs samples from some model of the distribution of relevant document scores, and 1000-rs samples from some model of the distribution of non-relevant scores. The details of how we do this vary between models. We sort the 1000 resulting scores into descending order, and treat them as the ranked document scores from a new test collection, sampled similarly to the original from the same large population. Since these scores are marked as `relevant' or `not relevant', we can apply any IR metric to this ranked list.
5.2 Analysis
For each topic and each run (assume S systems or runs, and T topics), we repeat the process described above some number C of times. This results in C simulated values of the metric M , for each topic and each run. In simulation, these values represent the outcome of C different test collections, all of the same size, all taken from the same infinite population of documents, all searched for the same topic by the same system/run (meaning in this case the same query formulation and the same weighting and scoring method). We note that we could also vary the size of the simulated collection to investigate the effect of such variation on evaluation results.
In what follows, we use these C ◊T ◊S values of M in two different ways. First, we compare the mean and variance of M over C = 1000 samples with the observed value of M for the real test collection. This is used to validate the simula-

tion model as producing reasonable unbiased results; it also serves to demonstrate the large variation between the variances of different topics, thus confirming our initial premise that different topics provide measurements with different associated precision (or estimation error or noise). Second, we take C = 10 samples and submit them to an analysis of variance process. This provides us with a way of conducting statistical significance tests which take account of this variation between topics.
5.3 Simulation models
As indicated, the simulation models vary in how they smooth the actual scores obtained from the system in modelling the two required distributions for each topic. In the BST (Bootstrap) simulation, we sample with replacement from the list of actual scores of the top 1000 documents, split into relevant and non-relevant. In this simulation, the scores are in effect labels only; it is therefore more-or-less equivalent to the bootstrap method used in [3]. One difference is in the handling of the zero cases. These turned out to be a problem in [3], because (a) the corpus used (TREC 6) included 5 topics with R < 5, and (b) the corpus was split randomly into two parts for their experiments. In the present corpus, there is one topic for which R = 6, and the next lowest value is 13; furthermore, we do not split the collection. Therefore it has been assumed to be safe not to treat these cases in any special way, and to simply ignore the bootstrap samples which would have rs = 0.
In the KDE (kernel density estimation) method, we add a small amount of noise (following some simple distributional assumption) to each datapoint, and mix the results into a single smooth distribution. Different noise models may be used, but a common form (used here) is Gaussian ≠ this can therefore be thought of as a Gaussian mixture model with one Gaussian for each observed score. In the present paper, we make use of a Perl module [5], applied separately to the relevant and non-relevant scores for each topic/returned document combination. There are various methods of determining the variance to be applied to the noise model; these are not discussed further in the present paper.
In the GMG (Gaussian Mixture and Gamma) model, developed by Kanoulas et al [6], the relevant distribution is modelled as a mixture of Gaussians (usually 1, 2 or 3 Gaussians, determined by the data), and the non-relevant as a Gamma distribution. Estimated parameters provided by the fitting process are the means and variances of the Gaussians, their relative preponderance, and the shape and scale parameters of the Gamma, as discussed in the cited paper.
As will be clear, simulation of IR evaluation evaluations is a complicated process, deserving of much more discussion. In the present paper, simulation is a means to an end, and such discussion is deferred to a later paper.
6. RESULTS OF THE SIMULATIONS
We present some results, as follows. In Figure 1 (simulation model BST, run 11), each point represents a single topic; the x-axis is the observed logit(AP ) on the real test collection, and the y-axis is the value of the same metric in the samples. The points represent the means over all C = 1000 samples, and the bars represent one standard deviation either side of the mean. We note that (a) the simulated results are very close to the observed ones; (b), more importantly, the standard-deviation bars are very variable.

894

Figure 1: Bootstrap simulation of run 11.
In relation to (a), we note also that this is as expected, since the bootstrap simulation uses the real results very directly; we have made no attempt to prevent overfitting. However, we can clearly see consistency; it is entirely plausible, from the means at least, that the observed results were drawn from the distributions indicated. Once again, we defer more detailed discussion of the fit to observation to a future paper devoted to simulation. Run 11 is the best-performing run according to this metric; Figure 2 shows similar graphs for three more runs, distributed over the ranking of runs, with very similar characteristics.
We note also that the per-topic standard deviation values are similar for different runs. Taking pairwise correlations between runs over the set of topic standard deviations, for the above 5 runs, the correlation values range from 0.60 to 0.97 (the bottom of the range is raised significantly if we ignore the worst-performing run). Thus (on the whole) the same topics have the least precise estimates, or the greatest noise, on the different runs. This result may well be affected by the choice of runs for the present experiment: a more varied set of runs might indicate less commonality in this respect.
Figure 3 shows one run under the Kernel Density simulation and two under the Gaussian Mixture and Gamma simulation. The variation from the observations is slightly greater, though not by very much, as a result of the smoothing; the variances are also broadly similar.
7. COMPARATIVE EVALUATION
So far we have introduced methods to simulate effectiveness measurements over different document corpora, samples from a very large (or infinite) document collection. Hence, the effectiveness of a system/run not only varies across topics but it further varies within each topic. The question that remains to be answered is how can we account for this new source of variability (within topic variability) and dis-

tinguish it from the variability across topics when evaluating retrieval systems in a comparative evaluation exercise.
Comparative systems-based evaluation typically uses a statistical hypothesis test such as the t-test to infer the significance of the difference in the effectiveness of two retrieval systems. However, the t-test is not adequate to handle multiple sources of variability into a systems-based evaluation. On the other hand, mixed-effects models, which will be introduced in this section, can incorporate arbitrary sources of variability into an analysis [7]. 2
We start with a simple model of a single source of variance (topic effect) and extend it to a model that better corresponds to our experiments of repeated measurements per topic. To illustrate the analysis, we take as example the simulated results for a single pair of systems.
The linear model that accounts only for the topic effect is,

yij = i + bj + ij

(1)

bj  N (0, 12), ij  N (0, 2)
where yij is the value of an evaluation measure calculated on topic j for system i, i is the effect of system i, bj is the effect of topic j, and ij is the residual error. The system effect is a so-called "fixed effect" since the systems we would like to compare are fixed and not a sample from some system distribution. On the other hand the topic effect is a "random effect", assuming that the topic is actually sampled from a population of topics.
The assumption behind this model is that the random variable, bj, is independent and identically normally distributed with zero mean and 12 variance. The residual error is also assumed to be independent and identically normally distributed with zero mean and 2 variance.
In the statistical programming environment R the following procedures fit the same linear effect model (Eq. 1) and result in equivalent inferences:

t.test(y ~ system, paired=TRUE, data=data)
lme(y ~ system, data=data, random=~1|topic)
In all cases our data consists of an evaluation measure calculated over a set of topics for the two systems under comparison. This is stored in a data frame data with three fields, the measurement y, the system id system, and the topic id topic. The first line runs a paired t-test; in the second one the response variable y is explicitly written as a function of a fixed effect (system) and a random effect (Error(topic)). The last line is explicitly fitting a mixedeffects model using the function lme in the nlme package.
The mixed-effects model in Eq. 1 models the effects of topics over the effectiveness scores of the compared systems. However, it fails to model a far more important effect, that of the interaction between the systems and the topics; that is the fact that a system may find a query hard while the same query is considered easy by a different system. The effect of this interaction in the above model is conflated with the residual error, since we only have one measurement per system-topic. In the case of repeated measurements this interaction effect can be decomposed from the random error and modeled separately,
2Mixed-effects models has been recently discussed by Carterette et al. [2]. A similar discussion is provided in this section.

895

Figure 2: Bootstrap simulation of runs 19, 38, 29

Figure 3: Kernel Density Estimation simulation of run 11, and Gaussian Mixture and Gamma simulation of runs 11, 19

yijk = i + bj + cij + ijk

(2)

bj  N (0, 12), cij  N (0, 22), ijk  N (0, 2)
where yijk is the value of an evaluation measure calculated on topic j for system i on the document collection sample k, i is the effect of system i, bj is the effect of topic j, cij is the interaction effect, and ijk is the residual error. As it can be seen the interaction effect is a random effect normally distributed with zero mean and 22 variance. We can fit Eq. 2 in R by,

> lme1 <- lme(y~system, data=df, random=~1|topic/system)

The output of R's lme function is,

> summary(lme1)

Linear mixed-effects model fit by REML

Data: df

AIC

BIC logLik

2173.5 2197.719 -1081.75

Random effects: Formula: ~1 | topic (Intercept)
StdDev: 1.539644

Formula: ~1 | system %in% topic (Intercept) Residual
StdDev: 0.6191864 0.6386645

Fixed effects: y ~ system

Value Std.Error DF t-value p-value

(Intercept) -1.3445 0.2438470 846 -5.514077 0.0000

system2

0.0999 0.1343512 46 0.744112 0.4606

Let's focus on the Random effects section of the output. We can observe that 12, the variance of the topic effect distribution, is estimated to 1.539644, 22, the variance of the system/topic interaction effect distribution is estimated to 0.6191864, while the variance of the error, 2, is estimated to 0.6386645.3 If we move to the Fixed effects section
3Note that even though system %in% topic appears as a nested effect of the systems within each topic, the function in fact calculates the interaction effect, because of the crossed

896

of the above results, we can observe that the effect of the first system is estimated to -1.3445 (intercept), while the difference to the second system of the pair under comparison is estimated to 0.0999 (system2). The lme function, further performs a significance test to examine whether this difference is statistically significant. The p-value over the system2 line above designates that the difference between the two systems is not statistically significant.
So far, we have assumed that the errors are independent and identically normally distributed, with mean zero and variance 2, and they are independent of the random effects. This implies that error over all system/topic pairs are i.i.d. which is the opposite of what we have demonstrated through Figures 1 and 3. To further examine whether this assumption is valid we first plot the boxplots of residual errors grouped by system/topic pairs (Figure 4). This plot is useful for verifying that the errors are centered at zero (i.e., E[] = 0), have constant variance across groups (V ar(ijk) = 2), and are independent of the group levels. Figure 4 indicates that the residuals are indeed centered at zero, but that the variability changes with system/topic group.4
We can further verify this by plotting the standardized residuals against the fitted values (left hand-side plot in Figure 5). As one can observe residuals vary with different variance across the different groups.
A more general model to better represent our measurements allows different variances by system/topic groups for the errors. We can define such a model by,

yijk = i + bj + cij + ijk

(3)

bj  N (0, 12), cij  N (0, 22), ijk  N (0, i2j )
This model is called "heteroscedastic", while the model of equal variances per system/topic group (Eq. 2) is call "homoscedastic". The difference between the heteroscedastic model in Eq. 3 and the homoscedastic model in Eq. 2 is the fact that the heteroscedastic model allows for different variances i2j for each system i topic j group of measurements. This can be expressed in lme by a varIdent parameter that defines the grouping factors.

> lme2 <- lme(y~system, data=df, random=~1|topic/system, weights=varIdent(form=~1|topic*system))

The results of fitting the heteroscedastic model in the data can be viewed below,

> summary(lme2)

Linear mixed-effects model fit by REML

Data: df

AIC

BIC logLik

1080.872 1555.559 -442.4358

Random effects: Formula: ~1 | topic (Intercept)
StdDev: 1.447164

design of our experiment, in which the two systems under comparison are run against all topics. 4For clarity purposes we only plot the residuals of the two systems against the first 10 topics.

Formula: ~1 | system %in% topic (Intercept) Residual
StdDev: 0.4537618 0.186183

Variance function:

Structure: Different standard deviations per stratum

Formula: ~1 | topic * system

Parameter estimates:

1*1

1*2

2*1

2*2 ...

1.0000000 1.6108387 1.3969085 1.5405710 ...

Fixed effects: y ~ system

Value Std.Error DF t-value p-value

(Intercept) -1.4385 0.22266286 846 -6.460817 0.0000

system2

0.1834 0.09844907 46 1.863342 0.0688

Apart from the random and fixed effects section, there is a Variance function section. The values of this section give the ratio of the standard error of each system/topic group to the standard error of the first system/topic group. Thus, for instance, the standard error for the second system when run against the first topic is about 60% more than that of the first system when run against the first topic, while the standard error for the first system run against the second topic is 40% more than the standard error running against the first topic. The standard error of the first system/topic group is itself presented as the Residual standard error, in the random effect section (0.186183).
We can confirm the adequacy of the heteroscedastic fit by re-examining plots of the standardized residuals versus the fitted values by system/topic. As expected, the standardized residuals in each block now have about the same variability, Figure 5 (right).
The need for a heteroscedastic model can be formally tested with the anova method.

anova(lme1,lme2)

Model df AIC BIC logLik Test L.Ratio p-value

lme1

1 5 2173 2197 -1081

lme2

2 98 1080 1555 -442 1 vs 2 1278.62 <.0001

The anova method compares the two models in terms of Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and Log Likelihood (logLik), with the smaller the better in all cases. Further it performs a significance test on the likelihood ratio of the two models. The very small p-value of the likelihood ratio statistic confirms that the heteroscedastic model explains the data significantly better than the homoscedastic model.

8. EXPERIMENTAL ANALYSIS
We have seen that the proposed view of evaluation experiments suggests that not all individual topic measurements have the same status. That is, each one carries its own noise variance, and these vary significantly between topics. This suggests in turn that the common approaches to statistical significance in comparing systems are over-simplified. Tests like the t-test assign the same status (give the same weight) to every individual topic measurement.
The view proposed by Cormack and Lynam [3] is that each topic should in some sense be regarded as a separate experiment, and that the summarisation of results over sets

897

Figure 4: Boxplots of residuals for homoscedastic mixed-effects model by system/topic.
Figure 5: Standardized residuals versus fitted values per system/topic for the homoscedastic (left) and heteroscedastic (right) mixed-effects model.
898

Figure 6: P-values for the different significance tests (GMG simulations).
of topics is analogous to a meta-analysis of a group of experiments. We proposed an approach to statistical significance testing that takes a step in this direction, using mixed-effects models. For each topic/run pair, we run the simulation 10 times, to obtain 10 values of the metric. In mixed-effects terms, we regard these values as repeated measurements, in the sense that the measurements have been taken at random from a population of possible values (i.e. a population of possible test collections). We are not interested in the differences between these individual values within the groups.
The effect of taking multiple values per topic/run combination is to indicate to the mixed-effects model how reliable or unreliable each topic is in determining the effectiveness of this run. One obvious issue is that these measurements are not real ≠ they are the result of simulations. We can appeal to the evidence of Section 6, that they are consistent with the observed results. We need to use the simulations rather than the directly observed results because only here do we have the necessary repetitions.
We compare the (heteroscedastic and homoscedastic) mixedeffects models with the t-test. Since we use simulated data for mixed-effects models, we also use the same data for the t-test: this is performed on the means of the 10 simulated samples for each topic. Again, since this is not the usual way to run a t-test on IR evaluation data, we also run it in the usual way, on the observed results.
In order to measure the power of the test, we follow Sakai [10] in performing all possible pairwise tests, sorting by pvalue, and plotting the number of pairs against the p-values: see Figure 6 for the Gaussian Mixture and Gamma simulation; similar plots are obtained by the other two simulation model (note that in this section we focus on the GMG simulations, which show the most discrepancy from the observed results). All these are 2-sided tests. We see that the all four tests appear to be equally powerful, as indicated by how much of the curve is below a significance threshold such as 0.05. Hence, decomposing and separately modeling the precision of each topic separately does not lead to a change in the power of the test.
What is more interesting however is to examine the disagreements between the p-values calculated by the different tests that could lead in disagreements between which system

pairs each test consider as significantly different in performance. Figure 7 presents the scatter plot of the p-values for the t-test on the means of the simulated measurements, the homoscedastic mixed-effects model and the heteroscedastic mixed-effects model against the t-test over the actual logit AP values (without any simulation). This again uses the evaluation scores over the Gaussian Mixture and Gamma simulation. Each point is a pair of systems under comparison. As it can be seen from the plots, there is a significant disagreement in the p-values computed by the different tests (but note that the visual effect is somewhat exaggerated because of the large number of superimposed points in the lower left). The t-test over the actual logit AP values agrees best with the heteroscedastic model. This is expected since the t-test over the mean of the repeated measurements per topic and the homoscedastic model do not handle correctly the extra source of variability. The points in the areas to the left and the bottom of the dashed lines in the plots of Figure 7 represent system pairs that have been found significantly different by at least one of the contrasted tests. If we focus at the heteroscedastic model vs t-test plot the percentage of disagreement between the two tests with respect to those system pairs that are found significant by at least one of the tests reaches about 20%. Examining the system pairs for which the two tests disagree is of particular interest; however we leave it as future work.
We also show contingency tables indicating agreement or otherwise between the tests at the threshold of 0.05 (Table 1). The results over all three simulation methods are summarized in these tables. Similarly to the Figures 6 and 7 we see that all test are comparable regarding their power, while there are certain system pairs for which different tests disagree with each other. Note that there is a disagreement in the results of the t-test on actual values across the three simulating methods; e.g. in the case of BST and KDE 303 comparisons have been found statistically significant according to the t-test, while for the case of GMG 318 are statistically significant. When simulating sampling the corpus there are topics for which the simulations produce the same logitAP value at every iteration. These topics are excluded when comparing two systems since the homoscedastic and heteroscedastic models cannot address zero variance within a topic, even though the t-test is not affected by this. Different simulation methods lead to different sets of zero variance topics and hence the t-test across different simulation methods is run over different topic sets.
9. CONCLUSIONS
The experiments reported here reinforce the notion that we need to worry about the statistical status and validity of evaluation results on individual topics. The usual approach in IR evaluation, of regarding each individual topic measurement as an indivisible atom, is not consistent with our use of particular collections of documents. The size of the document collections we use is actually no insulation against statistical issues, since measurements for individual topics typically depend on the small sets of documents judged relevant.
A good approach to this issue has the potential to inform the measurement of statistical significance in IR evaluations. We provide some evidence that it will affect significance tests and inferences of retrieval quality. The evidence presented is by no means complete, and many other developments and

899

(BST)

Figure 7: Scatter plots of p-values for the different tests (GMG simulation)

Heterosced. 10 samples

p < 0.05 p  0.05

t-test on actual values

p < 0.05 p  0.05

287

15

16

462

t-test on mean over 10 samples

p < 0.05 p  0.05

t-test on actual values

p < 0.05 p  0.05

291

2

12

475

(GMG)

Heterosced. p < 0.05 288

10 samples p  0.05

37

20 435

t-test on mean p < 0.05 281

9

over 10 samples p  0.05

44

446

(KDE)

Heterosced. p < 0.05 292

10 samples p  0.05

11

22 455

t-test on mean p < 0.05 288

3

over 10 samples p  0.05

15

474

Table 1: Contingency table on the agreement of different significance tests for 780 pairs of systems, on 3 simulations: (BST) Bootstrap, (GMG) Gaussian mixture + gamma, (KDE) Kernel Density Estimation.

experiments are required. One future direction would be to investigate whether we can characterise the high-variance topics in some simple way. However, the work reported here is seen as a step towards better understanding of this particular source of error in IR evaluation.
Acknowledgments
We would like to thank Jaap Kamps for valuable discussions on an earlier version of this paper. Further, we gratefully acknowledge the support provided by the European Commission grant FP7-PEOPLE-2009-IIF-254562.
10. REFERENCES
[1] David Banks, Paul Over, and Nien-Fan Zhang. Blind men and elephants: Six approaches to TREC data. Inf. Retr., 1(1-2):7≠34, 1999.
[2] Ben Carterette, Evangelos Kanoulas, and Emine Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In Proceedings of the 20th ACM Conference on Information and Knowledge Management, CIKM 2011, pages 611≠620. ACM, 2011.
[3] Gordon V. Cormack and Thomas R. Lynam. Statistical precision of information retrieval evaluation. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 533≠540. ACM, 2006.
[4] David Hawking and Stephen E. Robertson. On collection size and retrieval effectiveness. Inf. Retr., 6(1):99≠105, 2003.

[5] Philipp Jarnet. Statistics-KernelEstimation.
http://search.cpan.org/~janert/ Statistics-KernelEstimation/.
[6] Evangelos Kanoulas, Virgiliu Pavlu, Keshi Dai, and Javed A. Aslam. Modeling the score distributions of relevant and non-relevant documents. In Proceedings of Second International Conference on the Theory of Information Retrieval, ICTIR 2009, pages 152≠163. Springer, 2009.
[7] Jose C. Pinheiro and Douglas M. Bates. Mixed-Effects Models in S and S-PLUS. Statistics and computing. Springer-Verlag New York, Inc., 2000.
[8] Stephen Robertson. On smoothing average precision. In Proceedings of the 34th European Conference on IR Research, ECIR 2012, pages 158≠169. Springer, 2012.
[9] Stephen E. Robertson. On document populations and measures of ir effectiveness. In Studies in theory of information retrieval (Proceedings of ICTIR 2007), pages 9≠12, 2007.
[10] Tetsuya Sakai. Evaluating evaluation metrics based on the bootstrap. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 525≠532. ACM, 2006.
[11] Vishwa Vinay, Ingemar J. Cox, Natasa Milic-Frayling, and Kenneth R. Wood. On ranking the effectiveness of searches. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 398≠404. ACM, 2006.

900

An Uncertainty-aware Query Selection Model for Evaluation of IR Systems

Mehdi Hosseini, Ingemar J. Cox
Computer Science Department
University College, London, UK
{m.hosseini, ingemar}@cs.ucl.ac.uk

Natasa Milic¥-Frayling, Milad Shokouhi,
Emine Yilmaz
Microsoft Research, Cambridge, UK
{natasamf,milads,eminey}@microsoft.com

ABSTRACT
We propose a mathematical framework for query selection as a mechanism for reducing the cost of constructing information retrieval test collections. In particular, our mathematical formulation explicitly models the uncertainty in the retrieval effectiveness metrics that is introduced by the absence of relevance judgments. Since the optimization problem is computationally intractable, we devise an adaptive query selection algorithm, referred to as Adaptive, that provides an approximate solution. Adaptive selects queries iteratively and assumes that no relevance judgments are available for the query under consideration. Once a query is selected, the associated relevance assessments are acquired and then used to aid the selection of subsequent queries. We demonstrate the effectiveness of the algorithm on two TREC test collections as well as a test collection of an online search engine with 1000 queries. Our experimental results show that the queries chosen by Adaptive produce reliable performance ranking of systems. The ranking is better correlated with the actual systems ranking than the rankings produced by queries that were selected using the considered baseline methods.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.4[Systems and Software: Performance Evaluation]
General Terms
Measurements, Algorithms, Theory and Performance
Keywords
Information Retrieval, Test Collection, Query Selection
1. INTRODUCTION
Modern test collections are large, comprising millions of documents and thousands of queries that require relevance
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08... $15.00.

judgments in order to calculate metrics of retrieval effectiveness. Constructing a test collection incurs a cost which, in the simplest case, is proportional to the number of queries chosen to evaluate the retrieval systems and the number of documents that need to be judged per query. Hence, the cost can be reduced by: (i ) query selection that limits the number of queries for which relevance judgments need to be collected, (ii ) document selection that reduces the number of documents to be judged for each query, and (iii ) a combination of the two.
Much recent work has been devoted to effective and efficient construction of test collections [1, 5], with the primary focus on document selection [4]. In this paper, we explore methods for query selection as a means of dealing with a budget constraint. As Guiver et al. [10] showed, it is possible to reproduce the results of an exhaustive evaluation of systems by using a much reduced set of queries. However, it is still unclear how to select such a subset when relevance judgments are not available for the queries under consideration.
In our approach, we first formulate query selection as an optimization problem, minimizing the error in systems evaluation based on a subset of queries. In contrast to the previous work which is mostly retrospective and assumes that some relevance judgments are available for each query [10, 15, 17], our model is designed to work in practice and does not require relevance judgments for a query that is not yet selected. The mathematical formulation shows that an optimal subset of queries satisfies the following properties: (i ) selected queries have a low correlation with one another, thereby maximizing the information we gain from each, (ii ) selected queries have strong correlation with the remaining queries, as without this correlation there is no predictive capability, and (iii ) the total uncertainty associated with the selected queries is small. Here the correlation between two queries refers to their similarity in evaluating systems.
Since selecting the optimal subset of queries is a computationally intractable problem, we approximate the solution by an iterative query selection process referred to as Adaptive. The Adaptive algorithm starts by selecting the first query at random, assuming no information about relevance judgments. However, once this query is selected, the relevance judgments are acquired and used to assist with the selection of subsequent queries.
Specifically, at each iteration we use previously selected queries and associated relevance judgments to train a classifier that estimates the relevance of documents pooled for each of the unselected queries. Using the output of the clas-

901

sifier we compute the probability of relevance for the pooled documents and used it to estimate the performance metric, e.g. the average precision, and the corresponding approximation variance which we refer to as uncertainty.
We evaluate our method by comparing the ranking of systems based on the subset of queries with the ranking over the full set of queries. We report the results in terms of Kendall- and Pearson correlation coefficients and show that the query subsets chosen by our models are significantly more effective than those selected by the considered baseline methods.
It is known that using query subsets may lead to poor performance when estimating the performance of previously unseen (new) systems [17]. We conduct experiments to investigate how our method generalizes to new systems. We show that the iterative algorithm can be modified to improve generalizability. Additionally, we consider the query selection problem for multiple metrics. In our experiment we show that a query subset selected based on a particular metric may not provide reliable systems evaluation when another metric is used to measure retrieval performance. Thus, we modify our query selection algorithm to select a query subset that leads to reliable evaluations across multiple metrics.
In summary, the contributions of this paper are threefold: (i ) we provide a theoretical model for query selection that explicitly models uncertainty in retrieval effectiveness scores, (ii ) we develop Adaptive, an iterative algorithm, that efficiently implements our theoretical model in practice, and (iii ) we modify the iterative algorithm to investigate how the selected query subset generalizes to (a) new unseen systems and (b) changes to the evaluation metric.
2. RELATED WORK
The increased size of document corpora and query sets has made the cost of relevance assessments one of the main challenges in creating IR test collections. Spark-Jones and Van Rijsbergen [19] proposed a document pooling technique to select a subset of documents to be judged and the National Institute of Standard and Technology (NIST) adopted this method in most TREC experiments. For example, in the TREC AdHoc task, each participating system adds the top-100 ranked documents per query to the common pool to be judged by human assessors. Documents that are not included in this pool are assumed to be non-relevant. Several alternative approaches to the original pooling method have been suggested in order to judge more relevant documents at the same pool depth, e.g. Zobel [21] and Cormack et al. [7].
On average, NIST assessors judge 2,000 documents per query. This is often sufficient for a reliable evaluation of systems, even for recall-sensitive metrics such as average precision (AP ), and increases the reusability of the TREC test collection for other tasks and new systems. Nevertheless, it still demands a considerable human effort and incurs a significant cost when the test collection contains a large number of queries. As a result, most of the existing TREC test collections contain as few as 50 queries.
Using small samples of queries to measure retrieval performance may cause considerable errors in the systems evaluation. Therefore, recent studies have concentrated on the IR evaluation with large set of queries [5]. In order to make relevance assessments feasible, various document selection

methods are suggested [2, 4]. These methods reduce the number of documents that need to be judged per query and are often used in conjunction with evaluation metrics [2, 4, 20] designed for shallow sets of relevance judgments.
Following the trend of online search engines with large number of queries, NIST has recently launched the Million Query track [1]. In 2007, the Million Query track used 1800 queries for evaluation. The track mostly focused on (i ) analyzing the efficiency of document selection methods, and (ii ) the impact that incomplete relevance judgments have on the measurement accuracy.
Query selection is a complementary approach to document selection and is used to reduce the cost of creating test collections. Guiver et al. [10] have shown that query subsets vary in their accuracy of predicting the average performance of systems. Their results indicate that some subsets of queries, known as representative subsets, are particularly good predictors of the systems average performance as measured over the full set of queries. Mizzaro and Robertson [15] explored the characteristics of individual queries that were beneficial for systems evaluation. They defined the notion of hubness for queries where a higher hubness score indicates that a query is better than others at distinguishing the systems retrieval effectiveness. Robertson [17] later showed that the query selection based on the hubness scores alone does not necessarily result in a reliable prediction of the systems rankings. The work by Guiver et al. [10] shows that, indeed, representative query sets that are effective in approximating the systems ranking, comprise of queries that range in their individual ability to predict the systems performance.
Most studies on query selection are based on retrospective analysis [10, 15, 17] and assume that relevance judgments are available at the time the subset of queries is selected. Typical scenarios involve selecting queries to increase the pool of existing relevance judgments to accommodate evaluation of new systems. For example, Hosseini et al. [14] proposed a query selection approach to enhance the reusability of a test collection. Their focus was on generalizability, i.e. a reliable ranking of systems that did not contribute to the judged pool of documents. They assumed that an initial set of relevance judgments is available for each candidate query, and proposed a budget constrained model to select an optimal subset of queries. The selected queries were then used to expand relevance judgments based on the documents retrieved by new systems. The authors later extended their work [13] by considering a query-document pair selection problem when relevance judgments were to be created iteratively. Yet again, they assumed that the available budget allows for gathering at least a few relevance judgments per each query in the test collection. The authors applied convex optimization with a budget constraint to collect relevance judgments optimally across queries.
3. QUERY SELECTION
Previous research provided valuable insights on framing theoretical models for query selection [13, 14]. It focused on cases where there were judged documents for each query in the pool. The objective was to select a subset of queries to obtain additional relevance judgments for a more robust evaluation. In contrast, we focus on a scenario when no relevance judgments are available until a query is selected. We assume that a large set of queries has been initially compiled to measure the performance of a set of systems. Our goal

902

q1

s1

x1,1 x1,2

x2,1 x2,2

Systems (S) ...

...

x,1 x,2

...

...

Queries (Q) ... ... ... ... ...

x1,n x2,n
... x,n
...

(a) In Theory

s1 Systems (S)

q1 (x^1,1, v1,1) (x^2,1, v2,1)
...
(x^,1, v,1) ...

Queries (Q)

(x^1,2, v1,2) ... (x^1,n, v1,n)

(x^2,2, v2,2) ... (x^2,n, v2,n)

...

...

...

(x^,2, v,2) ... (x^,n, v,n)

...

...

...

(b) In Practice

Figure 1: (a) The true performance matrix X for systems in S and queries in Q. Each entry indicates the system performance score based on the available relevance judgments. (b) The approximated performance matrix X^ , for systems in S and queries in Q. Each pair indicates the estimated performance and associated uncertainty.

is to find a subset of queries that most closely approximates the system evaluation results that would be obtained if we judged documents for the full set of queries.
To that effect we first formulate the query selection problem as in [13], assuming that relevance judgements are available and, thus the performance scores are known. We then relax this assumption and introduce our query selection model in the next section.
Consider a set of n known queries, Q, together with a set of l known systems, S. When relevance judgments are available, we can represent the associated retrieval effectiveness scores of the l systems against the n queries using a l ◊ n performance matrix X  Rl◊n (Figure 1a). In X, the rows represent systems, the columns represent queries, and the value xs,q shows the performance score, e.g. the average precision, of sth system in S against qth query in Q. We also consider a column vector M  Rl◊1, as the average performance vector. The elements of M represent the average performance scores of individual systems across the set of queries. Thus, when the metric is AP , M represents the mean average precision (M AP ) scores for individual systems.
Now, let  = {j1, ..., jm} be a subset of {1, 2, ..., n} with 1  m  n and Q be the corresponding query subset. We define M  Rl◊1 as the column vector comprising the average performance of systems for the subset of queries, Q. The aim of a query selection method is to find a subset of m queries such that the corresponding column vector M closely approximates the M vector.
The approximation is quantified using the mean squared errors between elements of M and M, if the similarity in terms of the absolute values of performance scores is of interest. Alternatively, we can use Kendall- correlation, if the similarity in the ranking of systems is of interest, or Pearson linear correlation, if the similarities in the relative performance scores are of interest. As in [13], we choose Pearson correlation as it is amenable to mathematical optimization. However, we use Kendall- as our final evaluation measure for comparing the rankings of systems produced by full set and a subset of queries.
The Pearson correlation between the elements of M and M is



=

cov(M, M)

{var(M

)v

ar(M)}

1 2

(1)

where

var(M ) = var(M) = cov(M, M) =

n-2eT e m-2dT d n-1m-1dT e

where e = {1}n◊1 is the vector of n components, each equal to 1; d  {0, 1}n◊1 is a binary vector such that dj = 1 if j  , and dj = 0 otherwise. Note that this is slightly different from the formulation in [13] as the task is also different. In [13], d is defined as a vector of real values and its role is
to establish the proportions of the budget allocated across queries. In this paper, since we are interested in the query selection problem, the vector d is defined, without loss of generality, as a binary vector and considered as an indicator to selected queries.
Following Equation 1, we have  = cov(X) as the n ◊ n covariance matrix of the system-query performance scores such that (i, j)th element of  is the covariance between ith and jth columns of X. In practice, the covariance matrix  calculated based on the l systems is:

l
 = (l - 1)-1 (xi - )T (xi - )
i=1

where  = l-1

l i=1

xi

and

xi

is

the

ith

row

of

X.

The

op-

timum subset maximizes the Pearson correlation , where

substituting for the variances and the covariance we have

(dT e)



=

{(eT

e)(dT

d)}

1 2

This derivation assumes that the elements of the X matrix are true representatives of the systems performance as computed over the full set of relevance judgments. Of course, in practice this assumption does not hold because of the absence of relevance judgments during query selection. In the following section, we propose an extended model that uses performance predictors for approximating systems true performance. We then extend the model to incorporate explicitly the noise in the measurement of the systems performance.

4. UNCERTAINTY-AWARE SELECTION
We assume that instead of containing the true performance values, each element of X holds a predicted performance estimate with a variance from the true value that we refer to as uncertainty. Hence, the noisy X^ matrix can be represented as shown in Figure 1b where each of its ele-

903

ments represents a pair of values: x^s,q and vs,q = var(xs,q). In addition, let M^   Rl◊1 be the vector of l average performance scores computed based on the query subset, Q, and the performance matrix X^ . Thus, in practice we look
for a subset that maximizes the Pearson correlation between M^ and M . To compute the Pearson correlation we need to compute the variances and the covariance of M^  and M .
The variance of M^ is due to two sources, the variance across systems and the variance due to the measurement noise. The first variance is expressed by var(M) as calculated in Section 3. To compute the second variance we first note that each of the elements in M^  has its own variance. If ^i denotes the performance of ith system in M^, then the associated variance is
var(^i) = m-2 vi,j
j

Following the law of total variance [3], the variance of M^  is given by

var(M^ ) = var(M) + Es(var(^s)) =

(2)

m-2dT d + m-2 E(vj) = m-2dT ( + U )d

j

where 1  s  l and U = diag E(v1), ..., E(vn) is a diago-

nal matrix, referred to as the uncertainty matrix. E(vq) =

l-1

l i=1

vi,q

is

the

mean

uncertainty

for

a

query

q.

To compute the covariance between M^ and M , let us con-

sider an unknown system that is randomly sampled from the

l systems, and let x and x^ denote the associated performance row vectors in X and X^ . The system's average performance

computed based on X and the full set of queries is

 = n-1xe

Also the systems average performance based on the subset of m queries, Q, and X^ is
^ = m-1x^d

where e and d are the column vectors as defined in Section 3. The covariance between M^  and M is then
cov(M^, M )  cov(^, ) = m-1n-1cov(x^d, xe) = m-1n-1dT cov(x^T , x)e = m-1n-1dT e (3)

where x^d = dT x^T , and cov(x^T , x) = cov(xT + , x) =
E x - E(x) T x - E(x)  cov(X) = 

Note that, x^T = xT + where  R1◊n is the vector of
estimator's noise. Thus, the Pearson correlation between M^  and M is given
by

(dT e)

^ =

1

(4)

eT e dT ( + U )d 2

Formally, we seek a subset Q that maximizes ^. Reordering the correlation above we have





(eT

e)

1 2

^

=

(eT d)
1

dT ( + U )d 2

Selecting a subset of queries  that maximizes ^ is equiva-

lent to selecting a subset of queries that maximizes  since

(eT

e)

1 2

is a constant.

Let i,j be the (i, j)th element of

 and E(vj) be the jth diagonal element of the uncertainty

matrix U . We can then rewrite  as

max  = 

1in,j (i,j )
1
i,j(i,j ) + j E(vj ) 2

(5)

Equation 5 provides valuable insights into the query selection problem. In order to maximize  we seek a set of queries that minimizes the denominator and maximizes the numerator.
To minimize the denominator, we need to choose m queries for which the corresponding columns in X are least correlated with one another. This is equivalent to maximizing the information we derive from each query in the subset. Conversely, if the columns of X are perfectly correlated, then all the queries provide the same information and we may as well have a subset of size one. Additionally, the sum of the expected uncertainty, {E(vj )|j  }, of the selected queries should be a minimum.
The numerator is maximized if the columns in X, associated with the selected queries, have high correlation with the rest of columns in X. This is also intuitively clear. After all, if the selected subset is completely uncorrelated with the remaining queries, then it has no prediction power of the systems performance on the remaining queries. Thus, we seek a subset of queries that are highly correlated with the rest of queries.

5. ADAPTIVE QUERY SELECTION
Thus far, we introduced a theoretical model for query selection that extends the previous work by explicitly modeling uncertainty. The model allows for the elements of the performance matrix to represent predicted rather than actual performance values. Equation 5 shows how the predicted performance values can be incorporated into the optimization process, but does not indicate how they can be computed in practice. We describe in detail the Adaptive query selection algorithm that iteratively selects queries and refines the estimations in X^ . This method exploits the supervised prediction and uses the relevance judgments of already selected queries, to train a model for selecting subsequent queries.
The Adaptive method iteratively selects a new query, that in combination with the previously selected queries maximizes Equation 5. The relevance judgments of the previously selected queries are used to predict the relevance judgments of yet non-selected queries. The adaptive method subsequently estimates the performance scores, and updates the X^ matrix by adding the systems performance scores for the selected query, and those predicted for the non-selected queries. This process is repeated until we reach the maximum number of queries to be selected.
In order to predict the relevance of documents for queries that have not been selected yet, we train a classifier using judged documents of previously selected queries as training data. Each query-document pair is represented to the classifier as a vector of 7+l generic features where l refers to the number of systems. These features are:
∑ The number of systems that retrieved the query-document pair (one feature).

904

∑ The average, minimum and maximum ranks given to the query-document pair by the systems (three features).

∑ For systems that retrieve the query-document pair, we calculate their corresponding past-performance scores based on the subset of queries for which we have relevance judgments. For example, if the metric is AP , we compute a system's M AP based on its AP scores obtained for previously selected queries. We then determine the minimum, maximum and average across systems (three features).

∑ The l relevance scores provided by l systems for the given query-document pair (l features). If a system does not retrieve the document, the corresponding score is set to the minimum score obtained for the documents retrieved by that system.

We train a linear support vector machine (SVM) as our classifier [8]. For each query-document pair we map the output of the classifier to a probability score using the calibration method proposed in [16]. Briefly, let f  [a, b] be the output of the SVM classifier. We use a sigmoid function to map f to a posterior probability on [0,1]:

pi

=

P (ri

=

1|fi)

=

1

1 + exp(Afi

+

B)

where ri refers to the true relevance value of document i, pi is its probability of relevance, and A and B are the parameters of the sigmoid function, fitted using maximum likelihood estimation from a calibration set (ri, fi). The training data is the same as the training data used by the classifier. Thus, at each iteration we retrain the classifier and fit the sigmoid function to exploit the increase in training data from the new round of relevance judgments.
After each query-document pair is assigned a probability of relevance, we use these probabilities in the family of MTC estimators, proposed by Carterette et al. [4], to provide new estimates for the unknown performance scores in the X^ matrix. For example, when the metric of interest is P @k, the expectation and variance are calculated as:

E[P @k] =

1 k

k i=1

pi

var[P @k] =

1 k2

k i=1

pi(1

-

pi)

where pi is the calibrated relevance probability of the document retrieved at rank i. The formulations for other metrics, e.g. AP , can be found in [4].

6. EVALUATION SETTINGS
Query selection and document selection methods are often evaluated by considering the systems ranking they produce compared to the ground-truth systems ranking based on all the queries and the full set of relevance judgments. As in previous work [4, 10], we use Kendall- and Pearson coefficient as the correlation metrics. Kendall- penalizes disordering of high-performance and low-performance system pairs equally. However, in practice, distinguishing between best performing systems is often more important. Therefore, in many of our experiments, we also report separately the results for the subsets of best performing systems in terms of the average precision (AP ) and the precision at position 100 (P @100).

We run our experiments on both the TREC 2004 Robust track, and the TREC-8 AdHoc track data sets. The TREC 2004 Robust track includes a collection of 249 queries, 14 participants with a total of 110 automatic runs, and 311,410 relevance judgments. The TREC-8 consists of 50 queries, 39 participants with 13 manual runs and 116 automatic runs, and 86,830 relevance judgments. In our experiments, we consider individual runs as different search systems, taking special care when considering runs from the same participant. We also create a Web test collection based on the query logs of a commercial search engine. This dataset comprises 1,000 queries, 50 runs of a learning to rank system trained with different feature sets, and 30,000 relevance judgments. We compare the performance of our query selection against query sets obtained by three baseline methods: random, oracle and iterative query prioritization.
Random: randomly selects a subset of queries. We report the results averaged over 1,000 random trials and consider 95% confidence interval of the sample average.
Oracle(Ideal ): selects the best subset of queries by considering the full X matrix constructed from the full set of queries and all the relevance judgments in the test collection. For a given subset size m < 10 and m > (n - 10), we perform an exhaustive search to find the best subset. Exhaustive search is computationally expensive for 10 < m < (n - 10). Therefore, we estimate the best subset of size 10 < m < (n - 10) by randomly generating 10,000 query subsets from which the best subset is selected.1
Iterative Query Prioritization (IQP): we consider a modified version of the budget constrained optimization method proposed in [13] as a query selection baseline. The original method, referred to as query prioritization (QP ), cannot be used in our experiments because it is defined as a convex optimization that demands a set of initial judgments for all the queries. This assumption is not valid in our experiments. Thus, we consider a modified version of this method that does not rely upon such an assumption and is specialized for the query selection task. We replace the budget vector   [0, 1]n◊1 with a binary vector d  {0, 1}n◊1 as an indicator to selected queries. Therefore, the optimization function is changed to

dT e

max f (d) = d

1
dT d 2

subject to

n j=1

dj



m

This modified version, (IQP ), starts from zero relevance judgments and iteratively selects queries and updates the vector d. Similar to the original method in [13] IQP does not consider the uncertainty in estimating the entries of the X matrix. That is, it uses the same classifier (as in our iterative adaptive method) but directly maps the output of the classifier to 0 or 1, when the relevance judgments are binary, and regards them as the predicted absolute relevance values. Therefore there is no calibration of relevance probabilities involved. As such, it does not use the MTC estimators discussed in Section 5 but, instead, relies upon the standard metrics, e.g. AP , to measure the systems performance. Considering IQP in our experiments helps us investigate the effect of incorporating measurement uncertainty into the query selection.

1We considered the oracle subset in our experiments to provide an upper bound for the performance of the query selection algorithms.

905

1

1

Pearson correlation Kendall- correlation

0.9

0.8

0.7

IQP

0.6

Adaptive

Random

Oracle

0.5 0%

10%

20%

30%

40%

50%

60%

70%

80%

subset size

(a)

0.9

0.8

0.7

0.6

IQP

Adaptive

0.5

Random

Oracle

0.4 0%

10%

20%

30%

40%

50%

60%

70%

80%

subset size

(b)

Figure 2: Selecting queries using (i ) Oracle, (ii ) random, (iii ) IQP, (iv ) Adaptive query selection algorithm, for the Robust 2004 test collections with 249 queries. The first query is randomly selected. The results are averaged over 50 trials with AP metric.

7. EXPERIMENTAL RESULTS

In the experiments with TREC test collections, we con-

sider all the official retrieval runs. Each system contributes

100 documents to the pool for each query. After selecting

a query, the official TREC judgments are collected and re-

vealed. The Adaptive and IQP methods, then add these

recently judged documents to their training sets.

On each testbed, we report the results for three different

groupings of systems: (i ) all systems, (ii ) 30 best performing

systems, and (iii ) only pairs of systems with a statistically

significant performance difference, measured by the paired

t-test with the significance level of 0.05.

Figure 2 shows the results on the Robust 2004 test collec-

tion with 249 queries. The retrieval evaluation metric is AP .

Pearson and Kendall- correlation are used to measure the correlation of a query subset vector M^, and correspond-

ing vector M , calculated using the full set of 249 queries.

At the initialization step of the Adaptive and IQP meth-

ods, the first query is randomly selected. To deal with the

variation of random sampling, we consider 50 trials. We

report the average of 50 trials as the average results for

Adaptive and IQP . We also consider the 95% confidence

interval of the average performance to detect significant dif-

ferences between the query selection methods. For instance,

in Figure 2a when the subset covers 28% of the full query

set, the average of 50 Pearson correlation scores obtained

by the Adaptive method is 0.94 and the associated stan-

dard deviation is 0.07. Thus the 95% confidence interval

is:

0.94

±

1.96

◊

0.07 50

.

The confidence intervals are shown

as error bars. In general, the difference between two meth-

ods is statistically significant for a particular subset size, if

the associated error bars do not overlap. As seen, in Figure

2a and 2b, for both Pearson correlation and Kendall- , the

Adaptive method significantly outperforms the Random and

IQP baselines across different subset sizes. From Figure 2b

we see that the Adaptive method achieves a Kendall- cor-

relation of 0.9 with a subset that covers 50% of the queries

(125 out of 249 queries). However, the Random and IQP

methods require at least 70% of queries to achieve the same

Kendall- .

Table 1 summarizes the Kendall- and Pearson correlation

for the four query selection methods when selecting {20, 40,

60}% of queries in the Robust 2004 and the TREC-8 test collections.
The columns labeled `all' indicates the results for all the systems in a test collection. For both test collections and the three subset sizes, {20, 40, 60}%, the Adaptive method significantly outperforms IQP and Random in most cases. For instance, in the Robust 2004 test collection the Adaptive method obtains {15, 10, 5}% average improvements over Random and IPQ in Kendall- for subsets of {20, 40, 60}% respectively. Similar improvements are observed for the TREC-8 test collection.
The columns labeled `top' indicates the results for the 30 best performing systems, i.e. those with the highest MAP scores. When calculating Pearson and Kendall- correlations, the vectors M^  and M are constructed by considering only the top 30 systems. The remaining systems only contribute to the query selection process and are not used for evaluation. Once again, the Adaptive method significantly outperforms the IQP and Random methods in most of the cases. Interestingly, the improvements are even larger than the improvements of the full set of systems. For instance, for the Robust test collection, improvement in Kendall- is on average 10% for the full set of systems and it rises to 25% for the top 30 best performing systems. Similarly, the average improvement in Pearson correlation rises from 7% to 14% on average. Similar results are observed for the TREC-8 test collection.
The columns labeled `sig' indicates the results for the pairs of systems whose performances difference is statistically significant. If a difference in average performance scores of two systems is not statistically significant, it is reasonable that they may be ordered differently when evaluated over a subset of queries. Such tied systems increase the probability of a swap and thus may considerably decrease Kendall- since the common formulation of Kendall- does not distinguish between pairs of systems with and without significant differences. This is, in fact, the case for the Robust and the TREC-8 test collection where about 30% of pairs of systems are tied as measured by the paired t-test with the significance level of 0.05. Thus, we also compute the Kendall- of systems with a significant difference in M AP . Again, the

906

Table 1: Comparisons of the four query selection methods for the two TREC test collections based on the AP metric. The statistically significant improvements of Adaptive over IQP and Random are marked by .

Subset 20% 40% 60%

Method
Random IQP
Adaptive Oracle
Random IQP
Adaptive Oracle
Random IQP
Adaptive Oracle

Robust2004

Kendall-

Pearson

all top sig all top

0.68 0.45 0.75 0.83 0.68

0.67 0.47 0.78 0.86 0.70 0.77 0.63 0.85 0.92 0.79

0.90 0.81 0.90 0.97 0.95

0.80 0.58 0.82 0.93 0.76

0.80 0.56 0.85 0.94 0.78 0.87 0.69 0.89 0.98 0.89

0.92 0.86 0.95 0.99 0.96

0.85 0.71 0.88 0.97 0.90

0.88 0.73 0.90 0.96 0.91 0.91 0.83 0.95 0.99 0.96

0.94 0.92 0.97 0.99 0.99

TREC-8

Kendall-

Pearson

all top sig all top

0.72 0.45 0.88 0.92 0.77

0.74 0.53 0.92 0.93 0.81 0.83 0.69 0.95 0.95 0.92

0.88 0.80 0.97 0.97 0.95

0.77 0.58 0.95 0.95 0.86

0.81 0.66 0.96 0.95 0.89 0.90 0.81 0.99 0.97 0.95

0.93 0.85 1.0 0.98 0.97

0.87 0.70 0.97 0.97 0.90

0.88 0.80 0.99 0.93 0.85 1.0

0.98 0.92 0.98 0.96

0.95 0.91 1.0 0.99 0.99

Table 2: Comparisons of Random and Adaptive using the Web test collection.

#queries #relevance judgments

Method
Random Adaptive Random Adaptive

desired Kendall- 0.7 0.8 0.9
167 368 739 71 207 486 5010 10235 28804 2086 5803 15854

Adaptive method significantly outperforms IQP and Random in most cases.
We repeated the experiments in Table 1 for P @100 metric, and observed similar results for both the test collections.2
7.1 Results of the Web Data
We also investigate the performance of Adaptive on a test collection comprising the Web search results from a commercial search engine with 1,000 queries and 50 different search systems. Here, the systems are various rankers (runs) of a learning to rank system that were trained with different feature sets. To generate a ranker we randomly sample g = {5, 10, 20, 30 or 40} features from a given feature set and optimize the ranker on a common training set (the training details are out of the scope of this paper). For each query, the top 5 web pages returned by the rankers are pooled for relevance assessment. The performance of each ranker is measured according to the precision at the position rank 5 (P @5).
Table 2 reports the number of queries, and the number of relevance judgments required to reach Kendall- values of {0.7, 0.8, and 0.9} by the Adaptive, and the Random query selection method. We focus on the comparisons between Adaptive and Random because the random sampling is commonly used for selecting candidate queries. Thus, we can observe the cost reduction that can be achieved in practice by the Adaptive method.
The reported results for the Adaptive method are the average over 10 trials. In each trial, the initialization involves 20 queries instead of a randomly selected single query. This ensures a sufficiently large training set for the initial classifier without losing much in the query selection performance.
2The results on P @100 are not reported due to lack of space.

Kendall- Correlation

1

0.9

0.8

0.7

0.6

0.5

0.4

Oracle

0.3

Random

Query Selection Trials

0.2

0.1

0

5

10

15

20

25

30

35

40

45

Subset Size

Figure 3: Sensitivity of the query selection to the first query using TREC-8 comprising 50 queries. The subset size varies between 1 and 45.

As seen in Table 2, the required subset sizes for  ={0.7, 0.8, 0.9} are statistically significantly smaller than those required for random sampling. For instance, the random method obtains  = 0.9 for a subset of size 739 whereas the Adaptive method only requires 486 queries to reach the same  . This is equivalent to judging 12,950 fewer documents than the Random method. Similar results are observed for  ={0.7 and 0.8}.
7.2 Effects of Initialization
The initialization step of the Adaptive method involves randomly selecting the first query. We now consider the sensitivity of the Adaptive method to the selection of the first query. The choice of the first query could possibly affect the quality of both (i ) the queries selected in the subsequent stages and (ii ) the training data for the classifier. Our analysis focuses on (i ) since the impact on the training data cannot be separated from the characteristics of the classification method and thus is out of the scope of this paper.
In order to isolate the effects of the initial query on the subsequent queries, we assume that the true X matrix is available, i.e. that the estimator has access to all relevance judgments. Using the TREC-8 data set, we randomly select the first query. Subsequent queries are iteratively selected based on the query selection model in Equation 5 but using

907

kendall- correlation

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

random

oracle

0.6

Adaptive

Adaptive+

0.55 0%

10%

20%

30%

40%

50%

60%

70%

80%

subset size

Figure 4: The generalizability test for the query subsets selected by Adaptive and Adaptive+, the modified query
selection method.

the true X matrix where the corresponding uncertainty U matrix is zero. Results are shown in Figure 3 for 50 trials. Each trail contains a distinct query for the initialization. As seen, the Kendall- scores obtained in all the trials converge to the Kendall- for the ideal query set more quickly than the Kendall- obtained for Random. The kendall- variation across trials decreases as more queries are selected. For the query subsets of size greater than 10 queries the performance is similar across all the trials. This suggests that the query selection model is robust to the selection of the first query.
8. GENERALIZATION
We now consider the generalization of the Adaptive query selection method. In Section 8.1 we discuss the effectiveness of the resulting query set in evaluating new systems that did not contribute to the query selection process. In Section 8.2 we consider the reliability of the query subset for evaluating systems performance using multiple metrics. This is particularly important when systems are compared using various performance metrics.
8.1 Evaluation of New Systems
Previous work [14, 17] has shown that queries selected based on the performance of a particular set of systems may not be effective in evaluating new, previously unseen systems. We observed a similar tendency by the Adaptive query selection. Thus, to avoid over-fitting to the systems used to select the queries we modify the Adaptive algorithm.
The modified algorithm is referred to as `Adaptive+' and comprises the following changes. When selecting a query we consider c(c > 1) random subsets of the l systems of size h(h < l). We allow overlaps between the subsets and ensure that each system appears in at least one of the subsets. For each subset of systems we choose a query that, in combination with already selected queries in , maximizes  (see Equation 5). Finally, we pick the query that is selected by most of the systems subsets, and consider it for relevance assessments.
We test the generalization of the Adaptive+ approach using the two TREC test collections. We first randomly select 50% of systems and treat them as new systems. When selecting new systems, we hold out not only individual runs but the entire set of runs from the same participant. The remaining systems are considered as participating systems

Table 3: Comparing the generalization of a selected subset using two metrics: P @100 and AP .

Subset 20% 40% 60%

Method
Random Adaptive Adaptive+
Oracle Random Adaptive Adaptive+
Oracle Random Adaptive Adaptive+
Oracle

kendall-

Robust 2004

TREC-8

P@100 AP P@100 AP

0.77 0.80 0.75 0.78

0.76 0.82 0.76 0.80 0.84 0.87 0.81 0.85

0.91 0.95 0.86 0.89

0.82 0.87 0.84 0.85

0.80 0.85 0.84 0.86 0.89 0.92 0.90 0.90

0.93 0.97 0.94 0.93

0.89 0.91 0.89 0.90

0.84 0.88 0.87 0.91 0.93 0.96 0.95 0.95

0.96 0.98 0.97 0.97

and used to select queries. When computing the performance metrics for the participating systems, we also remove documents that are uniquely retrieved by the new (held-out) systems.
The results of the generalization test for the Robust 2004 test collection and Kendall- are shown in Figure 4. We created c = 100 random system subsets, each of size h = 0.2◊l, where l refers to the number of participating systems. Figure 4 clearly shows that the Adaptive algorithm overfits the set of queries to the participating systems and performs no better than Random when evaluating new systems. In contrast, Adaptive+ significantly outperforms Random across different sizes of the query subsets.
The detailed results of the generalization experiments are shown in Table 3 for both Robust and TREC-8 test collections, and two metrics, AP and P @100. In all cases the Kendall- obtained by Adaptive+ is significantly larger than the Kendall- of the Adaptive and Random algorithm.3
8.2 Use of Alternative Performance Metrics
One of the goals of IR test collections is to enable evaluation of systems in terms of various metrics. Ideally, the subset of queries used for systems evaluation should provide reliable estimates of the systems performance regardless of the metrics used. In the following experiments we show that for some methods that may not be the case. Namely, when the metrics used to select queries differs from the metrics used to evaluate systems, the query subset may not provide reliable estimates of the systems performance.
For that reason, we modified the Adaptive algorithm to generalize across multiple metrics. The modified version is referred to as `Adaptive*'. At each step of the query selection process, for each metrics, and for each non-selected query the Adaptive* computes the associated  scores. It averages the scores across different metrics and then selects the candidate query with the maximum average of  scores.
We consider four IR metrics: P @10, P @100, Recall and AP and measure the associated Kendall- scores (T1) for query subsets of various sizes selected. Let T2 be the set of Kendall- scores for various subset sizes calculated when the evaluation metric is different from the metric used for query selection ≠ the selection metric. Ideally the Kendall-
3Similar results were also observed for Pearson correlation but not reported due to lack of space.

908

Table 4: The average Kendall- loss mean(T2) - mean(T1) for four metrics using the TREC 2004 Robust track. Given a metric , T1 denotes the set of Kendall- scores for various query subset sizes when the metric  is used for both query selection and system evaluation. T2 denotes the set of Kendall- scores when the metric  is used to measure systems performance for a subset of queries selected using another metric.

Selection Metric
P@10 P@100 Recall
AP Adaptive* Random

Evaluation Metric

P@10 P@100 Recall AP

0.0

-0.082 -0.065 -0.068

-0.084

0.0

-0.042 -0.051

-0.076 -0.063

0.0

-0.073

-0.089 -0.070 -0.062

0.0

-0.011 -0.012 -0.018 -0.014

-0.114 -0.086 -0.056 -0.078

scores in T2 would comparable with those in T1. To measure the distances between T1 and T2 set of scores we use mean(T2) - mean(T1) , i.e. the average loss Kendall- .
Table 4 represents the results of our experiment for the Robust 2004 test collection. Each of the four metrics are used as a selection metric for the Adaptive method and as a system evaluation metric. The Recall metric leads to the smallest average loss for P @10 and P @100 but not for AP . The best selection metric for AP and Recall is P @100. Thus, there is no single selection metric that leads to the minimum loss in Kendall- for all of the evaluation metrics.
We also select queries by using the Adaptive* method. As seen from Table 4, the average loss for all the metrics is considerably reduced. The last row of Table 4 represents the results of random sampling averaged over 1000 trials. In order to investigate whether the Adaptive* method significantly outperforms other methods, we measured the differences in average Kendall- loss using the paired t-test at the significance level of 0.05. Table 4 shows that Adaptive* leads to average Kendall- losses that are significantly smaller than for the Random method and any of the selection metrics.
9. DISCUSSION
Our experiments demonstrated the advantages of the Adaptive query selection method and its variations. Here we consider how they may be practically used to support the query selection and assessment scenarios encountered in the TREC type experiments. In TREC tasks, a set of queries is typically selected in advance and delivered to assessors to collect relevance judgments. This setting ensures that all the assessors are efficiently involved in the construction of relevance judgments, and the full set of judgments is constructed in a reasonable time.
We first investigated if we can use Adaptive to collect a subset of queries before constructing any relevance judgments. We considered the use of the query performance predictors, e.g. [9, 18], to construct the performance matrix X^ and iteratively select a subset of queries in the absence of any relevance judgments. Since no relevance judgments are collected, the elements of X^ are fixed throughout the iterations. Once a set of queries are selected, the associated relevance judgments are collected concurrently.
We used the pseudo-relevance judgments approach proposed by Soboroff et al. [18] to construct X^ since (i ) it

directly estimates the performance metrics and provides the corresponding variance that is required for our model, and (ii ) it is reported to be among the best available performance predictors [11]. However, experiments with the Robust 2004 test collection showed that the accuracy of the performance prediction was not sufficiently high and, as a result, Adaptive failed to perform statistically better than the random sampling of queries.
We then took a different approach and modified Adaptive query selection to construct relevance judgments for multiple queries at a time. Instead of updating X^ at each iteration, we updated X^ after selecting a sequence of k  1 queries. Once k queries were selected, the associated relevance judgments were constructed and X^ matrix was updated.
We evaluated this approach on the Robust 2004 test collection with 249 queries and investigated the effect of size k on the performance of the method. We considered three configurations k = {1, 10, and 50} where the first k queries were randomly sampled. To deal with the sampling variance we repeated the experiments 50 times, every time with a new set of initial queries, and averaged the results across the experiments. For each of the configurations, we calculated Kendall- after selecting 60% of the queries. As the value of k increased the associated Kendall- decreased. However, for k = 10 the Kendall- was still statistically larger than the Kendall- obtained for the random sampling of queries. As k rose to 50, the Adaptive method performed no better than Random. These results suggest that for some k we can use our iterative method to collect relevance judgments for multiple queries at a time. However, further experiments will be needed to find the optimal setting for k across the iterations, and its relationship with the total number of queries.
10. CONCLUSION
In this paper we considered the task of selecting a representative set of queries and corresponding relevance judgments to achieve an optimal approximation of the systems performance. We started with the premise that no relevance judgments are available for a query before it is selected for the relevance assessment and that only a limited assessment budget is available. Thus, we provided a mathematical model for query selection that explicitly formulated the uncertainty in the performance scores that was due to the absence of relevance judgments. The mathematical formulation showed that the optimal subset of queries should be the least correlated with each other and maximally correlated with the remaining queries. Furthermore, the total uncertainty associated with the selected queries should be a minimum.
Since the optimization problem was intractable, we proposed the Adaptive algorithm in which queries were iteratively selected and their relevance judgments were obtained after they were added to the query set. The relevance judgments of the selected queries were used to train a classifier to facilitate the selection of subsequent queries.
We demonstrated the effectiveness of the Adaptive algorithm using two TREC test collections and a Web test collection of a commercial search engine. For all three test collections, the Adaptive algorithm significantly outperformed the considered baseline methods.
Generally, the query selection methods have been criticized for their lack of generalization to previously unseen systems and multiple evaluation metrics. Our Adaptive al-

909

gorithm exhibited the same problems. However, we refined the Adaptive algorithm and showed that the selected query subset provides effective evaluations of new systems and can reliably be used with multiple metrics.
One of the main advantages of our query selection model is its extendibility to accommodate different sources of uncertainty in measuring systems performance. In this paper, we focused on the uncertainty of the performance estimator due to lack of relevance judgments for a query. However, other sources of uncertainty could be considered. Recent research is particularly concerned with measuring uncertainty of the systems performance due to (i ) partial relevance judgments [2, 4, 20] and (ii ) errors in the relevance judgments made by human assessors [6, 12]. Our future work will expand the theoretical model to incorporate additional sources of uncertainty and explore more general cost models for constructing test collections.
11. REFERENCES
[1] J. Allan, B. Carterette, J. A. Aslam, V. Pavlu, B. Dachev, and E. Kanoulas. TREC 2007 million query track. In Notebook Proceedings of TREC 2007. TREC, 2007.
[2] J. A. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In SIGIR '06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 541≠548, New York, NY, USA, 2006. ACM.
[3] P. Billingsley. Probability and Measure. New York: Wiley, New York, NY, USA, 1995.
[4] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In SIGIR '06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 268≠275, New York, NY, USA, 2006. ACM.
[5] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation over thousands of queries. In SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 651≠658, New York, NY, USA, 2008. ACM.
[6] B. Carterette and I. Soboroff. The effect of assessor error on IR system evaluation. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, SIGIR '10, pages 539≠546, New York, NY, USA, 2010. ACM.
[7] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient construction of large test collections. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '98, pages 282≠289, New York, NY, USA, 1998. ACM.
[8] C. Cortes and V. Vapnik. Support-vector networks. Mach. Learn., 20(3):273≠297, Sept. 1995.
[9] F. Diaz. Performance prediction using spatial autocorrelation. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 583≠590, New York, NY, USA, 2007. ACM.

[10] J. Guiver, S. Mizzaro, and S. Robertson. A few good topics: Experiments in topic set reduction for retrieval evaluation. ACM Trans. Inf. Syst., 27(4), 2009.
[11] C. Hauff, D. Hiemstra, L. Azzopardi, and F. de Jong. A case for automatic system evaluation. In Proceedings of the 32nd European conference on Advances in Information Retrieval, ECIR'2010, pages 153≠165, Berlin, Heidelberg, 2010. Springer-Verlag.
[12] M. Hosseini, I. J. Cox, N. Milic-Frayling, G. Kazai, and V. Vinay. On aggregating labels from multiple crowd workers to infer relevance of documents. In ECIR'12: Proceedings of the 34th European conference on Advances in information retrieval, ECIR'12, pages 182≠194, 2012.
[13] M. Hosseini, I. J. Cox, N. Milic-Frayling, T. Sweeting, and V. Vinay. Prioritizing relevance judgments to improve the construction of IR test collections. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM '11, pages 641≠646, New York, NY, USA, 2011. ACM.
[14] M. Hosseini, I. J. Cox, N. Milic-Frayling, V. Vinay, and T. Sweeting. Selecting a subset of queries for acquisition of further relevance judgements. In Proceedings of the Third international conference on Advances in information retrieval theory, ICTIR'11, pages 113≠124, Berlin, Heidelberg, 2011. Springer-Verlag.
[15] S. Mizzaro and S. Robertson. Hits hits TREC: exploring IR evaluation results with network analysis. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 479≠486, New York, NY, USA, 2007. ACM.
[16] J. C. Platt. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In Advances in Large Margin Classifiers, 2000.
[17] S. Robertson. On the contributions of topics to system evaluation. In Proceedings of the 33rd European conference on Advances in information retrieval, ECIR'11, pages 129≠140, Berlin, Heidelberg, 2011. Springer-Verlag.
[18] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '01, pages 66≠73, New York, NY, USA, 2001. ACM.
[19] K. Sparck Jones and K. van Rijsbergen. Information retrieval test collections. Journal of Documentation, 32(1):59≠75, 1976.
[20] E. Yilmaz and J. A. Aslam. Estimating average precision with incomplete and imperfect judgments. In CIKM '06: Proceedings of the 15th ACM international conference on Information and knowledge management, pages 102≠111, New York, NY, USA, 2006. ACM.
[21] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In SIGIR '98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 307≠314, New York, NY, USA, 1998. ACM.

910

Improving Retrieval of Short Texts Through Document Expansion

Miles Efron
University of Illinois at Urbana-Champaign 501 E. Daniel Street, MC-492
Champaign, IL
mefron@illinois.edu

Peter Organisciak
University of Illinois at Urbana-Champaign 501 E. Daniel Street, MC-492
Champaign, IL
organis2@illinois.edu

Katrina Fenlon
University of Illinois at Urbana-Champaign 501 E. Daniel Street, MC-492
Champaign, IL
kfenlon2@illinois.edu

ABSTRACT
Collections containing a large number of short documents are becoming increasingly common. As these collections grow in number and size, providing effective retrieval of brief texts presents a significant research problem. We propose a novel approach to improving information retrieval (IR) for short texts based on aggressive document expansion. Starting from the hypothesis that short documents tend to be about a single topic, we submit documents as pseudo-queries and analyze the results to learn about the documents themselves. Document expansion helps in this context because short documents yield little in the way of term frequency information. However, as we show, the proposed technique helps us model not only lexical properties, but also temporal properties of documents. We present experimental results using a corpus of microblog (Twitter) data and a corpus of metadata records from a federated digital library. With respect to established baselines, results of these experiments show that applying our proposed document expansion method yields significant improvements in effectiveness. Specifically, our method improves the lexical representation of documents and the ability to let time influence retrieval.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.7 [Digital Libraries]: Systems Issues
General Terms
Algorithms, Experimentation, Performance
Keywords
Information retrieval, microblogs, twitter, Dublin Core, document expansion, language models, temporal IR
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08... $15.00.

1. INTRODUCTION
Collections of short documents present a host of challenges to information retrieval (IR) systems. The increasing influence of microblogging platforms such as Twitter1 makes this challenge especially timely. Twitter search exists alongside other short-text IR problems such as ranking product reviews and advertisement placement. These short documents join more familiar brief texts such as bibliographic and other metadata records.
Faced with a corpus of millions of documents, each of which contains only a few words, traditional IR models run into difficulty. First, the vocabulary mismatch problem becomes especially worrisome. If documents are very brief, the risk of query terms failing to match words observed in relevant documents is large. Second, most IR models rely on some sort of TF-IDF dynamic, with a term's frequency in a document lending evidence to our belief about the document's relevance. In very short documents, most terms occur only once, making simple operations such as language model estimation difficult.
However, we can mitigate these problems. We argue that a massive document expansion improves retrieval effectiveness for short texts. The mechanism that we propose for this expansion is simple. Because the documents that we are concerned with are so brief, each one tends to focus on only a single topic. A topically homogenous document is not very different from a query. Thus we propose submitting each document in a corpus as a pseudo-query. We show that analyzing the results obtained from this pseudo-query improves the models that underpin IR.
This paper proposes and tests two types of document expansion. Given a corpus of N documents C = D1 . . . DN and a document of interest D, we augment D's representation in the index by submitting the text of D as a query to a search engine over C. This gives a ranked listed of results R1 . . . Rk. This result set informs two expansions:
1. Lexical: We use the terms in R1 . . . Rk to improve our estimate of the language model for D.
2. Temporal: We use the timestamps associated with R1 . . . Rk to build a "temporal profile" for D≠a probability distribution over time. This probability distribution is helpful during time-sensitive document ranking.
Our chief contribution is a novel approach to representing collections of short text. Our methods elaborate on re-
1http://twitter.com

911

sults from the well-known language modeling approach to IR, especially the notion of relevance models. But we also show that the single-topic focus of most short documents opens avenues for consideration of information other than language≠for example, time.

TREC 8

Tweets2001

log p(w|D) -10 -8 -6 -4
log p(w|D) -7.5 -6.5 -5.5

2. PROBLEM STATEMENT AND MOTIVATION
The growing quantity of information published in small texts such as Twitter posts argues for a sustained analysis of the treatment of brief documents in IR. While this urgency is new, the need for improved retrieval of short texts is not. For instance, many digital libraries manage repositories of terse metadata records. While these repositories (such as the DCC repository we describe in this paper) are nominally searchable, the brevity of their metadata can frustrate effective IR. Other examples of short-text IR include query-specific advertisement and product review ranking.
But retrieval from collections of short documents is difficult for several reasons. First, brief documents attenuate one of the primary signals used by modern IR systems≠term frequency. Figure 1 can help us understand why this is the case. The figure shows data from two TREC collections (described in detail in Section 5). The TREC 8 data included a corpus of news articles. Tweets2011 is the collection of Twitter data used for the microblog track at TREC 2011. Figure 1 shows the distributions of the log-probabilities of query terms among the first 100 documents retrieved using a simple query likelihood model. That is, the points that comprise the distributions are the maximum likelihood estimates of log P (q|D) for each query term q.
Two problematic facts are clear from Figure 1. First, unlike the longer news documents, tweets lead to a distribution of query term probabilities that is strongly peaked. The majority of tweets that contain a query word only contain it once, and tweets are of similar length. Second, in the case of the TREC 8 data, the mean and median logprobability of a query term are higher in relevant documents than in non-relevant documents. This is not the case for the Twitter data. Median log P (q|D) for relevant TREC 8 documents is -6.92, with median -7.17 for non-relevant documents. The difference in log-probability in relevant versus non-relevant TREC8 documents is statistically significant (Mann-Whitney p 0.001 one-sided). But log P (q|D) for relevant Twitter documents has median -7.82 and -7.80 for non-relevant (Mann-Whitney p = 1 one-sided). To the extent that TREC 8 presents a "typical" statistical picture, Figure 1 shows that Twitter data are qualitatively different than more familiar TREC collections.
The impact of this difference is easy to see if we consider the language modeling approach to IR [21]. In the language modeling approach, we assume that each document in our collection was generated by a probability distribution≠a language model≠over terms in the vocabulary. We rank documents against a query Q by the likelihood that their corresponding language models generated the query:

P (D|Q)  P (Q|D)P (D).

(1)

Non Rel

Non Rel

Figure 1: log-Probabilities of Query Terms in Relevant and Non-Relevant Documents in Two Corpora.

If we assume uniform priors over documents and term independence we have:

|Q|

Y

P (Q|D) = P (wi|D)

(2)

i=1

where |Q| is the number of word tokens in the query.

Key to language modeling is the estimation of P (w|D).

Using multinomial language models, the maximum likeli-

hood estimator Pml(w|D) =

n(w,D) |D|

.

But when confronted

with a corpus of brief documents, estimating language mod-

els is difficult because the maximum likelihood estimator is

not very expressive. Since |D| is small, n(w, D) cannot be

large and is very often 1. We refer to this as the "headroom

problem:" in brief documents there is little chance for impor-

tant terms to stand out via repeated usage. Coupled with

a small domain of observed document lengths, the head-

room problem has the effect of making many documents'

estimated language models look nearly identical.

To improve our estimates, we typically smooth language

models, re-allocating probability mass away from the max-

imum likelihood estimator by reference to a background

model, such as P (w|C), the language model of the collec-

tion as a whole as described by Zhai and Lafferty in [28].

Though several smoothing techniques are common in the

IR literature, in this paper we select Bayesian updating with

Dirichlet priors. The form of a Dirichlet-smoothed language

model is:

|D|

µ

P (w|D)

=

|D| + µ Pml(w|D) +

P (w|C) |D| + µ

(3)

for µ  0. Smoothing in this way improves our estimated probabilities. But smoothing does little to alleviate the headroom problem.
Our goal is to improve the representation of short documents in IR. We argue that short text representation can be improved by an aggressive document expansion. In Section 4.1, we use this expansion to induce improved document language models. However, document expansion has a role to play in IR for short texts that goes beyond language model estimation. As an example of an alternative use of expansion information, Section 4.2 extends our results to allow temporal factors to influence retrieval.

912

3. RELATED WORK
The problematic nature of IR on short documents has seen little sustained research (though [22, 27] do treat the topic explicitly). However, recent interest in social media has drawn attention to this problem [22, 10]. Familiar operations such as measuring inter-document similarity, scholars have found, is difficult given the brevity of many documents in collections of user-generated content [20].
Though corpora of brief documents are increasingly common, the estimation problems that our proposed expansion methods address are not new. Most similar to our own work are results from research on cluster-based IR. Clusters have been applied to various points in the IR process, including relevance feedback [14, 9], rank fusion [12], and as a separate factor used during document ranking [13]. Clusters are appealing insofar as they afford a level of information about documents that resides at a level higher than intra-document word counts, but below the generalities of the collection at large. For instance, Liu and Croft applied clusters during document language model smoothing [18]. Ramage, Dumais and Leibling address the vocabulary mismatch problem in microblog text using latent Dirichlet allocation (LDA) [23], allowing the LDA model to supplement observed term probabilities in document representation. Other methods of document expansion have also seen sustained work (e.g. [24]), though in contexts different from those that we study here.
The most similar work to ours was given by Tao et al. [26]. In their work, Tao et al. proposed smoothing document language models by analyzing their lexical neighborhoods. That is, the model for document D was smoothed with counts obtained from its k nearest neighbors D1, . . . , Dk, with each document's influence in the smoothed model being proportional to its cosine similarity with D. Like Tao et al. we propose improving the representation D by an analysis of similar documents. However, the approach we outline in Section 4.1 differs from Tao et al. in its basic theoretical orientation, couching the estimation problem in the generative semantics of language modeling. Tao et al. define the neighborhood of a document in geometrical terms, relying on the cosine similarity. With an explicit assumption of normality, they smooth the model of D based on this neighborhood and this metric. On the other hand, we assume that D arises from an unseen model D, much as relevance modeling assumes the influence of an unseen model of relevance. To estimate P (w|D) we combine evidence from other documents, where the influence that some document Dj exerts on the final model is the likelihood that Dj's language model generated D.
Our work also relies on findings from recent studies on information retrieval and microblogs [6, 19, 2]. The field of microblog retrieval is relatively new, but has seen increasing interest, most noticeably in the 2011 TREC microblog track.
Unlike the methods described in this section, our document expansion technique also invites extension to nonlinguistic features. As we show in Section 4.2, aspects of relevance such as temporality fit naturally into our approach.

of each document in the collection, P (D|D1), . . . , P (D|DN ). We propose analyzing these probabilities to make an augmented representation D that provides a better basis for estimation and prediction than the original D affords.
From a practical standpoint, this amounts to submitting each document as a pseudo-query and ranking all documents using the standard query likelihood method. As we shall show, the analysis of P (D|D1), . . . , P (D|DN ) can play a role similar to relevance feedback, though how literally this comparison holds is variable.
With this in mind, we offer two definitions related to a document D:
Definition 1. Pseudo-query of D: The pseudoquery QD is a representation of the text in D that we may submit to an IR system.

Definition 2. Result set of D: By submitting QD as a query against the corpus C, we derive a ranking RD = RD1, . . . , RDk. RD consists of the top k documents retrieved for QD and their retrieval status value scores. In this paper we use query likelihood for retrieval, so the scores are the probabilities P (D|D1), . . . , P (D|Dk).

The use of QD and RD stems from our hypothesis that most short texts discuss only a single topic. If QD treats a coherent topical domain, we anticipate that analyzing the documents in RD will yield information about the topic that the document of interest D is about.

4.1 Lexical Evidence
Following Lavrenko and Croft's exposition of relevancebased language models [16], we use RD to induce D , a language model associated with D. In typical language modeling, we estimate a document's language model by combining evidence from the document itself with information from a background distribution such as the collection. Given RD, however, we can improve this estimate.
The language model D is

P (w|D ) = P (w|d1, . . . , d|D|)

(4)

= P (w, d1, . . . , d|D|) . P (d1, . . . , d|D|)

The denominator P (d1, . . . , d|D|) does not depend on w, leaving the joint probability as the quantity of interest. This joint probability is

X

P (w, d1, . . . , d|D|) =

P (Di)P (w, d1, . . . , d|D||Di). (5)

Di C

If we further assume that

|D|
Y P (w, d1, . . . , d|D||Di) = P (w|Di) P (dj |Di) (6)
j=1

then we have

|D|

4. DOCUMENT EXPANSION FOR SHORT

X

Y

P (w, d1, . . . , d|D|) =

P (Di)P (w|Di) P (dj |Di).

TEXTS

Di C

j=1
(7)

Let D be a document consisting of |D| word tokens d1, . . . , d|D|. The last factor in Eq. 7 reminds us that most of the prob-

Also let C be a corpus of N documents. Using to Eq. 2, we

ability mass in this joint distribution will derive from doc-

can calculate the likelihood of D given the language model

uments that are lexically similar to the document D. Thus

913

we can obtain a good estimate of P (w|D ) by performing the summation in Eq. 7 over only the k documents with the highest likelihood of generating D. Thus the probability of word w under the augmented model is a weighted average of the observed probabilities of w in the top k documents retrieved by submitting D as a pseudo-query, where the weights are the likelihoods of D given the retrieved documents' language models.
Having obtained our augmented representation, we may then rank documents by the likelihood that their augmented language models generated the query, P (Q|D ) which we calculate using the query likelihood model as usual, substituting P (w|D ) for the maximum likelihood estimator into Eq. 3. We refer to this method by the abbreviation LExp, for lexical expansion.
As is common when using relevance models for relevance feedback, interpolating the expanded model with the originally observed text is likely to be "safer" than relying on the expanded model alone. Thus we define another lexically expanded model:

P(w|D ) = (1 - )Pml(w|D) + P (w|D )

(8)

for a parameter  in [0, 1]. We may then substitute P(w|D ) for the maximum likelihood estimator in Eq. 3 for retrieval. We refer to rankings based on this estimator as LExp. For simplicity, when discussing LExp we set  = 0.5 throughout this paper.

4.2 Temporal Evidence
In addition to supplementing the lexical evidence that we store about documents, the expansion method described above can create new, extra-lexical features. For example, information in RD yields actionable information related to temporal aspects of relevance. Indeed, incorporating temporal evidence into IR entails a research area in its own right (cf. [1]). We pursue temporality here as an example of extra-linguistic information that our document expansion method allows.
We assume that for each document Di, we have a corresponding timestamp ti which is the time at which the document was published. We also define t0 as the earliest timestamp in the collection. In this paper we measure time in fractions of days, such that ti is how many days elapsed between the initial time t0 and the publication of Di.
The observed timestamp ti provides useful information for IR. But our goal is to learn additional temporal information about each document Di. We hypothesize that the empirical distribution of timestamps related to Di via document expansion helpfully supplements direct use of ti.
Working under a similar scenario, Jones and Diaz defined the notion of a query's "temporal profile" [11, 5]. The temporal profile of a query Q is a probability distribution over time, P (t|Q). Analyzing the empirical distribution of documents retrieved for Q gives information helpful in estimating P (t|Q), as shown in [4].
Our discussion above suggests that temporal profiles may be defined not only on queries, but also on documents. For each document D, we define a temporal profile P (t|D). This probability distribution expresses the extent to which D is associated with events that were discussed at various moments in time. Estimating P (t|D) follows the same logic that we use for query temporal profiles; we submit the text of D as a query, yielding RD = RD1, . . . , RDk. Each of

the documents Ri in RD contains a timestamp ti. From the empirical distribution of t1, . . . , tk we may estimate the underlying distribution P (t|D).
To estimate P (t|D) we imagine the following generative process. A person interested in D samples documents according to P (D1|D), . . . , P (DN |D). If this choice yields document Di with timestamp ti, we choose time t with probability P (t|ti). We let this probability follow an exponential distribution on |ti - t|.
The probability of choosing document Di in our generative process is simply the likelihood of D given Di. To derive a proper density, we normalize the k likelihoods in RD to sum to one:

si

=

P (Di|D)

=

P
j

P (D|Di)P (Di) RD P (D|Dj )P (Dj

)

.

We then have:

P^(t|D) = X si ∑ r ∑ e-r∑|ti-t|.

(9)

ti RD

The exponential rate parameter r governs the temporal influence; a large r strongly favors times near ti. It is worth noting that Eq. 9 is similar to a weighted kernel density estimate [25]. It allocates probability mass mostly around the timestamps of those documents that have a high likelihood of having generated D.
For each document and query we observe lexical evidence (i.e. their textual content) and temporal evidence. Let WQ and WD be the lexical representations of the query and a document, respectively, and TQ and TD be vectors of timestamps retrieved by submitting Q as a query or D as a pseudo-query. It is intuitive to rank on the joint probability P (WQ, WD, TQ, TD). If we assume conditional independence between lexical and temporal information and a uniform distribution over temporal profiles we have:

P (WQ, WD, TQ, TD)  P (WQ|WD) ∑ P (TQ|TD). (10)

This is simply the query likelihood multiplied by the likelihood that the timestamps returned by Q were generated by the same distribution that generated the timestamps observed in the result set of the pseudo-query for D. Using our estimate of the temporal profile of D, the likelihood of TQ given D is:

kQ
Y P (TQ|TD) = P (tQi|D)
i=1

(11)

where kQ is the number of timestamps in the query's temporal profile; we set this to kQ = 10. We then obtain a temporally informed retrieval by multiplying Eqs. 2 and 11. In the following discussion, we refer to this model as TExp (temporal expansion). Runs labeled TExp use documents' unexpanded text for WD. We will compare this method to a baseline using temporal document priors described by Li and Croft [17] (which we call TPrior). TPrior uses an exponential distribution on the age of documents as the prior in Eq. 2. Both methods≠TExp and TPrior≠rely on a rate parameter r for the exponential. Following Li and Croft, unless otherwise specified we set r = 0.01.

5. EXPERIMENTAL DATA
In the discussion that follows we rely mainly on two data sets. First, we use Tweets2011, the collection used for the

914

TREC 2011 microblog task. Second, we use a collection of metadata records describing holdings in a large digital library. Both of these collections consist mostly of brief documents, though as we shall show, they have many differences.
5.1 Microblog Data
The Tweets2011 collection uses a corpus of posts (called "tweets") made available by the microblogging service Twitter. Instead of distributing the microblog corpus via physical media or a direct download, TREC organizers distributed approximately 16M unique tweet ID numbers. Users of the collection downloaded these ID's, along with software that allowed them to fetch the tweets directly from Twitter. We obtained our data using the supplied HTML scraping tools on May 25≠26, 2011. Our corpus contained 15,653,612 indexable tweets, each containing: the screen name of the author, the time at which the tweet was posted, and the tweet text itself.
Our microblog data was preprocessed in several ways. First, in conformance with the track's guidelines all "retweets" were removed by deleting documents containing the string RT. Second, we made a simple pass at removing non-English tweets. We deleted tweets containing more than four characters with byte values greater than 255. We also defined a list of 133 words that are common in Spanish, French and German. Tweets containing any of these were removed. After these operations, the corpus contained 8,320,421 documents.
The 2011 microblog track involved a real-time search problem. Track organizers created 50 test topics2, each of which contained query text Q and a timestamp tQ. Only documents posted prior to tQ were assessed for relevance (all others are non-relevant). Aside from returning only documents published before query-time, for the sake of simplicity, in this paper we drop additional real-time strictures defined by the track organizers.
5.2 Digital Library Metadata Collection (IMLS DCC)
The Institute for Museum and Library Studies Digital Collections and Content (IMLS DCC) is a large, federated digital library offering access to the aggregated holdings of a broad spectrum of digital collections at distributed cultural heritage institutions3. In collaboration with IMLS DCC, we obtained a collection of 578,385 brief descriptions of cultural heritage resources. IMLS DCC administrators regularly harvest descriptions of several hundred participating institutions' digital resources via the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) [15]. Each of the 578,385 harvested documents is a Dublin Core metadata record describing a single item such as a photograph, manuscript, sound recording. Details of the process by which this corpus was built are given in [8].
To enable IR experimentation, we sampled 53 queries taken from the DCC search engine query logs. It must be stressed that these were not chosen at random. We used care in selecting queries to assure that no query returned zero relevant documents. Because many highly specific queries returned zero or very few items ≠ making changes in IR performance difficult to test ≠ queries with fewer than five
2NIST created relevance judgments using the standard TREC pooling method. Because it lacked relevant documents, one topic was removed from the final task. 3http://imlsdcc.grainger.uiuc.edu/

relevant documents (as gauged by the following process) in the collection were omitted.
Initial assessments of query suitability for our experiments were made by one of the authors, who is a former DCC administrator with significant experience in collection evaluation and user interactions. When choosing queries for inclusion on our test collection, this judge assessed potential relevance generously; if there seemed to be a reasonable chance that a document could satisfy a query, the document was considered potentially relevant. If a query accumulated at least five plausibly relevant documents, the query was included. Redundant queries and queries submitted by DCC administrators were also culled from the sample. Altogether we retained 53 queries.
We completed this collection by soliciting relevance judgments via Amazon's Mechanical Turk4 (AMT) service. To accomplish this, the same author wrote relevance criteria for each query. The process of writing relevance criteria is of course subjective and entails obvious limitations. However, without explicit knowledge of the information needs of people who created the queries, the author wrote these criteria to enhance the AMT workers' understanding of the queries and results without unduly limiting or biasing their sense of relevance. For example, relevance criteria for the query USS Monitor were given as, "Highly relevant documents should link to photographs or documentation about the Civil-Warera warship, the USS Monitor. Somewhat relevant documents will describe or provide photographic evidence of US Navy ships from the same era."
While the process of choosing and describing queries introduced subjectivity into our study, we pursued this method because we felt that using queries from the DCC's search logs was more realistic than writing queries of our own. Without attendant clickthrough data, this intervention was necessary to give AMT judges sufficient information for relevance assessment.
Documents to be assessed via AMT were gathered from pools created by running several retrievals over all 53 queries. Pools contained results from five of the conditions discussed in Section 6, as well as a run using BM25 ranking. All together, six runs contributed to the judging pools which were held to depth of 100.
Judgments were reconciled with two-thirds majority voting. After the first run of AMT ratings, we found that agreement was low with Fleiss kappa=0.24. However, the majority AMT ratings yielded a kappa=0.724 when compared to a set of oracle ratings by the study authors, suggesting that the problem of low inter-rater agreement was not systematic but due to scattered unreliable workers. To address this issue, we identified unreliable raters based on the proportion of each worker's judgments that were in agreement with the majority. Using an admittedly arbitrary agreement threshold of 0.67, we divided our workers into "good" and "bad" categories. 28 of 131 workers fell into the "bad" category. We removed all work done by these workers and re-submitted their tasks to AMT.
After the second judging round we recomputed reliability scores, this time finding only four bad workers whose sum proportion of ratings was 0.01. Only two workers who were good in round one became bad during round two. After round two, Fleiss kappa was 0.40. While this is lower than
4http://mturk.com

915

Table 1: Summary Statistics for Experimental Corpora. From left to right, columns give total number of documents, observed word types, observed word tokens, median document length, mean document length, standard deviation of document length, and the number of test queries.

Corpus

Indexed Docs Unique Terms Tokens

Median Doc Len Mean Doc Len SD Doc Len # Queries

Tweets2011 8,320,421

19,449,151

6,506,465,256 20

21.92

7.30

49

DCC

578,385

2,793,371

114,453,512 45

197.89

556.90

53

we would like, it marks a significant improvement over the initial work quality.
5.3 Data Statistics
Table 1 summarizes length-related aspects of our two test data sets. Not surprisingly, documents in the microblog corpus are very short (median=20, compared to median=328 in TREC 8), with a compressed distribution of lengths. Most DCC documents are a bit longer than microblog posts (median=45), and their lengths vary more than tweet lengths do. The corpora analyzed here are similar insofar as their documents are much shorter than documents in more standard IR collections. But the collections are also different from each other.
6. EXPERIMENTAL ASSESSMENT
All experiments were done using the Indri search engine and the Lemur toolkit5. For efficiency, we used the standard Indri stoplist and a custom Twitter-specific stoplist when forming document pseudo-queries. But subsequent retrievals used no stoplists or stemming except to mitigate common words' influence in the baseline relevance feedback condition≠feedback models were stripped of words in the Indri stoplist. Unless otherwise specified, all retrieval (both queries and pseudo-queries) used the Indri defaults of Dirichlet smoothing with µ = 2500. For each Tweets2011 query we retrieved 100 documents. We retrieved 50 documents per DCC query. To assess the merit of our proposed expansion methods we tested the conditions shown in Table 2.
6.1 The Effect of Lexical Expansion
Tables 3 and 4 list four effectiveness metrics for the conditions described above, on both of our test data sets6. The clearest result from the tables is the improvement over the QL baseline offered by our lexical expansion methods. Except for one case (P10 for LExp on DCC), the lexical expansion methods always outperform the QL baseline. The positive effect of document expansion is especially strong when we interpolated the expanded model with the observed document model (i.e the TExp condition).
We can see the positive effect of document expansion in Figure 2. Each panel in the figure schematizes the distribution of query terms in relevant and non-relevant documents, over a variety of data sets. Besides our two short document collections, as a point of reference we show the TREC 8 data described above and the small WT10g web collection. As we discussed earlier, the figure shows that query terms tend to
5http://lemurproject.org 6Reported metrics are mean average precision (MAP), Rprecision (Rprec), NDCG over all positions (NDCG) and precision at 10 (P10).

Table 2: Baseline and Experimental Retrieval Names and Parameters.

Baselines Experimental

Abbr. QL FB
TPrior LExp LExp
TExp LTExp TBoth

Details
Basic query likelihood. Dirichlet smoothing, µ = 2500 Relevance feedback using relevance models. 20 feedback docs; 15 terms. Interpolation with original query  = 0.5 Temporal priors to promote recent documents. Exponential rate parameter r = 0.01.
Lexical document expansion. k = 50 expansion documents. Lexical document expansion with linear interpolation of expansion model with MLE. k = 50 expansion. Expansion-MLE mixing proportion  = 0.5. Temporal document expansion. k = 50 expansion documents. Both lexical and temporal document expansion. i.e. A combination of LExp and TExp. Two types of temporal evidence are used: the prior of TPrior and the expansion method of TExp. No lexical expansion.

Table 3: Observed IR Effectiveness on TREC 2011

Microblog Data. The symbol indicates p < 0.05

on a permutation test against the baseline QL. The

symbol indicates p < 0.01.

MAP Rprec NDCG P10

QL

0.187 0.275 0.360 0.398

FB

0.189 0.273 0.361 0.394

TPrior 0.198 0.284 0.372 0.427

LExp 0.216 0.301 0.404 0.380

LExp 0.226 0.319 0.415 0.431

TExp 0.204 0.289 0.373 0.414

TBoth 0.206 0.289 0.378 0.427

LTexp 0.235 0.324 0.428 0.451

916

Figure 2: log Probabilities of Query Terms in Relevant and Non-Relevant Documents in Four Corpora.

TREC 8

WT10g

Microblog

Microblog (Exp)

DCC

DCC (Exp)

log p(w|D) -10 -8 -6 -4 -2 0
log p(w|D) -10 -8 -6 -4 -2 0
log p(w|D) -10 -8 -6 -4 -2 0
log p(w|D') -10 -8 -6 -4 -2 0
log p(w|D) -10 -8 -6 -4 -2 0
log p(w|D') -10 -8 -6 -4 -2 0

Non

Rel

Non

Rel

Non

Rel

Non

Rel

Non

Rel

Non

Rel

Table 4: Observed IR Effectiveness on DCC Data.

The symbol indicates p < 0.05 on a permutation

test against the baseline QL. The symbol indicates

p < 0.01.

MAP Rprec NDCG P10

QL

0.215 0.287 0.398 0.329

FB

0.205 0.256 0.366 0.300

LExp 0.227 0.292 0.414 0.304

LExp 0.302 0.359 0.502 0.402

Figure 3: Mean Average Precision Observed Over Varied Values of k. The x -axis is the number of documents used to fit the expansion model D'.
Twitter, LExp Twitter, LExpLambda DCC LExp DCC LExpLambda

Mean Avg. Precision 0.18 0.22 0.26 0.30

have a higher probability in relevant documents than in nonrelevant documents. The difference is especially stark for the standard (TREC 8 and WT10g) corpora. Observed query term probabilities in the unexpanded microblog corpus give no consistent evidence of relevance (among retrieved documents). The value of query term probabilities in the unexpanded DCC data is higher than in the Twitter data, but less so than in TREC 8 and WT10g. However, the panels labeled Microblog (Exp) and DCC(Exp) show query term probabilities in expanded document models. In these cases the estimated probability of query terms in relevant documents is significantly greater than the corresponding probability in non-relevant documents (Mann-Whitney p 0.001 one-sided).
A surprising result of our experiments is the poor performance of feedback using relevance models. Inspection of the expanded queries that led to these results showed that the induced query models contained very idiosyncratic terms such as user names (Twitter) and administrative vocabulary (DCC). We include the feedback results because it is worth noting that the expanded document models are capturing semantics that expanded query models cannot, given the conditions encountered in these data. We hypothesize that this effect arises because, in addition to alleviating the vocabulary mismatch problem, our expansion method yields more data for estimating language models. Expanded queries alleviate vocabulary mismatch and improve estimates of the query model, but expanded documents improve our estimates of the document models. In the context of short document retrieval, this effect seems to be crucial.
Figure 3 shows performance over a range of values (from 5 to 50) for k, the number of expansion documents used to fit an augmented language model D (cf. Eq. 7). Con-

10

20

30

40

50

Number of Expansion Docs.

trary to our expectations, there appears to be little risk in adding many documents to an expanded model. While a small decline in performance is visible for the LExp condition on the DCC data after k = 35, when we interpolate with the original model (LExp), the performance does not decrease over the range from 5≠50 documents. We suspect that this is due to the very small probabilities associated with documents deep in the retrieved set for D.
Our reliance on the retrieved sets of documents' pseudoqueries raises the issue of language model smoothing. How aggressively should we smooth when performing IR for document pseudo-queries? Table 5 shows that during document pseudo-query retrieval, previous studies of smoothing's critical role apply [28]. The table shows retrieval effectiveness statistics at three settings of the Dirichlet smoothing parameter µ during retrieval with document pseudo-queries. Our experiments use µ = 2500. This setting is the default in the Indri search engine (we chose it because it obviated the need for training data) and it has been shown to be effective. Table 5 shows that in this choice we were lucky. It is clear that 250 leads to under-smoothing, while 5000

917

Table 5: Comparison of Retrieval Effectiveness with Different Selections of µ, the Dirichlet Smoothing Pa-

rameter, Applied During Retrieval Based on Document Pseudo-Queries.

Tweets2011

DCC

Metric

µ = 250 µ = 2500 µ = 5000 µ = 250 µ = 2500 µ = 5000

REL RET 858

1032

796

441

661

633

MAP

0.1810 0.2155 0.1792 0.1702 0.2269 0.1990

Rprec

0.2764 0.3008 0.2584 0.2255 0.2919 0.2526

NDCG

0.3452 0.4034 0.3454 0.3310 0.4136 0.3787

P10

0.3735 0.3796 0.3694 0.2264 0.3038 0.2906

P30

0.3014 0.3190 0.2871 0.1987 0.2994 0.2780

is too high. All declines in MAP, Rprec, and NDCG from µ = 2500 are statistically significant. This suggests that document expansion carries some risk. We hypothesize that much of this risk comes from the conjunctive and disjunctive semantics of aggressive smoothing. Weak smoothing pushes retrieval towards a Boolean AND'ing of query terms, while strong smoothing allows the predominance of only a few query terms to promote a document. Balancing this economy is clearly important. However, it is also the case that Table 5 shows very extreme values for µ. Because computational constraints kept us from performing a full parameter sweep, we chose to test very high and very low smoothing parameters. In future work we plan to examine the "safe" region for smoothing in more depth.
6.2 The Effect of Temporal Expansion
Of our two data sets only Tweets2011 has a temporal component; the DCC documents lack any measurable chronology. Thus we tested the temporal expansion described in Section 4.2 only on the Twitter data. As a baseline we compared our method against the use of temporal priors introduced by Li and Croft. Effectiveness results for each condition are shown in Table 3. Both the baseline method (TPrior) and our temporal expansion (TExp) gave statistically significant improvements over the non-temporally informed QL baseline. However, the difference in effect between TPrior and TExp are small in Table 3, with both methods scoring nearly identically (the differences between them are not statistically significant).
Combining lexical and temporal expansion improves effectiveness further. in Table 3 the method using both expansion sources, LTExp, outperforms all other runs. For completeness, we compared LTExp against a run using the LExp condition modified with a temporal prior. LTExp improved on this baseline for all metrics shown in Table 3, with p < 0.05 for MAP and P10.
An interesting result is visible in Figure 4. The bar plot shows the difference in mean average precision (between TExp and TPrior) on a query-by-query basis. Though a few queries have near-identical scores under both TPrior and TExp, the majority of queries fare differently under each method. Both the baseline and proposed methods improve aggregate effectiveness on these data. But they appear to do so quite differently, suggesting that they are not interchangeable. The run in Table 3 labeled TBoth uses both temporal priors and temporal expansion. Though its performance is similar to temporal expansion alone, TBoth does see consistent improvement over TExp (p is 0.83, 0.12, 0.01, 0.15 for MAP, Rprec, NDCG and P10, respectively), sug-

TExp - TPrior

-0.15

-0.05

0.05

0.15

1 3 5 7 9 11 14 17 20 23 26 29 32 35 38 41 44 47
Figure 4: Comparison of Two Methods of Temporal Influence. Each bar is the difference in Mean Average Precision between TExp and TPrior (i.e. MAPT Exp-MAPT P rior on one the the 49 Tweets2011 Queries.
gesting that the prior introduces information distinct from temporal expansion.
In earlier work [7] it was shown that choosing the rate parameter r for the exponential distribution when applying temporal priors has a strong effect on retrieval. However, we found that a broad range of exponential parameterizations yielded little difference in performance from the data reported in Table 3. The same was true for the parameter in our temporal expansion; a broad sweep yielded no statistically significant change. We hypothesize that this robustness is due to the short temporal span of the Tweets2011 corpus (two weeks). If we repeated our experiments on collections spanning months or years, we anticipate seeing more sensitivity to setting r in both the baseline approach and our own.
7. DISCUSSION
In our previous exposition we treated temporal and lexical expansion as two distinct operations. But the same rationale and the same mechanism underpin both operations. In both cases, information gleaned from a document's result set is used to derive new information about that document. The influence of each member of the result set is proportional to P (D|Di). Thus in addition to lexical and temporal expansion, we could apply this operation on arbitrary features of documents. For instance, if each document had geolocation information, we could use the methods presented here to make an expanded location profile. Our main contribution is a way to create, for each document, an empirical distribu-

918

tion of features of whatever type. Our results suggest that for the feature types explored here (lexical and temporal), these distributions convey useful information for IR.
A notable advantage of the temporal expansion method proposed here is its flexibility with respect to the semantics of a query. For instance, the temporal prior method can account only for "recency queries" where the user seeks new information. However, our TExp approach assesses the similarity between the timestamps of documents retrieved by Q and those obtained from the pseudo-query of a document D. If Q retrieves documents clustered around a window of time in the past, TExp will reward documents whose pseudo-queries' results occupy the same window.
One issue that we have not addressed in this paper is the transformation of a document into a pseudo-query. In the interest of simplicity, we omitted any transformation other than removal of stopwords to improve retrieval speed. However, our results suggest room for improvement by a more thoughtful approach. In particular, we consider the difference between LExp and LExp on the DCC data evocative on this matter. Interpolation with the original document model helped retrieval to a great extent in this case, a fact emphasized by Figure 3. We suspect that this is due to the length of some DCC documents. Many of these documents are more verbose than, for example, tweets. These results suggest that longer documents do not lead to suitable pseudo-queries without some improvement.
8. CONCLUSION
We have proposed expanding document representations to improve retrieval effectiveness on corpora of very brief texts. Our contribution starts with the idea that short documents often make productive pseudo-queries. Because brief documents tend to discuss at most one topic, the retrieved set for a document D is informative with respect to making predictions about the relevance of D. We proposed two methods of document expansion based on analysis of document pseudoqueries' result sets. First, we augment the language model of D, obtaining an expanded language model D . Second, we induce a model of the temporal affinity of D by analyzing the temporal profile of D's result set. We find that the likelihood that this density generated the timestamps retrieved by a query Q yields an effective mechanism for letting time inform ranking.
In future work we plan to address several issues raised by this study:
∑ Self-retrievability and document priors. We hypothesize that the extent to which D is distinguishable from other documents in its result set is an indicator of topical coherence. This intuition could inform a prior over documents.
∑ Use of longer documents. Though short documents stand to gain significantly from expansion, there is no reason the gains we have seen in this study would not carry over to more standard collections.
∑ Query expansion vs. document expansion. Studying more conventional corpora will help us assess the differences between query and document expansion more thoroughly. We also plan to compare our approach to other document expansion methods such as Berger and Lafferty's translation model [3].

∑ Other settings. Short documents arise in many IR applications. Searching for relevant advertisements or product reviews are both domains that we believe could benefit from the approaches outlined here.
∑ Other expansion data. In future work we hope to examine the value of expansion methods for information such as geolocation data or user-created content.
∑ Scalable implementation. An efficient way to store and use document expansion data is crucial for our methods' viability in a production setting. In particular, future work will address the matter of changing documents' expansion data as the collection grows.
Despite these unexplored ideas, the results of this study show that document expansion is a compelling method for improving retrieval in corpora of short texts. As people's engagement with social media increases the pervasiveness of abbreviated documents, this problem will grow in importance. Considering short documents as pseudo-queries provides needed traction on this difficult task.
9. ACKNOWLEDGEMENTS
This work was supported by a Google academic research award and Institute of Museum and Library Services LG-0607-0020. Expressed opinions are those of the authors, not the funding agencies. The project is hosted by the Center for Informatics Research in Science and Scholarship (CIRSS).
10. REFERENCES
[1] Omar Alonso, Michael Gertz, and Ricardo Baeza-Yates. On the value of temporal information in information retrieval. SIGIR Forum, 41:35≠41, December 2007.
[2] Gianni Amati, Alessandro Celi, Cesidio Di Nicola, Michele Flammini, and Daniela Pavone. Improved stable retrieval in noisy collections. In Giambattista Amati and Fabio Crestani, editors, Advances in Information Retrieval Theory, volume 6931 of Lecture Notes in Computer Science, pages 342≠345. Springer Berlin, Heidelberg, 2011.
[3] Adam Berger and John Lafferty. Information retrieval as statistical translation. In SIGIR '99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 222≠229, New York, NY, USA, 1999. ACM.
[4] Wisam Dakka, Luis Gravano, and Panagiotis G. Ipeirotis. Answering general time-sensitive queries. IEEE Transactions on Knowledge and Data Engineering, 24(2):220≠235, 2012.
[5] Fernando Diaz and Rosie Jones. Using temporal profiles of queries for precision prediction. In SIGIR '04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 18≠24, New York, NY, USA, 2004. ACM.
[6] Miles Efron. Information search and retrieval in microblogs. Journal of the American Society for Information Science and Technology, 62(6):996≠1008, 2011.
[7] Miles Efron and Gene Golovchinsky. Estimation methods for ranking recent information. In Proceedings

919

of the 34th international ACM SIGIR conference on Research and development in Information, SIGIR '11, pages 495≠504, New York, NY, USA, 2011. ACM.
[8] Miles Efron, Peter Organisciak, and Katrina Fenlon. Building topic models in a federated digital library through selective document exclusion. In Proceedings of the Annual Meeting of the American Society for Information Science and Technology, 2011.
[9] Inna Gelfer Kalmanovich and Oren Kurland. Cluster-based query expansion. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR '09, pages 646≠647, New York, NY, USA, 2009. ACM.
[10] Giacomo Inches, Mark Carman, and Fabio Crestani. Investigating the statistical properties of user-generated documents. In Henning Christiansen, Guy De Tr, Adnan Yazici, Slawomir Zadrozny, Troels Andreasen, and Henrik Larsen, editors, Flexible Query Answering Systems, volume 7022 of Lecture Notes in Computer Science, pages 198≠209. Springer Berlin / Heidelberg, 2011.
[11] Rosie Jones and Fernando Diaz. Temporal profiles of queries. ACM Transactions on Information Systems, 25(3):14, 2007.
[12] Anna Khudyak Kozorovitsky and Oren Kurland. Cluster-based fusion of retrieved lists. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR '11, pages 893≠902, New York, NY, USA, 2011. ACM.
[13] Oren Kurland and Lillian Lee. Clusters, language models, and ad hoc information retrieval. ACM Trans. Inf. Syst., 27:13:1≠13:39, May 2009.
[14] Oren Kurland, Lillian Lee, and Carmel Domshlak. Better than the real thing?: iterative pseudo-query processing using cluster-based language models. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '05, pages 19≠26, New York, NY, USA, 2005. ACM.
[15] Carl Lagoze and Herbert Van de Sompel. The Open Archives Initiative: building a low-barrier interoperability framework. In JCDL '01: Proceedings of the 1st ACM/IEEE-CS joint conference on Digital libraries, pages 54≠62, New York, NY, USA, 2001. ACM.
[16] Victor Lavrenko and W. Bruce Croft. Relevance based language models. In SIGIR '01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 120≠127, New York, NY, USA, 2001. ACM.
[17] Xiaoyan Li and W. Bruce Croft. Time-based language models. In CIKM '03: Proceedings of the twelfth international conference on Information and knowledge management, pages 469≠475, New York, NY, USA, 2003. ACM.

[18] Xiaoyong Liu and W. Bruce Croft. Cluster-based retrieval using language models. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '04, pages 186≠193, New York, NY, USA, 2004. ACM.
[19] Kamran Massoudi, Manos Tsagkias, Maarten de Rijke, and Wouter Weerkamp. Incorporating query expansion and quality indicators in searching microblog posts. In Proceedings of the 33rd European conference on Advances in information retrieval, ECIR'11, pages 362≠367, Berlin, Heidelberg, 2011. Springer-Verlag.
[20] Donald Metzler, Susan Dumais, and Christopher Meek. Similarity measures for short segments of text. In Proceedings of the 29th European conference on IR research, ECIR'07, pages 16≠27, Berlin, Heidelberg, 2007. Springer-Verlag.
[21] Jay M. Ponte and W. Bruce Croft. A language modeling approach to information retrieval. Research and Development in Information Retrieval, pages 275≠281, 1998.
[22] Haoliang Qi, Mu Li, Jianfeng Gao, and Sheng Li. Information retrieval for short documents. Journal of Electronics (China), 23:933≠936, 2006. 10.1007/s11767-006-0044-2.
[23] Daniel Ramage, Susan Dumais, and Dan Liebling. Characterizing microblogs with topic models. In ICWSM, 2010.
[24] Nico Schlaefer, Jennifer Chu-Carroll, Eric Nyberg, James Fan, Wlodek Zadrozny, and David Ferrucci. Statistical source expansion for question answering. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM '11, pages 345≠354, New York, NY, USA, 2011. ACM.
[25] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Monographs on Statistics and Applied Probability. Chapman & Hall, Boca Raton, 1996.
[26] Tao Tao, Xuanhui Wang, Qiaozhu Mei, and Chengxiang Zhai. Language model information retrieval with document expansion. In Human Language Technology Conference of the North American Chapter of the ACL, pages 407≠414, New York, 2006.
[27] Fulai Wang and Jim E. Greer. Retrieval of short documents from discussion forums. In Proceedings of the 15th Conference of the Canadian Society for Computational Studies of Intelligence on Advances in Artificial Intelligence, AI '02, pages 339≠343, London, UK, UK, 2002. Springer-Verlag.
[28] Chengxiang Zhai and John Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems, 2(2):179≠214, 2004.

920

Extending BM25 with Multiple Query Operators

Roi Blanco
Yahoo! Research Barcelona, Spain
roi@yahoo-inc.com

Paolo Boldi
Dipartimento di Informatica Universit‡ degli Studi, Milano, Italy
boldi@dsi.unimi.it

ABSTRACT
Traditional probabilistic relevance frameworks for informational retrieval refrain from taking positional information into account, due to the hurdles of developing a sound model while avoiding an explosion in the number of parameters. Nonetheless, the well-known BM25F extension of the successful Okapi ranking function can be seen as an embryonic attempt in that direction. In this paper, we proceed along the same line, defining the notion of virtual region: a virtual region is a part of the document that, like a BM25F-field, can provide a (larger or smaller, depending on a tunable weighting parameter) evidence of relevance of the document; differently from BM25F fields, though, virtual regions are generated implicitly by applying suitable (usually, but not necessarily, positional-aware) operators to the query. This technique fits nicely in the eliteness model behind BM25 and provides a principled explanation to BM25F; it specializes to BM25(F) for some trivial operators, but has a much more general appeal. Our experiments (both on standard collections, such as TREC, and on Web-like repertoires) show that the use of virtual regions is beneficial for retrieval effectiveness.
Categories and Subject Descriptors
H.3.3 [Information Storage Systems]: Information Retrieval Systems
General Terms
Performance, Experimentation, Ranking
Keywords
Query processing, ranking, query segmentation, BM25
1. INTRODUCTION
Modern information retrieval ranking functions, like the ones used in today's search engines, employ a large number of features derived from different sources of evidence:
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

matches of query terms in documents, document query-independent quality measures, and click-through information among others. Prevalent among those signals, and central to most standard retrieval approaches such as BM25 and language models, is the term frequency (tf) information (i.e., the number of times a term appears in a document). Approaches derived from the probabilistic retrieval model are implemented as a summation of "weights" of the query terms that appear in the document, where the weight is essentially a normalized version of term frequency.
Traditional probabilistic relevance frameworks for informational retrieval [30] refrain from taking positional information into account, both because of the hurdles of developing a sound model while avoiding an explosion in the number of parameters and because positional information has been shown (somehow surprisingly) to have little effect on average [34]. Recently, though, it has been proved that considering sequences of terms that form query concepts is beneficial for retrieval [6]. Those extensions build up on the Markov Random Field retrieval model (MRF) and use a linear combination of different concepts and query-term scores in order to derive a final score for a document. Furthermore, the well-known BM25F extension of the successful Okapi ranking function (the latter of which we are aiming on building upon) can be seen as an embryonic attempt in the same direction: the basic idea there is that each document is made of regions (fields) and some fields may provide stronger evidence of relevance than others.
In this paper, we proceed along the same line, defining the notion of virtual region: a virtual region is a part of the document that, like a BM25F-field, can provide a different evidence of relevance of the document (the amount of evidence is, like in BM25F, expressed by a weight). Differently from BM25F fields, though, virtual regions are generated implicitly by applying suitable operators to the query: such operators may (and typically will) use positional information.
The techniques we propose can be seen as a two-stage ranking: in the first stage, a number of operators are applied to the incoming query to individuate virtual regions within the document; in the second stage, the regions are ranked much in the same way as with BM25F, using the weights attached to each operator. The idea is that there are "stricter" operators, that give a stronger evidence (e.g., "I want that at least three of the query terms appear in a span of at most ten words"), but are less likely to appear in document, and other that are "weaker" (e.g., "I want at least one of the query terms appearing somewhere", that is

921

the standard bag-of-words requirement) and contribute to recall.
The operator-based technique that we propose fits nicely in the usual eliteness model behind BM25 and provides a principled explanation to BM25F; it specializes to BM25(F) for some trivial operators, but we believe it has a more general appeal.
Abstracting from the positional aspect, we can think of our method as an attempt to understand more deeply the user's intent underlying a query [2], following the increasing interest in extracting features and learning about the query itself. For example, all major search engines are able to detect entities in queries in order to shortcut the user to an appropriate vertical (e.g., for e-commerce or news), to trigger different visualization schemes or simply to help ranking better the results that are being produced. The basic idea is that of being able to pre-process a plain set of query terms that a user submitted to build a model of the query (possibly with the help of contextual information, e.g., about the query session and/or the user's profile). This model might simply contain spelling corrections, term annotations or may exhibit more sophisticated expansions, obtained through gazetteers, synonym dictionaries or query-logs, just to name a few.
In general, we observe that information retrieval is moving from a document-centric to a query/user-centric approach, and modern search engines are investing large amounts of research in building better, more comprehensive query models. The question that we address in this paper is whether it is possible to extend the classical probabilistic formulation so as to accommodate in a natural way these extended query models for enhancing ranking, in both Web search and more classical TREC-like retrieval settings.
Contribution.
Summing up, this paper aims at proposing an extension of the probabilistic retrieval framework [30] that accounts for the information coming from queries and documents. Our approach can be seen as a principled way of integrating query-document features into a BM25-based model [32], by extending the event space using a number of operators that derive from a query model [7]. For instance, both BM25 and BM25F [33] could be regarded as a special case of the method presented here. The framework operates in a general manner by means of a set of operators that are materialized using query-derived information; each operator determines a (possibly empty) virtual region within the document, that is treated as a (weighted) field; query-term frequencies in each virtual region are used to compute the final score of the document. We shall frame our technique as an extension of BM25(F) and describe how it can be implemented efficiently. Finally, we provide experimental evaluation of our approach showing that the usage of operators outperforms state-ofthe-art ranking that use just matching of query terms and is especially helpful for difficult queries. The software implementing our technique and used for the experiments will be made available at http://mg4j.dsi.unimi.it/.
2. RELATED WORK
Some lines of research attempt at addressing the issue of adding semantic information to documents and queries [22]. The model that we present (which could indeed encode [22]

as a special case) can be seen as a template for grounding different graphical model instances, in the spirit of Markov Logic Networks [18, 29], even though in this paper we make no attempt to generalize the learning procedure of the probabilities involved in the model, and the inference we perform is restricted to one particular formulation and combination. However, the very structure of our method allows one to extend it to different combinations and aggregation functionals over probability distributions. Robertson et al. [33] introduced BM25F, a variation of BM25 that is able to deal with matches of query terms in different fields of the document, boosting them differently. Our framework stems from the same fundamental notions of BM25F and BM25 (term eliteness, re-weighting of term matches) and it is able to extend/accommodate both.
There are several recent papers that deal with spans of terms in ranking. Svore et al. [35] show that introducing spans of terms as a further feature for machine learning to rank model gives improvements over BM25. Other authors have dealt with the issue of incorporating these spans of terms into the language model framework, the first one being the Markov Random Field (MRF) model of Metzler and Croft [26], extending the language modeling framework for information retrieval [27, 39] to handle term dependencies. Some other approaches [11] compute the aggregated distance of matches and add it to the BM25 score, or define a kernel-like distance [23, 24] that can be successfully used for ranking by plugging it into a language-model divergence between the query and document estimation. One remarkable model close to ours is that of Bendersky et al. [5], who weight different query concepts using a parameterized combination of diverse importance features: those concepts can be single query keyword, phrases matching in the document, or matches of keywords that span a window of a certain size. The amount of matching concepts in the document are later integrated into the MRF ranking model. In our case we focus on extending BM25 and not the language modeling framework, the core difference being in the way information is aggregated for each term during ranking.
Besides taking spans into account, it may be beneficial to adopt some additional query segmentation technique, trying to grasp which words in a query should appear in shorter spans (or even consecutively), as successfully attempted in [28, 19].
3. GOALS AND GUIDING EXAMPLES
As explained in the introduction, the basic approach of this paper is to extract, from a given query, a number of regions in the document using suitable operators. Alternatively, you can think that a given input query is refined in a number of different ways using some refinement operators, that may (and typically will) use positional information; virtual regions are then the regions matching each of the refined queries.
As a concrete example, consider the following two operators:
∑ 1 requires that at least any two words in the query appear either consecutively or with an extra word between them;
∑ 2 just requires that at least one of the query words appears.

922

One could think of them as query-refinement operators; for example, using the query language syntax of MG4J [9], 1 applied to the query young nice girl gives rise to the query:1
(young nice)~3 OR (young girl)~3 OR (nice girl)~3.
When a document is considered against this query, all the areas where at least two of the three queried word appear either consecutively or one word apart are selected. This will determine a (possibly empty) sub-document, in which we can count the frequency of each of the three query words.
Operators 2, instead, applied to the same query will produce
young OR nice OR girl;
and would just extract the occurrences of either of the three query words from the document.
Clearly, a large frequency in the virtual region determined by 1 would be far more predictive of relevance than that determined by 2 (the latter would amount to actually counting the usual term frequency in the whole document).
To gain some experimental support for this intuition we performed the following experiment: we considered the topics 701-850 from the TREC GOV2 collection, and built queries using the words in their title. To each query, we applied three operators: the plain or operator (corresponding to the usual bag-of-words interpretation of the query), the 2-and operator (satisfied only by documents that contain at least any two of the query terms) and the 2-gram operator (satisfied by documents that contain two terms consecutively).
Then, for each matching document, we computed the frequency of the query terms within the virtual region and we determined if the document was relevant or not; the fraction of relevant documents is plotted against term frequency. Figure 1 shows the results of the experiment, and provides two fundamental evidences: first of all, stricter operators (2-grams, for instance) provide for the same term frequency a larger probability of relevance, as expected; secondly, the behavior shows in all cases the well-known phenomenon of saturation -- as the frequency increases, relevance also increases but at a slower and slower pace, giving rise to the typical sigmoid-shaped function.
Our approach is blind with respect to the operators being considered, which is part of its generality. In our experiments, among other operators, we employ a supervised phrase and entity detection algorithm and feed the different query chunks through the model in order to produce a document score.
Most previous works have addressed the combination of scores in a linear fashion: Bendersky et al. [5, 6] and Li et al. [20] focused on extensions of the language modeling framework. Linear combinations of features are able to bring increased performance; however, when taking into account evidence coming from the same source of information it is beneficial to understand the distributional properties of the signal the model has to deal with. In these cases, the information employed for ranking is always taken from the number of times one term or a sequence of terms matches a document. In contrast, if one was to incorporate other features, like query-independent document quality [17], or click-based information [1], a linear combination might be
1In MG4J, the ~ operator restricts matches to a span of words of a given maximum length.

0.0015

G or 2-gram 2-and

0.0010

P[relevant]

0.0005

0.0000

G G GGGGG

G

G

GGG

G GG

G

G G

GGGGGGGGGGGGGGGGGGGGGGGGGG

G

G

G GGGG
G
G

G G

0

10

20

30

40

50

tf

Figure 1: Term frequency versus probability of being relevant, depending on the operator applied to the query. Curves are obtained fitting the points through a spline with three degrees of freedom.

good enough, as long as the features integrated into the model are not correlated. Note however that (somehow surprisingly) even link-based features such as PageRank [10] and BM25 [32] turn out to be non-independent, given the presence of query terms in the anchor text of Web pages [17].
Not assuming feature independence is especially important when devising more complex ranking models that embody a large number of features, such as those employed in learned ranking functions [21]. In case of machine learning frameworks, this dependence is somehow captured by the complexity of learned functions, which in general might incorporate an over-engineering of features.

4. THE OPERATOR-BASED FRAMEWORK
Traditional probabilistic models, like BM25, assume that the relevance of a document to a query can be determined by aggregating individual contributions of the query terms. That is, given the binary random variable R representing relevance, and the vectors of random variables representing the document D and query Q, we want to rank documents according to their increasing odds-probability [30]. Here Q is (or can be thought of as) a set of terms, while Dt is a multi-state variable that encodes the features about the occurrence of term t in D (term frequency, position, etc.); we assume that those features contain a natural zero, corresponding to the absence of t and represented by 0. We let d and q denote two actual realizations of D and Q, and r (r) represent the event R = 1 (R = 0, respectively), i.e., the document being relevant (irrelevant, respectively). Then, within the probabilistic framework, documents are ranked

923

R

Et
tf t tq

Figure 2: BM25 plate diagram representing the assumptions over variable independence

according to p(r | Q = q, D = d), or equivalently

p(r | Q = q, D = d) p(r | Q = q, D = d)

q

p(D = d | r, Q = q) p(D = d | r, Q = q)

q

p(Dt = dt | r) tq p(Dt = dt | r)

q

log
tq,dt >0

p(Dt p(Dt

= =

dt dt

| |

r) r)

∑ ∑

p(Dt p(Dt

= =

0 0

| |

r) r)

q

wt,d,

tq,dt >0

where wt,d is the weight assigned for term t in document d, and you can think of dt as being the term frequency of t in d, later on denoted by tft,d.
In the derivation above, we assumed the terms to be conditionally independent, that is, p(Dt = x, Dt = y | r, Q) = p(Dt = dt | r, Q) ∑ p(Dt | r, Q) and p(Dt = x, Dt = y|r, Q) = p(Dt = dt|r, Q) ∑ p(Dt |r, Q) for any pair of terms t and t; this is a weaker assumption than term independence, in that we only require terms to be independent for each fixed query and relevance; this assumption is fundamental in practice, because it leads to tractable models, but it also has a deeper justification: as [16] proved, conditional term independence can be obtained when, for any given query, terms are statistically correlated but the correlation is the same on relevant and on non-relevant documents. Empirical evidence on retrieval performances of BM25 suggests that this indeed is often the case. For example, it is true that query terms New and York are correlated in relevant documents for the query New York pizza, but they are also correlated in the whole collection.
Further, the derivation imposes a vague prior assumption over terms not appearing in the query (p(Dt = x | r) = p(Dt = x | r) if t / Q). This can be weakened in the case of query expansion by explicitly linking unseen query terms to relevance.
The final arithmetic trick in the above derivation, known as removing the zeroes, is used to eliminate from the final score calculation terms that are not present in the document.

The determination of term weights in BM25 is based on the assumption that there is an Elite random variable, which can be cast as a simple topical model that perturbs the distribution of words over the text. That is [30], the author

is assumed first to choose which topics to cover, i.e., which are the elite terms and which are not. Furthermore, it is assumed that frequencies of terms on both the elite and the non-elite set follow a Poisson distribution, with two different means; in other words, for a given term t and for e  {0, 1} (denoting whether we are considering the term to be in the elite or not), there is a random variable Et,e that expresses the distribution of the frequencies of the (elite or non-elite) term t in a document, and Et,e  Poisson(t,e); clearly, we expect t,1 > t,0 (i.e., a single term will appear more frequently if it is elite than if it is not). Plugging this assumption in the general formula derived above determines a monotonic weighting function with an horizontal asymptote that may be interpreted as a form of saturation: the probability of relevance increases with term frequency, but the amount of increase is ultimately close zero when the frequency becomes large. In practice, this is well approximated by

wtB,dM25

=

t^f t,d t^ft,d + k1

∑ wtidf

where wtidf is the inverse document frequency for term t (that determines the asymptotic behavior when the frequency goes to infinity), k1 is a parameter and t^ft,d is a normalized term frequency with respect to the document length, i.e.,

t^f t,d

=

(1

tf t,d - b) + b ∑ |d|/avdl

,

(1)

where |d| is the length of document d, avdl the average length of documents in the collection, and b  [0, 1] is a tunable parameter. Putting things together, the weight derived from the 2-Poisson elite assumption wtB,dM25 is U BM25(t^ft,d) where

U BM25(x)

=

x

x +

k1

.

Regions and virtual regions.
In the following derivation we start again from the abovementioned estimation of

wtB,dM25

=

log

p(tf t,d p(tf t,d

= =

x x

| |

r) r)

∑ ∑

p(tf t,d p(tf t,d

= =

0 0

| |

r) r)

.

Here, the only features that we need to observe are term frequencies; this is a quite mild assumption and stems from the idea that documents are a single body of text with no structure whatsoever. Some earlier refinements of the probabilistic model, however, already introduced the idea that documents may have some structure. In BM25F [33], for example, a notion of region was introduced: each document is made up of regions, or fields, (e.g., title, body, footnotes etc.) and it is possible to observe term frequencies separately in each region. This extension accounts for the fact that some fields may be more predictive for relevance than others (for example, title will be more predictive than footnotes), and fits well in the eliteness model. The idea is that eliteness of terms is decided beforehand, for every given document, and it is the same across fields; term-frequency, instead, will be again modeled as in standard BM25, although it will be influenced by the length of each field--of course, shorter fields (such as title) are expected to contain more elite terms than longer ones.

924

As said, the present paper extends this idea by defining

what we call virtual regions. Ideally, suppose that you have

some way (as yet unspecified) to single out some parts of the

document that you know will provide high-quality informa-

tion about relevance; then, you may think of that area as a

single virtual region, and apply to that region the evaluation

described for BM25F. Those virtual regions are actually ob-

tained by the query itself, through a process that we shall

now describe.

An operator is a function that associates, to a given query and a given document, another document2 called a "virtual

region". We are given a set of such operators, each endowed

with a weight, { j, wj }jM: for each query q and doc-

ument d, we individuate the virtual regions j(d, q) (the

region of document d matching query q according to opera-

tor j), and for each of them we count the term frequency

of each term t  q in that virtual region; such a frequency is

denoted

by

tf

q,j t,d

(the

term

frequency

of

term

t

in

the

virtual

region of document d matching query q according to opera-

tor j); as it is customary in the RSJ model [31], we shall

omit

the

query

q,

and

just

write

tf

j t,d

.

To capture this idea, as with BM25F, we proxy the depen-

dence of the eliteness of the occurrences using a set {j}jM

of Bernoulli random variables, which reflect the probability

of occurrences in each region generated from a particular

operator. De Finetti [8] proved that any set of exchangeable

random variables has a representation as a mixture distribu-

tion, in general an infinite mixture. Therefore we represent

the operators as

p(tf

j t,d

=

x

|

r)

=

p(tf

j t,d

=

x

|

e,

r)p(e

|

r)

=

e{0,1}

p(tf

j t,d

=

x

|

j

=

, e, r)p(j

=



|

e, r)p(e

|

r)

qQ e,{0,1}

and

similarly

for

p(tf

j t,d

= x | r).

For

x

> 0,

the

first

of

the

three factors in the summation is zero if  = 0, otherwise it

is xt,e ∑ e -t,e/x! (for the Poisson-mixture assumption). The

second factor for  = 1 is just j, the parameter that governs

the j-th Bernoulli j. The last factor is the probability that

the document is elite for the term if it is relevant.

Let us write  (µ) for t,1 (t,0, respectively) and let p (q) be the probability that a document is elite for the term,

given that it is relevant (irrelevant, respectively).

So

p(tf

j t,d

=

x

|

r)

=

x x!

e

- j

p

+

µx x!

e

-µ j

(1

-

p),

and similarly

p(tf

j t,d

=

x

|

r)

=

x x!

e

-

j

q

+

µx x!

e

-µ j

(1

-

q).

Now following Robertson [30], we can divide both probabilities by xe -/x! getting

p(tf

j t,d

p(tf

j t,d

= =

x x

| |

r) r)

=

j p + j q +

µ  µ 

x e -µ(1 - p) x e -µ(1 - q) .

Observe that, since  > µ, the latter tends to p/q as x  , as in [30].
2As explained in the next section, technically the operator produces a set of queries that is then matched against the document to obtain a virtual region, but the difference is immaterial for the moment.

R

Et

j

tf

j t

jM tq
Figure 3: Plate diagram with the extended event space.

The treatment for the case x = 0 is slightly more involved, because

p(tf

j t,d

=

0

|

r)

=

e -j p

+

e -µj (1

-

p)

+

(1

-

j )

and similarly for the case of irrelevant documents: the last summand depends on the fact that a term can have zero occurrences in a region simply because of j; so

p(tf

j t,d

p(tf

j t,d

= =

0 0

| |

r) r)

=

e -j q + e -µj (1 - q) + (1 - j ) e -j p + e -µj (1 - p) + (1 - j )

or equivalently

e µ-q + (1 - q) + e µ-p + (1 - p) +

1 j
1 j

-1 .
-1

This term behaves like (1 - q)/(1 - p), under the usual assumption [30] that µ -  is small, and assuming further that j is sufficiently close to 1.

4.1 Implementation of the system
We observe that our underlying retrieval system has a rich query language, the set of whose queries is denoted by Q. Every document d can be matched against the query q  Q producing a sub-document M (d, q) (i.e., a subsequence of the words the document d is made of).3 This function is naturally extended to sets of queries, by letting M (d, A) be the union of all sub-documents M (d, q) for q  A.
Conversely, a raw query is just a sequence of terms r = t1, . . . , tu : this is what we suppose that the user inputs to the system; the set of all raw queries is denoted by R.
An operator is a function  : R  2Q mapping a raw query to a set of queries. Here are some simple examples of operators that we will be using in our experiments:

3The exact semantics of the match depends on the query language and on the retrieval system used and will be not described further, but see [9, 13] for two examples.

925

∑ bag-of-words: maps a raw query t1, . . . , tu to the set of queries whose element are the single terms (i.e., to {t1, . . . , tu});

∑ p-grams: maps a raw query t1, . . . , tu to the set of all p-grams of p consecutive terms in the query (i.e., to {"t1 ∑ ∑ ∑ tp", "t2 ∑ ∑ ∑ tp+1", . . . , "tu-p+1 ∑ ∑ ∑ tu"});

∑ p-AND: maps a raw query t1, . . . , tu to the set of all conjunctions of p arbitrary terms in the query (e.g., for p = 2, to {ti AND tj | i = j});

∑ phrasal: maps a raw query t1, . . . , tu to the single phrasal query "t1 ∑ ∑ ∑ tu";

∑ segments: maps a raw query to the set of consecutive terms that make up a concept (see Section 5 for a complete description).4

Let now { j, wj }jM be a set of operators and weights;

for a fixed raw query r =

t1, . . . , tu

,

let

tf j
t,d

be

the

number

of occurrences of term t in M (d, j(r)). The average term

frequency of term t in document d is then defined to be5

t^f t,d

=

jM

(1

-

bj

wj ∑ tf ) + bj

j t,d
∑ |d|/avdl

.

The score of document d for query q is then computed as

tq

t^f t,d t^ft,d + k1

∑ wtidf ,

where the latter is the standard term idf.

BM25(F) as a special case.
Note that using a single bag-of-words operator reduces our scoring formula to the usual BM25 score. Conversely, suppose your collection has G fields, and let 1, . . . , G are operators that work like a standard bag-of-words, but where i tries to find matches only in field i. So, for example, M (d, title(t)) would return the sub-document of the title made only by the occurrences of term t. Then, an application of the above formula would reduce to the standard BM25F score.
4.2 Remarks and variants
BM25, following the original probabilistic relevance framework, adopts a disjunctive semantics, that is, there is no need for a document to contain every query term in order to receive a non-zero score. The key point behind this assumption is that we need an external mechanism to decide which eliteness models we want to take into account for each query, and this will simplify the number of estimations and scores we need to compute. After that, it is important to
4Both the p-gram, the phrasal and the segment operators can be endowed with an enlargement factor that allows for some extra word to sneak in--also this point will be fully explained in Section 5. 5The length-normalization factor here might actually be different for each virtual region, but this solution turns out to be extremely expensive to implement, because the average region length is unknown unless the whole collection is examined. A good approximation can be obtained by using the standard document length as a measure of "verbosity" of all the virtual regions it contains.

decide what is an appropriate shape of the functional estimating relevance probability as a function of term scores: in Section 3 we showed empirically that conjunctive and proximity operator produce the same shape as in the 2-Poisson eliteness model.
An issue raised by our model, and that we must take into account, is the fact that the very same occurrence of a term within a document will be counted more than once, because virtual regions (differently from BM25F fields) may overlap. For instance, if we want to score separately matches of stemmed query terms and matches of unstemmed query terms we would be double-scoring some of the occurrences. This remark calls for discounting signals coming from the same source; one way to obtain this result would be to establish some dependence between the operators j: in the example above, we might correct the estimation using a p(sjt | ejx, r) correction factor (here, and in the following, the superscripts st and ex stand for "stemmed" and "exact", respectively).
In practice, however, we can avoid this estimation by recalibrating the different weights. We empirically know that both exact and stemmed matches should contribute to the score on which the saturation function is applied, and we want to aggregate those contributions together, using the proper weights. We can then correct the double counting and substitute accordingly in equation (1) as

t^f t,d

=

wex

∑

tf

ex t,d

∑

wiedxf

1 - b0 + b0 ∑ |dex|/avdlex

+

wst

∑

tf

st t,d

∑

wisdtf

1 - b1 + b1 ∑ |dst|/avdlst

Some observations about possible variants of our model are worth being remarked here:

∑ We could have used a different saturation function at

the eliteness level; in fact, as with BM25, we could in

principle learn the real function shape using an appro-

priately large dataset (but we would run the risk of

over-fitting, though). The one we adopted t^f/(t^f + k)

is appealing: it resembles a logistic function passing

through the origin and when the class conditional dis-

tributions p(x|r), p(x|r) belong to the same exponen-

tial family, the log-odds ratio of the class posteriors

log

p(r|x) p(r|x)

will belong to a logistic family [4] (see also

[25] for a connection of the log-logistic model and the

term frequency normalization of BM25).

∑ We could have tackled the problem of incorporating positional information within the probabilistic framework by coming up with a higher-order model adding the positions of terms as variables; as noticed, though, this approach would lead either to an ad-hoc kernel-like method or to an exponential number of parameters to estimate. Given the scarcity of publicly available training data, and for the sake of domain transparency, we believe that the proposed framework provides an acceptable solution and a good trade-off between learning requirements, complexity and performance.

∑ The operators that form the basis upon which our system builds can be introduced in many ways: a set of operators can be fixed and applied silently to all queries introduced by the user, as a form of multilevel enrichment (this is what we are assuming in the rest of the paper), or it can be defined externally on a

926

per-query basis, or on a per-query-type basis; we may instead think that it is the user herself to introduce operators in the query, which may be sensible and may find applications in some contexts where users are expected to adopt a richer query language. Finally, it is certainly possible to mix the two approaches, having some operators introduced directly by the user and others added automatically by the system.
5. QUERY CONCEPT SEGMENTATION
Albeit today's search engines offer a limited number of operators (phrasal, proximity etc.) most users tend to stick at introducing plain queries, typically a sequence of few words (about 3.08 on average, according to recent studies [36]). Nonetheless, as observed previously [37], many queries are actually made up by some basic conceptual units, each of them being formed possibly by many words. For example, the query indian summer victor herbert is clearly formed by two conceptual elements ("indian summer" and "victor herbert", the former referring to a meteorological phenomenon, the second to a person); breaking such conceptual units apart, or inverting the order of the words that label them, would produce information loss--of course, many documents will contain both the words "indian" and "summer" without referring to "indian summer"; also "summer indian" will also probably appear in some document, for example in reference to summer indian food, without any relevance to the concept sought. In some cases, it may be wise to allow for one spurious word to be inserted within a segment (so that, for example, san jose airport can also match the sequence "San Jose international airport", or george bush match "George W. Bush").
Among our goals, we would like the model to accommodate for query term-dependence, or concept-detection, at the query level. For example, if there is evidence that the query san jose aiport consists of two concepts, one referring to a city and the other one denoting a place or an action, we would like the model to be able to weight differently documents that only match one concept from those that match both, taking also into account the distance between the terms making up the concepts. To this end, one of our operators will employ query segmentation, an emerging NLP task that aims at identifying sub-sequences of strings that refer to a single unit or concept [7, 37], as described above. There have been limited attempts to integrate segments into ranking [5, 6, 20] in the context of the language modeling framework [39], but to the best of our knowledge this is the first time that a similar attempt is being applied to BM25-like techniques.
In the experimental section, we use both a segmentation operator based on generative language models and Wikipedia (as described in [37]) and a simple operator extracting pgrams of consecutive terms: the latter is of course less precise (because some p-grams do not correspond to concepts), and is given a lower weight -- it serves the purpose of identifying possible word-sequences that for some reason the segmentation algorithm failed to guess. Moreover, for both types of operators we allowed for a variety of amount of spurious words appearing in the segment: this is obtained by introducing different enlargement factors µ (a multiplicative factor determining the maximum allowed ratio between the length of the span found and the length of the segment);

an enlargement factor µ = 1 corresponds to accepting only exact matches, whereas for example allowing for an enlargement µ = 2 corresponds to accepting at most one extra word for every single word in the segments (e.g., if the segment is made up of two words, it is still acceptable if we find them two words apart). Different enlargement factors are used in the experiments, of course assigning larger weights to smaller enlargements.

Collection TERA04 TERA05 TERA06 WEB WEB-Phrasal

Size 436G 436G 436G 100G 100G

Documents 25M 25M 25M 10M 10M

Topics TREC 701-750 TREC 751-800 TREC 801-850 1000 400

Table 1: Data collections

6. EXPERIMENTAL RESULTS
We tested the usefulness and accuracy of the operators described in Section 4.1 and 5 in a series of experiments. We posit that query segments will only affect to a limited number of queries in the data-set; however the working question is whether combining them has some positive retrieval effect or not. Retrieval performance is optimized iteratively using MAP as a target metric. We sweep one parameter at a time over the allowed parameter range, holding every other parameter fixed, in a similar fashion to the method of Metzler and Croft [26]. Each operator introduces two parameters: a weight wj controlling the relative importance of the operator, and a factor bj determining the impact of termfrequency normalization with respect to the corresponding operator. Different enlargement factors µ (for segments, pgrams and phrasal operators) are reported in separate rows of the table, as well as different values for the number p of consecutive terms that are considered while building pgrams.
We report on MAP and P@10 and check for statistical significance using a one-sided t-test with p < 0.05. The operators are trained on two different Web collections: GOV2, TREC's Terabyte track collection [14, 15, 12], and a subsample of the Web from 2011. The collections are described in Table 1. Training and testing is performed on different topic sets; for TREC topics, we train and test on different years (train on TERA04, test on TERA05/06; train on TERA05, test on TERA04), and for the Web collection we perform 10-fold cross validation across the whole topic set. For the TREC collection, we used the topic title as raw query.
Table 2 presents the results of applying one single operator, combined with BM25 (that is, with the bag-of-words operator). Experiments show that all the operators are able to improve the performance on their own to a reasonable extent. The impact of single operators, specially the segments, is limited. However, it is worth noting that the methods using a single operator that restrict the semantics of the matching (like AND or segments) perform comparably to the best values reported at TREC for early precision (P@20) [14, 15, 12]. Table 3 presents the results of combining segment and p-gram operators with BM25 and BM25F. The different operators have been chosen using various enlargement factors; we further enriched the segment combination with a 2-gram

927

BM25 BM25F p-AND phrasal segment segment segment p-grams p-grams

p=2 µ=3 µ=1 µ = 1.5 µ=3 p = 2,µ = 1 p = 3,µ = 1

TERA04

MAP

P@10

0.2648 0.5327

0.2697 0.5510

0.2679 0.5429

0.2673 0.5306

0.2685* 0.5143

0.2695* 0.5347

0.2690* 0.5143

0.2786* 0.5530

0.2670 0.5060

P@20
0.5071 0.5143 0.5286 0.4939 0.4970 0.5010 0.5204 0.5120 0.4950

TERA05

MAP

P@10

0.3228 0.6140

0.3284 0.6200

0.3396* 0.6260

0.3369* 0.6060

0.3274 0.6080

0.3272 0.6140

0.3295* 0.6300

0.3294 0.5860

0.3272 0.5980

P@20
0.5600 0.5570 0.5920 0.5790 0.5580 0.5620 0.5840 0.5470 0.5560

TERA06

MAP

P@10

0.2928 0.5380

0.2935 0.5460

0.3069 0.5900

0.3082* 0.5720

0.3180* 0.5860

0.3122* 0.5940

0.3277* 0.5940

0.3137* 0.5860

0.3198* 0.6160

P@20
0.5140 0.5160 0.5300 0.5290 0.5350 0.5460 0.5460 0.5470 0.5600

Table 2: Performance of single operators (* = statistical significance at p < 0.05 using a one-sided t-test with respect to BM25, MAP only).

operator. The purpose of this experiment is to determine whether the sigmoid-like term-frequency normalizing function is able to accommodate for different features which stem from the same source of evidence (matches of the query term in the document). Results are significantly better than the baseline and outperform state-of-the-art ranking functions that just use matching of query terms (note we are not adding query-independent evidence, like link information, click-through data, etc.). For instance, the best MAP value for TREC 2004 at TERA04 [14] was 0.2844, the sequential dependence model of the Markov random field for IR peaks at MAP 0.2832 [26] and the two-stage segmentation model of Bendersky et al. had an average MAP of 0.2711 over the 150 topics [5].
In order to explore further the usefulness of query segmentation and p-grams for difficult queries, we selected a subsample of 1000 queries taken from Yahoo! Search; the data corpus was a 100GB sub-sample of the Web. The queries had been evaluated by a trained editorial team, with about 130 judged documents per query on average: relevance was assigned on a 4-level scale, from Bad to Excellent; given that we had graded relevance available, we report on NDCG (gain values at the relevance level, from 0 to 4) and MAP. The average query length was 3.14. We report the performance of BM25 and BM25F as baselines. In addition, we selected a sub-sample of 400 queries where the user herself had employed query segments (i.e., phrasal queries), and removed the segmentation information. This experiment was aimed at establishing if a more elaborate query interpretation mechanisms is able to be of help in these cases.
Table 4 shows that segmentation and p-grams, when mixed with the operator combination are able to improve the performance of a large number of Web queries. When looking at the differences between the regular and phrasal queries, we observe that gains, even if consistent over the two different sets, are slightly higher in the second group ( 12.3% vs.  7.1% in MAP for p-grams vs. BM25F). This fact indicates that the method is helpful for queries that contain difficult concepts, as they have been expressed by users manually, whereas maintaining the performance of queries in which identifying concepts is not so critical and standard bag-of-word approaches perform as well.
Anecdotal study.
It is useful to look into the reasons behind the increased retrieval performance we observed in our experiments; with this goal, we considered separately the mean average pre-

cision on each of the TREC topics 701-850 and examined the ten queries that produced the largest difference in precision between our system and the BM25 baseline. Table 5 shows the queries that obtained the greatest benefit from the use of segmentation and/or p-gram operators: in the rightmost column of the table, we identified the segments (or the p-grams) that most contributed to the increased precision, allowing to retrieve relevant documents that BM25 missed (because they were ranked too low).
7. CONCLUSIONS AND FUTURE WORK
In this paper we proposed a way to extend the probabilistic relevance framework with a notion of virtual region based on the use of operators applied to the query. Our experiments (both on standard collections, such as TREC, and on other Web-like repertoires) show that the use of virtual regions is especially beneficial for hard queries where positional information is actually precious. The method has room for improvements and further study should be undertaken to understand which operators are more useful and under which circumstances. Even if we explored a reasonable number of combinations, we have not made any systematic attempt to develop a method to select the individual best operators; we just limited ourselves to handpick a few--it was out of the scope of this paper to analyze automated operator selection. In contrast, a machine learning approach [3] would derive features for as many operators as possible and try to combine them optimizing a loss function of MAP or NDCG [38]. One might even envisage the adoption of a query-classification tool to decide which operators should be used, based on the presumed nature of the query. In any case, our experiments show the practical usefulness of the non-linear operator score combination for retrieval [33]. As a final remark, it is interesting to observe that most of the studied operators (actually, all of them except for segments) do not employ any source of external information, and still produce a significant performance improvement.
8. REFERENCES
[1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. Learning user interaction models for predicting web search result preferences. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, volume 1, pages 3≠10. ACM, 2006.
[2] R. Baeza-Yates. Query intent prediction and recommendation. In Proceedings of the fourth ACM

928

BM25 BM25F BM25 + p-grams BM25 + segment BM25F + p-grams BM25F + segment

TERA04

MAP

P@10

0.2648 0.5327

0.2697 0.5510

0.2898* 0.5939

0.2866* 0.5653

0.2908* 0.5959

0.2869* 0.5653

P@20
0.5071 0.5143 0.5531 0.5306 0.5541 0.5306

TERA05

MAP

P@10

0.3228 0.6140

0.3284 0.6200

0.3368* 0.6260

0.3285 0.5980

0.3398* 0.6280

0.3282* 0.5920

P@20
0.5600 0.5570 0.5800 0.5710 0.5830 0.5510

TERA06

MAP

P@10

0.2928 0.5380

0.2935 0.5460

0.3361* 0.6000

0.3188 0.5720

0.3319* 0.5940

0.3315* 0.5940

P@20
0.5140 0.5160 0.5048 0.5100 0.5350 0.5350

Table 3: Performance of a combination of operators (* = statistical significance at p < 0.05 using a one-sided t-test with respect to BM25, MAP only). Configuration for p-gram is p = 2, µ = {1, 2, 3}, and for segment is µ = {1, 2, 3}.

BM25 BM25F BM25 + p-grams BM25 + segment BM25F + p-grams BM25F + segment

MAP
0.1553 0.1722* 0.1822* 0.1810* 0.1854* 0.1842*

WEB NDCG 0.2728 0.3008 0.3126 0.3126 0.3216 0.3190

P@10
0.3892 0.4285 0.4390 0.4344 0.4400 0.4392

WEB-phrasal

MAP

NDCG P@10

0.1350 0.2345 0.3133

0.1575* 0.2842 0.3747

0.1750* 0.3078 0.4010

0.1725* 0.3029 0.4040

0.1769* 0.3123 0.4101

0.1749* 0.3165 0.4005

Table 4: Performance of a combination of operators on the WEB collection (* = statistical significance at p < 0.05 using a one-sided t-test with respect to BM25, MAP only). Configuration for p-gram is p = 2, µ = {1, 2, 3}, and for segment is µ = {1, 2, 3}.

conference on Recommender systems, pages 5≠6. ACM, 2010.
[3] B. Bai, J. Weston, D. Grangier, R. Collobert, K. Sadamasa, Y. Qi, O. Chapelle, and K. Weinberger. Learning to rank with (a lot of) word features. Information Retrieval, 13:291≠314, 2010. 10.1007/s10791-009-9117-9.
[4] A. Banerjee. An analysis of logistic models: exponential family connections and online performance. In SIAM International Conference on Data Mining, 2007.
[5] M. Bendersky, B. Croft, and D. a. Smith. Two-stage query segmentation for information retrieval. Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval - SIGIR '09, page 810, 2009.
[6] M. Bendersky, D. Metzler, and W. Croft. Learning concept importance using a weighted dependence model. In Proceedings of the third ACM international conference on Web search and data mining, pages 31≠40. ACM, 2010.
[7] S. Bergsma and Q. Wang. Learning noun phrase query segmentation. In Proc. of EMNLP-CoNLL, number June, pages 819≠826, 2007.
[8] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993≠1022, Mar. 2003.
[9] P. Boldi and S. Vigna. MG4J at TREC 2005. In E. M. Voorhees and L. P. Buckland, editors, The Fourteenth Text REtrieval Conference (TREC 2005) Proceedings, number SP 500-266 in Special Publications. NIST, 2005. http://mg4j.dsi.unimi.it/.
[10] S. Brin and L. Page. The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems, 30(1-7):107≠117, Apr. 1998.

[11] S. Bu®ttcher, C. L. A. Clarke, and B. Lushman. Term proximity scoring for ad-hoc retrieval on very large text collections. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '06, pages 621≠622, New York, NY, USA, 2006. ACM.
[12] S. Bu®ttcher, C. L. A. Clarke, and I. Soboroff. The TREC 2006 terabyte track. In TREC, 2006.
[13] C. L. A. Clarke, G. V. Cormack, and F. J. Burkowski. An algebra for structured text search and a framework for its implementation. The Computer Journal, 38:43≠56, 1995.
[14] C. L. A. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2004 terabyte track. In TREC, 2004.
[15] C. L. A. Clarke, F. Scholer, and I. Soboroff. The TREC 2005 terabyte track. In TREC, 2005.
[16] S. Cooper. Some Inconsistencies and Misnomers Retrieval in Probabilistic Information of Library taken Effective. Proceedings of the 14th annual international ACM SIGIR conference on Research and development in information retrieval, pages 57≠61, 1991.
[17] N. Craswell, S. Robertson, H. Zaragoza, and M. Taylor. Relevance weighting for query independent evidence. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, volume 1, pages 416≠423. ACM, 2005.
[18] P. Domingos, S. Kok, H. Poon, M. Richardson, and P. Singla. Unifying logical and statistical AI. In Proceedings of the Twenty-First National Conference on Artificial Intelligence, pages 2≠7, 2006.
[19] M. Hagen, M. Potthast, B. Stein, and C. Braeutigam. The power of naive query segmentation. In Proceeding of the 33rd international ACM SIGIR conference on

929

Topic no.
810 750 806 834 745 835 732 723 843 785

BM25 MAP
0.2759 0.0903 0.1279 0.2819 0.1801 0.1306 0.1438 0.1187 0.2420 0.2717

Operators MAP
0.5243 0.3251 0.3446 0.4859 0.3828 0.3222 0.3309 0.3030 0.4228 0.4323

Query
Timeshare resales John Edwards womens' issues Doctors Without Borders Global positioning system earthquakes Doomsday cults Big Dig pork U.S. cheese production Executive privilege Pol Pot Ivory billed woodpecker

Which segment / p-gram helped?
"Timeshare resales" "John Edwards" and "womens' issue" "Doctors Without Borders" "Global positioning" "Doomsday cult" "Big Dig" "U.S." and "cheese production" "Executive privilege" "Pol Pot" "Ivory billed"

Table 5: The ten queries that produced the largest gap between standard BM25 and our operator-based scoring. The methods in this table use a combination of BM25, segments and 2-grams.

Research and development in information retrieval, pages 797≠798. ACM, 2010.
[20] Y. Li, B.-j. P. Hsu, C. Zhai, and K. Wang. Unsupervised Query Segmentation Using Clickthrough for Information Retrieval. Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 285≠294, 2011.
[21] T.-Y. Liu. Learning to Rank for Information Retrieval. Information Retrieval, 3(3):225≠331, 2009.
[22] Y. Lu, F. Peng, G. Mishne, X. Wei, and B. Dumoulin. Improving web search relevance with semantic features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing Volume 2 - EMNLP '09, number August, page 648, Morristown, NJ, USA, 2009. Association for Computational Linguistics.
[23] Y. Lv and C. Zhai. Positional language models for information retrieval. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 299≠306. ACM, 2009.
[24] Y. Lv and C. Zhai. Positional relevance model for pseudo-relevance feedback. In Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 579≠586. ACM, 2010.
[25] Y. Lv and C. Zhai. A log-logistic model-based interpretation of TF normalization of BM25. In Proceedings of the 34th European Conference on Information Retrieval, ECIR'12, 2012.
[26] D. Metzler and B. Croft. A Markov random field model for term dependencies. Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval SIGIR '05, page 472, 2005.
[27] J. Ponte and W. B. Croft. A language modeling approach to information retrieval. Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275≠281, 1998.
[28] L. Ramshaw. Text chunking using transformation-based learning. of the Third ACL Workshop on Very, pages 82≠94, 1995.
[29] M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62(1):107≠136, 2006.

[30] S. Robertson. The probability ranking principle in IR. Journal of documentation, 1977.
[31] S. Robertson. On event spaces and probabilistic models in information retrieval. Information Retrieval, 8(2):319≠329, 2005.
[32] S. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. Proceedings of the SIGIR conference on Research and development in information retrieval, (1), 1994.
[33] S. Robertson, H. Zaragoza, and M. Taylor. Simple BM25 extension to multiple weighted fields. In Proceedings of the thirteenth ACM international conference on Information and knowledge management, CIKM '04, pages 42≠49, New York, NY, USA, 2004. ACM.
[34] S. E. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333≠389, 2009.
[35] K. Svore, P. Kanani, and N. Khan. How good is a span of terms?: exploiting proximity to improve web retrieval. In Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 154≠161. ACM, 2010.
[36] M. Taghavi, A. Patel, N. Schmidt, C. Wills, and Y. Tew. An analysis of web proxy logs with query distribution pattern approach for search engines. Computer Standards and Interfaces, 34(1):162 ≠ 170, 2012.
[37] B. Tan and F. Peng. Unsupervised query segmentation using generative language models and wikipedia. In Proceeding of the 17th international conference on World Wide Web, pages 347≠356. ACM, 2008.
[38] M. Taylor, J. Guiver, S. Robertson, and T. Minka. Softrank: optimizing non-smooth rank metrics. In Proceedings of the international conference on Web search and web data mining, WSDM '08, pages 77≠86, New York, NY, USA, 2008. ACM.
[39] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems, 22(2):179≠214, Apr. 2004.

930

Rhetorical Relations for Information Retrieval

Christina Lioma
Computer Science University of Copenhagen
Denmark
c.lioma@diku.dk

Birger Larsen
Royal School of Library and Information Science
Copenhagen Denmark
blar@iva.dk

Wei Lu
School of Information Management
Wuhan University China
reedwhu@gmail.com

ABSTRACT
Typically, every part in most coherent text has some plausible reason for its presence, some function that it performs to the overall semantics of the text. Rhetorical relations, e.g. contrast, cause, explanation, describe how the parts of a text are linked to each other. Knowledge about this socalled discourse structure has been applied successfully to several natural language processing tasks. This work studies the use of rhetorical relations for Information Retrieval (IR): Is there a correlation between certain rhetorical relations and retrieval performance? Can knowledge about a document's rhetorical relations be useful to IR?
We present a language model modification that considers rhetorical relations when estimating the relevance of a document to a query. Empirical evaluation of different versions of our model on TREC settings shows that certain rhetorical relations can benefit retrieval effectiveness notably (> 10% in mean average precision over a state-of-the-art baseline).
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing--linguistic processing
Keywords
Rhetorical relations, discourse structure, retrieval model, probabilistic retrieval
1. INTRODUCTION
According to discourse analysis, every part in most coherent text tends to have some plausible reason for its presence, some function that it performs to the overall semantics of the text. Rhetorical relations, e.g. contrast, explanation, condition, are considered critical for text interpretation, because they signal how the parts of a text are linked to each other to form a coherent whole [23]. Unlike grammatical relations, which are generally explicitly manifest in
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

Figure 1: Rhetorical relations example (from [11]).
language, rhetorical relations may be unstated. The goal of discourse analysis is therefore to infer rhetorical relations, and specifically to identify their span, constraints and function.
There is a large body of research on both descriptive and predictive models of rhetorical structure and discourse analysis in natural language text. For instance, annotation projects have taken significant steps towards developing semantic [12, 18] and discourse [5] annotated corpora. Some of these annotation efforts have already had a computational impact, making it possible to automatically induce semantic roles [15] and to automatically identify rhetorical relations [14], achieving near-human levels of performance on certain tasks [27]. In addition, applications of discourse analysis to automatic language processing tasks such as summarisation or classification (overviewed in section 2) indicate that rhetorical relations can enhance the performance of welltrained natural language processing systems.
Motivated by these advances, this work brings perspectives from discourse analysis into Information Retrieval (IR) with the aim of investigating if and how rhetorical relations can benefit retrieval effectiveness. Is there a correlation between certain rhetorical relations and retrieval performance? Can knowledge about a document's rhetorical relations be useful to IR? For example, consider the rhetorical relations of the text shown in Figure 1 (borrowed from [11]). Should some of the terms in this sentence be given extra weight by an IR system, according to their rhetorical relations? Can some rhetorical relations be considered more informative and hence more useful for IR ranking than others? These questions have been posed before (see discussion in section 2), however to our knowledge this is the first time that a principled integration of rhetorical relations into a probabilistic IR model improves precision by > 10%.

931

Reasoning about query - document relevance using the language modeling formalism [9], we present a model that conditions the probability of relevance between a query and a document on the rhetorical relations occurring in that document. We present an application of this model to an IR re-ranking task, where, given a list of documents initially retrieved for a query, the goal is to improve the ranking of the documents by refining their estimation of relevance to the query. Experimental evaluation of different versions of our model on TREC data and standard settings demonstrates that certain rhetorical relations can be beneficial to retrieval, with notable improvements to retrieval effectiveness (> 10% in mean average precision and other standard TREC evaluation measures over a state-of-the-art baseline).
2. RELATED WORK
Discourse analysis and rhetorical structures have been studied in the context of several automatic text processing applications. This has been partly enabled by the availability of discourse parsers - see [11, 14] for up-to-date overviews of discourse parsing technology. Studies of discourse analysis in relation to IR and its broader applications are briefly overviewed below. For a more general overview of discourse analysis approaches, see Wang et al. [33], section 2.
Sun & Chai [28] investigate the role of discourse processing and its implication on query expansion for a sequence of questions in scenario-based context question answering (QA). They consider a sequence of questions as a mini discourse. An empirical examination of three discourse theoretic models indicates that their discourse-based approach can significantly improve QA performance over a baseline of plain reference resolution.
In a different task, Wang et al. [33] parse Web user forum threads to determine the discourse dependencies between posts in order to improve information access over Web forum archives. They present three different methods for classifying the discourse relationships between posts, which are found to outperform an informed baseline.
Heerschop et al. [16] perform document sentiment analysis (partly) based on a document's discourse structure. They hypothesise that by splitting a text into important and less important text spans, and by subsequently making use of this information by weighting the sentiment conveyed by distinct text spans in accordance with their importance, they can improve the performance of a sentiment classifier. A document's discourse structure is obtained by applying rhetorical structure theory on a sentence level. They report a 4.5% improvement in sentiment classification accuracy when considering discourse, in comparison to a nondiscourse based baseline. Similarly to this study, Somasundaran et al. [26] report improvements to opinion polarity classification when using discourse, and Morato et al. [24] report a positive dependence between classification performance and certain discourse variables. An overview of discourse analysis for opinion detection can be found in Zhou et al. [36].
In the area of text compression, Louis et al. [21] study the usefulness of rhetorical relations between sentences for summarisation. They find that most of the significant rhetorical relations are associated to non-discriminative sentences, i.e. sentences that are not important for summarisation. They report that rhetorical relations that may be intuitively perceived as highly salient do not provide strong indicators of

informativeness; instead, the usefulness of rhetorical relations is in providing constraints for navigating through the text's structure. These findings are compatible with the study of Clarke & Lapata [7] into constraining text compression on the basis of rhetorical relations. For a more indepth look into the impact of individual rhetorical relations to summarisation see Teufel & Moens [30].
In domain-specific IR, Yu et al. [34] focus on psychiatric document retrieval, which aims to assist users to locate documents relevant to their depressive problems. They propose the use of high-level discourse information extracted from queries and documents, such as negative life events, depressive symptoms and semantic relations between symptoms, to improve the precision of retrieval results. Their discourseaware retrieval model achieves higher precision than the vector space and Okapi models.
Closer to our work, Wang et al. [31] extend an IR ranking model by adding a re-ranking strategy based on document discourse. Specifically, their re-ranking formula consists of the original retrieval status value computed with the BM11 model, which is then multiplied by a function that linearly combines inverse document frequency and term distance for each query term within a discourse unit. They focus on one discourse type only (advantage-disadvantage) which they identify manually in queries, and show that their approach improves retrieval performance for these queries. Our work differs on several points. We use an automatic (not manual) discourse parser to identify rhetorical relations in the documents to be retrieved (not queries). We consider 15 rhetorical relations (not 1) and we study their impact to retrieval performance using a modification of the IR language model.
Finally, Suwandaratna & Perera [29] also present a reranking approach for Web search that uses discourse structure. They report a heuristic algorithm for refining search results based on their rhetorical relations. Their implementation and evaluation is partly based on a series of ad-hoc choices, making it hard to compare with other approaches. They report a positive user-based evaluation of their system for ten test cases.

3. RANKING WITH RHETORICAL
RELATIONS
There may be various ways of considering rhetorical relations in an IR setting. In this work, we view rhetorical relations as non-overlapping text spans, rather than a graph or a tree with structure and overlapping nodes [27]. We select a principled integration of rhetorical relation information into the retrieval model that ranks documents with respect to queries. The goal is to enable evidence about the rhetorical relations in a document to have a quantifiable impact upon the estimation of relevance of this document to a query, and to study that impact.

3.1 Model Derivation
Let q be a query, d a document, D a collection of documents, and g a rhetorical relation in the collection (so that
g p(g|d) = 1). In probabilistic IR, each d in D can be ranked by its probability p(d|q) of being relevant to q. Using Bayes' law:

p(d|q) = p(q|d)p(d) ra=nk p(q|d)

(1)

p(q)

932

where the right-hand side of Equation 1 is derived as follows: p(q) is dropped because it is fixed for all documents, and p(d) can be dropped on the assumption that it is uniform in the absence of any prior knowledge about any document. Using the language modeling approach to IR [9], p(q|d) can be interpreted as the probability of generating the terms in q from a model induced by d, or more simply how likely it is that the document is about the same topic as the query. p(q|d) can be estimated in different ways, for instance using Dirichlet, Jelinek-Mercer, or two-stage smoothing [35].
We introduce into Equation 1 the probability of generating the query terms from a model induced by d and by its rhetorical relations g  d as follows:

p(q|d) = p(q|d, g)p(g|d)

(2)

g

We now explain the two components in Equation 2. The first component, p(q|d, g), can be interpreted as the probability of generating the query terms from a model induced by d and g. We estimate p(q|d, g) as a simple mixture of the probabilities of generating q from d and g:

p(q|d, g) = (1 - ) ∑ p(q|d) +  ∑ p(q|g)

(3)

where p(q|d) is the (baseline) probability of relevance between q and d mentioned in the beginning of this section,  is a free parameter, and p(q|g) can be interpreted as the probability of generating q from a model induced by the rhetorical relation g, or more simply, the `likelihood of relevance' between the terms in the query and the terms in the rhetorical relation.
The second component of Equation 2, p(g|d), is the probability of the rhetorical relation given the document. Similarly to above, this can be interpreted as the probability of generating the terms in g from a model induced by d, or more simply the likelihood of relevance between the terms in the rhetorical relation and the terms in the document.

3.2 Model Induction
To make Equations 2-3 operational we need to compute p(q|g) and p(g|d). One simple way of doing so is using the respective maximum likelihood estimations:

log

p(q|g )

=

|q| i=1

f (qi, g) |g |

(4)

where f (qi, g) is the frequency of the query term qi in g, and |g| is the number of terms in g.

log

p(g |d)

=

|g |

f (gj , |d|

d)

(5)

j=1

where f (gj, d) is the frequency of the rhetorical relation

term gj in d, and |d| is the number of terms in d. In this

work, we use the above equations and, to compensate for

zero-frequency cases, we apply add-one smoothing.

Alternative principled estimations of Equations 4-5 are

possible (e.g. Dirichlet, Good-Turing) and could poten-

tially improve the performance reported in this work. For

instance, one could discount the frequencies in Equations

4-5 by a respective collection model using Dirichlet smooth-

ing: log ps(q|g) =

|q| f (qi,g )+∑p(qi|)

i=1

|g |+

where



would

be the smoothing parameter and  would be the collec-

tion of all rhetorical relations in D. A similarly Dirichlet

smoothed alternative estimation of Equation 5 would be:

log ps(g|d) =

. |g | f (gj ,d)+∑p(gj |D)

j=1

|d|+

We choose to use

maximum likelihood instead of Dirichlet to avoid introduc-

ing the extra Dirichlet smoothing parameter  when inves-

tigating the effect of rhetorical relations upon retrieval.

Another alternative would be to use Good-Turing smooth-

ing, however doing so would scale down the maximum like-

lihood

estimations

in

Equations

4-5

by

a

factor

of

1

-

E(1) |g |

and

1-

E(1) |d|

respectively,

where

E(1) |g |

(resp.

E(1) |d|

)

is

the

estimate of how many items in the numerator of Equation 4

(resp. Equation 5) have occurred once in the sample of the

denominator (see Gale & Sampson [13] for more on Good-

Turing smoothing). In effect, for Equation 4 this scaling

down would reduce the probability of the query terms that

we have seen in g, making room for query terms that we

have not seen. For our setting this would not be necessary,

because in practice most queries and most rhetorical rela-

tions correspond to rather short text spans. Good-Turing

smoothing might be better suited for larger samples [13].

Overall, the model presented in this section can be seen as

a `basic model' for ranking documents (partly) according to

their rhetorical relations. Different variations on this basic

model are certainly possible, however we choose to use the

simple maximum likelihood version of this model for this

exploratory investigation into the potential benefits of using

rhetorical relations for IR.

4. EVALUATION
4.1 Experimental Setup
We evaluate our model on the task of re-ranking an initial list of documents, which has been retrieved in response to a query. Re-ranking is a well-known IR practice that can enhance retrieval performance notably [19]. The baseline of our experiments consists of the top 1000 documents retrieved for each query using a state-of-the-art retrieval model (language model with Dirichlet smoothing1 [9]). Our approach reranks these documents using Equation 2.
4.1.1 Dataset and Pre-processing
We experiment with the TREC datasets of the Web 2009 (queries 1-50) and Web 2010 (queries 51-100) tracks, that contain collectively 100 queries and their relevance assessments on the Clueweb09 cat. B dataset2 (50,220,423 web pages in English crawled between January and February 2009). We choose these datasets because they are used widely in the community, allowing comparisons with stateof-the-art. We remove spam using the spam rankings of Cormack et al. [8] with the recommended setting of percentilescore < 70 indicating spam3.
We consider a subset of this collection, consisting of the top 1000 documents that have been retrieved in response to each query by the baseline retrieval model on tuned settings
1We also experimented with Jelinek-Mercer and two-stage smoothing for the baseline retrieval model. Dirichlet and two-stage gave higher scores. We chose Dirichlet over twostage because it includes one less parameter to tune. 2http://lemurproject.org/clueweb09.php/ 3Note that removing spam from Clueweb09 cat B. is known to give overall lower retrieval scores than keeping spam [3].

933

Table 1: Examples of the 15 rhetorical relations (in bold italics) of our dataset, identified by the SPADE

discourse parser [27]

Rhetorical relation Example sentences with rhetorical relations italicised and bold

attribution

... the islands now known as the Gilbert Islands were settled by Austronesian-speaking people ...

background

... many whites had left the country when Kenyatta divided their land among blacks ...

cause-result

... I plugged "wives" into the search box and came up with the following results ...

comparison

... so for humans, it is stronger than coloured to frustrate these unexpected numbers ...

condition

... Conditional money based upon care for the pet ...

consequence

... voltage drop with the cruise control switch could cause erratic cruise control operation ...

contrast

... Although it started out as a research project , the ARPANET quickly developed into ...

elaboration

... order accutane no prescription required ...

enablement

... The project will also offer exercise programs and make eye care services accessible ...

evaluation

... such advances will be reflected in an ever-greater proportion of grade A recommendations ...

explanation

... the concept called as "evolutionary developmental biology" or shortly "evo-devo" ...

manner-means

... Fill current path using even-odd rule, then paint the path ...

summary

... Safety Last, Girl Shy, Hot Water, The Kid Brother, Speedy (all with lively orchestral scores) ...

temporal

... Take time out before you start writing ...

topic-comment

... Director Mark Smith expressed support for greyhound adoption ...

(described in section 4.1.2) using the Indri IR system4 for indexing and retrieval. For this subset, we strip HTML annotation using our in-house WHU-REAPER crawling and web parsing toolkit5. Rhetorical relations are identified using the freely available SPADE discourse parser [27]. Table 1 shows the 15 types of rhetorical relations identified by this process, with examples taken from the re-ranking dataset.
4.1.2 Parameter Tuning
Two parameters are involved in these experiments: the Dirichlet smoothing parameter  of the retrieval model (used by both the baseline and our approach) and the mixture parameter  of our model. Both parameters are tuned using 5-fold cross validation for each query set separately; results reported are the average over the five test sets.  is tuned across {100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000, 10000} (using the range of Zhai & Lafferty [35]) and  is tuned across {0.1, 0.3, 0.5, 0.7, 0.9}.
Performance is reported and tuned separately for Mean Average Precision (MAP), Binary Preference (BPREF), and Normalised Discounted Cumulated Gain (NDCG). These measures contribute different aspects to the overall evaluation: BPREF measures the average precision of a ranked list; it differs from MAP in that it does not treat non-assessed documents as explicitly non-relevant (whereas MAP does) [4]. This is a useful insight, especially for a collection as large as Clueweb09 cat. B where the chances of retrieving non-assessed documents are higher. NDCG measures the gain of a document based on its position in the result list. The gain is accumulated from the top of the ranked list to the bottom, with the gain of each document discounted at lower ranks. This gain is relative to the ideal based on a known recall base of relevance assessments [17]. Finally, we test the statistical significance of our results using the t-test at 95% and 99% confidence levels [25].
4.2 Findings
Figure 2 shows the distribution of the rhetorical relations in our re-ranking dataset as a percentage of the total number of rhetorical relations. Elaboration, attribution
4http://www.lemurproject.org/ 5Freely available by emailing the third author.

and background are the most frequent rhetorical relations, whereas topic-comment is the most infrequent. This happens because quite often in text a topic forms the nucleus of the discourse, which is then linked by a number of different rhetorical relations, for instance about its background, elaborating on an aspect, or attributing parts of it to some entity. As a result, several types of other rhetorical relations can correspond to a single topic-comment. Note that the distribution of rhetorical relations reported here is in agreement with the literature, e.g. Teufel & Moens [30] also report a 5% occurrence of contrast, albeit in the domain of scientific articles.
4.2.1 Retrieval-Enhancing Rhetorical Relations
Table 2 shows the performance of our model against the baseline, for each rhetorical relation and evaluation measure. The baseline performance is among the highest reported in the literature for these setings; for instance Bendersky et al. [3] report MAP=0.1605 for a tuned language model baseline with the Web 2009 track queries on Clueweb cat. B without spam.
We observe that different rhetorical relations perform differently across evaluation measures and query sets. The four rhetorical relations that improve performance over the baseline consistently for all evaluation measures and query sets (shaded rows in Table 2) are: background, cause-result, condition and topic-comment. Topic-comment is one of the overall best-performing rhetorical relations, which in simple terms means that boosting the weight of the topical part of a document improves its estimation of relevance.
A closer look at which rhetorical relations decrease performance presents a more uneven picture as no relations consistently underperform for all measures and query sets. Some relations, such as explanation and enablement for Web 2009, and summary and evaluation for Web 2010, are among the lowest performing, but are not under the baseline across all measures and both query sets. This implies that separating rhetorical relations into those that generally can enhance retrieval performance and those that cannot may not be straight-forward. Even though exploring the family likeness between useful relations and ones that give no mileage is an interesting discussion, in the rest of the pa-

934

Table 2: Retrieval performance with rhetorical relations and without (baseline). * (**) marks stat. signif-

icance at 95% (99%) using the t-test. Bold means > baseline. % shows the difference from the baseline.

Shaded rows indicate consistent improvements over the baseline at all times.

rhetorical relation

MAP

Web 2009 (queries 1-50) BPREF

NDCG

MAP

Web 2010 (queries 51-100) BPREF

NDCG

none (baseline)

0.1625

0.3230

0.3893

0.0986

0.2240

0.2920

attribution

0.1654* +1.8% 0.3275** +1.4% 0.3927** +0.9% 0.0924

-6.2% 0.2549** +13.8% 0.3008** +3.0%

background

0.1646 +1.3% 0.3291** +1.9% 0.3910 +0.4% 0.1086* +10.2% 0.2623** +17.1% 0.3070** +5.1%

cause-result

0.1626 +0.1% 0.3255** +0.8% 0.3900 +0.2% 0.1015 +2.9% 0.2491* +11.2% 0.3079 +5.4%

comparison

0.1610

-0.9% 0.3251* +0.6% 0.3877

-0.4% 0.1017 +3.1% 0.2282

+1.9% 0.3040** +4.1%

condition

0.1632 +0.5% 0.3258** +0.9% 0.3903 +0.3% 0.0999 +1.3% 0.2470** +10.3% 0.2936 +0.5%

consequence

0.1602

-1.4% 0.3250 +0.6% 0.3874

-0.5% 0.0945

-4.1% 0.2377* +6.1% 0.2840** -2.7%

contrast

0.1549* -4.6% 0.3269** +1.2% 0.3897 +0.1% 0.1103* +11.8% 0.2531** +13.0% 0.3069** +5.1%

elaboration

0.1556* -4.2% 0.3292** +1.9% 0.3866

-0.7% 0.0951

-3.5% 0.2598** +16.0% 0.3005** +2.9%

enablement

0.1601

-1.4% 0.3240 +0.3% 0.3869*

-0.6% 0.1010 +2.4% 0.2316* +3.4% 0.2992* +2.5%

evaluation

0.1632 +0.5% 0.3242 +0.4% 0.3886

-0.2% 0.0814** -17.4% 0.2313* +3.3% 0.2902

-0.6%

explanation

0.1546

-4.9% 0.3259* +0.9% 0.3813

-2.1% 0.1034 +4.9% 0.2645** +18.1% 0.3069** +5.1%

manner-means

0.1623

-0.1% 0.3253* +0.7% 0.3884

-0.2% 0.0986

- 0.2324* +3.7% 0.2897

-0.8%

summary

0.1626 +0.1% 0.3241 +0.3% 0.3879

-0.4% 0.0862

-12.6% 0.2220*

-0.9% 0.2928 +0.3%

temporal

0.1615

-0.6% 0.3262** +1.0% 0.3887

-0.2% 0.0921

-6.6% 0.2546** +13.7% 0.3052 +4.5%

topic-comment

0.1673 +3.0% 0.3375 +4.5% 0.3976* +2.1% 0.1090* +10.5% 0.2476* +10.5% 0.3009 +3.1%

Table 3: Effect of the rhetorical relation to the re-

trieval model as indicated by parameter  (see Equa-

tion 3), for the tuned runs of Table 2. Shaded

rows indicate rhetorical relations that consistently

improve performance over the baseline at all times.

rhetorical

Web 2009 (queries 1-50) Web 2010 (queries 51-100)

relation

MAP BPREF NDCG MAP BPREF NDCG

attribution

0.1 0.5

0.1

0.3 0.5

0.3

background

0.2 0.6

0.2

0.3 0.7

0.3

cause-result

0.3 0.7

0.3

0.5 0.7

0.5

comparison

0.4 0.7

0.4

0.3 0.5

0.3

condition

0.3 0.7

0.3

0.3 0.5

0.3

consequence

0.5 0.7

0.5

0.5 0.7

0.5

contrast

0.3 0.7

0.3

0.3 0.5

0.3

elaboration

0.1 0.5

0.1

0.3 0.5

0.3

enablement

0.1 0.9

0.1

0.3 0.5

0.3

evaluation

0.5 0.7

0.5

0.5 0.7

0.5

explanation

0.5 0.7

0.5

0.5 0.7

0.5

manner-means 0.5 0.7

0.5

0.5 0.7

0.5

summary temporal

0.5 0.7

0.5

0.3 0.7

0.3

0.1 0.7

0.1

0.3 0.5

0.3

topic-comment 0.5 0.5

0.5

0.5 0.7

0.5

topic-comment evaluation consequence
summary enablement
explanation comparison manner-means cause-result
temporal contrast condition
background

attribution elaboration

0

5

10

15

20

25

% of all rhetorical relations

Figure 2: % distribution of rhetorical relations in our dataset.

per we focus on those rhetorical relations that consistently improve retrieval performance (for these datasets).
Improvements over the baseline are generally higher for Web 2010 than Web 2009, possibly because the former baseline is weaker, with potentially more room for improvement. An interesting trend is that more rhetorical relations improve performance according to BPREF than according to MAP and NDCG. As BPREF is the only of these evaluation measures that does not consider non-assessed documents as non-relevant, this indicates the presence of non-assessed documents in the ranking.
The scores shown in Table 2 are averaged over tens of queries, meaning that they can be affected by outliers. Figure 3 presents a detailed per-query overview of the performance of each query in relation to the baseline for each of the 15 rhetorical relations6. The plotted points represent the difference in MAP between our approach and the baseline. Positive points indicate that our approach outperforms the baseline. The points are sorted.
We observe that although the overall performance of the Web 2010 query set is lower than that of the Web 2009 query set, the improvements over the baseline of the 2010 set are consistently larger. Only in one case, topic-comment, do the plotted points clearly cross. Overall both query sets show similar plots with outliers at both ends of the scale. However, the 2009 query set tends to have a somewhat larger proportion of negative outliers, which goes some way towards explaining the lower improvements over the baseline observed for Web 2009. The Web 2010 set shows improvements over the baseline for most of the rhetorical relations and for the majority of the queries.
4.2.2 Quantifying the Contribution of Rhetorical Relations to the Ranking
Exactly how much impact each rhetorical relation has on the ranking can be seen in Table 3. The table lists the  values for the best performing tuned runs from Table 2, where high  values mean that the rhetorical relations are given more weight in the ranking (see Equation 3). We see that none of the values are above 0.5 for MAP and NDCG, indicating that too much emphasis on the rhetorical rela-
6Similar trends are observed in the corresponding figures for BPREF and NDCG, which are not included here for brevity.

935

attribution 0.08

background 0.1

cause-result 0.2

0 -0.02
0.1 0
-0.1 0.2

comparison contrast

0
0.1 0
-0.1 0.1

condition elaboration

0 consequence
0.05 0
-0.1 enablement
0.15

0 -0.1
evaluation 0.05
0

0 -0.05
0.1 0

explanation

0.05 0 manner-means
0.05 0

-0.1 summary

-0.2 temporal

-0.1 topic-comment

0.1

0.05

0.05

0

0

0

-0.05

-0.1

-0.05

Web 2009 Web 2010

Figure 3: Sorted per-query difference in MAP between the baseline and our model (y-axis), for each rhetorical relation. The horizontal line marks the baseline. + and o mark the 2009 and 2010 query sets.

936

tions may not be beneficial to performance. Consistent with Table 2, BPREF follows a different trend than MAP and NDCG, which could be due to the fact that it is a different type of evaluation measure as discussed above in section 4.1.2. With BPREF, non-assessed documents are not explicitly penalised in the evaluation (as in MAP and NDCG) resulting in overall higher  values for best performing runs, typically of around 0.5-0.7.
Further we observe that the rhetorical relations that consistently improve performance over the baseline, as indicated in Table 2, differ in  values for their best performing runs. For example,  = 0.2 - 0.3 for background and  = 0.5 for topic-comment. This implies that, to use rhetorical relations successfully for IR, it is not sufficient to know which rhetorical relations should be considered in the ranking and which not; also knowledge about how much emphasis to put on each rhetorical relation is needed for optimal IR performance.
Finally, note that the frequency of rhetorical relations does not affect their impact to retrieval. For instance, the three best performing rhetorical relations, topic-comment, background and cause-result constitute respectively approximately 1%, 11% and 5% of all rhetorical relations, as shown in Figure 2.

5. OPTIMISED RANKING WITH RHETORICAL RELATIONS

5.1 Rhetorical Relation Selection

The findings in section 4.2 show that some rhetorical relations can be more beneficial to retrieval performance than others. An ideal solution would not consider the lexical statistics of all rhetorical relations in a document, but rather it would select to include in the ranking only those rhetorical relations that have a higher likelihood of enhancing retrieval performance. This can be formulated as finding the optimal rhetorical relation ^g that maximises the expected retrieval scores according to an evaluation measure (e.g. MAP) for a query-document pair:

^g = arg max E[y|q, d]

(6)

g 

where E denotes the expectation and y the retrieval score

(rest of notation as defined in section 3).

Bayesian decision theory allows to reason about this type

of expectation, for instance see [32]. In this work, we treat

this as a problem of Bayesian posterior inference, where the

goal is to estimate the retrieval performance associated with

a rhetorical relation, given the observed retrieval scores it

fetches on a number of queries. Then, we can consider

the rhetorical relation associated with the highest retrieval

performance as optimal. For this estimation, we split our

dataset into different parts so that we use the observations

from one to make inferences about the other (see section 5.2

for details).

Let n = 15 be the rhetorical relations shown in Table 2,

and xj be the number of queries for which retrieval with the jth rhetorical relation gets a retrieval score yj. For now

we assume that all rhetorical relations may be expected to

have similar retrieval performance, with the jth rhetorical

relation having an average performance ratio per query j

(estimated

as

yj xj

).

Various

densities

can

be

used

to

fit

simi-

lar data [22], one of which is the Poisson distribution. Let us

assume that, conditional on j, the retrieval scores yj have independent Poisson distributions with means jxj. Let us further assume that the j are independent realisations of a gamma variable with parameters  and , and that  itself has a prior gamma distribution with parameters  and . Thus
f (y|) = n (xj j )yj e-xj j j=1 yj !
(|) = n j -1 e-j ()
j=1
() =  -1 e- ()

so that the joint probability density of the retrieval scores y, the average performance ratios , and  is

n
f (y|)f (|)() = c {jyj +-1e-j (xj +)}∑n+-1e-
j=1
(7) where c is a constant of proportionality.
The conditional density of  can be computed by various numerical approximations, one of which is the Laplace method [2], which we use here. To find the conditional density of  we integrate over the j to obtain
n
f (y, ) = c {(xj +)-(yj +)(yj +)}∑n+-1e- (8)
j=1

from which the marginal density of y is obtained by further integration to give

n



f (y) = c (yj + ) ∑ e-h()d

(9)

j=1

0

where h() =  - (n +  - 1)log + (yj + )log(xj + ). Let I denote the integral in this expression. In this work,
we take an uninformative prior for , with  = 0.1 and  = 1 and use  = 1.87. We then apply Laplace's method
to I, resulting in the approximate posterior density for , ~(|y) = I~-1e-h().
To calculate approximate posterior densities for j we integrate Equation 7 over i, i = j and then we apply Laplace's method to the numerator and denominator inte-
grals of

where

(j |y)

=

jyj +-1e-j xj

 0

e-hj

()d

(yj + )

 0

e-h()d

hj() = ( + j) - (n +  - 1)log + (yi + )log(xi + )
i=j
The resulting denominator is again I~1, while the numerator must be recalculated at each of a range of values for j. The output is the (posterior) expected retrieval performance associated with each rhetorical relation.
5.2 Experiments
7These values are not tuned; they are the default values of this approach as illustrated in [10], chapter 11.3, pages 603-604.

937

Table 4: Retrieval performance with optimal rhetorical relations (inferred, observed) and without rhetorical

relations (baseline). (1)-(5) refers to the five randomised samplings used to infer the optimal rhetorical

relations. Bold marks better than baseline.

rhetorical relation

MAP

Web 2009 (queries 1-50) BPREF

NDCG

MAP

Web 2010 (queries 51-100)

BPREF

NDCG

none (baseline)

0.1625

0.3230

0.3894

0.0967

0.2198

0.2890

optimalinferred (1) optimalinferred (2) optimalinferred (3) optimalinferred (4) optimalinferred (5) optimalobserved

0.1879 0.1948 0.1984 0.1952 0.1950 0.2157

+15.6% +19.9% +22.1% +20.1% +20.0% +32.7%

0.3503 0.3585 0.3532 0.3479 0.3528 0.3660

+8.5% +11.0%
+9.3% +7.7% +9.2% +13.3%

0.4224 0.4202 0.4169 0.4282 0.4287 0.4412

+8.5% +7.9% +7.1% +10.0% +10.1% +13.3%

0.1355 0.1285 0.1358 0.1360 0.1340 0.1474

+40.1% +32.9% +40.0% +40.6% +38.6% +52.4%

0.2859 0.2841 0.2906 0.2874 0.2865 0.2978

+30.1% +29.3% +32.2% +30.8% +30.3% +35.5%

0.3347 0.3394 0.3388 0.3336 0.3322 0.3569

+15.8% +17.4% +17.2% +15.4% +14.9% +23.5%

5.2.1 Setup
The observations required to make the above inference are triples of rhetorical relation - query number - retrieval score. To avoid overfitting, we pool randomly 50% of the observations from the 2009 Web query scores and 50% of the observations from the 2010 Web query scores. We use this pool to infer the expected retrieval performance of each rhetorical relation. We repeat this randomised pooling five times, each time randomly pertrubing the data, producing five different sets of observations. We then use each set to infer the expected best performing rhetorical relation per query, in accordance to Equation 6. Following this, we use the model introduced in section 3, Equation 2, to rank documents with respect to queries only for optimal (as inferred) rhetorical relations. We evaluate the above method using the same experimental settings described in section 4.1.

5.2.2 Findings

Table 4 shows the runs corresponding to the five differ-

ent inferences of the best rhetorical relation that use our

model (optimalinferred (1)-(5) respectively). We also report

the optimal retrieval performance actually observed in the

dataset when using the best rhetorical relation per query

(optimalobserved). Optimal here means with respect to the

choice of rhetorical relation, not with respect to the Dirichlet

 parameter of the baseline retrieval model.

Table 4 shows that our optimised ranking model for rhetor-

ical relations is better than the baseline for any of the five

random inferences on all three evaluation measures. The

probability of getting such a positive result by chance is

1 25

<

0.05,

and

thus

the

improvements

are

statistically

sig-

nificant. The improvements over the baseline are consider-

able, a very promising finding given the relatively low num-

ber of observations used for optimising the choice of rhetor-

ical relations. Experiments involving larger query sets can

be reasonably expected to perform on a par with state-of-

the-art performance.

More generally, the improvements in Table 4 signal that

rhetorical relations (derived automatically as shown in this

work) could potentially be useful features for `linguistically-

uninformed' learning-to-rank approaches.

6. DISCUSSION
6.1 Rhetorical Relation Distribution
The distribution of the 15 rhetorical relations we identified in our dataset is not the same for all rhetorical relations (see Figure 2). Some types, e.g. topic-comment, tend to be very sparse, whereas relations such as elaboration prevail.

This has no impact on the model presented in section 3, but it can bias the optimised inference of the model presented in section 5. The lower the occurrence of a rhetorical relation in the dataset, the fewer the observations of retrieval performance associated with it, and hence the weaker the predictions we can infer about whether it is optimal or not. A fairer setting would be to have the same number of `query - retrieval performance' observations for all rhetorical relations - however that would imply fiddling with the document distribution of our dataset significantly, potentially harming its quality as a test collection.
6.2 Limitations
A general limitation of discourse analysis is that not all types of text are susceptible to it. For instance, legal text, contracts, or item lists often lack rhetorical structure. In this work, we made no effort to identify and exempt such types of text from the discourse parsing. We reasoned that, as the SPADE parser includes a first-step grammatical parsing, the initial grammatical parsing of these types of text would flag out ill-formed parts (e.g. missing a verb, or consisting of extremely long sentences), which would then be skipped by the discourse analysis. This was indeed the case, however at a certain efficiency cost. Overall processing speed for SPADE was approximately 19 seconds per document (including the initial grammatical parsing), on a machine of 9 GB RAM, 8 core processor at 2.27GHz. One way of improving this performance would be to update the first-step grammatical parsing. Currently this depends on the well-known Charniak parser [6], which is one of the best performing grammatical parsers, however no longer supported. Other state-of-theart faster grammatical parsers, e.g. the Stanford parser8, could be adapted and plugged into SPADE instead.
The choice of applying our model for re-ranking as opposed to ranking all documents was closely related to the efficiency concerns discussed above. Our model is not specific to re-ranking only, however, using SPADE on approximately 50 million documents was too expensive at this point. Improving the discourse parser's efficiency is something we are currently working on, with the aim to apply our model for full ranking and see if the conclusions drawn from this work hold.
Finally, the accuracy of the discourse parser was not considered in this work, apart from indications in the literature that SPADE is a generally well-performing parser [27]. Given that the default version of the parser we used is trained on news articles, one may reason that its accuracy could improve if we train it on the retrieval collection, or on doc-
8http://nlp.stanford.edu/software/lex-parser.shtml

938

uments of the same domain. Note that, parsing accuracy aside, rhetorical relations assignment is not an entirely unambiguous process, even to humans [23]. For the purposes of this work, this type of fine-grained ambiguity may however not be important to retrieval performance.
6.3 Future Extensions
Future extensions include primarily making SPADE scalable on large collections of documents as discussed above, as well as using more than one rhetorical relation per document. For instance, the posterior probabilities estimated in section 5.1 could be used to weight the text in each rhetorical relation. If those posteriors are too flat, an exponent could make them peakier. As the exponent goes to infinity, the maximum relation model presented in section 5.2 would be recovered. In addition, we intend to refine the discourse analysis by considering the nucleus (i.e. central) versus satellite (i.e. peripheral) rhetorical relations for IR, as well as to improve the effectiveness of the discourse parser by training it on data of the same domain. As discussed in section 3.2, we will also investigate alternative estimations of Equations 2-3.
An interesting future research direction is the potential relation between rhetorical relations and user context: for instance, in a search session including several query reformulations, is there a correlation between the progression of the information need of the user and the rhetorical relations that the retrieval system should boost in a document (e.g. elaboration), as indicated by Sun & Chai [28]? Another interesting future extension of this work is in relation to evaluation measures of graded relevance measures on an inter-document level, as investigated in XML retrieval [20] for instance. If parts of a document can be regarded as more or less relevant, this may be reflected to their discourse structure. This might be especially useful for multi-threaded documents, such as multiple-user reviews and opinions, where the discourse relations tend to shift markedly. Finally, the current operationalisation of our model is simplistic in the sense that the term `rhetorical relation' is coerced into meaning `non-overlapping text fragment' and the actual relation between bits of text is discarded in the process. In future work we could apply fielded XML retrieval models in order to investigate nested structuring among rhetorical relations.
7. CONCLUSIONS
Rhetorical relations, e.g. contrast, explanation, condition, indicate the different ways in which the parts of a text are linked to each other to form a coherent whole. This work studied two questions: Is there a correlation between certain rhetorical relations and retrieval performance? Can knowledge about a document's rhetorical relations be useful to IR? To address these, we presented a retrieval model that conditions the probability of relevance between a query and a document on the rhetorical relations occurring in that document. We applied that model to an IR re-ranking scenario for Web search. Experimental evaluation of different versions of our model on TREC data and standard settings demonstrated that certain rhetorical relations can be beneficial to retrieval, with >10% improvements to retrieval precision. Furthermore, we showed that these improvements over the baseline can improve significantly, when the optimal rhetorical relation per document is selected for retrieval.
Overall, three rhetorical relations were found to benefit

retrieval performance notably and consistently for different evaluation measures and query sets: background, causeresult and topic-comment. In retrospect, this is perhaps not surprising, since these are among the most salient discourse relations on an intuitive basis: the main topic or theme of a text, its background, causes and results [21]. Future extensions and research directions of this work include considering more than one rhetorical relation per document, applying our model for ranking all documents (as opposed to re-ranking only) and experimenting with alternative estimations of its components.
8. ACKNOWLEDGMENTS
We thank Kasper HornbÊk, Jakob Grue Simonsen, Raf Guns, Qikai Cheng and the anonymous reviewers for helping improve this paper. Work partially funded by the Danish International Development Agency DANIDA (grant no. 10-087721) and the National Natural Science Foundation of China (grant no. 71173164).
9. REFERENCES
[1] Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL. ACL, 2011.
[2] A. Azevedo-Filho and R. D. Shachter. Laplace's method approximations for probabilistic inference in belief networks with continuous variables. In R. L. de M¥antaras and D. Poole, editors, UAI, pages 28≠36. Morgan Kaufmann, 1994.
[3] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased ranking of web documents. In I. King, W. Nejdl, and H. Li, editors, WSDM, pages 95≠104. ACM, 2011.
[4] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In M. Sanderson, K. J®arvelin, J. Allan, and P. Bruza, editors, SIGIR, pages 25≠32. ACM, 2004.
[5] L. Carlson, D. Marcu, and M. E. Okurowski. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Current Directions in Discourse and Dialogue, pages 85≠112. Kluwer Academic Publishers, 2003.
[6] E. Charniak. A maximum-entropy-inspired parser. In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, pages 132≠139, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.
[7] J. Clarke and M. Lapata. Discourse constraints for document compression. Computational Linguistics, 36(3):411≠441, 2010.
[8] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. CoRR, abs/1004.5168, 2010.
[9] W. B. Croft and J. Lafferty. Language Modeling for Information Retrieval. Kluwer Academic Publishers, Norwell, MA, USA, 2003.
[10] A. C. Davison. Statistical Models. Cambridge University Press, New York, 2009.
[11] D. A. duVerle and H. Prendinger. A novel discourse parser based on support vector machine classification.

939

In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL '09, pages 665≠673, Stroudsburg, PA, USA, 2009. Association for Computational Linguistics.
[12] C. J. Fillmore, C. F. Baker, and S. Hiroaki. The framenet database and software tools. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages 1157≠1160, 2002.
[13] W. A. Gale and G. Sampson. Good-turing frequency estimation without tears. Journal of Quantitative Linguistics, 2(3):217≠237, 1995.
[14] S. Ghosh, R. Johansson, G. Riccardi, and S. Tonelli. Shallow discourse parsing with conditional random fields. In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP), pages 1071≠1079, Chiang Mai, Thailand, 2011.
[15] D. Gildea and D. Jurafsky. Automatic labeling of semantic roles. In ACL. ACL, 2000.
[16] B. Heerschop, F. Goossen, A. Hogenboom, F. Frasincar, U. Kaymak, and F. de Jong. Polarity analysis of texts using discourse structure. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM '11, pages 1061≠1070, New York, NY, USA, 2011. ACM.
[17] K. J®arvelin and J. Kek®al®ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422≠446, 2002.
[18] P. Kingsbury and M. Palmer. From treebank to propbank. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages x≠x, 2002.
[19] E. Krikon and O. Kurland. A study of the integration of passage-, document-, and cluster-based information for re-ranking search results. Inf. Retr., 14(6):593≠616, 2011.
[20] M. Lalmas. XML Retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan & Claypool Publishers, 2009.
[21] A. Louis, A. K. Joshi, and A. Nenkova. Discourse indicators for content selection in summarization. In R. Fern¥andez, Y. Katagiri, K. Komatani, O. Lemon, and M. Nakano, editors, SIGDIAL Conference, pages 147≠156. The Association for Computer Linguistics, 2010.
[22] R. Manmatha, T. M. Rath, and F. Feng. Modeling score distributions for combining the outputs of search engines. In W. B. Croft, D. J. Harper, D. H. Kraft, and J. Zobel, editors, SIGIR, pages 267≠275. ACM, 2001.
[23] W. C. Mann and S. A. Thompson. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8:243≠281, 1988.

[24] J. Morato, J. Llorens, G. Genova, and J. A. Moreiro. Experiments in discourse analysis impact on information classification and retrieval algorithms. Inf. Process. Manage., 39:825≠851, November 2003.
[25] M. D. Smucker, J. Allan, and B. Carterette. Agreement among statistical significance tests for information retrieval evaluation at varying sample sizes. In J. Allan, J. A. Aslam, M. Sanderson, C. Zhai, and J. Zobel, editors, SIGIR, pages 630≠631. ACM, 2009.
[26] S. Somasundaran, G. Namata, J. Wiebe, and L. Getoor. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In EMNLP, pages 170≠179. ACL, 2009.
[27] R. Soricut and D. Marcu. Sentence level discourse parsing using syntactic and lexical information. In HLT-NAACL, 2003.
[28] M. Sun and J. Y. Chai. Discourse processing for context question answering based on linguistic knowledge. Know.-Based Syst., 20:511≠526, August 2007.
[29] N. Suwandaratna and U. Perera. Discourse marker based topic identification and search results refining. In Information and Automation for Sustainability (ICIAFs), 2010 5th International Conference on, pages 119≠125, 2010.
[30] S. Teufel and M. Moens. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409≠445, 2002.
[31] D. Y. Wang, R. W. P. Luk, K.-F. Wong, and K. L. Kwok. An information retrieval approach based on discourse type. In C. Kop, G. Fliedl, H. C. Mayr, and E. M¥etais, editors, NLDB, volume 3999 of Lecture Notes in Computer Science, pages 197≠202. Springer, 2006.
[32] J. Wang and J. Zhu. On statistical analysis and optimization of information retrieval effectiveness metrics. In F. Crestani, S. Marchand-Maillet, H.-H. Chen, E. N. Efthimiadis, and J. Savoy, editors, SIGIR, pages 226≠233. ACM, 2010.
[33] L. Wang, M. Lui, S. N. Kim, J. Nivre, and T. Baldwin. Predicting thread discourse structure over technical web forums. In EMNLP [1], pages 13≠25.
[34] L.-C. Yu, C.-H. Wu, and F.-L. Jang. Psychiatric document retrieval using a discourse-aware model. Artif. Intell., 173:817≠829, May 2009.
[35] C. Zhai and J. D. Lafferty. Two-stage language models for information retrieval. In SIGIR, pages 49≠56. ACM, 2002.
[36] L. Zhou, B. Li, W. Gao, Z. Wei, and K.-F. Wong. Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities. In EMNLP [1], pages 162≠171.

940

Modeling Higher-Order Term Dependencies in Information Retrieval using Query Hypergraphs

Michael Bendersky
Dept. of Computer Science Univ. of Massachusetts Amherst
Amherst, MA
bemike@cs.umass.edu
ABSTRACT
Many of the recent, and more effective, retrieval models have incorporated dependencies between the terms in the query. In this paper, we advance this query representation one step further, and propose a retrieval framework that models higher-order term dependencies, i.e., dependencies between arbitrary query concepts rather than just query terms. In order to model higher-order term dependencies, we represent a query using a hypergraph structure ≠ a generalization of a graph, where a (hyper)edge connects an arbitrary subset of vertices. A vertex in a query hypergraph corresponds to an individual query concept, and a dependency between a subset of these vertices is modeled through a hyperedge. An extensive empirical evaluation using both newswire and web corpora demonstrates that query representation using hypergraphs is highly beneficial for verbose natural language queries. For these queries, query hypergraphs significantly improve the retrieval effectiveness of several state-of-the-art models that do not employ higher-order term dependencies.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Experimentation
Keywords
Query hypergraphs, query representation, retrieval models
1. INTRODUCTION
Over the past decade, information retrieval research has undergone a gradual shift of focus from retrieval models that use bag-of-words query representations to retrieval models that incorporate term dependencies. Some recent examples of retrieval models that incorporate term dependencies
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

W. Bruce Croft
Dept. of Computer Science Univ. of Massachusetts Amherst
Amherst, MA
croft@cs.umass.edu
include, among others, Markov random fields [27], linear discriminant model [14], dependence language model [13], quasi-synchronous dependence model [30], and positional language model [26].
In this paper, we propose a novel retrieval framework that takes a further step toward a more accurate modeling of the dependencies between the query terms. Rather than modeling the dependencies between the individual query terms, our framework models dependencies between arbitrary concepts in the query.
We broadly define a query concept as a syntactic expression that models a dependency between a subset of query terms. Query concepts may model a variety of linguistic phenomena, including n-grams, term proximities, noun phrases, and named entities. Therefore, a dependency between query concepts represents a dependency between term dependencies, i.e., a higher-order term dependency1.
To the best of our knowledge, there is little prior work on modeling this type of higher-order term dependencies for information retrieval. Most retrieval models limit their attention to either pairwise term dependencies [11, 26] or, at most, dependencies between multiple terms [2, 27]. In contrast, the retrieval framework proposed in this paper can model dependencies between arbitrary concepts, e.g., a dependency between a phrase and a term. We hypothesize that an accurate modeling of concept dependencies is especially important for verbose natural language queries. This is due to the fact that the grammatical complexity of these queries often challenges the capabilities of the current retrieval models [2, 20].
As an example, consider the natural language query in Figure 1:
"Provide information on the use of dogs worldwide for law enforcement purposes."2
Figure 1(a) shows an excerpt from the top document retrieved by a sequential dependence model [27], a state-ofthe-art retrieval model that incorporates term dependencies. As evident from this excerpt, the top-retrieved document is non-relevant with respect to the query. Even though it contains many instances of the phrase "law enforcement" as well as the terms provided and information it does not mention the use of dogs.
On the other hand, an excerpt from the document in Figure 1(b) clearly indicates the relevance of the top document
1In the remainder of this paper, we shall use the definitions "higher-order term dependency" and "concept dependency" interchangeably. 2A description of the TREC topic #426.

941

...linking law enforcement duties to the definition of "law enforcement officer" for retirement purposes....must be handled within the context of...FEPCA and law enforcement retirement law and regulations....Adding a discussion of these issues would add unnecessarily to the complexity...of information already provided...definitions of "law enforcement officer" in these regulations should provide guidance...
(a)

...Simi Valley, West Covina and Los Angeles police departments were among the first law enforcement agencies to receive money through the forfeiture program....a narcotics-sniffing dog in a Simi Valley police investigation...led to the largest seizure of cocaine ever by authorities from Ventura County...dog's efforts are expected to yield a substantial amount of money...for the 21-officer department...
(b)

Figure 1: Excerpts from (a) the top document retrieved by the sequential dependence model [27], and (b) the top document retrieved using a query hypergraph in response to the query: "Provide information on the use of dogs worldwide for law enforcement purposes". Non-stopword query terms are marked in boldface.

retrieved by our method with respect to the query. Even though this excerpt matches less of the query terms than the excerpt in Figure 1(a), it contains a relationship between the term dog and the phrase "law enforcement", which is highly indicative of its relevance. This relationship cannot be modeled without accounting for higher-order term dependencies.
As Figure 1 shows, the evidence of the concepts co-occurring within a passage of text is a strong indicator of their dependency. This is somewhat akin to term dependencies, which are often modeled based on the frequency of the terms cooccurring next (or close) to each other in the document [27, 39, 26].
In the case of concept dependency, however, instead of relying on the entire document, we only examine a single document passage that is deemed to be the most relevant with respect to the query. This focused evidence can distinguish between relevant documents and documents which simply contain many repeated concept instances, as in Figure 1(a). This approach is reminiscent of the passage retrieval models that often make use of the evidence from the highest-scoring document passage [3, 9, 8, 16, 41].
In contrast to the approach presented in this work, most passage retrieval methods are based on a conjunctive retrieval model and treat a query as a bag of words. However, as the excerpts in Figure 1 demonstrate, such a simple conjunctive retrieval model is not sufficient, especially for verbose, natural language queries.
Instead, the proposed retrieval framework distinguishes between the concepts and the dependencies that are crucial for conveying the query intent, and the concepts and the dependencies of lesser importance. For instance, in the case of the query in Figure 1, the dependency (dog, "law enforcement") in Figure 1(b) is crucial for expressing the query intent, while the dependency (information and "law enforcement") in Figure 1(a) is not.
To summarize, unlike any of the current retrieval models, the retrieval framework proposed in this paper integrates three main characteristics that we believe are crucial for improving the effectiveness of retrieval with verbose queries. First, it models arbitrary term dependencies as concepts. Second, it uses passage-level evidence to model the dependencies between these concepts. Finally, it assigns weights to both concepts and concept dependencies, proportionate to the estimate of their importance for expressing the query intent. In this paper, we show that by integrating these characteristics, the proposed retrieval framework can significantly improve the effectiveness of several current state-ofthe-art retrieval models.

Structure  Terms Bigrams Noun Phrases Named Entities Dependencies

Concepts { :   }
["members", "rock", "group", "nirvana"] ["members rock", "rock group", "group nirvana"] ["members", "rock group nirvana"] ["nirvana"] ["members nirvana", "rock group"]

Table 1: Examples of the possible structures and the concepts they might contain for the query "members of the rock group nirvana" (stopwords removed).

The proposed retrieval framework is based on a query representation using a hypergraph structure ≠ a generalization of a graph, where an edge can connect more than two vertices. A vertex in a query hypergraph corresponds to an individual query concept. The vertices are grouped by structures, which model various linguistic phenomena. For instance, as shown in Table 1, a structure can group together terms, ngrams or noun phrases. Finally, any subset (rather than just a pair as in a standard graph) of vertices can be connected via a hyperedge, which models concept dependencies.
We use the query hypergraph representation to derive a ranking function that incorporates concepts and concept dependencies in a principled manner, based on the factorization of the hypergraph. We then propose two approaches for the parameterization of the ranking function. The first parameterization approach assigns weights to the concepts and the concept dependencies based on their respective structures. The second parameterization approach assigns weights based on a set of importance features associated with each concept and concept dependency.
The remainder of this paper is organized as follows. First, in Section 2 we provide the theoretical underpinnings of the query hypergraph representation and ranking with query hypergraphs. Then, in Section 3 we describe the related work and its connection to query representation using hypergraphs. In Section 4 we report the details of the empirical evaluation of the proposed framework. Section 5 concludes the paper.
2. QUERY HYPERGRAPHS
2.1 Query Representation with Hypergraphs
In this paper, we base the query representation on two modeling assumptions. First, we assume that given a query Q, we can model it using a set of linguistic structures
Q {1, . . . , n}.

942

({international, art, crime, "art crime"},D)

Terms

D

international

art

crime

Phrases "art crime"

({international},D)

({art},D)

({crime},D)

({"art crime"},D)

Figure 2: Example of a hypergraph representation for the query "international art crime".

The structures in the set Q are both complete and disjoint. The completeness of the structure implies that it can be used as an autonomous query representation. The disjointness of the structures means that there is no overlap in the linguistic phenomena modeled by the different structures. In other words, each structure groups together concepts of a single type (e.g., terms, bigrams, noun phrases, etc.).
Second, within each structure, arbitrary term dependencies can be modeled as concepts. In other words, each structure i  Q is represented by a set of concepts
i {1i , 2i , . . .}.

Each such concept is considered to be an atomic unit for the purpose of query representation. In addition, for convenience, we adopt the notation

n

KQ

[ i,

i=1

to refer to the union of all the query concepts, regardless of their respective structures.
These modeling assumptions, while conceptually simple, create an expressive formalism for hierarchical query representation. This formalism is flexible enough to specify a wide range of specific instantiations. Table 1 shows that it can model a wide spectrum of linguistic phenomena that are often encountered in natural language processing and information retrieval applications.
For instance, as we can see in Table 1, a structure can contain single terms as concepts, resulting in a bag-of-words query representation. A structure can also contain adjacent bigrams or noun phrases. Concepts need not be defined over contiguous query terms, as is demonstrated by the last structure in Table 1, which models a set of linguistic dependency links between the query terms.
For the purpose of information retrieval, we are primarily interested in using the resulting hierarchical query representation to model the relationship between a query Q and a document D in the retrieval corpus. Specifically, given a set of query structures Q and a document D, we construct a hypergraph H(Q, D)3.
A hypergraph is a generalization of a graph where an edge can connect an arbitrary set of vertices. A hypergraph H is

3 For conciseness, we use the abbreviation H H(Q, D) in the remainder of this paper.

represented by a tuple V, E , where V is a set of elements or vertices and E is a set of non-empty subsets of V , called hyperedges. In other words, the set E  PS(V ) of hyperedges is a subset of the powerset of V [18].
Specifically for the scenario of document retrieval, we de-
fine the hypergraph H over the document D and the set of query concepts KQ as

V

KQ  {D}

E

{(k, D) : k  PS(KQ)}.

(1)

Figure 2 demonstrates an example of a hypergraph H for the search query "international art crime". In this particular example, we have two structures. The first structure contains the query terms denoted i, a, and c, respectively. The second structure contains a single phrase, ac. Over these concepts, we can define a set of five hyperedges ≠ four hyperedges connecting document D and each of the concepts, and one hyperedge connecting D and all of the concepts.
Formally, for the hypergarph H in Figure 2, the vertices and the hyperedges are defined as follows

VFig.2 = {D, i, a, c, ac} EFig.2 = {({i}, D), ({a}, D), ({c}, D),
({ac}, D), ({i, a, c, ac}, D)}.

Note that this hypergraph configuration is just one possible choice. In fact, any subset of query terms can serve as a query concept, and similarly, any subset of query concepts can serve as a hyperedge, as shown by Equation 1.

2.2 Ranking with Query Hypergraphs
In the previous section, we defined the query representation using a hypergraph H = V, E . In this section, we define a global function over this hypergraph, which assigns a relevance score to document D in response to query Q. This relevance score is used to rank the documents in the retrieval corpus.
A factor graph, a form of hypergraph representation which is often used in statistical machine learning [6], associates a factor e with a hyperedge e  E. Therefore, most generally, a relevance score of document D in response to query Q represented by a hypergraph H is given by

sc(Q, D) Y e(ke, D) ra=nk X log(e(ke, D)). (2)

eE

eE

943

It is interesting to note that Equation 2 is reminiscent of the recently proposed log-linear retrieval models, including the Markov random field model [27] and the linear discriminant model [14]. Similarly to these models, Equation 2 scores a document using a log-linear combination of factors e(ke, D).
However, an important difference from these retrieval models is related to the fact that the factors e(ke, D) in Equation 2 are defined over concept sets, rather than single concepts, as in previous work [14, 27]. This definition enables the modeling of higher-order dependencies between query terms. Higher-order term dependencies cannot be easily modeled by the existing retrieval models that incorporate term dependencies [4, 14, 26, 27, 30, 39].
Thus far, we have provided only the most abstract definition of the query representation and ranking with query hypergraphs. In the remainder of this section, we provide an in-depth discussion of the query hypergraph induction and a detailed derivation of the ranking function.
First, in Section 2.3, we fully specify the structures, concepts, and hyperedges in the query hypergraph H. Then, in Section 2.4, we instantiate the factors e(ke, D) in the ranking function in Equation 2 using these specifications. Finally, in Section 2.5, we examine the different parameterizations of the ranking function.
2.3 Query Hypergraph Induction
2.3.1 Hypergraph Structures
There are many potential ways in which we could define the set of structures Q in the query hypergraph. In this work, we focus on three types of structures that are successfully used in previous work on modeling term dependencies for information retrieval [4, 5, 27, 32]. We leave a further exploration of other possible hypergraph structures to future work.
(1) QT-structure. The query term (QT) structure contains the individual query words ti as concepts. Terms are the most commonly used concepts in information retrieval, both in bag-of-words models [33, 34] and models that incorporate term dependencies [27, 29, 14].
(2) PH-structure. The phrase (PH) structure contains the combinations of query terms that are matched as exact phrases in the document. Exact phrase matching has often been used for improving the performance of retrieval methods [12, 42]. Most recently, it has been shown that using query bigrams for exact phrase matching is a simple and efficient method for improving the retrieval performance in large scale web collections [4, 5, 27, 29, 32]. Following this finding, we define the concepts in the PH-structure as adjacent query word pairs (titi+1).
(3) PR-structure. Unlike the PH-structure, the proximity (PR) structure can contain arbitrary subsets of query terms of the form {t : t  Q} as concepts. The PR-structure also differs from the PH-structure in the way the concepts in the structure are matched in the document. In order to match the document, the individual terms in a concept in the PRstructure may occur in any order within a window of fixed length. In this paper, we fix the window size to 4|t| terms, where |t| is the number of terms in the concept. This approach follows the definition of term proximity as defined by Metzler and Croft [27].

2.3.2 Hyperedges
As described in Section 2.1, a na®ive induction approach may result in an exponential number of hyperedges in a query hypergraph. Instead, for the purpose of this paper, we limit our attention to two types of hyperedges, which have an intuitive appeal from an information retrieval perspective.
(1) Local hyperedges. For each concept   KQ, we define a hyperedge ({}, D). This local edge4 represents the contribution of the concept  to the total document relevance score, regardless of the other query concepts. As we show in the next section, the factors defined over the local edges are akin to the functions that are usually employed in the existing log-linear retrieval models [14, 27].
(2) Global hyperedge. In addition to the local edges, we define a single global hyperedge (KQ, D) over the entire set of query concepts KQ. This global hyperedge provides the evidence about the contribution of each concept   KQ given its dependency on the entire set of query concepts KQ. Unlike in the case of local edges, the factors defined over the global hyperedge cannot be easily expressed using the existing log-linear retrieval models.
Figure 2 provides a simple example of these two types of hyperedges. The hyperedges at the bottom of the hypergraph in Figure 2 are the local edges, while the hyperedge at the top is the global hyperedge.

2.4 Factors e(ke, D)
Following the hyperedge induction process described in Section 2.3.2, in this section we define two types of factors. The local factors ≠ corresponding to the local edges ≠ are defined in Section 2.4.1; the global factor ≠ corresponding to the global hyperedge ≠ is defined in Section 2.4.2.
Both local and global factors incorporate a matching function f (, X), which assigns a score to the occurrences of the concept  in a text fragment X. As a matching function, following some previous work on log-linear retrieval models [4, 14, 27], we use a log of the language modeling estimate for concept  with Dirichlet smoothing [45], i.e.

f (, X)

log

tf (,

X)

+

µ

tf (,C) |C|

µ + |X|

,

(3)

where tf (, X) and tf (, C) are the number of occurrences of the concept  in the text fragment and the collection, respectively; µ is a free parameter; |X| is the number of terms in X, and |C| is the total number of terms in the collection.

2.4.1 Local Factors
The local factors are defined over the local edges ({}, D). A local factor assigns a score to the occurrences of concept  in the document D, regardless of the other query concepts. Therefore, a local factor is defined similarly to the previously proposed log-linear retrieval models [4, 14, 27]

"

"

({}, D) exp ()f (, D) ,

(4)

4From now on, we refer to the local hyperedges simply as edges, since they are defined over a vertex pair, rather than an arbitrary set of vertices.

944

where () is an importance weight assigned to the concept , and f (, D) is a matching function between the concept  and the document D.
2.4.2 The Global Factor
The global hyperedge (KQ, D) described in Section 2.3.2, represents a dependency between the entire set of query concepts. In this section, we present a global factor that is defined over this hyperedge.
A common way to estimate a dependency between query terms is using a measure of their proximity in a retrieved document [11, 26, 27, 39]. Analogously, we may simply choose to estimate a dependency between query concepts using similar proximity measures. However, there are two notable difficulties that impede an application of this approach to concept dependency.
First, the existing term proximity measures usually capture close, sentence-level, co-occurrences of the query terms in a retrieved document [27, 32, 39]. The dependency range is much longer for concept dependencies. For instance, in the example in Figure 1(b), the concepts dog and law enforcement do not ever appear in the same sentence. However, the dependency between them is revealed when examining their co-occurrences in a larger text passage.
Second, since concepts can be arbitrarily complex syntactic expressions, the probability of observing a concept cooccurrence is much lower than the probability of observing a term co-occurrence, even in large collections. For instance, most documents in the retrieved list for the query in Figure 1, do not contain both of the concepts dog and law enforcement in a context of a single passage.
Therefore, instead of estimating the dependency between query concepts using the standard proximity measures, we leverage a long history of research on passage retrieval [3, 8, 9, 25, 16, 40, 41] for the derivation of the global factor.
In the passage retrieval literature, a document is often segmented into overlapping passages of text of fixed size [16, 17]. The document is then scored using some combination of document-level and passage-level scores. One of the most successful and frequently-used score combinations is the Max-Psg combination, which uses the highest scoring passage to assign a score to the document [3, 8, 16, 24, 41].
Similarly to the Max-Psg retrieval model, we define the global factor using a passage , which receives the highest score among the set D of passages extracted from the document D. Formally,

(KQ, D)

" exp max

X

(,

KQ

)f

(,



" ),

(5)

D KQ

where (, KQ) is the importance weight of the concept  in the context of the entire set of query concepts KQ, and f (,) is a matching function between the concept  and a passage   D.
Intuitively, the global factor in Equation 5 assigns a higher
relevance score to a document that contains many important concepts in the confines of a single passage. Note that the importance weight (, KQ) of a concept in the global factor is determined not only by the concept itself ≠ as in the case of the importance weights (, D) in the local factors ≠ but also by the concepts that co-occur together with the concept
in the passage .

Feature GF() WF() QF() CF() DF() AP()

Description
Frequency of  in Google n-grams Frequency of  in Wikipedia titles Frequency of  in a search log Frequency of  in the collection Document frequency of  in the collection A priori constant weight (=1)

Table 2: Concept importance features .

2.5 Query Hypergraph Parameterization
In the previous section, we introduced two types of concept weights that parameterize the ranking function in Equation 2. First, there are the independent importance weights () that parameterize the local factors (see Equation 4). Second, there are the importance weights (, KQ) that assign weight to a concept, while taking into account the rest of the concepts in the query (see Equation 5).
In this section, we consider two possible parameterization schemes for these concept weights. In Section 2.5.1, we consider parameterization by structure. Conversely, in Section 2.5.2, we examine parameterization by concept.

2.5.1 Parameterization By Structure
A simple way to parameterize the importance weights () and (, KQ), is to make the assumption that the weights of all the concepts in the same structure are tied. Formally:

i, j   : (i) = (j) = () i, j   : (i, KQ) = (j, KQ) = (, Q)

This assumption has the benefit of significantly reducing the number of free parameters in the retrieval model, thereby greatly simplifying the estimation process. Due to its simplicity, parameterization by structure is often used in the log-linear retrieval models [14, 27, 32].
Using parameterization by structure and the definitions of local and global factors in Section 2.4, we can explicitly rewrite the ranking function in Equation 2 as

sc(Q, D) = X () X f (, D) +

 Q



+ max X (, Q) X f (, ).

D Q



(6)

2.5.2 Parameterization By Concept
The main drawback of parameterization by structure is the fact that it implies that all the concepts in the same structure are equally important for expressing the query intent. This implication is not always true, especially for more verbose, natural language queries, which may benefit from assigning varying concept weights [2, 4, 22].
Therefore, we may wish to remove the restriction imposed in the previous section, and parameterize the concept weights based on the concepts themselves rather than their respective structures. Assigning a single weight to each concept is clearly infeasible, since the number of concepts is exponential in the size of the vocabulary. Therefore, we take a parameterization approach proposed in recent work on query modeling [2, 4, 5, 22, 35, 38], and represent each concept using a combination of importance features, , described in Table 2. These importance features are based

945

on concept frequencies, and can be efficiently computed and cached, even for large-scale collections.
Using these importance features, we can explicitly rewrite the ranking function in Equation 2 as

sc(Q, D) = X X (, ) X ()f (, D) +

Q 



+ max X X (, , Q) X ()f (, ).

D Q 



(7)

2.5.3 Parameter Estimation
To estimate the free parameters (∑) in Equation 6 and Equation 7, we rely on a large and growing body of literature on the learning to rank methods for information retrieval (see Liu [23] for a survey). As a base algorithm for parameter optimization we make use of the coordinate ascent (CA) algorithm proposed by Metzler and Croft [28].
The CA algorithm iteratively optimizes a target metric (in our case, retrieval metric such as MAP) by performing a series of one-dimensional line searches. It repeatedly cycles through each of the parameters (∑), while holding all other parameters fixed. This process is performed iteratively over all parameters until the gain in the target metric is below a certain threshold.
We use the CA algorithm primarily for its simplicity, efficiency and effectiveness, as demonstrated by the previous work [4, 5, 27]. However, any other learning to rank approach that estimates the parameters for linear models such as RankSVM [15] or RankNet [7] can be adopted as well.
To ensure the scalability of our retrieval model, we compute the global factor (Equation 5) only for the top thousand documents retrieved by the local factors (Equation 4). Therefore, the setting of the importance weights () will affect the document ranking, which, in turn, will affect the choice of the highest-scoring passages and subsequently the setting of the importance weights (, KQ).
Accordingly, we perform the optimization in two stages. We decompose sc(Q, D) into its local and global components. First, we optimize the local component (i.e., the weights ()). Then, we fix the weights of the local component, and optimize the global component (i.e., the weights (, KQ)). Each of these optimizations is done using the standard CA algorithm.

3. RELATED WORK
In this paper we describe a general retrieval framework that models dependencies between arbitrary query concepts using a query hypergraph. It is important, therefore, to examine the connections between some of the well known retrieval models and query hypergraphs.
3.1 Bag-of-Words Models
As Zobel and Mofat [46] point out, the majority of the standard bag-of-words models in IR can be generally expressed by the following summation:
sc(Q, D) X (t, Q)f (t, D),
tQ
where (t, Q) and f (t, D) are some arbitrary functions (which may include normalization constants) defined over a query

term t and its occurrences in the query and the document, respectively. Examples of such models include, among others, the query likelihood model [33], BM25 [34] and divergence from randomness [1].
Therefore, it is easy to show that all of these bag-of-words models can be straightforwardly modeled using a query hypergraph. To induce such a hypergraph, we simply need to define a single QT-structure t = {t1, t2, . . .}, and a set of local edges
E = {(t, D) : t  t}.
3.2 Passage Retrieval
There is a long history of passage-based retrieval models in information retrieval [3, 8, 9, 16, 41, 40, 24]. These retrieval models are typically defined using vector space models [9, 16, 17] or language models [2, 24, 40], and employ a simple bag-of-words query representation. One of the most common passage retrieval techniques is Max-Psg, which uses the passage with the highest score for document score derivation [3, 8, 16, 24, 41].
Max-Psg with the bag-of-words query representation is a special case of the general query hypergraph described in this paper. Our model combines the recent advances in retrieval models that go beyond the bag-of-words query representations with passage retrieval models.
In addition, it is important to mention some recent work on query expansion [21] and query reformulation [43] using passage-based evidence, which uses hierarchical graphical representation of the query, similar to the one presented in this paper. This work is orthogonal to ours, as it uses passage evidence to augment the query with new concepts, rather than to model the query and the retrieval function. Combining this work on query expansion and reformulation with the retrieval models based on query hypergraphs is a promising direction for future work.
3.3 Term Dependencies
The advent of large-scale web corpora encouraged the development of retrieval models that employ phrases and proximity matches to model term dependencies [27, 29, 39, 14, 26, 32]. Most of these retrieval models take a log-linear form, and can be modeled using a query hypergraph with the structures described in Section 2.3.1, but without the inclusion of the global hyperedege.
Retrieval models that employ term dependencies usually resort to parameterization by structure [27, 14, 39, 32] (as described in Section 2.5.1). While this assumption significantly reduces the number of the free parameters in the retrieval model, it may be detrimental to the performance of verbose natural language queries that may contain concepts of variable importance.
Recently, researchers started to examine retrieval models that employ parameterization by concept. To avoid learning a separate weight for each concept, these models represent a concept using a set of features [4, 5, 22, 35, 38]. This approach significantly outperforms parameterization by structure, especially for verbose natural language queries. Accordingly, we also employ parameterization by concept in the retrieval with query hypergraphs (see Section 2.5.2).
3.4 Higher-Order Term Dependencies
To the best of our knowledge, there is very little prior work on retrieval with higher-order term dependencies (i.e., de-

946

pendencies between arbitrary concepts rather than terms). One notable exception is an early work on generalized term dependencies by Yu et al. [44], which derives higher-order dependencies from pairwise term dependencies. However, the model proposed by Yu et al. [44] is infeasible for largescale collections, since it requires an explicit computation of the probability of relevance for each individual query term, as well as pairs and triples of query terms.
A more recent retrieval model that attempts to incorporate higher-order term dependencies is the Full Dependence (FD) variant of the Markov random field model proposed by Metzler and Croft [27]. The FD model, however, is only able to capture dependencies between multiple terms, rather than multiple concepts. For instance, it can model a dependency between the terms in the triple (dog, law, enforcement), but it cannot model a dependency between the pair of concepts (dog, "law enforcement").

4. EVALUATION
4.1 Experimental Setup
All the empirical evaluation described in this section is implemented using Indri, an open-source search engine [37]. The structured query language implemented in Indri natively supports multiple types of concepts, including exact phrases and proximity matches, as well as customizable concept weighting schemes. As a result, Indri provides a flexible and convenient platform for evaluating the retrieval performance of query hypergraphs.
Table 4 presents a summary of the TREC corpora used in our experiments. The corpora vary both by type (Robust04 is a newswire collection, Gov2 is a crawl of the .gov domain, and ClueWeb-B is a set of pages with the highest crawl priority derived from a large web corpus), number of documents, and number of available topics, thereby providing a diverse experimental setup for assessing the robustness of retrieval with query hypergraphs.

Name Robust04 Gov2 ClueWeb-B

# Docs
528,155 25,205,179 50,220,423

Topic Numbers
301-450, 601-700 701-850 1-100

Table 4: Summary of the TREC collections and topics used for evaluation.
For the Robust04 and Gov2 collections, a standard Porter stemmer is used. In contrast, the ClueWeb-B collection is stemmed using the Krovetz stemmer, which is a "light" stemmer, as it makes use of inflectional linguistic morphology [19] and is especially suitable for web collections where aggressive stemming can decrease precision at top ranks [31]. Stopword removal is performed on both documents and queries using the standard INQUERY stopword list. The free parameter µ in the concept matching function f (, X) (see Equation 3) is set according to the default Indri configuration of the Dirichlet smoothing parameter.
Since query hypergraphs attempt to capture complex dependencies between query concepts, we apply them to the description portions of the TREC topics. TREC topic descriptions express the information needs behind the topics using verbose natural language queries. For instance, a description portion of the TREC topic entitled "hydrogen energy" is a question "What is the status of research on hy-

drogen as a feasible energy source?". As shown by previous work, these queries are more likely to benefit from complex representation and weighting schemes than their keyword counterparts [2, 20, 22].
In order to compute the global factor (Equation 5), we segment each document into semi-overlapping passages of 150 words (i.e., the overlap between the passages is 75 words). As shown in previous work on passage retrieval [3, 9, 8, 16], this passage configuration leads to improved effectiveness on most TREC collections.
The optimization of the free parameters for all the baselines and the proposed retrieval methods is done using threefold cross-validation with mean average precision (MAP) as the target metric. In addition to MAP, we also report ERR@20, an early precision metric that was adopted as the official retrieval performance metric at the TREC 2010 Web Track [10]. The statistical significance of differences in the performance of the proposed retrieval methods with respect to their respective baselines is determined using a two-sided Fisher's randomization test [36] with 25,000 permutations and  < 0.05.
4.2 Retrieval Performance Evaluation
In this section, we compare the performance of the retrieval with query hypergraphs to a number of state-of-theart baselines that incorporate exact phrase matches, proximities, and concept weight parameterization. These baselines do not, however, incorporate concept dependencies.
The query hypergraph representation, proposed in this paper, further extends each of these baselines with higher-order term dependencies via the inclusion of the global hyperedge and the corresponding global factor (KQ, D) (see Equation 5). In the remainder of this section, we examine the improvements in the retrieval performance (or lack thereof) of these baselines when they are extended with the query hypergraph representation.
4.2.1 Query Likelihood Baseline
Query likelihood [33] is a popular retrieval method that employs a bag-of-words query representation. In this section, we juxtapose the retrieval performance of the query likelihood baseline (denoted QL) to the performance of a query hypergraph that includes a single QT-structure (structure that contains the individual query terms as concepts). We denote this hypergraph representation H-QL. This juxtaposition demonstrates the contribution of the global factor (KQ, D) (see Equation 5) to the retrieval performance.
Table 3(a) shows the comparison between the QL and the H-QL methods. The results in Table 3(a) demonstrate that the addition of the global factor (KQ, D) into a bag-ofwords representation significantly improves its retrieval effectiveness in all the cases.
Note that the H-QL method is equivalent to the Max-Psg method proposed in the previous work [3, 8, 9, 16, 41], which ranks the documents in the collection by a combination of the document score and the score of its highest-scoring passage. The improvements in retrieval performance shown in Table 3(a) are in line with the improvements attained by the Max-Psg method reported in this previous work.
4.2.2 Markov Random Fields Baselines
Markov random fields for information retrieval (MRF-IR) is a state-of-the-art retrieval framework that incorporates

947

Robust04

Gov2

ERR@20 MAP

ERR@20 MAP

QL

11.44

24.24

15.06

25.66

H-QL 11.66

25.49q (+5.2%) 15.33

27.24q (+6.2%)

(a) Query likelihood (QL) and its hypergraph representation (H-QL).

ERR@20 7.32 7.63

ClueWeb-B MAP 12.75 13.07q (+2.5%)

Robust04

Gov2

ClueWeb-B

ERR@20 MAP

ERR@20 MAP

ERR@20 MAP

SD

11.76

25.62

15.73

27.97

7.58

12.99

H-SD 11.93

26.65s (+4.0%) 15.93

28.63s (+2.4%) 7.78

13.08 (+0.7%)

(b) Sequential dependence model (SD) and its hypergraph representation (H-SD) parameterized by structure.

Robust04

Gov2

ClueWeb-B

ERR@20 MAP

ERR@20 MAP

ERR@20 MAP

FD

11.87

25.69

16.10

28.25

8.21

13.28

H-FD 11.94

26.50f (+3.1%) 16.02

28.70f (+1.6%) 8.15

13.35 (+0.5%)

(c) Full dependence model (FD) and its hypergraph representation (H-FD) parameterized by structure.

Robust04

Gov2

ClueWeb-B

ERR@20 MAP

ERR@20 MAP

ERR@20 MAP

WSD 12.04

27.41

16.52

29.36

8.58

14.56

H-WSD 12.34w

27.79w (+1.4%) 16.56

29.82w (+1.6%) 8.31

14.68 (+0.8%)

(d) Weighted sequential dependence model (WSD) and its hypergraph representation (H-WSD) parameterized by concept.

Table 3: Evaluation of the performance of the retrieval with query hypergraphs. Best result per column is marked in boldface. Statistically significant differences with a non-hypergraph baseline are marked by the first letter in its title. The numbers in the parentheses indicate the percentage of improvement in MAP over the baseline.

term dependencies. It was first proposed by Metzler and Croft [27], and was shown to be highly effective, especially for large-scale web collections.
Metzler and Croft propose two instantiations of the general MRF-IR framework. The first instantiation is the sequential dependence model (denoted SD), which incorporates only dependencies between adjacent query terms. The second instantiation is the full dependence model (FD), which incorporates dependencies between all query term subsets5.
The SD and FD baselines can be extended with a respective hypergraph that includes three structures: QT, PR and PH (refer to Section 2.3.1 for the exact definitions of these structures). We denote these hypergraph representations HSD and H-FD, respectively. These hypergraphs are parameterized by structure, and their ranking functions are derived according to Equation 6.
Table 3(b) compares the performance of the sequential dependence baseline (SD) and its corresponding hypergraph H-SD. As evident from Table 3(b), in most cases (except for the ClueWeb-B collection) the retrieval effectiveness (in terms of MAP) is significantly improved by the hypergraph extension. However, these improvements are smaller than in the case of the QL baseline.
Similarly, Table 3(c) compares the performance of the full dependence baseline (FD) and its corresponding hypergraph H-FD. Comparing Table 3(b) and Table 3(c), we can see that in most cases the FD baseline slightly outperforms the SD baseline. However, these differences were not found to be statistically significant.
When comparing the performance of the FD baseline and its corresponding hypergraph H-FD, Table 3(c) demonstrates
5Due to the verbosity of the description queries, in this paper, we limit our evaluation to query term subsets with at most three terms.

that the inclusion of the global factor results in an improved retrieval effectiveness (in terms of MAP) for all collections, and in statistically significant improvements for the Robust04 and Gov2 collections.
In addition, we can compare between the retrieval performance of the hypergraphs H-SD and H-FD. Similarly to the case of the baselines SD and FD, no statistically significant differences were found in the performance of these hypergraphs. H-FD is slightly more effective for the ClueWeb-B and the Gov2 collections, while being slightly less effective for the Robust04 collection.
4.2.3 Weighted Sequential Dependence Model
A major drawback of the SD and the FD baselines is that they use the parameterization-by-structure approach (see Section 2.5.1), which ties the importance weights (∑) of all the concepts that belong to the same structure (i.e., all the terms, phrases and proximities get the same respective weights). This parameterization can be detrimental, especially for longer, more verbose queries that may mix concepts of differing importance.
Recently, Bendersky et al. [4] proposed a weighted variant of the sequential dependence mode (denoted WSD) that overcomes this drawback. The concept weights in the WSD method are parameterized using a set of importance features, associated with each concept based on its respective structure, as described in Section 2.5.2.
We extend the WSD baseline with a query hypergraph HWSD. The H-WSD includes the global factor (KQ, D), which is also parameterized by concept. The ranking function for the H-WSD hypergraph is presented in Equation 7.
Table 3(d) compares the retrieval performance of the WSD baseline and its corresponding hypergraph H-WSD. While the retrieval improvements that stem from this hypergraph extensions are not as pronounced as in the cases of the QL, SD

948

and FD baselines, the addition of the global factor to the WSD baseline still results in effectiveness gains for all the collections and most of the metrics. For instance, for the Gov2 collection, the H-WSD method improves the performance (in terms of MAP) for 60% of the queries compared to the WSD baseline, while hurting only 30% of the queries. For 7% of the queries MAP is improved by more than 25%, while there is a 25% drop in performance for only 2% of the queries.
4.2.4 Further Analysis
In addition to comparing each individual query hypergraph model to its respective baseline, some general trends can be observed in Table 3. First, it is interesting to compare the relative differences in gains across the baselines, when the global factor is added. The gains are the largest for the QL baseline, which does not include any term dependencies, and decrease as more term dependencies are added by the SD and the FD baselines. As an example, for the Gov2 collection, the effectiveness gain as a result of the global factor inclusion decreases from 6.2% for the QL baseline to 1.6% for the FD baseline.
These diminishing returns demonstrate that there is some degree of overlap between the effect of term dependencies and higher-order term dependencies on the retrieval effectiveness. The overlap is not complete, however, since the addition of the global factor still has a statistically significant impact on the retrieval performance in most cases. This is true even for the FD baseline, which includes term dependencies between all query term pairs and triples.
Finally, we note that the parameterization of the ranking function by concept (as in the WSD baseline) (a) significantly improves the retrieval performance of the ranking function parameterized by structure (as in the SD baseline), and (b) further diminishes the gains obtained through the inclusion of the global factor. While H-WSD is the best-performing retrieval method (in terms of MAP) in Table 3, its average effectiveness gain over the WSD baseline is only 1.3%. For comparison, the average effectiveness gain of the H-QL method over the QL baseline is 4.7%.
4.3 Parameterization Analysis
In this section we analyze the parameterization of the query hypergraph. We examine both parameterization-bystructure and parameterization-by-concept regimes, which are described in detail in Section 2.5.1 and Section 2.5.2, respectively.
Recall that the parameters of the query hypergraph are optimized using the coordinate ascent algorithm such that the ranking function is decomposed into local and global factors (see Section 2.5.3). In this section, due to the space constraints, we focus our attention on the resulting parameterization for the Robust04 collection. We choose this collection, since it has the largest number of queries, and the learned parameterization is stable across all folds. However, it is important to note that the findings in this section hold for the other two collections as well.
4.3.1 Parameterization by Structure
Table 5 shows the hypergraph parameters for the local factors (()) and the global factor ((, Q)), averaged across folds, when the parameterization-by-structure approach is used (see Equation 6). These parameters correspond to the H-SD model, the results for which are shown in Table 3(b).

() (, Q) QT +0.520 +0.322 PH +0.065 +0.017 PR +0.065 -0.011
Table 5: Query hypergraph parameterization by structure (Robust04 collection).

(, )

 QT PR+PH

GF -0.007

0

WF +0.017 +0.007

QF +0.012

0

CF -0.021

0

DF -0.018

0

AP +0.540 +0.029

(, , Q) QT PR+PH -0.005 -0.001 +0.002 +0.002 +0.007 +0.008 -0.008 0 -0.001 0 +0.298 +0.003

Table 6: Query hypergraph parameterization by concept (Robust04 collection).

Note that both for the local and the global factors the weights assigned to the term structure (QT) are the highest, which is in line with other models that incorporate term dependencies [27]. This demonstrates that despite the importance of term dependencies, individual term occurrences are still the most important indicators of relevance.
In addition, in Table 5, the parameters of the local factors are weighted higher than the parameters of the global factor. Recall that the global factor is defined over the highestscoring passage in the document. Thus, the lower weight of the global factor parameters is in line with previous work, where passage evidence is typically weighted lower than the document evidence [3, 41, 16].
Finally, note the negative weight assigned to the proximity (PR) structure in the global factor. While small, this negative weight is consistent across folds, as well as in the other collections. Intuitively, this negative weight indicates that in the highest-scoring passage of the relevant document we expect to encounter exact phrase concepts, rather than unordered proximity concepts.
4.3.2 Parameterization by Concept
Table 6 shows the hypergraph parameters for the local factors ((, )) and the global factor ((, , Q)), averaged across folds, when the parameterization-by-concept approach is used (see Equation 7). These parameters correspond to the H-WSD model, the results for which are shown in Table 3(d). For the convenience of presentation and to reduce weight sparsity, we combine the weights of the PH and PR structures in the PR+PH column.
Note that a priory constant importance feature AP generally receives the highest weight. This is due to the fact that setting all the other feature weights to zero yields exactly the parameterization-by-structure approach.
Features such as document frequency (DF), collection frequency (CF) and Google frequency (GF) receive, as expected, negative weights in most cases. In contrast, the query frequency (QF) and the Wikipedia title frequency (WF) features get positive weights, which indicates that the appearance of the concept in page title or in a search query is positively correlated to the concept importance.

949

5. CONCLUSIONS
The retrieval framework proposed in this paper represents a query by means of a hypergraph. In the query hypergraph, each vertex corresponds to a concept, and these concepts are grouped into disjoint structures. A hyperedge in the query hypergraph represents a concept dependency. We describe a principled derivation of a ranking function based on the factorization of the query hypergraph. We then propose two parameterization regimes for the derived ranking function, based on either structures or concepts.
The proposed retrieval framework exhibits three important characteristics. First, it models term dependencies as concepts. Second, it models dependencies between these concepts (i.e., higher-order term dependencies). Finally, it assigns weights to concepts and concept dependencies, proportionate to their importance for expressing the query intent. For verbose natural queries, the proposed retrieval framework significantly improves the retrieval effectiveness of several state-of-the-art retrieval methods that do not incorporate higher-order term dependencies.
6. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
7. REFERENCES [1] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Transactions on Information Systems, 20(4):357≠389, October 2002. [2] M. Bendersky and W. B. Croft. Discovering key concepts in verbose queries. In Proc. of SIGIR, pages 491≠498, 2008. [3] M. Bendersky and O. Kurland. Utilizing passage-based language models for document retrieval. In Proc. of ECIR, pages 162≠174, 2008. [4] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In Proc. of WSDM, pages 31≠40, 2010. [5] M. Bendersky, D. Metzler, and W. B. Croft. Parameterized concept weighting in verbose queries. In Proc. of SIGIR, 2011. [6] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. [7] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proc. of ICML, pages 89≠96, 2005. [8] D. Cai, S. Yu, J.-R. Wen, and W.-Y. Ma. Block-based web search. In Proc. of SIGIR, pages 456≠463, 2004. [9] J. Callan. Passage-level evidence in document retrieval. In Proc. of SIGIR, pages 302≠310, 1994. [10] C. L. A. Clarke, N. Craswell, I. Soboroff, and G. V. Cormack.
Overview of the TREC 2010 Web Track. In Proc. of TREC-10, 2011. [11] R. Cummins and C. O'Riordan. Learning in a pairwise term-term proximity framework for information retrieval. In Proc. of SIGIR, pages 251≠258, New York, NY, USA, 2009. [12] J. Fagan. Automatic phrase indexing for document retrieval. In Proc. of SIGIR, pages 91≠101, 1987. [13] J. Gao, J.-Y. Nie, G. Wu, and G. Cao. Dependence language model for information retrieval. In Proc. of SIGIR, pages 170≠177, 2004. [14] J. Gao, H. Qi, X. Xia, and J.-Y. Nie. Linear discriminant model for information retrieval. In Proc. of SIGIR, pages 290≠297, 2005. [15] T. Joachims. Optimizing search engines using clickthrough data. In Proc. of KDD, pages 133≠142, 2002. [16] M. Kaszkiel and J. Zobel. Passage retrieval revisited. In Proc. of SIGIR, pages 178≠185, 1997.

[17] M. Kaszkiel and J. Zobel. Effective ranking with arbitrary passages. Journal of the American Society for Information Science, 52:344≠364, February 2001.
[18] M. Kaufmann, M. van Kreveld, and B. Speckmann. Subdivision Drawings of Hypergraphs. In Graph Drawing, pages 396≠407. Springer, 2009.
[19] R. Krovetz. Viewing morphology as an inference process. In Proc. of SIGIR, pages 191≠202, 1993.
[20] G. Kumaran and V. R. Carvalho. Reducing long queries using query quality predictors. In Proc. of SIGIR, pages 564≠571, 2009.
[21] H. Lang, D. Metzler, B. Wang, and J.-T. Li. Improved latent concept expansion using hierarchical markov random fields. In Proc. of CIKM, pages 249≠258, 2010.
[22] M. Lease, J. Allan, and W. B. Croft. Regression rank: Learning to meet the opportunity of descriptive queries. In Proc. of ECIR, pages 90≠101, 2009.
[23] T.-Y. Liu. Learning to Rank for Information Retrieval. Foundations and Trends in Information Retrieval, 3(3), 2009.
[24] X. Liu and W. B. Croft. Passage retrieval based on language models. In Proc. of CIKM, pages 375≠382, 2002.
[25] X. Liu and W. B. Croft. Cluster-based retrieval using language models. Proc. of SIGIR, pages 186≠193, 2004.
[26] Y. Lv and C. Zhai. Positional language models for information retrieval. In Proc. of SIGIR, pages 299≠306, 2009.
[27] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. Proc. of SIGIR, pages 472≠479, 2005.
[28] D. Metzler and W. B. Croft. Linear feature-based models for information retrieval. Information Retrieval, 10(3):257≠274, 2007.
[29] G. Mishne and M. de Rijke. Boosting Web Retrieval Through Query Operations. In Proc. of ECIR, pages 502≠516, 2005.
[30] J. H. Park, W. B. Croft, and D. A. Smith. A quasi-synchronous dependence model for information retrieval. In Proc. of CIKM, pages 17≠26, 2011.
[31] F. Peng, N. Ahmed, X. Li, and Y. Lu. Context sensitive stemming for web search. In Proc. of SIGIR, pages 639≠646, 2007.
[32] J. Peng, C. Macdonald, B. He, V. Plachouras, and I. Ounis. Incorporating term dependency in the DFR framework. In Proc. of SIGIR, pages 843≠844, 2007.
[33] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proc. of SIGIR, pages 275≠281, 1998.
[34] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. In Proc. of SIGIR, pages 232≠241, 1994.
[35] L. Shi and J.-Y. Nie. Using various term dependencies according to their utilities. In Proc. of CIKM, pages 1493≠1496, 2010.
[36] M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In Proc. of CIKM, pages 623≠632, 2007.
[37] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indri: A language model-based search engine for complex queries. In Proc. of IA, 2004.
[38] K. M. Svore, P. H. Kanani, and N. Khan. How good is a span of terms?: exploiting proximity to improve web retrieval. In Proc. of SIGIR, pages 154≠161, 2010.
[39] T. Tao and C. Zhai. An exploration of proximity measures in information retrieval. In Proc. of SIGIR, pages 295≠302, 2007.
[40] M. Wang and L. Si. Discriminative probabilistic models for passage based retrieval. In Proc. of SIGIR, pages 419≠426, 2008.
[41] R. Wilkinson. Effective retrieval of structured documents. In Proc. of SIGIR, pages 311≠317, 1994.
[42] J. Xu and W. B. Croft. Query expansion using local and global document analysis. Proc. of SIGIR, pages 4≠11, 1996.
[43] X. Xue, W. B. Croft, and D. A. Smith. Modeling reformulation using passage analysis. In Proc. of CIKM, pages 1497≠1500, 2010.
[44] C. T. Yu, C. Buckley, K. Lam, and G. Salton. A Generalized Term Dependence Model in Information Retrieval. Technical report, Cornell University, 1983.
[45] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems, 22(2):179≠214, 2004.
[46] J. Zobel and A. Moffat. Exploring the similarity space. SIGIR Forum, 32(1):18≠34, 1998.

950

Time-Based Calibration of Effectiveness Measures

Mark D. Smucker
Department of Management Sciences University of Waterloo, Canada
mark.smucker@uwaterloo.ca

Charles L. A. Clarke
School of Computer Science University of Waterloo, Canada
claclark@plg.uwaterloo.ca

ABSTRACT
Many current effectiveness measures incorporate simplifying assumptions about user behavior. These assumptions prevent the measures from reflecting aspects of the search process that directly impact the quality of retrieval results as experienced by the user. In particular, these measures implicitly model users as working down a list of retrieval results, spending equal time assessing each document. In reality, even a careful user, intending to identify as much relevant material as possible, must spend longer on some documents than on others. Aspects such as document length, duplicates and summaries all influence the time required. In this paper, we introduce a time-biased gain measure, which explicitly accommodates such aspects of the search process. By conducting an appropriate user study, we calibrate and validate the measure against the TREC 2005 Robust Track test collection. We examine properties of the measure, contrasting it to traditional effectiveness measures, and exploring its extension to other aspects and environments. As its primary benefit, the measure allows us to evaluate system performance in human terms, while maintaining the simplicity and repeatability of system-oriented tests. Overall, we aim to achieve a clearer connection between user-oriented studies and system-oriented tests, allowing us to better transfer insights and outcomes from one to the other.
Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation (efficiency and effectiveness)
General Terms
Experimentation, Human Factors, Measurement
Keywords
Information retrieval, search evaluation
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12≠16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

1. INTRODUCTION

System-oriented tests and user-oriented studies represent competing approaches to search engine evaluation. A systemoriented test typically develops a set of relevance judgments to determine the quality of ranked document lists returned in response to a query [39]. Effectiveness measures computed over these ranked lists provide a method for tuning search engines and comparing one engine to another. In contrast, a user-oriented study examines actual user behavior during interactive retrieval sessions. These studies span a spectrum from intensive laboratory investigations involving a relatively small number of subjects [18] to large-scale analyses of interaction logs capturing the search activities of millions of people [9, 42, 43].
The value of a user-oriented study lies in its ability to reflect many aspects of the search process, including interaction and presentation. Unfortunately, user studies can be complex and costly. Running subjects through a laboratory experiment can require substantial time and money, and we may need to repeat experiments as elements of the interface and engine change. On the other hand, interaction logs are noisy, uncontrolled, and capture only a portion of the user experience.
The value of a system-oriented test lies in its simplicity and repeatability. Effectiveness measures isolate and evaluate a single element of the search engine, i.e., the quality of its ranked list. Once we develop a set of relevance judgments, evaluation measures can be re-computed with little additional expense or effort. As software developers modify and train ranking algorithms, effectiveness measures may be applied over and over again, as often as needed.
Various researchers [7, 10, 43] suggest a generic framework that encapsulates many of the effectiveness measures in current use, including Normalized Discounted Cumulative Gain (nDCG) [17], Rank Biased Precision (RBP) [23], and Expected Reciprocal Rank (ERR) [9]. All of these measures can be expressed as a sum over documents in a ranked list,

1

N

gk dk ,

(1)

k=1

where gk represents the gain associated with the document appearing at rank k, and dk represents a discount associated with rank k, which is independent of the document. In practice, we either compute the sum to some fixed but arbitrary depth K or until we know its value within some . The normalization factor N is optional, but if it is present, it serves to map the value of the sum into the range [0 : 1], which may be required for averaging over multiple queries.

95

RBP uses the discount formula dk = k-1, where  is a constant in the range [0 : 1]. Gain values for RBP are binary, with gk = 1 if the document at rank k is judged relevant, and gk = 0 if it is not judged relevant. ERR uses a reciprocal rank discount formula of dk = 1/k, and nDCG uses the discount formula dk = 1/ (log2(1 + k)). For both of these measures, a gain value may be interpreted as a relevance probability, i.e., the probability that a user viewing the document would judge it to be relevant. When applying these measures, the probabilities are estimated from graded relevance judgments, e.g., "definitive", "excellent", "good", etc. [24]. In the case of ERR, the gain value of a document at rank k depends on the gain values of the documents at ranks 1 to k - 1, with gain values increasingly attenuated when more relevant documents appear at higher ranks.
All of these measures may be interpreted in terms of a simple user model: The user starts at the first document and works their way down the list, eventually stopping when they become tired or bored. The discount dk indicates the probability that the user continues to rank k, and the gain gk represents the benefit (i.e., utility) to the user of viewing the document at rank k. Thus, the sum in Equation 1 can be understood as the expected total benefit experienced by the user, with the various gain values and discount formulae corresponding to different assumptions about user needs and behaviors. In particular, the attenuated gain values in ERR correspond to the idea that, after viewing each relevant document, the user derives less benefit from viewing additional relevant documents.
Underlying this user model is an implicit assumption that the user views documents at a constant rate, spending the same time assessing each one. Because of this assumption, these effectiveness measures can fail to accommodate aspects of the search process that directly impact the quality of the ranked list as experienced by the user. For example, most modern search engines provide captions, or summaries, that describe each of the documents [36]. A user scanning these summaries might quickly skip those results that are obviously not relevant to their needs, without ever viewing the contents of the associated documents. Similarly, we might expect shorter documents to take less time to assess than longer documents, since a user may spend less time skimming them. In addition, when a document is similar in content to a document a user has already viewed, it may be easy to assess quickly, regardless of its length. Once a user recognizes the duplicated content, they can immediately return to the result list and continue to the next result.
In this paper, we propose a framework for evaluation measures based on time, allowing us to directly accommodate aspects of the search process ignored by standard effectiveness measures. We retain the simple model of a user working their way down a result list, but with both gain and discount now expressed in terms of time. As the user works their way down the list, let G(t) be the cumulative gain experienced by the user at time t. In other words, if the user invests time t in assessing the result list, they will derive a total benefit of G(t). We model the possibility that the user stops at some point by a decay function D(t), which indicates the probability that the user continues until time t. Thus, D(0) = 1 and decreases monotonically to 0 as t  . Under this model, an equivalent of Equation 1 may be expressed as

1  dG

N0

D(t)dt. dt

(2)

Of course, measuring instantaneous gain would prove difficult in practice, and we quickly return to considering gain values one document at a time, but retaining decay as a function of time. By smearing gain equally across each document and converting D(t) into a step function, we may approximate Equation 2 with

1

N gkD(T (k)),

(3)

k=1

where T (k) is the expected time it would takes for a user to reach rank k and begin to assess the document. Although we have returned to a summation over ranks, we now explicitly consider time as a part of the measure.
We refer to Equation 3 as time-biased gain, and we spend the remainder of this paper developing and exploring this effectiveness measure. In its development, we face the potentially controversial and complex problem of providing a realistic estimate for T (k). Ideally, for use in an effectiveness measure, the estimate should be straightforward both to compute and to comprehend. Competing with this requirement is the need for a clear connection back to actual user behavior, so that the effectiveness measure provides a meaningful guide for training and tuning ranking algorithms.
We note that setting T (k) = c(k - 1), for some constant c, reduces Equation 3 back to Equation 1, which corresponds to a scenario in which the user views documents at a constant rate. To move beyond this simple scenario, we need to examine user behavior in a realistic context, calibrating our estimate of T (k) against the actions of these users. As a result, time-biased gain is more closely tied to a usage scenario than traditional evaluation measures. Moving to a different scenario requires a different calibration of the evaluation measure, a price we pay for the closer connection to user behavior.
After a review of related work, we discuss the calibration and validation of time-biased gain against the test collection employed in the TREC 2005 Robust Track experiments [39]. As part of this calibration, we report a user study in which we presented subjects with an interface styled after a modern web search engine, including a result page with ten querybiased summaries, and instructed them to find and save as many relevant documents as possible in the time allowed, while making as few mistakes as possible. The associated scenario is of a careful and determined user, with a topical information need, intending to identify as much relevant material as they can in the time they have available.
We believe this scenario is appropriate for the TREC Robust Track, and similar experiments, where a document is considered relevant if it contains any relevant material at all, and evaluation measures traditionally consider documents that are relatively deep in the result list. For other scenarios, a different calibration might be appropriate. For example, in the context of web search, where users may interact with fewer results and rarely dig as deeply in the result list, calibration might be based on interaction logs taken from the search engine itself.
After developing a T (k) function appropriate to our usage scenario, we consider other components of our time-biased gain measure, including the choice of a decay function. We explore properties of the measure and compare it to other evaluation measures. Finally, we end the paper with a discussion of extensions and future work.

96

2. RELATED WORK
We may interpret many current effectiveness measures in terms of the simple user model outlined in the introduction [7, 9, 17, 23, 43]. While all of these measures implicitly assume that a user progresses down the ranked list at a constant rate, they incorporate different assumptions about when and why the user will stop, and how much benefit or utility they can be expected to gain. All of these measures may be expressed in form of Equation 1 [7, 43]. Even precision at k may be interpreted in terms of a user who always views exactly the top k documents, and no more. Along the same lines, Robertson [26] retrofits a similar user model into the venerable Mean Average Precision (MAP) effectiveness measure, which may also be extended to incorporate graded relevance values similar to those employed by nDCG [15, 27, 30].
Turpin et al. [37], Yilmaz et al. [42], and Dupret [14] have gone beyond this simple model by explicitly incorporating summaries into their user models and thus into their metrics. An important difference between their work and ours is that we explicitly model time in addition to the click decisions. Summaries are designed to, and do speed the rate at which users find relevant documents [20, 36]. We believe it is important that metrics reflect the value of summaries by having a user model that incorporates the time required to separately evaluate summaries and documents. Time is important to users. Su [35] found from interviews with 40 users that the success category of "Efficiency - Time" was ranked first with the highest frequency of mention.
Turpin and Hersh [38] hypothesized that the time to read documents was a reason why batch-style and user evaluations produce different results. In their study, Turpin and Hersh found that the baseline system, which had a lower MAP, ranked shorter documents higher than the improved system. The improved system had a pivoted length normalization [31] component that preferred longer documents -- longer TREC documents have a higher prior probability of being relevant. Turpin and Hersh found that users of the baseline system were able to read more documents than the users of the improved system. As a result, users of the baseline system were able to compensate for its lower precision and find equal amounts of relevant material in the same amount of time. Our time-biased gain metric explicitly takes document length into consideration, for we have also found that users take longer to judge longer documents. Yang and Lad [41] and Arvola et al. [2] also consider the cost of reading in the evaluation of IR systems.
Our time-biased gain metric aligns well with Fuhr's probability ranking principle for interactive IR [16]. Fuhr carefully outlines that users of interactive IR systems have to make a series of decisions and that these decisions each have their own cost. For our metric, cost is measured in time, and both cost and gain are directly tied to the series of probabilistic decisions made by the user.
Much of this work can be viewed as following a framework for evaluation laid out by Cooper [11, 12] and Dunlop [13]. Cooper believed that effectiveness measures should be based on the utility a user personally gains from a retrieval system. A critical part of Cooper's plan was the use of validation experiments to measure the ability of an evaluation measure to predict utility. Dunlop built on and refined the ideas of Cooper. Rather than subjectively measure a user's utility, Dunlop established the use of HCI methods to predict user

performance in terms of the number of relevant documents found in a given amount of time.
Efforts to relate evaluation measures to models of user behavior can be seen as an attempt to bridge the gap between system-oriented tests and user-oriented studies. In these cases, the goal is to move system-oriented measures closer to user-oriented studies. Working from the other direction, efforts have been made to simulate the use of interactive retrieval systems in order to determine their quality [1, 4, 21, 40]. Efforts related to this approach includes the use of simulation to better understand interactive retrieval behavior [3, 19]. We see these two directions of research merging as system-oriented measures continue to increase the resolution of their user models [8].
3. CALIBRATION
At the core of our new metric, time-biased gain, is the computation of the time to reach rank k of a ranked list, which we will refer to as the function T (k). Given that we are retaining the traditional model of a user working down a ranked list in order, we need to estimate the amount of time it takes to process a given document. As such, we need to consider what is a reasonable process that a user would go through to determine if a document is relevant.
The majority of search interfaces now present a ranked list in two parts. The first part is the display of a series of document summaries that are intended to help the user decide if the corresponding full documents should be viewed or skipped. The second part is the full document itself.
Faced with this interface, the user must first examine the document summary and decide whether to click and view the full document or not. While it is certainly possible that a user may realize benefit from viewing the summary alone, we believe that for TREC-style topics and information needs, that a user needs to view the full document in order to realize gain from finding relevant material in the document.
Thus, we need to model the decision of whether to click or not and produce both an estimate of the time the decision takes to be made as well as an estimate of what the decision will be. If the user decides to click on a summary, then the user must study the document and make a decision about the document's relevance. Only if the user clicks on a relevant document's summary and then decides the document is relevant will the user realize any gain. Gain cannot be obtained from non-relevant documents nor from skipped relevant documents. As with the summary, we need to estimate time spent on the full document and the final relevance decision of the user.
There are nearly endless variables that could be taken into consideration in the models of decision making for summaries and documents. In this paper, we choose to focus on a few variables that are most likely to affect user decisions.
The most obvious variables affecting these decisions are the user and the topic. While users vary greatly and contribute large amounts of variance to user studies, we do not attempt to model different user strategies for processing ranked lists. Instead, our user model is an idealized individual representing the population as a whole. As for the topic, at this time we do not see an easy way to identify a priori how a topic will affect users' behavior, and we are forced to treat all topics the same.
After the user and topic, the document itself is likely the most important variable to consider. We consider two at-

97

tributes of the document to use in our model: the relevance of the document and the document's length. Relevance should be a strong indicator of how likely summaries will be clicked and whether or not the document will be recognized by users to be relevant [37]. The time to make decisions based on textual items is likely to be dependent on their length.
Given that summaries tend to be short and of nearly equal length, we will assume they are all assessed in the same amount time, TS seconds. For documents, let TD(l) = al+b, be the time in seconds to assess a document of length l, where l is measured in words. TD(l) models an evaluation process of scanning the document at a rate of a seconds per word and then a constant amount of time, b seconds, to account for carefully assessing a small amount of text and the overhead of making a decision about relevance. As we will justify, if a user views a duplicate of a document later, then we treat that duplicate as if it had zero length (l = 0).
We use a document's relevance to predict the probability that a summary will be clicked and to predict the probability that when viewed, a full document is recognized as relevant. Formally, let P (C|R) be the probability of a click given NIST relevance, where C and R are binary random variables. Also, let P (S|R) be the probability of saving a viewed document as relevant given NIST relevance, where S and R are binary random variables. Saving a document is the equivalent of recognizing a document to be relevant.
Putting both the time to make decisions and the probabilities of the decisions together, we have a model of the time it takes to process a document in the ranked list and can now estimate the time to reach rank k, i.e. T (k).
Let li be the length of the document at rank i. Let ri be the NIST binary relevance judgment associated with the document at rank i, where ri = 1 if the document is relevant, ri = 0 otherwise. Then, we estimate the expected time for the user to reach rank k as:

k-1

T (k) = TS + TD(li)P (C = 1|R = ri)

(4)

i=1

Not present in T (k) are the probabilities of the user saving a document as relevant, which are only needed for computation of the gain.
With Equation 4 in hand, we need to calibrate it to produce estimates in line with actual human behavior. We next explain how we calibrated T (k) and estimated P (S|R) based on a user study.

3.1 User Study
To calibrate our metric, we need actual user data that involves the processing of ranked lists. We utilize data collected as part of the user study conducted by Smucker and Jethani [34].
The user study presented participants with a user interface styled after modern web search engines. The interface had two types of web pages. One page looked like a search engine results page with 10 query-biased document summaries. Clicking on a summary took a participant to a page with the full document. On the full document page, participants could decide to save the document as relevant or simply use the web browser's back button to return to the summaries page. At the bottom of the summaries page, participants could click a link to obtain the next 10 results. The system recorded all clicks and times spent on the pages

as well the decisions to save documents as relevant. The user interface only allowed participants to search the given ranked list of documents. No query reformulation was possible.
The study used 8 topics from the TREC 2005 Robust track, which used the AQUAINT newswire document collection. The topics (310, 336, 362, 367, 383, 426, 427, 436) were selected to be of possible interest to study participants.
48 participants worked on 4 search topics for 10 minutes each. Topics were balanced across participants, list precision, and task order. The study instructed participants to find and save as many relevant documents as possible in the 10 minutes while making as few mistakes as possible. In cases of technical issues affecting data collection, or when it was apparent that participants did not follow instructions, these participants were removed and new participants were recruited until the study was a fully balanced design with 48 participants.
The user study had dual purposes. One purpose was to investigate the effect of precision on user performance, and the other was to provide behavioral data for calibration of our time-biased gain metric. As a result, the ranked lists of documents had a uniform precision of either 0.6 or 0.3, which were representative of the precision at rank 10 performance of the best and lower performing systems in the TREC 2005 Robust track. By uniform, we mean that for every 10 documents shown, either 6 or 3 were relevant. The construction of the result lists placed both relevant and non-relevant documents at ranks reflective of their overall likelihood of being ranked highly by retrieval systems. The result was that we were able to control the precision, and the study participants still had a realistic task of distinguishing between highly ranked relevant and non-relevant documents. Full details of the result lists construction, screenshots of the user interface, and other study details, can be found in [34], which also reports on the results of the precision component of the study.
3.2 Calibration Values
Given the user study, we can estimate the time it takes users to process a document summary and a full document. At first glance, computing these times would appear to be as simple as taking the average of each, for each user, and reporting the user average. Instead, since we are interested in computing gain versus time for a population, we need to compute harmonic means, or as we do in this paper, weighted averages where each weight reflects the amount of a user's activity in the study.
From the user study, we have measurements of the time participants spent on the summaries page before they clicked on a summary or the next link. Likewise, we have measured the time spent viewing documents before leaving the page.
To compute the population average for the time spent on a summary, we need to consider that we can only measure the time spent on the summaries page before clicking on a summary. Without eyetracking we cannot know for certain how much time is spent on each individual summary. Thus, we compute the amount of time per summary for a participant to be the sum of all time spent on the summaries page divided by the maximum rank of a clicked summary. In this way, we spread the time out over both the summaries clicked and not clicked. To compute the population average time on a single summary, we weight the individual partici-

98

Time (seconds) 0 10 20 30 40 50 60

Normal Duplicate

0

500

1000

1500

2000

2500

Document Length (words)

Figure 1: Time to judge document relevance with linear fits to 4212 data points that have been binned and averaged for visualization purposes.

Parameter P(C=1|R=1) P(C=1|R=0) P(S=1|R=1) P(S=1|R=0)
TS TD (l)

Description Probability of click on relevant summary Probability of click on non-rel. summary Prob. of save/judge relevant document as rel. Prob. of save/judge nonrelevant doc. as rel. Time to evaluate a summary (seconds) Time in seconds to judge doc. of l words. Duplicates treated as l = 0.

Value 0.64 0.39 0.77 0.27 4.4 s 0.018 ∑ l + 7.8

Table 1: Calibration values. P(S=1|R=0) is not used by the metric, but is shown for completeness.

pant's time by the maximum rank. As such, the population average time per summary is:

TS =
p

i Sip Mp

Mp = p Mp

1 p Mp p

Sip
i

(5)

where Sip is the i-th time spent by participant p on the summary page and Mp is the maximum click rank by participant p. For our data, TS = 4.4 seconds. If we did not compute a weighted average, the average would be 5.3 seconds.
For the time spent viewing a full document before saving it as relevant or clicking the web browser's back button to return to the summaries page, we take all of the recorded times and fit a linear model to the time in terms of the document's length. By using all of our participants' data, we will obtain a model of the time to judge a document's relevance that is weighted based on the level of activity of that participant.
Figure 1 show a plot of time to judge document relevance vs. document length. We measured document length as the number of word occurrences in a document as parsed by Indri [32]. For the purposes of the plot, we binned the individual data points such that approximately 15 documents were in each bin. Each single document was judged by an average of 9.1 participants, and thus each plotted data point represents the average of many document lengths and times. Averaging the data points in the plot hides the considerable variation in time to judge documents.
In Figure 1, we have separately plotted the times for documents when first viewed by a participant, and the times for a participant's later views of document duplicates. We identified near duplicates in the AQUAINT collection with a variant of Broder's duplicate detection algorithm [5], and only considered documents with the highest possible similarity to be duplicates of each other, i.e. all shared shingles are identical. We refer to these near duplicates as duplicates. We found 10.1% of the documents to be duplicates.
The linear fits to all of the first-viewed documents (3614 data points) and the duplicate views (598 points) are shown in Figure 1. The linear model for first-viewed documents is given by: TD(l) = 0.018 ∑ l + 7.8, and has an adjusted R-squared of 0.12, which means that 12% of the variance is explained by the model. We looked at including the rel-

evance of the documents and/or the precision of the lists in the linear regression, but neither increased the adjusted R-squared.
The linear fit to the duplicates does not explain any of the variance in the time to judge a duplicate, and as such the time to judge a duplicate can be estimated equally well by taking the mean, which gives a result of 6.8 ± 0.5 seconds. The constant in the fit to the first viewed documents is 7.8± 0.7. Since the standard errors of the mean duplicate time and the constant term of the linear fit overlap, there is no significant difference between the two, and we simply treat duplicates as zero length documents.
Lorigo et al. [22] found that on information style tasks, participants spent 40% of their time looking at the search results and the remainder elsewhere (presumably at the full web pages). Our participants spent 33% of their time on the summaries and 67% on the full documents.
Of note, user studies typically report user averages. For example, with our data, the average participant took 26.5 seconds per viewed document. In contrast, for the same set of document viewings, the mean of TD(l) is 18.9 seconds. The difference comes about because the participants who worked faster, took less time per document and also viewed more documents in the same amount of time. If we used the participant averages, we would overestimate the amount of time it takes our population to reach rank k. This difference also means that most statistics reported by user studies cannot be directly used in metrics like ours.
We obtained the parameter settings for the probabilities of clicking on summaries and documents in a straightforward fashion. For both the probability to click on a summary given the relevance, P (C|R), and the probability to save a document as relevant given given relevance, P (S|R), we calculate them as weighted averages. We know the relevance of each document as given by the NIST assessors and can calculate the fraction of views on relevant documents that result in a document being saved, P (S = 1|R = 1), as well as the fraction of views on non-relevant documents that result in a save, P (S = 1|R = 0). For calculating the probability of a click on a summary given the document's relevance, P (C|R), we make the assumption that all summaries up to the maximum clicked rank are viewed. Given this assumption, we can calculate the probabilities of clicking on sum-

99

50

50

50

Average T(k) for all Ranked Lists Average Rank of Participants

40

Average T(k) for P@k = 0.3 Lists Average T(k) for P@k = 0.6 Lists Average Participant Rank for P@k = 0.3 Average Participant Rank for P@k = 0.6

40

Average Participant Rank, Topic 383, P@k = 0.3 Average Participant Rank, Topic 383, P@k = 0.6 T(k) for Topic 383 List with P@k = 0.3 T(k) for Topic 383 List with P@k = 0.6 Custom Topic 383 T(k), P@k = 0.3 Custom Topic 383 T(k), P@k = 0.6

40

30

30

30

Rank

Rank

Rank

20

20

20

10

10

10

0

0

0

0

100

200

300

400

500

600

Time (seconds)

0

100

200

300

400

500

600

Time (seconds)

0

100

200

300

400

500

600

Time (seconds)

Figure 2: These plots show the predicted time to rank k, T (k), vs. the measured time to rank k for the 48 participants in the user study. Please see the text for a detailed description of each plot.

maries in the same fashion as for documents. In the case of computing the weighted average for documents, we weight a participant's P (S|R) by the number of documents viewed during a search task. In the case of summaries, we weight a participant's P (C|R) by the participant's maximum clicked rank. Table 1 shows the computed probabilities.
Yilmaz et al. [42] reported for a commercial web search engine the probabilities of users clicking on documents of varying relevance: Bad: 0.49, Fair: 0.45, Good: 0.55, Excellent: 0.71, and Perfect: 0.94. In comparison, our probability of clicking on a relevant document is 0.64 and falls in-between "Good" and "Excellent", which seems appropriate. Our probability of clicking on a non-relevant document is 0.39 and falls below "Bad", and shows that our summaries on average provided the study participants with an ability to discriminate between relevant and non-relevant documents. Like Yilmaz et al., we find that a significant fraction of lowquality or non-relevant documents are clicked on.
The probabilities for saving a document show that given a full document, participants are better able to discriminate between relevant and non-relevant as compared to summaries, but that they still have a true positive rate, P (S = 1|R = 1) = 0.77 less than one. In other words, not all relevant documents are recognized as such, and this makes sense given a population of users as opposed to modeling the single NIST assessor. Given constraints on time and abilities, not all users will detect when a document is relevant.
4. VALIDATION
In Section 3 we proposed a method to estimate the time it takes a user population to reach rank k, i.e. T (k). Our T (k) consists of Equation 4 and the calibration values for T (k) estimated from a 48 participant user study. Table 1 shows a summary of the calibration values.
Our T (k) is a simple model of user behavior for what is a task of considerable complexity. In processing a ranked list of documents, users must make complex decisions regarding the time they devote to summaries and full documents and the accuracy with which they make decisions. In the user study data that we used to calibrate time-biased gain, there is considerable variation in observed user strategies

for the searching of ranked lists [33]. For example, some users quickly select summaries to click while others take 2-3 times longer to make their selection. Another example is that while the study participants primarily moved forward (down) the ranked lists with 94% of the average participant's clicks being on a summary lower ranked than the previously clicked summary, not all movement is forward.
In this section we discuss the ability of our T (k) to make reasonable estimates of the time it takes the population to reach rank k. In the user study, participants searched for relevant documents in 16 ranked lists of documents. Each of 8 topics had two lists. One of the two lists had a precision of 0.3 and the other a precision of 0.6. The design of the study meant that 12 participants worked on each list. Participants worked for 10 minutes on each list.
Figure 2 shows three plots comparing T (k) and actual participant behavior. The far left plot averages T (k) for all 16 of the ranked lists and 192 traces of participant behavior. On average, T (k) produces a linear rate of progress down a ranked list. Given how we produced the calibration values, this average behavior for the 16 ranked lists is expected. As we will show briefly, T (k) produces non-linear predictions for individual ranked lists. At 600 seconds, the average T (k) matches the average rank of the participants and shows that our calibration on average works. If we had not used weighted averages to produce the calibration values, T (k) would have underestimated the rank reached at 600 seconds.
We also see in the far left plot of Figure 2 that the study participants increased their rate as they progressed down the ranked lists. This speedup makes sense given that participants will take some time to learn a model of what is and is not relevant. We also noticed that participants working down some of the 0.3 precision lists dramatically increased their rate of work once getting past the first page of 10 results. Our T (k) is an average of the slower initial behavior and the faster behavior found later in the search process. We leave for future work the possibility of having the parameters of T (k) vary with time.
IR user interfaces employ document summaries as a mechanism to allow users to find relevant material faster. The better the document summaries are, the easier it is for users

100

Rank G(t)

500 Maximum Average Minimum
400

300

200

100

0

0

400

800

1200

1600

2000

2400

2800

3200

3600

t (seconds)

Figure 3: Maximum, average, and minimum rank at time t over all queries and runs from the TREC 2005 Robust Track.

to save time by skipping non-relevant results. In the center plot of Figure 2, we see that the study participants processing the 0.3 precision list do process more of the lists compared to the 0.6 precision lists. T (k) only models precision in the form of different summary click probabilities given the relevance of the document, but it too predicts that the population will reach a larger rank k with a lower precision list as compared to a higher precision list in the same amount of time. At 600 seconds, the predictions of T (k) fall between the actual ranks reached by the participants.
As mentioned in Section 3, our study design precluded us from using the search topic as part of our model even though it is well-known that topics vary considerably. In the far right plot of Figure 2, we show the participants' behavior for the 0.3 and 0.6 precision lists of Topic 383 (mental illness drugs). Topic 383 requires that a relevant document name an actual drug. The documents in the result list tend to be long documents. As the far right plot shows, the participants work down the ranked list at a much slower rate than on average (far left plot). In addition, the participants show little difference in rank reached for the 0.3 and 0.6 lists. Also shown in the plot are the predictions of T (k) for these two ranked lists. As can be seen here, T (k) is not simply a linear function of rank. The different documents with their differing ranks and relevance judgments produce unique T (k) values for each ranked list.
While our T (k) correctly estimates a slower rate for Topic 383, it does overestimate the rate of progress, and it also predicts a gap between the ranks reached for the 0.3 and 0.6 lists. To illustrate the degree to which modeling of the topic could help future versions of T (k), we calibrated T (k) using only Topic 383. In the far right plot, we label this version of T (k) the "Custom Topic 383 T(k)." As shown, the custom calibrated T (k) much better reflects the actual participant behavior on Topic 383.
Given the importance of modeling the topic, a possibility for producing per-topic, custom calibrated versions of T (k) would be to assign multiple assessors to a topic during test collection construction. The behavior of the assessors could be recorded and packaged alongside the relevance judgments, and then each topic could have its own T (k).
To what extent does T (k) vary on actual runs? Figure 3 shows the maximum, average, and minimum rank at time

40

indri05RdmmT (MAP 0.332)

ASUDE (MAP 0.289)

indri05RdmeD (MAP 0.282)

35

uic0501 (MAP 0.310)

30

25

20

15

10

5

0

0

400

800

1200

1600

2000

2400

2800

3200

3600

t (seconds)

Figure 4: G(t), cumulative gain at time t, for top scoring runs from the TREC 2005 Robust Track.

t for all of the submitted runs to the TREC 2005 Robust Track. As shown, there is a considerable range of ranks at a given time. In terms of Equation 3, this result means that at a given rank k, different ranked lists will vary in their predicted time to reach that rank and will have different amounts of decay, D(t), applied at that rank.

5. TIME-BIASED GAIN
In this section, we pull together the pieces of the timebiased gain measure defined in Equation 3, suggesting specific formulae for the remaining components, including gain, decay and normalization.

5.1 Gain
For the remainder of the paper, we define

gk =

P (C = 1|R = 1)P (S = 1|R = 1) if ri = 1

0

if ri = 0

Under this definition, if the document at rank k is relevant, the gain is equivalent to the probability of viewing it and saving it. If the document is not relevant, the gain is zero. This definition is consistent with the binary relevance values typical of TREC tasks and adopted for our calibration process. In principle, we could adopt the graded gain values employed by nDCG, the attenuated gain values employed by ERR, or even gain values that take document length and other features into account. We leave the exploration of these ideas for future work.
Ignoring decay for now, we combine gain values with T (k), our formula for estimated rank at time t, allowing us to compute G(t) for individual runs. Figure 4 shows G(t) for four runs taken from the TREC 2005 Robust Track, along with their official MAP values [39]. All four runs performed well, with indri05RdmmT and uic0501 being the best and secondbest title-only runs, and ASUDE and indri05RdmeD being the best and second-best description-only runs. For three of these runs, their relative cumulative gain remains consistent over the full time period. The story for uic0501 is different. It leads the other runs for the first twenty minutes, but begins to fall behind after that, providing some insight into its behavior. While similar insight might be gained from a standard recall-precision plot, calibrating gain against time may provide a better sense of the performance experienced by a user.

101

D(t) 0.0 0.2 0.4 0.6 0.8 1.0

Users Exponential decay (half-life = 224 seconds)

0

500

1000

1500

t (seconds)

Figure 5: User data and fitted decay curve for D(t).

5.2 Decay
Our calibration process provides no guidance regarding the form of the decay function D(t), but standard exponential decay provides one possibility:

D(t)

=

e-t

ln 2 h

,

(6)

where h is the "half-life" of users, i.e., the time at which half of the initial users have stopped scanning the result list.
To support this choice of decay function, we turn to an interaction log taken from a commercial search engine, in this case from the MSN search engine. This anonymized log was made available to selected researchers across the information retrieval community during 2006 and 2007. For example, Zhang et al. [43] employed it to validate the discount function appearing in RBP. The log contains user interaction data for 5 million searches during May 2006.
We treat the time between the query and its last click as a proxy for the time spent scanning the result list and viewing documents, allowing us to estimate D(t). Since our scenario assumes a careful and determined user, we try to filter out users with navigational needs, etc., by eliminating searches with less than five clicks. We also filter out searches that take longer than 30 minutes.
Figure 5 plots the D(t) estimated from this log. As shown in the figure, exponential decay with a half-life of 224 seconds provides a good fit to this data. We use this half-life in the remainder of the paper.
We note that it should be possible to employ log data to estimate T (k), as well as D(t), although we would need document length and other data not present in this log. This approach would be particularly appropriate for calibration in the context of web search. We leave this idea for future work.

5.3 Interpretation
Before we move on to the normalization factor, we pause in our development of time-biased gain to consider one way in which the measure may be interpreted. Recall that gk represents the expected gain at rank k and T (k) represents the expected time to reach rank k. Let Gk be a random variable indicating the actual gain experienced by a particular user from the document at rank k, and let Tk be a random variable indicating the actual time taken by a user to reach

rank k. Thus, we have





gkD(T (k)) =

E [Gk ]D(E [Tk ])

k=1

k=1





E [Gk ]E [D(Tk )]

k=1



=

E [Gk D(Tk )]

k=1



=E

GkD(Tk) .

k=1

(7) (8) (9) (10)

Equation 8 holds by Jensen's inequality, since exponential decay is convex. Equation 9 holds since Gk and Tk are independent. Equation 10 is the number of documents a user is expected to save as relevant. Hence, if we ignore normalization by setting N = 1, we may interpret the measure as a lower bound on this value.

5.4 Normalization

Evaluation measures are typically normalized so that their values fall into the range [0:1]. Depending on the measure, this normalization may be required for the measure to be reasonably averaged over multiple queries, at least if an arithmetic mean is used. For example, the raw summation appearing in MAP is normalized by dividing it by the number of known relevant documents, so that it treats "all queries equally" [25].
The nDCG measure bases its normalization on the set of known relevant documents. It computes a maximum value for the summation in Equation 1 over a ranked list of these documents, which becomes the value for N . If we know all relevant documents in the collection, we may follow a similar approach for time-biased gain, ranking these relevant documents in the order that maximizes Equation 2. Under this approach, the value for N varies from topic to topic, according to the set of relevant documents known for that topic. Unfortunately, as it does for nDCG, this approach suffers from the need to know all relevant documents, and the value of the measure will change if new relevant documents surface.
RBP bases its normalization on an idealized collection containing an unlimited number of relevant documents. We may follow a similar approach for time-biased gain by assuming an unlimited number of zero-length relevant documents. If we set Tx = TS + TD(0)P (C = 1|R = 1), then over this idealized collection we have T (k) = Tx ∑ (k - 1), so that



N=

gkD(T (k))

(11)

k=1



= P (C = 1|R = 1)P (S = 1|R = 1)

e-Tx

k

ln 2 h

k=0

P (C = 1|R = 1)P (S = 1|R = 1)

=

1

-

e-Tx

ln 2 h

Under this approach, N is now constant across all queries. For our calibration values, N  17.1.
Given our interpretation of time-biased gain as a lower bound on the number of documents a user is expected to save as relevant, normalization may not be necessary, and in this paper, we set N = 1. The creators of ERR, who

102

0.35 Kendall's  = 0.803
0.3

0.25

Mean Average Precision

0.2

0.15

0.1

0.05

0

0

1

2

3

4

5

6

Mean Time Biased Gain

Figure 6: Time-biased gain compared against MAP over runs from the TREC 2005 Robust Task.

emphasize a similar interpretation, do not normalize that measure. In addition, normalization is not required if we average over topics using a geometric mean [25, 39]. We leave further exploration of normalization for future work.
6. COMPARISON
In this section, we provide a brief comparison between time-biased gain and other effectiveness measures used at TREC and elsewhere. Apart from time-biased gain, these measures are computed by the standard TREC evaluation program (trec_eval). All measures are computed over the runs submitted to the TREC 2005 Robust Track.
Figure 6 presents a scatter plot of MAP vs. time-biased gain. The measures are correlated, with a Kendall's  of 0.794. Due to exponential decay, time-biased gain tends to emphasize early gain, a point illustrated by the run uic0501. Reflecting the behavior seen in Figure 4, it achieves the second-best performance, a time-biased gain value of 5.39 vs. a MAP value of 0.310, outperforming runs with higher MAP values.
With the best time-biased gain value of 5.43, and a MAP value of only 0.262, the run sab05ror1 forms an interesting case study [6]. This run was trained on TREC vol. 4 & 5 qrels and documents and had very mixed performance across topics as measured by average precision. While the attempt appeared to fail when measured by MAP, the run's status as an obvious outlier suggests that the attempt may have succeeded after all (or at least warrants further investigation).
Sakai [29] proposes discriminative power as one method for assessing the behavior of effectiveness measures. While high discriminative power does not necessarily indicate a good measure, low discriminative power would raise questions about the measure. To calculate discriminative power, we compute a significance test between each pair of runs submitted to an experimental task, such as the TREC Robust Track. Discriminative power is the number of pairs that are significant at some fixed level. Table 2 compares the discriminative power of time-biased gain against several standard measures. MAP is known to have high discriminative power, due to the depth at which relevant document can influence its value. However, time-biased gain provides good

Measure MAP precision@10 nDCG@10 nDCG@20 time-biased gain

t-test 66.0% 51.6% 49.5% 54.5% 57.5%

Randomization 66.4% 51.4% 49.6% 54.6% 57.5%

Bootstrap 67.2% 52.7% 51.0% 55.6% 58.8%

Table 2: Discriminative power of measures according a two-tailed paired t-test, a randomization test, and a bootstrap test (significance level 0.05).

discriminative power when compared to simple precision@10 and nDCG@K, for typical values of K.
7. CONCLUDING DISCUSSION
Equation 2 lies at the heart of our work. The remainder of the paper represents only one possible route for developing this equation. In following this route, we have adhered as closely as possible to the typical assumptions underlying a TREC task. We adopt the scenario of a careful and determined user, with a topical information need, intending to identify as much relevant material as they can in the time they have available. In another context, such as web search, another scenario will be more appropriate, requiring different calibration of the measure.
In following this route, we have also adhered as closely as possible to the typical assumptions underlying the creation of evaluation measures. Similar to the generic framework of Equation 1, time-biased gain is structured as a normalized sum over discounted gain values. Our primary innovation is to discount by time instead of rank.
However, a better numeric approximation to Equation 2 might be achieved through simulation. The user model described in Section 3 could easily be adapted for this purpose. By simulating thousands of users interacting with a result list, we may approximate both expected gain and variance. We leave this idea for future work.
Time-based calibration allows us to accommodate aspects of the search process that are ignored by traditional effectiveness measures. For example, in TREC collections longer documents are more likely to be relevant than shorter documents [31]. During the early years of TREC, considerable effort was made to adjust ranking functions according to document length [28, 31]. In light of the greater time required to judge longer documents, it may be that adjustment of the evaluation measures was also required.
We recognize that our approach to duplicates rewards behavior that might be viewed as undesirable by many users. Since we are following typical TREC guidelines for relevance, our calibration of time-biased gain rewards a system for returning duplicate documents, since they are faster to assess. In other contexts, it is likely that duplicates should provide no gain, thus penalizing runs through the extra time required to assess them. We considered following this approach in this paper, but decided to stay as true as possible to the TREC guidelines.
In this paper we focused on the population level behavior of determined, informational searchers. In future work, it would be good to study different classes of user behavior, and produce for each class a calibrated time-biased gain.

103

8. ACKNOWLEDGMENTS
We thank all of the anonymous reviewers for their comments. In particular, we thank Reviewer 3 for providing several pages of thoughtful feedback. Like Reviewer 3, we recognize the limitations of this paper's version of time-biased gain, and we plan to address these limitations in future work. Thanks to Ben Carterette and Falk Scholer for their helpful feedback on an early version of this paper. David Hu wrote the software to compute the sets of near-duplicate documents.
This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC), in part by GRAND NCE, in part by Google, in part by an Amazon Web Services in Education Research Grant, in part by the facilities of SHARCNET, and in part by the University of Waterloo.
9. REFERENCES
[1] I. J. Aalbersberg. Incremental relevance feedback. In SIGIR, pp. 11≠22. 1992.
[2] P. Arvola, J. Kek®al®ainen, and M. Junkkari. Expected reading effort in focused retrieval evaluation. Information Retrieval, 13:460≠484, 2010.
[3] L. Azzopardi. The economics in interactive information retrieval. In SIGIR, pp. 15≠24, 2011.
[4] L. Azzopardi, K. J®arvelin, J. Kamps, and M. D. Smucker. Report on the SIGIR 2010 workshop on the simulation of interaction. SIGIR Forum, 44:35≠47, 2011.
[5] A. Broder. On the resemblance and containment of documents. In Compression and Complexity of Sequences, pp. 21≠29, 1997.
[6] C. Buckley. Looking at limits and tradeoffs: Sabir Research at TREC 2005. In TREC. 2005.
[7] B. Carterette. System effectiveness, user models, and user utility: A conceptual framework for investigation. In SIGIR, pp. 903≠912, 2011.
[8] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In CIKM, pp. 611≠620, 2011.
[9] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In CIKM, pp. 621≠630, 2009.
[10] C. L. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A comparative analysis of cascade measures for novelty and diversity. In WSDM, pp. 75≠84, 2011.
[11] W. S. Cooper. On selecting a measure of retrieval effectiveness. JASIS, 24(2):87≠100, Mar/Apr 1973.
[12] W. S. Cooper. On selecting a measure of retrieval effectiveness: Part II. implementation of the philosophy. JASIS, 24(6):413≠424, Nov/Dec 1973.
[13] M. D. Dunlop. Time, relevance and interaction modelling for information retrieval. In SIGIR, pp. 206≠213. 1997.
[14] G. Dupret. Discounted cumulative gain and user decision models. In SPIRE, pp. 2≠13. 2011.
[15] G. Dupret and B. Piwowarski. A user behavior model for average precision and its generalization to graded judgments. In SIGIR, pp. 531≠538, 2010.
[16] N. Fuhr. A probability ranking principle for interactive information retrieval. Information Retrieval, 11:251≠265, 2008.
[17] K. J®arvelin and J. Kek®al®ainen. Cumulated gain-based evaluation of IR techniques. ACM TOIS, 20(4):422≠446, 2002.
[18] D. Kelly. Methods for Evaluating Interactive Information Retrieval Systems with Users, now Publishers, 2009.

[19] H. Keskustalo, K. J®arvelin, T. Sharma, and M. L. Nielsen. Test collection-based IR evaluation needs extension toward sessions: A case of extremely short queries. In AIRS, pp. 63≠74, 2009.
[20] R. Khan, D. Mease, and R. Patel. The impact of result abstracts on task completion time. In Workshop on Web Search Result Summarization and Presentation, WWW'09.
[21] J. Lin and M. D. Smucker. How do users find things with PubMed? Towards automatic utility evaluation with user simulations. In SIGIR, pp. 19≠26. 2008.
[22] L. Lorigo, M. Haridasan, H. Brynjarsdo¥ttir, L. Xia, T. Joachims, G. Gay, L. Granka, F. Pellacini, and B. Pan. Eye tracking and online search: Lessons learned and challenges ahead. JASIS, 59(7):1041≠1052, 2008.
[23] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM TOIS, 27(1):1≠27, 2008.
[24] M. A. Najork, H. Zaragoza, and M. J. Taylor. HITS on the Web: How does it compare? In SIGIR, pp. 471≠478, 2007.
[25] S. Robertson. On GMAP: and other transformations. In CIKM, pp. 78≠83, 2006.
[26] S. Robertson. A new interpretation of average precision. In SIGIR, pp. 689≠690, 2008.
[27] S. E. Robertson, E. Kanoulas, and E. Yilmaz. Extending average precision to graded relevance judgments. In SIGIR, pp. 603≠610, 2010.
[28] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. In SIGIR, pp. 232≠241, 1994.
[29] T. Sakai. Evaluating evaluation metrics based on the bootstrap. In SIGIR, pp. 525≠532, 2006.
[30] T. Sakai and S. Robertson. Modelling a user population for designing information retrieval metrics. In EVIA, pp. 30≠41, 2008.
[31] A. Singhal, G. Salton, M. Mitra, and C. Buckley. Document length normalization. IPM, 32(5):619≠633, 1996.
[32] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indri: A language-model based search engine for complex queries (extended version). Tech Report IR-407, CIIR, University of Massachusetts Amherst, 2005.
[33] M. D. Smucker. An analysis of user strategies for examining and processing ranked lists of documents. In HCIR, 2011.
[34] M. D. Smucker and C. Jethani. Human performance and retrieval precision revisited. In SIGIR, pp. 595≠602, 2010.
[35] L. T. Su. Evaluation measures for interactive information retrieval. IPM, 28:503≠516, March 1992.
[36] A. Tombros and M. Sanderson. Advantages of query biased summaries in information retrieval. In SIGIR, pp. 2≠10, 1998.
[37] A. Turpin, F. Scholer, K. J®arvelin, M. Wu, and J. S. Culpepper. Including summaries in system evaluation. In SIGIR, pp. 508≠515. 2009.
[38] A. H. Turpin and W. Hersh. Why batch and user evaluations do not give the same results. In SIGIR, pp. 225≠231, 2001.
[39] E. M. Voorhees. Overview of the TREC 2005 Robust Retrieval Track. In TREC, 2005.
[40] R. W. White, I. Ruthven, J. M. Jose, and C. J. van Rijsbergen. Evaluating implicit feedback models using searcher simulations. ACM TOIS, 23(3):325≠361, 2005.
[41] Y. Yang and A. Lad. Modeling expected utility of multi-session information distillation. In ICTIR, pp. 164≠175, 2009.
[42] E. Yilmaz, M. Shokouhi, N. Craswell, and S. Robertson. Expected browsing utility for web search evaluation. In CIKM, pp. 1561≠1564, 2010.
[43] Y. Zhang, L. A. Park, and A. Moffat. Click-based evidence for decaying weight distributions in search effectiveness metrics. Information Retrieval, 13:46≠69, 2010.

104


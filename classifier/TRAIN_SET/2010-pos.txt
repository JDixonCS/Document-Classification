The Importance of Anchor Text for Ad Hoc Search Revisited

Marijn Koolen1 Jaap Kamps1,2
1 Archives and Information Studies, University of Amsterdam, The Netherlands 2 ISLA, Informatics Institute, University of Amsterdam, The Netherlands
{m.h.a.koolen,kamps}@uva.nl

ABSTRACT
It is generally believed that propagated anchor text is very important for effective Web search as offered by the commercial search engines. "Google Bombs" are a notable illustration of this. However, many years of TREC Web retrieval research failed to establish the effectiveness of link evidence for ad hoc retrieval on Web collections. The ultimate resolution to this dilemma was that typical Web search is very different from the traditional ad hoc methodology. So far, however, no one has established why link information, like incoming link degree or anchor text, does not help ad hoc retrieval effectiveness. Several possible explanations were given, including the collections being too small for anchors to be effective, and the density of the link graph being too low.
The new TREC 2009 Web Track collection is substantially larger than previous collections and has a dense link graph. Our main finding is that propagated anchor text outperforms full-text retrieval in terms of early precision, and in combination with it, gives an improvement in overall precision. We then analyse the impact of link density and collection size by down-sampling the number of links and the number of pages respectively.
Other findings are that, contrary to expectations, (inter-server) link density has little impact on effectiveness, while the size of the collection has a substantial impact on the quantity, quality and effectiveness of anchor text. We also compare the diversity of the search results of anchor text and full-text approaches, which show that anchor text performs significantly better than full-text search and confirm our findings for the ad hoc search task.
Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--performance evaluation (efficiency and effectiveness)
General Terms: Experimentation, Measurement, Performance
Keywords: Ad hoc, Anchor text, Collection size, Link density
1. INTRODUCTION
The use of anchor text for Web retrieval is well studied, with the broad conclusion that it is very effective for finding entry pages of sites­often outperforming approaches based on document text alone­but not for ad hoc search. Based on claims from commer-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

cial search engine companies, over the course of several years of Web search experiments at TREC [34], organisers and participants have tried to establish the effectiveness of link information, including anchor text, for retrieval. Despite the enthusiasm and effort of many participating groups, in the first two years, 1999­2000, participants failed to show any improvements due to link information [17]. After the first year, the main reason was deemed to be the low number of inter-server links [19]. For the second year, an artificially crafted collection with more inter-server links (WT10g, [4]) was used, and participants combined content information with link evidence such as PageRank, HITS and anchor text, but again without success. This time, the main difference between the results at TREC and the general belief that link information is valuable, was considered to be the difference in search tasks [15, 33]. Typical Web search behaviour is very different from the user model assumed for the traditional ad hoc methodology. Web searchers tend to "prefer the entry page of a well-known topical site to an isolated piece of text, no matter how relevant" [17]. According to Hawking and Craswell [17], p.215:
Hyperlink and other web evidence is highly valuable for some types of search task, but not for others.
Although the switch to more Web-centric search tasks like home page and named page finding showed link information to be very effective for these tasks [9, 24, 26, 29], there is no clear explanation of why anchor text is not effective for ad hoc retrieval. Anchor text provides short summaries often by different authors about the topic of a page. This could potentially improve the document representation of web pages that have incoming links, and thereby the precision of search results. On the Web, which is infinitely large, early precision is the important criterion. Recall is almost impossible to measure, but also not important for most users.
Gurrin and Smeaton [14] pointed out that the inter-server link density of the WT10g collection was still very low, and extracted a subset of the collection, WT-dense, which has a much higher interserver link density. Within this tiny subset they found that a combination of content and link information could improve precision on the ad hoc topics of the TREC-9 Web track. This led them to come up with a list of requirements a representative test-collection must satisfy to study the value of link information. A good Web collection needs to be sufficiently large and have sufficiently high inter- and intra-server link densities.
The size issue was addressed in the Terabyte Tracks of 2004­ 2006, which used the GOV2 collection, based on a crawl of the .gov domain in 2004, consisting of 25 million documents.1 Again, anchor text was found to be highly effective for Web-centric tasks,
1The crawl on the .gov domain was exhausted before reaching the targeted 100 million pages, and plans to rectify this by crawling additional pages from the .edu domain were never realised.

122

but not for ad hoc search [22, 23]. However, the .gov domain is very different in nature from the .com domain used for the crawl on which the WT10g collection is based, and the .GOV2 collection has fewer incoming links per page. Thus, although it is larger than the earlier Web Track collections, its link density is much lower, making it hard to investigate the impact of collection size.
At the TREC 2009 Web Track [6] a new, large Web collection-- ClueWeb09 [7]--was introduced and the traditional Ad hoc Task was paired with the new Diversity task. This new collection is much larger than the collections used at TREC 8 and 9, and was crawled to reflect Tier 1 of a commercial search engine, so should have a relatively dense link structure, allowing us to study both aspects of collection size and link density. If a large number of documents and a high link density are indeed requirements for anchor text to be effective, this new collection might finally reveal us its potential. This urges us to revisit the question:
· What is the importance of anchor text for ad hoc search?
Surely, the issue of having enough (inter-server) links for anchor text is critical for its success on any search task, but perhaps the link density needs to be higher for anchor text to be effective for ad hoc retrieval than for entry page finding. Intuitively, the number of links in the collection plays a direct role in the quantity of anchor text and might therefore affect its quality as well. Links within the same site are often navigational links, with anchor terms such as `click', `here' and `next' [11]. Therefore, it is generally assumed that links between sites are more meaningful, including their anchor text [27].
The other factor mentioned by Gurrin and Smeaton [14] is the size of the collection. We know that precision increases with collection size [18], which holds for document text indexes, but should hold for anchor text indexes as well. With larger collections, the number of documents that have incoming links increases and as a consequence, so does the number of those documents relevant to a given topic. The new ClueWeb collection used at the TREC 2009 Web Track is much larger than the collections used at TREC 8 and 9 and should have many incoming links per page, resulting in more anchor text and thereby possibly better document representations.
The direct relation between anchor text and link in-degree, and thereby the relation between anchor text and the popularity or importance of a page, helps locate home pages and popular pages described by the search terms. This offers an interesting perspective on the task of finding diverse search results. If anchor text is effective for ad hoc search as well, it has the potential to find good results for both informational and navigational information needs.
These considerations lead us to break down our main research question into several, more specific research questions:
· Is anchor text effective for improving ad hoc retrieval?
· What is the impact of link density on the effectiveness of anchor text? And what is the relative importance of interand intra-server links?
· What is the impact of collection size on the effectiveness of anchor text?
· What is the importance of anchor text for the diversity of ad hoc search results? And what is the impact of anchor text on informational and navigational information needs?
The rest of the paper is organised as follows. We will first discuss related work in Section 2. In Section 3 we describe our initial experiments to compare the effectiveness of anchor text and full-text search. Then, in Section 4 we analyse how link density and collection size affect the quantity and quality of anchor text, followed by Section 5 where we investigate the diversity of anchor text search results. We draw conclusions in Section 6.

2. RELATED WORK
The importance of anchor text has been studied extensively at TREC. At TREC 8, participants could not show consistent improvements over content-only baselines using link information [19]. This unexpected result led many to believe that the collection had too few inter-server links for link evidence to be effective, in response to which a new collection was constructed focusing on interserver link density [4]. At the TREC 9 Web Track, the first reported attempts at exploiting anchor text for ad hoc retrieval did not show any improvements either [15]. Singhal and Kaszkiel [32] raised doubts about the TREC evaluation methodology used to model Web search, as they found different results for anchor text when comparing TREC results against their in-house tests.
Several studies [8, 33] pointed at the differences between traditional ad hoc search (as evaluated at TREC) and Web search behaviour. As new, more realistic Web tasks were introduced [16], the value of link information was finally shown [9, 26] and anchor text was found to be be very effective for site- and home page finding tasks. Craswell et al. [10] recently showed the effectiveness of anchor text for diversity. Eiron and McCurley [11] showed that anchor text behaves much like user queries. If Web authors use the same labels to describe pages as Web searchers use to find pages, anchor text can potentially bridge the gap between queries and pages and lead to high precision, if the anchors and pages in the collection are of high quality.
The quality is another important difference between the new ClueWeb collection and previous TREC Web collections, which is related to the way it is constructed, and which directly affects the density of (inter-server) links. Several studies have looked at the impact of crawling policy on the quality [3] and search effectiveness [12, 13] of the crawled collection. Page importance metrics can be used to schedule the most important or useful pages to be crawled first. Since page importance is usually derived using linkbased measures such as PageRank [30] or On-line Page Importance Computation [OPIC, 1], which give a higher score to a page if it has more incoming links, the first part of a crawl based on such policies tends to have a high link density. One of the primary goals of creating the ClueWeb data set was "to approximate Tier 1 of a web search engine index" [5]. The category B data set, which we use here, consists of the first 50 million English pages of this crawl.
3. INITIAL EXPERIMENTS
We will first describe our initial experiment with plain full-text and anchor text approaches to see if the relative effectiveness of anchor text merits further investigation.
3.1 Data, Index and Runs
We use the ClueWeb09 category B, which contains a sample of 50 million English pages of the larger ClueWeb09 crawl [7]. As the pages were crawled based on a large set of seed URLs with high PageRank and at later stages were crawled in order of OPIC value, we assume this data set contains many of the most important Web pages and a relatively dense Web graph.
We used Indri [21] for indexing. Stopwords are removed and all other terms are stemmed with the Krovetz stemmer. We created two indexes, a full-text index containing only the document text and an anchor text index containing only the propagated anchor text.
With anchor text we mean the underlined text to which a hyperlink is anchored. We only use the string of text appearing in the anchor text. To extract anchor text from the ClueWeb09 category B collection, we used the harvestlinks method, which comes with Indri. Because harvestlinks does not compress its data during processing, which would use up more disk space than we have avail-

123

able, we only used the harvesting option to extract the anchor text

with source url, document ID and target url per bundle of pages,

compressed the output, then mapped the target url to a document

ID ourselves. We extracted over 1.5 billion links pointing to pages

within the collection, with anchor text for more than 75% of all the

pages. Quite a large number of pages have multiple links to the

same target URL (repeated links). If we collapse those repeated

links we end up with 1.18 billion links between just over 50 mil-

lion pages, which leads to a mean in-degree of 23.30. The median

in-degree is 2. For anchor text, repeated links mean more (and po-

tentially different) descriptions from the same source page.

The full-text and anchor text runs use the Indri language model

approach and linear smoothing with collection = 0.15. For ad hoc

search, the length of a document is related with the probability of

relevance. Documents are scored using the document length as a

prior probability p(d)

=

|d| |D|

,

where

d

is

a

document in collection

D. The length prior on the anchor text is determined by the total

length of the all the anchor text of a particular page. This means

that pages with many incoming links have long document repre-

sentations, while pages with a single incoming link have a very

short document representation. The length prior boosts pages with

many links, i.e. pages with high in-degrees. We report on the ef-

fectiveness of the length prior in [25]. We also made a mixture

run, combining the full-text and anchor runs using the weighting

Scoremix(d) = 0.7 · Scorefull(d) + 0.3 · Scoreanchor(d), where

the scores are normalised by the sum of the top 1000 scores before

addition. If a document d is retrieved by only one index, it receives

a zero probability score for the other index. The mixture run was

submitted as an official run at the TREC 2009 Web Adhoc task and

contributed to the pool. The other two runs can be considered as

baseline runs. No tuning was done after the relevance judgements

were made available. Furthermore, we have created two runs us-

ing the incoming link degree of the pages returned by the Text run.

We chose to use in-degree as it was to found to be as effective as

PageRank [2, 28] but easier to compute. For the In-degree run, the

full-text results are ranked only by in-degree. For the Text · In-

degree run we use the in-degree as another document prior in the

language model of Indri.

3.2 Results
The results are shown in Table 1. To put this into perspective, we compared them against the results of the best official submissions of other participants. Both MTC and statAP were used to construct the judgement pools. We use statAP [35], as it is more robust when evaluating runs that did not contribute to the pool. We test for significant changes with respect to the full-text baseline using a one-tailed bootstrap test with 100,000 resamples.
Is anchor text effective for ad hoc retrieval? The Anchor run has a low statMAP compared to the Text run. A possible explanation is that many pages in the collection have no or few incoming links, including many relevant pages. With no or only a few words as document representation, these pages are hard to find using anchor text only, which has consequences for average precision. In contrast, anchor text is very effective for early precision. The Anchor run scores much better on MPC(30) than the Text run and supports the above explanation for its low statMAP score. The anchor text run ranks the relevant pages in its index highly, but seems to miss many relevant pages. We will further investigate this issue below. More importantly, the Mix run leads to significant improvements in statMAP showing that the two indexes are complementary and that Web structure can be used to improve ad hoc search.
The in-degree priors only hurt the underlying Text run. This is not surprising given that in-degree is blind to the topic of a query.

Table 1: Results for the 2009 Adhoc Task. Significance tests are with respect to the full text run, confidence levels are 0.95 (), 0.99 (·) and 0.999 (·)

Full collection

No Wikipedia

Run

statMAP MPC(30) statMAP MPC(30)

Text

0.1442 0.3079 0.1038 0.2557

Anchor Mix

0.0567 0.5558 0.1643 0.4812

0.0617 0.4289 0.1213 0.4773

In-degree

0.0823 0.1876 0.0592 0.1258

Text · In-degree 0.1098 0.2694 0.0746 0.2059

UDWAxQEWeb 0.1999 0.5010

­

­

uogTrdphCEwP 0.2072 0.4966

­

­

ICTNETADRun4 0.1746 0.4368

­

­

Within a much larger collection containing spam pages and other pages with low PageRank, degree-based link ranking algorithms have an important function of separating the high quality pages from the rest. Within this particular collection, which is crawled to reflect Tier 1, most pages are of high quality, so the work of finding important and reliable pages is already done. Further use of in-degrees only disrupts the subtle relevance ranking of text-based retrieval models. Anchor text gives more precise results because it focuses on the subset of links that have query terms in the anchors, and is thus more sensitive to the topical context than in-degree.
The best runs of the top 3 groups of the TREC 2009 Web Ad hoc task, according to MPC(30), score substantially better on statMAP, but lower on MPC(30). This shows that anchor text alone can meet or exceed the precision of the top-performing systems.
Over the top 1000, the overlap between the Anchor and Text runs is 4.6%, showing that anchor text really leads to very different document representations and targets very different pages. The overlap between the Ad hoc judgements and the Anchor run is also very small. In the top 5 results of the Anchor run, over 30% of the pages are unjudged, while at rank 16, more than half of the results are unjudged. Indeed, it seems that the Anchor run is also very different from all runs that contributed to the assessment pool. In contrast, the overlap between the Ad hoc judgements and the Text run is much higher. At rank 5, on average less than 11% is unjudged, while at rank 16 just over 22% is unjudged. These percentages are probably much closer to the actual sampling rates. Thus, the improvement of the Mix run over the Anchor run might simply be caused by the larger number of judged results in that run.
Perhaps anchor text is more effective than in previous TREC experiments because this collection contains the full Wikipedia, which has a dense link structure and many anchors matching the titles of the target pages. Wikipedia pages are edited by many contributors, so the quality might be higher than that of many Web pages. The relevance judgements reveal that more than 21% of the relevant pages in ClueWeb B are Wikipedia pages, while the whole Wikipedia forms only 12% of all the pages in the collection. We built separate full- and anchor text indexes of all non-Wikipedia pages. If the presence of Wikipedia is the main reason for the effectiveness of anchor text, we would expect the non-Wikipedia Anchor run to perform worse than the non-Wikipedia Text run. Columns 4 and 5 in Table 1 show the results of these runs. Although scores are lower than over the full collection indexes--perhaps partly explaining why ClueWeb B runs tend to outperform ClueWeb A runs [6]--the Anchor run still has higher early precision and the Mix run still has higher statMAP than the Text run. Wikipedia is not the sole reason for the effectiveness of anchor text.
Is anchor text effective for improving ad hoc retrieval? On a large collection of high quality pages, anchor text gives good precision

124

and in combination with full-text leads to significant improvements in overall precision. This new Web collection finally shows the long expected value of Web link structure for ad hoc search. Gurrin and Smeaton [14] suggested that the benefits of link information for retrieval would become clear with a sufficiently large collection and a high inter-server link density. Our results support their statement and urge us to address these issues.
4. WHY ANCHOR TEXT WORKS
In this section we seek to understand what makes the anchor text representation effective. We look at the impact of link density and collection size, which we do by down-sampling either links or pages. If we down-sample the pages, we can investigate the impact of collection size on the effectiveness of anchor text. If, on the other hand, we keep the number of pages the same, but instead down-sample the links, we can see the impact of link density.
If we randomly sample 50% of the pages and remove the outgoing links of those pages, we would expect to end up with roughly 50% of all the links. If we remove the pages themselves from the collection, we lose both the outgoing and incoming links of those pages. Thus, if we sample 50% of the pages, we remove more than 50% of the links. Previous TREC Web collections were smaller which could explain why anchor text was not effective earlier. The number of links does not grow linearly with collection size. How does page sampling affect the link density of the collection?
Randomly sampling pages is different from using earlier stages of the crawl as a smaller collection. The first 25 million pages of the crawl have a different composition from randomly sampling 25 million pages [13]. With random sampling, the most important pages will be affected in the same way as the rest of the pages. If the crawl is stopped at half the number of pages, the collection will still have most of the important pages, as modern crawling strategies focus on crawling the most important pages first. However, since ClueWeb B is assumed to be a subset of Tier 1 of a Web search index, based on a large number of seed URLs, we expect that the composition of any sample of the ClueWeb B collection approximates the composition of the full collection. One of the favourable aspects of randomly sampling pages is that the probability of relevance is unaffected [18].
Down-sampling either pages or links means the anchor text representation of a page changes. For each sample, we have to filter the link graph and anchors, and build a separate anchor text index. For the full-text index, we only have to make separate indexes for the page filtered samples. Sampling links has no impact on the fulltext document representations. Since page sampling has an effect on both collection size and link density and link sampling only affects link density, we will first look at the impact of sampling links.
4.1 The Impact of Link Density
If we remove links from the collection, we can expect the performance of anchor text to go down. If we remove all links, the anchor text index is empty and no page will ever be returned. The more links in the collection, the more information we have to distinguish between pages. Therefore, we expect to see effectiveness increase with link density, at least while the density is low. Beyond a certain point the effectiveness might stabilise or become worse.
We filter links by randomly selecting n% of all documents and removing their outgoing links. The impact of sampling outgoing links on the number of inter- and intra-server links is shown in Figure 1. Reading from right to left, both the number of inter- and intra-server links decrease, linear to the sample size. How does this affect the pages with anchor text? If we remove 50% of the links, pages will lose roughly half of their incoming anchors. For

1e+10 1e+09

Inter-server (Link filtered) Intra-server (Link filtered) Inter-server (Page filtered) Intra-server (Page filtered)

Number of links

1e+08

1e+07

1e+06

100000 1

10

100

Sample size (% of collection)

Figure 1: The impact of outgoing link sampling on the number of inter- and intra-server links per sample.

Table 2: Impact of link filtering on the percentage of pages with

anchor text

All pages

Relevant pages

Percent Inter Intra All Inter Intra All

100.000 15.30 70.26 75.43 25.54 74.46 80.96

50.000 11.41 56.35 61.51 21.04 64.84 71.96

25.000 8.24 43.79 48.36 17.14 54.87 61.97

12.500 5.78 33.06 36.75 13.77 44.15 50.75

6.250 3.94 24.17 26.96 10.94 35.81 41.80

3.125 2.61 17.00 19.00 8.35 28.41 33.33

pages with high in-degree, this might not affect the document representation much. Pages with only one or a few incoming anchors could lose most or all of their anchor text. In other words, the most important pages are robust against random link sampling.
In Table 2 we see how filtering affects the percentage of pages that have at least one incoming link with anchor text. The interserver links cover only 15% of all pages but 25% of the relevant pages. Apparently, pages with incoming links from other sites have a higher probability of being relevant. The intra-server links cover a much larger part of the collection (70%). The ratio of inter- to intraserver links is about 1 to 5.5. At a sample size of 12.5% the number of intra-server links is lower than the number of inter-server links at a sample size of 100%. However, at 12.5%, the intra-server links cover 33% of the pages (44% of the relevant pages), while at 100% the inter-server links cover only 15% of the pages and 25.54% of the relevant pages. The intra-server links are thus more uniformly distributed, leading to fewer anchors per page. What does this mean for the effectiveness of inter- and intra-server links?
The impact of sampling links on the effectiveness of full-text and anchor text is shown in Figure 2. The full-text index is not affected by link sampling, hence the straight line in the figures. The statMAP of the Anchor run slowly decreases as we remove more links because the index covers fewer pages. The Mix run scores better at statMAP with even the smallest samples of links, indicating that even very few links can improve the Text run. We also look at traditional MAP and found very similar results. Contrary to our expectations, with smaller samples, the MPC(30) scores of the anchor text run stay well above the Text score. We note that below 12.5% of the links (less than 3 incoming links per page), the density is well below the link densities of earlier TREC Web collections. The impact of link density seems small. One possible explanation is that the highest quality pages have so many incoming links that they are robust against link sampling. This is reflected in the percentages shown in Table 2. With fewer links, the number

125

StatMAP MPC(30)

0.18 0.16 0.14 0.12
0.1 0.08 0.06 0.04 0.02
0 0

Text Anchor
Mix
10 20 30 40 50 60 70 80 90 100 Percentage sampled of collection

0.6 Text

Anchor

0.5

Mix

0.4

0.3

0.2

0.1

0 0 10 20 30 40 50 60 70 80 90 100
Percentage sampled of collection

MRR

0.7 Text

Anchor

0.6

Mix

0.5

0.4

0.3

0.2

0.1

0 0 10 20 30 40 50 60 70 80 90 100
Percentage sampled of collection

Figure 2: Impact of link sampling on effectiveness of full-text, anchor text and mixture runs.

MPC(30) MRR

0.5 0.45
0.4 0.35
0.3 0.25
0.2 0.15
0.1 0

0.55 Inter-server links Intra-server links
0.5

0.45

0.4

0.35

0.3

0.25

0.2

2e+08 4e+08 6e+08 8e+08 1e+09 1.2e+09 1.4e+09 Number of links

0.15 0

Inter-server links Intra-server links
2e+08 4e+08 6e+08 8e+08 1e+09 1.2e+09 1.4e+09 Number of links

Figure 3: Comparison of inter- and intra-server link anchor effectiveness.

of pages with anchors goes down, but the number of relevant pages decreases more slowly. As the link density goes down, the relevant pages form a larger part of the index. On the other hand, the Anchor run might find relevant pages ranked much lower by the runs that contributed the pages to the pool, which represent many estimated relevant pages. To rule out that the MPC(30) score is over-estimated we transformed the relevance judgements to traditional binary judgements and looked at the Mean Reciprocal Rank (MRR, right side of Figure 2). The MRR never over-estimates as it simply counts the rank of the highest ranked relevant document. It supports that anchor text gives better early precision than full-text.
What is the qualitative difference between inter- and intra-server links? We already saw there is a big quantitative difference. Most of the links are between pages on the same server (85%). The left side in Figure 3 shows the MPC(30) scores for the inter- and intraserver Anchor runs. Of course, any observed difference could be due to the larger quantity of intra-server links. Therefore, we show the scores with the actual number of links on the x-axis. Even at a similar number of links, the intra-server Anchor run scores better. An explanation is that the intra-server link anchors cover more pages, because they are more evenly distributed, and can thus find more relevant pages. The difference in MRR between the inter- and intra-server links is smaller, and both scores go up with more links, showing that both are better able to identify relevant pages with higher link density. However, the impact of link density quickly stabilises beyond a certain point. In the first tier of a Web index, containing high quality pages, there seems to be little qualitative difference between inter- and intra-server links. Insight in search engine optimisation may have taught Web site owners to make internal anchor text more meaningful. The larger quantity of intraserver links, and their more even distribution makes them more effective for finding multiple relevant pages.
What is the impact of link density on the effectiveness of anchor text? It plays a role at low densities, but its impact stabilises quickly. Inter- and intra-server links have different distributions and a different coverage of the collection. Within a collection of high PageRank pages, the difference between inter- and intra-server links is more quantitative than qualitative. Without a crawling pol-

Table 3: Number of documents and topics per sample

Percent Size in Docs # Rel. Docs Topics # Rel./topic

100.000 50,220,423

4,002

49

81.57

50.000 25,110,211

1,987

49

40.55

25.000 12,555,105

965

47

20.53

12.500 6,277,552

486

46

10.57

6.250 3,138,776

253

44

5.75

3.125 1,569,388

132

42

3.14

icy that focuses on finding high quality pages first, the quality of the crawl and therefore the quality of the intra-server links might go down. Inter-server links might be more robust, as they tend to have less navigational anchors and are harder to use nepotistically.

4.2 The Impact of Collection Size
Next, we look at the impact of the collection size. How effective is anchor text if we reduce the size of the collection? To see this, we need to down-sample the collection, which we do by randomly selecting half of the pages. We show the number of (relevant) pages in each filtered sample in Table 3. At each step, the ratio of all pages and relevant pages is roughly the same. If we remove 50% of the pages in the collection, and remove those from the relevance judgements as well, we end up with about 50% of the relevant pages. For smaller samples this has consequences for the number of topics with any relevant document. At 3.125%, we have 132 relevance judgements left for 42 topics (3.14 relevant pages per topic). Although the number of topics is still large enough to be representative, the low number of relevant pages per topic might make per topic results unreliable.2
As mentioned above, if we sample pages, the number of links does not decrease linearly to the sample size. In Table 4 we see the percentage of pages in the sample that have at least one incoming link. One interesting observation is that page sampling has a similar impact on the coverage of inter-server links as link sampling, but a very different impact on the coverage of intra-server links. If we sample 3.125% of the links (Table 2), the intra-server anchors cover 17% of all pages, while if we sample 3.125% of the pages, the intra-server anchors cover 43% of the remaining pages. Yet

2The relevance judgements of the Ad hoc task include the proba-

bility of a page being included in the assessment pool. This proba-

bility is used to estimate how many pages a pooled page represents,

which is based on the size of the collection. If a relevant page has

a pool probability of 0.2 it represents

1 0.2

=

5 relevant pages in

the full collection. With uniform down-sampling, the probability

of a page being sampled stays the same, but the number of pages

it represents is proportional to the sample size. At a sample size of

25% of the full collection it represents 25% of the relevant pages

it represents in the full collection. Most judged pages have a unit

pooling probability, so are not affected by down-sampling.

126

Table 4: Impact of page filtering on the percentage of pages

with anchor text All pages

Relevant pages

Percent Inter Intra All Inter Intra All

100.000 15.30 70.26 75.43 25.54 74.46 80.96

50.000 11.40 61.09 65.49 19.23 67.54 73.63

25.000 8.23 53.96 57.50 15.34 61.04 66.32

12.500 5.77 48.73 51.41 14.40 59.26 63.99

6.250 3.94 45.07 46.98 10.28 52.57 56.52

3.125 2.61 42.59 43.88 7.58 50.76 53.79

the inter-server anchors cover 2.61% of the collection, whether we sample 3.125% of the links or the pages.
The impact of sampling pages on the effectiveness of full-text and anchor text is shown in Figure 4. The statMAP (left figure) of the Text run goes up slowly--possibly due to losing topics with little relevance--while for the Anchor run it goes down slowly. Theory suggests that statMAP should remain relatively stable over random samples of a collection [18]. The drop in statMAP for the Anchor run can be explained by looking at the precision scores. The Text run gains precision at rank 30 (MPC(30), centre figure) as the collections grows, as predicted [18]. The anchor text precision is more affected by collection size. With half the collection, anchor text is nowhere near as effective as full-text. The MRR (right figure) of the Text run is similar to that of the statMAP. The average rank of the first relevant document increases slowly, partly due to losing topics. However, the MRR of the Anchor run drops rapidly with smaller samples. With fewer relevant documents left, and an increasingly smaller coverage of the collection, it becomes harder to find relevant pages through anchor text.
What is the impact of the collection size on anchor text? For precision at a fixed cut-off, the impact of the collection size is much larger for anchor text than for full-text. First, collection size affects the anchor text representation but not the full-text representation. Second, the number of pages in the full-text index grows linearly with collection size, but more than linearly for anchor text. For ad hoc search, where the task is to find pages with relevant text no matter their popularity, this coverage is essential. We stress again the importance of having a collection of high quality pages; expanding the collection with low quality pages will probably also lower the quality of link anchors.
In summary, link density does not explain the effectiveness of anchor text as even a small number of links lead to improved performance. Intra-server links are at least as effective as inter-server links because they cover a larger part of the collection. The size of the collection plays a larger role than link density, because it has a larger impact on the number of pages with anchor text and the quality of their anchor text representation. The effectiveness of anchor text rapidly increases as we expand the collection. We now look at how these findings hold up in a more Web-oriented search task.

5. DIVERSITY TASK
We now look at the effectiveness of full-text and anchor text approaches for result diversity, where the task is to present a diverse set of results in the top 10 or 20 results. The Diversity task uses the same topics, but breaks them down into a number of informational and navigational sub-topics. This allows a deeper analysis of the various strengths and weaknesses of full-text and anchor text approaches. The task thus combines both ad hoc search and entry and named page finding. Given the earlier successes with anchor text for the latter tasks [9, 29], and its good performance on ad hoc

alpha-nDCG@10 alpha-nDCG@10

Table 5: Ad hoc and Diversity evaluation using the Diversity

relevance judgements. Significance tests are with respect to the full text run, confidence levels are 0.95 (), 0.99 (·) and 0.999 (·)

Run

-nDCG@10 nDCG@10 IA-P@10 P@10

T ext Anchor T ext + Anchor

0.120 0.257· 0.223·

0.1564 0.2780· 0.2459·

0.054 0.1700 0.082 0.2460 0.083· 0.2420·

uogTrDYCcsB

0.282

­

0.132 ­

ICTNETDivR3

0.272

­

0.095 ­

UamsDancTFb1

0.250

­

0.079 ­

0.3

0.3

Text

Text

Anchor

Anchor

0.25

Mix

0.25

Mix

0.2

0.2

0.15

0.15

0.1

0.1

0.05

0.05

0 0 10 20 30 40 50 60 70 80 90 100
Percentage sampled of collection

0 0 10 20 30 40 50 60 70 80 90 100
Percentage sampled of collection

Figure 5: Impact of link sampling (left) and page sampling (right) on diversity of full-text, anchor text and mixture runs.

precision described in the previous sections, we conjecture anchor text to perform better than full-text on the diversity task as well.
Anchor text shares characteristics with queries [11]. A wellknown problem with IR research is the fact that different users can type the same query but have different information needs. Thus, a query can and will be used to search for different types of information, or different aspects or facets of a topic. The same might hold for anchor text. Two Web page authors can use the same anchor text to link to different target pages, covering different topics or different aspects of the same topic.The Diversity relevance judgements are based on a pool of top 20 results of all official runs and were made independently from the ad hoc judgements, thus provide a sanity check on our findings from the ad hoc experiments.
We first compare the performance of the anchor text and full-text runs on the full collection in Table 5. The official measures are nDCG@10 and IA-P@10. The differences are very clear. Anchor text performs significantly better than full-text search. In the diversity score, the ad hoc relevance ranking plays an important role. If the relevance ranking is low, that is, there are many irrelevant documents in the top ranks, the diversity score will therefore be low as well. Therefore, we turned the diversity qrels into standard TREC qrels by making a page relevant for a topic if it is relevant for at least one sub-topic. This allows us to compare the diversity specific measures with their non-diversity counterparts (columns 3 and 5 in Table 5). We see very similar patterns for -nDCG@10 and normal nDCG@10. The same holds for IA-P@10 and P@10. These results suggest the anchor text scores much better on the diversity measures simply because it has a better underlying relevance ranking, supporting our findings in the previous section.
What is the impact of link density and collection size on the diversity of anchor text results? We use the same indexes and runs as described in the previous section. The results are shown in Figure 5. Sampling links (left figure) has a similar effect on diversity as on ad hoc search. Again, we see that anchor text becomes more effective with more links, but the improvements become smaller beyond 25% of the links. However, even with 3.125% of the links, the anchor text run is at least as good as the full-text run. Again, link density seems to matter only at very low densities. We looked

127

StatMAP MPC(30)
MRR

0.4 Text

Anchor

0.35

Mix

0.3

0.25

0.2

0.15

0.1

0.05

0 0 10 20 30 40 50 60 70 80 90 100
Percentage sampled of collection

0.6 Text

Anchor

0.5

Mix

0.4

0.3

0.2

0.1

0 0 10 20 30 40 50 60 70 80 90 100
Percentage sampled of collection

0.7 Text

Anchor

0.6

Mix

0.5

0.4

0.3

0.2

0.1

0 0 10 20 30 40 50 60 70 80 90 100
Percentage sampled of collection

Figure 4: Impact of page sampling on effectiveness of full-text, anchor text and mixture runs.

Table 6: Impact of page sampling on diversity of the TREC

2009 Diversity topics

Qrels

Found in top 10

Sub-topics Full-text Anchor

Percentage Topics Inf. Nav. Avg. Inf. Nav. Inf. Nav.

100.000

50 168 31 3.98 40 2 63 8

50.000

50 145 20 3.30 42 2 47 7

25.000

48 126 15 2.94 53 2 32 3

12.500

45 103 9 2.49 44 2 22 2

6.250

41 78 5 2.02 42 1 14 1

3.125

37 62 4 1.78 38 2 11 2

at the impact of inter- and intra-server links and found that interserver links find more relevant pages for navigational topics than intra-server links, while the reverse is true for informational topics. Inter-server links tend to point to entry pages of sites, while intraserver links cover a much larger part of sites, which is in line with earlier studies [20]. In other words, for ad hoc search, intra-server links are at least as important as inter-server links.
On the right side of Figure 5 we see the impact of sampling pages on the effectiveness of anchor text and full-text. We only evaluate on the topics that have at least one relevant page in the sampled collections. Similar to the impact on the ad hoc performance of anchor text, the size of the collection plays a huge role. Performance drops as the collection becomes smaller, although the drop from 100% to 50% of the collection is less severe than for the ad hoc task. Oddly enough, the full-text run gets better with smaller collections. How can these observations be explained?
The impact of page sampling on the relevance judgements for the Diversity task are shown in Table 6. The number of informational sub-topics with at least one relevant document drops from 168 to 62, while the number of navigational sub-topics drops from 31 to 4. At the smallest samples, the Diversity relevance judgements have almost been reduced to ad hoc judgements. With only 1.78 subtopics per topic, there is not much to diversify and the incurred penalty for retrieving pages on the same sub-topic is small.
What is the impact of anchor text on informational and navigational topics? The final four columns show the number of subtopics for which the full-text and anchor text runs find relevant pages in the top 10. The diversity of the full-text run is hardly affected by the sample size, explaining why at smaller samples with less sub-topics available, its score goes up. The Anchor run finds both more informational and navigational topics in the full collection, showing it actually does better on the informational part of the topics than the Text run. However, it suffers greatly from the reduced collection size, especially for the informational sub-topics. Collection size plays a large role in the effectiveness of anchor text for informational search.

6. CONCLUSIONS
The history of scientific benchmarking for Web IR is plagued with the apparent contradiction between the experiences of Internet search engines, and the results of experiments at TREC [15, 16, 19, 33]. This led to Google's Larry Page calling the entire formal evaluation process "irrelevant" during a heated panel debate at the 2000 Infornotics Search Engine Meeting [31]. After several years of disappointing results at TREC Web Tracks, it was surmised that Web structure is simply not effective for ad hoc search tasks. TREC moved on to Web-centric tasks, where link topology, anchor text, and URL structure were proven very effective for navigational search, such as site finding and home page finding.
The availability of a new test collection, ClueWeb09 [7], which is a much closer approximation of the index of Internet Search Engines than earlier collections, prompts us to revisit the standing question of the importance of anchor text for ad hoc search. Our main finding is that in contrast with earlier results, the anchor text leads to significant improvements in retrieval effectiveness for ad hoc informational search. More specifically, the pure anchor text runs lead to substantially higher precision than the full-text runs but the full-text runs have better recall. The straightforward combination of document and anchor text runs leads to significantly better scores throughout.
A negative finding is that link evidence like in-degree does not contribute to retrieval effectiveness. In contrast with the anchor text, link degree is ignorant of the query at hand, and its main use is in separating the authoritative or important pages from the less popular ones. An underlying difference with earlier collections is that the ClueWeb crawl is based on a PageRank/OPIC policy rather than the standard breadth-first strategy [12]. As a result, all pages in the collection have a relatively high level of `importance,' and on top of that there is no additional value in link degrees.
The main focus of this paper is what makes the anchor text representation effective. Previous research pointed at the need of high (inter-server) link density [e.g., 4]. Our finding is that link density has little impact on anchor text effectiveness. Anchor text proved remarkably robust, and even with a small number of links it is effective for high early precision. Contrary to expectations, we find that intra-server link anchors are at least as effective as inter-server link anchors, even at equal density. Within a collection of high quality pages, such as the first tier of Web search engine indexes, the qualitative difference between inter- and intra-server links is minimal. But the greater quantity of intra-server links makes them more effective than inter-server links. Another factor is crawl or collection size, and ClueWeb09 is substantially larger than earlier testbeds. Our finding is that collection size has a big impact on the anchor text representations, affecting quantity, quality and effectiveness. A larger collection has more anchor-text covering a larger part of the collection. Especially within a crawl of high quality pages, more

128

links mean more high quality anchor text, leading to higher early precision than full-text search.
We also looked at the diversity task, which is a more Web-centric search task and does away with the notion that information is relevant no matter how often the user has seen it. The diversity task used the same topics and collection, but different judgments and measures. Our finding is that anchor text significantly outperforms full-text search, with greater differences and significance than for the ad hoc search task. This result also broadly confirms our findings for the ad hoc task. Anchor text is effective even at low link density; however, on smaller collections, the anchor-text covers an increasingly small part of the collection and loses its power. Fulltext search is less affected by collection size.
Perhaps the main contribution of this paper is that it solves the apparent contradiction between the experiences of Internet search engines, and the results of experiments at TREC. Negative results for ad hoc informational search using Web structure have tainted the reputation of reproducible IR evaluation. The positive results in this paper may help to set the record straight. This turns, the earlier negative results into something positive in a sense: they aid to our understanding of when and why link evidence works, and when not.
Acknowledgments
This work was generously supported by the Netherlands Organization for Scientific Research (NWO, grants # 612.066.513, 639.072.601, and 640.001.501).
We will make the link anchors and samples available upon request. For details see: http://staff.science.uva.nl/ ~kamps/museum/anchors.
REFERENCES
[1] S. Abiteboul, M. Preda, and G. Cobena. Adaptive on-line page importance computation. In WWW, pages 280­290, 2003.
[2] B. Amento, L. Terveen, and W. Hill. Does `authority' mean quality? predicting expert quality ratings of web documents. In SIGIR, pages 296­303. ACM, 2000.
[3] R. A. Baeza-Yates, C. Castillo, M. Marín, and A. Rodríguez. Crawling a country: better strategies than breadth-first for web page ordering. In WWW, pages 864­872. ACM, 2005.
[4] P. Bailey, N. Craswell, and D. Hawking. Engineering a multipurpose test collection for web retrieval experiments. Inf. Process. Manage., 39(6):853­871, 2003.
[5] J. Callan, C. Yoo, and L. Zhao. Web08-PR Dataset, 2008. Project planning document.
[6] C. A. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 Web Track. In TREC 2009.
[7] CMU-LTI. The ClueWeb09 Dataset, 2009. URL http:// boston.lti.cs.cmu.edu/Data/clueweb09/.
[8] N. Craswell, P. Bailey, and D. Hawking. Is it fair to evaluate Web systems using TREC ad hoc methods? In ACM SIGIR Workshop on Evaluation of Web Document Retrieval, 1999.
[9] N. Craswell, D. Hawking, and S. E. Robertson. Effective site finding using link anchor information. In SIGIR, pages 250­ 257, 2001.
[10] N. Craswell, D. Fetterly, M. Najork, S. Robertson, and E. Yilmaz. Microsoft Research at TREC 2009. In TREC, 2009.
[11] N. Eiron and K. S. McCurley. Analysis of anchor text for web search. In SIGIR '03, pages 459­460. ACM, 2003.
[12] D. Fetterly, N. Craswell, and V. Vinay. The impact of crawl

policy on web search effectiveness. In SIGIR, pages 580­587.
ACM, 2009.
[13] D. Fetterly, N. Craswell, and V. Vinay. Measuring the search effectiveness of a breadth-first crawl. In ECIR, pages 388­
399, 2009.
[14] C. Gurrin and A. F. Smeaton. Replicating web structure in small-scale test collections. Inf. Retr., 7:239­263, 2004.
[15] D. Hawking. Overview of the TREC-9 Web Track. In TREC,
2000.
[16] D. Hawking and N. Craswell. Overview of the TREC-2001 Web Track. In Proceedings of TREC-2001, 2001.
[17] D. Hawking and N. Craswell. Very large scale retrieval and web search. In TREC: Experiment and Evaluation in Information Retrieval, chapter 9. MIT Press, 2005.
[18] D. Hawking and S. Robertson. On collection size and retrieval effectiveness. Inf. Retr., 6(1):99­105, 2003.
[19] D. Hawking, E. M. Voorhees, N. Craswell, and P. Bailey. Overview of the trec-8 web track. In TREC, 1999.
[20] D. Hawking, F. Crimmins, N. Craswell, and T. Upstill. How
valuable is external link evidence when searching enterprise webs? In ADC, pages 77­84, 2004.
[21] Indri. Language modeling meets inference networks, 2009. http://www.lemurproject.org/indri/.
[22] J. Kamps. Effective smoothing for a terabyte of text. In TREC,
2005.
[23] J. Kamps. Experiments with document and query representations for a terabyte of text. In TREC, 2006.
[24] J. Kamps. Web-centric language models. In O. Herzog, H.-
J. Schek, N. Fuhr, A. Chowdhury, and W. Teiken, editors, CIKM, pages 307­308. ACM, 2005. ISBN 1-59593-140-6.
[25] R. Kaptein, M. Koolen, and J. Kamps. Result diversity and
entity ranking experiments: Text, anchors, links, and wikipedia. TREC, 2009.
[26] W. Kraaij, T. Westerveld, and D. Hiemstra. The importance of prior probabilities for entry page search. In SIGIR, pages
27­34. ACM, 2002.
[27] D. Metzler, J. Novak, H. Cui, and S. Reddy. Building enriched
document representations using aggregated anchor text. In SIGIR '09, pages 219­226. ACM, 2009.
[28] M. A. Najork, H. Zaragoza, and M. J. Taylor. HITS on the Web: How does it compare? In SIGIR '07, pages 471­478.
ACM, 2007.
[29] P. Ogilvie and J. P. Callan. Combining document representations for known-item search. In SIGIR, pages 143­150, 2003.
[30] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank
citation ranking: Bringing order to the web. Technical report,
Stanford Digital Library Technologies Project, 1998.
[31] C. Sherman. `old economy' info retrieval clashes with
`new economy' web upstarts at the fifth annual search engine conference. Information Today Newsbreaks, 2000. http:// web.archive.org/web/20001217211000/www.
infotoday.com/newsbreaks/nb000424-2.htm. [32] A. Singhal and M. Kaszkiel. AT&T at TREC-9. In TREC,
2000.
[33] A. Singhal and M. Kaszkiel. A case study in web search using TREC algorithms. In WWW10, pages 708­716, 2001.
[34] TREC. Text-REtrieval Conference, 2009. http://trec. nist.gov/.
[35] E. Yilmaz and J. A. Aslam. Estimating average precision with incomplete and imperfect judgments. In CIKM '06, pages
102­111, New York, NY, USA, 2006. ACM.

129

Learning to Efficiently Rank

Lidan Wang
Dept. of Computer Science University of Maryland College Park, MD
lidan@cs.umd.edu

Jimmy Lin
The iSchool University of Maryland
College Park, MD
jimmylin@umd.edu

Donald Metzler
3333 Empire Ave. Yahoo! Research
Burbank, CA
metzler@yahoo-inc.com

ABSTRACT
It has been shown that learning to rank approaches are capable of learning highly effective ranking functions. However, these approaches have mostly ignored the important issue of efficiency. Given that both efficiency and effectiveness are important for real search engines, models that are optimized for effectiveness may not meet the strict efficiency requirements necessary to deploy in a production environment. In this work, we present a unified framework for jointly optimizing effectiveness and efficiency. We propose new metrics that capture the tradeoff between these two competing forces and devise a strategy for automatically learning models that directly optimize the tradeoff metrics. Experiments indicate that models learned in this way provide a good balance between retrieval effectiveness and efficiency. With specific loss functions, learned models converge to familiar existing ones, which demonstrates the generality of our framework. Finally, we show that our approach naturally leads to a reduction in the variance of query execution times, which is important for query load balancing and user satisfaction.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Performance
Keywords: learning to rank, linear models, effectiveness and efficiency tradeoff
1. INTRODUCTION
Web search engines allow users to find information on almost any topic imaginable. To be successful, a search engine must return the most relevant information to the user in a short amount of time. However, efficiency (speed) and effectiveness (relevance) are competing forces that often counteract each other. It is often the case that methods developed for improving relevance incur moderate-to-large computational costs. Therefore, sustained relevance gains must often be counter-balanced by buying more (or faster) hardware,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

implementing caching strategies if possible, or spending additional effort in low-level optimizations.
It is common for search engines to select a single operating point in the space of all possible efficiency-effectiveness tradeoffs. However, users and information needs are diverse. While most users may want their search results immediately, others may not mind waiting a little extra time if it means their results, on average, would be better. This same idea can be applied to information needs. Certain classes of queries, such as those for simple information needs, are expected to be answered immediately. However, for very complex information needs, users may be willing to incur additional latency for better results. Hence, operating at a "one size fits all" point along the tradeoff curve may not be optimal for all users and queries.
In this paper, we explore issues related to the efficiencyeffectiveness tradeoff in the context of developing highly effective, highly efficient search engine ranking functions. We propose a framework for automatically learning ranking functions that optimize the tradeoff between efficiency and effectiveness. Traditional learning to rank approaches [15], have focused entirely on effectiveness. Therefore, it is more appropriate to think of our proposed approach as a learning to efficiently rank method. We will show that learning to rank (i.e., only optimizing effectiveness) is simply a special case of our proposed framework.
Our framework consists of two components. The first is a set of novel metrics for quantifying the tradeoff between efficiency and effectiveness. The second is an approach to optimizing the metrics for a class of linear ranking functions. As we will show, the framework is robust and effective. It can be used to learn a "one size fits all" ranking function, or be used to learn different ranking functions for different classes of users and information needs that may have their own unique efficiency-effectiveness tradeoff requirements.
There are four primary contributions of our work. First, we motivate and formulate the problem of jointly optimizing the effectiveness and efficiency of search engine ranking components. Second, we introduce novel metrics for quantifying the tradeoff between effectiveness and efficiency. We demonstrate that such metrics can robustly support a full spectrum of effectiveness-efficiency tradeoff scenarios that can easily be adapted to a variety of search tasks, or targeted towards specific user populations, depending on preferences. Third, we present learning to rank methods for optimizing our proposed tradeoff metrics. Experiments show that models learned in this way outperform traditional machine-learned models in terms of achieving an optimal

138

balance between effectiveness and efficiency. Finally, we discuss how our framework yields models with significantly reduced query execution time variance, which is a desirable property for load balancing and user satisfaction in practical search settings.
The remainder of this paper is laid out as follows: First, Section 2 surveys related work. Next, in Section 3 we propose novel metrics for quantifying the tradeoff between effectiveness and efficiency. Section 4 discusses methods for automatically learning models that are both effective and efficient by optimizing our proposed tradeoff metrics. Section 5 presents experimental results under different tradeoff scenarios. Finally, we conclude the paper and propose areas for future work in Section 6.
2. RELATED WORK
There has been a great deal of research devoted to developing efficient and effective retrieval systems. This has given rise to two distinct research threads. The focus of the first thread is on designing effective retrieval models. This has given rise to a steady stream of effectiveness-centric models, such as language models for information retrieval [20], the BM25 model [21], numerous term proximity models [16, 9, 23, 3], and learning to rank [12, 18, 6, 15]. The other thread is devoted to building efficient retrieval systems. Improved query execution strategies [22, 1] and advanced index pruning techniques [10, 8, 19] are just two examples of successful research directions along this thread.
The fact that these two threads are almost always investigated exclusively of each other has created a virtual dichotomy in the information retrieval research community. On one side there are researchers who develop highly effective, yet practically infeasible models and methods en masse. On the other side of the dichotomy are the researchers who design blazingly fast, yet spectacularly ineffective systems. One of the goals of our approach is to take a step towards eliminating this dichotomy by taking an efficiency-minded look at building effective retrieval models.
Our problem is quite different from previous work in index pruning [10, 7, 8, 1, 19] and query segmentation [4]. The primary goal of index pruning is to create a small index and search over this reduced index to gain better efficiency. In query segmentation, a syntactic parser is used to identify key term dependence features in a query, and only these key features, rather than all term dependence features, are used to retrieve documents. While both techniques are designed for dealing with query latency, these methods do not directly optimize the underlying efficiency and effectiveness metrics, e.g., optimizing index pruning or optimizing segmentation accuracy is not guaranteed to optimize retrieval effectiveness and efficiency, and their tradeoff.
Another way to speed up query evaluation is through caching [2] (i.e., term posting lists or query search results caching). Our problem and caching can be viewed as two complementary approaches for improving efficiency in search. Cached postings/results for a given query can be used during the query evaluation stage to improve efficiency, where the ranking function has been specified. In contrast, we learn efficient ranking functions, where a query evaluation strategy and a caching strategy are assumed to be given.
There have been several solutions proposed for dealing with the efficiency-effectiveness tradeoff in various contexts. First, in the machine learning community, it was shown that

l1 regularization is useful for "encouraging" models to have only a few non-zero parameters, thereby greatly decreasing the time necessary to process test instances [24]. Thus, l1 regularized loss functions balance between model effectiveness (e.g., mean squared error, classification accuracy, etc.) and efficiency (number of non-zero parameters). However, quantifying efficiency in this way is overly simple and not very flexible. Indeed, the efficiency of most ranking functions can not be modeled simply as a function of the number of non-zero parameters, since the costs associated with evaluating different features are unlikely to be uniform (e.g., unigram scoring vs. term proximity scoring). The efficiency of a system ultimately depends on the specific implementation, architecture, etc. Therefore, l1 regularization is too simple to be effective for jointly optimizing the effectiveness and efficiency of ranking functions.
In a similar direction, Collins-Thompson and Callan [11] investigated strategies for robust query expansion by modeling expansion term selection and weighting using convex programming. Their model included a variant of l1 regularization that imposed a penalty for including common terms in the expanded query, since such terms would likely increase query execution time. This was the first effort that we are aware of that modeled efficiency in a search enginespecific manner. However, we note that query expansion and constructing ranking functions are two different problems and hence present different challenges. Furthermore, in this work, we model system efficiency using actual query execution times, instead of simple surrogates, such as term frequency, that may or may not accurately model the actual efficiency of the underlying retrieval system.
Finally, our work can be viewed as an enhancement of existing learning-to-rank strategies, which have, until now, focused exclusively on effectiveness. We expect the exploitation of efficiency in constructing effective ranking functions will allow for rapid development of highly effective and efficient retrieval models.
3. TRADEOFF METRICS
In this section we propose a novel class of tradeoff metrics that consider both effectiveness and efficiency. We begin by defining a general class of efficiency functions and describing how different functions yield different tradeoffs.
3.1 Measuring Efficiency
There are many different ways to measure the efficiency of a search engine, such as query execution time and queries executed per second. We are primarily interested in measuring how efficient a ranking function is at producing a ranked list for a single query. Throughout the remainder of this paper, we will assume that our measure of interest is query execution time, although any other query-level measure of efficiency could also be used.
Query execution times are, in theory, unbounded. This makes them difficult to work with from an optimization point of view. Instead, we would like to map query execution times into the range [0, 1]. We accomplish this by defining a function (·) : R+  [0, 1] that takes a query execution time, denoted by  (Q) as input and returns an efficiency metric in the range [0, 1], where 0 represents an inefficient ranking function and 1 represents an efficient ranking function.
We now define four different efficiency metrics. Each metric differs by how (·) is defined.

139

Constant. The most trivial efficiency metric is defined as (Q) = c, for c  [0, 1]. This constant efficiency metric is always the same, regardless of the query execution time. This is the default assumption made by previous learning to rank approaches, which ignore efficiency altogether.
Exponential Decay. This loss function is defined as:
(Q) = exp( ·  (Q))
where  < 0 is a parameter that controls how rapidly the efficiency metric decreases as a function of query execution time. If a large (negative) decay rate (i.e., ) is specified, then the metric will drop off very quickly, penalizing all but the fastest query execution times.
Step Function. Often it is necessary to incorporate query execution time preferences into the efficiency metric. For instance, users may have a certain tolerance level for query execution time, such that they would expect the time to be less than a target t milliseconds for each query. A step function can naturally account for this requirement, as follows:
( 1, if  (Q)  t
(Q) = 0, if  (Q) > t

The step function metric is maximal (1) when query execution time is less than t and minimal (0) otherwise.
Step + Exponential Decay. If query execution time exceeds the threshold t in the step function efficiency metric, but only by a small amount, the metric assigned will still be 0, which may be overly harsh. Instead, it may be more reasonable to define a soft loss-like function, as follows:

(

1,

if  (Q)  t

(Q) =

exp( · ( (Q) - t)), if  (Q) > t

where  < 0. The resulting function is a step function up until the threshold t and an exponential decay after time t with parameter .
Figure 1 summarizes the four efficiency metrics just described. Note that there are many other ways to define (·) beyond those explored here. The best functional form for a given task will depend on many factors, including dataset size, hardware configuration, among others.
3.2 Measuring Effectiveness
There has been a great deal of research into evaluating the effectiveness of information retrieval systems. Therefore, we simply make use of existing effectiveness measures here. We define the effectiveness of a query Q as (Q).
As with the efficiency metrics, we are primarily interested in effectiveness measures with range [0, 1]. Most of the commonly-used effectiveness metrics satisfy this property, including precision, recall, average precision, and NDCG. In this paper we will exclusively focus on average precision as the effectiveness metric of interest, although any of the above metrics can be substituted in our framework without loss of generality.
3.3 Efficiency-Effectiveness Tradeoff Metric
Our goal is to automatically learn ranking models that achieve an optimal middle ground between effectiveness and efficiency. However, before we can learn such a well-balanced model, we must define a new metric that captures the tradeoff. Our metric, which we call Efficiency-Effectiveness Trade-

Constant

Exponential decay

1

Step function

Step + exponential decay

0.8

Efficiency

0.6

0.4

0.2

0

0

200

400

600

800

Ranking time (ms)

Figure 1: Efficiency functions. Constant and exponential decay are self explanatory. The step function and step + exponential function can model time preferences (threshold t=300ms here); when exceeding the time requirement, the ranking model either gets zero efficiency value or an exponentially lower efficiency value (respectively).

off (eet), is defined for a query Q as the weighted harmonic mean of efficiency (Q) and effectiveness (Q):

(1 + 2) · ((Q) · (Q)) eet(Q) = 2 · (Q) + (Q)

where  is a parameter that controls the relative importance between effectiveness and efficiency. In this work, we set  = 1, which weighs both equally, but other settings can be trivially applied in our approach as well.
Given a ranking model R, the value of eet is computed for each query. To quantify the average tradeoff performance across N queries for a given ranking function, we define the following metric:

Meet(R)

=

1 N

X eet(Q)

which is simply the mean eet value for the set of N queries. It should now be clear that different choices of efficiency
metrics will have a direct influence on Meet. For instance, a sharply decaying exponential efficiency metric represents a low tolerance for inefficient ranking models. Under such a function, the efficiency metric for a ranking function with high query execution time will likely be extremely low, resulting in a small Meet value, even if the ranking function is effective. On the other hand, if the efficiency function decays slowly or is constant, a ranking function with high effectiveness will also likely have a large Meet value.
Different combinations of efficiency metric and effectiveness metric will give rise to different Meet instantiations. Therefore, Meet is not a single metric, but a family of tradeoff metrics that depends on an efficiency component (Q), an effectiveness component (Q), and a tradeoff factor .

4. LEARNING TO EFFICIENTLY RANK
In this section, we describe a class of linear ranking functions we will subsequently use to optimize our proposed tradeoff metrics. This simple class of ranking functions is used to show the benefits possible from optimizing Meet, rather than effectiveness alone.

140

Weighting

fT

(q,

D)

=

log

» tfq,D

+µ

cfq |C|

|D|+µ

­

fO(qj , qj+1, D)

=

log

" tf#1(qj

,qj+1

),D

+µ

cf#1(qj ,qj+1 |C|

)

|D|+µ

#

fU (qj , qj+1,

D)

=

log

" tf#uw8(qj

,qj+1

),D

+µ

cf#uw8(qj ,qj |C|

+1

)

|D|+µ

#

Description Weight of unigram q in document D. Weight of exact phrase "qj qj+1" in document D.
Weight of unordered window qj qj+1 (span = 8) in document D.

Table 1: Features used in the WSD ranking function. Here, tfe,D is the number of times concept e matches in document D, cfe,D is the number of times concept e matches in the entire collection, |D| is the length of document D, and |C| is the total length of the collection. Finally, µ is a weighting function hyperparameter.

4.1 Model
We focus our attention on a class of linear feature-based ranking functions that have the following form:

X

S(Q, D) = jfj(Q, D)

(1)

j

where Q is a query, D is a document, fj(Q, D) is a feature function, and j is the weight assigned to feature j. This ranking function is linear with respect to the model parameters . Various learning to rank approaches exist for finding parameters that optimize retrieval effectiveness metrics for these types of ranking functions [15]. One of the main benefits of such models is their ability to combine many different kinds of features in a straightforward manner, as we demonstrate below.
However, since we are also interested in optimizing for efficiency, we would like a mechanism for altering the efficiency characteristics of the ranking function. The most straightforward way to accomplish this is to eliminate one or more features from the ranking function. A logical way of choosing features to eliminate are those with small weights, as is done with l1 regularization. We adopt a slight variant of this approach, where we assume that each weight  also takes on a parametric form, as follows:

X j (Q) = wigi(Q)
i

where gi(Q) is a meta-feature that takes Q as input (discussed a bit later), and wi is the weight assigned to the meta-feature. Notice that the weights  are now query dependent, which means they can adapt better to different query scenarios via the feature functions gi. We will show shortly that allowing  to depend on Q provides an intuitive way to prune features.
Plugging these query-dependent weights into our original linear ranking function (Equation 1) gives us the following ranking function:

XX S(Q, D) = wi gi(Q)fj(Q, D)

i

j

which is still a linear ranking function, but now with respect to wi, which are the global, query-independent free parameters that must be learned.
As a concrete example of a highly effective ranking function that takes on this functional form, we consider the weighted sequential dependence (WSD) ranking function that was recently proposed by Bendersky et al. [5]. The WSD ranking function is formulated as:

Feature
g1t (q) g2t (q) g3t (q) g4t (q) g5t (q) g1b(qj , qj+1) g2b(qj , qj+1) g3b(qj , qj+1) g4b(qj , qj+1) g5b(qj , qj+1)

Description # times q occurs in the collection # documents q occurs in the collection
# times q occurs in ClueWeb # times q occurs in a Wikipedia title
1 (constant feature) # times bigram occurs in the collection # documents bigram occurs in the collection
# times bigram occurs in ClueWeb # times bigram occurs in a Wikipedia title
1 (constant feature)

Table 2: Meta-features used in WSD ranking.

S(Q, D) = X wit X git(q)fT (q, D) +

i

qQ

X wib X gib(qj , qj+1)fO(qj , qj+1, D) +

i

qj ,qj+1Q

X wib X gib(qj , qj+1)fU (qj , qj+1, D)

i

qj ,qj+1Q

where the features fT and git are defined over query unigrams, and fO, fU , and gib are defined over query bigrams. We define these features in a similar way as to how they were defined by Bendersky et al. [5]. Table 1 shows how the features fT , fO, and fU are defined, while Table 2 summarizes the meta-features git and gib. Hence, this specific ranking function consists of three general components: 1) a unigram term score, 2) a bigram phrase score, and 3) a bigram proximity score.
While the effectiveness of the ranking function depends on the weights w, models of this form provide a natural mechanism for eliminating features in a query-dependent manner, thereby improving efficiency. We propose to drop features according to the magnitude of i(Q), the query-dependent weight assigned to feature i. If i(Q) is nearly zero, then feature i is unlikely to have a significant impact on the final ranking. Therefore, it should be safe to drop feature i from the ranking function, thereby increasing the efficiency of the model, with minimal impact on effectiveness. This suggests the general strategy of pruning features if |i(Q)|  , where
is a pruning threshold. However, in this work, we are dealing with a model that we have specific domain knowledge about, and therefore use a more suitable pruning strategy. Previous work by Lease [13] demonstrated that unigrams have more positive impact on

141

retrieval effectiveness than bigrams; hence, we only prune bigram features from the WSD ranking function. Bigram features are pruned if they satisfy the following condition:
(qi, qi+1)  (qi) + (qi+1)
This condition says that if the ratio between the bigram feature weight and the sum of individual unigram feature weights is less than , then the bigram is eliminated. Preliminary experiments found the general strategy of pruning according to |i(Q)|  to be effective, but found this ranking function-specific strategy to yield superior results. Therefore, it is likely that different ranking functions will require different pruning strategies to be maximally effective.
4.2 Parameter Estimation
We now describe our method for automatically learning the parameters of our proposed model from training data. We must not only learn the parameters w, but also the concept pruning threshold . Although there are many learning to rank approaches for learning a linear ranking function (i.e., estimating w), our optimization problem is complicated by the fact that we also have to learn the best , which is directly tied to the efficiency of the ranking function. Since the relationship between our metric and can not be modeled analytically, we are forced to directly estimate the parameters using a non-analytical optimization procedure.
We used a simple optimization procedure that directly optimizes Meet: a coordinate-level ascent algorithm similar to the one that was originally proposed in [17]. The algorithm performs coordinate ascent in the Meet metric space. Each (single dimensional) optimization problem is solved using a simple line search. Given that the Meet space is unlikely to be convex, there is no guarantee that this greedy hill climbing approach will find a global optimum, but, as we will show, it tends to reliably find good solutions for our particular problem. The final solution to the optimization problem is a setting of the parameters w and a pruning threshold that is a local maximum for the Meet metric.
This algorithm is used due to its simplicity and the fact that our model has a small number of parameters. Each function evaluation (i.e., Meet measurement) in the optimization procedure requires measuring both efficiency and effectiveness of the current parameter setting as applied to the training set. This can be costly for large training sets and large feature sets. There are several other approaches for directly optimizing non-smooth functions using similar types of hill-climbing methods, such as simultaneous perturbation stochastic approximation [25], which uses fewer function evaluations and could be used to speed up training. Although it is beyond the scope of our current work, we are very interested in exploring how to efficiently learn models with hundreds or even thousands of parameters.
5. EXPERIMENTS
In this section we present our experimental results. We begin by describing our experimental setup and then present a comprehensive evaluation of our proposed method using publicly available datasets.
5.1 Experimental setup
We implemented our learning to efficiently rank framework on top of Ivory, a newly-developed open-source web-

Name Wt10g Gov2 Clue

Number of Docs 1,692,096 25,205,179 50,220,423

TREC Topics 451-550 701-850 1-50

Table 3: Summary of TREC collections and topics used in our experiments.

scale information retrieval engine [14]. Experiments were run on a SunFire X4100, with two Dual Core AMD Opteron Processor 285 at 2.6GHz and 16GB RAM (although all experiments used only a single thread).
To illustrate the benefits of our proposed work across a diverse set of document collections, we used the three TREC web collections shown in Table 3. Wt10g is a small web collection with 1.7 million documents, while Gov2 is a larger 25 million page crawl of the .gov domain. Finally, Clue is the first English segment of ClueWeb09, a recently-released web crawl consisting of 50 million documents.
We used the title portions of TREC topics as queries for these collections. In each case, queries were split sequentially into a training and test set of equal size; results are reported on the test sets. In all cases we ran retrieval on a single, monolithic index (i.e., no document partitioning), returning 1000 hits. Although real-world systems are likely to divide large document collections into smaller, independentlyindexed partitions and coordinate parallel query evaluation via a broker, we decided not to adopt this strategy since it would conflate characteristics of the framework we wish to illustrate with unrelated issues such as latencies associated with network traffic. As a result, our query execution times on the larger collections may not be adequate for interactive retrieval; this, however, does not detract from the generality of our proposed framework.
We compare our proposed model, which we call the efficient sequential dependence model (ESD), to two baseline models. One is the bag-of-words query likelihood model [20] (QL), with Dirichlet smoothing parameter µ = 1000. The other is the less efficient, but more effective sequential dependence model [16] (SD). The SD model is a special case of the WSD and ESD models. The best practice implementation of the SD model uses the same features and functional form as the WSD model, but sets w5t = 0.82, w5b = 0.09. All of the other parameters are set to 0, yielding query independent i weights. The SD model does not prune features, meaning that all features are evaluated for every query.
Effectiveness is measured in terms of mean average precision (MAP), although as previously noted a variety of other effectiveness metrics can be substituted. As for efficiency, we explored several different efficiency functions, and analyzed the resulting impacts on the tradeoff between efficiency and effectiveness (detailed below). When training our model, we directly optimized the Meet metric. A Wilcoxon signed rank test with p < 0.05 was used to determine the statistical significance of differences in the metrics.
5.2 Results
In this section we describe the performance of our model in terms of its ability to optimally balance effectiveness and efficiency. We show the impact of different efficiency functions on the learned models and also present an analysis of the distribution of query times, demonstrating reduced variance. Finally, we show that with specific efficiency functions, our

142

QL SD
ESD

Wt10g (t = 150ms,  = -0.05)

Time(s) MAP

Meet

0.168 21.51

21.75

0.401 22.43*

22.12

(+4.2)

(+1.7)

0.235 24.04*

23.03*

(+11.8/+7.2) (+5.8/+4.1)

"Slow" Decay Rate

Gov2 (t = 5s,  = -0.1)

Time(s) MAP

Meet

1.97 31.94

31.96

7.09 33.57*

32.91

(+5.1)

(+2.9)

6.42 34.74*

33.94*

(+8.7/+3.5) (+6.2/+3.1)

Clue (t = 7s,  = -0.01)

Time(s) MAP 4.09 20.75

Meet 20.55

13.46 21.68

21.41

(+4.5) 11.18 22.34*

(+4.2) 22.14

(+7.7/+3.0) (+7.7/+3.4)

QL SD
ESD

Wt10g (t = 150ms,  = -0.45)

Time(s) MAP

Meet

0.168 21.51

21.03

0.401 22.43*

19.63*

(+4.2)

(-7.1)

0.215 23.42* (+8.9/+4.4)

21.55* (+2.5/+9.8)

"Fast" Decay Rate

Gov2 (t = 5s,  = -0.35)

Time(s) MAP

Meet

1.97 31.94

31.87

7.09 33.57*

31.77

(+5.1)

(-0.3)

5.46 33.65*

32.58

(+5.4/+0.2) (+2.2/+2.5)

Clue (t = 7s,  = -0.1)

Time(s) MAP 4.09 20.75

Meet 20.53

13.46 21.68

21.26

(+4.5)

(+3.5)

8.55 21.24

21.08

(+2.4/-2.0) (+2.7/-0.8)

Table 4: Comparison between models under step + exponential efficiency function (slow decay on top, fast decay on bottom); parameters t (time threshold) and  (decay rate) are shown in the column headings for each collection. Symbol * denotes significant difference with QL;  denotes significant difference with SD. Percentage improvement shown in parentheses: over QL for SD, and over QL/SD for ESD.

learned models converge to either baseline query-likelihood or the weighted sequential dependence model, thus illustrating the generality of our framework in subsuming ranking approaches that only take into account effectiveness.
5.2.1 Tradeoff between effectiveness and efficiency
Table 4 presents results of two sets of experiments using the step + exponential function, with what we subjectively characterize as "slow" decay and "fast" decay. The time threshold t (below which efficiency is one) was chosen to be roughly halfway between the QL and SD running times for each collection. Due to the differences in collection size, it is unlikely that a common decay rate () is appropriate for all collections. Therefore, we manually selected a separate decay rate for each collection. Both t and  are shown in the column headings of Table 4. The fast decay function penalizes low efficiency ranking functions more heavily, thus a highly-efficient ranking function with reasonable effectiveness is preferred over a less efficient function with potentially better effectiveness. With the slow decay function, effectiveness plays a greater role.
For both fast and slow decay, we compared our proposed model (ESD) with the query likelihood model (QL) and the sequential dependence model (SD) in terms of query evaluation time, MAP, and Meet. In both tables, percentage improvements for MAP and Meet are shown in parentheses: over QL for SD, and over QL/SD for ESD. Statistical significance is denoted by special symbols.
As expected, the mean query evaluation time for ESD is greater than that of QL, but less than that of SD for both sets of experiments. Furthermore, the mean query evaluation time for ESD is lower for the fast decay rate than for the slow decay rate, which suggests that our efficiency loss function is behaving as expected. In the learned models, ESD is 41.4%, 9.4%, and 16.9% faster than SD for the slow decay rate on Wt10g, Gov2, and Clue, respectively; ESD is 46.4%, 23.0%, and 36.5% faster than SD for the fast decay rate on the same three collections, respectively. Once

again, this makes sense, since the fast decay rate penalizes inefficient ranking functions more heavily.
In terms of mean average precision, in five out of the six conditions, the ESD model was significantly better than baseline QL. In the one condition in which this was not the case (Clue with fast decay), SD was not significantly better than QL either. While the ESD model is much more efficient than the SD model, it is able to retain the same effectiveness as SD, and in some cases actually performs significantly better (in the case of Wt10g and Gov2 for slow decay rate). We believe that this result demonstrates the ability of our framework to select a more optimal operating point in the space of effectiveness-efficiency tradeoffs than previous approaches.
In terms of Meet, the ESD model outperforms QL and SD, although gains are not consistently significant. Interestingly, we note that SD has a lower Meet score than default QL in two out of three collections when the fast decay rate is used. This suggests that the default formulation of the sequential dependence model trades off a bit too much efficiency for effectiveness, at least based on our metrics.
Lastly, note that setting the time target t in the efficiency function implies that the ranking model will be penalized by an exponential decay in efficiency if its query ranking time exceeds t. This is a soft penalization factor, which contrasts with the more harsh step function where efficiency is assigned a zero value for time exceeding t. An implication of using this soft efficiency loss function is that for a ranking function with time > t, if it is highly effective for user queries, it may still have a reasonable tradeoff value, because essentially, its high effectiveness compensates for the loss in efficiency. This fact is also confirmed by results shown in Table 4, where the average query time of ESD is consistently greater than the time threshold t.
5.2.2 Analysis of query latency distribution
Another benefit of our proposed framework is that learned models exhibit low variance in query execution times. Fig-

143

Count of queries in each time bin Count of queries in each time bin
Count of queries in each time bin

40 30 20 10
0 100 300 500 700 900 1100 1300 1500 Time(ms)
(i)

40 30 20 10
0 100 300 500 700 900 1100 1300 1500 Time(ms)
(ii)

40 30 20 10
0 100 300 500 700 900 1100 1300 1500 Time(ms)
(iii)

Figure 2: Distribution of query execution time for Wt10g queries for (i) query likelihood (QL); (ii) sequential dependence model (SD); (iii) efficient sequential dependence model (ESD).

ure 2 plots histograms of query execution for QL, SD, and ESD on the Wt10g collection. The ESD model was trained using the step + exponential (fast decay) efficiency function. Distributions for the other conditions look similar, and therefore we omit in the interest of space.
We can see that most queries with baseline QL are evaluated in a short amount of time, with a small number of outliers. The sequential dependence model has a heavier tail distribution with increased variance in query execution times: most queries still finish relatively quickly, but a significant minority of the queries take much longer to evaluate. Our ESD model reduces the number of long running queries so that the histogram is less tail heavy, which greatly improves the observed variance of query execution times. This improved behavior is due to the fact that our model considers efficiency as well as effectiveness, hence penalizes longrunning queries, even if they are more effective. Note that although query likelihood has the most desirable query execution profile, it comes at the cost of effectiveness. Experiments in the previous section showed that ESD is at least as effective as SD, but much more efficient. The distribution of query execution times further supports this conclusion.
Why is reduced variance in query execution time important? For real-world search engines, it is important to ensure that a user, on average, gets good results quickly. However, it is equally important to ensure that no user waits too long, since these represent potentially dissatisfied users who never come back. A basic principle in human-computer interactions is that the user should never be surprised, and that system behavior falls in line with user expectations. Reducing the variance of query execution times helps us accomplish this goal.
Furthermore, from a systems engineering point of view, lower variance in query execution time improves load balancing across multiple servers. In real-world systems, high query throughput is achieved by replicated services across which load is distributed. If variance of query execution times is high, simple approaches (e.g., round-robin) can result in uneven loads (consider, for example, that in SD one query can take an order of magnitude longer than another to execute). Therefore, the reduced variance exhibited by our learned models is a desirable property.
5.2.3 Relationships to other retrieval models
Finally, we demonstrate that previous ranking models that consider effectiveness only can be viewed as special cases in our proposed family of ranking functions that account for both effectiveness and efficiency. More specifically, the flexible choices for efficiency functions used in our general

framework can capture a wide range of tradeoff scenarios for different effectiveness/efficiency requirements. For instance, if we care more about efficiency than effectiveness, then we can set the time threshold in the efficiency function to be low, which forces us to learn a ranking function with high efficiency. On the other hand, if the focus is to learn the most effective ranking function possible (disregarding efficiency), then we can use a constant efficiency value. We would expect that in the first case, the learned model would look very similar to baseline query likelihood (efficient but not effective). Correspondingly, we would expect that in the latter case, the learned model would look very similar to the sequential dependence model (effective but not efficient). For particular choices of the efficiency function, the learned models should converge to (i.e., acquire similar parameter settings as) existing models that encode a specific effectiveness/efficiency tradeoff.
Table 5 compares ESD to the SD model with a constant efficiency function. Our model, when trained with constant efficiency values, is equivalent to the WSD model [5]. The ESD model in this case significantly outperforms SD in MAP and Meet scores; the differences are significant in two of the three collections.
Similarly, Table 6 illustrates the relationship of ESD to QL under a step efficiency function with low time targets (t = 100ms is used for Wt10g and t = 3s is used for Clue and Gov2). Step efficiency functions heavily penalize long query execution times, so the model essentially converges to simple bag of words. An interesting observation is that while retaining similar effectiveness as the QL model, the ESD model achieves a better time efficiency than QL due to its joint optimization of effectiveness and efficiency (allowing the model to prune query terms that have little impact on effectiveness, but nevertheless have an efficiency cost).
6. CONCLUSIONS AND FUTURE WORK
In this paper, we presented a principled and unified framework for learning ranking functions that jointly optimize both retrieval effectiveness and efficiency. This new problem was motivated by the fact that although current learning to rank approaches can learn highly effective ranking functions, the important issue of efficiency has been ignored. For realworld search engines, both efficiency and effectiveness are important factors.
We proposed novel metrics to quantify the tradeoff between retrieval effectiveness and efficiency and presented a strategy for automatically learning models that directly optimize the tradeoff metrics. We demonstrated that these metrics can support a full spectrum of effectiveness-efficiency

144

SD ESD

Time 0.401 0.425

Wt10g MAP 22.43 24.11 (+7.5)

Meet 22.44 23.34 (+4.0)

Time 7.09 7.13

Gov2 MAP 33.57 34.35 (+2.3)

Meet 33.27 34.08 (+2.4)

Time 13.46 13.87

Clue MAP 21.68 22.43 (+3.5)

Meet 21.42 22.26 (+3.9)

Table 5: Comparison of SD and ESD under constant efficiency (i.e., only effectiveness is accounted for in the tradeoff metric).

QL ESD

Time 0.168 0.145

Wt10g

MAP Meet

21.51 13.37

21.50 13.50

(­)

(+1.0)

Time 1.97 1.93

Gov2 MAP 31.94 31.63 (-1.0)

Meet 25.82 25.39 (-1.7)

Time 4.08 3.68

Clue MAP 20.75 20.70 (-0.2)

Meet 16.02 16.02 (­)

Table 6: Comparison of QL and ESD under step efficiency functions. A step function with t = 3s is used for Clue and Gov2, and a step function with t = 100ms is used for Wt10g.

tradeoff scenarios. Furthermore, we described a class of linear feature-based models that can be directly optimized for our proposed tradeoff metrics.
Our experimental evaluation showed that a linear ranking function optimized in this manner is capable of balancing between retrieval effectiveness and efficiency, as desired. Our learned ranking functions achieved similar strong effectiveness as a state-of-the-art learning to rank model, but with significantly decreased average query execution times; the variance of query execution times was reduced as well. Therefore, our proposed framework is capable of learning highly effective models that are also efficient, which makes them desirable from a practical point of view.
There are several possible directions for future work. First, it would be interesting to apply our learning to efficiently rank framework to other ranking functions that use different types of features. Second, we would like to study scalable parameter estimation techniques that would allow us to train on ranking functions with hundreds or even thousands of features. Finally, we have only begun to scratch the surface in various efficiency functions that model different tradeoff scenarios. Due to limited space, we were only able to show a few possibilities under a small set of parameter settings. A more thorough exploration of the parameter space would help us better understand how to cater efficiency functions to real-world situations.
7. ACKNOWLEDGMENTS
This work was supported in part by the NSF under awards IIS-0836560 and IIS-0916043; by DARPA contract HR001106-02-001 (GALE). Any opinions, findings, conclusions, or recommendations expressed are the authors' and do not necessarily reflect those of the sponsors. The second author is grateful to Esther and Kiri for their loving support.
8. REFERENCES
[1] V. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. SIGIR, p. 372­379, 2006.
[2] R. Baeza-Yates, A. Gionis, F. Junqueira, V. Murdock, V. Plachouras, and F. Silvestri. The impact of caching on search engines. SIGIR, p. 183­190, 2007.
[3] J. Bai, Y. Chang, H. Cui, Z. Zheng, G. Sun, and X. Li. Investigation of partial query proximity in web search. WWW, p. 1183­1184, 2008.

[4] M. Bendersky, W. Croft, and D. Smith. Two-stage query segmentation for information retrieval. SIGIR, p. 810­811, 2009.
[5] M. Bendersky, D. Metzler, and W. Croft. Learning concept importance using a weighted dependence model. WSDM, p. 31­40, 2010.
[6] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. ICML, p. 89­96, 2005.
[7] S. Buttcher and C. Clarke. Efficiency vs. effectiveness in terabyte-scale information retrieval. TREC, 2005.
[8] S. Buttcher, C. Clarke, and P. Yeung. Indexing pruning and result reranking: Effects on ad-hoc retrieval and named page finding. TREC, 2006.
[9] S. Bu¨ttcher, C. Clarke, and B. Lushman. Term proximity scoring for ad-hoc retrieval on very large text collections. SIGIR, p. 621­622, 2006.
[10] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. Maarek, and A. Soffer. Static indexing pruning for information retrieval systems. SIGIR, p. 43­50, 2001.
[11] K. Collins-Thompson and J. Callan. Query expansion using random walk models. CIKM, p. 704­711, 2005.
[12] F. Gey. Inferring probability of relevance using the method of logistic regression. SIGIR, p. 222­231, 1994.
[13] M. Lease. An improved Markov Random Field model for supporting verbose queries. SIGIR, p. 476­483, 2009.
[14] J. Lin, D. Metzler, T. Elsayed, and L. Wang. Of Ivory and Smurfs: Loxodontan MapReduce experiments for web search. TREC, 2009.
[15] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225­331, 2009.
[16] D. Metzler and W. Croft. A Markov Random Field model for term dependencies. SIGIR, p. 472­479, 2005.
[17] D. Metzler and W. Croft. Linear feature-based models for information retrieval. Information Retrieval, 10(3):257­274, 2007.
[18] R. Nallapati. Discriminative models for information retrieval. SIGIR, p. 64­71, 2004.
[19] A. Ntoulas and J. Cho. Pruning policies for two-tiered inverted index with correctness guarantee. SIGIR, p. 191­198, 2007.
[20] J. Ponte and W. Croft. A language modeling approach to information retrieval. SIGIR, p. 275­281, 1998.
[21] S. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. TREC, p. 109­126, 1994.
[22] T. Strohman, H. Turtle, and W. Croft. Optimization strategies for complex queries. SIGIR, p. 219­225, 2005.
[23] T. Tao and C. Zhai. An exploration of proximity measures in information retrieval. SIGIR, p. 295­302, 2007.
[24] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc. Ser. B, 58(1):267­288, 1996.
[25] Y. Yue and C. Burges. On using simultaneous perturbation stochastic approximation for IR measures, and the empirical optimality of LambdaRank. NIPS Machine Learning for Web Search Workshop, 2007.

145

Learning to Rank Only Using Training Data from Related Domain


Wei Gao1, Peng Cai2 , Kam-Fai Wong1, and Aoying Zhou2
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong, China
{wgao, kfwong}@se.cuhk.edu.hk
2East China Normal University, Shanghai, China
pengcai2010@gmail.com, ayzhou@sei.ecnu.edu.cn

ABSTRACT
Like traditional supervised and semi-supervised algorithms, learning to rank for information retrieval requires document annotations provided by domain experts. It is costly to annotate training data for different search domains and tasks. We propose to exploit training data annotated for a related domain to learn to rank retrieved documents in the target domain, in which no labeled data is available. We present a simple yet effective approach based on instanceweighting scheme. Our method first estimates the importance of each related-domain document relative to the target domain. Then heuristics are studied to transform the importance of individual documents to the pairwise weights of document pairs, which can be directly incorporated into the popular ranking algorithms. Due to importance weighting, ranking model trained on related domain is highly adaptable to the data of target domain. Ranking adaptation experiments on LETOR3.0 dataset [27] demonstrate that with a fair amount of related-domain training data, our method significantly outperforms the baseline without weighting, and most of time is not significantly worse than an "ideal" model directly trained on target domain.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Storage and Retrieval ­ Retrieval models
General Terms
Algorithms, Experimentation
Keywords
instance weighting, related domain, learning to rank, domain adaptation, RankSVM, RankNet
This work was done when the author was visiting the Chinese University of Hong Kong
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1. INTRODUCTION
Ranking in information retrieval (IR) aims to order retrieved documents according to their relevance to a given query. In recent years, learning-based ranking algorithms, known as learning to rank, were extensively studied in IR related communities [5, 6, 15, 18, 35]. Compared to the traditional approaches, such as vector space model [30], BM25 [29] and language-modeling method [26, 37], learning to rank aims to optimize a ranking function that incorporates a wide variety of relevance features and avoid tuning a large number of parameters empirically.
Like supervised algorithms, learning to rank requires large training sets annotated for the specific domain where search tasks are performed, for which much effort should be made by domain experts. For example, building a medical search engine needs experts and labeling standard different from what a musical search engine does; in TREC's Web track [11], named page finding and topic distillation are different tasks, and separate sets of queries and relevance judgements have to be prepared. It is prohibitive to annotate documents for each search domain, especially when fast deployment of a ranking model is demanded.
A promising direction for solving this predicament is to transfer ranking knowledge from the training data of a related domain to the target domain, where only a few or no labeled data is available1. Since common information may exist between two domains, we can expect to reuse the training data to derive a ranking model for the target domain considering this kind of commonality. However, due to the different joint distributions of feature and relevance, ranking model directly trained on the related domain generally cannot perform well when tested in target domain.
Cross-domain adaptation has been well studied for classification in nature language processing [4, 12, 21] and machine learning [2, 3, 13]. Although the similar intuition can be applied in ranking adaptation, the fundamental distinctions between ranking and classification need to be considered in algorithm design. In ranking, the main concern is the preference order of two documents or the full order of a list of documents. It is difficult to directly apply classifier adaptation for ranking [14, 17].
Recently, ranking adaptation was received more and more attention [7, 8, 10, 16, 17]. Existing approaches assume a small amount of training data in target domain, which plays an important role in transferring ranking knowledge across domains. However, a small set of labeled data of target domain can only convey limited rank-
1Throughout this paper, target domain refers to the domain where the search and ranking is performed, and related domain refers to a domain where large training sets are available for learning a ranking model. Related domain was also called source domain in the literature.

162

ing knowledge, and other useful information not contained in them can be important but is ignored.
In this paper, we assume a more general scenario where no labeled data but a large number of unlabeled data are available with target domain. We propose to learn a target ranking model by only using related-domain training data that are weighted appropriately with the aid of unlabeled data in target domain. A simple yet effective instance-weighting framework is created that consists of two steps: (1) we estimate the importance of each relateddomain document to the target domain; (2) heuristic schemes are explored to transform the document importance to the weights of document pairs that can be directly incorporated into the popular pairwise ranking algorithms. Due to importance weighting, the ranking function trained on the related-domain data can be highly adaptable to the data in target domain.
The remainder of the paper is organized as follows: Related work are given in Section 2; Section 3 reviews general ranking framework; Section 4 analyzes the problem in ranking across domains; Section 5 presents our importance weighting on documents for ranking adaptation; Experiments and results are discussed in Section 6; Finally, we conclude in Section 7.
2. RELATED WORK
Learning to rank is to optimize a ranking function given data consisting of queries, the retrieved documents and their relevance judgements. Given a new query, the learned function is used to predict the order of retrieved documents. Based on input spaces, three categories of approaches have been proposed, namely pointwise, pairwise and listwise approach. Probabilistic classification and metric regression are typically used in pointwise approach. Popular ranking models like Ranking SVM [18], RankBoost [15], RankNet [5], etc., aim to optimize pairwise loss based on order preference and classify the relevance order between two documents, thus falling into the pairwise approach. Listwise approach [6, 35] considers the entire group of documents associated with the same query in the input space. A comprehensive survey on learning to rank is given in [23].
Domain adaptation was originally to address the classification problems where training data and test data do not follow the same distribution. Existing approaches can be grouped into two directions, namely instance-based and feature-based approaches. The former assumes the same feature space between two domains and attempts to estimate the weights of individual examples in the related domain, which measures their importance to the target domain [19, 21, 36]. The latter tries to infer some common feature representation to bridge the gap between data distributions of two domains, which is regarded as feature weighting scheme [2, 4, 12, 24]. Our method falls into instance-weighting approach.
Existing ranking adaption methods assume a large set of training data in related domain and a small amount of labeled data in target domain. A tree-based adaptation algorithm was proposed by [10]. Gradient Boosting Tree (GBT) was first used to train a ranking model on the related domain. The tree structure was then adjusted with the labeled data in target domain. TransRank [8] proposed to extract k-best adaptive queries in related domain used for training with the help of labeled data in target domain. Based on the same assumption of available target-domain labels, [7] presented methods for feature-level as well as instance-level adaptation. [17] treated the model parameters learned from related domain as prior knowledge, and the similarity of parameters between two domains is considered when training on target domain. [16] used model interpolation and error-driven approaches for ranking adaptation, where ranking model was trained on target domain and then com-

bined with a background model learned on the related domain. [34] studied ranking adaptation in heterogeneous domains. [9, 17] proposed ranking adaptability and domain similarity measures. Different from these methods, no labeled data is assumed available in the target domain for our study.
Ranking model training with test data can be thought of as a semi-supervised learning that treats separate domains like the same domain [14, 22]. In these methods, training must be done online once new test data come in, and this is inefficient for web search ranking. In contrast, our training is off-line since test data are not seen at training time.

3. RANKING MODEL REVIEW
Typical training data for learning a ranking model is a set of input-output pairs (x, y), where x is a feature vector containing relevance scores of a query-document pair, and y is the rank label for the document (e.g., relevance or irrelevant). The features commonly include query-dependent relevance measures such as term frequency and BM25, and query-independent measures such as PageRank. The objective is to optimize a scoring function f (x) that maps x to a real value that indicates the ranking score of the document given the query. If f (xi) > f (xj) for documents di and dj, then di should be ranked higher than dj, denoted as di dj. Our method is based on pairwise approach, although it can be easily generalized to other approaches.
Pairwise algorithms, such as RankSVM [18] and RankNet [5], aim to minimize a loss based on the preference order of each pair of documents. RankSVM minimizes the number of discordant pairs and maximizes the margin of pair, which is equal to minimize the L2 norm of the model f 's hyperplane parameter and a Hinge Loss on pairs:

min ||f ||2 + f

1 - zij  f (xi - yj ) +

(1)

i,j=1

where zij =

+1, if di -1, if dj

dj ; di

is the binary preference de-

pending on the ground truth of two documents, (.)+ is the pairwise

hinge loss function,  is the coefficient of trade-off between model

complexity and loss term, and is the number of documents of the

query.

At high level, RankNet differs from RankSVM in that it mini-

mizes a loss function based on cross entropy that models the poste-

rior probabilities of rank order [5], which has the following form:

min ||f ||2 +

L(Pij , P¯ij )

(2)

f

i,j=1

L(Pij , P¯ij )  -P¯ij log Pij - (1 - P¯ij ) log(1 - Pij) is the cross entropy loss of a pair (di, dj ), where Pij is the posterior P (di dj), and P¯ij is the desired target values for the posteriors and takes one of three values {0, 0.5, 1} depending on the ground truth which

includes the ties (documents with the same labels). Pij is mapped

from outputs

of f

using a logistic function:

Pij



eoij 1+eoij

,

where

oij = f (xi) - f (xj ).

As compared to RankSVM, RankNet usually does not consider

kernel and its loss function is pairwise differentiable. Thus, gradientbased optimizer can be used. Also, RankNet is modeled as a twolayer neural network, where input and hidden nodes correspond to features and their combinations. In theory, it can capture any

nonlinear correlations among features. RankBoost [15] is another

typical pairwise algorithm, for which we do not give the details, as

it is similar for our method to apply (see Section 5).

163

1

1

0.9

0.9

0.8

0.8

0.7

0.7

RankingSVM for NP

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

Ranking SVM for TD

0

0

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

(a) Named page finding

(b) Topic distillation

Figure 1: Ranking directions of RankSVM on name page (NP) and topic distillation (TD) queries in TREC-2003.

1

0.9

0.8

0.7 RankingSVM for NP
0.6

0.5

d4

0.4

d2

0.3

d3

d1 0.2

0.1 0 0

Ranking SVM for TD

0.2

0.4

0.6

0.8

1

Figure 2: Ranking errors are made for TD query #18 when applying NP model to TD task.

4. PROBLEM ANALYSIS
Document distributions, i.e., the joint distribution of features and relevance of documents, are commonly different between domains. This is caused by many reasons such as different languages, countries and topics [10]. Therefore, ranking model directly trained on one domain may not perform well on the other. Figure 1 and 2 illustrate this predicament of cross-domain ranking using RankSVM on two of TREC's 2003 Web track tasks, namely named page finding (NP) and topic distillation (TD) [11] (Section 6 gives details of the tasks). To plot the figures, we conducted principle component analysis (PCA) on this TREC collection that was released through LETOR3.0 [27] for learning to rank study2. Widely used in multivariate analysis, PCA computes a linear combination of features to extract patterns and reduce the dimensionality of data. The goal is to find the direction, known as the principal axis, along which the most variance of the patterns in the data set is captured. The projection of a data point on the principal axis is called the principal component.
Let the horizontal axis and vertical axis represent the first and second principle axes, respectively, and let the blue circles (and red stars) denote relevant documents and the gray squares denote irrel-
2http://research.microsoft.com/en-us/um/ beijing/projects/letor/

evant ones. The arrowed line in each figure is the ranking direction of RankSVM learned for the corresponding task. As shown in Figure 1(a) and 1(b), documents for NP and TD queries follow different distributions in this two-dimensional PCA space even though they share the same original feature space. Their ranking directions are clearly divergent, which means that when a model trained on one domain is blindly applied to the other domain (moving the two arrowed lines into the same space), projecting documents onto the ranking directions may result in very different orders as compared to the ground truth. This problem is revealed more clearly by Figure 2, where the four documents of TD query #18 are ranked as d4 d2 d1 d3 using the TD model (which is correct as the relevant document d4 should be placed ahead of others), whereas NP model gives the incorrect order d1 d2 d3 d4. Therefore, the learned NP model can hardly be applied to TD domain directly, and vice versa.

5. RANKING ADAPTATION BY WEIGHTING IMPORTANCE OF DOCUMENTS
An intuition of ranking adaptation is to tune the ranking direction of related domain to that of target domain as closely as possible. In practice, however, since the data of target domain are not labeled, the target ranking direction cannot be obtained. Our idea is to make use of unlabeled data of target domain for weighting training instances in the related domain so that the knowledge of relevance judgement can be strengthened on those documents that are similar to the target domain. As a result, the learning can be focused on giving better ranking for these important documents. It can be expected that the model trained in this way will work well on ranking actual target-domain documents. To this end, it is critical to measure the extent to which the instances are similar across two domains. The similarity indicates the importance of an instance that can be related to the target domain.
5.1 Cross-Domain Adaptive Ranking Model
Suppose the importance of instances is appropriately weighted in the related domain, we can extend the ranking algorithms described in Section 3 by incorporating the weights into their loss functions. The rationale is that the loss on important instances should be penalized more heavily once ranking errors are made on these documents. Weighting can be developed corresponding to the levels of instance. Pairwise models take document pairs as instances. Thus, the weighting should be made compatible with the pairwise loss.
Let IW (xi, xj) be pairwise importance weight for (di, dj), the objective function can be extended for adaptation as follows:

min ||f ||2 +

IW (xi, xj)  Lij (.)

(3)

f

i,j=1

where Lij (.) is the loss function on pairs that takes the forms like Equations 1 and 2. Note that this scheme can be similarly applied to other pairwise models such as RankBoost. Overall, adaptation training consists of two key steps: (1) instance weighting; (2) weight-based model training.
However, it is difficult to estimate the importance of pairs in related domain without labeled data in target domain. This is because document pairs are constructed from preference orders based on ground truth. When rank labels are not provided, one has to enumerate all possible pairs. This may produce tremendous noises rendering weight estimation unreliable as most of the pairs are not meaningful for ranking. Weighting individual documents is intuitively more straightforward. Then we can try to derive pairwise weights from document weights.

164

Classification hyperplane
D
r
D
t

Figure 3: A classification hyperplane can be built between Dr and Dt to measure the extent, to which query-document instance in the related domain is similar to the target domain.

5.2 Importance Weighting Schemes
5.2.1 Weighting Individual Documents
Despite that target-domain data are not labeled, the relevance features of documents for the ranking purpose are readily available to us. These features may not only reflect document's relevancy, but also encode the correlation or similarity of documents in different domains, which conceals some important cross-domain commonalities. For example, many query-independent features that are not focused on query-document relevancy, such as document length, link structure, anchor text, etc., may express some important intrinsic properties of documents. Their discriminative nature is expected to correlate the similar documents across domains and differentiate those that are not similar.
We model document weighting as a classification problem based on the relevance features and the source of domains the document come from. Figure 3 explains this intuition. Given document sets Dr and Dt that denote respectively the related and target domain, we build a classification hyperplane Hrt to separate Dr from Dt. If a Dr example is similar to a large number of Dt examples, it should be close to Hrt. Hence, we can measure the importance of a related-domain document using its distance to the hyperplane. In practice, the probability that a document in the related domain may be classified into the target domain is adopted.

Algorithm 1 Weight estimation for training documents in related domain

Input:

RTaerlgaetet-dd-odmomaianindodcoucmumenetnst,sD, Dt r=={x{ixti}rni}tm i=r1=;1; Output:

1: 2:

Importance weights of related-domain documents, IW

lr lt

= =

+1, -1,

Dr Dt

= =

{{((xxiitr,,lltr))}}nitm ir==11;

;

= {wir }m ir =1;

3: Find classification hyperplane Hrt which separates Dr from Dt;

4: for i = 1; i  m; i + + do

5: Calculate (xir ), the distance of xir to Hrt;

6:

P (xir



Dt|qr) =

1+exp(

1 (xir

)+)

;

7: Add wir|qr = P (xir  Dt|qr) to IW ;

8: end for

9: return IW ;

Algorithm 1 estimates the probability that each xir can be classified into Dt, denoted as P (xir  Dt|qr), indicating the importance of xir given a related-domain query qr. In step 1-2, the algorithm constructs a training corpus using ±1, the source of domain,
as class label. Then it solves the hyperplane that separates the two

domains. For each related-domain document, its distance to the hyperplane is calculated in step 6. A sigmoid function is employed to transform the distance into a posterior probability [25], for which free parameters  and  can be determined by cross-validation or hold-out method. Note that the probability is conditional as each document is in the search result of the given query.

5.2.2 Weighting Document Pairs
Document weights obtained above can be easily integrated into
pointwise ranking algorithms. But they cannot be used directly in pairwise algorithms that are generally more effective. Here we address how to transform document weighting to pairwise weighting.
The weight of a pair of documents in related domain can be approximated using the marginal probability of each document in that pair. For a document pair xir , xjr of a query qr from related domain, we can reasonably assume that P (xir  Dt|qr) is independent of P (xjr  Dt|qr). Thus, the conditional joint probability P ( xir , xjr  Dt|qr) can be calculated straightforwardly based on this independence assumption:

wir,jr |qr = P ( xir , xjr  Dt|qr )

(4)

= P (xir  Dt|qr) · P (xjr  Dt|qr)

where wir,jr|qr denotes the pairwise weight given qr and can be directly applied to substitute the importance weight in Equation 3.

5.2.3 Weighting Queries

Document weighting and pairwise weighting do not explicitly measure the importance of individual queries. Ranking algorithms constrain the instances to be learned in the range of queries. It is thus reasonable to take into account how likely each query is related to the target domain. If a query is not similar to the type of queries across domains, its retrieved documents should be weighted with lower values accordingly. This aims to generate a model that favors important queries.
We estimate query importance based on the weighting of document pairs considering the compatibility with the pairwise loss. Assuming that the occurrence of document pairs are independent events given a query, query importance can be estimated as the mean value of the importance probabilities of all available pairs, which is formulated as follows:

wqr = P (qr  Dt) =

ir ,jr P ( xir , xjr  Dt|qr ) M

(5)

where M denotes the number of document pairs in query qr that can be derived from the ground truth.
When applied to pairwise ranking algorithms, Equation 5 can simply substitute the importance weight in Equation 3.

5.2.4 Weight Combination

If we regard ranking across the related and target domains as a two-step procedure, Equations 4 and 5 can be combined in a principled way so that different weighting schemes naturally become complementary to each other since they reflect the degree of crossdomain similarity from different perspectives. The first step is to select an important query that is similar to the target domain, and with this condition, the second step is to choose a pair of important documents from the search result of this query. This process is modeled as follows by the probability P ( qr, xir , xjr  Dt):

wqr ,ir ,jr = P ( qr, xir , xjr  Dt)

(6)

= P (qr  Dt) · P ( xir , xjr  Dt|qr )

The advantage of the combined weighting scheme above lies in the scaling effect of query weight that influences pairwise weight

165

Algorithm 2 Weight-based adaptive ranking model training

Input: RTaerlgaetet-dd-odmomaianindodcoucmumenetnst,sD, Dt r=={x{ixti}rni}tm i=r1=;1; Weighting scheme K  {pair, query, comb}

Output:

Ranking model f ;

1: 2:

Dr Dt

= =

{{((xxiitr,,-+11))}}nim itr==11;;

3: Find classification hyperplane Hrt which separates Dr from Dt;

4: Split Dr into Dr1 and Dr2;

5: Calculate IWrK1 , IWrK2 according to K;

6: Training model on Dr1 with weight IWrK1 ;

7: Select model f which makes minimum weighted loss on Dr2 with

IWrK2 ;

8: return f ;

in two aspects: (1) if many document pairs of query qr have high weight values, then P (qr  Dt) should be also high. The scaling tends to strengthen the importance of xir , xjr ; (2) if many pairs are weighted low and only a few pairs high, the scaling can lower down the importance of these a few pairs that may be outliers.
5.3 The Framework
Algorithm 2 summarizes our overall framework, which contains two key components, i.e., instance weighting and weight-based model training.
Instance weighting is done by steps 1-5. Steps 6 and 7 perform weight-based model training and selection. Note that Dr is split into two parts in step 4, where Dr1 is used for weight-based training and Dr2 is for weight-based model selection (validation). With no labeled data in target domain, models can be validated only using labeled data in related domain. In step 7, we select the model f that produces minimum weighted loss on Dr2, where the loss is weighted with IWrK2. Including weights makes model selection more meaningful because the best performance on Dr2 without considering the weights may not generalize well to the target domain. Weight-based validation can select the best model biased towards those highly weighted instances, and thus can adapt better to the target domain.
We use the data in target domain, which are unseen during training, to evaluate this framework. Thus, our method falls into inductive learning [32]. This is an important difference of our method from transductive and semi-supervised adaptation [1, 14] where test data can can be seen during training.
6. EXPERIMENTS AND RESULTS
6.1 Dataset and Setup
We evaluated the proposed framework on TREC-2003 and 2004 Web track datasets, which were released through LETOR3.0 benchmark collection for learning to rank research [27]. Three query tasks were defined on the collection, namely topic distillation (TD), home page finding (HP) and named page finding (NP). TD is to find a list of entry points for good websites that can guide user to page collection which cover the topic. HP is to find the home page of entity such as an person or organization. NP focuses on the page which directly contains the short description of query but not entry point to website [33]. HP and NP tasks are more similar to each other, and TD is distinct from the other two. Ranking adaptation takes place when a task is performed (tested) using the models trained on another task.
There are 64 features for describing a query-document instance

Table 1: The number of queries in TREC-2003 and TREC-2004

Web track

Query task

2003 2004

Topic distillation

50 75

Home page finding 150 75

Named page finding 150 75

(see [27]). The number of queries for different tasks are given in Table 1. Each task has three data sets, i.e., for training, validation and testing, each of which was partitioned into five folds. Since no training data in target domain is assumed in our case, we reused the date sets in the following way while maintaining the fold number unchanged:
1. All data (training, validation and test sets) in related domain and partial data (training and validation sets) in target domain were used to build classification hyperplane for instance weighting;
2. Related-domain data were grouped into two parts, where one contained only training set for ranking model training, and the other contained validation and test sets for model selection. Note that model selection can only use labeled data from related domain;
3. The test sets in target domains were always kept unseen during training and only used for model testing. The training and validation sets of target domains were not involved.
Ranking model trained on related domain without instance weighting acted as baseline, denoted as no-weight. Three weight-based models, namely pair-weight, query-weight and comb-weight, corresponded to pairwise weighting, query weighting and their combination, respectively. In addition, we randomized document weights and used Equation 4 to generate a random weighting method, denoted as rand-weight, to testify the solidity of our probabilistic weighting as compared to the groundless random. Finally, a targetonly model was trained and tested directly on target domain to provide the corresponding "ideal" performance for reference.
We implemented RankSVM and RankNet using the fast Stochastic Gradient Descent (SGD) optimizer3 [31]. The ranking results were evaluated by averaging the performance over the five folds with cross validation.
6.2 Results
We carried out HP to NP and NP to TD ranking adaptation tasks. Typically, HP and NP are considered similar domains whereas NP and TD are largely different. Other adaptations such as NP to HP and HP to TD are duplicates of above two and omitted here. Performance was measured by Mean Average Precision (MAP) [28] and Normalized Discounted Cumulative Gain (NDCG) [20] at positions 1,3,5 and 10. Due to space limit, NDCG is only given for RankSVM and similar trend of NDCG was achieved by RankNet.
6.2.1 HP to NP Adaptation in the Same Years
As shown in Table 2 and Figure 4, all our three weighting methods outperform the baseline (no-weight) based on MAP and NDCG, and comb-weight performs best among three weighting methods. T-test on MAP in Table 2 indicates that all improvements over noweight are statistically significant (p < 0.03). This is because the
3Source codes will be provided upon request. Due to SGD, our RankSVM may be suboptimal and perform a little worse than SVMLight (http://svmlight.joachims.org).

166

Table 2: MAP results of HP to NP adaptation. , ,

and boldface indicate significantly better than no-weight, pair-

weight and comb-weight and rand-weight, respectively (confi-

dence level=95%).

RankSVM

RankNet

model

2003 2004 2003 2004

no-weight pair-weight query-weight comb-weight rand-weight target-only

0.417 0.502 0.510 0.542
0.431
0.660

0.596 0.647 0.647 0.665
0.618
0.681

0.497 0.532 0.534 0.562
0.471
0.672

0.610 0.661
0.645 0.679
0.619
0.669

Table 3: MAP results of NP to TD adaptation. , ,

and boldface indicate significantly better than no-weight, pair-

weight and comb-weight and rand-weight, respectively (confi-

dence level=95%).

RankSVM

RankNet

model

2003 2004 2003 2004

no-weight pair-weight query-weight comb-weight rand-weight target-only

0.146 0.194 0.194 0.222
0.145
0.235

0.176 0.171 0.175 0.179 0.163 0.205

0.196 0.233 0.226 0.235
0.199
0.266

0.164 0.151 0.157 0.158 0.168 0.180

data distributions of two domains are similar, and our method can easily capture cross-domain similarity by weighting the importance of related-domain documents. Furthermore, comb-weight is significantly better than both pair-weight and query-weight on the 2003 data. On the 2004 data, although not significantly, comb-weight still outperforms pair-weight 2.78% by RankSVM and 2.72% by RankNet. Thus, weight combination is generally better than than the separate weighting of queries and document pairs.
No obvious difference can be observed between pair-weight and query-weight. The reason is that only one document is labeled as relevant for each home page query. Thus all the generated pairs contain this document, rendering the values of pairwise weight and query weight close to each other, as the query weight is simply calculated as the mean weight over all document pairs of the query.
All our probabilistic weighting methods can always outperform random-weight significantly. This is due to the solid probability ground of our method. Interestingly, random-weight sometimes can outperform no-weight. The reason is that no-weight can be considered as the extreme case where weights are equal to 1, which assumes no difference across domains and all the related-domain documents equally important to the target domain. This is however too strong to be true. Although not stable, random-weight could be better by randomly taking into account some extent of uncertainty.
6.2.2 NP to TD Adaptation in the Same Years
Topic distillation is fairly different from named page finding not only on task definition but also on data distribution shown as Figure 1 (see Section 4). Here we examine whether ranking adaptation can be done for such two domains that are not similar. NDCG results of RankSVM are presented in Figure 5. MAP results are given in Table 3 for both algorithms.
The lower MAP scores in Table 3 implies that NP to TD adaptation is generally a harder task as compared to HP to NP. There may be less similar features, but because of this the found clas-

Table 4: MAP results of 2003 NP to 2004 TD adaptation. , ,

and boldface indicate significantly better than no-weight, pair-

weight and comb-weight and rand-weight, respectively (confi-

dence level=95%).

model

RankSVM RankNet

no-weight pair-weight query-weight comb-weight rand-weight target-only

0.145 0.164 0.163 0.166
0.145
0.205

0.136 0.160 0.157 0.164
0.144
0.180

Table 5: For NP to TD adaptation, the number of NP training documents in different weight ranges.
weight range 04 NP04 TD 03 NP04 TD

0.5­1 0.1­0.5 0­0.1

35,773 73,968 111,761

47,851 (+33.8%) 237,223 (+220.7%) 160,897 (+44%)

sification hyperplane tends to be more discriminative between the two domains. Therefore, we can still expect effective importance weighting.
On the 2003 data, weighting methods demonstrate consistent effectiveness as achieved in HP to NP adaptation. T-test on MAP indicates that pair-weight and query-weight significantly outperform no-weight (p < 0.02). Also, comb-weight significantly outperforms pair-weight as well as query-weight (p < 0.04). This implies that our adaptation method can work well for these two domains that are not similar.
On the 2004 data, however, weighting is not consistently better than no-weight and sometimes performs even worse. T-test does not indicate any statistical significance either. We analyzed the reason: Compared to TREC-2003 where 150 named page queries are provided, the number of named page queries in TREC-2004 is only 75 (see Table 1), rendering less amount of training data. It turns out that important documents become even less. Hence the model overfits to this small number of important instances, thus cannot adapt (generalize) well. We believe it can be improved with more named page queries provided, which is proved in the next section.
6.2.3 NP 2003 to TD 2004 Adaptation
Here we examine the effectiveness across both different tasks and years. Table 4 shows that weight-based models trained with 2003 NP data significantly outperform no-weight when tested on 2004 TD task. This is because a larger number of important documents can be found in NP 2003 which provides 150 name page queries. Table 5 shows statistics on the number of training documents in different ranges of weights we obtained. For example, there are 33.8% more NP 2003 documents than NP 2004 for the importance weight range greater than 0.5 (highly important to target domain). This implies that our weight-based methods favor a fair amount of training data in the related domain for finding enough important documents to avoid overfitting.
We also studied the influence of weight-based model selection (see Algorithm 2) as compared to the selection without weights. As shown in Table 6, for no-weight selection, weight-based model training can still outperform the baseline, but is consistently worse than using the weight-based selection. This proves that weightbased selection is helpful for adaptation.
We note that RankSVM performs better than RankNet on NP to

167

1

no-weight

pair-weight

0.9

query-weight

comb-weight 0.8
random-weight

0.7

target-only

0.6

0.5

0.4

0.3

0.2

0.1

0 NDCG@1

NDCG@3

NDCG@5

(a) HP to NP on TREC-2003

NDCG@10

1

no-weight

pair-weight

0.9

query-weight

comb-weight 0.8
random-weight

0.7

target-only

0.6

0.5

0.4

0.3

0.2

0.1

0 NDCG@1

NDCG@3

NDCG@5

(b) HP to NP on TREC-2004

NDCG@10

Figure 4: NDCG of adaptation from home page (HP) finding to named page (NP) finding by RankSVM.

0.5

no-weight

pair-weight

0.45

query-weight

comb-weight

0.4

random-weight

0.35

target-only

0.3

0.25

0.2

0.15

0.1

0.05

0 NDCG@1

NDCG@3

NDCG@5

(a) NP to TD on TREC-2003

NDCG@10

0.5

no-weight

pair-weight

0.45

query-weight

comb-weight

0.4

random-weight

0.35

target-only

0.3

0.25

0.2

0.15

0.1

0.05

0 NDCG@1

NDCG@3

NDCG@5

(b) NP to TD on TREC-2004

NDCG@10

Figure 5: NDCG of adaptation from named page (NP) to topic distillation (TD) by RankSVM.

Table 6: Comparison of using and not using weight-based

model selection (w. vs n./w.) in 2003 NP to 2004 TD adapta-

tion.

RankSVM

RankNet

model

w. n./w. w. n./w.

pair-weight query-weight comb-weight rand-weight

0.164 0.163 0.166 0.145

0.160 0.161 0.164 0.128

0.160 0.157 0.164 0.144

0.157 0.150 0.161 0.130

TD within 2004 and 2003 NP to 2004 TD, but relatively worse on other tasks. This seems to be related to the hyperplane generated by margin-based domain classifier. In easier adaptation, the weights may be less accurate since cross-domain data are more similar and may be not linearly separable. Nonlinear RankNet may generalize better in this case. In difficult adaptation where data are more separable, the weights may be more accurate for RankSVM to take the advantage of maximum margin. We will leave the influences of different weight generation methods on different ranking algorithms for future study.
7. CONCLUSION AND FUTURE WORK
We introduced a simple yet effective instance-weighting framework for ranking adaptation in IR by only using training data from related domain. The basic idea is that the related-domain train-

ing instances can be weighted appropriately according to their importance (or similarity) to the target domain. We first estimated the probability of each related-domain document that can be classified into the target domain. Then three instance-weighting methods were proposed based on the document importance. After that, the ranking model was learned in such a way to minimize the weighted loss of ranking instances, so that the ranking function is biased towards those important document and thus can be adaptable to the target domain. Our approach was evaluated on LETOR3.0 dataset for ranking adaptation between different tasks, and we found that it significantly outperformed the baseline without weighting, and most of time was not significantly worse than the model directly trained on the target domain.
Although based on pairwise approach, our method can be generalized easily and combined with other learning to rank approaches. In the future, we will improve the framework from two directions: one is to investigate how different importance estimation methods can influence ranking algorithms; the other is to apply our weighting schemes to various ranking algorithms for ranking adaptation.
8. ACKNOWLEDGEMENT
This work is partially supported by the Innovation and Technology Fund of Hong Kong SAR (No. ITS/182/08) and National 863 program (No. 2009AA01Z150). In addition, P. Cai and A. Zhou are partially supported by an NSFC grant (No. 60925008) and 973 program (No. 2010CB731402). We would like to thank anonymous reviewers for their helpful comments.

168

9. REFERENCES
[1] M. R. Amini, T. V. Truong, and C. Goutte. A Boosting algorithm for learning bipartite ranking functions with partially labeled data. In Proceedings of SIGIR, pages 99­106, 2006.
[2] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Proceedings of NIPS, pages 41­48, 2006.
[3] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from different domains. Machine Learning, 79(1-2):151­175, 2010.
[4] J. Blitzer, R. Mcdonald, and F. Pereira. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, 2006.
[5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd International Conference on Machine Learning, pages 89­96, 2005.
[6] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. In Proceedings of the 24th International Conference on Machine Learning, pages 129­136, 2007.
[7] D. Chen, Y. Xiong, J. Yan, G.-R. Xue, G. Wang, and Z. Chen. Knowledge transfer for cross domain learning to rank. Information Retrieval, 2009.
[8] D. Chen, J. Yan, G. Wang, Y. Xiong, W. Fan, and Z. Chen. TransRank: A novel algorithm for transfer of rank learning. In IEEE International Conference on Data Mining Workshops, pages 106­115, 2008.
[9] K. Chen, J. Bai, S. Reddy, and B. L. Tseng. On domain similarity and effectiveness of adapting-to-rank. In Proceeding of the 18th ACM Conference on Information and Knowledge Management, pages 1601­1604, 2009.
[10] K. Chen, R. Lu, C. Wong, G. Sun, L. Heck, and B. Tseng. Trada: Tree based ranking function adaptation. In Proceeding of the 17th ACM Conference on Information and Knowledge Management, pages 1143­1152, 2008.
[11] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu. Overview of the TREC 2003 web track. In Proceedings of TREC-2003, pages 78­92. NIST, 2003.
[12] H. Daumé III. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256­263, 2007.
[13] H. Daumé III and D. Marcu. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26(1):101­126, 2006.
[14] K. Duh and K. Kirchhoff. Learning to rank with partially-labeled data. In Proceedings of SIGIR, pages 251­258, 2008.
[15] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933­969, 2004.
[16] J. Gao, Q. Wu, C. Burges, K. Svore, Y. Su, N. Khan, S. Shah, and H. Zhou. Model adaptation via model interpolation and boosting for web search ranking. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language, 2009.
[17] B. Geng, L. Yang, C. Xu, and X.-S. Hua. Ranking model adaptation for domain-specific search. In Proceeding of the 18th ACM Conference on Information and Knowledge Management, pages 197­206, 2009.
[18] R. Herbrich, T. Graepel, and K. Obermayer. Large Margin

Rank Boundaries for Ordinal Regression. MIT Press, Cambridge, 2000. [19] J. Huang, A. J. Smola, A. Gretton, K. M. Borgwardt, and B. Schölkopf. Correcting sample selection bias by unlabeled data. In Proceedings of NIPS, pages 601­608, 2006.
[20] K. Järvelin and J. Kekäläinen. IR evaluation methods for retrieving highly relevant documents. In Proceedings of SIGIR, pages 41­48, 2000.
[21] J. Jiang and C. Zhai. Instance weighting for domain adaptation in NLP. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264­271, 2007.
[22] M. Li, H. Li, and Z.-H. Zhou. Semi-supervised document retrieval. Information Processing and Management, 45(3):341­355, 2009.
[23] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225­331, 2009.
[24] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. In Proceedings of the 21st International Jont Conference on Artifical Intelligence, pages 1187­1192, 2009.
[25] J. C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers, pages 61­74. MIT Press, 1999.
[26] J. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of SIGIR, pages 275­281, 1998.
[27] T. Qin, T.-Y. Liu, J. Xu, and H. Li. LETOR: A benchmark collection for research on learning to rank for information retrieval. Information Retrieval, 2010.
[28] S. E. Roberson. The probability ranking principle in IR. Journal of Documentation, 33(4):294­304, 1977.
[29] S. E. Robertson, S. Walker, M. M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In Proceedings of TREC-3, pages 109­128, 1995.
[30] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513­523, 1988.
[31] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In Proceedings of the 24th International Conference on Machine Learning, pages 807­814, 2007.
[32] V. N. Vapnik. The nature of statistical learning theory. Springer Verlag, 1995.
[33] E. M. Voorhees. Overview of TREC 2004. In Proceedings of TREC-2004, pages 1­12, 2004.
[34] B. Wang, J. Tang, W. Fan, S. Chen, Z. Yang, and Y. Liu. Heterogeneous cross domain ranking in latent space. In Proceeding of the 18th ACM Conference on Information and Knowledge Management, pages 987­996, 2009.
[35] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support vector method for optimizing average precision. In Proceedings of SIGIR, pages 271­278, 2007.
[36] B. Zadrozny. Learning and evaluating classifiers under sample selection bias. In Proceedings of the 25th International Conference on Machine Learning, 2004.
[37] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR, pages 334­342, 2001.

169

Analysis of Structural Relationships for Hierarchical Cluster Labeling

Markus Muhr
Know-Center Graz Inffeldgasse 21a 8010 Graz, Austria
mmuhr@know-center.at

Roman Kern
Know-Center Graz Inffeldgasse 21a 8010 Graz, Austria
rkern@know-center.at

Michael Granitzer
Know-Center Graz Graz University of Technology
Inffeldgasse 21a 8010 Graz, Austria
mgrani@know-center.at

ABSTRACT
Cluster label quality is crucial for browsing topic hierarchies obtained via document clustering. Intuitively, the hierarchical structure should influence the labeling accuracy. However, most labeling algorithms ignore such structural properties and therefore, the impact of hierarchical structures on the labeling accuracy is yet unclear. In our work we integrate hierarchical information, i.e. sibling and parentchild relations, in the cluster labeling process. We adapt standard labeling approaches, namely Maximum Term Frequency, Jensen-Shannon Divergence, 2 Test, and Information Gain, to take use of those relationships and evaluate their impact on 4 different datasets, namely the Open Directory Project, Wikipedia, TREC Ohsumed and the CLEF IP European Patent dataset. We show, that hierarchical relationships can be exploited to increase labeling accuracy especially on high-level nodes.
Categories and Subject Descriptors
H.3.1 [Content Analysis and Indexing]: Linguistic processing; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms
Keywords
Cluster Labeling, Statistical Methods, Topic Hierarchies, Structural Information
1. INTRODUCTION
Browsing large-scale document collections usually requires a structural organization form like topic hierarchies. Unsupervised machine learning techniques, foremost document clustering, overcome the labor intensive, manual creation of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

such topic hierarchies by automatic partitioning of unstructured document collections into browse-able cluster hierarchies. This cluster based browsing approach has been shown to successfully improve access to unstructured document collections [5, 16].
However, even if an algorithm achieves a perfect hierarchical partitioning, users must guess the content of each cluster somehow. Automatically generated labels provide such a descriptive cluster summary. Obviously, the label quality strongly influences the navigation effectiveness as shown by human created topic hierarchies like the Open Directory Project: while users may disagree on the exact label of a topic, every user can exploit labeling information for navigation purposes as long as labeling is of high quality - this is especially true for high level nodes like sports, computers etc.
Most existing labeling approaches extract labels by comparing term distributions of a cluster to a reference collection and taking the statistically most discriminative terms. Intuitively, for a flat partitioning this seems to be sufficient, but insufficient for creating topic hierarchies similar to the Open Directory Project (ODP) 1; Child clusters have to be described in the context of their parent cluster and must not contain the same labels. Such a constraint cannot be ensured without taking structural relationships between clusters into account. Moreover, label quality tends to decrease on higher levels due to higher degree of abstraction. Most state of the art labeling approaches (e.g. [11, 1]) do not use structural relationships. Although there are approaches considering hierarchical relationships - either through supervised learning [19] or through hierarchical post-processing of flat cluster labels [14] - to our best knowledge there is no systematic investigation whether the intuitive claim above holds or not. Intuitively it also seems to be natural that labeling performance as well as the influence of structural relationships depends on a topic hierarchies structural properties - a claim hard to investigate due to missing test corpora.
In this paper we investigate the influence of hierarchical relationships on the cluster labeling process. We extend standard labeling approaches, namely Maximum Term Frequency, Jensen-Shannon Divergence, 2, and Information Gain, to include structural information. First, Maximum Term Frequency labeling is extended by a sibling based weighting scheme, yielding to a new labeling algorithm called ICWL (Inverse Cluster frequency Weighted) labeling. Second, we extend all labeling approaches with parent-
1http://dmoz.org

178

child relationships. Comparing all labeling approaches on four datasets, namely the Open Directory Project (ODP), Wikipedia, TREC-Ohsumed and European Patents (EP), shows that hierarchical information influences the labeling process. Especially Wikipedia shows the biggest dependency on hierarchical information, followed by the ODP dataset. This finding yields to an important point for future work on this topic: top level nodes, which are most important in the users browsing process, are labeled badly. Hence, the browsing process of automatically created cluster hierarchies could be improved by using hierarchical cluster label algorithms.
With our work we contribute to the field of cluster labeling by
· extending standard labeling approaches to take use of hierarchical information
· showing, that sibling relationships can be exploited to improve statistical labeling methods
· showing, that the structure of the hierarchy and the domain of the test dataset have a strong influence on the labeling accuracy, especially for top level nodes, which are crucial for user navigation
· using traditional, but also new test corpora for evaluating cluster labeling algorithms
The paper is structured as follows: first a related work section provides an overview of the state of the art relevant for our work, second we introduce a formal definition of structure based labeling, followed by the third part, a description of the utilized corpora (ODP, Wikipedia, Ohsumed, European Patents) together with the implemented preprocessing steps. The paper ends by outlining results with an obligatory discussion pointing out our findings, and finally a summary of our work with a discussion of the implications for future work.
2. RELATED WORK
Standard labeling approaches extract most prominent terms in a specific cluster by statistical feature selection [11]. A straightforward feature selection method takes the maximum sum of the individual term frequencies of documents assigned to a cluster [5]. In [14], a weighting schema has been introduced to improve maximum sum of term frequencies by neglecting stop-words or general words.
However, maximum-sum approaches prefer terms which are over-represented in the whole document collection. This increases the probability that all cluster are getting similar labels. More sophisticated approaches consider a terms discriminative power compared to a reference collection usually the documents of all sibling clusters. Well-known methods include an adapted versions of Information Gain [7], 2 Test [14], and the Jensen-Shannon Divergence (JSD) [2]. Comparative studies of reference collection based labeling approaches done in [1] favor JSD for the non-hierarchical case. However, in the hierarchical case such an in-depth comparison between labeling approaches is missing. Further, the evaluation conducted in [1] favors leaf nodes of a hierarchy without addressing questions on the influence of hierarchical structures.
Besides statistical term selection methods, researchers focused on using different document parts like title [5], hyperlink anchors [9] etc. or different features like named entities

[17], frequent phrases [13] or text summarization [15]. However, our work focuses not on finding the very best labeling approach including the best document representation, but to include structural properties into statistical labeling approaches. Treeratpituk et. al. [19] addressed this problem via supervised learning. Weights for different term importance measures depending on parent-child relationships are estimated using supervised learning. However, their method needs training data to determine the actual weights. Through supervised training and the use of synonyms in the labeling process the actual influence of hierarchical relationships remains unclear. Further work in hierarchical labeling has been done by Popescul and Ungar [14], where parent child relationships are exploited in a post processing step and not directly included into the feature selection process. Hence, errors from the labeling process are propagated to the post processing step.
In the past years researchers investigated the usage of external knowledge to enhance machine learning tasks through extending the given term set with synonyms, hypernyms, hyponyms etc. Most prominent resources for this task are WordNet [3] and Wikipedia [1]. Especially Carmel et. al. [1] showed that using Wikipedia as a thesaurus and applying this thesaurus as post-processing step to statistical labeling approaches improves cluster labeling dramatically. Although they used statistical labeling approaches in a first step, hierarchical information was not included. So by improving the statistical labeling their approach may be further enhanced.
Evaluating cluster labeling approaches face the dataset sparsity problem. Most methods mentioned above use the Open Directory Project (ODP) [12] as well as the flat 20 newsgroup dataset. This basically restricts the obtained results on the domain of web resources. Whether the results apply also to other domains like patents, medicine etc. remains an open question. We address this question by conducting experiments, in addition to the ODP, also on Wikipedia [6], the TREC-Ohsumed collection [18] and the European Patents provided by the 2009 CLEF IP Task [4].
3. STRUCTURE BASED LABELING
In order to analyze the impact of hierarchical relationships we gradually incorporate structure information into well-known labeling techniques. We use a maximum term weight labeling approach as well as a reference collection based labeling approach to estimate the labels of a cluster. Through weighting methods based on the position of a document relative to the label candidate cluster, we introduce structure information in terms of (i) sibling relations and (ii) parent-child relations. All approaches consider bag-of-word document representations and hierarchical relationships among clusters. Before describing the labeling approaches in detail, we introduce a formalism for denoting cluster hierarchies, document sets as well as a notation for selecting particular sub-trees and and sub document sets.
Formally, we define D as the set of all documents and a cluster cl as a sub set of documents cl  D; C = {c1 . . . ck} denotes as set of non-overlapping clusters, i.e. i=jci  cj = . The hierarchical structure between clusters is formalized as is-a relationship cj  ci (cj = ci), defining that cj is the parent (direct parent) of ci. For specifying the set of clusters with parent cj, we write Ccj and denote all documents contained in this sub-hierarchy as Dcj. Se-

179

mantically the is-a relationship should resemble a classical topic hierarchy, which assumes that if a document is assigned to a topic, it is also assigned to its parent topic. For the rest of this paper we consider the is-a relationship as implicitly given when referring to the set of clusters C or any subset of them. Further, documents are represented as a term frequency vector d  d. Using a vector representation allows us to consider different weighting methods to assess the a-priori importance of terms in a corpus resp. a cluster. Labels Lj of a cluster cj are represented as set of terms Lj = {t1 . . . tl}.

3.1 Maximum Term Weight Labeling

Labeling can now be seen as function Lj  label(C, cj) selecting the most suitable terms for describing cluster cj as label Lj. The simplest labeling functions are basic feature selection techniques [11]: Documents assigned to a cluster are aggregated and the k largest features are taken as labels. We refer to this as Maximum Term Weight Labeling (MTWL): given cluster cj as labeling candidate the MTWL can be written as

"X "

Lj  bestk

di

(1)

diDcj 

"" where bestk v is a function returning the terms associated
with the k largest dimensions of vector v. In our experiments we refer to this approach as M T W Lraw.
Maximum Term Weight Labeling strongly depends on the document representation. Given only term frequency document vectors, labeling will likely extract terms occurring in a large number of documents with high frequency. Such labels do not necessarily discriminate between clusters. Global weighting schemes like TFIDF or Okapi BM25 [11] allow to increase the discrimination capability of labels based on the underlying document distribution. Since documents in the same cluster share similar terms, the inverse document frequency reduces the influence of terms occurring in a high number of documents and therewith in a high number of cluster. To introduce such global weighting, MTWL can be simply extended as follows:

"X

"

Lj  bestk

idfglobal · tf W eight(di) (2)

diDcj 

where idfglobal  d is a vector containing the corpus dependent inverse document frequencies, tf W eight() is a function applying the document specific part of the weighting scheme and · is the Hadamard point-wise product. The inverse document frequency for a term k is calculated as

" |D|

"

idfglobal,k = log

+1 #(tk, D)

(3)

with #(tk, D) returning the number of documents in the collection containing term k. For the term frequencies we used
the document specific part of the standard Okapi BM25 as well as plain term frequencies2. Since idfglobal is defined over the collection D of all documents we refer to this approach
as global weighting approach.

2We split the weighting scheme in a collection and a document specific part in order to have a homogeneous notation over our different labeling approaches.

Global weighting penalizes terms, which are over-represented in the whole collection. However, terms over-represented in a particular cluster sub-tree only, will be likely selected for all siblings in the cluster hierarchy. For example, given that term tk is over-represented in cluster cj and equally distributed among the direct children ci with cj = ci, then it is very likely that term tk will become a label of the direct child ci. Hence, term distributions among siblings have to be taken into account to avoid siblings getting similar labels.
For considering sub-tree dependent term distribution we again add a local, sub-tree based inverse document frequency term to eq. 2. Formally, the labeling function is defined as

"X

"

Lj  bestk

idfglobal · idflocal,j · tf W eight(di)

diDcj 
(4)

where idflocal,j is the inverse document frequency vector over the document collection Dcpwith cp = cj where cp is defined as the parent cluster of cj. Simply speaking this document collection consists of all documents in the subtree

spanned by the parent cluster cp. In particular the idf entry for term k in cluster cj is calculated as

idflocal,j

=

" log

|Dcp  |

#(tk, Dcp)

+

" 1

(5)

with #(tk, Dcp) returning the number of documents in the reference collection which contain term k. In our exper-
iments we refer to this approach as M T W Lidf .

3.2 Reference Collection based Labeling

Besides taking the largest dimension of a centroid vector, comparative statistics like the 2-Test or the JensenShannon Divergence (JSD) can estimate whether occurrences of a term differ between a cluster and a reference collection with statistical significance. Such terms yield good labels for a cluster. In our Reference Collection based Labeling approach (RCL) we use well known comparative statistics, namely the Jensen-Shannon Divergence, Information Gain and 2, in order to compare terms contained in clusters to terms contained in a reference collection of documents, denoted as Dref . The k terms with the best test values are taken as labels. Hierarchical information is incorporated through the selection of the reference collection. Formally, we denote

"

"

Lj  bestk J SD(Dref , Dcj )

(6)

as labeling function where J SD(Dref , Dcj)  d returns the Jensen-Shannon Divergence (see [2]) for each dimen-
"" sion in form of a d-dimensional vector. Again, bestk v
is used to select the k terms with the best statistical test
value. Similarly we abbreviate the Information Gain as IG(Dref , Dcj ) [7] and 2 as 2(Dref , Dcj ) [14]. The probability for a term is estimated in a standard manner as
the number of occurrences of a term divided by the total
number of occurrences of terms.
For labeling cluster cj we define the reference collection as all documents belonging to the cluster sub-tree of its di-
rect parent excluding all documents contained in cj. Formally, the reference collection is given as Dref = Dcp \ Dcj with cp = cj , where cp is the parent of cj . In the non-hierarchical case this corresponds to the best known
standard labeling approaches [1].

180

3.3 Inverse Cluster Weight Labeling
MTWL incorporates hierarchical information through a specialized weighting function idflocal,j. The weighting function depends on the term distribution over documents in the sub-tree, but does not take the term distribution over sibling cluster into account.
To integrate sibling information we add another weighting factor. The weighting factor is inspired by the recently introduced Class-Feature-Centroid (CFC) classifier using class discriminative terms for achieving classification accuracies similar to Support Vector Machines [10]. Similarly we want to take use of clusters discriminative terms. Basically, if one term occurs often in one sibling cluster only, this term should be preferred over terms occurring in all sibling cluster: a term k in cluster cj is weighted by its inverse cluster frequency calculated as

icfj,k

=

exp " #(tk, Dcj) " log " #(cp)

|Dcj |

#(tk, cp)

+

" 1

(7)

with cp being the direct parent of cj, #(tk, cp) being the number of direct subcluster of cp containing term k and #(cp) being the number of direct sub-clusters. The exponential component, similar to the CFC classifier, promotes terms occurring in a larger fraction of documents. In our experiments we refer to this approach as ICW Lraw if M T W Lraw is extended by the inverse cluster frequency weight and to ICW Lidf in the case of M T W Lidf .

3.4 Hierarchical Labeling

Especially in the hierarchical case, relying only on sibling information may be problematic; a term occurring often in the parent cluster (and all its documents) may also occur often in one or several child clusters. While incorporating sibling information potentially removes parent labels equally distributed over potentially all siblings, it cannot overcome parent labels occurring often in a few child clusters. Hence, parent child relationships have to be taken into account.
Hierarchical labeling extends all labeling approaches introduced before, by weighting the influence of a term inverse proportional to the path length of the child cluster to the label candidate cluster cj. By denoting the path length between two clusters as l(j, i), the labeling function of cluster cj can be formally written as

"X 1

"

Lj  bestk

l(j, i)  cfl(j,i) · vj,i

(8)

ciCcj 

where cfl(j,i) is the sibling based cluster frequency term vector and vj,i is the result of the comparison statistics formally written as

X

vj,i = idfglobal · idflocal,j ·

tf W eight(do) (9)

do ci

in case of MTWL labeling strategy. Simply speaking, vj,i is the centroid vector of cluster ci weighted in the local context of the label candidate cluster cj. This principle is similarly applied to the ICWL and the RCL approach.
Contrary to the inverse cluster frequency weighting above, the cluster frequency term vector penalizes terms occurring only in a single cluster on a particular hierarchy level (i.e. l(j, i)). The cluster frequency weight for term tk is simply the number of clusters a term occurs in divided by the total number of clusters on this particular hierarchy level.

The idea behind the cluster frequency weight is to promote terms occurring in a higher number of child clusters since those terms are most likely representative labels for their parent. In our experiments, all results involving hierarchical weighting are prefixed with "Hier".
4. DATASETS
For our experiments we used 4 different datasets: two general domain corpora, namely Wikipedia and Open Directory Project, and two domain specific corpora, namely European Patents (EP) and Ohsumed. All datasets have been preprocessed in the same way: document tokenization has been done using OpenNLP3; Tokens haven been stemmed afterwards using the Snowball4 stemmer. Finally, stop-words have been removed by using the list supplied by the Snowball stemmer.
Open Directory Project (ODP): We imported a large part of the hierarchy including the top categories arts, business, games, health, home, news, society, and sports with their complete subtree. We took only hard links into account ignoring symbolic links and related topic links. Letter categorizations are ignored as well. This yielded about 150,000 categories and about 800,000 documents. In order to compare the effect of the manually created descriptions and titles for each ODP entry, we created two sub-datasets. In the first dataset, named ODP Title & Description, each document consists of the description and title as provided in the ODP hierarchy. For the second dataset, named ODP HTML, we crawled the HTML page a ODP entry pointed to and all HTML pages links in the crawled page pointed to, i.e. we performed a crawl of depth 2. This crawling strategy should provide sufficient content rich pages producing a rather different dataset compared to the classical ODP Title & Description dataset.
Wikipedia: To extract the structural information out of the Wikipedia we started with the XML dump of the English version. From each entry within the dump we extracted the title and all links that indicate an assignment to a Wikipedia category. This was done for all articles and category pages such that we were able to reconstruct the classification relationships. We filtered out categories that do not carry any semantic information and assembled a category blacklist (e.g. "Wikipedia maintenance') and filtered out categories containing the word "by", "of" and "in" to eliminate categories like "Authors by Year". In order to create a tree structure of of the acyclic category graph we started by each main topic - namely arts, computing, health, and sports we traversed the graph in a breath-first manner with a maximum depth of 10. Since a breath-first search of depth 10 would return a too large portion of the Wikipedia graph, we randomly chose 10 outgoing links and 80 documents for each topic. Roughly we had about 50,000 categories with about 400,000 documents as test dataset.
TREC Ohsumed: We used the Ohsumed collection from the 2001 TREC evaluation. The hierarchical structure has been obtained downloading the Mesh Tree hierarchy5 of 2004 with 7724 different categories and 348,564 documents.
European Patents (EP): European patents are taken from
3http://opennlp.sourceforge.net/ 4http://snowball.tartarus.org/ 5http://www.nlm.nih.gov/mesh/

181

80

Document - Label Overlap
ODP - Title & Description ODP - HTML Wikipedia Oshumed European Patents

60

40

Percent

20

0

1

2

3

4

5

6

7

8

Hierarchy Depth

Figure 1: Document-Label-Overlap: Fraction of documents containing all label terms depending on the distance to the category.

the dataset which has been created for the Intellectual Property track of CLEF 2009 (CLEF-IP) [8]. From this dataset we selected only patents that were granted and limited the timespan from 1991 to 2000. We ended up with 265,409 patents, each of them having at least one assignment to the IPC classification scheme6. This IPC classification hierarchy consists of over 60,000 classes arranged in a tree-like manner with 8 root categories. The claims section has been used as document content.
5. RESULTS
In order to measure labeling accuracy we use the mean average precision (MAP) averaged over all categories. To calculate the MAP, category labels - similar to documents - are tokenized, stemmed and stopword filtered resulting in a set of terms. This set of terms is compared to the ranked list of candidate terms returned by the labeling algorithm, which gives the MAP value for one category.
We did not use any synonyms or external linguistic resources. To ensure that terms in the document set contain the terms extracted from the category labels, we estimated the Document-Label-Overlap, as outlined in the next section.
5.1 Document Label Overlap
The Document-Label-Overlap estimates whether a certain label of a cluster is contained in its connected documents at all. The Overlap is calculated as the fraction of documents containing all label terms to the number of total documents in the sub-tree with depth d. Thus, the overlap determines the baseline on getting a correct label for a topic. Further, by considering the overlap of documents with a particular path
6http://www.wipo.int/classifications/ipc/en/

length d to the label candidate cluster we get an evidence on the influence of documents on particular hierarchy depths.
Figure 1 depicts the document label overlap for all datasets. Clearly, results show a significant decrease in the fraction of documents containing the actual label with the hierarchy level; a correct label is more likely found in documents close to the cluster. Furthermore, documents with high path lengths are more specialized and thus tend to use a more specialized vocabulary. For example, an article on Support Vector Machines might not mention the words machine learning explicitly, since it is a specialized topic in the field of machine learning. Hence, this analysis supports the evidence that structural properties play a role in cluster labeling.
A comparison between datasets point out interesting differences: for Ohsumed, ODP with Title & Description and the Wikipedia dataset the overlap drops significantly with increasing depth while it decreases rather slowly for the ODP HTML dataset. Clearly, ODP HTML contains more terms per document therewith increasing the likelihood of finding the correct label. The European Patents dataset shows its special nature: the overlap is constantly low over all hierarchy depths.
5.2 Labeling Accuracy
To evaluate the influence of the hierarchy depth on the labeling process, we plot the MAP on each dataset and labeling approach for sub-hierarchies of depth 1-8. We limit our analysis to sub-hierarchies with a maximum depth of 8 since there are too few sub-hierarchies with a larger depth making a statistical evaluation infeasible. Note also that the distribution of hierarchies is skewed: there are far more hierarchies with depth 1 than with depth > 1. Therefore, the overall labeling accuracy is approximately the labeling accuracy of depth 1 sub-hierarchies. This is contrary to the browsing behavior of a user who needs high labeling accuracy on the top nodes, i.e. on sub-trees with depth >1.
Figure 2 shows the labeling accuracy for the ODP dataset, split into description based documents and crawled HTML based documents. Figure 3 reports results on the Ohsumed and Wikipedia dataset. Note that for the clarity of presentation we only show the JSD labeling approach for RCL based labeling techniques. Compared to IG and 2 (as well as their hierarchical counterparts), JSD always achieved the best performance. This supplements the findings in literature and extends them also to the hierarchical case, see [1].
Comparison of labeling techniques: Comparing the different labeling techniques it can be seen that the sibling based labeling approach with local and global weighting ICW Lidf is performing about as good as the maximum term weighting approach, with exception of the the Wikipedia dataset where the integration of the sibling information does improve the accuracy. The labeling method that uses a reference collection JSD provides good results for every dataset. This is especially pronounced for the ODP Title & Description dataset when incorporating the hierarchical structure into the creation of the reference collection.
Flat vs. Hierarchical labeling: Table 1 depicts the absolute MAP differences between hierarchical and flat approaches. For each of the tested combinations of datasets and labeling algorithms, the integration of hierarchical information always improves the accuracy. One exception is the Ohsumed dataset, where hierarchical and flat methods perform ap-

182

ODP Title & Description

ODP HTML

0.6

0.6

0.5

0.5

0.4

0.4

0.3

MAP

0.3

MAP

0.2

0.2

0.1

0.1

0.0

MTWLraw MTWLidf JSD ICWLraw ICWLidf

hierMTWLraw hierMTWLidf hierJSD hierICWLraw hierICWLidf

1

2

3

4

5

6

7

8

Hierarchy Level

0.0

MTWLraw MTWLidf JSD ICWLraw ICWLidf

hierMTWLraw hierMTWLidf hierJSD hierICWLraw hierICWLidf

1

2

3

4

5

6

7

8

Hierarchy Level

Figure 2: Performance of the different labeling algorithms for the Open Directory Project dataset. Algorithms that exploit the hierarchical structure generally produce better results.

proximately equal. Analyzing the sub-trees of top nodes we could observe a high variance of the influence of hierarchical labeling approaches which deserves further analysis. We assume that the increase depends on structural properties. A assumption to be validated in future work.
Ohsumed comes from a rather narrow domain compared to ODP and Wikipedia and is structurally different from all other datasets: the MESH-categorization includes documents only at leaf categories. Further, descriptions are rather small even compared to the ones from ODP. One can expect that these descriptions will not contain information about categories despite their direct parent - a fact also supported by the Document-Label-Overlap. For this reason, it is not surprising that hierarchical information increases labeling accuracy only slightly, since an already sparse information on term distributions is further reduced. This is also reflected by the fact that no labeling algorithm could outperform the other. The relative average difference for the MAP measure of the hierarchical labeling approaches in comparison with their flat counterparts is 0.004 for this dataset.
The Wikipedia dataset and the ODP datasets show a completely different picture. Flat labeling accuracy decreases significantly with the depth of the hierarchy; hierarchical labeling gives a slight average accuracy increase of 0.059 in case of the ODP HTML dataset, an accuracy increase of around 0.099 MAP on average in case of the ODP Description & Title dataset and a larger accuracy increase of around 0.132 MAP on average in case of Wikipedia (see also table 1). In case of the ODP dataset, the increase through hierarchical relationships on the Title & Description dataset is slightly better than on the HTML dataset. As depicted by the Document-Label-Overlap analysis, the HTML crawled documents most likely contain a broader range of terms

compared to the title and description of the original ODP dataset. A broader range of terms increases the likelihood that a document contains a cluster label, even if it is farther away. This indicates that non-specialized documents on deeper nodes in a tree do not decrease labeling accuracy - a rather seldom case in topic hierarchies. In such a case, hierarchical weighting actually removes information on the topic instead of reducing the impact of certain more specific documents.
Regarding Wikipedia, the graph like category structure has to be considered, which has been adopted to a treestructure in our case. For this reason, we took four main categories (Arts, Computing, Health, Sports) and created a topic hierarchy using the mentioned sampling strategy. While the sampling strategy seems to be fair, it does not reproduce the correct Wikipedia Category graph. Instead, we get a rather balanced tree like structure. Nevertheless, given such a balanced structure it is quite obvious that local idf weighted as well as hierarchical approaches benefit to a large degree. Especially the labeling of high level nodes could be dramatically increased through incorporating hierarchical information and significantly outperforms the nonhierarchical approaches. Moreover, this holds for all different labeling approaches.
The results for the European Patents are not depicted due to rather low accuracies. Nevertheless we mention them to support recent findings in the CLEF-IP challenge where it was shown that patents are a rather specialized domain [8]. Well-known information retrieval approaches failed to achieve good results in the challenge and it seems to be the same with cluster labeling methods. All approaches fail completely by only achieving values in the field of 0.03, although it seems to be the case the ICWL again improved the results. Also, the Document-Label Overlap shows that

183

0.6

Wikipedia

MTWLraw MTWLidf JSD ICWLraw ICWLidf

hierMTWLraw hierMTWLidf hierJSD hierICWLraw hierICWLidf

0.6

Oshumed

MTWLraw MTWLidf JSD ICWLraw ICWLidf

hierMTWLraw hierMTWLidf hierJSD hierICWLraw hierICWLidf

0.5

0.5

0.4

0.4

0.3

MAP

0.3

MAP

0.2

0.2

0.1

0.1

0.0

0.0

1

2

3

4

5

6

7

8

Hierarchy Level

1

2

3

4

5

6

7

8

Hierarchy Level

Figure 3: Performance of the different labeling algorithms for the Wikipedia and the Oshumed dataset. Both datasets demonstrate different characteristics, for the Wikipedia dataset the structural information increases the labeling performance, whereas for the Oshumed dataset neither the sibling nor the child-parent relationships help to find matching labels.

ODP - Title & Description ODP - HTML Wikipedia Oshumed

M T W Lraw
0.06 0.04 0.08 0.00

M T W Lidf
0.09 0.07 0.12 0.01

JSD
0.15 0.09 0.19 0.01

ICW Lraw
0.08 0.05 0.12 0.00

ICW Lidf
0.12 0.05 0.16 0.00

Average
0.099 0.059 0.132 0.004

Table 1: Average relative difference of the MAP for all hierarchy levels greater than 2 for all datasets between the different methods either with and without exploitation of hierarchical information. Exploiting the hierarchical structure always improves the accuracy, although for the Oshumed dataset the difference is not pronounced.

only below 10 % of the documents connected to a category contain a label term.
Overall our results imply that incorporating hierarchical information improves labeling accuracy on average.
Moreover, our results have impact on the evaluation of cluster labeling approaches for browsing topic hierarchies. Viewed from a users point of view, flat labeling approaches support the browsing of leaf nodes rather than the browsing of high level nodes - a result quite contradictory to the users need. Especially in the case of the de-facto standard benchmark dataset, the ODP Description & Title dataset, this has to be taken into account for future evaluations. First, sampling data for an evaluation should consider the a-priori distribution of sub-hierarchies with different depths. By using depth independent random samples for cluster labeling evaluation it is very likely to draw hierarchies of depth 1 and to achieve good labeling performance using flat labeling approaches. Second, hierarchies of different depth should be

evaluated separately in order to deduce the impact of the labeling strategy on the users navigational support.
6. CONCLUSION AND OUTLOOK
Our results show that structural relationships influence the labeling accuracy. Using sibling information increases labeling accuracy in some datasets; integrating hierarchical information produces better labeling results for all datasets. This insight has several consequences.
Firstly, evaluation of cluster labeling approaches have to take hierarchical properties into account, especially if the goal is to support user navigation.
Secondly, correlations between the properties of a hierarchy, like for example maximum depth, branching factor, documents per leaf node etc., the richness of the assigned documents and the achievable labeling accuracy should be further analyzed. While we followed the evaluation approach conducted by other researchers in the field, there should be a closer evaluation whether cluster hierarchies and manually created hierarchies resemble the same statistical properties w.r.t the document collection.

184

Thirdly, more sophisticated approaches like for example the extension of JSD with hierarchical information may further increase the accuracy in the hierarchical case. Although we integrated parent-child relationships in an ad-hoc manner, we observed an effect on the labeling accuracy. Clearly we would expect more sophisticated approaches to increase accuracy further.
Fourthly, labeling accuracy is strongly domain dependent. The generalization of labeling approaches to different domains remains an open issue.
Finally, external knowledge in form of thesauri, ontologies etc. has to be considered also in the hierarchical case. We restricted our work to term frequency vectors only and focused poorly on statistical approaches that do not incorporate any external knowledge in form of thesauri, ontologies etc. However, our labels depend solely on the document representation and hence the term frequency vectors may be replaced by more sophisticated preprocessing utilizing external knowledge. Furthermore, labeling approaches using external knowledge most often depend on good statistical label selection and thus our approach contributes to their improvement.
Acknowledgments
The Know-Center GmbH Graz is funded within the Austrian COMET Program - Competence Centers for Excellent Technologies - under the auspices of the Austrian Federal Ministry of Transport, Innovation and Technology, the Austrian Federal Ministry of Economy, Family and Youth and by the State of Styria. COMET is managed by the Austrian Research Promotion Agency FFG.
7. REFERENCES
[1] D. Carmel, H. Roitman, and N. Zwerdling. Enhancing cluster labeling using wikipedia. In SIGIR '09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 139­146. ACM Press, 2009.
[2] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In SIGIR '06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 390­397. ACM Press, 2006.
[3] O. S. Chin, N. Kulathuramaiyer, and A. W. Yeo. Automatic discovery of concepts from text. In Web Intelligence, pages 1046­1049. IEEE Computer Society, 2006.
[4] E. P. (CLEF-IP). European patents (clef-ip), 2009. [Online; accessed 07-January-2010].
[5] D. R. Cutting, J. O. Pedersen, D. Karger, and J. W. Tukey. Scatter/gather: A cluster-based approach to browsing large document collections. In SIGIR '92: Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 318­329. ACM Press, 1992.

[6] W. English. Wikipedia english, 2009. [Online; accessed 12-May-2009].
[7] F. Geraci, M. Pellegrini, M. Maggini, and F. Sebastiani. Cluster generation and labeling for web snippets: A fast, accurate hierarchical solution. Internet Mathematics, 3(4):413­443, 2007.
[8] F. P. Giovanna Roda, John Tait and V. Zenz. Clef-ip 2009: Retrieval experiments in the intellectual property domain. In Working Notes for the CLEF 2009 Workshop, 2009.
[9] E. J. Glover, D. M. Pennock, S. Lawrence, and R. Krovetz. Inferring hierarchical descriptions. In CIKM '02: Proceedings of the eleventh international conference on Information and knowledge management, pages 507­514. ACM Press, 2002.
[10] H. Guan, J. Zhou, and M. Guo. A class-feature-centroid classifier for text categorization. In WWW '09: Proceedings of the 18th international conference on World wide web, page 201. ACM Press, 2009.
[11] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, Cambridge, UK, 2008.
[12] O. D. P. (ODP). Open directory project (odp), 2009. [Online; accessed 29-September-2009].
[13] S. Osinski and D. Weiss. A concept-driven algorithm for clustering search results. IEEE Intelligent Systems, 20(3):48­54, 2005.
[14] A. Popescul and L. H. Ungar. Automatic labeling of document clusters, 2000.
[15] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based summarization of multiple documents. 2004.
[16] V. Sabol, W. Kienreich, M. Muhr, W. Klieber, and M. Granitzer. Visual knowledge discovery in dynamic enterprise text repositories. In IV '09: Proceedings of the 2009 13th International Conference Information Visualisation, pages 361­368. IEEE Computer Society, 2009.
[17] H. Toda and R. Kataoka. A clustering method for news articles retrieval system. In WWW '05: Special interest tracks and posters of the 14th international conference on World Wide Web, pages 988­989. ACM Press, 2005.
[18] O. T. C. TREC-9. Ohsumed test collection trec-9, 2000. [Online; accessed 14-December-2009].
[19] P. Treeratpituk and J. Callan. Automatically labeling hierarchical clusters. In DGO '06: Proceedings of the 2006 International Conference on Digital Government Research, pages 167­176, 2006.

185

On the Existence of Obstinate Results in Vector Space Models

Milos Radovanovic´
Department of Mathematics and Informatics
University of Novi Sad Serbia
radacha@dmi.uns.ac.rs

Alexandros Nanopoulos
Institute of Computer Science University of Hildesheim
Germany
nanopoulos@ismll.de

Mirjana Ivanovic´
Department of Mathematics and Informatics
University of Novi Sad Serbia
mira@dmi.uns.ac.rs

ABSTRACT
The vector space model (VSM) is a popular and widely applied model in information retrieval (IR). VSM creates vector spaces whose dimensionality is usually high (e.g., tens of thousands of terms). This may cause various problems, such as susceptibility to noise and difficulty in capturing the underlying semantic structure, which are commonly recognized as different aspects of the "curse of dimensionality." In this paper, we investigate a novel aspect of the dimensionality curse, which is referred to as hubness and manifested by the tendency of some documents (called hubs) to be included in unexpectedly many search result lists. Hubness may impact VSM considerably since hubs can become obstinate results, irrelevant to a large number of queries, thus harming the performance of an IR system and the experience of its users. We analyze the origins of hubness, showing it is primarily a consequence of high (intrinsic) dimensionality of data, and not a result of other factors such as sparsity and skewness of the distribution of term frequencies. We describe the mechanisms through which hubness emerges by exploring the behavior of similarity measures in high-dimensional vector spaces. Our consideration begins with the classical VSM (tf-idf term weighting and cosine similarity), but the conclusions generalize to more advanced variations, such as Okapi BM25. Moreover, we explain why hubness may not be easily mitigated by dimensionality reduction, and propose a similarity adjustment scheme that takes into account the existence of hubs. Experimental results over real data indicate that significant improvement can be obtained through consideration of hubness.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Experimentation, Measurement, Performance, Theory
Keywords
Text retrieval, vector space model, nearest neighbors, curse of dimensionality, hubs, similarity concentration, cosine similarity
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1. INTRODUCTION
The vector space model (VSM) [13] is a popular and widely applied information retrieval (IR) model that represents each document as a vector of weighted term counts. A similarity measure is used to retrieve a list of documents relevant to a query document. VSM allows for many variations in the choice of term weights and similarity measure used, with prominent representatives including tf-idf weighting and cosine similarity, as well as more recently proposed schemes Okapi BM25 [12] and pivoted cosine [14].
Typically, the number of terms used in VSM is large, producing a high-dimensional vector space (with, e.g., tens of thousands of dimensions). This high dimensionality has been identified as the source of several problems, such as susceptibility to noise and difficulty in capturing the underlying semantic structure. Such problems are commonly recognized as different aspects of the "curse of dimensionality," and their amelioration has attracted significant research effort, mainly based on dimensionality reduction.
1.1 Related Work and Motivation
In this paper we investigate a novel aspect of the dimensionality curse, called hubness, which refers to the tendency of some vectors (the hubs) to be included in unexpectedly many k-nearest neighbor lists of other vectors in a high-dimensional data set, according to commonly used similarity/distance measures. Hubness has previously been observed in various application fields, such as audio retrieval [1, 2] and fingerprint identification [9], where it is described as a problematic situation. Nevertheless, none of the existing studies provide full explanations of the mechanisms underpinning it. On the other hand, we have explored the hubness phenomenon for general vector-space data, mostly with Euclidean distance in the context of machine learning [11], and also conducted a preliminary examination of the phenomenon for cosine and cosine-like similarity measures with respect to collaborative filtering applications [10]. To our knowledge, hubness has not been thoroughly examined in connection to VSM and IR.
Hubness is worth studying in the context of IR, because it considerably impacts VSM by causing hub documents to become obstinate results, i.e., documents included in the search results of a large number of queries to which they are possibly irrelevant. This problem affects the performance of an IR system and the experience of its users, who may consistently observe the appearance of the same irrelevant results even for very different queries.
1.2 Contributions and Layout
We commence our investigation by demonstrating the emergence of hubness in the context of IR (Section 2). We continue with one of our main contributions, which is the explanation of the origins of the phenomenon (Section 3), describing that it is mainly a con-

186

sequence of high intrinsic dimensionality of vector-space data and not of other factors, such as sparsity and skewness of the distribution of term frequencies (caused, e.g., by differences in document lengths [14]). We link hubness with the behavior of similarity/distance measures in high-dimensional vector spaces and their concentration, i.e., the tendency of all pair-wise similarity/distance values to become almost equal. Although the study of concentration has attracted significant research effort for lp norms (e.g., Euclidean distance) [7], we analytically prove the emergence of concentration for the cosine similarity measure used in IR, and express the differences compared to lp norms. To ease the presentation of hubness, our discussion first considers the classical VSM based on tf-idf term weighting and cosine similarity, and then continues by demonstrating its generality on the more advanced variation Okapi BM25 [12], since hubness is an inherent characteristic of high-dimensional vector spaces that form the basis of various IR models. Moreover, it is explained why hubness is not easily mitigated by dimensionality reduction techniques.
We next proceed to examine how hubness affects IR applications (Section 4) by causing hubs to become frequently occurring but possibly irrelevant results to a large number of queries. For this purpose, we investigate the interaction between hubness and the notion of the cluster hypothesis [15], and propose a similarity adjustment scheme that takes into account the existence of hubs. The experimental evaluation (Section 4.2) of the proposed scheme over real data indicates that significant performance improvements can be obtained through consideration of hubness. Finally, we provide the conclusions and directions for future work (Section 5).
2. THE HUBNESS PHENOMENON
This section will demonstrate the existence of the hubness phenomenon, initially on synthetic data (Section 2.1), and then on real text data (Section 2.2), focusing on the classical tf-idf weighting scheme and cosine similarity. A more advanced document representation, Okapi BM25, is discussed in Section 4.3.
2.1 An Illustrative Example
To measure the existence of hubness, let D denote a set of vectors in a multidimensional vector space, and Nk(x) the number of k-occurrences of each vector x  D, i.e., the number of times x occurs among the k nearest neighbors of all other vectors in D, with respect to some similarity measure. Nk(x) can also be viewed as the in-degree of node x in the k-nearest neighbor directed graph of vectors from D.
We begin by considering an illustrative example, the purpose of which is to demonstrate the existence of hubness in vector-space data, and its dependence on dimensionality. Let us consider a random data set of 2,000 d-dimensional vectors (i.e., points) drawn uniformly from the unit hypercube [0, 1]d, and standard cosine similarity between them (Eq. 1 in Section 3.4). Figure 1(a­c) shows the observed distribution of Nk (k = 10) with increasing dimensionality. For d = 3, the distribution of Nk in Figure 1(a) is consistent with the binomial distribution. Such behavior of Nk would also be expected if the graph was generated following a directed version of the Erdos-Rényi (ER) random graph model [5], where neighbors are randomly chosen instead of coordinates.
With increasing dimensionality, however, Figures 1(b) and (c) illustrate that the distribution of Nk departs from the random graph model and becomes skewed to the right, producing vectors (called hubs) with Nk values much higher than the expected value k. The same behavior can be observed with other values of k and data distributions. This simple example with dense and uniformly distributed data is helpful to illustrate the connection between high dimensionality and hubness, since uniformity may not be intuitively

expected to generate hubness for reasons other than high dimensionality. To illustrate hubness in a setting more reminiscent of text data that have sparsity and skewed distribution of term frequencies, we randomly generate 2,000 vectors with the number of nonzero values for each coordinate ("term") being drawn from Lognormal(5; 1) distribution (rounded to the nearest integer), and random numbers (drawn uniformly from [0, 1]) spread accordingly throughout the data matrix. Figures 1(d­f) demonstrate the increase of hubness with increasing dimensionality in this setting.
A commonly applied practice in IR research is to reduce the influence of long documents (having many nonzero term frequencies and/or high values of term frequencies), by using various normalization schemes [14] to prevent them from being similar to many other documents. However, as observed above and as will be analyzed in Section 3, the high dimensionality that is an inherent characteristic of VSM is the main cause of hubness, as opposed to other data characteristics, since it emerges even when such normalization (cosine) is applied to sparse-skewed data, and also in the case of dense-uniform data where "long documents" are not expected.
2.2 Hubness in Real Text Data
Before elaborating on the mechanisms through which hubs form, we verify the existence of the phenomenon on real text data sets. Figure 2 shows the distribution of Nk (k = 10) for tf-idf term weighting and cosine similarity on three text data sets selected with the criterion of having large difference in their dimensionality. Similarly to the synthetic data sets, it can be seen that hubness tends to become stronger as dimensionality increases, as observed in the longer "tails" of these distributions.
Table 1 summarizes the text data sets examined in this study. Besides basic statistics, such as the number of points (n), dimensionality (d) and number of classes, the table also includes a column measuring the skewness of the distribution of N10 (SN10 ), as its standardized third moment:
SNk = E(Nk - Nk )3/N3 k ,
where Nk and Nk are the mean and standard deviation of Nk, respectively.1 The SN10 values in Table 1 indicate a high degree of hubness in all data sets. (The remaining columns will be explained in the sequel.)
3. THE ORIGINS OF HUBNESS
3.1 The Mechanism of Hub Formation
To describe the mechanisms through which hubness emerges, we begin the discussion by considering again the random data introduced in Section 2, i.e., the dense data matrix with iid uniform coordinates, and the sparse data set that simulates skewed term frequencies. For the same data sets and dimensionalities, Fig. 3 shows the scatter plots of N10 against the similarity of each vector to the data set mean, i.e., its center. In the chart titles, we also give the corresponding Spearman correlations. It can be seen that, as dimensionality increases, this correlation becomes significantly stronger, to the point of almost perfect correlation of hubness to the proximity to the data center.
The existence of the described correlation provides the main reason for the formation of hubs: owing to the well-known property of vector spaces, vectors closer to the center tend to be closer, on average, to all other vectors. However, this tendency becomes amplified as dimensionality increases, making vectors in the proximity to the data center become closer, in relative terms, to all other vectors, thus substantially raising their chances of being included in nearest-neighbor lists of other vectors.
1If SNk = 0 there is no skew, positive (negative) values signify right (left) skew.

187

p(N )
10

p(N )
10

0.4 0.3 0.2 0.1
0 0
(a)
0.4 0.3 0.2 0.1
0 0
(d)

uniform, d = 3

5

10

15

20

N
10

sparse, d = 50

10

20

30

40

50

N
10

(b) (e)

p(N )
10

p(N )
10

uniform, d = 20 0.25
0.2
0.15
0.1
0.05
0 0 10 20 30 40 50 60 70 N 10

sparse, d = 200 0.4

0.3

0.2

0.1

0

0

20

40

60

80

N
10

log (p(N ))
10 10

log (p(N ))

0 -1 -2 -3 -4
0
(c)
0 -1 -2 -3 -4
0
(f)

10

10

uniform, d = 100

50

100

N
10

sparse, d = 2000

50

100

150

N
10

Figure 1: Distribution of N10 for cosine similarity on (a­c) iid uniform and (d­f) skewed sparse random data with varying dimensionality (in c and f the vertical axis is in log scale).

oh15, tf-idf, d = 3182 0.4

0.3

p(N )
10

0.2

0.1

0 0 10 20 30 40 50 60

(a)

N
10

(b)

p(N )
10

wap, tf-idf, d = 8460 0.4
0.3
0.2
0.1
0 0 10 20 30 40 50 60 N 10

ohscal, tf-idf, d = 11465 0

-1

10

log (p(N ))

-2

10

-3

-4

-5 0 20 40 60 80 100 120

(c)

N
10

Figure 2: Distribution of N10 for cosine similarity on text data sets with increasing dimensionality (c has log-scale vertical axis).

Table 1: Text data sets. The top 19 data sets, used in form released by Forman [6], include documents from TREC collections, the OHSUMED collection, Reuters and Los Angeles Times news stories, etc. The dmoz data set consists of a selection of short Web-page descriptions from 11 toplevel categories from the dmoz Open Directory. The remaining reuters-transcribed and newsgroup data sets are available, e.g., from the UCI machine learning repository (for feasibility of analyzing pairwise distances, we split the 20-newsgroups data set into two parts). For all data sets, stop words were removed, and stemming was performed using the Porter stemmer.

Data set

n

d

Cls. SN10 SNS 10 CdNm10 CcNm10

ClNen110

ClNen1w0 BgN 10 CAV

fbis

2463 2000 17 1.884 2.391 0.083 0.440 0.188 0.219 0.323 0.400

oh0

1003 3182 10 1.933 2.243 0.468 0.626 0.210 0.212 0.295 0.322

oh10

1050 3238 10 1.485 1.868 0.515 0.650 0.185 0.124 0.415 0.552

oh15

913 3100 10 1.337 2.337 0.477 0.624 0.180 0.146 0.410 0.588

oh5

918 3012 10 1.683 2.458 0.473 0.662 0.154 0.124 0.345 0.587

re0

1504 2886 13 1.421 2.048 0.310 0.493 -0.016 -0.021 0.332 0.512

re1

1657 3758 25 1.334 1.940 0.339 0.587 0.075 0.071 0.305 0.385

tr11

414 6429 9 2.957 0.593 0.348 0.658 0.193 0.157 0.257 0.199

tr12

313 5804 8 2.577 0.841 0.364 0.620 0.199 0.180 0.323 0.326

tr21

336 7902 6 5.016 2.852 0.213 0.572 0.369 0.352 0.172 0.176

tr23

204 5832 6 1.184 0.392 0.052 0.503 -0.057 -0.034 0.239 0.281

tr31

927 10128 7 1.843 2.988 0.218 0.448 0.118 0.109 0.132 0.117

tr41

878 7454 10 1.257 1.413 0.377 0.586 0.110 0.092 0.133 0.288

tr45

690 8261 10 1.490 1.060 0.304 0.638 0.077 0.089 0.175 0.203

wap

1560 8460 20 1.998 1.753 0.479 0.598 0.209 0.203 0.364 0.304

la1s

3204 13195 6 1.837 2.277 0.398 0.498 0.161 0.165 0.296 0.570

la2s

3075 12432 6 1.462 1.876 0.419 0.496 0.203 0.207 0.268 0.531

ohscal

11162 11465 10 3.016 5.150 0.223 0.315 0.052 0.077 0.521 0.793

new3s

9558 26832 44 2.795 2.920 0.146 0.424 0.120 0.129 0.338 0.640

reuters-transcribed 201 3029 11 1.165 1.187 0.671 0.537 0.185 0.140 0.642 0.627

dmoz

3918 10690 11 2.212 2.853 0.443 0.433 -0.100 -0.249 0.613 0.866

mini-newsgroups 1999 7827 20 1.980 1.243 0.388 0.603 0.168 0.152 0.524 0.832

20-newsgroups1

9996 19718 20 2.930 3.571 0.187 0.411 0.125 0.133 0.378 0.850

20-newsgroups2

9995 19644 20 2.716 3.424 0.204 0.405 0.127 0.133 0.375 0.868

188

uniform, d = 3, CdNm10 = 0.032 20

uniform, d = 20, CdNm10 = 0.918 80

uniform, d = 100, CdNm10 = 0.930 150

N
10

15

60

100

N
10

N
10

10

40

50

5

20

0

0.6

0.7

0.8

0.9

1

(a)

Similarity with data set mean

0

0.7

0.8

0.9

1

(b)

Similarity with data set mean

0

0.75

0.8

0.85

0.9

0.95

(c)

Similarity with data set mean

sparse, d = 50, CdNm10 = 0.266 50 40 30 20 10

N
10

sparse, d = 200, CdNm10 = 0.775 80
60
40
20

N
10

sparse, d = 2000, CdNm10 = 0.927 150 100 50

N
10

0

0

0.2

0.4

0.6

0.8

(d)

Similarity with data set mean

0

0

0.2

0.4

0.6

0.8

(e)

Similarity with data set mean

0

0.35

0.4

0.45

0.5

0.55

(f)

Similarity with data set mean

Figure 3: Scatter plots of N10(x) (and its Spearman correlation denoted in the chart titles as CdNm10) against the cosine similarity of each vector to the data set center for (a­c) iid uniform and (d­f) sparse random data and various dimensionalities (denoted as d in chart titles).

To examine further the amplification caused by dimensionality, we compute separately for each of the two examined random data settings (dense-uniform and sparse-random) the distribution, S, of similarities between all vectors in the data set to the center of the data set. From each data set we select two vectors: x0 is selected to have similarity value to the data set center exactly equal to the expected value E(S) of the computed distribution S (i.e., at 0 standard deviations from E(S)), whereas x2 is selected to have higher similarity to the data set center, being equal to 2 standard deviations added to E(S) (we were able to select such vectors with negligible error compared to the similarities sought). Next, we compute the distributions of similarities of x0 and x2 to all other vectors, and the denote the means of these distributions x0 and x2 , respectively. Figure 4 plots, separately for the two examined cases of random data sets, the difference between the two similarity means, normalized (as explained in next paragraph) by dividing with the standard deviation, denoted all, of all pairwise similarities, i.e.: (x2 - x0 )/all. These figures show that, with increasing dimensionality, x2, which is more similar to the data center than x0, becomes progressively more similar (in relative terms) to all other vectors, a fact that demonstrates the aforementioned amplification.
One question that remains is: in high-dimensional spaces, why is it expected to have some vectors closer to the center and thus become hubs? In Section 3.4 we will analyze the property of the cosine similarity measure, referred to as concentration [7], which in this case states that, as dimensionality tends to infinity, the expectation of pairwise similarities between all vectors tends to become constant, whereas their standard deviation (denoted above as all) shrinks to zero. This means that the majority of vectors become about equally similar to each other, thus to the data center as well. However, high but finite dimensionalities, typical in IR, will result in a small but non-negligible standard deviation, which causes the existence of some vectors, i.e., the hubs, that are closer to the center than other vectors. These facts also clarify the aforementioned normalization by all, which comprises a way to account for concentration (shrinkage of all) and meaningfully compare x0 and x2 across dimensionalities.
Finally, we need to examine the relation between hubness and additional characteristics of text data sets, such as sparsity and the skewed distribution of term frequencies in "long" documents (see Section 2.1). Since Figures 1 and 3 demonstrate hubness for both dense and sparse random data sets, sparsity on its own should

Norm. diff. between means

Norm. diff. between means

uniform
0.9
0.8
0.7
0.6

sparse
0.75 0.7
0.65

0.5

0

20

40

60

80 100

d

0

500

1000 1500 2000

d

Figure 4: Difference between the normalized means of two distributions of similarity with a point which has: (1) the expected similarity with the data center, and (2) similarity two standard deviations greater; for uniform (left) and sparse random data (right).

not be considered as a key factor. Regarding the skewness in the distribution of term frequencies, we can consider two cases [14]: (a) more (in number) distinct terms, and (b) higher (in value) term frequencies. For the sparse data set with d = 2, 000 dimensions (Fig. 3(f)) we measured the correlations of N10 with the number of nonzero simulated "terms" of a vector and with the total sum of term weights of a vector, and found both to be weak, 0.142 for case (a) and 0.19 for case (b), in comparison with correlation 0.927 (see title of Fig. 3(f)) between N10 and the similarity with the data set mean, which has been described as the main factor behind hubness. The weak correlations in cases (a) and (b), which will also be verified with real data (Section 3.2), are expected because normalization schemes (cosine in this example) are able to reduce the impact of long documents. What is, thus, important to note is that, even if the correlations of cases (a) and (b) are completely eliminated with another normalization scheme, the hubness phenomenon will still be present, since it is primarily caused by the inherent properties of high-dimensional vector space.
3.2 Hub Formation in Real Data
In the previous discussion we have used synthetic data that allow the control of important parameters. To verify the findings with real text data, we need to take into account two additional factors: (1) real data sets usually contain dependent attributes, and (2) real data sets are usually clustered, that is, documents are organized into groups produced by a mixture of distributions instead of originating from one single distribution.
To examine the first factor (dependent attributes), we adopt the approach from [7] used in the context of lp-norm concentration. For each data set we randomly permute the elements within every

189

attribute. This way, attributes preserve their individual distributions, but the dependencies between them are lost and the intrinsic dimensionality of data sets increases [7]. In Table 1 we give the skewness, denoted SNS10 , of the modified data. In most cases SNS10 is considerably higher than SN10 , implying that hubness depends on the intrinsic rather than embedding (full) dimensionality.
To examine the second factor (many groups), for every data set we measured: (i) Spearman correlation, denoted by CdNm10 , of Nk and the similarity with the data set center, and (ii) correlation, denoted by CcNm10 , of Nk and the similarity with the closest group center. Groups are determined using K-means clustering, where the number of clusters was set to the number of document categories of the data set.2 In most cases, CcNm10 is much stronger than CdNm10 . Thus, generalizing the conclusion of Section 3.1 to the case of real data, hubs are more similar, compared with other vectors, to their respective cluster centers.
Regarding long documents (see Section 3.1), for each data set we computed the correlation between Nk and the number of nonzero term weights for a document, denoted by ClNen110 , and also the correlation of Nk with the sum of term weights of a document, denoted by ClNen1w0 . The corresponding columns of Table 1 signify that these correlations are weaker or nonexistent (on occasion even negative) compared to the correlation with the proximity to the closest cluster mean (CcNm10 ). The above observations are in accordance with the conclusions from the end of Section 3.1.
3.3 Effect of Dimensionality Reduction
The attribute shuffling experiment in Section 3.2 suggested that hubness is actually related more to the intrinsic dimensionality of data. We elaborate further on the interplay of skewness and intrinsic dimensionality by considering dimensionality reduction (DR) techniques. The main question is whether DR can alleviate the issue of hubness altogether.
We examined the singular value decomposition (SVD) dimensionality reduction method, which is widely used in IR through latent semantic indexing. Figure 5 depicts for several real data sets from Table 1 the relationship between the percentage of features (dimensions) maintained by SVD, and the skewness SNk (k = 10). All cases exhibit the same behavior: SNk stays relatively constant until a small percentage of features is left, after which it suddenly drops. This is the point where the intrinsic dimensionality is reached, and further reduction may incur loss of information. This observation indicates that, when the number of maintained features is above the intrinsic dimensionality, dimensionality reduction cannot significantly alleviate the skewness of k-occurrences, and thus hubness. This result is useful in most practical cases, because moving bellow the intrinsic dimensionality may cause loss of valuable information from the data.
3.4 Concentration of Cosine Similarity
Distance concentration, which has been examined mainly for lp norms [7], refers to the tendency of the ratio between some notion of spread (e.g., standard deviation) and some notion of magnitude (e.g., the mean) of the distribution of all pairwise distances (or, equivalently, the norms) within a data set to converge to 0 as dimensionality increases.
Hereby, we examine concentration in the context of cosine similarity that is widely used in IR. We will prove the concentration of cosine similarity by considering two random d-dimensional vectors p and q with iid components. Let cos(p, q) denote the cosine simi-
2We report averages of CcNm10 over 10 runs of K-means clustering with different random seeding, in order to reduce the effects of chance.

SVD 2

1.5

S
N
1 0

1

tr31

oh5

tr45

0.5

re0

oh15

0 10 20 30 40 50 60 70 80 90 100 Features (%)

Figure 5: Skewness of N10 at the percentage of features kept by SVD.

larity between p and q, defined in Eq. 1.3 Note that our examination

does not differentiate between sparse and dense data (concentration

occurs in both cases).

cos(p, q) = pT q

(1)

pq

From the extension of Pythagoras' theorem we have Eq. 2 that

relates cos(p, q) with the Euclidean distance between p and q.

cos(p, q) =

p 2+ q 2- p-q 2 2p q

(2)

Define the following random variables: X = p , Y = q ,

and Z = p - q . Since p and q have iid components, we assume

that X and Y are independent of each other, but not of Z. Let C

be the random variable that denotes the value of cos(p, q). From

Eq. 2, with simple algebraic manipulations and substitution of the

norms

with

the

corrCes=pon21di,,ngXYra+ndoXYm-varXZiaY2ble«s,

we

obtain

Eq.

3. (3)

Let E(C) and V(C) denote the expectation and variance of C, respectively. An established way [7] to demonstratp e concentration is by examining the asymptotic relation between V(C) and E(C) when dimensionality d tends to infinity. To express this asymptotic relation, we first need to express the asymptotic behavior of E(C) and V(C) with regards to d. Since, from Eq. 3, C is related to functions of X, Y , and Z, we start by studying the expectations and variances of these random variables.

THEO`REM 1 (FR´ANÇOIS ET AL. [7], ADAPTED). limd E(X)/ d = const , and limd V(X) = const . The same holds for random variable Y .

CORO`LLARY 1. ´ limd E(Z)/ d = const , and limd V(Z) = const .
PROOF. Follows directly from Theorem 1 and the fact that, since vectors p and q have iid components, vector p-q also has iid components.

COROLLARY 2. limd(E(X2)/d) = const , and limd(V(X2)/d) = const . The same holds for random variables Y 2 and Z2.
PROOF. From Theorem 1 and the equation E(X2) = V(X)+E(X)2 it follows that limd(E(X2)/d) = const . The same holds for E(Y 2) and, taking into account Corollary 1, for E(Z2). By using the delta method to approximate the moments of a function of a random variable with Taylor expansions [3], we have V(X2)  (2E(X))2 V(X). From Theorem 1 it now follows that limd(V(X2)/d) = const . Analogous derivations hold for V(Y 2) and V(Z2).
3Henceforth, · denotes the Euclidean (l2) norm.

190

uniform
1

sparse
1

Cosine similarity Cosine similarity

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0

20

40

60

80 100

d

0

0

500

1000

1500

2000

d

Figure 6: Concentration of cosine similarity for uniform (left) and sparse random data (right).

pBased on the above results, the following two theorems show that V(C) reduces asymptotically to 0, while E(C) asymptotically
remains constant (proofpsketches are given in the Appendix). THEOREM 2. lim V(C) = 0.
d
THEOREM 3. lim E(C) = const . d
Figure 6 illustrates these findings for the uniform and sparse random data used in previous sections. With respect to the distribution of all pairwise similarities, the plots include, from top to bottom: maximal observed value, mean value plus one standard deviation, the mean value, mean value minus one standard deviation, and minimal observed value. The figures illustrate that, with increasing dimensionality, expectation becomes constant and variance shrinks.
It is worth noting that the concentration of cosine similarity results from different reasons than the concentration of Euclidean (l2) distance. For the latter, its standard deviation converges to a constant [7], whereas its expectation asymptotically increases with d. Nevertheless, in both cases the relative relationship between the standard deviation and the expectation is similar.
4. IMPACT OF HUBNESS ON IR
4.1 Hubness and the Cluster Hypothesis
This section examines the ways that hubness affects VSM towards the main objective of IR, which is to return relevant results for a query document. We consider the commonly examined case of documents that belong to categories (e.g., news categories, like sport or finance). However, a similar approach can be followed for other sources of information about documents, such as indication of their relevance to a set of predefined queries. In the presence of information about documents as in the form of categories, k-occurrences can be distinguished based on whether category labels of neighbors match. We define the number of "bad" k-occurrences of document vector x  D, denoted BN k(x), as the number of vectors from D for which x is among the first k nearest neighbors and the labels of x and the vectors in question do not match. Conversely, GN k(x), the number of "good" k-occurrences of x, is the number of such vectors where labels do match. Naturally, for every x  D, Nk(x) = BN k(x) + GN k(x).
We define BgN k as the sum of allP"bad" k-occurrences of a data set normalized by dividing it with x Nk(x) = kn. The motivation behind the measure is to express the total amount of "bad" k-occurrences within a data set. Table 1 includes BgN 10. "Bad" hubs, i.e., documents with high BN k, are of particular interest to IR, since they affect the precision of retrieval more severely than other documents by being among the k nearest neighbors (i.e., in the result list) of many other documents with mismatching categories. To understand the origins of "bad" hubs in real data, we rely on the notion of the cluster hypothesis [15]. This hypothesis will be approximated by the cluster assumption from semi-supervised learning [4], which roughly states that most pairs of vectors in a high density region (cluster) should belong to the same category.

To measure the degree to which the cluster assumption is violated in a particular data set, we define a simple cluster assumption violation (CAV) coefficient as follows. Let a be the number of pairs of documents which are in different category but in the same cluster, and b the number of pairs of documents which are in the same category and cluster. Define CAV = a/(a + b), which gives a number in range [0, 1], higher if there is more violation. To reduce the sensitivity of CAV to the number of clusters (too low and it will be overly pessimistic, too high and it will be overly optimistic), we choose the number of clusters to be 3 times the number of categories of a data set. As in Section 3.2, we use K-means clustering.
For all examined text data sets, we computed the Spearman correlation between BgN 10 and CAV, and found it strong (0.844). In contrast, BgN 10 is not correlated with d nor with the skewness of N10 (measured correlations are -0.03 and 0.109, respectively). The latter indicates that high intrinsic dimensionality and hubness are not sufficient to induce "bad" hubs. Instead, we can argue that there are two, mostly independent, factors at work: violation of the cluster assumption on one hand, and hubness induced by high intrinsic dimensionality on the other. "Bad" hubs originate from putting the two together; i.e., the consequences of violating the cluster assumption can be more severe in high dimensions than in low dimensions, not in terms of the total amount of "bad" koccurrences, but in terms of their distribution, since strong hubs are now more prone to "pick up" bad k-occurrences than non-hubs.

4.2 A Similarity Adjustment Scheme
Based on the aforementioned conclusions about "bad" hubness, in this section we propose and evaluate a similarity adjustment scheme with the objective to show how its consideration can be used successfully for improving the precision of a VSM-based IR system. Our main goal is not to compete with the state-of-the art methods for improving the precision and relevance of results obtained using baseline methods, but rather to demonstrate the practical significance of our findings in IR applications, and the need to account for hubness. Thus, the elaborate examination of more sophisticated methods is addressed as a point of future work.
Let D denote a set of documents, and Q a set of queries independent of D. We will also refer to D as the "training" set, and to Q as the "test" set, and by default compute Nk, BN k and GN k on D. We adjust the similarity measure used to compare document vector x  D with query vector q  Q by increasing the similarity in proportion with the "goodness" of x (GN k(x)), and reducing it in proportion with the "badness" of x (BN k(x)), both relative to the total hubness of x (Nk(x)), for a given k:
sima(x, q) = sim(x, q)+sim(x, q)(GN k(x)-BN k(x))/Nk(x) .

The net effect of the adjustment is that strong "bad" hub documents

become less similar to queries, reducing the chances of the docu-

ment to be included in a list of retrieved results. To prevent doc-

uments from being excluded from retrieval too rigorously, the ad-

justment scheme also considers their "good" side and awards the

presence of "good" k-occurrences in an analogous manner.

We experimentally evaluated the improvement gained by the pro-

posed scheme compared to the standard tf-idf representation and

cosine similarity (all computations involving hubness use k = 10),

through 10-fold cross-validation on data sets from Table 1. First,

we focus on the impact of the adjustment scheme on the error introduced to the retrieval system by the strongest "bad" hubs. Let W p%

be the set of the top p% of documents with highest BN k, as de-

termined

from

the

training

set,

and

let

BN

test k

(x)

and

Nktest (x)

be the ("bad") k-occurrences of document x from the training set,

as determined from similarities with documents from the test set.

191

Precision@m (%) Precision@m (%) Precision@m (%) Precision@m (%)

oh10, tf-idf + cosine

66

With sim. adj.

No sim. adj.

64

62

60

58 1 2 3 4 5 6 7 8 9 10 m

re0, tf-idf + cosine 80
With sim. adj. No sim. adj. 75
70
65 1 2 3 4 5 6 7 8 9 10 m

tr41, tf-idf + cosine

93

92

With sim. adj.

No sim. adj.

91

90

89

88

87

86 1 2 3 4 5 6 7 8 9 10 m

la1s, tf-idf + cosine

82

80

With sim. adj. No sim. adj.

78

76

74

72

70

1 2 3 4 5 6 7 8 9 10 m

Figure 7: Precision at the number of retrieved results m, measured by 10-fold cross-validation.

Table 2: Retrieval "badness" of 5% of the strongest "bad" hubs (B5%) and precision at 10 (P@10), with (columns labeled by suffix a) and without similarity adjustment (in %).

Data set
fbis oh0 oh10 oh15 oh5 re0 re1 tr11 tr12 tr21 tr23 tr31 tr41 tr45 wap la1s la2s ohscal new3s reuters-transcribed dmoz mini-newsgroups 20-newsgroups1 20-newsgroups2

B5a%
47.73 49.47 64.17 56.21 51.63 54.77 59.47 44.86 65.60 23.89 45.03 44.36 35.34 37.40 54.57 49.01 52.17 66.17 50.54 68.10 69.92 66.22 55.18 57.48

B5%
68.58 55.93 70.58 68.71 56.68 67.78 69.37 43.38 64.15 27.65 52.50 55.50 49.04 52.05 60.44 57.86 61.67 72.38 65.77 72.34 75.37 70.86 63.59 65.09

P@10a
72.10 71.85 61.97 62.96 67.85 69.58 72.04 74.70 69.23 83.70 75.94 88.43 87.81 84.08 65.18 72.94 75.54 51.01 69.00 38.55 40.68 49.39 63.89 64.22

P@10
67.59 70.03 58.58 59.19 64.84 66.41 68.98 74.06 67.11 82.90 75.60 86.33 86.31 81.88 63.42 69.89 72.69 47.80 65.66 36.81 38.24 47.16 61.27 61.50

We define as Bp% =

`thPe total
xW

p"%baBdNnetksess"t (oxf)´th/e`sPtroxngWespt%pN%kteosft

"ba´d" hubs (x) , where

normalization with Nktest is done to keep the measure in the [0, 1]

range. The Bp% measure focuses on the contribution of "bad" hubs

to erroneous retrieval of false positives. Table 2 shows Bp% on the same p = 5% of "bad" hubs before

and after applying similarity adjustment. It can be seen that for the

majority of data sets, the adjustment scheme greatly reduces the

amount of erroneous retrieval caused by "bad" hubs.

To illustrate the improving effect of the adjustment scheme on

the precision of retrieval, Fig. 7 plots, for several data sets from Ta-

ble 1, the precision of 10-fold cross-validation against the varying

number (m) of documents retrieved as results.

Moreover, Table 2 also shows 10-fold cross-validation precision

at 10 retrieved results, demonstrating the improvement of precision introduced by similarity adjustment on all data sets.4

4.3 Advanced Representations

The issues examined in previous sections relate to characteristics of VSM that are existing in most of its variations, particularly the high dimensionality. To examine the generality of our findings, we

4We verified the statistical significance of improvement of precision using the t-test at 0.05 significance level on all data sets (except tr11 and tr23). The motivation for selecting m = 10 results to report precision is the common use of this number by retrieval systems. We obtained analogous results for various other values of m.

consider the Okapi BM25 weighting scheme [12], which consists

of separate weightings for terms in documents and terms in queries.

The comparison between document and query can then be viewed

as taking the (unnormalized) dot-product of the two vectors. We

examine the following basic variant of the BM25 weighting. Providing that n is the total number of documents in the collection,

df the term's document frequency, tf the term frequency, dl the

document length (the total number of terms), and avdl the average

document length, term weights of documents are given by

log

n

- df df +

+ 0.5 0.5

·

(k1 + 1)tf

k1

((1

-

b)

+

b

dl avdl

)

+

tf

,

while the term weights of queries are (k3 + 1)tf /(k3 + tf ), where k1, b, and k3 are parameters for which we take the default values k1 = 1.2, b = 0.75, and k3 = 7 [12].
The existence of hubness within the BM25 scheme is illustrated in Figure 8, which plots the distribution of Nk (k = 10) for several real text data sets from Table 1 represented with BM25. Figure 9

demonstrates the improvement of precision obtained through the

similarity adjustment scheme described in Section 4.2, when BM25

representation is considered.

5. CONCLUSION
We have described the tendency, called hubness, of VSM-based models to produce some documents that are retrieved surprisingly more often than other documents in a collection. We have shown that the major factor for hubness is the high (intrinsic) dimensionality of vector spaces used by such models. We described the mechanisms from which the phenomenon originates, investigated its interaction with dimensionality reduction, and demonstrated its impact on IR by exploring its relationship with the cluster hypothesis.
In order to simplify analysis by allowing quantification of the degree of violation of the cluster hypothesis, in this research we focused on data containing category labels. In future work we plan to extend our evaluation to larger data collections where relevance judgements are provided in a non-categorical fashion. Also, we will consider in more detail advanced models like BM25 [12] and pivoted cosine [14]. Finally, the similarity adjustment scheme described in this paper was proposed primarily with the intent of demonstrating that hubness should be considered for the purposes of IR. In future research we intend to explore other strategies for assessing and mitigating the influence of ("bad") hubness in IR.

Acknowledgments. The second author acknowledges the partial co-funding of his work through European Commission FP7 project MyMedia under grant agreement no. 215006.

6. REFERENCES
[1] J.-J. Aucouturier and F. Pachet. A scale-free distribution of false positives for a large class of audio similarity measures. Pattern Recogn., 41(1):272­284, 2007.
[2] A. Berenzweig. Anchors and Hubs in Audio-based Music Similarity. PhD thesis, Columbia University, New York, NY, USA, 2007.
[3] G. Casella and R. L. Berger. Statistical Inference. Duxbury, second edition, 2002.

192

p(N )
10

oh10, BM25 0.3
0.2
0.1

p(N )
10

re0, BM25 0.3
0.2
0.1

log (p(N ))
10 10

tr41, BM25 0
-1
-2

log (p(N ))
10 10

la1s, BM25 0
-1
-2
-3

0

0

20

40

60

N
10

0 0 10 20 30 40 50 60 N 10

-3

-4

0

50

100

150

200

0

N
10

100

200

300

N
10

Figure 8: Distribution of N10 for real text data sets in the BM25 representation.

oh10, BM25

76

With sim. adj.

74

No sim. adj.

72

70

68

66

64 1 2 3 4 5 6 7 8 9 10 m

Precision@m (%)

re0, BM25

80

With sim. adj.

78

No sim. adj.

76

74

72

70
1 2 3 4 5 6 7 8 9 10 m

Precision@m (%)

tr41, BM25

95

With sim. adj.

93

No sim. adj.

91

89

87

85 1 2 3 4 5 6 7 8 9 10 m

Precision@m (%)

la1s, BM25

82

With sim. adj.

No sim. adj. 80

78

76

74
1 2 3 4 5 6 7 8 9 10 m

Figure 9: Precision at the number of retrieved results m, measured by 10-fold cross-validation, for the BM25 representation.

Precision@m (%)

[4] O. Chapelle, B. Schölkopf, and A. Zien, editors. Semi-Supervised Learning. The MIT Press, 2006.
[5] P. Erdos and A. Rényi. On random graphs. Publ. Math-Debrecen, 6:290­297, 1959.
[6] G. Forman. BNS feature scaling: An improved representation over TF-IDF for SVM text classification. In Proc. ACM Conf. on Inform. and Knowledge Management (CIKM), pages 263­270, 2008.
[7] D. François, V. Wertz, and M. Verleysen. The concentration of fractional distances. IEEE T. Knowl. Data En., 19(7):873­886, 2007.
[8] L. A. Goodman. On the exact variance of products. J. Am. Stat. Assoc., 55(292):708­713, 1960.
[9] A. Hicklin, C. Watson, and B. Ulery. The myth of goats: How many people have fingerprints that are hard to match? Technical Report 7271, National Institute of Standards and Technology, USA, 2005.
[10] A. Nanopoulos, M. Radovanovic´, and M. Ivanovic´. How does high dimensionality affect collaborative filtering? In Proc. ACM Conf. on Recommender Systems (RecSys), pages 293­296, 2009.
[11] M. Radovanovic´, A. Nanopoulos, and M. Ivanovic´. Nearest neighbors in high-dimensional data: The emergence and influence of hubs. In Proc. Int. Conf. on Machine Learning (ICML), pages 865­872, 2009.
[12] S. Robertson. Threshold setting and performance optimization in adaptive filtering. Inform. Retrieval, 5(2­3):239­256, 2002.
[13] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613­620, 1975.
[14] A. Singhal. Term Weighting Revisited. PhD thesis, Cornell University, Ithaca, NY, USA, 1997.
[15] C. J. van Rijsbergen. Information Retrieval. Butterworths, second edition, 1979.

APPENDIX

Proof sketch for Theorem 2. From Equation 3 we get:

X

Y

Z2

4V(C) = V( ) + V( ) + V( ) +

(4)

Y

X

XY

XY

X Z2

Y Z2

2Cov( , ) - 2Cov( , ) - 2Cov( , ).

YX

Y XY

X XY

For the first term, using the delta method [3] and the fact that X and Y are independent:

V(

X Y

)



V(X) E2 (Y )

+

E2 (X ) E4(Y )

V(Y

),

from

which

it

follows,

based

on

Theorem

1,

that

V(

X Y

)

is

O(1/d)

(for

brevity,

we

resort

to

oh

notation

in

this

proof

sketch).

In

the

same

way,

V(

Y X

)

is

also

O(1/d).

For the third term of Equation 3, again from the delta method:

Z2

V(Z2)

V( ) XY



E2(X)E2(Y ) -

(5)

2E(Z 2 ) E3 (X )E3 (Y

Cov(Z2, XY )

)

+

E2 (Z 2 ) E4 (X )E4 (Y

) V(XY

).

In Equation 5, based on Theorem 1 and Corollary 2, the first term is O(1/d). Since V(XY ) = E2(X)V(Y ) + E2(Y )V(X) + V(X)V(Y ) [8], it follows that V(XY ) is O(d), thus the third term is O(1/d), too. Cov(Z2, XY ) is O(d), because from the definition of the correlation coefficient we have |Cov(Z2, XY )|  max(V(Z2), V(XY )). Thus, the second term of Equation 5 is O(1/d). Since all

its

terms

are

O(1/d),

V(

Z2 XY

)

is

O(1/d).

Returning to Equation 4 and its fourth term, from the definition of the corre-

lation

coefficient

it

follows

that

|Cov(

X Y

,

Y X

)|



max(V(

X Y

),

V(

Y X

)),

thus

Cov(

X Y

,

Y X

)

is

O(1/d).

For

the

fifth

term,

again

from

the

definition

of

the

correla-

tion

coefficient

we

have

|Cov(

X Y

,

Z2 XY

)|



max(V(

X Y

),

V(

Z2 XY

)).

Based

on

the

previously

expressed

V(

X Y

)

and

V(

Z2 XY

),

we

get

that

Cov(

X Y

,

Z2 XY

)

is

O(1/d).

Similarly,

the

sixth

term,

Cov(

Y X

,

Z2 XY

),

is

O(1/d).

Hqaving determined

all

6 terms,

4V(C), thus V(C), is O(1/d). It follows that lim V(C) = 0. 2
d

Proof sketch for Theorem 3. From Equation 3 we get:

X

Y

Z2

2E(C) = E( ) + E( ) - E( ).

(6)

Y

X

XY

For the

E(

X Y

)

first term,



E(X) E(Y )

using (1 +

the delta V(Y )).

method [3] and the fact Based on the limits for

that X and Y E(X)/ d,

are independent: E(Y )/ d, and

V(Y

)

in

Theorem

1,

it

follows

that

limd

E(

X Y

)

=

const .

For

the

second

term,

in

the

same

way,

limd

E(

Y X

)

=

const .

For the third term in Equation 6, again from the delta method:

Z2

E(Z2)

Cov(Z2, XY )

E(Z 2 )

E( XY

)

E(X)E(Y )

-

E2(X)E2(Y )

+

E3(X)E3(Y ) V(XY ).

(7)

In Equation 7, based on the limits derived in Theorem 1 and Corollary 2, it fol-

lows that the limit of the first term, limd

E(Z2 ) E(X)E(Y )

=

const. The limit of the

second term in ting limd

ECqouva(tZido22n,X7 Yca)n,,beliemxpdres sedEb2y(Xmd)u2Elt2ip(lYyi)n«g - an1d.

dividing by d2, getFrom the definition

of the correlation coefficient we have:

dl im

Cov(Z 2 , d2

X

Y

)



s  lim
d

s

V(Z2)

d2

lim
d

V(XY ) d2

.

From V(XY ) = E2(X)V(Y ) + E2(Y )V(X) + V(X)V(Y ) [8], based on

Theorem 1 and Corollary 2, we find that both limits on the right side are equal to 0,

implying

that

limd

Cov(Z2 ,XY ) d2

=

0.

On the other hand, from Theorem 1

we

have

limd

E2 (X)E2 (Y ) d2

= const .

The preceding two limits provide us

with the limit for the second term of Equation 7:

limd

Cov(Z2 ,XY ) E2 (X)E2 (Y )

=

0.

Finally, for the third term of Equation 7, again based on the limits given in Theo-

rem 1 and Corollary 2 and the previously derived limit for V(XY )/d2, we obtain

limd

E(Z2 ) E3 (X)E3 (Y

)

V(X Y

)

=

0.

Summing up all partial limits, it follows that limd 2E(C) = const , thus limd E(C) = const . 2

193

On Statistical Analysis and Optimization of Information Retrieval Effectiveness Metrics
Jun Wang and Jianhan Zhu
Department of Computer Science, University College London, UK
wang.jun@acm.org, j.zhu@cs.ucl.ac.uk

ABSTRACT
This paper presents a new way of thinking for IR metric optimization. It is argued that the optimal ranking problem should be factorized into two distinct yet interrelated stages: the relevance prediction stage and ranking decision stage. During retrieval the relevance of documents is not known a priori, and the joint probability of relevance is used to measure the uncertainty of documents' relevance in the collection as a whole. The resulting optimization objective function in the latter stage is, thus, the expected value of the IR metric with respect to this probability measure of relevance. Through statistically analyzing the expected values of IR metrics under such uncertainty, we discover and explain some interesting properties of IR metrics that have not been known before. Our analysis and optimization framework do not assume a particular (relevance) retrieval model and metric, making it applicable to many existing IR models and metrics. The experiments on one of resulting applications have demonstrated its significance in adapting to various IR metrics.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H3.1Content analysis and Indexing; H.3.3 Information Search and Retrieval
General Terms
Algorithms, Experimentation, Measurement, Performance
1. INTRODUCTION
In Information Retrieval Modelling, the main efforts have been devoted to, for a specific information need (query), automatically scoring individual documents with respect to their relevance states. Representative examples include the Probabilistic Indexing model that studies how likely a query term is assigned to a relevant document [17], the RSJ model that derives a scoring function on the basis of the log-ratio of probability of relevance [20], to name just a few. And yet, given the fact that in many practical situations relevance information is not steadily available, major developments have shifted their focus to estimating text statistics in the documents and queries and then building up the link through these statistics[12, 21, 34]. For example, scoring functions
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

such as TF·IDF, Vector Space Model, and the Divergence from Randomness (DFR) model [1] have been developed [16]. A practical approximation of the RSJ model led to the popular BM25 scoring function [21]. Another direction in probabilistic modelling was to build a "language model" of a document and assess its likelihood of generating a given query [34]; a query language model is also covered under the Kullback-Leibler divergence based loss function [15].
Despite the efforts for retrieval, when in the evaluation phase, many IR tasks have evaluation criteria that go beyond simply counting the number of relevant documents in a ranked list. Measuring IR effectiveness by different metrics is critical because, for different retrieval goals, we need to capture different aspects of retrieval performance. In the case where the preference goes strongly towards earlyretrieved documents, MRR (Mean Reciprocal Rank) is a good measure [28], whereas if we try to capture a broader summary of retrieval performance, MAP (Mean Average Precision) becomes suitable [13]. Thus, there is a gap between the underlying (ranking) decision process of retrieval models and the final evaluation criterion used to measure success in a task. Ideally, it is desirable to have retrieval systems adapted to the specific IR effectiveness metrics.
In fact, IR researchers have already started to explore the opportunity. One extreme case is learning to rank ; it directly constructs a document ranking model from training data, bypassing the step of estimating the relevance states of individual documents [8]. Under this paradigm, some attempts have been made to directly optimizing IR metrics such as NDCG (Normalized Discounted Cumulated Gain) and MAP [23, 33]. However, it is known that some evaluation metrics are less informative than others [4]. As argued in [32], some IR metrics thus do not necessarily summarize the (training) data well; if we begin optimizing IR metrics right from the data, the statistics of the data may not be fully explored and utilized.
A somewhat opposite direction is to focus still on designing a scoring function of a document, but with the acknowledgement of various retrieval goals and the final rank context. The "less is more" model proposed in [10] is one of the examples. By treating the previously retrieved documents as non-relevant when calculating the relevance of documents for the current rank position, the algorithm is shown to be equivalent to maximizing the Reciprocal Rank measure. In [35], a more general and flexible treatment in this direction is proposed. In the framework, Bayesian decision theory is applied to incorporate various ranking strategies through predefined loss functions. Despite its generality, the resulting IR models, however, lack the ability of directly incorporating IR metrics into the rank decision.
In this paper, we argue that regarding the retrieval task solely as either optimizing IR metrics or deriving a (rele-

226

Figure 1: The two distinct stages in the statistical document ranking process.
vance) scoring function presents a partial view of the underlying problem; a more unified view is to divide the retrieval process into two distinct stages, namely relevance prediction and ranking decision optimization stages, and solve them sequentially. In the first stage, the aim is to estimate the relevance of documents as accurate as possible, and summarize it by the joint probability of documents' relevance. Only in the second stage is the rank preference specified, possibly by an IR metric. The rank decision making is a stochastic one due to the uncertainty about the relevance. As a result, the optimal ranking action is the one that maximizes the expected value of the IR metric. We shall show that statistical analysis of the expected value of IR metrics gives insight into the properties of the metrics. One of the findings is that AP (Average Precision) encourages documents whose relevance is positively correlated with previous retrieved documents, while RR (Reciprocal Rank) does otherwise. It follows that if a rank achieves superior results on AP, it must pay with inferiority on RR. Apart from a theoretical contribution, our experiments on TREC data sets demonstrate the significance of our probabilistic framework.
The remainder of the paper is organized as follows. We first establish our optimization scheme, and study major expected IR metrics and practical issues. We then provide an empirical evaluation, and finally conclude our work.
2. STATISTICAL RANKING MECHANICS
In this section, we present the framework of optimizing IR metrics in the situation where the relevance of documents is unknown. To keep our discussion simple, we consider binary relevance, while graded relevance can be extended similarly. Given an information need, let us assume each document in the corpus is either relevant or non-relevant. We denote them jointly as a vector r  (r1, ..., rk, ..., rN )  {0, 1}N , where k = {1, ..., N }, N denotes the number of documents. rk = 1 if document k is relevant; otherwise rk=0.
Our view is the following: firstly the IR model should focus on estimating the relevance of documents. The relevance in this stage is the "true" topical relevance [18], different from the user "perceived" relevance that will be qualified in the next stage. In statistical modelling, we assign to every possible relevance state r a number p(r|q), which we interpret as the probability that a user, who issues query q, will find the documents' relevance states as r. Given the observation so far (the query, the user's interaction etc), the posterior probability p(r|q) presents our (or the IR model's) belief about the relevance states of the documents in the collection as a whole. Note that we use the joint distribution of relevance instead of the marginal distribution p(rk|q) to cover the dependency of relevance among documents.
It is argued that only in the second stage does the retrieval model make a ranking decision under the uncertainty specified by the joint probability of relevance. To formulate this, we follow the terminology in natural language processing [6]; a ranking order is represented by a vector a  (a1, ..., ai, ..., aN ), where ai  {1, ..., N }. If a document k is in rank position i, then ai = k. The retrieval task is, thus, to find an optimal rank order a to maximize a certain retrieval objective. Formally, an IR metric (measure) m(a|r) is defined as a score function of a given r. A good metric should be able to measure the user's gain or utility of a rank order a when the true relevance states of all the documents, r, are known. m(a|r) can be also seen as a measure

of the user's perceived relevance in the context of a ranked list. For example, Precision concerns a solution that finds relevant documents as many as possible in the list regardless of their order, while Reciprocal Rank (inverse of the rank of the first relevant document retrieved) makes sure to retrieve the first relevant document as early as possible regardless of the rank positions of remaining relevant documents.
Given the fact that different IR effectiveness metrics are useful for capturing different aspects of retrieval quality, it is desirable to optimize a with respect to the specific metric m. Bayesian decision theory suggests that the optimal rank order ^a is obtained by maximizing the expected IR metric:

X

^a = argmax Er[m|q] = argmax

a

a

m(a|r)p(r|q), (1)

r{0,1}N

where E[·|q] denotes an expectation with respect to a conditional distribution p(·|q). The subscript r indicates it is averaged over all possible r. Eq. (1) shows that: firstly the true relevance state of documents, r, is generated from probability p(r|q) estimated by an IR model. Under the relevance state r, the score of a given rank order a is calculated. Er[m|q], the expected score of the rank order, is the one averaging over all possible relevance states of r. Finally, the optimal rank order is chosen by maximizing Er[m|q].
Although the formulation can be thought of as a special instantiation of the general retrieval decision framework in [15, 35], our underlying idea and development are quite different from their instantiated models. The advantage is that, as illustrated in Figure 1, in our framework, the IR metric (utility) relies only on the true relevance and ranking order, while (relevance) IR models are for estimating the relevance. Decoupling them is essential to directly use any retrieval metric and plug it into the optimization procedure. More discussion can be found in Section 4.
To obtain Eq. (1), we analyze the expected IR metrics Er[m|q] in Section 2.1 and present a practical implementation and maximization (search) method in Section 2.2.
2.1 Analysis of Expected IR metrics
2.1.1 Expected Average Precision
Average Precision (AP) is a widely-adopted metric. For each query, it is the average of the precision scores obtained across rank positions where each relevant document is retrieved; relevant documents that are not retrieved receive a precision score of zero [7]. The metric, in fact, is the area under the Precision-Recall curve, capturing a broad summary of retrieval performance with a single value [4].
By definition, the Average Precision measure is as follows:

mA(a|r)



1 NR

M
X rai
i=1

(1

+

Pi-1
j=1
i

raj ) ,

(2)

where M  N (Pij-=11 raj  0 when i=1). NR is the num-

ber of relevant documents, and its expected value equals

PN
i=1

p(rai

=

1),

the

summation

of

the

marginal

probability

of relevance. For simplicity, we define p(rai = 1)  p(Rai )

in the remainder of the paper. Because during retrieval r

is hidden, mA(a|r) cannot be calculated exactly. Instead,

its expected value under the joint probability of relevance

is derived by making use of the properties of expectation

(Throughout this paper the expectation is all conditioned

on a given query q and with respect to r. For simplicity, we

drop the subscript r and notation q in E[·] from now on):

E[mA] = X p(NR|q)E[mA|NR]

(3)

NR

227

Weight Ratio Weight Ratio Weight Ratio

1

0.95

0.9 1

0.85 0.8

0.8 0.6

0.75 0.4

0.7 0.2

0.65

0

1 0.8

0.6 10

0.6 0.4

8 0.55
6

0.2

4

p(r)

0

2

i

0.5

(a)

0.9

0.8

1

0.7

0.8

0.6

0.6

0.5

0.4 0.4

0.2 0.3

0

1

0.2

0.8

10

0.6 0.4

8

0.1

6

0.2

4

p(r)

0

2

i

0

(b)

1

0.9

0.8

0.7

0.6

0.5

0.4
Expected AP: P(r) =0.9 0.3
Expected AP: P(r) =0.5
Expected DCG 0.2
Expected RR: P(r) =0.5
Expected RR: P(r) =0.9 0.1

0

1

2

3

4

5

6

7

8

9

10

i

(c)

Figure 2: (a) The adaptive weight wiA of the expected Average Precision, (b) The adaptive weight wiR of the expected Reciprocal Rank, and (c) Comparison of the weights in different expected IR metrics.

=

X
NR

p(NR

|q

" )

1 NR

M
X(

E

[rai |NR i

]

i=1

+

i-1
X
j=1

E

[rai

raj i

|NR

]

" )

=

X
NR

p(NR

|q

" )

1 NR

M
X(

E

[rai |NR i

]

+

i=1

i-1
X

C ov (rai

,

raj

|NR )

+

E [rai i

|NR ]E [raj

|NR ]

" ),

j=1

where Cov(rai , raj |NR) denotes the correlation between the relevance values of documents at rank i and j given the number of relevant documents is NR. Eq. (3) shows that the expected AP can be interpreted as: for the given query, an IR model first estimates the number of relevant documents in the collection, and then estimates the expected AP for that number of relevant documents. The final expected measure is the average, weighted by p(NR|q), across all the possible numbers of relevant documents.
We can obtain more insight into the expected AP by making a simple approximation to the average over NR. By assuming that the posterior distribution of NR is sharply peaked around the most probable value (the mode) N^R, we can use the mode to approximate the average [5]. This gives:

ER[mA]



1 N^R

M
X "wiAp(Rai )
i=1

+

i-1
X
j=1

C

ov(rai i

,

raj

)

" ,

(4)

where

E[rai ]

=

P
rai

rai p(rai )

=

p(Rai ),

the

marginal

prob-

ability of a document's relevance at rank i. Note that the

equation removes the dependency of N^R because the con-

ditional expectation and variance are well approximated by

the non-conditional ones when P (N^R|q)  1. To simplify

the

equation,

we

also

define

wiA



, 1+Pij-=11 p(Raj )
i

which

is

regarded as an adaptive weight of rank i.

The first term in this simple approximation indicates that

the expected AP is a weighted average of the scores across

all rank positions, and as we increase the marginal probabil-

ity of relevance p(Rai) in the ranked list, the expected AP increases. Furthermore, because the weight ratio:

wia+1 wia

=

i

i +

1

(1

+

1

+

p(Rai )

Pi-1
j=1

p(Raj

)

)

(5)

is

in

the

range

between

i 1+i

and

2i 1+i

.The

ratio

is

adaptive

to

the

expected

relevance

(defined

as

Pi-1
j=1

p(Raj

))

received

so far. To get the insight into it, we approximate the weight

by setting p(Rai ) all equal to p(r). We plot the weight ratio against the marginal distribution p(r) and rank position

i in Figure 2 (a). It illustrates that when we have more

confidence about the relevance of the early retrieved docu-

ments (p(r) approaches one), the weight ratio becomes near

one. As a result, the metric is less worried about the early

retrieved documents, thus putting equal weights to the later-

retrieved documents. This is similar to the Precision metric.

But once less confident documents (p(r) approaches zero)

are retrieved, particularly in the top ranked positions, the

weight

ratio

approaches

its

lower

bound

i i+1

.

As a conse-

quence, the weight penalizes more the later-retrieved rele-

vant documents, and the ratio of the expected AP behaves

more like that of the expected DCG, which will be discussed

later.

The second term in Eq. (4) indicates that a document

will contribute more to the expected AP if its relevance is

more positively correlated with those of previous retrieved

documents. The consequence is that it will push positively

correlated documents up in the ranked list. This is an in-

teresting finding because it shows that the expected AP is

in fact nonlinear ­ it models well the dependencies between

documents' relevance and incorporates them in deciding the

preferred rank order. The rational of encouraging positively

correlated relevant documents is that if a document is rele-

vant, it is likely that its positively correlated documents are

also relevant. It theoretically explains why pseudo relevance

feedback, i.e., the top ranked documents are generally likely

to be relevant, and finding other documents similar to these

top ranked ones helps improve MAP [24].

2.1.2 Expected DCG and Precision
Discounted Cumulative Gain (DCG) is another popular measure for ranking effectiveness, especially in web search. DCG measures the usefulness, or gain, of a document based on its (graded) relevance[14] (for the moment, let us consider rai to cover the graded relevance too); the gain is accumulated from the top of the result list to the bottom. To penalize late-retrieved relevant documents, the gain of each result is discounted by a function of its rank position. By definition, we have the DCG measure as:

M

mD(a|r) = X wiDg(rai ),

(6)

i=1

where wiD is the discount weight for rank position i, and g(rai) is a gain function mapping the relevance value to the retrieval gain. Unlike the expected AP, the expect DCG is
linear with respect to rank positions. We thus have:

M

Er[mD] = X wiDE[g(rai )]

(7)

i=1

Since g(rai) is infinitely differentiable in the neighborhood

228

of the mean of rai , i.e., r^ai  E[rai], the mean of g(rai) can be represented by a Taylor power series as:

E[g(rai )] =E[g(r^ai )] + E[(rai - r^ai )g(r^ai )]+

E

[

1 2

(rai

-

r^ai )2g(r^ai )]

+

...

=g(r^ai )

+

0

+

1 2

V

ar(rai )g(r^ai )

+

...

(8)

g(r^ai

)

+

V

ar(rai

)

g(r^ai 2

)

,

The expected DCG is thus approximated by:

Er [mD ]



M

X

wiD

" g

`r^ai

´

+

1 2

g

(r^ai

)V

" AR(rai) ,

(9)

i=1

where V AR(rai) denotes the variance of rai . Eq. (9) shows that the expected value of DCG is determined by both the

mean and variance of the relevance of documents at rank

positions from 1 to M . Whether it should add variance or

minus variance depends on the sign of the second deriva-

tive of the gain function. In the case of graded relevance,

if consider highly relevant documents more valuable than

marginally relevant documents and give them more gain, we

can then use a gain function like g(rai) = 2rai - 1. In this case, we need to add variance.

It is shown ment with the

that when w1D highest score of

> w2D...

g(r^ai

)

+

1 2

g>(r^waiM D)V,

the docuAR(rai ) is

retrieved first, the document with the next highest score

is retrieved second, and so on. It is common to define

wiD



log2

1 (i+1)

.

Compared to the adaptive weight in the

expected AP, it penalizes more the late-retrieved relevant

documents. Figure 2 (c) compares their weight ratios.

Precision at M is a special case of DCG, where the

discount is a constant and the gain function is linear. Thus,

the expected Precision measure is

E[mP ]

=

1 M

M
X E(rai )



1 M

M
X p(Rai )

i=1

i=1

(10)

2.1.3 Expected Reciprocal Rank

In the cases like web search and question answering tasks, we quite often expect a relevant document to be retrieved as early as possible [10, 28]. Expected Search Length and Reciprocal Rank (RR) are strongly biased towards earlyretrieved documents. This section analyzes RR, while Expected Search Length can be derived similarly. RR is the inverse of the rank of the first relevant document and bounded between 0 and 1. It is formally defined as:

mR(a|r)

=ra1

1 1

+

ra2 (1

-

ra1

)

1 2

+

ra3 (1

-

ra1 )(1

-

ra2

)

1 3

+

...

(11)

N
X =

rai i

i-1
Y (1 - raj )

=

N
X

1 i

vi

rai

,

i=1

j=1

i=1

where

we

define

vi



Qi-1
j=1

(1

-

raj ),

a

function

of

the

rel-

evance values of documents ranked above i; (vi  1 when

i = 1). Conceptually, RR measure can be thought of as a

weighted average of relevance values at different rank po-

sitions, where the weights are adaptive to earlier retrieved

documents.

The expected value of the RR measure is the following:

E[mR]

M
=E[X

1 i

virai ]

=

M
X

E[virai ] i

i=1

i=1

=

M
X

E[vi]E[rai ]

+

Cov(rai ,

vi )

i

i=1

M
= X `wiRp(Rai )

+

1 i

C

ov(rai

,

vi

)´,

i=1

(12)

where,

similarly,

we

consider

E[vi] i

as

an

adaptive

weight

and

denote it as wiR. It can be approximated by assuming that

the irrelevance of documents above rank i is independent

when

calculating

wiR,

i.e.,

wiR



E[vi] i



1 i

Qi-1
j=1

(1

-

p(Raj

)).

Thus wiR > wiR+1. On the one hand, similar to the expected DCG, the weight wiR is a discount factor penalizing late retrieved relevant documents. As a result, maximiz-

ing the measure intends to push documents that have high

marginal distribution of relevance p(rj) to the top. However,

the penalty is much larger than the ones in expected DCG

and expected AP. To see this, let us again approximate the

weight by setting p(Rai)  p(r). The weight ratio is compared with those of the expected AP and expected DCG in

Figure 2 (c). It shows that expected RR has the smallest

weight ratio, while expected AP has the largest. Expected

DCG is the one in the middle.

One the other hand, the weight is updated in a completely

different way compared to expected AP. Figure 2 (b) plots

the weight ratio against the marginal distribution p(r) and

rank position i. Different from expected AP, the weight

ratio of expected RR becomes larger when p(r) is larger, re-

inforcing the discount further. As a consequence, it entirely

focuses on the quality of a few early retrieval documents.

For

example,

the

upper

bound

for

w3R

is

1 12

.

If

we

consider

p(Rai ) > 0.5 for i = {1, 2, 3}, while for DCG it usually

equals

1 log2 4

=

1 2

and

for

expected

AP

even

larger.

The covariance bit in Eq. (12) shows that overall the ex-

pected value of RR increases when relevance of a document

is more positively correlated with vi, the product of non-

relevancies (1 - raj ) of the documents above. The effect is that negatively correlated documents will have higher ex-

pected RR than positively correlated documents. Such effect

will be discounted by a factor 1/i at rank i. This is an en-

tirely opposite preference compared to the expected AP. To

see this, suppose we have two documents to rank:

E [mRR ]

=E[Ra1 ]

+

E[Ra2 ] 2

-

E[ra1 ra2 ] 2

=p(Ra1 )

+

p(Ra2 ) 2

-

Cov[ra1 , ra2 ]

+ p(Ra1 )p(Ra2 ) 2

(13)

=p(Ra1 )

+

w2Rp(Ra2 )

-

C

ov[ra1 2

,

ra2

]

,

where

w2R

=

(1-p(Ra1 2

))

.

It

shows

that

negatively

correlated

document has a higher value of the expected RR, confirming

the findings in [10, 29] that the RR metric is optimized by

diversifying the ranked list of documents.

2.1.4 A General View

Through our analysis, it can be seen that the expected IR metrics roughly have two components. A unified definition is given as follows:

E[m(a|r)]



M
X

"

"

Wip(Rai )

+

M
X

V

(rai , ..., i

ra1 )

,

i=1

i=1

(14)

where Wi is the discount weight in position i, and V is a

229

Definition:
Wi V (rai , ..., ra1 )

Table 1: A unified view of expected IR metrics.

Precision

DCG

AP

RR

PM
i=1

rai

PM 2rai -1
i=1 log2(i+1)

1 NR

PM
i=1

rai

(1+Pij-=11 raj ) i

PM
i=1

rai i

Qi-1
j=1

(1

-

raj

)

Expected Precision Expected DCG

Expected AP

Expected RR

1

1 log2 (i+1)

1+Pij-=11 p(Raj ) i

Qi-1 j=1

(1-p(Raj

))

i

0

0

Pi-1
j=1

C ov(rai

,

raj

)

Cov(rai , Qij-=11(1 - raj ))

function defining the correlation between documents. The specific definitions with respect to different metrics are summarized in Table 1. Notice that for DCG, in the case of binary relevance, g(rai) = 2rai - 1 can be approximated as a linear function, and the variance bit vanishes in Eq. (9).
The first bit is a linear one with respect to the marginal probability p(Rai). Strictly speaking, this is untrue as W is adaptive to previously retrieved documents. But since the weight ratio Wi+1/Wi is usually smaller than one, the maximum value of the first bit is still achieved by ranking in the decreasing order of the marginal probability of relevance. This is identical to what the Probability Ranking Principle has suggested [19]. We call it the general ranking preference. The second bit makes the IR metrics different from each other. It is called the specific ranking preference. A more detailed discussion and comparison about it is presented in Section 3.1 through a simulation.
2.2 Practical Considerations
Stack Search Maximizing Eq. (14) is a non-trivial task because it needs to search over all possible ranking combinations. We use stack search similar to [30], which keeps a list of the best n ranking combinations as candidates seen so far. These candidates are incomplete solutions till rank i. It then iteratively expands each of the best partial solutions by adding a document at rank i + 1. For each candidate, we select top-n documents that have the maximum increases of the expected IR metric in Eq. (14). We then put all resulting partial solutions (in this case, n × n) onto the stack and then trim the resulting list of partial solutions to the top n candidates again. We repeat the loop until the end of the rank list is reached. The solution is the one having the maximum value among the candidate solutions. Such a sequential update may not necessarily provide a global optimization solution, but it provides an excellent trade off between accuracy and efficiency by adjusting n. When n is 1, it goes back to the greedy approach. When we increase n, better solutions may be found at the expense of more computational cost. For details refer to [30].
IR Model Calibration To calculate the expected IR metrics during retrieval, we need to estimate the joint probability of relevance. An obvious solution is to directly estimate it from the (training) data [20]. Relevance information is, however, not steadily available in many practical situations to build a robust relevance model. In this paper, we intend to conduct an indirect estimation using existing IR models. It is observed that in many text retrieval experiments that the calculated ranking scores can serve as robust indicators of documents' relevance with respect to queries. Thus, a mapping function can be developed to map from the ranking scores to the probability of relevance. Similar to [29], the joint probability of relevance p(r|q) is summarized by the marginal probability p(rai|q) and covariance Cov[rai , raj ].
Let us first look at p(rai|q), and treat it as the utility of ranking scores. We expect the utility, defined as u, to be a non-decreasing function of the ranking score. Thus the first derivative u > 0. It is also expected that u has a maximum value as the ranking score increases. Thus the

Figure 3: By adjusting the correlation between documents from -0.2 to 1.0, the gain on performance for average precision, DCG, and RR, respectively.
second derivative u < 0. Our experiment (Section 3.2) on TREC data has confirmed our intuition. Applying an exponential utility function (u > 0 and u < 0) [2] gives the mapping function as:

p(Rai|q)  u(s) = 1 - e-bs,

(15)

where u(s), in the range [0, 1), is the utility of the ranking score s, where s  0. b denotes a constant. For the empirical

study of the mapping, we refer to Section 3.2.

The next question is how to estimate the covariance

q Cov[rai , raj ] = (rai , raj ) V ar[rai ]V ar[raj ],

(16)

where V ar[rai ] = (1-p(Rai))p(Rai) if rai follows a Bernoulli distribution. The correlation coefficient (rai , raj ) models the dependency of relevance between documents at rank i

and j. During retrieval, it is reasonable to use the docu-

ments' score correlation to estimate the relevance correla-

tion, i.e., (rai, raj )  (sai, saj ). Strictly speaking, the score correlation is query-dependent. A practical solution

is, however, to approximate it by sampling queries and cal-

culating the correlation between documents' ranking scores

from an IR model. In our implementation, we construct each

of these queries by randomly sampling query terms from the

vocabulary of a data set.

For the expected RR, we need to compute the covari-

ance between document ai and variable vi, where vi is the

"meta-relevance" of previously retrieved i-1 documents, i.e.,

vi



Qi-1
j=1

(1

-

raj )

as

defined

in

Section

2.1.3.

In our im-

plementation, we aggregate the content of the top i-1 doc-

uments as a meta document, and estimate the correlation

between rai and vi as 1 minus the correlation between the meta document's ranking score and document ai's ranking

score.

3. EXPERIMENTS
3.1 Simulation
In this section, we carried out a simulation as a confirmation of our analysis about the effect of correlation between different documents' relevance on a range of IR metrics. The relevance states of documents were generated for 10,000 trials. At each trial, for each rank position i, we kept

230

Figure 4: Probability that a result from each bin is relevant against the median of each bin.
the marginal probability of relevance p(Rai |q) unchanged and generated the relevance/nonrelevance states of the document. The samples were then randomly perturbed so that the correlation between each pair of variables increases from negative to positive (x axis in Figure (3) ). For each sample in each trial we calculated the value of an IR metric. We then averaged the metric values across all the trials to obtain the average value. We used the value of the IR metric when the correlation is set as zero as the basis for calculating the gain on the metric when the correlation changes. The results for AP, DCG, and RR are shown in Figure (3). It confirms our derivation of the expected DCG that it is insensitive to correlation. AP value increases when correlation increases, whereas RR does otherwise.
We tried with different settings such as the number of documents, and marginals etc, and got similar findings to the reported above. Previous empirical studies on TREC data have found out that one cannot optimize both the RR and AP metrics at the same time [24, 29]. The analytical forms and the simulation provide direct evidence that the AP metric encourage positively correlated documents whereas the RR metric encourages the opposite.
3.2 IR Model Calibration
In this section, TREC data is used to get an insight into how the mapping function u looks like. Similar to the experimental setup in [22], we measured the utility of ranking scores by the probability that documents given the ranking scores are judged relevant. Documents were binned based on their ranking scores for analysis; we judged the probability that a randomly picked document from each bin is judged as relevant. More specifically, we ran the Jelinek-Mercer smoothing language model on the TREC2004 Robust Track 249 topics with the parameter  set as its typical value 0.1 [34]. The top 1000 documents were returned for each topic, and there were in total 241,606 results returned for these 249 queries, among which there are 7,029 relevant documents out of a total number of 17,412 relevant documents in the track. The queries contain different numbers of terms. To making the ranking scores comparable across queries, we normalized the ranking scores for all results of each query by dividing these ranking scores by the number of terms in the query.
We sorted the 241,606 results in the descending order in terms of their scores, and divided this ranked list into bins of 1,500 results each, yielding 161 bins: the first 160 bins containing 1,500 results each, and the last bin containing the 1606 documents with the lowest scores. We selected the median score in each bin to represent the bin. In Figure 4, the utility of each bin, i.e., the probability that a randomly chosen result from the bin is relevant, is estimated as the number of relevant documents in each bin divided by the bin size. The data points are based on the pairs of the median of each bin and probability of relevance, and the data points are connected by smoothed curves.

Table 2: Overview of six TREC collections.

Name Description

Size # Docs Topics

TREC8 TREC disks 1.86 GB 528,155 401-450

4&5 minus CR

Robust TREC disks 1.86 GB 528,155 301-450 and 601-

2004

4&5 minus CR

700 minus 672

Robust TREC disks 1.86 GB 528,155 50 difficult Ro-

Hard

4&5 minus CR

bust2004 topics

WT10g TREC Web 11 GB 1,692,096 501-550

collection

CSIRO CSIRO crawl 4.2 GB 370,715 1-50 minus 8 un-

judged topics

.Gov

2002 crawl of 18 GB 1,247,753 551-600

.gov domain

Figure 4 confirms our intuition that the mapping function is approximately a concave curve (u > 0 and u < 0) and fitting Eq. (15) to the data in Figure 4 gives b= 9.133. Our experiments showed that the performance of our approach is robust with respect to the choice of b, and a value of b anywhere between 7.0 and 12.0 results in negligible changes of the performance on all the test collections. For the remaining experiments, we fix the parameter b as 9, while bearing in mind that tuning it from training data might have potentials for further performance improvement.

3.3 Performance
We continued our empirical study of the proposed probabilistic retrieval framework, focusing on understanding its ability of optimizing IR metrics. Dirichlet and Jelinek-Mercer smoothing language models were chosen as the two baseline IR models since they are frequently reported for good performance on TREC test collections [34]. For each query, the ranking score of each document, calculated by either of the two IR models, is normalized by dividing them over the number of terms in the query. It is used as the input to estimate the marginal probabilities and covariance on the basis of the discussion in Section 2.2. The stack search is then applied to find an optimal ranking list that maximizes a given IR metric in Eq. (14). For the stack search, we simply set n=1, i.e., equivalent to a greedy approach, while leaving this line of research to future work.
Standard stemming and stopword removing were carried out for both queries and documents. The smoothing parameters of the language models were tuned for the optimal performance for a metric on each data set. The results are reported on six TREC test collections, described in Table 2. TREC8, Robust 2004, and Robust 2004 Hard topics are three plain text collections, and TREC 2001 ad hoc task on WT10g data, TREC 2007 enterprise track document search task on CSIRO data, and TREC 2002 topic distillation task on .Gov data are on three Web collections.
The results in Table 3 indicate that if we choose a certain IR metric to maximize, we obtained in most cases the best performance on this metric than optimizing other metrics and the baselines. More specifically, our approach always had the best performance with respect to MAP and MRR when the objective was to maximize the expected AP and RR, respectively. When we aimed to optimize the expected DCG, our approach improved the baseline on 8 out of 12 occasions in terms of NDCG. It is worth mentioning that no parameter was needed when optimizing the metrics. Without any parameter tuning, our approach consistently outperformed the two baseline models, and eight improvements are statistically significant.
Recall the analysis in Section 2 that the expected AP and RR have a rather "opposite" rank preference (utility) ­ the expected AP favors a document whose relevance is positively correlated with those of the documents ranked above, whereas the expected RR suggests otherwise. Table 3 demonstrates that the optimization of the expected RR always leads to better performance on MRR than optimization

231

Table 3: Performance on MAP, NDCG and MRR when the objective is to optimize AP, DCG, and RR,

respectively. We used the Dirichlet and Jelinek-Mercer smoothing language models, whose smoothing pa-

rameters were tuned for the optimal performance of a metric on each data set, as the baselines in optimization.

We highlight the highest performance in bold. A Wilcoxon signed-rank test (p <0.05) is conducted and sta-

tistically significant improvements over the baselines are marked with .

TREC8

MAP NDCG MRR

Robust2004

MAP NDCG MRR

Robust hard

MAP NDCG MRR

Dirichlet (Baseline) 0.224 0.428 0.606 Dirichlet (Baseline) 0.221 0.410 0.596 Dirichlet (Baseline) 0.088 0.21 0.393

Maximize AP

0.236 0.428 0.602

Maximize AP

0.227 0.412 0.593

Maximize AP

0.089 0.21 0.387

Maximize DCG

0.224 0.44 0.615

Maximize DCG

0.219 0.411 0.593

Maximize DCG

0.0890.235 0.399

Maximize RR

0.189 0.436 0.628

Maximize RR

0.208 0.391 0.597

Maximize RR

0.076 0.23 0.410

Jelinek-Mercer (Baseline) 0.228 0.404 0.458 Jelinek-Mercer (Baseline) 0.221 0.401 0.542 Jelinek-Mercer (Baseline) 0.09 0.225 0.36

Maximize AP

0.239 0.44 0.469

Maximize AP

0.228 0.412 0.593

Maximize AP

0.092 0.23 0.358

Maximize DCG

0.227 0.416 0.476

Maximize DCG

0.22 0.406 0.543

Maximize DCG

0.09 0.245 0.37

Maximize RR

0.196 0.404 0.477

Maximize RR

0.18 0.364 0.546

Maximize RR

0.087 0.24 0.374

WT10g

MAP NDCG MRR

CSIRO

MAP NDCG MRR

.Gov

MAP NDCG MRR

Dirichlet (Baseline) 0.202 0.4 0.550 Dirichlet (Baseline) 0.398 0.692 0.782 Dirichlet (Baseline) 0.147 0.272 0.419

Maximize AP

0.204 0.392 0.546

Maximize AP

0.408 0.692 0.785

Maximize AP

0.151 0.272 0.417

Maximize DCG

0.199 0.405 0.551

Maximize DCG

0.395 0.692 0.779

Maximize DCG

0.148 0.2930.428

Maximize RR

0.181 0.316 0.552

Maximize RR

0.367 0.636 0.789

Maximize RR

0.132 0.238 0.427

Jelinek-Mercer (Baseline) 0.168 0.360 0.472 Jelinek-Mercer (Baseline) 0.374 0.684 0.849 Jelinek-Mercer (Baseline) 0.167 0.286 0.45

Maximize AP

0.176 0.376 0.48

Maximize AP

0.384 0.704 0.85

Maximize AP

0.1870.306 0.449

Maximize DCG

0.168 0.360 0.472

Maximize DCG

0.371 0.676 0.850

Maximize DCG

0.169 0.286 0.444

Maximize RR

0.153 0.36 0.481

Maximize RR

0.349 0.644 0.870

Maximize RR

0.147 0.245 0.454

of the expected AP, and vice versa. The result supports our theoretical finding that RR and AP are two different types of metrics, and optimizing either of them cannot lead to the optimal performance of the other.
Table 3 also shows that optimization of AP can sometimes lead to better performance on NDCG than direct optimization of DCG. Similar finding appeared in the learning to rank paradigm, and it was argued that the reason is due to the fact that MAP is more informative than DCG [32]. Yet, we think that the informative explanation, although true in learning to rank, does not necessarily hold in our probabilistic framework since we do not use IR metrics to summarize the training data. Our belief is supported by the results from the simulation in Section 3.1 that the expected DCG is invariant to the changes of relevance correlation between documents; and as a result, optimzing AP (prompting documents whose relevance is positively correlated with previous documents) shouldn't do any better than directly optimizing DCG for the NDCG metric. We thus believe the somewhat contradicted finding in the real data set may be attributed to the estimation of the joint probability of relevance, more specifically the relevance correlation, given the fact we used textual content to infer relevancy. As the cluster hypothesis suggests that relevant documents tend to be similar to each other to form clusters [25], a document is likely to be relevant if it is similar to relevant documents. As a result, the expected AP biases towards putting documents similar with each other in the top rank positions. When top ranked documents are relevant, these other documents are also likely to be relevant - their marginal probabilities of relevance might be higher than the estimated. As a result, metrics such as NDCG and Precision are improved.
Finally, we provide a further account of RR and AP, the two differently behaving metrics. Recall that in Figure 2 the properties of the expected RR and AP were depicted by adjusting the weight functions wiA and wiR using a single parameter p(r). Figure (5) used TREC8 test collection to further show the effect of p(r) on the resulting MRR and MAP performance. For comparison, the performance of the baseline Dirichlet smoothing language model, and the exact optimization of RR, MAP and DCG was also plotted.
It shows that adjusting p(r) to approximate AP is very stable since the solution keeps roughly the same for all eight values of p(r). This could be explained by the fact that the weight ratio between wiA+1 and wiA saturates at 1 for

Figure 5: MRR v.s. MAP
all values of p(r) when i increases above 4. By contrast, the RR approximation is more volatile with respect to p(r). As p(r) increases from 0.1 to 0.5, the MRR performance increases whereas the MAP performance decreases. This is due to the fact that as p(r) decreases, the weight ratio of RR becomes similar to that of DCG and AP. p(r) can be used to trade off between the performance of MAP and MRR. When p(r) = 0.3 and 0.4, the performance on MRR even slightly exceeds that on the exact optimization of RR. This suggests that there might be still scope to improve our stack search algorithm by setting n higher than 1.
4. LINKS TO OTHER WORK
To complement Section 1, we continue the discussion of related work. In the learning to rank paradigm, optimizing IR metrics is conducted in a discriminative manner where Support Vector Machines or Neural Networks were commonly used [23, 33]. By contrast, we study the problem in a probabilistic framework where the intention is to combine both the generative and discriminative processes. Our formulation of optimal ranking also fundamentally departs from the idea in [26], where a probability distribution over document permutations (rank) is defined, and the expectation of IR metrics is considered under this distribution. In this paper, we, however, believe that the expectation of IR metrics should be with respect to a distribution of relevance, because the uncertainty comes only from the fact that we cannot know the relevance of documents with absolute certainty.
For the purpose of evaluation, the estimation of IR metrics, particularly MAP, has been investigated in the past.

232

For example, to reduce the variability of test collection, a normalization technique was introduced [11]; to deal with incomplete judgements, sampling approaches were proposed [3, 31]. Empirically, their error rates were measured [7]; and the uncertainty from the variability of relevance judgments in TREC were also examined [27]. By contrast, our study is for the purpose of retrieval, and thus the IR metric estimation and optimization were explored in a complete different situation where the relevance is not known a priori.
The most relevant work can be found in [10, 15, 35]. The study in [10] argued that in some tasks users would be satisfied with a limited number of relevant documents, rather than requiring all relevant documents. The authors therefore proposed to maximize the probability of finding a relevant document among the top n. By treating the previously retrieved documents as non-relevant ones, their algorithm is equivalent to optimizing Reciprocal Rank. A more general solution is proposed in [35] on the basis of the Bayesian rank decision framework in [15]. In their solutions, different rank preferences are expressed by different utility functions and can be incorporated when calculating the score for each of the documents. The two ideas are close in spirit to the Maximal Marginal Relevance (MMR) criterion in [9], and can be called "marginal relevance" IR models because they are designed to calculate the additional information a document contributes in a result list. But unfortunately this framework does not allow the capacity to model and optimize different IR metrics.
This paper takes a rather different view, although similar to [15, 35] we also follow the Bayesian decision theory. We argue that the rank utility is nothing to do with the (relevance) model parameters but only with the hidden true topical relevance; and the relevance states of documents need to be estimated before knowing any user (rank) utility. A good IR metric could be able to specify one type of rank utilities. Once we summarize our belief about the true relevance by the joint probability of relevance, the utility, expressed by an evaluation metric, can be estimated under such uncertainty, and the optimal decision is the one that optimizes that expected value. The two distinct retrieval steps do not assume a particular (relevance) retrieval model, making it applicable to many existing IR models and IR metrics.
Our work is also related to the portfolio theory of document ranking [29]. By an analogy with the financial problems, they argued that an optimal rank order is the one that balances the overall relevance (mean) of the ranked list against its risk level (variance). This paper follows the idea of using mean and variance to summarize a distribution and to analyze the expected IR metrics. Our analytical forms of expected IR metrics on the basis of the mean and variance reveal some interesting properties that have not been shown in the past.
5. CONCLUSIONS
In this paper, we have studied the statistical properties of expected IR metrics when the relevance of documents is unknown. An implementation based on our analysis and the two-stage framework has already shown its ability of optimizing major IR metrics in a probabilistic framework. In the future, it is of great interest to seek its usage in web search where click-through data can be viewed as indirect evidence of documents' relevance. Also, during evaluation, the "Cranfield paradigm" considers relevance as deterministic values, either binary or graded ones. It is, however, more general to consider IR evaluation as a stochastic process too. Thus, despite the fact that our study of the expected IR metrics is for retrieval, the analysis and development are also rel-

evant to evaluation if the disagreement between relevance assessors needs to be modelled.
6. REFERENCES [1] G. Amati and C. J. V. Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.
[2] K. Arrow. Aspects of the Theory of Risk-Bearing. Helsinki: Yrj¨o Hahnsson Foundation, 1965.
[3] J. A. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In SIGIR, 2006.
[4] J. A. Aslam, E. Yilmaz, and V. Pavlu. The maximum entropy method for analyzing retrieval measures. In SIGIR, 2005.
[5] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
[6] P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. Mercer. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 1993.
[7] C. Buckley and E. M. Voorhees. Evaluating evaluation measure stability. In SIGIR, 2000.
[8] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In ICML '05, 2005.
[9] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR, 1998.
[10] H. Chen and D. R. Karger. Less is more: probabilistic models for retrieving fewer relevant documents. In SIGIR, 2006.
[11] G. V. Cormack and T. R. Lynam. Statistical precision of information retrieval evaluation. In SIGIR, 2006.
[12] W. B. Croft and D. J. Harper. Using probabilistic models of document retrieval without relevance information. Document Retrieval Systems, 1988.
[13] D. Harman. Overview of the second text retrieval conference (trec-2). In HLT '94, 1994.
[14] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 2002.
[15] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In SIGIR, 2001.
[16] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, 2008.
[17] M. E. Maron and J. L. Kuhns. On relevance, probabilistic indexing and information retrieval. J. ACM, 1960.
[18] S. Mizzaro. Relevance: The whole history. Journal of the American Society of Information Science, 1997.
[19] S. E. Robertson. The probability ranking principle in IR. Journal of Documentation, pages 294­304, 1977.
[20] S. E. Robertson and K. Sp¨arck Jones. Relevance weighting of search terms. Journal of the American Society for Information Science, 27(3):129­46, 1976.
[21] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR, 1994.
[22] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In SIGIR, pages 21­29, 1996.
[23] M. Taylor, J. Guiver, S. Robertson, and T. Minka. Softrank: optimizing non-smooth rank metrics. In WSDM, 2008.
[24] S. Tomlinson. Early precision measures: implications from the downside of blind feedback. In SIGIR, 2006.
[25] C. J. van Rijsbergen. Information Retrieval. Butterworths, London, London, UK, 1979.
[26] M. N. Volkovs and R. S. Zemel. Boltzrank: learning to maximize expected ranking gain. In ICML '09, 2009.
[27] E. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. In Information Processing and Management, pages 315­323. ACM Press, 1998.
[28] E. M. Voorhees. The TREC-8 question answering track report. In TREC-8, pages 77­82, 1999.
[29] J. Wang and J. Zhu. Portfolio theory of information retrieval. In SIGIR, 2009.
[30] Y. Wang and A. Waibel. Decoding algorithm in statistical machine translation. In EACL, 1997.
[31] E. Yilmaz, E. Kanoulas, and J. A. Aslam. A simple and efficient sampling method for estimating ap and ndcg. In SIGIR, 2008.
[32] E. Yilmaz and S. Robertson. On the choice of effectiveness measures for learning to rank. Information Retrieval, 2009.
[33] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support vector method for optimizing average precision. In SIGIR, 2007.
[34] C. Zhai. Statistical language models for information retrieval a critical review. Found. Trends Inf. Retr., 2(3):137­213, 2008.
[35] C. Zhai and J. D. Lafferty. A risk minimization framework for information retrieval. Inf. Process. Manage., 42(1):31­55, 2006.

233

Information-Based Models for Ad Hoc IR

Stéphane Clinchant
XRCE & LIG, Univ. Grenoble I Grenoble, France
stephane.clinchant@xrce.xerox.com

Eric Gaussier
LIG, Univ. Grenoble I Grenoble, France
eric.gaussier@imag.fr

ABSTRACT
We introduce in this paper the family of information-based models for ad hoc information retrieval. These models draw their inspiration from a long-standing hypothesis in IR, namely the fact that the difference in the behaviors of a word at the document and collection levels brings information on the significance of the word for the document. This hypothesis has been exploited in the 2-Poisson mixture models, in the notion of eliteness in BM25, and more recently in DFR models. We show here that, combined with notions related to burstiness, it can lead to simpler and better models.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Theory, Algorithms, Experimentation
Keywords
IR Theory, Probabilistic Models, Burstiness
1. INTRODUCTION
The purpose of this paper is to introduce the family of information based model for ad hoc information retrieval (IR). By information, we refer to Shannon information when observing a statistical event. The informativeness of a word in a document has a rich tradition in information retrieval since the influential indexing methods developed by Harter ([11]). The idea that the respective behaviors of words in documents and in the collection bring information on word type is, de facto, not a novel idea in IR. It has inspired the 2-Poisson mixture model, the concept of eliteness in BM25 models and is at the heart of DFR models. In this paper, we come back to this idea in order to present a new family of IR models: information models. To do so, we first present,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

in section 2, the conditions a retrieval function should satisfy, on the basis of the heuristic retrieval constraints proposed by Fang et al. [9]. Section 3 is then devoted to the presentation of information models, and their link with the retrieval conditions and the phenomenon known as burstiness. We present two instances of information models, based on two power law distributions, and show how to perform pseudo-relevance feedback for information models. Section 4 provides an experimental validation of our models. Our experiments show that the information models we introduce significantly outperform language models and Okapi BM25. They are on par with DFR models, while being conceptually simpler, when pseudo-relevance feedback is not used. When using pseudo-relevance feedback, they significantly outperform all models, including DFR ones.

2. PRELIMINARIES
The notations we use throughout the paper are summarized in table 2 (w represents a term). They slightly differ from standard notations for convenience reasons, i.e. their easiness of use in the mathematical framework we deploy. We

Notation xqw xdw tdw yd m L N M Fw
Nw
zw

Description

Number of occurrences of w in query q

Number of occurrences of w in document d

Normalized version of xdw Length of document d

Average document length

Length of collection d

Number of documents in the collection

Number of terms in the collection

Number of occurrences of w in collection:

Fw

=

P
d

xdw

Number of documents containing w:

Nw

=

P
d

I (xdw

>

0)

zw = Fw or zw = Nw

Table 1: Notations

consider here retrieval functions, denoted RSV , of the form:
RSV (q, d) = X a(xqw)h(xdw, yd, zw, )
wq
where  is a set of parameters and where h, the form of which depends on the IR model considered, is assumed to be of class1 C2 and defined over R+ ×R+ ×R+ ×, where 1A function of class C2 is a function for which second derivatives exist and are continuous.

234

 represents the domain of the parameters in  and a is often the identity function. Language models [21], Okapi [15] and Divergence from Randomness [3] models as well as vector space models [16] all fit within the above form. For example, for the pivoted normalization retrieval formula [17],  = (s, m, N ) and:

h(x,

y,

z,

)

=

I (x

>

0)

1

+

ln(1 + ln(x)I(x>0))

1

-

s

+

s

y m

ln(

N

+ z

1

)

where I is an indicator function which equals 1 when its argument is true and 0 otherwise. A certain number of hypotheses, experimentally validated, sustain the development of IR models. In particular, it is important that documents with more occurrences of query terms get higher scores than documents with less occurrences. However, the increase in the retrieval score should be smaller for larger term frequencies, inasmuch as the difference between say 110 and 111 is not as important as the one between 1 and 2 (the number of occurrences has doubled in the second case, whereas the increase is relatively marginal in the first case). In addition, longer documents, when compared to shorter ones with exactly the same number of occurrences of query terms, should be penalized as they are likely to cover additional topics than the ones present in the query. Lastly, it is important, when evaluating the retrieval score of a document, to weigh down terms occurring in many documents, i.e. which have a high document/collection frequency, as these terms have a lower discrimination power. These different considerations can be analytically formalized as a set of simple conditions the retrieval function h should satisfy:

(y, z, ),

h(x, y, z, x

)

>

0

(condition 1)

(y, z, ),

2h(x, y, z, ) x2

<

0

(condition 2)

(x, z,

),

h(x, y, z, y

)

<

0

(condition 3)

(x, y, ), h(x, y, z, ) < 0 (condition 4) z
Conditions 1, 3 and 4 directly state that h should be increasing with the term frequency, and decreasing with the document length and the document/collection frequency. Conditions 1 and 2, already mentioned in this form by Fang et al. [9], state that h should be an increasing, concave function of the term frequency, the concavity ensuring that the increase in the retrieval score will be smaller for larger term frequencies. We will refer to the above conditions as the form conditions inasmuch as they define the general shape the function h should have. They respectively correspond to the heuristic retrieval constraints TFC1, TFC2, LNC1 and TDC2 introduced by Fang et al. [9]. In addition to this form conditions, Fang et al. [9] used two additional constraints to regulate the interaction between frequency and document length, i.e. between the derivatives wrt to x and y. These conditions, which we will refer to as adjustment conditions, allow to adjust the functions h satisfying the form conditions 1, 2, 3 and 4. They correspond to:
2Condition 4 is in fact a special case of TDC, but this is beyond the scope of the current paper.

Condition 5 LNC2: Let q a query. k > 1, if d1 and d2 are two documents such that yd1 = k × yd2 and for all words w, xdw1 = k × xdw2, then RSV (d1, q)  RSV (d2, q)
Condition 6 TF-LNC: Let q = w a query with only word w. if xdw1 > xdw2 et yd1 = yd2 + xdw1 - xdw2, then RSV (d1, q) > RSV (d2, q).
We are now ready to proceed to the presentation of information models.

3. INFORMATION MODELS

In order to take into account the fact that one is comparing documents of different length, most IR models do not rely directly on the raw number of occurrences of words in documents, but rather on normalized versions of it. Language models for example use the relative frequency of words in the document and the collection. Other classical term normalization schemes include the well know Okapi normalization, as well as the pivoted length normalization [17]. More recently, [14] propose another formulation for the language model using the notion of verbosity. DFR models usually adopt one of the two following term frequency normalizations (c is a multiplying factor):

tdw

=

xdw

c

m yd

or

xdw

log(1

+

c

m yd

)

(1)

The concept of the information brought by a term in a document has been considered in several IR models. Harter [11] observed that 'significant', 'specialty' words of a document do not behave as 'functional' words. Indeed, the more a word deviates in a document from its average behavior in the collection, the more likely it is 'significant' for this particular document. This can be easily captured in terms of information: If a word behaves in the document as expected on the collection, then it has a high probability of occurrence in the document p, according to the distribution collection, and the information it brings to the document, - log(p), is small. On the contrary, if it has a low probability of occurrence in the document, according to the distribution collection, then the amount of information it conveys is more important. Because of the above consideration, this idea, at the basis of DFR models, has to be applied to the normalized form of the term frequency. This leads to the general and simple retrieval function:

RSV (q, d) = X -xqw log P rob(Xw  tdw|w) (2)
wq

where tdw is the normalized form of xdw and w is a parameter for the probability distribution of w in the collection. We simply consider here that w is set to either the average number of occurrences of w in the collection, or to the average number of documents in which w occurs, that is:

 = zw = Fw or Nw

(3)

NNN

It is interesting to note that the retrieval function defined
by equation 2, which is rank invariant by the change of the logarithmic base, satisfies the heuristic retrieval conditions 1 and 3. Indeed, P rob(Xw  tdw|w) is a decreasing function of tdw. So, as long as tdw is an increasing function of xdw and a decreasing function of yd, which is the case for all the normalization functions we are aware of, conditions 1 and 3
are satisfied for this family of models.

235

3.1 Burstiness (and condition 2)
Church and Gale [6] were the first to study, to our knowledge, the phenomenon of burstiness in texts. The term "burstiness" describes the behavior of words which tend to appear in bursts, i.e., once they appear in a document, they are much more likely to appear again. The notion of burstiness is similar to the one of aftereffect of future sampling ([10]), which describes the fact that the more we find a word in a document, the higher the expectation to find new occurrences. Burstiness has recently received a lot of attention from different communities. Madsen [13], for example, proposed to use the Dirichlet Compound Multinomial (DCM) distribution in order to model burstiness in the context of text categorization and clustering. Elkan [8] then approximated the DCM distribution by the EDCM distribution, which learning time is faster, and showed the good behavior of the model obtained on different text clustering experiments. A related notion is the one of preferential attachment ([4] and [5]) often used in large networks, such as the web or social networks. It conveys the same idea: the more we have, the more we will get. In the context of IR, Xu and Akella [19] studied the use of a DCM model within the Probability Ranking Principle for modeling the dependency of word repetitive occurrences (a notion directly related to burstiness), and argue that multinomial distributions alone are not appropriate for IR within this principle. More formally, Clinchant and Gaussier [7] introduced the following definition (slightly simplified here for clarity's sake) in order to characterize discrete distributions which can account for burstiness:

Definition 1. [Discrete case] A discrete distribution P is bursty iff for all integers (n, n), n  n:

P (X  n + 1|X  n) > P (X  n + 1|X  n)

We generalize this definition to the continuous case as follows:

Definition 2. [General case] A distribution P is bursty iff the function g defined by:

 > 0, g(x) = P (X  x + |X  x)

is a strictly increasing function of x. A distribution which verifies this condition is said to be bursty.

which translates the fact that, with a bursty distribution, it is easier to generate higher values of X once lower values have been observed. We now show that this notion is directly related to the heuristic retrieval condition 2.
In the retrieval function defined by equation 2, the function h we have considered so far corresponds to:

- log(P rob(X  tdw))

In this case, condition 2 can be re-expressed as:

2h(x, y, z, ) x2

<

0



2

log(P rob(X  (xdw )2



tdw ))

>

0

But:

2f t2

=

2f x2

(

x t

)2

+

f x

2x t2

.

Furthermore,

f x

is

here

negative

as

f

is

log(P rob(X



tdw )).

So,

as

long

as

2x t2



0

(which is the case for all the normalization functions we are

aware of, in particular the ones provided by equation 1), a

sufficient condition for condition 2 is:

2 log(P rob(X  (tdw )2



tdw ))

>

0

The following theorem (the proof of which is given in the appendix) shows that bursty distributions satisfy this condition.

Theorem 3. Let P be a "bursty" probability distribution of class C2. Then:

2 log(P (X x2



x))

>

0

We thus see that under certain assumptions, IR models de-

fined by equation 2 satisfy the form conditions 1, 2 and 3.

We now summarize these assumptions which characterize

information models.

3.2 Characterization of Information Models
We characterize information models by the following three elements:
1. Normalization function The normalization function tdw, function of xdw and yd (respectively the number of occurrences of the word in the document and the length of the document), satisfies:

 tdw  xdw

> 0;

 tdw yd

<

0;

 2 xdw  (tdw )2

0

2. Probability distribution The probability distribution at the basis of the model has to be:

· Continuous, the random variable under consideration, tdw, being continuous;
· Compatible with the domain of tdw, i.e. if tmin is the minimum value of tdw, then P rob(Xw  tmin|w) = 1 (because of the first inequality above, tmin is obtained when xdw = 0);
· Bursty according to definition 2 above.

3. Retrieval function The retrieval function satisfies equation 2, i.e.:

RSV (q, d) = X -xqw log P rob(Xw  tdw|w)
wq
= X -xqw log P rob(Xw  tdw|w)
wqd
where the second equality derives from the fact that the probability function verifies P rob(Xw  tmin|w) = 1, with tmin obtained when xdw = 0. The above ranking function corresponds to the mean information a document brings to a query (or, equivalently, to the average of the document information brought by each query term). Furthermore, the parameter w is set as in equation 3:
 = zw = Fw or Nw NNN
The general form of the retrieval function and the first two inequalities on the normalization function ensure that the model satisfies conditions 1 and 3. Theorem 3, in conjunction with the last condition on the normalization function, additionally ensures that it satisfies condition 2. Hence, information models satisfy three (out of four) form conditions. The choice of the particular bursty distribution to be used has to be made in such a way that the last form condition and the two adjustment conditions are satisfied.

236

3.3 Two Power-law Instances
We present here two power law distributions which are bursty and lead to information models satisfying all form and adjustment conditions. The use of power law distributions to model burstiness is not entirely novel, as other studies ([4, 5]) have used similar distributions to model preferential attachment, a notion equivalent to burstiness.
Log-Logistic Distribution
The log-logistic (LL) distribution is defined by, for X  0:

PLL (X

<

x|r, )

=

x x + r

We consider here a restricted form of the log-logistic distribution where  = 1, so that the the log-logistic information model takes the form:

RSV (q, d) = X -xqw log(PLL(X  tdw|w))

wqd

=

X
wqd

-xqw

log

(

tdw

w + w

)

(4)

The log-logistic motivation resorts to previous work on text modeling. Following Church and Gale [6] and Airoldi [1], Clinchant and Gaussier [7] studied the negative binomial distribution in the context of text modeling. They then assumed a uniform Beta prior distribution over one of the parameters, leading to a distribution they refer to as the Beta negative binomial distribution, or BNB for short. One problem with the BNB distribution is that it is a discrete distribution and cannot be used for modeling tdw. However, the log-logistic distribution, with its  parameter set to 1, is a continuous counterpart of the BNB distribution since PLL(x  X < x + 1; r) = PBNB (x).
A Smoothed Power-Law (SPL) Distribution
We consider here the distribution, which we will refer to as SPL, defined, for x > 0, by:

x

f (x; )

=

- log  1-

 x+1 (x + 1)2

(0

<



<

1)

P (X > x|)

=

Z f (x; )
x

=

x
 x+1 -  1-

where f denotes the probability density function. Based on this distribution, the SPL information model thus takes the form:

tdw

RSV

(q,

d)

=

X
wqd

-xqw

log(

 tdw +1
w
1-

- w w

)

(5)

From equations 4 or 5, and using the normalization functions defined by equation 1, one can verify (a) that the log-logistic and SPL distributions are bursty, and (b) that their corresponding information models additionally satisfy conditions 4, 5 and 6 (the demonstration is purely technical, and is skipped here). The log-logistic and SPL information models thus satisfy all the form and adjustment conditions.
Figure 1 illustrates the behavior of the log-logistic model, the SPL model and the InL2 DFR model (referred to as INL for short). To compare these models, we used a value of 0.005 for  and computed the term weight obtained for term frequencies varying from 0 to 15. For information models, the weight corresponds to the quantity - log P rob, whereas

r = 0.005

8

6

4

2

0

loglogistic inl spl

0

5

10

15

Figure 1: Plot of Retrieval Functions

in the case of DFR models, this quantity is corrected by the Inf2 part, leading to, with the underlying distributions retained:

8
> >

-

log(

w tdw +w

)

> > <

tdw

weight = >

-

log(

wtdw +1 -w 1-w

)

> > > :

- tdw
tdw +1

log(

Nw +0.5 N +1

)

(log-logistic)
(SPL) (InL2)

As one can note, the weight values obtained with the two information models are always above the ones obtained with the DFR model, the log-logistic model having a sharper increase than the other ones for low frequency terms.

3.4 PRF in Information Models
Pseudo-relevance feedback (PRF) in information models can be performed following the same approach as the one used in other models: The weight of a term in the original query is updated on the basis of the information brought by the top retrieved documents on the term. Denoting by R the set of top n documents retrieved for a given query, R = (d1, . . . , dn), the average information this set brings on a given term w can directly be computed as:

InfoR(w)

=

1 n

X - log(P (Xw

>

tdw |w ))

(6)

dR

where the mean is taken over all the documents in R. This is a major difference with the approach in [2] where all documents in R are merged into a single document. Considering the documents in R as different documents allows one to take into account the differences in document lengths and number of occurrences. The original query is then modified, following standard approaches to PRF, to take into account the words appearing in R as:

xqw2

=

xqw maxw

xqw

+



InfoR(w) maxw InfoR(w)

(7)

where  is a parameter controlling the modification brought by R to the original query. xqw2 denotes the updated weight of w in the query.

4. EXPERIMENTAL VALIDATION
To assess the validity of our models, we used standard IR collections, from two evaluation campaigns: TREC (trec.nist.gov) and CLEF (www.clef-campaign.org). Table 2 gives the number of documents (N ), number of unique terms (M ), aver-

237

age document length and number of test queries for the collections we retained: ROBUST (TREC), TREC3, CLEF03 AdHoc Task, GIRT (CLEF Domain Specific Task, from the years 2004 to 2006). For the ROBUST and TREC3 collections, we used standard Porter stemming. For the CLEF03 and GIRT collections, we used lemmatization, and an additional decompounding step for the GIRT collection which is written in German.

Table 2: Characteristics of the different collections

N

M Avg DL # Queries

ROBUST 490 779 992 462 289

250

TREC-3 741 856 668 648 438

50

CLEF03 166 754 80 000 247

60

GIRT 151 319 179 283 109

75

We evaluated the log-logistic and the SPL model against

language models, with both Jelinek-Mercer and Dirichlet

Prior smoothing, as well as against the standard DFR mod-

els and Okapi BM25. For each dataset, we randomly split

queries in train and test (half of the queries are used for

training, the other half for testing). We performed 10 such

splits on each collection. The results we provide for the

Mean Average Precision (MAP) and the precision at 10 doc-

uments (P10) is the average of the values obtained over the

10 splits. The parameters of the different models are opti-

mized (respectively for the MAP and the precision at 10) on

the training set. The performance is then measured on the

test set. To compare the different methods, a two-sided t-

test (at the 0.05 level) is performed to assess the significance

of the difference measured between the methods. All our

experiments were carried out thanks to the Lemur Toolkit

(www.lemurproject.org). In all the following tables, ROB-t

represents the robust collection with query titles only, ROB-

d the robust collection with query titles and description

fields,CL-t represent titles for the CLEF collection, CL-d

queries with title and descriptions and T3-t query titles for

TREC-3 collection. The GIRT queries are just made up of

a single sentence.

The version of the log-logistic model used in all our ex-

periments is based on w =

nw N

and the second length nor-

malization in equation 1 (called L2 in DFR). We refer to

this model as the LGD model. The same settings are cho-

sen for the SPL model. As the parameter c in equation 1

is not bounded, we have to define a set of possible values

from which to select the best value on the training set. We

make use of the typical range proposed in works on DFR

models, which also rely on equation 1 for document length

normalization. The set of values we retained is: {0.5, 0.75,

1, 2, 3, 4, 5, 6, 7, 8, 9}.

Comparison with Jelinek-Mercer and Dirichlet language models

As the smoothing parameter of the Jelinek-Mercer language model is comprised between 0 and 1, we use a regular grid on [0, 1] with a step size of 0.05 in order to select, on the training set, the best value for this parameter. Table 3 shows the comparison of our models, LGD and SPL, with the JelinekMercer language model (LM). On all collections, on both short and long queries, the LGD model significantly outperforms the Jelinek-Mercer language model. This is an interesting finding as the complexity of the two models is the same (in a way, they are both conceptually simple). Further-

more, as the results displayed are averaged over 10 different splits, this shows that the LGD model consistently outperforms the Jelinek-Mercer language model and thus yields a more robust approach to IR. Lastly, the SPL model is better than the Jelinek-Mercer model for most collections for MAP and P10.

Table 3: LGD and SPL versus LM-Jelinek-Mercer after 10 splits; bold indicates significant difference
MAP ROB-d ROB-t GIR T3-t CL-d CL-t JM 26.0 20.7 40.7 22.5 49.2 36.5 LGD 27.2 22.5 43.1 25.9 50.0 37.5 P10 ROB-d ROB-t GIR T3-t CL-d CL-t JM 43.8 35.5 67.5 40.7 33.0 26.2 LGD 46.0 38.9 69.4 52.4 33.6 26.6

MAP JM SPL P10 JM SPL

ROB-d 26.6 26.7
ROB-d 44.4 47.6

ROB-t 23.1 25.2
ROB-t 39.8 45.3

GIR 39.2 41.7 GIR 66.0 69.8

T3-t 22.3 26.6 T3-t 43.9 56.0

CL-d 47.2 44.1 CL-d 34.0 34.0

CL-t 37.2 37.7 CL-t 25.6 25.6

For the Dirichlet prior language model, we optimized the smoothing parameter from a set of typical values, defined by: {10, 50, 100, 200, 500, 800, 1000, 1500, 2000, 5000, 10000}. Table 4 shows the results of the comparison between our models and the Dirichlet prior language model (DIR). These results parallel the ones obtained with the JelinekMercer language model on most collections, even though the difference is less marked. For the ROB collection with short queries, the Dirichlet prior language model outperforms in average the log-logistic model (the difference being significant for the precision at 10 only). On the other collections, with both short and long queries and on both the MAP and the precision at 10, the log-logistic model outperforms in average the Dirichlet prior language model, the difference being significant in most cases. The Dirichlet model has a slight advantage in MAP over the SPL model, but SPL is better for precision. Overall, the information-based models outperform in average language models.

Table 4: LGD and SPL versus LM-Dirichlet after 10 splits; bold indicates significant difference
MAP ROB-d ROB-t GIR T3-t CL-t CL-d DIR 27.1 25.1 41.1 25.6 36.2 48.5 LGD 27.4 25.0 42.1 24.8 36.8 49.7 P10 ROB-d ROB-t GIR T3-t CL-t CLF-d DIR 45.6 43.3 68.6 54.0 28.4 33.8 LGD 46.2 43.5 69.0 54.3 28.6 34.5

MAP DIR SPL P10 DIR SPL

ROB-d 26.7 25.6
ROB-d 45.2 46.6

ROB-t 25.0 24.9
ROB-t 43.8 44.7

GIR 40.9 42.1 GIR 68.2 70.8

T3-t 27.1 26.8 T3-t 52.8 55.3

CL-t 36.2 36.4 CL-t 27.3 27.1

CL-d 50.2 46.9 CL-d 32.8 32.9

Comparison with BM25
We adopt the same methodology to compare information models with BM25. We choose only to optimize the k1 pa-

238

rameter of BM25 among the following values: {0.3, 0.5, 0.8, 1.0, 1.2, 1.5, 1.8, 2, 2.2, 2.5}. The others parameters b and k3 take their default values implemented in Lemur (0.75 and 7). Table 5 shows the comparison of the log-logistic and SPL models with Okapi BM25. The log-logistic is either better (4 collections out of 6 for mean average precision, 3 collections out of 6 for P10) or on par with Okapi BM25. The same thing holds for the SPL model, which is 3 times better and 3 times on par for the MAP, and 4 times better, 1 time worse and 1 time on a par for the precision at 10 documents. Overall, information models outperform in average Okapi BM25.

Table 5: LGD and SPL versus BM25 after 10 splits; bold indicates best performance significant difference
MAP ROB-d ROB-t GIR T3-t CL-t CL-d BM25 26.8 22.4 39.8 25.4 34.9 46.8 LGD 28.2 23.5 41.4 26.1 34.8 48.0 P10 ROB-d ROB-t GIR T3-t CL-t CL-d BM25 45.9 42.6 62.6 50.6 28.5 33.7 LGD 46.5 44.3 66.6 53.8 28.7 34.4

MAP BM25 SPL P10 BM25 SPL

ROB-d 26.9 27.1
ROB-d 45.7 47.6

ROB-t 24.2 25.4
ROB-t 41.4 44.1

GIR 38.5 40.5 GIR 62.8 67.9

T3-t 25.3 26.8 T3-t 51.0 57.0

CL-t 35.1 34.5 CL-t 28.5 28.0

CL-d 47.3 47.0 CL-d 36.1 35.4

Comparison with DFR models To compare our model with DFR ones, we chose, in this latter family, the InL2 model, based on the Geometric distribution and Laplace law of succession, and the PL2 model based on the Poisson distribution and Laplace law. These models have been used with success in different works ([3, 7, 18] for example). All the models considered here make use of the same set of possible values for c, namely: {0.5, 0.75, 1, 2, 3, 4, 5, 6, 7, 8, 9}. It is however interesting to note that both PL2 and InL2 make use of discrete distributions (Geometric and Poisson) over continuous variables (tdw) and are thus theoretically flawed. This is not the case of the information models which rely on a continuous distribution.
The results obtained, presented in tables 6 and 7 are more contrasted than the ones obtained with language models and Okapi BM25. In particular, for the precision at 10, LGD and InL2 perform similarly (LGD being significantly better on GIRT whereas InL2 is significantly better on ROB with long queries, the models being on a par in the other cases). For the MAP, the LGD model outperforms the InL2 model as it is significantly better on ROB (for both sort and long queries) and GIRT, and on a par on CLEF. SPL is better than InL2 for precision but on a par for MAP. Moreover, LGD and PL2 are on a par for MAP, while PL2 is better for P10. Lastly, PL2 is better than SPL for MAP but not for the precision at 10 documents. Overall, DFR models and information models yield similar results. This is all the more so interesting that information models are simpler than DFR ones: They rely on a single information measure (see equation 2) without the re-normalization (Inf2 part) used in DFR models.

Table 6: LGD and SPL versus INL after 10 splits; bold indicates significant difference
MAP ROB-d ROB-t GIR T3-t CL-t CL-d INL2 27.7 24.8 42.5 27.3 37.5 47.7 LGD 28.5 25.0 43.1 27.3 37.4 48.0 P10 ROB-d ROB-t GIR T3-t CL-t CL-d INL2 47.7 43.3 67.0 52.4 27.3 33.4 LGD 47.0 43.5 69.4 53.2 27.2 33.3

MAP INL SPL P10 INL SPL

ROB-d 26.9 26.6
ROB-d 47.6 47.8

ROB-t 24.3 24.6
ROB-t 42.8 44.1

GIR 40.4 40.7 GIR 63.4 68.0

T3-t 24.8 25.4 T3-t 52.5 53.9

CL-t 35.5 34.6 CL-t 28.8 28.7

CL-d 49.4 48.1 CL-d 33.8 33.6

Table 7: LGD and SPL versus PL2 after 10 splits; bold indicates significant difference
MAP ROB-d ROB-t GIR T3-t CL-t CL-d PL2 26.2 24.8 40.6 24.9 36.0 47.2 LGD 27.3 24.7 40.5 24.0 36.2 47.5 P10 ROB-d ROB-t GIR T3-t CL-t CL-d PL2 46.4 44.1 68.2 55.0 28.7 33.1 LGD 46.6 43.2 66.7 53.9 28.5 33.7
MAP ROB-d ROB-t GIR T3-t CL-t CL-d PL2 26.3 25.2 42.8 25.8 37.3 45.7 SPL 26.3 25.2 42.7 25.3 37.4 44.1 P10 ROB-d ROB-t GIR T3-t CL-t CL-d PL2 46.0 45.2 69.3 54.8 26.2 32.7 SPL 47.0 45.2 69.8 55.4 25.9 32.9

Pseudo-relevance feedback
There are many parameters for pseudo-relevance feedback algorithms: The number of document to consider (N ), the number of terms to add the query (T C) and the weight to give to those new query terms (parameter  in equation 7). Optimizing all these parameters and smoothing ones at the same time would be very costly. We thus modify here our methodology. For each collection, we choose the optimal smoothing parameters for each model (c,µ,k1) on all queries. The results obtained in this case are given in table 8, where LM+MIX corresponds here to the Dirichlet language model. They show, for example, that on the ROBUST collection there is no difference between the baseline systems we will use for pseudo-relevance feedback in terms of MAP. Overall, the precision at 10 is very similar for the different systems, so that there is no bias, with the setting chosen, towards a particular system. We compare here the results obtained with the information models to two state-ofthe-art pseudo-relevance feedback models: Bo2, associated with DFR models ([2]), and the mixture model associated with language models ([20]). For each collection, we average the results obtained over 10 random splits, the variation of N and T C being made on each split so as to be able to compare the results of the different settings. For each setting, we optimize the weight to give to new terms:  (within {0.1, 0.25, 0.5, 0.75, 1, 1.5, 2}) in information and Bo2 models,  ( within {0.1, 0.2, . . . , 0.9}) in the mixture-model for

239

feedback in language models. In this latter case, we set the feedback mixture noise to its default value (0.5). As before, we used Lemur to carry our experiments and optimize here only the mean average precision. Table 9 displays the results for the different models (as before, a two-sided t-test at the 0.05 level is used to assess whether the difference is statistically significant, which is indicated by a ). As one can note, the information models significantly outperform the pseudo-relevance feedback versions of both language models and DFR models. The SPL model is the best one for N = 5 and T C = 5, while the LGD model yields the best performance in most other cases. Altough DFR and information models perform similarly when no feedback is used, their pseudo-relevance feedback versions do present differences, information models outperforming significantly both language and DFR models in this latter case.

Table 8: Performances of baseline setting for PRF (N = 0, T C = 0): bold indicates significant difference

MAP LM+MIX
LGD P10 LM+MIX LGD

ROB-t 25.4 25.4
ROB-t 44.6 44.1

GIRT 41.1 42.4 GIRT 68.3 68.7

T3-t 28.3 27.1 T3-t 56.3 55.3

CLEF-t 37.0 37.5
CLEF-t 27.5 27.2

Table 9: Mean average precision of PRF experiments; bold indicates best performance,  significant

difference over LM and Bo2 models Model N TC ROB-t GIR T3-t

CL-t

LM+MIX 5 5 27.5 44.4 30.7 36.6

LGD

5 5 28.3 44.3 32.9 37.6

INL+Bo2 5 5 26.5 42.0 30.6 37.6

SPL

5 5 28.9 45.6 32.9 39.0

LM+MIX 5 10 28.3 45.7 33.6 37.4

LGD

5 10 29.4 44.9 35.0 40.2

INL+Bo2 5 10 27.5 42.7

SPL

5 10 29.6 47.0

32.6 34.6

37.5 39.5

LM+MIX 10 10 28.4 45.5 31.8 37.6 LGD 10 10 30.0 46.8 35.5 38.9

INL+Bo2 10 10 27.2 43.0 32.3 37.4

SPL

10 10 30.0 48.9 33.8 39.1

LM+MIX 10 20 29.0 46.2 33.7 38.2 LGD 10 20 30.3 47.6 37.4 38.6

INL+Bo2 10 20 27.7 43.5 33.8 37.7

SPL

10 20 29.9 50.2 34.3 39.7

LM+MIX 20 20 28.6 47.9 32.9 37.8 LGD 20 20 29.5 48.9 37.2 41.0

INL+Bo2 20 20 27.4 44.3 33.5 36.8

SPL

20 20 28.8 50.3 33.9 39.0

5. DISCUSSION
The Divergence from Randomness (DFR) framework proposed by Amati and van Rijsbergen [3] is based on the informative content provided by the occurrences of terms in documents, a quantity which is then corrected by the risk of accepting a term as a descriptor in a document (first

normalization principle) and by normalizing the raw occurrences by the length of a document (second normalization principle). The informative content Inf1(tdw) is based on a first probability distribution and is defined as: Inf1(tdw) = - log P rob1(tdw). The first normalization principle is associated with a second information defined from a second probability distribution through: Inf2(tdw) = 1 - P rob2(tdw). The overall IR model is then defined as a combination of Inf1 and Inf2:
RSV (q, d) = X xqwInf2(tdw)Inf1(tdw)
wqd
= X -xqwInf2(tdw) log P rob1(tdw)
wqd
The above form shows that DFR models can be seen as information models, as defined by equation 2, with a correction brought by the Inf2 term. If Inf2(tdw) was not used in DFR models, the models with Poisson, Geometric, Binomial distributions would not respect condition 2, i.e would not be concave. In contrast, the use of bursty distributions in information models, together with the conditions on the normalization functions, ensure that condition 2 is satisfied. Another important difference between the two models is that DFR models make use of discrete distributions for realvalued variables, a conceptual flaw that information models do not have. Lastly, if the log-logistic, SPL and INL models have very simple forms (see for example the formulas given above for the weight they generate), the PL2 DFR model, one of the top performing DFR models, has a much more complex form ([18]). Information models are thus not only conceptually simpler, they also lead to simpler formulas.
6. CONCLUSION
We have presented in this paper the family of information models. These models draw their inspiration from a long standing idea in information retrieval, namely the one that a word in a document may not behave statistically as expected on the collection. Shannon information can be used to capture whenever a word deviates from its average behavior, and we showed how to design IR models based on this information. In particular, we showed that the choice of the distribution to be used in such models was crucial for obtaining good retrieval models, the notion of good retrieval models being formalized here on the basis of the heuristic retrieval constraints developed in [9]. Our theoretical development also emphasized the notion of "burstiness", which has been central to several studies. We showed how this notion relates to heuristic retrieval constraints, and how it can be captured through, e.g., power-law distributions. From these two distributions, we have proposed two effective IR models. The experiments we have conducted on four different collections illustrate the good behavior of these models. They outperform in average the Jelinek-Mercer and Dirichlet prior language models as well as the Okapi BM25 model. They yield results similar to state-of-the-art DFR models (InL2 and PL2) when no pseudo-relevance feedback is used. When using pseudo-relevance feedback, however, the information models we have considered significantly outperform all the other models.

240

Acknowledgements
This research was partly supported by the Pascal-2 Network of Excellence ICT-216886-NOE and the French project Fragrances ANR-08-CORD-008.
7. REFERENCES
[1] E. M. Airoldi, W. W. Cohen, and S. E. Fienberg. Bayesian methods for frequent terms in text: Models of contagion and the 2 statistic.
[2] G. Amati, C. Carpineto, G. Romano, and F. U. Bordoni. Fondazione Ugo Bordoni at TREC 2003: robust and web track, 2003.
[3] G. Amati and C. J. V. Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.
[4] A. L. Barabasi and R. Albert. Emergence of scaling in random networks. Science, 286(5439):509­512, October 1999.
[5] D. Chakrabarti and C. Faloutsos. Graph mining: Laws, generators, and algorithms. ACM Comput. Surv., 38(1):2, 2006.
[6] K. W. Church and W. A. Gale. Poisson mixtures. Natural Language Engineering, 1:163­190, 1995.
[7] S. Clinchant and E´. Gaussier. The BNB distribution for text modeling. In Macdonald et al. [12], pages 150­161.
[8] C. Elkan. Clustering documents with an exponential-family approximation of the dirichlet compound multinomial distribution. In W. W. Cohen and A. Moore, editors, ICML, volume 148 of ACM International Conference Proceeding Series, pages 289­296. ACM, 2006.
[9] H. Fang, T. Tao, and C. Zhai. A formal study of information retrieval heuristics. In SIGIR '04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, 2004.
[10] W. Feller. An Introduction to Probability Theory and Its Applications, Vol. I. Wiley, New York, 1968.
[11] S. P. Harter. A probabilistic approach to automatic keyword indexing. Journal of the American Society for Information Science, 26, 1975.
[12] C. Macdonald, I. Ounis, V. Plachouras, I. Ruthven, and R. W. White, editors. Advances in Information Retrieval , 30th European Conference on IR Research, ECIR 2008, Glasgow, UK, March 30-April 3, 2008. Proceedings, volume 4956 of Lecture Notes in Computer Science. Springer, 2008.
[13] R. E. Madsen, D. Kauchak, and C. Elkan. Modeling word burstiness using the dirichlet distribution. In L. D. Raedt and S. Wrobel, editors, ICML, volume 119 of ACM International Conference Proceeding Series, pages 545­552. ACM, 2005.
[14] S.-H. Na, I.-S. Kang, and J.-H. Lee. Improving term frequency normalization for multi-topical documents and application to language modeling approaches. In Macdonald et al. [12], pages 382­393.
[15] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR '94:

Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 232­241, New York, NY, USA, 1994. Springer-Verlag New York, Inc.
[16] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, Inc., New York, NY, USA, 1983.
[17] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In SIGIR '96: Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 21­29, New York, NY, USA, 1996. ACM.
[18] I. O. V. Plachouras, B. He. University of Glasgow at TREC 2004: Experiments in web, robust and terabyte tracks with terrier, 2004.
[19] Z. Xu and R. Akella. A new probabilistic retrieval model based on the dirichlet compound multinomial distribution. In SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 427­434, New York, NY, USA, 2008. ACM.
[20] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In CIKM '01: Proceedings of the tenth international conference on Information and knowledge management, pages 403­410, New York, NY, USA, 2001. ACM.
[21] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, 2004.

APPENDIX

A. PROOF OF THEOREM 3

Let us recall what property 3 states: Let P be a probability distribution of class C2. A necessary condition for P to be bursty is:

2 log(P (X x2



x))

>

0

Proof Let P be a continuous probability distribution of class C2. y > 0, the function gy defined by:

y > 0,

gy(x) = P (X

 x + y|X

 x) =

P (X  x + y) P (X  x)

is increasing in x (by definition of a bursty distribution).

Let F be the cumulative function of P . Then: gy(x) =

F (x+y)-1 F (x)-1

.

For

y

sufficiently

small,

using

a

Taylor

expansion

of F (x + y), we have:

gy (x)



F (x) + yF (x) - 1 F (x) - 1

=

g(x)

where

F

denotes

F x

.

ering only the sign of

Then, derivating g, we get:

g

wrt

x

and

consid-

sg[g]

=

sg[F F

-

F 

-

F 2]

=

sg[(

F

F -

1 )]

= sg[(log(1 - F ))] = sg[(log P (X  x))]

As

gy

is

increasing

in

x,

so

is

g,

and

thus

2 log(P (Xx)) x2

>

0,

which establishes the property.

241

Score Distribution Models: Assumptions, Intuition, and Robustness to Score Manipulation

Evangelos Kanoulas

Keshi Dai

Virgil Pavlu

Javed A. Aslam

Department of Information Studies University of Sheffield
Regent Court, 211 Portobello Street Sheffield S1 4DP, UK
e.kanoulas@sheff.ac.uk

College of Computer and Information Science Northeastern University
360 Huntington Ave, #202 WVH Boston, MA 02115, USA
{daikeshi, vip, jaa}@ccs.neu.edu

ABSTRACT
Inferring the score distribution of relevant and non-relevant documents is an essential task for many IR applications (e.g. information filtering, recall-oriented IR, meta-search, distributed IR). Modeling score distributions in an accurate manner is the basis of any inference. Thus, numerous score distribution models have been proposed in the literature. Most of the models were proposed on the basis of empirical evidence and goodness-of-fit. In this work, we model score distributions in a rather different, systematic manner. We start with a basic assumption on the distribution of terms in a document. Following the transformations applied on term frequencies by two basic ranking functions, BM25 and Language Models, we derive the distribution of the produced scores for all documents. Then we focus on the relevant documents. We detach our analysis from particular ranking functions. Instead, we consider a model for precision-recall curves, and given this model, we present a general mathematical framework which, given any score distribution for all retrieved documents, produces an analytical formula for the score distribution of relevant documents that is consistent with the precision-recall curves that follow the aforementioned model. In particular, assuming a Gamma distribution for all retrieved documents, we show that the derived distribution for the relevant documents resembles a Gaussian distribution with a heavy right tail.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval] Retrieval models
General Terms: Theory, Measurement
Keywords: information retrieval, score distribution, density functions, recall-precision curve
1. INTRODUCTION
Given a user request an information retrieval system assigns scores to each document in the underlying collection
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

according to some definition of relevance of each document to the user's request and returns a ranked list of documents to the user. In reality, this ranked list of documents is a mixture of both relevant and non-relevant documents. For a wide range of retrieval applications (e.g. information filtering, topic detection, meta-search, distributed IR), modeling and inferring the distribution of relevant and non-relevant documents over scores in a reasonable way can be highly beneficial. For instance, in information filtering, topic detection and recall-oriented retrieval, modeling the score distributions of relevant and non-relevant documents can be utilized to find the appropriate threshold between relevant and non-relevant documents [16, 17, 2, 19, 9, 15]. In distributed IR and meta-search it can be used to normalize document scores and combine different collections or the outputs of several search engines [5, 12].
Inferring the score distribution for relevant and non-relevant documents in the absence of any relevance information is an extremely difficult task, if at all possible. Modeling score distributions is often the basis of any possible inference. Due to this, numerous combinations of statistical distributions have been proposed in the literature to model score distributions of relevant and non-relevant documents. In the 1960s and 70s, Swets attempted to model the score distributions of non-relevant and relevant documents with two Gaussians of equal variance [16], two Gaussians of unequal variance, and two exponentials [17]. Bookstein instead proposed a two Poisson model [7] and Baumgarten a two Gamma model [5]. A negative exponential and a Gamma distribution [12] has also been proposed in the literature. The dominant model has been a negative exponential for the non-relevant documents and a Gaussian for the relevant ones [2, 12, 19]. Bennett [6] observed that when using a two-Gaussians model for text classification, document scores outside the modes of the two Gaussians (corresponding to "extremely irrelevant" and "obviously relevant" documents) demonstrated different empirical behavior than the scores between the two modes (corresponding to "hard to discriminate" documents). This motivated him to introduce several asymmetric distributions to capture these differences. Kanoulas et al. [11] recently proposed a Gamma distribution for the non-relevant documents and a mixture of Gaussians for the relevant documents.
The complexity of the underlying process that generates document scores makes it hard to theoretically argue about the actual distribution of document scores. Most of the aforementioned models were proposed on the basis of empirical fits to scores produced over different document corpora.

242

There have also been several attempts to intuitively argue about the shape of the different distributions. The starting point for most of these attempts has been some basic assumptions about the frequency of query term occurrences in documents (e.g. in Manmatha et al. [12]). Harter [10] and Bookstein and Swanson [8] used a mixture of Poisson distributions to model the distribution of words in a document, with one Poisson corresponding to the distribution of words in relevant documents and the other to the distribution of words in non-relevant documents.
In a different line of work, Arampatzis and van Hameren [2] showed that the distribution of relevant document scores rapidly converges to a Gaussian via the Central Limit Theorem as the number of query terms increases, under some basic assumptions. Further, they claimed that this is not true in the case of non-relevant documents.
Finally, Robertson [14] considered various combinations of distributions and examined whether these combinations exhibit anomalous behavior with respect to theoretical properties of precision and recall. Arampatzis et al. [1] proposed two truncated versions of the exponential-Gaussian model to overcome the theoretical problems associated with the original exponential-Gaussian model.
In this work, we model score distributions in a rather different, systematic manner. We start with a basic assumption on the distribution of terms in a document. Following the transformations applied on term frequencies by two basic ranking functions, BM25 and Language Models, we derive the distribution of the produced scores for all documents in an analytical form and illustrate that the derived distribution can be well approximated by a Gamma distribution.
Further, we also consider the score distribution for relevant documents. We detach our analysis from particular ranking functions. Instead, we consider a simple model for precision-recall curves proposed by Aslam and Yilmaz [3], which makes some very basic assumptions about the shapes of precision-recall curves that are produced by reasonable retrieval system on average. Given this model, we present a general mathematical framework which, given any score distribution for all retrieved documents, produces an analytical formula for the score distribution of relevant documents that is consistent with the precision-recall curves that follow the aforementioned model. In particular, assuming a Gamma distribution for all retrieved documents, we show that the derived distribution for the relevant documents resembles a Gaussian distribution with a heavy right tail.
2. FROM TERM FREQUENCIES TO RETRIEVAL SCORES
Traditional retrieval models score documents based on how well their language matches the language of the user's request. Thus, the essential component of all traditional scoring functions is the number of occurrences of query terms within a document (term frequency, TF). Different retrieval models apply different transformations over the term frequencies to produce a score per query term. The final score of a document is usually an aggregate of the document scores for each individual term.
Before we consider the distribution of term frequencies and the transformation applied by ranking functions over them in an analytical manner we illustrate the evolution of the term frequency distribution for all retrieved documents

(documents that contain at least one of the query terms) for a sample query from the TREC 8 ad hoc collection (Ireland Peace Talks) and for two different retrieval models, BM25 and Language Models, in Figure 1.
The left panel corresponds to the transformation of TF distribution by BM25, while the right panel corresponds to the transformation by the Jelinek-Mercer Language Model.1 Each column then, in both panels, corresponds to an individual query term and each row to progressively more complex transformations of the term frequency. The bottom row plots illustrate the final score distribution by the two retrieval models.
As can be observed, for both retrieval models, there is a critical step in the term frequency transformation (from Row 2 to Row 3) after which the score distribution radically changes and appears to be closer to the final score distribution. Furthermore, the shape of the final score distribution appears to be dominated by the most frequent query term in the collection (as expected) -- for the sample query this is the term talk -- and thus our main goal will be to derive the score distribution for each individual query term.

3. DERIVING THE DISTRIBUTION OF RAW
STATISTICS
For a fixed query, consider a partition of the collection into relevance classes, such that DQ is the class of documents that satisfy the information need to a certain degree Q>0. Depending on several factors like the user, the information need, the collection of documents etc, Q can take a range of values from "completely irrelevant" (the lowest Q) to "extremely relevant" (the highest Q). Note that in test collections (such as TREC) for simplicity only two or three classes are considered. The discussion in this section assumes a fixed quality/relevance class Q, and assumes all documents in the class contain all query terms at least once.
A query term t has a certain contribution to the document quality in response to the user query. For a given document quality Q, we assume an approximately constant probability of seeing the term t at any position in a document in class DQ; hence we can model term t occurrences in documents in class DQ with a Poisson process with rate  = t = f (g, Q), where g = gt relates to the general rarity of the term in the language. Such a model is memoryless and implies that the query term appears equally likely at any moment. We do not model the dependence f -- any monotonic function can be used, depending on the class model.
Counting the occurrences of a term t when reading a random document d  DQ is analogous to counting buses at a bus station: arrive at the station, wait for the first bus, for the second bus, etc., and leave at some point (when the document ends). It is well known that the waiting times w1, w2, w3, . . . among Poisson generated events are exponentially distributed i.i.d. random variables

wl  e-x.

(1)

The average waiting time is  = 1/, the mean of the exponential distribution. Intuitively,  corresponds to a notion of the expected ratio of document length to term frequency, i.e., DL/TF.

1The parameter values used for BM25 are k1=1.2 and b=0.75, and  = 0.2 for the Jelenik-Mercer Language Model.

243

6000 4000 2000
0 0
600 400 200
0 0
200

ireland, 6.155, 7698 docs TF

5

10

15

20

25

30

DL / TF

200

400

600

800

(k1+1)*TF / (k1*DL/AVDL + TF)

1000

100

0 0
200

0.5

1

1.5

2

2.5

(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF)

100

0

0

0.5

1

1.5

2

2.5

Roberston's IDF*(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF) 200

100

0 0
4000

5

10

15

x 104 2

peac, 3.876, 35454 docs TF

1

0 0
1500 1000
500 0 0
1000

5

10

15

20

25

30

DL / TF

200

400

600

800

(k1+1)*TF / (k1*DL/AVDL + TF)

1000

500

0 0
1000

0.5

1

1.5

2

2.5

(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF)

500

0

0

0.5

1

1.5

2

2.5

Roberston's IDF*(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF) 1000

500

0

0

2

4

6

8

10

BM25 Scores

x 104 6

talk, 2.777, 70795 docs TF

4

2

0 0
2000

5

10

15

20

25

30

DL / TF

1000

0 0
1500 1000
500 0 0
2000

200

400

600

800

(k1+1)*TF / (k1*DL/AVDL + TF)

1000

0.5

1

1.5

2

2.5

(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF)

1000

0

0

0.5

1

1.5

2

2.5

Roberston's IDF*(k1+1)*TF / (k1((1-b)+b*DL/AVDL) + TF) 2000

1000

0

0

1

2

3

4

5

6

2000

0

0

5

10

15

20

25

30

6000 4000 2000
0 0
1000

ireland, 6.155, 7698 docs TF

5

10

15

20

25

30

Normalized TF

500

0 0
300

0.02

0.04

0.06

0.08

0.1

log(Normalized TF)

200

100

0

-14 -12 -10 -8

-6

-4

-2

log(Normalized TF + CTF/TN) 300

200

100

0

-10

-8

-6

-4

-2

log(lambda*Normalized TF + (1-lambda)*CTF/TN) 200

100

0

-10

-8

-6

-4

-2

4000

x 104 2

peac, 3.876, 35454 docs TF

1

0 0
4000

5

10

15

20

25

30

Normalized TF

2000

0 0
2000

0.02

0.04

0.06

0.08

0.1

log(Normalized TF)

1000

0

-14 -12 -10 -8

-6

-4

-2

1000

log(Normalized TF + CTF/TN)

500

0

-8

-7

-6

-5

-4

-3

-2

log(lambda*Normalized TF + (1-lambda)*CTF/TN) 1000

500

0

-8

-7

-6

-5

-4

-3

BM25 Scores

x 104 6

talk, 2.777, 70795 docs TF

4

2

0 0
15000

5

10

15

20

25

30

Normalized TF

10000

5000

0 0
3000

0.02

0.04

0.06

0.08

0.1

log(Normalized TF)

2000

1000

0

-12

-10

-8

-6

-4

-2

2000

log(Normalized TF + CTF/TN)

1000

0

-8

-7

-6

-5

-4

-3

-2

log(lambda*Normalized TF + (1-lambda)*CTF/TN) 3000

2000

1000

0

-8

-7

-6

-5

-4

-3

2000

0

-24

-22

-20

-18

-16

-14

-12

Figure 1: The empirical histograms of term frequencies evolving and resulting to the final scores for a sample query (Ireland Peace Talks) over the TREC 8 Ad Hoc Track collection for both BM25 (left) and JM Language Model (right). Each column corresponds to single query term while the rows correspond to progressively more complex transformations of the term frequency (TF) up to the final score for the two ranking functions. DL is the document length, ADL is the average document length, CTF is the collection term frequency, and TN is the number of terms in the collection.

Our purpose is to model the distribution of the random variable DL/TF for documents in class DQ. We will do so separately for each frequency and then express the general distribution as a mixture.
Let us now fix a term frequency k = 1, 2, 3, . . . and denote DQk = {d  DQ | T F (t, d) = k} the set of documents in DQ that contain term t exactly k times. Here, we make the approximation that the document ends exactly after the k-th othcecusrurmencoef ,kanwdaistoinwg eticmaneswPritkl=e 1thwel ,dwohcuicmh einmtmleendgitahteDlyLimasplies that DL is Gamma distributed (and more specifically Erlang-distributed), with shape k and scale  = 1/ :

DLQk  Gamma(k, ).

(2)

Since k is a constant for the subclass DQk, the waiting time XQk is also Gamma distributed:

XQk

=

DocLength T ermF requency

=

DL k



Gamma(k, /k).

(3)

Since the quality class DQ is partitioned into the classes DQk for k = 1, 2, 3, . . ., the waiting time X on DQ follows a mixture of Gamma distributions with a constant mean ,

while DL on DQ follows a mixture of Gamma distributions

with a constant scale :

X

DLQ  PQ(k) · Gamma(k, )

(4)

k

X

XQ  PQ(k) · Gamma(k, /k)

(5)

k

where PQ(k) = P r[T F (d, t) = k | d  DQ] denotes the probability that a document in class DQ contains the term t exactly k times.
Assuming a constant probability p that a term occurrence gives quality Q, PQ(k) can be expressed as probability of k - 1 failures (term occurrences that do not imply quality Q) followed by one success (term occurrence when quality Q is reached); therefore we model the mixture probabilities

PQ(k) with a geometric distribution (equivalent to a nega-

tive binomial distribution with  = 1),

PQ(k) = p(1 - p)k-1

(6)

where p = pt = /ADLQ expresses the correlation between the term and the information need on the class DQ (the average document length, the general rarity of the term t, and the quality Q). For example, p = 0.5 implies that there are twice as many documents containing k terms than documents containing k + 1 terms in the class DQ. Intuitively p can be thought as a notion of inverse term frequency:

p = /ADLQ  avg(DL/T F )/ADLQ  avg(1/T F ).

Note that a number of different mixtures could be used,

perhaps based on the query type. For instance, an infor-

mational query could use a negative binomial or a Poisson

mixture. For the particular case of a geometric mixture how-

ever, an interesting result follows: Neuts and Zachs [13] show

that under certain conditions similar to ours, a negative bi-

nomial mixture of Gamma distributions with constant scale

is actually itself a Gamma distribution. With a different

noX tation, their result is pk · Gamma( + k, ) = Gamma(, /p) when (7)

k

!

pk = N egBinomial(p, ) =

k + -1  -1

p(1 - p)k

(8)

Applying this on DL (with =1) implies that DL is exponentially distributed on DQ with mean /p. Of course this must hold for all query terms, not only for t, which requires a proportionality /p = constant = ADLQ. In practice, for a given quality class, the document length variable will not be exactly exponentially distributed for two reasons: (1) relevance judgments cover a range of qualities inducing an average effect, (2) our Poisson process model for query term occurrence works reasonably well for frequent terms, but can fail on rare terms. However, this model is fairly accurate in that DL can be modeled well by a Gamma distribution

244

with a small shape parameter (the exponential distribution is Gamma with shape = 1.)
Figure 2 illustrates the empirical histogram of DL/TF for the query term system. As can be observed, a Gamma distribution appears to be a good approximation of the empirical score distribution, offering empirical evidence that the assumptions and approximations in our theory are reasonable.2
0.03
Empirical Histogram MLE Gamma Fit
0.025

0.02

0.015

0.01

0.005

0

0

100

200

300

400

500

600

700

800

900 1000

Figure 2: The empirical histogram and the Gamma

density

function

fit

over

the

DL TF

scores

for

term

sys-

tem in TREC8.

A rudimentary transformation of interest is just the in-

verse of X = DL/TF, which gives the normalized term fre-

quency T F/DL. DL/T F  fX =

PAccording to k1 PQ(k) 

the previous section, X =

Gamma(k,

 k

).

It is known

that a mixture of Gamma can approximate any smooth func-

tion [18]. By approximating PQ(k) with a geometric distribution inverting T F/DL has the effect displayed in Figure 3.

A relevant class of documents (high Q) implies:

· the geometric rates 1 - p = 1 - 1/( · ADL) for query terms are higher, which means the mean 1/p is higher, or the mixture PQ will have non-negligible coefficients for higher scale parameters k. This will make the mixture look more "hill"-like due to more effective components.

· for each query term, the Poisson generating process will be governed by a higher rate, 1/, which dictates a lower mean to all Gamma components of the mixture, or a "light" right-side tail. When the inverse transformation is performed (see below), the result distribution will have a heavier tail.

Conversely, a lower quality Q implies a mixture with effectively significant coefficients only for the lower k values, and also that the components of the mixture are less skewed towards the left-side, overall producing a more exponentiallike distribution (after inversion).

4. DERIVING THE SCORE DISTRIBUTION FROM SCORING FUNCTIONS
In this section, we derive the score distribution of the retrieved documents in a systematic manner. We consider the transformation applied on the distribution of the elementary statistics described in the previous section by two scoring functions, BM25 and Jelinek-Mercer Language Model. The derivations presented here can be applied in the case of other retrieval models, such as TF-IDF and Divergence From Randomness (DFR).

4.1 Score Transformations
Consider a transformation of the random variable X by a monotonic, differentiable function r, Y = r(X). The probability density function (pdf) of Y , fY (y), can then be computed as a function of the pdf of X, fX (x) [4]. Let FY (y) and FX (x) be the cumulative density function (cdf) of Y and X, respectively. Without loss of generality let r be a non-decreasing function. Then,

FY (y) = P r{Y  y} = P r{r(X)  y} = P r{X  r-1(y)} = FX (r-1(y)) and

fY (y)

=

d dy FY (y)

=

d dy

FX

(r-1(y))

=

r-1(y) y

· fX (r-1(y))

In the general case of a monotonic function r,

fY

(y)

=

|

r-1(y) y

|

·

fX (r-1(y))

2Some fits will be better than others, depending on the ex-

ample. No theoretical model will fit all empirical examples,

of course.

Figure 3: Mixture of gamma before and after the inversion, for different quality classes

Note that in practice fitting a Gamma, an inverse Gamma or an inverse Gaussian distribution in the T F/DL scores of existing collections/judgments (like TREC) are likely to differ in goodness-of-fit mostly due to random effects than other theoretical reasons - this is primarily due to complex score manipulations, and due to the sparsity and inaccuracy of the judgment process.

4.2 BM25 and Jelinek-Mercer LM

Assuming that query terms appear only once within a

query the BM25 for a single query term can be calculated

as:

BM25

score

=

(k1 + 1)TF

k1

((1

-

b)

+

b

DL ADL

)

+

TF

· IDF

(9)

where TF is the term frequency, IDF is the BM25 inverse document frequency, DL is the document length, and ADL is the average document length in the collection. By setting

245

Frequency Frequency

0.045 0.04
0.035

BM25 score histogram Analytically Numerical MLE Gamma fit Model (theory)

0.03

0.025

0.02

0.015

0.01

0.005

0

0

1

2

3

4

5

6

BM25 score

0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01
0 -7

BM25 score histogram Analytically Numerical MLE Gamma fit Model (theory)

-6

-5

-4

-3

-2

-1

LM (Jelinek-Mercer smoothing) score

Figure 4: The empirical histograms, analytically numerical plot, and MLE Gamma fitting of the distribution of approximated BM25 scores and JM language model scores for term system in TREC8.

the parameter b equal to 1 (fixing the document length normalization) and defining the variable X = DL/TF , BM25 can be approximated by,

Y = r(X) = IDF (k1 + 1) , X > 0

(10)

CX + 1

where C = k1/ADL. Given Equation 10 it can be shown

that

r-1(Y )

=

IDF (k1 + 1) - Y CY

.

Now,

let

fX (x) be the

pdf

of X and fY (y) the pdf of Y . Since function r is a monotonic

and differentiable when X is positive, based on the principle

of function transformations of random variables [4], we can

calculate the pdf of Y as a function of the pdf of X,

fY (y)

=

-IDF (k1 Cy2

+

1)

fX

(

IDF

(k1 + Cy

1)

-

y

)

(11)

when 0 < y < IDF (k1 + 1) and 0 otherwise. In other words we can model the pdf of an approxima-
tion of BM25 as a function of the density function of the reverse relative term frequency. Essentially, one can plug in the above formula any distribution for the relative term frequency and get an analytical form distribution of BM25.
Based on the previous section DL/T F approximately follows a Gamma distribution. Let k^ and ^ are estimated parameters of the Gamma distribution from X via maximum likelihood estimation (MLE) for all retrieved documents (see Figure 2). Then, the approximated pdf of BM25 score for a single term can be reached as follows,

fY

(y)

=

-IDF (k1 Cy2

+

1) Gamma( IDF (k1 + Cy

1)

- y ; k^, ^)

(12)

We repeat the exact same derivation in the case of lan-

guage models with Jelinek-Mercer smoothing. The score for

each term is computed as,

JMLM

score

=

TF log (

+ C(1 - ))

(13)

DL

where C = CTF /TN . CTF is collection term frequency and TN is the number of unique terms in the collection. As before, we let X = DL/TF , then the LM score can be

written as,

Y = r(X) = log (  + C(1 - ))

(14)

X

Using the previous assumption that DL/TF is modeled by a Gamma distribution and since the function r is a monotonic and differentiable, after the random variable transform over X we get the pdf of the LM scores as a function of the Gamma distribution that models the reverse relative term frequency.

fY

(y)

=

(ey

-ey - C(1 - ))2 Gamma( ey

-

 C(1

-

)

;

k^,

^)

(15)

Figure 4 shows the comparison among the empirical his-

togram, the analytical model derived from the distribution

of DL/T F , and the Gamma distribution obtained by MLE

over BM25 and JM language model scores all retrieved docu-

ments for query system in TREC8 collection. As illustrated

on the plots, the analytical model has more freedom than

the Gamma distribution, but the Gamma is still a reason-

able approximation to the term score distribution. Further,

the mixture model presented in the previous section with

the best-fit  is also shown on Figure 4 (black line denoted

as "Model (theory)" in the legend).

Remark on the Shape of the Distribution
Most term frequency weighting functions are nonlinear monotonically increasing functions of the raw term frequency. In BM25 Roberston's TF grows fast when the raw term frequency is small and gets gradually saturated. The parameter k1 controls the speed of the saturation. The logarithm function in Language Models also has this saturation property but without the power of controlling the saturating speed. Therefore, the JM language model scoring function has a similar to BM25 impact on transforming the distribution of low level statistics, such as DL/TF or normalized TF to the final score distribution.
As it is illustrated in Figure 2 the typical shape of the distribution for the DL/TF tends to have a long right tail but a fast rising-up left tail. After applying a transforma-

246

tion function with the saturation property, the imbalance between two tails of the original distribution is alleviated, so the peak of the new distribution is right shifted, and with a shorter right tail compared to the original one. The amount of difference is dominated by the parameter controlling the saturating speed. This can be viewed in Figure 5. As k1 becomes larger and the weighting function more linear the empirical histograms of BM25 looks more similar to the distribution of DL/TF in Figure 2. This implies that the term score distribution can also be approximated by a Gamma distribution by adjusting the shape and the scale parameters.
4.3 Summation over Query Terms
In this paper we have considered scorinP g functions with the following property: score(d,query) = tquery r(Xt), where Xt = DL/tf (t, d). This class of scoring functions includes BM25, TF-IDF, some Language Models etc, but does not include scores like PageRank. Assuming terPm independence, the intuition for the summation score = t r(Xt) is as follows:
· For non-relevant documents (low quality Q) each r(Xt) will be distributed approximately as a Gamma(low shape, low scale). If the scales are approximately equal their sum follows a Gamma distribution with the same scale (gamma distribution exhibits infinite divisibility).
· For relevant documents, the mixture for each term has more effective components, thus making the sum a rich mixture, usually Gaussian like (or Gamma-like with higher scale and shape).
Thus, the distribution of the summation of several term scores could also be modeled using a Gamma distribution if we use a Gamma distribution to model the term score distribution. Figure 6 shows this summation process.

5. INFERRING THE SCORE DISTRIBUTION

OF RELEVANT DOCUMENTS

In this section, we relate the score distributions for rele-

vant and non-relevant documents with precision-recall curves.

That the score distributions for relevant and non-relevant

documents are related to precision-recall curves is well known

and unsurprising: Given the two score distributions, one can

easily infer a precision-recall curve [14], and we shall do so

below as part of the treatment that follows. More interest-

ingly, we demonstrate that one can infer the score distri-

bution for relevant documents given a score distribution for

non-relevant documents and a precision-recall curve, and we

use the technique described to show that the score distribu-

tions for relevant documents will tend to have a Gaussian-

like form, with a heavy right tail.

Let fR(s) and fN (s) be the score distributions for rel-

evant and non-relevant documents, respectively. For any

score threshold t, consider the set of documents whose scores

are t or higher. The recall and fallout associated with this

document set are easily defined in terms of fR(s) and fN (s)

as follows:

Z

r(t) =

fR(s) ds

(16)

Zt 

fo(t) =

fN (s) ds.

(17)

t

Robertson's TF

5

k1=1

4

k1=3

k1=5
3

2

1

0

0

2

4

6

8 10 12 14 16 18 20

TF

Frequency

0.035 0.03
0.025 0.02

k1=1 k1=3 k1=5

0.015

0.01

0.005

0

0

1

2

3

4

5

6

BM25 Scores

Figure 5: Roberston's TF and empirical histograms of BM25 scores with different k1 for term system in TREC8

0.04 0.03 0.02 0.01
0 0

0.05

0.04

0.03

0.02

0.01

0

5

10

15

0

0.05

0.04

0.03

0.02

0.01

0

5

10

0

2

4

6

0.05 0.04 0.03 0.02 0.01
0 0

BM25 Scores Histogram MLE Gamma Fit

5

10

15

20

25

30

Figure 6: MLE Gamma fitting over scores of all retrieved documents for all query terms and query "Ireland Peace Talks"

Now let C be the size of the collection and let  be the fraction of the collection that is relevant to a given query. Then there are R =  C total relevant documents and N = (1 - )C total non-relevant documents. At score t or above, there are
R · r(t) =  C · r(t)

relevant documents and

N · fo(t) = (1 - )C · fo(t)

non-relevant documents. Thus, the precision associated with this document set is simply

 C · r(t)

r(t)

p(t) =  C · r(t) + (1 - )C · fo(t) = r(t) + O · fo(t) (18)

where O = (1 - )/ is the odds of non-relevance in the collection. Equations 16 and 18 are parametric equations

247

defining a precision-recall curve: Given the score distribu-

tions fR(s) and fN (s) (and ), one can vary the score threshold t in Equations 16 and 18 to obtain the precision-recall

curve. (A substantially similar treatment can be found in

Robertson [14].) Now suppose that one has a candidate score distribution
for either relevant or non-relevant documents and one has a

candidate form for a precision-recall curve: Can one derive

a form for the other score distribution? In what follows,

we show how this can be accomplished, and using the score distributions described in Section 4 and a simple form for precision-recall curves, we infer a form for the score distributions of relevant documents.

Consider the simple model for precision-recall curves de-

scribed by Aslam and Yilmaz [3] and shown in Figure 7.

This family of precision-recall curves is defined by the following equation, implicitly parameterized by the value of R-precision rp:

p(r)

=

1-r 1+·

. r

(19)

(Here  = (1/rp - 1)2 - 1 governs the "shape" of the curve.)

While it is certainly the case that"real" precision-recall curves

are never this "clean", this simple model captures many

properties found in real precision-recall curves, such as high

precisions at low recall levels, low precisions at high recall

levels, and so on. Furthermore, Aslam and Yilmaz show that

this simple model allows one to explicitly and accurately re-

late average precision, R-precision, precision-at-cutoff, and

other seemingly disparate measures of retrieval performance.

Using such a model for precision-recall curves, we can re-

late the score distributions for relevant and non-relevant doc-

uments as follows. We first parameterize Equation 19 by the

score threshold t, obtaining

p(t)

=

p(r(t))

=

1

1 - r(t) +  · r(t)

.

(20)

We now equate Equations 18 and 20

r(t) r(t) + O · fo(t)

=

1 - r(t) 1 +  · r(t)

and solve for r(t) as a function of fo(t)

p

-O · fo(t) + (O · fo(t))2 + 4(1 + )O · fo(t)

r(t) =

(21)

2(1 + )

Differentiating Equation 21 by t immediately establishes a closed-form relationship between the score distributions for relevant and non-relevant documents, since by Equations 16 and 17 and the Fundamental Theorem of Calculus, we have

r (t) = -fR(t) fo (t) = -fN (t).

As an example of this methodology, let us assume that the score distribution for all documents follows a Gamma distribution, as we argued in Section 4. Since the overwhelming majority of documents are non-relevant, the score distribution for non-relevant documents will then tend to follow a Gamma distribution as well. Now consider the Gamma that fits the non-relevant documents for the TREC8 query "Estonia Economy". Using this Gamma distribution for the nonrelevant documents, together with a precision-recall curve3
3We set  and  = (1/rp -1)2 -1 to match those parameters from the BM25 run on that query.

from the family show in Equation 19, and employing the method described above, we obtain the score distribution for relevant documents shown in Figure 8.
While Figure 8 gives just one such example, the form of this curve is quite consistent across all tested input distributions from the Gamma family (which includes the negative exponential distribution) and all tested precision-recall curves from the family defined by Equation 19: The distribution is roughly Gaussian in form, but with a heavy right tail. That the score distribution is "Gaussian-like" is much assumed (as discussed in the introduction), but the heavy right tail is also necessary to avoid problems with a simple Gaussian, such as those described by Manmatha et al. [12] and others. Figure 9 shows the typical form of the relevant document score distribution we obtained in TREC 8. We here for the first time derive such a form, given reasonable forms for non-relevant score distributions and precisionrecall curves.
Our results in this section are descriptive rather than prescriptive, and as such, we conclude the following:
The tendency of the score distributions for relevant documents to look Gaussian with a heavy right tail is a natural and inevitable consequence of the facts that (1) the score distributions of non-relevant documents tend to look Gamma and (2) precision-recall curves tend to have the form shown in Figure 7.
6. CONCLUSIONS
In this work, we attempt to model score distributions in a rather systematic manner. We start with a basic assumption that query terms are generated via a Poisson process and induced that the distribution the relative term frequency in a document is a inverse Gamma distribution. Following the mathematical transformations applied on the relative term frequencies by two basic ranking functions, BM25 and Language Models, we derived the distribution of the produced scores, in an analytical form and illustrate that the derived distribution can be well approximated by a Gamma distribution. Further, we also considered the score distribution for relevant documents by relating score distributions with precision-recall curves. In particular, we adopted a precision-recall curve model that has previously been proposed and given this model we presented a general mathematical framework under which given any score distribution for all retrieved documents we can derive an analytical formula for the score distribution of relevant documents. The framework is general enough such that the same derivations can be repeated for different models of precision recall curves. Finally, under the assumption that non-relevant documents follow a Gamma distribution for all retrieved documents, we show that there is a tendency of the derived distribution for the relevant documents to look Gaussian with a heavy right-hand tail.
7. ACKNOWLEDGEMENTS
We gratefully acknowledge the support provided by the NSF grants IIS-0533625 and IIS-0534482 and by the European Commission grant FP7-ICT-248347 (Accurat project) and the Marie Curie Fellowship FP7-PEOPLE-2009-IIF-254562.

248

precision

Precision-recall curves for various values of rp
1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.2

0.4

0.6

0.8

1

recall

Figure 7: A family of precisionrecall curves fit through the points {(0, 1), (rp, rp), (1, 0)} for rp = 0.1, 0.2, . . . , 0.9.

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

5

6

7

8

9

10

11

Figure 8: Inferred relevant document score distribution and empirically histogram for the TREC8 query "Estonia, economy".

0.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01
0 0

5

10

15

20

25

30

Figure 9: Typical form of the relevant document score distribution in TREC8.

8. REFERENCES
[1] A. Arampatzis, J. Kamps, and S. Robertson. Where to stop reading a ranked list?: threshold optimization using truncated score distributions. In SIGIR '09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 524­531, New York, NY, USA, 2009. ACM.
[2] A. Arampatzis and A. van Hameren. The score-distributional threshold optimization for adaptive binary classification tasks. In SIGIR '01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 285­293, New York, NY, USA, 2001. ACM.
[3] J. A. Aslam and E. Yilmaz. A geometric interpretation and analysis of R-precision. In Proceedings of the Fourteenth ACM International Conference on Information and Knowledge Management, pages 664­671. ACM Press, October 2005.
[4] R. D. Barr and W. P. Zehna. Probability: Modelling Uncertainty. Addison-Wesley, 1983.
[5] C. Baumgarten. A probabilistic solution to the selection and fusion problem in distributed information retrieval. In SIGIR '99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 246­253, New York, NY, USA, 1999. ACM.
[6] P. N. Bennett. Using asymmetric distributions to improve text classifier probability estimates. In SIGIR '03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 111­118, New York, NY, USA, 2003. ACM.
[7] A. Bookstein. When the most "pertinent" document should not be retrieved--an analysis of the swets model. Information Processing & Management, 13(6):377­383, 1977.
[8] A. Bookstein and D. R. Swanson. Probabilistic models for automatic indexing. Journal of the American Society for Information Science, 25(5):312­318, 1974.
[9] K. Collins-Thompson, P. Ogilvie, Y. Zhang, and J. Callan. Information filtering, novelty detection, and named-page finding. In In Proceedings of the 11th Text Retrieval Conference, 2003.

[10] S. P. Harter. A probabilistic approach to automatic keyword indexing: Part i. on the distribution of specialty words in a technical literature. Journal of the American Society for Information Science, 26(4):197­206, 1975).
[11] E. Kanoulas, V. Pavlu, K. Dai, and J. A. Aslam. Modeling the score distributions of relevant and non-relevnat documents. In In Proceedings of the 2nd International Conference on the Theory of Information Retrieval, September 2009.
[12] R. Manmatha, T. Rath, and F. Feng. Modeling score distributions for combining the outputs of search engines. In SIGIR '01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 267­275, New York, NY, USA, 2001. ACM.
[13] M. F. Neuts and S. Zacks. On mixtures of 2- and f-distributions which yield distributions of the same family. Annals of the Institute of Statistical Mathematics, 19(1):527­536, 1966.
[14] S. Robertson. On score distributions and relevance. In G. Amati, C. Carpineto, and G. Romano, editors, Advances in Information Retrieval, 29th European Conference on IR Research, ECIR 2007, volume 4425/2007 of Lecture Notes in Computer Science, pages 40­51. Springer, June 2007.
[15] M. Spitters and W. Kraaij. A language modeling approach to tracking news events. In In Proceedings of TDT workshop 2000, pages 101­106, 2000.
[16] J. A. Swets. Information retrieval systems. Science, 141(3577):245­250, July 1963.
[17] J. A. Swets. Effectiveness of information retrieval methods. American Documentation, 20:72­89, 1969.
[18] M. Wiper, D. R. Insua, and F. Ruggeri. Mixtures of gamma distributions with applications. Journal of Computational and Graphical Statistics, 10(3):440­454, September 2001.
[19] Y. Zhang and J. Callan. Maximum likelihood estimation for filtering thresholds. In SIGIR '01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 294­302, New York, NY, USA, 2001. ACM.

249

Geometric Representations for Multiple Documents

Jangwon Seo jangwon@cs.umass.edu

W. Bruce Croft croft@cs.umass.edu

Center for Intelligent Information Retrieval Department of Computer Science
University of Massachusetts, Amherst Amherst, MA 01003

ABSTRACT
Combining multiple documents to represent an information object is well-known as an effective approach for many Information Retrieval tasks. For example, passages can be combined to represent a document for retrieval, document clusters are represented using combinations of the documents they contain, and feedback documents can be combined to represent a query model. Various techniques for combination have been introduced, and among them, representation techniques based on concatenation and the arithmetic mean are frequently used. Some recent work has shown the potential of a new representation technique using the geometric mean. However, these studies lack a theoretical foundation explaining why the geometric mean should have advantages for representing multiple documents. In this paper, we show that the arithmetic mean and the geometric mean are approximations to the center of mass in certain geometries, and show empirically that the geometric mean is closer to the center. Through experiments with two IR tasks, we show the potential benefits for geometric representations, including a geometry-based pseudo-relevance feedback method that outperforms state-of-the-art techniques.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms
Algorithms, Measurement, Experimentation
Keywords
multiple documents, information geometry, geometric mean
1. INTRODUCTION
A typical goal in Information Retrieval (IR) is to find relevant documents, where we rank the documents using a representation for a single document. Often, however, a
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

representation for multiple documents is needed. For example, tasks such as relevance feedback, passage retrieval and resource selection in distributed information retrieval or in aggregated search, use representations for sets of multiple documents.
One of standard approaches for relevance feedback is to estimate an underlying relevance model from given feedback documents and sample likely terms from the model for query expansion. That is, the estimated underlying model can be considered as a representation of the feedback documents. In passage retrieval, representations of text passages can be used to rank passages or documents. In the latter case, we represent a document using a combination of some or all of its passages. In resource selection tasks, the resource or collection is represented using the documents in the collection.
As many tasks require representations for multiple documents, various approaches have been introduced. Among them, representation techniques based on the arithmetic mean and concatenation are frequently used. Representation techniques based on the arithmetic mean literally compute the arithmetic mean of multiple language models or vector representations. Representation techniques based on concatenation make a large document by concatenating multiple documents and use a language model or vector to represent the large document.
In addition to traditional group representation techniques, some recent studies show the potential of a new representation technique, the geometric mean representation of language models [26, 30, 11, 31]. Liu and Croft [26] compared various representation techniques for cluster retrieval and demonstrated that representations using the geometric mean outperformed others via empirical evaluation. Seo and Croft [30] applied a resource selection technique based on the geometric mean to blog site search. Moreover, Elsas and Carbonell [11] and Seo et al. [31] showed that a thread representation using the geometric mean of postings in the thread can be a good choice for online forum search.
The previous work which uses the geometric mean to represent a group of documents, however, did not theoretically analyze the geometric mean in the language modeling framework. In other words, although they have demonstrated the performance of representation techniques based on the geometric mean empirically, theoretical evidence or the assumptions behind the geometric mean have not been sufficiently addressed to justify its use in IR.
Therefore, in this paper, we give a theoretically grounded explanation for geometric mean-based techniques for representing multiple documents. To do this, we consider Information Geometry as a tool and discuss how the arithmetic mean as well as the geometric mean can be inter-

251

preted in certain geometries. More specifically, we show that the arithmetic mean and the geometric mean relate to the Fr´echet sample mean which minimizes the Fr´echet sample function. Furthermore, we empirically show that the geometric mean is closer to the Fr´echet mean.
In addition, we address two applications considering the geometric interpretation: cluster retrieval and pseudo-relevance feedback. Particularly, for pseudo-relevance feedback, we introduce a variation of the relevance model [21], the geometric relevance model, and show that this new approach performs better than the relevance model.
The remainder of this paper is organized as follows. Section 2 reviews previous work. In Section 3, we introduce the Fr´echet mean and geometric representations correspond to the Fr´echet mean in two different metric spaces using Information Geometry. In Section 4, we provide empirical evidence for the geometric representations through experiments for two IR tasks. Section 5 discusses other evidence for the geometric representations. Section 6 concludes this paper.
2. PREVIOUS WORK
Combining multiple evidence is one of the most frequently addressed topics in Information Retrieval. Belkin et al. [2] showed that different representations of the same information object leads to different results and combinations of such representations can improve retrieval performance. Various combination heuristics suggested by Fox and Shaw [12] and analyzed by Lee [23] are still used in many IR tasks such as passage retrieval and resource selection. Using passage-level evidence [7, 25, 3] for document retrieval necessarily requires combination techniques. Resource selection where a collection is represented by its own documents [6, 32] actively uses combination techniques as well.
Relevance feedback (and pseudo-relevance feedback) is another task using combination-based representation techniques. To estimate a query model for query expansion, the top ranked documents are combined. Rocchio [29] introduced a feedback technique to combine positive or negative feedback documents in vector spaces. Lavrenko and Croft [21] introduced a technique that estimates a underlying relevance model in the language modeling framework. In fact, these standard relevance feedback approaches implicitly use the arithmetic mean. Recently, Collins-Thompson and Callan [9] used a parametric approach using re-sampling to estimate a posterior Dirichlet distribution for the documents. That is, they use the mean and the variance of the Dirichlet distribution to get a feedback model.
The geometric mean-based representation technique was relatively recently introduced. Liu and Croft [26] demonstrated that representation by the geometric mean works well for cluster retrieval via comparisons with vairous representation techniques. Seo and Croft [30] suggested a resource selection technique by the geometric mean for blog site retrieval. Furthermore, the technique was shown to work well for thread search in online forums [11, 31]. The geometric mean is often used in other fields. For example, Kogan et al. [18] used the geometric mean for k-means clustering. Veldhuis [34] showed that a centroid of the symmetrical Kullback-Leibler divergence is related to the arithmetic mean and the normalized geometric mean.
In this paper, to justify the use of the geometric mean in IR, we find evidence from Information Geometry. Rao

[28] and Jeffreys [14] are the first people who considered the Fisher information metric as a Riemannian metric. Later, Efron [10] focused on differential geometry in statistics considering the curvature of statistical models. Recently, Lebanon [22] applied the theory to many machine learning tasks. See Amari and Nagaoka [1] and Kass and Vos [16] for comprehensive introduction to Information Geometry.

3. GEOMETRY OF MULTIPLE DOCUMENTS
We introduce the Fr´echet mean and derive the mean in two different metric spaces, i.e., the Euclidean metric space and the Riemannian manifold defined by the Fisher information metric.

3.1 Fréchet Mean
Let us consider a Riemannian manifold M with a distance measure dist(x, y) where x and y are points on the manifold. Assume that we have a distribution Q on a convex set U  M. Now we define a function F : M  R as follows:

(c) =

dist2(c, p)Q(dp)

pU

This function is known as the Fr´echet function. A set of points which minimize the function is called the Fr´echet mean set of Q. If there is only a point in the set, the point is called the Fr´echet mean. This general notation for a center or centroid associated with a probability distribution was introduced by Fr´echet [13] and Karcher [15]. This mean is called by various names, e.g., the center of mass, barycenter, Karcher mean and Fr´echet mean. In this work, we refer to this mean as the Fr´echet mean1. The concept of the Fr´echet mean is general and not limited to any specific metric; accordingly, this can be applied to any metric space. Indeed, as we will see soon, it also generalizes the ordinary Euclidean mean.
Kendall [17] proved that if the support of Q is in a geodesic ball of sufficiently small radius r, then one Fr´echet mean uniquely exists. As we see later, we consider a statistical manifold for multinomial distributions, and the distributions are mapped onto a simplex or a positive sphere. Since the mapped area is sufficiently small, a unique Fr´echet mean exists. For example, in case of a sphere, the radius of the geodesic ball is /4 and the positive sphere is contained in the ball.
If we have n unique points p1, p2, · · · , pn in m i.i.d. samples from distribution Q, then we consider the sample Fr´echet mean which minimizes the Fr´echet sample function given by

n

¯ (c) = dist2(c, pi)Q^(pi)

(1)

i=1

where Q^ is an empirical distribution estimated from the samples.
Bhattacharya and Patrangenaru [5] showed that every measurable choice from the Fr´echet sample mean set of Q^ is a strongly consistent estimator of the Fr´echet mean of Q. In this paper, we consider multiple documents to represent as samples and the Fr´echet sample mean as a representation.

1Strictly speaking, this is the intrinsic Fr´echet mean in that we use a geodesic distance. However, since we address only the intrinsic Fr´echet means in this paper, we omit term "intrinsic".

252

1

2

0

0

0

0

0

0

1

1

2

2

Figure 1: Assuming the Euclidean metric space, a n + 1 dimensional multinomial distribution is mapped to a point in the n-simplex in Euclidean space (left). Assuming the Riemannian manifold defined by the Fisher information metric, the same point is mapped to a point in the positive n-sphere of radius 2 (right).

Therefore, we address how to compute the sample Fr´echet mean from the multiple documents in the following sections.

3.2 Euclidean Metric space
Let's begin with the Euclidean metric space. We assume that terms observed in a document are samples from a multinomial distribution and each document has a distinct distribution. Assuming a conjugate Dirichlet prior, we estimate the multinomial distribution, i.e. a language model, using Dirichlet smoothing [35] as follows:

Pr(w|D)

=

tfw,D

+  · cfw/|C| |D| + 

(2)

where tfw,D is the occurrence of term w in document D, cfw is the occurrence of w in a set of observations C considered for the prior distribution (typically, a corpus), |D| is the number of observations, i.e. the length of D, |C| is the length of C, and  is the Dirichlet smoothing parameter. Note that P (w|D) is a parameter which corresponds to outcome w in the multinomial distribution.
The size of vocabulary of a language model is defined as the number of terms observed in C, which also determines the number of dimensions of the Euclidean metric space for a multinomial distributions. When the number of dimensions is n + 1, a multinomial distribution corresponds to a point in n-simplex Pn which is defined as follows:

Pn =

n+1
x  Rn+1 : i, x(i) > 0, x(i) = 1
i=1

An example of 2-simplex embedded in 3-dimensional Euclidean space is shown in Figure 1.
Since a geodesic linking two points in n-simplex is a straight line, the distance between two multinomial distributions is calculated by the Euclidean distance as follows:

dist(x, y) =

n+1
(x(i) - y(i))2
i=1

Consider multinomial distributions of k given documents, p1, p2, · · · , pk as samples from distribution Q over the nsimplex. Then, the Fr´echet sample function is given by

k

n+1

¯ (c) = Q^(pi) (c(j) - p(ij))2

i=1

j=1

Therefore, we have the following optimization problem to

obtain the Fr´echet sample mean.

minimize

k

n+1

Q^(pi) (c(j) - p(ij))2

i=1

j=1

n+1

subject to

c(j) = 1,

j, c(j) > 0

(3)

j=1

It is trivial to solve this problem using the method of Lagrange multipliers. Finally, we have a solution as follows:

k

c(j) =

p(ij ) Q^ (pi )

(4)

i=1

This is the Fr´echet sample mean in the Euclidean metric space. Indeed, if Q^(pi) is uniform, i.e, 1/k, then this is the same as the ordinary Euclidean mean or the arithmetic mean. Therefore, the Fr´echet sample mean in the Euclidean metric space generalizes the arithmetic mean.
We use the Fr´echet sample mean as a representative multinomial distribution for the given group of multiple documents.

3.3 Riemannian manifold defined by the Fisher information metric
Many IR approaches assume that data is embedded in the Euclidean geometry. However, assumptions of non-Euclidean geometries may lead to a better understanding of data. We here consider a Riemannian space where a Riemannian metric is the Fisher information metric. This metric space is used for investigating the geometric structures of statistical models in most of the Information Geometry literature [28, 1, 16]. Furthermore, a number of approaches assume this metric space for statistical inference and machine learning [20, 22, 1]. Particularly, for text classification, Lafferty and Lebanon [20] showed that techniques based on this metric space perform better than techniques based on the Euclidean metric.
The Fisher information metric is defined as follows:

gi,j() =



log p(x;  (i)

)



log p(x;  (j )

)

p(x;

)dx

 log p(x; )  log p(x; )

= E

 (i)

 (j )

where  is a point in a differential manifold and corresponds to a statistical model in a parametric familty p(x; ), i and j are indices for a coordinate system. In this work, it is easy

253

to think that  is a multinomial model for a document while i and j are indices for unique terms in vocabulary.
This metric has some nice properties. By Cram´er-Rao inequality [28], the variance of unbiased estimators is bounded by the inverse of the metric. Particularly, an unbiased estimator achieving the bound is called an efficient estimator which is the best unbiased estimator because it minimizes the variance. Furthermore, by Chentsov's theorem [8], the Fisher information metric is the only Riemannian metric which is invariant under basic probabilistic transformations.
We now look into the Riemannian geometry with the Fisher information metric as a Riemannian metric. First of all, let us consider the positive n-sphere of radius 2, S~n+ instead of n-simplex Pn.

S~n+ =

n+1
x  Rn+1 : i, x(i) > 0, (x(i))2 = 22
i=1

Figure 1 shows an example of the positive 2-sphere of radius
2. We can define transformation  : Pn  S~n+ by
 z(j) = (x)(j) = 2 x(j)

The inverse transformation -1 is well known to pull back the Fisher information metric on Pn to the Euclidean metric on S~n+ [16, 22]. Therefore, the transformation is an isometry, and we can compute the distance between two statistical models by the Fisher information metric using the geodesic distance between two corresponding points on the sphere. In other words, the distance is the length of the shortest curve linking two corresponding points on the sphere and is given by

n+1
dist(x, y) = 2 arccos
j=1

x(j ) y (j )

This is called the information distance. With this distance, we have the following Fr´echet sample
function.

k

n+1

¯ (c) = 4 arccos2

x(j)y(j) Q^(pi)

i=1

j=1

Unfortunately, there is no closed form solution for the Fr´echet sample mean which minimizes this function. Although we can use some convex optimization techniques, such approaches may be impractical in case that n is large. Indeed, in many IR tasks, n + 1 is the size of vocabulary and can be very large.
Therefore, to find the Fr´echet sample mean, we try an approximation approach using the Kullback-Leibler (KL) divergence which is defined as follows:

D(x||y)

=

n+1

x(j)

log

x(j) y(j)

j=1

As y  x, approximately by the Taylor expansion,

log x(j) - log y(j)

=

-

(y

(j) - x(j)) x(j)

+

(y(j) - x(j))2 2(x(j))2

+ O((y(j)

- x(j))3)

From this,

D(x||y) + D(y||x)

n+1

=

x(j) log x(j) - log y(j) + y(j) log y(j) - log x(j)

j=1

=

1 2

n+1

(y(j) - x(j))2 x(j)

+

1 2

n+1

(x(j) - y(j))2 y(j)

+

O(||y

-

x||3)

j=1

j=1

(5)

Since y approaches x along geodesic c linking them, we can parameterize the path by arclength s so that c(s0) = x, c(s1) = y and s1 - s0 = dist(x, y). The difference between two points is expressed by a product of the geodesic length and the tangent vector to the curve as follows:

y(j) - x(j)

=

(s1

-

s0)

 c(j ) s

= dist(x, y) c(j) s

Then, the first term in Equation (5) can be rewritten as follows:

1 n+1 1

2

x(j)

j=1

 c(j ) dist(x, y)
s

2

=

1 dist2(x, y) n+1 2

1 c(j)(s)

j=1

c(j) 2 s

= 1 dist2(x, y) n+1 c(j)(s)  log c(j) 2 = 1 dist2(x, y)I(s)

2

s

2

j=1

where I(s) is the Fisher information for s. By definition of the length of the curve,
s1
I(s)ds = dist(x, y) = s1 - s0
s0
Hence, I(s) = 1, and we finally have the following:

1 2

n+1

(y(j) - x(j))2 x(j)

=

1 dist2(x, y) 2

(6)

j=1

Similarly, the second term in Equation (5) can be also written as Equation (6). Therefore, we have an approximation of Equation (5) as follows:
D(x||y) + D(y||x) = dist2(x, y) + O(||y - x||3)  dist2(x, y)

Similar relationships between divergences and distances can be founded in various texts [1, 16].
From this approximation, we can express the Fr´echet sample mean with the KL divergence as follows:

k

¯ (c)  (D(pi||c) + D(c||pi)) Q^(pi)

(7)

i=1

This means that finding the Fr´echet sample mean is reduced to finding the symmetrized Bregman centroid cF [27] which
is defined as follows:

cF = arg min c

k

1 2

(DF

(pi||c)

+

DF

(c||pi))

Q^(pi)

i=1

where DF (x||y) is the Bregman divergence defined by F (x)- F (y)- x-y, F (y) and F is a generator function. For example, if F is the negative Shannon entropy, i.e. j x(j) log x(j),

254

then the Bregman divergence is the same as the KL diver-
gence. That is, the Bregman divergence is a generalized divergence. In addition, right-sided centroid cFR and left-sided centroid cFL are defined as follows:

k

cFR

=

arg

min c

DF (pi||c)Q^(pi)

i=1

k

cFL

=

arg min c

DF (c||pi)Q^(pi)

i=1

Nielsen and Nock [27] show that symmetrized Bregman centroid cF lies on a geodesic linking cFR and cFL via the Bregman Pythagoras' theorem. We can apply the result to
the KL divergence. We can easily compute cFR using the method of Lagrange
multipliers with the same constraints as Equation (3), and
the solution coincides with the arithmetic mean as follows:

k

cFR(j) =

Q^ (pi )p(ij )

i=1

Similarly, using the method of Lagrange multipliers, we compute cFL as follows:

k
cFL (j) =
i=1

p(ij ) Q^ (pi )

n+1 k
/
j=1 i=1

p(ij ) Q^ (pi )

If Q^ = 1/k, then this is the ordinary normalized geometric mean.
Therefore, the symmetrized Bregman centroid when F is the negative Shannon entropy, or the approximated Fr´echet sample mean lies on the geodesic linking the arithmetic mean and the normalized geometric mean.
We consider the two means as approximations to the Fr´echet sample mean and take the following approach to decide a representation among them:

1. Compute the arithmetic mean cA and the normalized geometric mean cG from multinomial models of multiple documents.

2. Compute ¯ (cA) and ¯ (cG) by Equation (1)
3. As a representation, choose cG if ¯ (cA) > ¯ (cG), cA otherwise.

That is, we choose a point which is closer to the Fr´echet sample mean as a representation. We call this approach "geometric selection".

4. EXPERIMENTS
To evaluate representation techniques derived in the previous section, we conduct experiments for two different tasks: cluster retrieval and pseudo-relevance feedback.
For the experiments, we use 3 standard collections from TREC. Table 1 shows the statistics of the collections. To estimate a language model from each document, we use the Dirichlet smoothing. For each task, the initial results are obtained by query-likelihood scores which are computed under an independence assumption as follows:
P (Q|D) = P (q|D)
qQ
where P (q|D) is estimated by Equation (2).

AP

WSJ

GOV2

TREC topics 51-200 51-200 701-800

#docs

242,918 173,252 25,205,179

Table 1: Test collections.

For index building, we used the Indri system [33]. Each document was stemmed by the Krovetz stemmer and stopped by a standard stopword set. To test the significance of results, we performed a randomization test.
4.1 Cluster Retrieval
Cluster retrieval involves finding the best document cluster [24, 26]. We first retrieve the top 100 documents for each query according to query-likelihood scores. Next, we perform kNN clustering [19]. That is, assuming that each returned document is a cluster centroid, a cluster is formed by its k - 1 nearest neighbors (k is set to 5). We use cosine similarity as a similarity measure. In fact, since cosine similarity assumes the Euclidean metric space, other similarity measures may perform better for our representation technique which assumes a different metric. However, since arbitrary clusters are assumed in cluster retrieval, we use the same similarity measure as used in previous work [26].
Once we have clusters, we represent each cluster by the arithmetic mean of language models of documents in a cluster assuming the Euclidean metric. On the other hand, assuming the Fisher information metric, we can determine a representation via geometric selection between the arithmetic mean and the normalized geometric mean of the documents.
Evaluation of various representation techniques such as concatenation or CombMax [12] for cluster retrieval has been already done by Liu and Croft [26]. They concluded that the geometric mean representation outperforms other techniques. Therefore, we do not intend to repeat the same work. Instead, we focus on geometric interpretations for experimental results.
For a fair comparison, the same clusters are given to each representation technique. The only parameter to be tuned is the smoothing parameter for the initial results. We set the parameter so that Mean Average Precision (MAP) for the initial results by the query-likelihood P (Q|D) is maximized. Evaluation is performed using all topics. Since our goal is to find the best cluster, we use Precision at 5 (P@5) in order to evaluate the cluster first ranked by each representation technique, i.e. how many relevant documents the cluster has. Table 2 shows the results. In addition to the arithmetic mean and geometric selection, we present results using the geometric mean as well.
For all collections, representations by the geometric mean and geometric selection show better performance than representations by the arithmetic mean. Except for GOV2, The improvements are statistically significant. These experiments indicate some interesting points. First, in geometric selection, the normalized geometric means were selected as representations which minimize the Fr´echet sample function for all queries across all collections. In other words, the normalized geometric means are better approximations to the Fr´echet sample mean. Second, since the normalized geometric means selected by geometric selection lead to consistently better retrieval results, we may say that the goodness of a representation for this task is related to how close the rep-

255

A-MEAN G-MEAN SELECT

AP
0.3053 0.3347 0.3347

WSJ
0.4747 0.5040 0.5027

GOV2 0.5374 0.5576 0.5556

Table 2: Results for cluster retrieval. A-MEAN, GMEAN and SELECT mean representations by the arithmetic mean, by the geometric mean, and by geometric selection, respectively. The numbers are P@5 scores. A * indicates a statistically significant improvement over A-MEAN (p < 0.05).

resentation is to the center of mass, i.e. the Fr´echet sample mean. Moreover, this justifies the assumption of the geometry defined by the Fisher information metric. Lastly, since geometric selection does not consider the geometric mean but the normalized geometric mean, the results in the `SELECT' row are exactly the same as those by the normalized geometric means. Therefore, the differences between the `G-MEAN' row and the `SELECT' row are caused by the normalization. As you see, since the differences are small, we suggest that the geometric mean without normalization can be a better choice in practice.

4.2 Pseudo-Relevance Feedback

Lavrenko and Croft's relevance model [21] is one of the standard language modeling approaches for pseudo-relevance feedback. The model assumes that the top k retrieved documents for query q are sampled from an underlying relevance model for q. That is, a hidden multinomial model relevant to a user information need exists, and we estimate the model from the top k documents. Then, we sample terms which describe the information need better than the original query and use the terms for query expansion.
Estimation of the relevance model is done by the following formula:

P (w|q) =

k i=1

p(w|Di

)P

(q|Di

)P

(Di)

(8)

p(q)

where q is a user query, w is a candidate for expansion terms, and Di is a document in the top k initial results, respectively.
Although this is derived from a Bayesian model, we can see this as a representation for the top k documents by the arithmetic mean rewriting Equation (8) as follows:

k

p(w|Di

)

P

(q|Di)P p(q)

(Di

)

=

k

p(w|Di)P (Di|q)

i=1

i=1

This has the same form as the weighted arithmetic mean of Equation (4). In other words, P (w|Di) is a multinomial parameter and P (Di|q) represents a distribution over a sample space limited by q, i.e, Q^. In the standard implementation
of the relevance model by the Indri system [33], P (D) is
assumed to be uniform. Hence,

P (Di|q) =

P (q|Di)P (D)

k i=1

P

(q|Di

)P

(D)

=

P (q|Di)

k i=1

P

(q|Di

)

That is, the weight Q^ = P (Di|q) is the normalized querylikelihood scores obtained in the initial retrieval phase. Therefore, we can say that the relevance model represents a group of the top k documents combining the language models by the arithmetic mean weighted by the initial search results.

RM GRM

AP
0.2541 0.2769

WSJ
0.3531 0.3851

GOV2
0.3204 0.3300

Table 3: Results for pseudo-relevance feedback. RM and GRM mean the relevance model and the geometric relevance model, respectively. The numbers are MAP scores. A * indicates a statistically significant improvement over RM (p < 0.01).

In this sense, we can say that the relevance model implicitly assumes the Euclidean metric space.
We can replace the arithmetic mean by the normalized geometric mean to develop a new representation as follows:

k
P (w|q) = p(w|Di)P (Di|q)/

k
p(w|Di)P (Di|q) (9)

i=1

wV i=1

We can consider the original relevance model and this model as two approximated representations in the Riemannian manifold defined by the Fisher information metric. To determine a representation, we use geometric selection and call the selected model the "geometric relevance model".
We compare the geometric relevance model with the relevance model. For each query, we first retrieve the top k documents by query-likelihood scores and build a relevance model or geometric relevance model for the documents. Then, we choose the top M terms according to probabilities of the terms in the models. Finally, we expand the original query combining the expansion terms using an interpolation weight  in the Indri query language. The paremeters k, M and  are tuned so that MAP scores by the relevance model are maximized. The same parameters are used for the geometric relevance model. Topic 51-150 for AP and WSJ and topic 701-750 for GOV2 are used as training topics to learn the parameters. Topic 151-200 for AP and WSJ and topic 751800 for GOV2 are used as test topics. We retrieve up to 1000 results for each expanded query and use MAP as the evaluation metric.
Table 3 shows the results. The geometric relevance model significantly outperforms the relevance model for all three collections. Similar to cluster retrieval, geometric selection selected models by Equation (9) rather than the original relevance model as representations for all queries except for three queries of GOV2. That is, the geometric mean is a better approximation to the center of mass for this task. This provides more empirical evidence that the geometric mean can be an appropriate choice for representation.

5. DISCUSSIONS

5.1 Visualization of geometries
To show how multiple documents, the arithmetic mean and the normalized geometric mean are distributed in each geometry, we use the following visualization. First, we construct a weighted complete graph, where each node is a document or the mean and a weight is determined by a kernel reflecting each geometry.
For the Euclidean metric, we use the following heat kernel:

K(x1, x2) = exp

n+1
-

x(1j) - x(2j)

2

/4t

j=1

256

Figure 2: Geometric visualization of the top 20 documents for Topic 770 (GOV2), the arithmetic mean (AM) and the normalized geometric mean (GM) for different metrics, i.e. the Euclidean metric (left) and the Fisher information metric (right).

TxM

x

V m' V

M

m

y'

y

Figure 3: Determinination of a middle point m on a geodesic linking x and y

where t is a time parameter. For the Fisher information metric, we use the following
information diffusion kernel [20]:

n+1

K(x1, x2) = exp - arccos2

x(1j)x(2j) /4t

j=1

We visualize each geometry using CCVisu [4] which is a tool implementing energy models so that the higher weight between two points results in the smaller Euclidean distance between them. A visualization example is shown in Figure 2. As you see, the arithmetic mean appears closer to the center in the Euclidean metric space while the normalized geometric mean appears closer in the Riemannian manifold defined by the Fisher information metric. Since the visualization tool uses random seeds to initialize the layout, the results vary every time. However, the trend for the locations of the means was consistent.

5.2 More accurate estimation
Geometric selection is a somewhat simple approach to determine the approximated Fr´echet sample mean. That is, we choose one among only two options: the normalized geometric mean and the arithmetic mean. We now consider a more accurate estimation technique for the Fr´echet sample mean.
A point which minimizes the approximated Fr´echet sample function of Equation (7) lies on a geodesic linking the arithmetic mean and the normalized geometric mean. Let M , x, y and c be the statistical manifold defined by the Fisher information metric, the arithmetic mean, the normalized geometric mean and a geodesic linking the two points, respectively. First, we get vector V on tangent space TxM via log map logx : M  TxM . In case of a sphere, the log

Figure 4: Relative locations of the more accurately estimated Fr´echet sample means. The x-axis corresponds to the relative locations, and the y-axis corresponds to queries for each collection. As a relative location is closer to 1.0, the estimated mean for the topic is located near the normalized geometric mean.
AP WSJ GOV2 GRM+ 0.2769 0.3852 0.3309
Table 4: Pseudo-relevance feedback results of the more accurately estimated Fr´echet sample mean in the Riemannian manifold defined by the Fisher information metric.

map is given by:

V (j)

=

logx (y)(j )

=

arccos( x, y ) 1 - x, y 2

y(j) - x, y x(j)

Then, V links x to y on TxM corresponding to y on M .
m denotes a middle point between x and y on TxM , reached by V (0    1). We now get a middle point m on c via exponential map expx : TxM  M . The exponential map of a sphare is:

m(j)

=

expx(V

)(j)

=

cos (||V

||)

+

sin

(||V ||V ||

||) V

(j)

Figure 3 illustrates this procedure. Note that the arithmetic mean x and the geometric mean y are interchangeable in the above formulation because a sphere is symmetric.
We apply this result to pseudo-relevance feedback experiments. We perform grid search on the geodesic varying  in [0,1] by step-size 0.1, and a point which minimizes the Fr´echet sample function of Equation (1) is selected as a representation. Figure 4 shows 's selected for test queries for each collection. For all test topics except for three topics of GOV2, the selected 's are equal to or greater than 0.5. That is, the more accurately estimated Fr´echet sample means are also closer to the normalized geometric mean than the arithmetic mean. Table 4 shows the results when the representations are used for pseudo-relevance feedback. All results are equal to or a little bit better than the results of the GRM in the Table 3, but not significantly. Therefore, we can say that the geometric relevance model is a reasonable approximation to the Fr´echet sample mean for this task.

5.3 Anoher reason for the geometric mean
We have addressed so far theoretical and empirical reasons explaining why the geometric mean should have advantages

257

for many IR tasks. There can be many other explanations. One of them is the log-linearity of the geometric mean. As more documents contain a specific term, the geometric mean for the term increases exponentially while the arithmetic mean increases linearly. Accordingly, the arithmetic mean can be sensitive to a few dominant terms in a small number of documents. On the other hand, the geometric mean favors the common terms across a whole set of documents and is relatively insensitive to such a few dominant terms. This shows the robustness of the geometric mean which can lead to a good representation for multiple documents.
6. CONCLUSIONS
Previous work which uses the geometric mean as a representation technique does not provide enough theoretical evidence explaining why the geometric mean should have advantages as a representation for IR. There are various explanations. In this work, we showed that using Information Geometry, the arithmetic mean and the normalized geometric mean are approximation points to the center of mass in the Euclidean space or in a statistical manifold. In particular, through empirical evidence, we demonstrated that the normalized geometric mean is closer to the center in the statistical manifold. In addition to this discovery, we introduced a new approach to pseudo-relevance feedback that outperformed the relevance model. For future work, we will investigate how geometric interpretations can be applied to other IR tasks. We expect that this effort will lead to not only the discovery of novel IR theories but also development of effective algorithms.
7. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #IIS0711348, and in part by NSF grant #IIS-0534383. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
8. REFERENCES
[1] S. Amari and H. Nagaoka. Methods of Information Geometry. American Mathematical Society, 2000.
[2] N. J. Belkin, C. Cool, W. B. Croft, and J. P. Callan. The effect multiple query representations on information retrieval system performance. In SIGIR '93, 1993.
[3] M. Bendersky and O. Kurland. Utilizing passage-based language models for document retrieval. In ECIR '08, 2008.
[4] D. Beyer. CCVisu: Automatic visual software decomposition. In Proc. Int'l Conf. on Software Engineering, 2008.
[5] R. Bhattacharya and V. Patrangenaru. Nonparametic estimation of location and dispersion on riemannian manifolds. Journal of Statistical Planning and Inference, 108, 2002.
[6] J. Callan. Distributed information retrieval. In W. B. Croft, editor, Advances in Information Retrieval. Kluwer Academic Publishers, 2000.
[7] J. P. Callan. Passage-level evidence in document retrieval. In SIGIR '94, 1994.
[8] N. N. Chentsov. Statistical Decision Rules and Optimal Inference. American Mathematical Society, 1982.
[9] K. Collins-Thompson and J. Callan. Estimation and use of uncertainty in pseudo-relevance feedback. In SIGIR '07, 2007.

[10] B. Efron. Defining the curvature of a statistical problem. The Annals of Statistics, 3(6).
[11] J. L. Elsas and J. G. Carbonell. It pays to be picky: an evaluation of thread retrieval in online forums. In SIGIR '09, 2009.
[12] E. A. Fox and J. A. Shaw. Combination of multiple searches. In TREC-2, 1994.
[13] M. Fr´echet. Les ´el´ements al´eatoires de nature quelconque dans un espace distanci´e. Ann. Inst. H. Poincar´e, 10, 1948.
[14] H. Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, 186(1007), 1946.
[15] H. Karcher. Riemannian center of mass and mollifier smoothing. Communications on pure and applied mathematics, 30(5), 1977.
[16] R. E. Kass and P. W. Vos. Geometrical Foundations of Asymptotic Inference. Wiley-Interscience, 1997.
[17] W. Kendall. Probability, convexity, and harmonic maps with small image i: Uniqueness and fine existence. Proc. London Math. Soc., 61, 1990.
[18] J. Kogan, M. Teboulle, and C. Nicholas. The entropic geometric means algorithm: An approach for building small clusters for large text datasets. In the Workshop on Clustering Large Data Sets, 2003.
[19] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. In SIGIR '04, 2004.
[20] J. Lafferty and G. Lebanon. Diffusion kernels on statistical manifolds. The Journal of Machine Learning Research, 6, 2005.
[21] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR' 01, 2001.
[22] G. Lebanon. Riemannian Geometry and Statistical Machine Learning. PhD thesis, 2005.
[23] J. H. Lee. Analyses of multiple evidence combination. In SIGIR '97, 1997.
[24] A. Leuski. Evaluating document clustering for interactive information retrieval. In CIKM '01, 2001.
[25] X. Liu and W. B. Croft. Passage retrieval based on language models. In CIKM '02, 2002.
[26] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In ECIR '08, 2008.
[27] F. Nielsen and R. Nock. Sided and symmetrized Bregman centroids. IEEE Transactions on Information Theory, 55(6), 2009.
[28] C. Rao. Information and the accuracy attainable in the estimation of statistical parameters. Bulletin of the Calcutta Mathematical Society, 37, 1945.
[29] J. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, editor, The SMART Retrieval System Experiments in Automatic Document Processing. Prentice Hall, 1971.
[30] J. Seo and W. B. Croft. Blog site search using resource selection. In CIKM '08, 2008.
[31] J. Seo, W. B. Croft, and D. A. Smith. Online community search using thread structure. In CIKM '09, 2009.
[32] L. Si and J. Callan. Unified utility maximization framework for resource selection. In CIKM '04, 2004.
[33] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indri: A language model-based search engine for complex queries. In Proc. of the Intl. Conf. on Intelligence Analysis, 2005.
[34] R. Veldhuis. The centroid of the symmetrical Kullback-Leibler distance. IEEE Signal Processing Letters, 9(3), 2002.
[35] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In SIGIR '01, 2001.

258

Using Statistical Decision Theory and Relevance Models for Query-Performance Prediction

Anna Shtok1

Oren Kurland1

annabel@tx.technion.ac.il kurland@ie.technion.ac.il

David Carmel2 carmel@il.ibm.com

1. Faculty of Industrial Engineering and Management, Technion, Haifa 32000, Israel 2. IBM Research lab, Haifa 31905, Israel

ABSTRACT
We present a novel framework for the query-performance prediction task. That is, estimating the effectiveness of a search performed in response to a query in lack of relevance judgments. Our approach is based on using statistical decision theory for estimating the utility that a document ranking provides with respect to an information need expressed by the query. To address the uncertainty in inferring the information need, we estimate utility by the expected similarity between the given ranking and those induced by relevance models; the impact of a relevance model is based on its presumed representativeness of the information need. Specific query-performance predictors instantiated from the framework substantially outperform state-of-the-art predictors over five TREC corpora.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation
Keywords: query-performance prediction, relevance models, statistical decision theory, rank correlation
1. INTRODUCTION
The effectiveness of search engines can significantly vary across queries [27, 9]. Thus, the ability to identify which queries are more difficult than others could be of great benefit. Indeed, there is a large body of work on predicting query performance, that is, estimating the effectiveness of a search performed in response to a query in lack of relevancejudgments information. (See Section 2 for a survey.)
We present a novel framework for query-performance prediction that is based on statistical decision theory. Specifically, we consider a ranking induced by a retrieval method in response to a query as a decision taken so as to satisfy the underlying information need [15]. The quality of the ranking -- i.e., query-performance -- is then estimated based on the utility it provides with respect to the presumed information need. However, there is often uncertainty about the actual
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

information need, especially when examples of relevant documents are not provided and queries are ambiguous.
To address the uncertainty in inferring the information need, we first assume a "true" (latent) model of relevance, e.g., a relevance language model [16] that generates terms in the query and in relevant documents. Then, we estimate the utility of the given ranking by its expected similarity with the rankings induced by estimates of this relevance model.
We instantiate various query-performance predictors from the framework by varying the (i) estimates of the relevance model, (ii) measures for the similarity between the given ranking and that induced by a relevance-model estimate, and (iii) measures for the quality of a relevance-model estimate, that is, the extent to which it presumably represents the underlying information need.
A key observation, which enables to derive effective predictors from our framework, concerns the representativeness of relevance-model estimates for the underlying information need. We argue, and empirically show, that relevance-model representativeness can be estimated using various queryperformance predictors that were originally devised to predict the quality of the document list from which the relevance model is constructed, e.g., the Clarity method [6].
Empirical evaluation performed using five TREC corpora shows that the predictors instantiated from our framework substantially and consistently outperform, in terms of prediction quality, four state-of-the-art predictors. A case in point, the average relative prediction improvement over the Clarity method [6] is above 30%.
2. RELATED WORK
Query-performance predictors can be roughly categorized to pre-retrieval and post-retrieval methods [13]. Pre-retrieval methods analyze the query expression before search is performed [6, 13, 21, 19, 11, 31, 10]. Linguistic features and/or statistical properties of the query-terms distribution are often used along with some corpus-based statistics.
Post-retrieval predictors analyze also the result list -- the list of documents most highly ranked in response to the query. The Clarity method [6], for example, estimates the "focus" of the result list with respect to the corpus, as measured by the KL-divergence between their induced (language) models. Variants of clarity were proposed for improving prediction performance [2, 7, 4, 12].
Estimating result-list robustness is another effective postretrieval prediction paradigm. Intuitively, the more robust the result list with respect to different factors, the less "dif-

259

ficult" the query is. For example, the cohesion of the result list, as measured by its clustering patterns, indicates query performance [26]. The effects on the result list of document perturbations [26, 33], query perturbations [29], query-model modifications [34], and different retrieval functions [3], have been used to devise performance predictors.
Retrieval scores often reflect document-query similarity. Hence, analyzing retrieval-scores distribution can potentially help to predict query performance. Indeed, the highest retrieval score and the mean of top scores were shown to indicate query performance [25]. Query performance was also shown to be correlated with the extent to which similar documents in the result list are assigned with similar retrieval scores [8]. In addition, high retrieval scores at top ranks of the list with respect to that of the corpus [34], and large variance of retrieval scores in the list [22], were shown to indicate effective performance.
We argue, and empirically show, that some post-retrieval predictors can effectively be used in our framework to estimate the extent to which relevance models [16] presumably represent the information need expressed by a query. We use state-of-the-art predictors that represent the different paradigms mentioned above, namely, the Clarity method [6], the query-feedback (QF) approach, which is based on ranking robustness [34], and the WIG [34] and NQC [22] measures that utilize retrieval scores. (See Section 3.2.2 for a discussion on these predictors.) The effectiveness of the predictors we derive from the framework is substantially better than that of using those four predictors, as originally proposed, to estimate the quality of the result list.
Some work on ranking documents in response to a query [5, 28, 24] addresses the uncertainty in inferring the information need by using multiple relevance models (or more generally, query-expansion-based models) as we propose in our framework. Furthermore, representativeness measures for relevance models have also been used [28, 24], albeit not QF, WIG, and NQC that we use here. More importantly, the task we pursue -- predicting the quality of a given document ranking -- is different than that of inducing document ranking in response to a query [5, 28, 24].
Our treatment to document ranking as a decision made by the retrieval method in response to a query is inspired by the risk minimization framework [15]. However, while the latter aims at inducing document ranking, our framework is intended to estimate the utility of a given ranking.
3. QUERY-PERFORMANCE PREDICTION BASED ON UTILITY ESTIMATION
Let q, d, and D denote a query, a document and a corpus of documents, respectively. We assume that q is used to express some information need Iq.
We use M(q; S) to denote the ranking induced over a set of documents S in response to q using retrieval method M. Our goal is to predict the query-performance of M with respect to q. In other words, we would like to quantify the quality (effectiveness) of the ranking of all documents in the corpus, M(q; D), with respect to the information need Iq in lack of relevance-judgments information.
3.1 Prediction framework
The corpus ranking, M(q; D), could be viewed as a decision made by the retrieval method M in response to q so

as to satisfy the user's (hidden) information need Iq [15]. The ranking effectiveness reflects the utility provided to the user, denoted U (M(q; D); Iq). In what follows we devise estimates for this utility, i.e., query-performance predictors.
Suppose that there is an oracle that provides us with a "true" model of relevance RIq representing the information need Iq. Suppose also that RIq can be used by M for ranking -- e.g., that RIq has a query-model representation to be used by M. A statistical relevance language model that generates the terms in relevant documents is an example for such a query model in the language modeling framework [16]. Then, according to the probability ranking principle [20], using M with RIq yields a ranking M(RIq ; D) of maximal utility (e.g., all relevant documents are positioned at the highest ranks, and all non-relevant documents are positioned below the relevant documents). Thus, we can use the maximal-utility ranking M(RIq ; D) to estimate the utility of the given ranking, M(q; D), based on their "similarity", measures of which we discuss in Section 3.2.3:
U (M(q; D); Iq) d=ef Sim(M(q; D), M(RIq ; D)). (1)
In practice, we have no explicit knowledge of the underlying information need Iq, except for the information in q, nor do we have an oracle to provide us with a model of relevance. Hence, we use estimates R^q for RIq that are based on the information in q and in the corpus. Using statistical decision theory principles, we can approximate Equation 1 by the expected similarity between the given ranking and those induced by the estimates for RIq :

U (M(q; D); Iq) 
Z Sim(M(q; D), M(R^q; D))p(R^q|Iq) dR^q; (2)
R^q
p(R^q|Iq) is the probability that R^q represents Iq -- i.e., that R^q is the "true" relevance model RIq .
In practice, users' utility is often determined based on the documents most highly ranked. Indeed, evaluation measures of retrieval effectiveness (e.g., MAP and precision at top k) consider only top-retrieved documents -- a.k.a., the result list. This is also the case for many post-retrieval queryperformance prediction methods (e.g., [6, 29, 33, 34, 4]) that analyze the result list. Thus, we confine the utility analysis to the result list DM [k];q -- the k most highly ranked documents in the corpus D by M with respect to q:

U (M(q; D); Iq) 

(3)

Z R^q Sim(M(q; DM [k];q), M(R^q; DM [k];q))p(R^q|Iq) dR^q;

That is, we measure the expected inter-ranking similarity between that of the top-k results of the original ranking and their re-ordering as induced by the relevance model estimates; if we set k to the number of documents in the corpus, then Equation 3 reduces to Equation 2.
Equation 3 can be instantiated in numerous ways to yield specific query-performance predictors. We have to (i) derive estimates R^q for the true relevance model (see Section 3.2.1), (ii) estimate the extent to which these estimates represent the hidden information need (p(R^q|Iq)) -- a task which we address in Section 3.2.2, and (iii) select measures of similarity between ranked lists (see Section 3.2.3).

260

3.2 Framework instantiation

3.2.1 Relevance models
We use relevance language models (RM3) [16, 1] for the estimates R^q. Relevance-model estimation is performed using a pseudo-feedback-based approach, that is, utilizing a list DQ[L] ;q of the  highest ranked documents by the querylikelihood (QL) method [23]. Specifically, if p(w|x) is the
probability assigned to term w by a language model induced from text (or text collection) x, then the query-likelihood approach scores x in response to q = {qi} by

ScoreQL(q; x)

d=ef

log

p(q|x)

d=ef

log

Y p(qi|x).

(4)

qi

A relevance-model estimate, R^q;S, constructed from a set
S ( DQ[L] ;q) of highly ranked documents, is a probability distribution over the vocabulary:

p(w|R^q;S) d=ef p(w|q) + (1 - ) X p(w|d)p(d|q); (5)
dS

p(d|q) d=ef

p(q|d)

P d S

p(q|d )

is d's

normalized

query

likelihood;



is a free parameter. To rank documents using R^q;S, e.g., in

Equation 3, the negative cross entropy (CE) is used:

ScoreCE (R^q;S; d) d=ef X p(w|R^q;S) log p(w|d). (6)
w
Following some work on addressing the performance robustness issues of pseudo-feedback-based methods [5, 17, 24], we create relevance-model estimates by sampling sets S of documents from DQ[L] ;q. (See Section 4 for specific details.) Then, we construct from each set S a relevance-model estimate R^q;S using Equation 5, that is used to rank DM [k];q according to Equation 6. Finally, we approximate the utility in Equation 3 by:

U (M(q; D); Iq) 

(7)

X Sim(M(q; DM [k];q), M(R^q;S ; DM [k];q))p(R^q;S|Iq ).
R^ q;S

3.2.2 Representativeness of relevance models
Quantifying the extent to which a relevance-model estimate, R^q;S, represents the information need Iq (i.e., p(R^q;S|Iq)) is a prediction challenge at its own right [7, 28, 24]. It is important to point out that the framework proposed above is not committed to any specific paradigm of quantifying representativeness. Specifically, there is no coupling in the framework between the way relevance model estimates are devised and the methods used for quantifying their representativeness. Here, we adapt four effective query-performance measures that were originally proposed for predicting the quality of a result list as surrogates for estimates p^(R^q;S|Iq) of relevance model representativeness. These four measures represent the different post-retrieval query-performance prediction paradigms surveyed in Section 2.

Clarity. A natural measure for the representativeness of a
relevance-model estimate R^q;S is its clarity [6]; that is, the "distance" between R^q;S and the corpus model, which can

be measured by the KL divergence:

p^Clarity(R^q;S |Iq)



X

p(w|R^ q;S )

log

p(w|R^q;S) . p(w|D)

w

The larger the KL divergence is, the more distant R^q;S is from the corpus model, and hence, is considered more coherent ("clear"). Since R^q;S is constructed from documents highly ranked in response to q, we assume, as in some recent work on utilizing multiple relevance models for retrieval [28, 24], that higher clarity indicates better representativeness of the information need. We note that originally [6], the clarity of some relevance model was used as an estimate for the quality of the result list from which the model was constructed, rather than as an estimate for the representativeness of the relevance model.

WIG. The Clarity measure estimates the representativeness
of R^q;S by directly measuring its "quality" as a language model. An alternative paradigm is based on estimating the
presumed percentage of relevant documents in the set S from which R^q;S is constructed. The higher this presumed percentage, the better representative R^q;S is assumed to be. Recall that S is a set of documents highly ranked by the query-likelihood method. Hence, estimating the relevantdocument percentage in S is a form of a query-performance prediction task. To that end, we use the WIG measure [34]1:

p^W IG(R^q;S|Iq) 

1 p|q|

1 |S|

X (ScoreQL(q;
dS

d)

-

S coreQL (q ;

D));

normalization with respect to the query length, which affects query-likelihood scores (see Equation 4), is performed for inter-query compatibility.
Note that WIG relies on the premise that high retrieval scores with respect to that of the corpus imply to relevance.
Thus, WIG was originally [34] computed based on the retrieval scores of the most highly ranked documents in the result list so as to predict the quality of the result list itself. Here, we use WIG to measure the presumed representativeness of a relevance model constructed from S ( DQ[L] ;q), as we assume that WIG is correlated with the percentage of relevant documents in S.2

NQC. A recently proposed query-performance predictor [22],
NQC, is based on the hypothesis that the standard deviation of retrieval scores in the result list is negatively correlated with the potential amount of query drift [18] -- i.e., non-query-related information manifested in the list. Specif-
ically, the mean (QL) retrieval score in the list was shown to be the retrieval score of a pseudo non-relevant document, namely, a centroid of the result list. Thus, result lists with retrieval scores much higher/lower than the mean were argued, and shown, to be of high quality.
Hence, NQC can potentially help to estimate the quality of DQ[L] ;q, and thereby, the extent to which a relevance-model
1WIG was originally proposed in the Markov Random Field framework [34]. If no term-dependencies are considered, WIG measures the query-performance of the querylikelihood approach, and is effective to this end [32, 22]. 2There are similar measures used to this end in work on cluster-based retrieval [14].

261

estimate constructed from DQ[L] ;q is a good representative of the information need. However, NQC does not have a natural implementation for a subset S of DQ[L] ;q. Therefore, we use (as a rough approximation) for each such S the NQC
computed for DQ[L] ;q:

p^NQC (R^q;S|Iq)



q1


P
dDQ [L] ;q

(ScoreQL

(q;

d)

-

µ)2

;

|ScoreQL(q; D)|

µ is the mean retrieval score in DQ[L] ;q; normalization with the corpus score is for inter-query compatibility [22].

QF. The query-feedback (QF) performance predictor mea-
sures ranking robustness [34]. Specifically, the quality of the list DQ[L] ;q is presumed to be correlated with the overlap between the top-nQF ranked documents in DQ[L] ;q and the top-nQF ranked documents in the corpus by a search performed using R^q;DQ [L] ;q -- a relevance model constructed from DQ[L] ;q.3 The overlap is simply the number of shared documents. The idea is that a relevance model constructed
from a high quality list would not yield a ranking that drifts
much from the original ranking. Thus, here we utilize QF as a measure for the representativeness of R^q;DQ [L] ;q -- the less drift the ranking it induces manifests, the more likely it
is to represent the information need [7, 28, 24]. In our experiments we use QF only with R^q;DQ [L] ;q and not
with relevance-model estimates that are constructed from subsets of DQ[L] ;q. (See Section 4 for details.)
3.2.3 Similarity between ranked lists
The remaining task for instantiating Equation 3 is the estimation of the similarity Sim(M(q; DM [k];q), M(R^q;S; DM [k];q)) between two rankings of the given result list DM [k];q. We use three popular measures for similarity between rankings:
Pearson's coefficient, which measures the linear correlation
between the retrieval scores used to induce the two rankings,
and Spearman's- and Kendall's- that rely only on ranks.
All three correlation measures assign values in [-1, +1].

4. EVALUATION
In what follows we evaluate the effectiveness of predictors instantiated from our proposed framework, denoted UEF for utility estimation framework. As noted above, to derive a specific predictor from Equation 7 we have to (i) devise a sampling technique for document sets from which relevance-model estimates are created, (ii) select a representativeness measure for relevance-model estimates, and (iii) select a measure of similarity between ranked lists.
As noted above, we use Clarity, WIG, NQC, and QF as measures for the representativeness of relevance model estimates; and, Pearson's coefficient, Spearman's-, and Kendall's as inter-ranking similarity measures. We use two strategies for sampling sets S of documents from DQ[L] ;q -- the documents most highly ranked by the query likelihood approach -- so as to define relevance-model estimates. The first, denoted Single, is using DQ[L] ;q as a single sampled set. As this
3To maintain consistency with all other predictors, we use RM3 as defined in Equation 5, which is somewhat different than the query model used originally [34].

is the standard, highly effective, approach for selecting documents for relevance-model estimation [16, 1], the resultant predictor instantiated from Equation 7 could be regarded as the posterior-mode-based estimate for the integral in Equation 3 [15]. Note that the predictor in this case is the similarity between the ranking induced over DM [k];q by the single relevance model and the original ranking of DM [k];q, scaled by the representativeness estimate of the relevance model.
The second document sampling strategy, Multi, is based on using multiple clusters of similar documents from DQ[L] ;q [17, 24]. Specifically, we employ a simple nearest-neighborsbased clustering approach wherein each document d ( DQ[L] ;q) and its  - 1 nearest neighbors from DQ[L] ;q serve as a cluster [14]; we use the KL divergence between document language models for a similarity measure [14]. Thus, we sample  (overlapping) clusters of  documents.
4.1 Experimental setup
We evaluate the prediction quality of a query-performance predictor by measuring Pearson's correlation between the actual average precision (AP at cutoff 1000) for a set of queries -- as measured by using relevance judgments -- and the values assigned to the queries by the predictor [4, 34]. All correlation numbers that we report are statistically significant at a 95% confidence level.
We conducted experiments on several TREC collections that were used in previous query-performance-prediction studies [29, 33, 32, 8]. Table 1 provides the details of the collections and topics used.

Collection TREC4 TREC5 WT10G ROBUST
GOV2

Data Disks 2&3 Disks 2&4 WT10g Disk 4&5-CR
GOV2

Num Docs 567,529 524,929 1,692,096 528,155
25,205,179

Topics 201-250 251-300 451-550 301-450, 601-700 701-850

Rels/topic 130.06 110.48 61.14 69.92
181.79

Table 1: Test collections and topics. The last column reports the average number of relevant documents per topic.
We use titles of TREC topics for queries, except for TREC4 for which no titles are provided, and hence, topic descriptions are used. We applied tokenization, Porter stemming, and stopword removal (using the INQUERY list) to all data via the Lemur/Indri toolkit (www.lemurproject.org), which was also used for experiments.
The query likelihood (QL) model [23] from Equation 4 serves as the retrieval model M; k, the size of the result-list (DM [k];q) considered for predicting performance, is set to 150. (Experiments with k = 100 yielded slightly worse performance.) Recall that documents highly ranked by QL (i.e., in the list DQ[L] ;q) are those used for relevance-model estimation. To downplay the effect of parameter tuning, we set  = k in all experiments to follow; hence, DQ[L] ;q  DM [k];q. Thus, the document list for which we predict query performance is also the one utilized for devising estimates of relevance models. We come back to this point in Section 4.2.2. For the M ulti sampling strategy, we set the cluster size, , to 20; smaller clusters yield less effective prediction.
The representativeness measures of relevance model estimates that we use -- Clarity, WIG, NQC, and QF -- are

262

state-of-the-art query-performance predictors at their own right. Hence, we use optimized versions of these as reference comparisons to our predictors. The Clarity method uses a relevance model constructed from all documents in DQ[L] ;q;  = 150 indeed yields optimal Clarity performance.  = 150 also yields highly effective prediction performance for the NQC measure as previously reported [22]. The QF measure, as Clarity, uses a relevance model constructed from DQ[L] ;q; optimal QF prediction performance is attained when setting the number of top documents it depends on, nQF , to 50. The WIG measure is highly effective when using the 5 top-ranked documents in DQ[L] ;q as previously reported [34].
To facilitate comparison with the optimized reference comparisons just described, we use those as relevance-model representativeness measures in our framework; except, for WIG and Clarity with M ulti sampling that use the information within a cluster for devising a representative measure for the relevance model constructed from it.
Language models. We use Dirichlet smoothed unigram
document language models with the smoothing parameter, µ, set to 1000 [30]. To construct a relevance model from a document set using Equation 5, we set µ = 0 for language models of documents in the set, and  = 0 (i.e., we use RM1); all relevance models use 100 terms [1]. These parameter values yield very good performance both for our predictors and for Clarity and QF that utilize relevance models.
4.2 Experimental results
In Section 4.2.5 we study the effect of varying the interranking-similarity measure, the document-sets sampling strategy, and the representativeness measure on the instantiated predictors' performance. We show that the best performing predictors are those that use Pearson correlation as interranking similarity measure, and a single relevance model estimate (Single). Hence, in Sections 4.2.1-4.2.4 we present an in-depth analysis of the performance of these predictors.
We note that the computational overhead posted by these predictors on top of computing the representativeness estimates they incorporate, which is performed by current predictors adapted to this end, is quite small. That is, a relevance model is constructed from documents in the (short) result list and is used to rank this list; then, Pearson correlation between the original list ranking and its relevancemodel-based ranking is computed.
4.2.1 Main results
In what follows we fix the inter-ranking similarity measure to Pearson's coefficient, and use a single relevance model estimate constructed from the entire initial list, DQ[L] ;q. The prediction quality of the UEF-based predictors, when using the four representativeness measures for the relevance model estimate, is presented in Table 2.
Evidently, our predictors consistently and substantially improve over using the representativeness measures as predictors at their own right -- i.e., to directly predict search effectiveness; recall that these are state-of-the-art predictors4. A case in point, using UEF with Clarity improves prediction quality by more than 30% on average, over the
4Somewhat similar relative prediction performance patterns are observed when using Kendall's- rather than Pearson's coefficient to measure prediction quality. Specifically, the average improvements over Clarity, WIG, NQC and QF are

5 TREC benchmarks, with respect to direct use of Clarity
for performance prediction. Furthermore, the improvements
over all four representative measures for the WT10G bench-
mark are quite striking as WT10G is known to post a hard
challenge for performance prediction [12]. The relative im-
provements for GOV2, on the other hand, are in general
smaller than those for the other collections.
4.2.2 Quality of representativeness measures
As noted above, the representativeness measures that we
use were originally shown to be highly effective predictors
for the quality of the initial QL-based ranking from which DQ[L] ;q was created -- the task that we pursue here as well -- rather than for the representativeness of a relevance model constructed from DQ[L] ;q. Thus, when using a single relevance model estimate (Single), our UEF-based predictors
could be viewed in this specific setting as combining two predictors for the ranking quality of DQ[L] ;q itself. The first is the representativeness measure and the second is the sim-
ilarity between the original ranking and that induced by the relevance model estimate5. We hasten to point out, however,
that this specific operational consequence does not contra-
dict the fundamentals of our framework. On the contrary, highly effective predictors for the ranking quality of DQ[L] ;q should serve as effective measures for the representativeness of the single relevance model constructed from DQ[L] ;q, as was argued in Section 3.2.2. That is, the more relevant documents there are in DQ[L] ;q, and the higher they are ranked, the higher the quality of the relevance model estimate is
(refer to Equation 5).
We thus turn to empirically examine the premise just
stated that effective predictors of the quality of the ranking using which DQ[L] ;q is created are indeed effective measures of the representativeness of the relevance-model constructed from DQ[L] ;q. To that end, we study the performance of the representativeness measures (Clarity, WIG, NQC, QF) when
predicting the quality of the ranking induced by the relevance model over the entire corpus6. Prediction performance
is measured, as usual, by the Pearson correlation between
the true AP of the relevance-model-based corpus ranking (at
cutoff 1000) and that which corresponds to the predicted val-
ues. For reference comparison, we report the performance
of using the measures to directly predict the quality of the
initial QL-based ranking, as originally proposed.
The results in Figure 1 support our premise. That is,
the measures are indeed high-quality representativeness es-
timates for the relevance model, as the high correlation num-
bers attest. While in general the measures are more effec-
tive in directly predicting the quality of the QL-based initial
ranking rather than serving as representativeness estimates,
the reverse holds for the QF measure over most collections.
26%, 24%, 27%, and 21%, respectively. Specific prediction results are omitted due to space considerations. 5This ranking-similarity-based predictor yields prediction performance of .607, .579, .46, .578 and .402 for TREC4, TREC5, WT10G, ROBUST, and GOV2, respectively. Hence, while it is an effective predictor at its own right, its integration with the representativeness measure yields, in general, much better prediction performance. 6Similar prediction performance patterns -- actual numbers are omitted to avoid cluttering the presentation -- are observed with respect to the ranking induced by the relevance model over the inital list DQ[L] ;q.

263

Predictor Clarity UEF(Clarity) WIG UEF(WIG) NQC UEF(NQC) QF UEF(QF)

TREC4 0.453
0.623 (+37.5%) 0.544
0.638 (+17.3%) 0.588
0.641 (+9%) 0.627
0.666 (+6.2%)

TREC5 0.42
0.629 (+49.8%) 0.297
0.555 (+86.9%) 0.354
0.545 (+53.9%) 0.414
0.538 (+29.9%)

WT10G 0.348
0.483 (+38.8%) 0.376
0.453 (+20.5%) 0.488
0.522 (+6.9%) 0.426
0.526 (+23.5%)

ROBUST 0.512
0.635 (+24%) 0.543
0.644 (+18.6%) 0.566
0.619 (+9.4%) 0.285
0.459 (+61%)

GOV2 0.433 0.462 (+6.7%) 0.479 0.458 (-4.4%) 0.36 0.393 (+9.2%) 0.476 0.491 (+3.1%)

avg. improv. (+31.4%) (+27.8%) (+17.7%) (+24.7%)

Table 2: Prediction quality of UEF when fixing the sampling technique to Single, fixing the inter-ranking similarity measure to Pearson, and using the four representativeness estimates. The prediction quality of each representativeness estimate when used to directly predict search effectiveness is presented for reference in the first row of a block. Best result in a column is boldfaced.

Figure 1: Effectiveness of the performance predictors for estimating the representativeness of a relevance model (RelM) as measured by the prediction quality of the ranking it induces over the corpus. The prediction quality of the initial querylikelihood-based ranking is presented for reference.
The finding regarding QF sheds some light on its high effectiveness when used in our framework. (See Table 2.) Recall that UEF(QF) operates as follows: a relevance model is constructed from DQ[L] ;q and is used to rank the corpus. The overlap between top-ranked documents and those at top-ranks of DQ[L] ;q serves for the relevance-model representativeness estimate; small overlap presumably attests to query-drift manifested by the relevance model. Then, the estimate is multiplied by the similarity (Pearson's coefficient) between DQ[L] ;q's original ranking and that induced over it by the relevance model.
4.2.3 Prediction for rankings produced by other retrieval methods
All the predictors that we have studied insofar operate in the language modeling framework. More specifically, we have focused on predicting the effectiveness of the ranking used to create the list DQ[L] ;q using a relevance language model constructed from this list. We now turn to study the effectiveness of our framework in predicting the quality of a ranking produced by another retrieval method. This challenge fits, for example, the scenario of predicting the performance of a retrieval method that may not be known to the predictor, but rather only the induced ranking and/or retrieval scores.

We predict performance for vector-space-based (VS) re-
trieval with the cosine measure, and for Okapi's BM25 re-
trieval model. In both cases Lemur's implementation with
default parameter settings is used.
Note that the effectiveness of a ranking of the corpus by
M (VS or BM25) is estimated by our predictors as follows.
We measure the similarity (Pearson's coefficient) between the original ranking of the result list DM [k];q of top-k retrieved documents by M, and the ranking of DM [k];q by a single relevance model constructed from DQ[L] ;q -- the QL-based result list; this similarity is then scaled by the estimated represen-
tativeness of the relevance model. Thus, the result list for which we predict performance, DM [k];q, is different than the list DQ[L] ;q used to construct a relevance model.
As in Table 2, we want to compare our predictor's perfor-
mance with that of applying the representativeness measure as a predictor at its own right directly to DM [k];q. Among the four measures used, NQC is the only predictor that has
a non language-model-based implementation; specifically, it
was shown to be effective with VS and BM25 [22]. Hence, in
Table 3 we compare the performance of our framework us-
ing NQC as a representativeness measure, UEF(NQC), with that of using NQC directly to predict performance7. We set
=k=150 as at the above.
The results in Table 3 clearly attest to the general ef-
fectiveness of our framework. Indeed, UEF(NQC) yields
better prediction performance for vector-space and Okapi-
BM25 retrieval than that of NQC over most collections.

VS-NQC VS-UEF(NQC) BM25-NQC BM25-UEF(NQC)

TREC4 0.660 0.678 0.578 0.668

TREC5 0.440 0.503 0.423 0.499

WT10G 0.407 0.393 0.31 0.508

ROBUST 0.535 0.625 0.6 0.592

Table 3: Using our framework, UEF(NQC), to predict the performance of vector space (VS) and Okapi-BM25 retrieval in comparison to using NQC to directly predict their performance. Boldface marks the best result in a block.

4.2.4 Integrating predictors
Integrating predictors using linear interpolation was shown to be of merit [8, 34]. As such integration yields a predictor for the quality of the initial list from which we construct a relevance model, we can use it in our framework as a repre-
7The original reports for NQC have not used GOV2 [22].

264

sentative measure. Specifically, when integrating WIG and QF [34] the resultant predictor is denoted UEF(WIG+QF), where WIG+QF is the interpolation-based predictor. We also study the performance of UEF(WIG)+UEF(QF) that interpolates UEF(WIG) and UEF(QF). Interpolation with equal weights is performed in all cases upon the min-max normalized values assigned by predictors. The prediction performance numbers are presented in Table 4.
In accordance with previous findings [34], we see in Table 4 that integrating WIG and QF (WIG+QF) results in performance superior to that of each over most corpora. Using the integrated predictor in our framework (UEF(WIG+QF)) yields further improvements for 3 out of the 5 corpora. Furthermore, for all corpora, except for GOV2, it is better to integrate our predictors that are based on WIG and QF -- i.e., UEF(WIG) and UEF(QF) -- than to integrate WIG and QF directly. (Compare UEF(WIG)+UEF(QF) and WIG+QF.)

WIG UEF(WIG)
QF UEF(QF)
WIG+QF UEF(WIG+QF) UEF(WIG)+ UEF(QF)

TREC4 0.544 0.638 0.627 0.666 0.676 0.663
0.679

TREC5 0.297 0.555 0.414 0.538 0.446 0.562
0.591

WT10G 0.376 0.453 0.426 0.526 0.472 0.513
0.521

ROBUST 0.543 0.644 0.285 0.459 0.503 0.586
0.591

GOV2 0.479 0.458 0.476 0.491 0.555 0.501
0.490

Table 4: Integrating predictors using linear interpolation (+). Best result in a column is boldfaced.

4.2.5 Deeper inside UEF
Heretofore, we have focused on instantiating our framework by utilizing the Single sampling strategy (i.e., a single relevance model estimate), and using Pearson's coefficient to measure inter-ranking similarities. We now turn to examine the effect of varying these factors, along with the measures used for estimating representativeness. To study the effectiveness of the latter when using M ulti sampling (i.e., constructing relevance models from clusters), we use as a reference comparison the Uniform measure that assigns all relevance models the same representativeness value. Table 5 presents the performance of all predictors.8
We can see in Table 5 that all instantiated predictors are effective as the relatively high performance numbers indicate. Specifically, almost all of these predictors yield positive average improvements (refer to the last column) over the representativeness measures that they incorporate when the latter are used to directly predict performance. A notable exception is the ROBUST benchmark with the M ulti strategy. (See the below for further discussion.) All in all, as the representativeness measures are state-of-the-art predictors at their own right, we find these results gratifying.
Among the inter-ranking similarity measures, we see that Pearson's coefficient in general performs best, attesting to the importance of considering the retrieval scores used to induce the rankings.
We can also see in Table 5 that using a single relevance model estimate (Single) yields superior performance to that
8Results for QF with M ulti are not presented as those require running tens of relevance models per query over the corpus. This is computationally demanding, and accordingly, does not constitute a realistic prediction scenario.

of utilizing multiple relevance models (M ulti) constructed from clusters. This finding is not surprising as most representativeness measures that we use are not well suited for estimating representativeness of a relevance model constructed from a small cluster, which is composed of some top-retrieved documents. This is of utmost importance as potentially very few of the clusters contain a high percentage of relevant documents, and identifying these is a hard challenge [14]. Indeed, WIG, which is based on retrieval scores within the clusters, yields performance that is inferior under M ulti sampling to that of using uniform values for representativeness. On the other hand, Clarity, which measures the "quality" of the relevance model constructed from the cluster with respect to the corpus, does improve consistently over uniform representativeness scores. This finding attests to the potential of cluster-based sampling.
5. CONCLUSION AND FUTURE WORK
We presented a novel framework, which is based on statistical decision theory, for predicting query performance. The quality of a given document ranking is predicted based on its expected similarity with those induced by estimates of relevance models; the presumed representativeness of a relevance-model estimate of the underlying information need determines its impact. Relevance-model representativeness is measured using state-of-the-art query-performance predictors that were originally designed to estimate the quality of the initial search. Empirical evaluation shows that predictors instantiated from our framework are substantially more effective than current state-of-the-art predictors.
Improving the sampling technique used for relevance-model construction, and devising and adapting [24] better measures of representativeness for relevance models constructed from clusters, are future directions we intend to explore.
Acknowledgments We thank the reviewers for their comments. This paper is based upon work supported in part by Israel's Science Foundation under grant no. 890015. and by G. S. Elkin research fund at the Technion. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsoring institutions.
6. REFERENCES
[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey, X. Li, M. D. Smucker, and C. Wade. UMASS at TREC 2004 -- novelty and hard. In Proceedings of TREC-13, pages 715­725, 2004.
[2] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness and selective application of query expansion. In Proceedings of ECIR, pages 127­137, 2004.
[3] J. A. Aslam and V. Pavlu. Query hardness estimation using Jensen-Shannon divergence among multiple scoring functions. In Proceeding of ECIR, pages 198­209, 2007.
[4] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In Proceedings of SIGIR, pages 390­397, 2006.
[5] K. Collins-Thompson and J. Callan. Estimation and use of uncertainty in pseudo-relevance feedback. In Proceedings of SIGIR, pages 303­310, 2007.
[6] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proceedings of SIGIR, pages 299­306, 2002.
[7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A language modeling framework for selective query expansion. Technical Report IR-338, Center for Intelligent Information Retrieval, University of Massachusetts, 2004.

265

Sampling Single
M ulti

Similarity P earson S pearman s- K endall s-
P earson S pearman s- K endall s-

Rep. Clarity WIG NQC QF
Clarity WIG NQC QF Clarity WIG NQC QF Clarity WIG NQC QF
Uniform Clarity WIG NQC Uniform Clarity WIG NQC Uniform Clarity WIG NQC

TREC4 0.453 0.544 0.588 0.627
0.623(+37%) 0.638(+17%) 0.641(+9%) 0.666(+6%) 0.687(+52%) 0.644(+18%) 0.633(+8%) 0.642(+2%) 0.685(+51%) 0.633(+16%) 0.617(+5%) 0.632(+0.8%)
0.617 0.641(+41%) 0.603(+11%) 0.611(+4%)
0.636 0.653(+44%) 0.612(+13%) 0.599(+2%)
0.644 0.656(+45%) 0.61(+12%) 0.591(+0.5%)

TREC5 0.42 0.297 0.354 0.414
0.629(+50%) 0.555(+87%) 0.545(+54%) 0.538(+30%) 0.419(-0.2%) 0.369(+24%) 0.369(+4%) 0.464(+12%) 0.428(+2%) 0.377(+27%) 0.374(+6%) 0.470(+13%)
0.441 0.479(+14%) 0.433(+46%) 0.416(+17%)
0.478 0.489(+16%) 0.441(+48%) 0.418(+18%)
0.493 0.5(+19%) 0.451(+52%) 0.426(+20%)

WT10G 0.348 0.376 0.488 0.426
0.483(+39%) 0.453(+20%) 0.522(+7%) 0.526(+23%) 0.387(+11%) 0.351(-7%) 0.436(-11%) 0.433(+2%) 0.369(+6%) 0.322(-14%) 0.411(-16%) 0.425(-0.3%)
0.443 0.454(+30%) 0.425(+13%) 0.531(+9%)
0.371 0.379(+9%) 0.349(-7%) 0.454(-7%)
0.377 0.382(+10%) 0.347(-8%)
0.45(-8%)

ROBUST 0.512 0.543 0.566 0.285
0.635(+24%) 0.644(+19%) 0.619(+9%) 0.459(+61%) 0.489(-4%) 0.521(-4%) 0.526(-7%) 0.369(+29%) 0.51(-0.4%) 0.537(-1%) 0.535(-5%) 0.384(+35%)
0.391 0.457(-11%) 0.443(-19%) 0.539(-5%)
0.3 0.356(-30%) 0.358(-34%) 0.454(-20%)
0.33 0.381(-26%) 0.379(-30%) 0.47(-17%)

GOV2 0.433 0.479 0.36 0.476
0.462(+7%) 0.458(-4%) 0.393(+9%) 0.491(+3%) 0.458(+6%) 0.459(-4%) 0.39(+8%) 0.503(+6%) 0.461(+7%) 0.457(-4%) 0.376(+5%) 0.501(+5%)
0.435 0.436(+1%) 0.471(-2%) 0.418(+16%)
0.447 0.441(+2%) 0.474(-1%) 0.42(+17%)
0.445 0.439(+1%) 0.467(-2%) 0.403(+12%)

avg. improv.
+31% +28% +18% +25% +13% +5% +0.4% +10% +13% +5% -1% +11%
+15% +10% +8%
+8% +4% +2%
+10% +5% +1%

Table 5: The effectiveness of predictors instantiated from our framework. We vary the document-sets sampling approach, the inter-rankings similarity measure, and the representativeness (rep.) measure of the relevance model estimate; "Uniform" stands for using the same constant representativeness value for all relevance models. The first four rows present the performance of the representativeness measures when used to directly predict performance; percentages of improvements are with respect to these values.

[8] F. Diaz. Performance prediction using spatial autocorrelation. In Proceedings of SIGIR, pages 583­590, 2007.
[9] D. Harman and C. Buckley. The NRRC reliable information access (RIA) workshop. In Proceedings of SIGIR, pages 528­529, 2004.
[10] C. Hauff, L. Azzopardi, and D. Hiemstra. The combination and evaluation of query performance prediction methods. In Proceedings of ECIR, pages 301­312, 2009.
[11] C. Hauff, D. Hiemstra, and F. de Jong. A survey of pre-retrieval query performance predictors. In Proceedings of CIKM, pages 1419­1420, 2008.
[12] C. Hauff, V. Murdock, and R. Baeza-Yates. Improved query difficulty prediction for the web. In Proceedings of CIKM, pages 439­448, 2008.
[13] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In Proceedings of SPIRE, pages 43­54, 2004.
[14] O. Kurland. The opposite of smoothing: A language model approach to ranking query-specific document clusters. In Proceedings of SIGIR, pages 171­178, 2008.
[15] J. D. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR, pages 111­119, 2001.
[16] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proceedings of SIGIR, pages 120­127, 2001.
[17] K.-S. Lee, W. B. Croft, and J. Allan. A cluster-based resampling method for pseudo-relevance feedback. In Proceedings of SIGIR, pages 235­242, 2008.
[18] M. Mitra, A. Singhal, and C. Buckley. Improving automatic query expansion. In Proceedings of SIGIR, pages 206­214, 1998.
[19] J. Mothe and L. Tanguy. Linguistic features to predict query difficulty. In ACM SIGIR 2005 Workshop on Predicting Query Difficulty - Methods and Applications, 2005.
[20] S. E. Robertson. The probability ranking principle in IR. Journal of Documentation, pages 294­304, 1977.
[21] F. Scholer, H. E. Williams, and A. Turpin. Query association surrogates for web search. Journal of the American Society for Information Science and Technology (JASIST), 55(7):637­650, 2004.

[22] A. Shtok, O. Kurland, and D. Carmel. Predicting query performance by query-drift estimation. In Proceedings of ICTIR, pages 305­312, 2009.
[23] F. Song and W. B. Croft. A general language model for information retrieval (poster abstract). In Proceedings of SIGIR, pages 279­280, 1999.
[24] N. Soskin, O. Kurland, and C. Domshlak. Navigating in the dark: Modeling uncertainty in ad hoc retrieval using multiple relevance models. In Proceedings of ICTIR, pages 79­91, 2009.
[25] S. Tomlinson. Robust, Web and Terabyte Retrieval with Hummingbird Search Server at TREC 2004. In Proceedings of TREC-13, 2004.
[26] V. Vinay, I. J. Cox, N. Milic-Frayling, and K. R. Wood. On ranking the effectiveness of searches. In Proceedings of SIGIR, pages 398­404, 2006.
[27] E. M. Voorhees. Overview of the TREC 2004 Robust Retrieval Track. In Proceedings of TREC-13, 2004.
[28] M. Winaver, O. Kurland, and C. Domshlak. Towards robust query expansion: Model selection in the language model framework to retrieval. In Proceedings of SIGIR, pages 729­730, 2007.
[29] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In Proceedings of SIGIR, pages 512­519, 2005.
[30] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR, pages 334­342, 2001.
[31] Y. Zhao, F. Scholer, and Y. Tsegay. Effective pre-retrieval query performance prediction using similarity and variability evidence. In ECIR, pages 52­64, 2008.
[32] Y. Zhou. Retrieval Performance Prediction and Document Quality. PhD thesis, University of Massachusetts, September 2007.
[33] Y. Zhou and W. B. Croft. Ranking robustness: a novel framework to predict query performance. In Proceedgins of CIKM, pages 567­574, 2006.
[34] Y. Zhou and W. B. Croft. Query performance prediction in web search environments. In Proceedings of SIGIR, pages 543­550, 2007.

266

Estimation of Statistical Translation Models Based on Mutual Information for Ad Hoc Information Retrieval

Maryam Karimzadehgan
Department of Computer Science University of Illinois at Urbana-Champaign
Urbana, IL 61801
mkarimz2@illinois.edu
ABSTRACT
As a principled approach to capturing semantic relations of words in information retrieval, statistical translation models have been shown to outperform simple document language models which rely on exact matching of words in the query and documents. A main challenge in applying translation models to ad hoc information retrieval is to estimate a translation model without training data. Existing work has relied on training on synthetic queries generated based on a document collection. However, this method is computationally expensive and does not have a good coverage of query words. In this paper, we propose an alternative way to estimate a translation model based on normalized mutual information between words, which is less computationally expensive and has better coverage of query words than the synthetic query method of estimation. We also propose to regularize estimated translation probabilities to ensure sufficient probability mass for self-translation. Experiment results show that the proposed mutual information-based estimation method is not only more efficient, but also more effective than the synthetic query-based method, and it can be combined with pseudo-relevance feedback to further improve retrieval accuracy. The results also show that the proposed regularization strategy is effective and can improve retrieval accuracy for both synthetic query-based estimation and mutual information-based estimation.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms
Algorithms, Theory
Keywords
Statistical Machine Translation, Language Models, Estimation, Smoothing, Feedback
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

ChengXiang Zhai
Department of Computer Science University of Illinois at Urbana-Champaign
Urbana, IL 61801
czhai@cs.illinois.edu
1. INTRODUCTION
Designing effective retrieval models is central for information retrieval. In the past, many retrieval models such as vector space model [28, 29, 30] and probabilistic model [6, 22, 25, 27, 34] have been proposed and gained certain success. Recently, language modeling approaches have received considerable attentions because of its sound statistical foundation and good empirical performance [22, 42]. In language modeling approaches, documents are ranked according to how likely a query is generated from the corresponding document models. In basic language models, document models are estimated based on multinomial distribution and smoothing techniques are critical for document model estimation [42]. When ranking documents, the basic language modeling approach is primarily based on exact matching of terms between documents and queries. Since queries are generally succinct and relevant documents may use different vocabulary, such an approach can suffer from vocabulary gap problem.
As a principled approach to capturing semantic word relations, statistical translation language models have been proposed for information retrieval to reduce the gap between documents and queries [2, 8]. Based on statistical machine translation [3], the basic idea of translation language models is to estimate the likelihood of translating a document to a query. Since a term has certain probability to be translated into a different term, translation language models can alleviate the vocabulary gap problem in a direct manner. As a result, translation language models have been successfully applied to different tasks such as cross-lingual information retrieval [12, 20, 39], question answering [40], sentence retrieval [19], and tracking information flow [18].
Surprisingly, there has been little work on applying translation models to ad hoc retrieval. Indeed, the original paper [2] that proposed translation models for ad hoc retrieval appears to be the only study that we are aware of. One possible reason may be because of the difficulty in estimating translation models. In [2], authors solved the problem by generating synthetic queries. Unfortunately, this method has two deficiencies: (1) it is inefficient; (2) there is no guarantee that a query word is covered.
In this paper, we propose a simpler method for estimating a translation model, which is based on normalized mutual information between words. Our Contributions are as follows:
1. We propose an efficient and effective way of estimating word-to-word translation probabilities based on mutual information.

323

2. We propose regularization of self-translation probabilities, which can improve retrieval performance of translation models with both the existing estimation approach and the proposed mutual information-based approach.
3. We study the issue of smoothing in the context of translation language modeling and show that translation language models are less sensitive to the effect of smoothing.
4. We show that with mutual information, the translation language model can be combined with pseudorelevance feedback to further improve the retrieval accuracy.

2. STATISTICAL TRANSLATION MODEL FOR RETRIEVAL
In this section, we review basic language modeling approach, statistical translation language model and smoothing methods for statistical translation model. Finally, we discuss the estimation of translation model.

2.1 Basic Language Modeling Approach
The language modeling approach to information retrieval was first introduced by Ponte and Croft [22]. The basic idea can be described as follows. We assume that a query q is generated by a probabilistic model based on a document d. Given a query q = q1, q2, . . . , qm, and a document d, we are interested in estimating p(d|q) , i.e. the probability that document d has been used to generate query q. By applying Bayes' formula, we have:

p(d|q)  p(q|d)p(d)

p(d) on the right hand side of the above formula is our prior belief that document d is relevant to any query. p(q|d) is the query likelihood for the given document d, which intuitively measures how well document d matches query q. p(d) is often assumed to be uniform and thus can be ignored for ranking documents. Further assuming that each query word is generated independently, we can rewrite the above formula as (in the form of log likelihood):

log p(d|q) = rank

c(w, q). log p(w|d)

wV

where = rank means equivalence for the purpose of ranking documents, c(w, q) is count of word w in query q, and V is the vocabulary set. The challenging part is to estimate a document model p(w|d). Based on multinomial distribution, the simplest way to estimate p(w|d) is the maximum likelihood estimator :

pml(w|d) =

c(w, d) w c(w , d)

Where c(w, d) is count of word w in document d. Due to the data sparseness problem, maximum likelihood estimator under-estimates the probability of unseen words in a document. Smoothing techniques address this problem by assigning non-zero probabilities to the unseen words and thus improving the accuracy of probability estimation. Specifically, smoothing is to discount the probabilities of words seen in the text and then assign extra probability mass to the unseen words according to some fallback model. Usually, collection

language model is used as fallback model [42]. Two commonly used methods are Jelinek-Mercer and Dirichlet Prior smoothing methods:

Jelinek-Mercer Method (JM Smoothing): This is a linear interpolation of maximum likelihood model with the collection model, using  as a coefficient weight.

p(w|d) = (1 - )pml(w|d) + p(w|C)

(1)

Where p(w|C) is probability of word w in collection C. Bayesian Smoothing using Dirichlet Prior (Dirichlet Prior
Smoothing): Since the conjugate prior of a multinomial distribution is the Dirichlet distribution, we can specify a Dirichlet prior distribution parameterized as

(p(w1|C), p(w2|C), . . . , p(wn|C))

where  is a parameter. The estimated document model based on the posterior mean is then:

p(w|d)

=

|d| |d| +

 pml(w|d)

+

 |d| +

p(w|C ) 

(2)

2.2 Statistical Translation Language Model
Another interesting way of estimating p(w|d) introduced by Berger and Lafferty [2] is based on statistical machine translation [3]. In order to assess the relevance of a document to a user's query, they have estimated the probability that the query would have been generated as a translation of the document. In other words, they allow the query likelihood to be computed based on a translation model of form p(w|u), which is the probability that word u is semantically translated to word w.
To put it more formally, in their model, the query likelihood can be calculated by using the following "translation document model":

pt(w|d) = pt(w|u)p(u|d)
ud
where pt(w|u) is the probability of "translating" word u into word w and it allows us to score a document by counting the matches between a query word and semantically related words in the document. If pt(w|u) only allows a word to be translated into itself, the simple exact matching query likelihood would be achieved. However, pt(w|u) would in general allow us to translate u into other semantically related words with non-zero probabilities, thus achieving "semantic smoothing" of the document language model.

2.3 Smoothing for Translation Language Model
In this section, we consider statistical machine translation when combined with two basic smoothing methods described in section 2.1.
The basic component in the translation language model is pt(w|d) = ud pt(w|u)p(u|d) which can be used to replace pml(w|d) in all basic language model approaches. This will give us 1) translation language model with Dirichlet prior smoothing and 2) translation language model with Jelinek-Mercer smoothing. When we replace pml(w|d) with pt(w|d) = ud p(u|d)pt(w|u) in equation 2, we have the following:

pt(w|d)

=

|d| |d| +



[

p(u|d)

·

pt(w|u)]

+

 |d| +

 p(w|C)

(3)

ud

324

And when pt is replaced with pml in equation 1, we have the following:

pt(w|d) = (1 - )[ p(u|d) · pt(w|u)] + p(w|C) (4)
ud
Equations 3 and 4 give us Dirichlet prior smoothing and Jelinek-Mercer (JM) smoothing with translation language model, respectively.
Authors in [2] only considered translation language model with Jelinek-Mercer smoothing.

2.4 Estimation of Translation Model

The key part for translation language model is to learn

the word-to-word translation probability, pt(w|u). It is clear

that the performance of the proposed smoothed translation

model depends on the quality of the word-to-word trans-

lation probabilities. In the scenario of statistical machine

translation [3], a parallel corpus of two languages is often

assumed to be available, and the EM algorithm [5] can be

used to estimate a translation model.

In order to gain word-to-word probabilities in monolingual

scenario, ideally, we should have a sample of queries and rel-

evant documents, but since we do not often have, Berger and

Lafferty [2] use the idea of synthetic queries as their training

data. The idea is to take a document and synthesize a query

to which the document would be relevant. They proposed

a sampling technique which distinguishes a document from

other documents.

In order to select words which are representative of a doc-

ument, for each document d  D, they compute the mutual

information statistics [7] for each of its words according to:

I(w, d)

=

p(w, d) log

p(w|d) p(w|D)

,

where

p(w|d)

is

the

probabil-

ity of word w in document d, and p(w|D) is the probabil-

ity of word w in the collection. Their proposed algorithm

for generating synthetic queries is shown in figure 1, where

synthetic queries are sampled based on normalized mutual information I~, and the Poisson parameter  is set to 15. The

resulting (d, q) of documents and synthetic queries are used

to estimate the probabilities with the EM algorithm. More

details can be found in [2].

1. Begin

2.

Do for each document d  D

3.

Do for x = 1 to 5

4.

Begin

5.

Select a length m for this query according to

Poisson distribution

6.

Do for i = 1 to m

7.

Select the next query word by sampling the

scaled distribution: qi  I~

8.

Record (d, q)

9.

End

10. End

Figure 1: A sampling for synthetic queries
Although generating synthetic queries is a reasonable way to estimate the translation probabilities, this method has two deficiencies: (1) it is inefficient; (2) there is no guarantee that a query word is covered. In the next section, we propose a mutual information-based estimation which is more efficient than this method and has a better word coverage.

3. ESTIMATION OF TRANSLATION MODEL BASED ON MUTUAL INFORMATION
In this section, we propose a more efficient way to estimate translation probabilities which can have a better coverage of query words than the existing method discussed in the previous section. We will also present a way to combine translation language model with pseudo-relevance feedback.

3.1 Mutual Information-Based Approach

Mutual information [26] is a good measure to assess how two words are related. In our method, for each word in the collection, we compute all words which have high mutual information scores with it and normalize the computed mutual information scores as follows:
First, we compute the mutual information scores for each pair of two words w and u in the collection. Informally, mutual information compares the probability of observing w and u together (the joint probability) with the probabilities of observing w and u independently. The mutual information between words w and u are calculated as follows:

I (w;

u)

=

Xw =0,1

Xu =0,1

p(Xw,

Xu)

log

p(Xw, Xu) p(Xw )p(Xu )

(5)

where Xu and Xw are binary variables indicating whether u
or w is present or absent. The probabilities are estimated as follows:

p(Xw = 1)

=

c(Xw = 1) N

p(Xw = 0) = 1 - p(Xw = 1)

p(Xu = 1)

=

c(Xu = 1) N

p(Xu = 0) = 1 - p(Xu = 1)

p(Xw = 1, Xu = 1)

=

c(Xw = 1, Xu = 1) N

p(Xw = 1, Xu = 0)

=

(c(Xw = 1) - c(Xw = 1, Xu = 1)) N

p(Xw = 0, Xu = 1)

=

(c(Xu = 1) - c(Xw = 1, Xu = 1)) N

p(Xw = 0, Xu = 0) = 1 - p(Xw = 0, Xu = 1)

-p(Xw = 1, Xu = 0) - p(Xw = 1, Xu = 1)

where c(Xw = 1) and c(Xu = 1) are the numbers of documents containing word w and u, respectively, c(Xw = 1, Xu = 1) is the number of documents that contain both w and u, and N in the total number of documents in the collection.
We then normalize the mutual information score to obtain a translation probability:

pmi(w|u) =

I(w; u) w I(w ; u)

(6)

pmi(w|u) gives us the probability of translating word u to another word w; intuitively, the probability would be higher if the two words tend to co-occur with each other.

3.2 Optimizing Self-Translation Probability
The approaches described in sections 3.1 and 2.4 might under-estimate the self-translation probabilities, i.e., it is possible that p(w|u) > p(w|w). This may lead to nonoptimal retrieval performance because it is possible that a document that matches a query word exactly (p(w|w)) gets

325

Table 1: Sample word translation probabilities using

synthetic queries (left) and mutual information (right).

Note that words are stemmed.

w=everest

w=everest

q

p(q|w)

q

p(q|w)

everest 0.079

everest 0.1051

climber 0.042

climber 0.0423

climb 0.0365

mount 0.0339

mountain 0.0359

028

0.0308

mount 0.033

expedit 0.0303

reach 0.0312

peak

0.0155

expedit summit

0.0314 0.0253

himalaya 0.01532

nepal

0.015

whittak 0.016

sherpa 0.01431

peak

0.0149

hillari 0.01431

MAP Precision at 10

0.29 0.28 0.27 0.26 0.25 0.24 0.23 0.22 0.21
0

0.44 Synthetic Queries Mutual Information
0.43

0.42

0.41

0.4

0.39

0.38

0.2

0.4

0.6

0.8

Alpha

0.37

1

0

Synthetic Queries Mutual Information

0.2

0.4

0.6

0.8

1

Alpha

Figure 2: Comparison of mutual information and synthetic queries according to MAP (Left) and Precision at 10 (right). (Both are according to Dirichlet prior smoothing).

less score contribution from matching the query word exactly than a document that "matches" a query word through translation (p(w|u)). To overcome this bias, we introduce a parameter  to control the effect of self-translation. This is a general method that can be applied to adjust the estimated probabilities from any given estimation method.

pt(w|u) =

 + (1 - )p(u|u) (1 - )p(w|u)

w=u w=u

and p(w|u) is estimated either with mutual information or synthetic queries.  is a parameter that controls the effect of self-translation probability and when we set  = 1, we recover the basic query likelihood method.
The "regularized" translation model pt(w|u) can then be used in Equations 3 and 4 to rank documents.
3.3 Translation Language Model with Feedback
Feedback techniques have been shown to improve retrieval accuracy substantially[13, 27, 41]. A natural question with translation model is whether translation model can benefit from feedback techniques. In this section, we use pseudorelevance feedback to expand our query model [41] and then score the expanded query model with translation language model based on the negative cross entropy of the expanded query language model and the translation document model (also equivalent to scoring based on negative KL-divergence):
p(w|q). log pt(w|d)
p(w|q )>0
where p(w|q) is the query model generated by pseudo-relevance feedback and pt(w|d) is a smoothed translation model and can be computed using either of equations 3 or 4.

4. EXPERIMENTS
4.1 Data Set
The experiments in this section use four main document collections: (1) news articles (AP90) with TREC topics 51100 and 78,321 articles. (2) San Jose Mercury News (SJMN) articles with TREC topics 51-100 and 90,250 articles (3) ad hoc data in TREC7 with topics 351-400 and 528,155 articles and (4) TREC8 with topics 401-450 and 528,155 articles.
In the experiments, we only use title of the queries. As for preprocessing, we do stemming using Porter stemmer [23] and stop word removal. All experiments are done using the

Lemur toolkit 1. The performance is measured using two standard measures: MAP(mean average precision) and precision @10 (precision at 10).
The optimal value for Dirichlet prior smoothing for baseline is 1000 for all data sets and optimal value for JM smoothing for baseline method is gained when coefficient is set to 0.5 for AP90 data set and 0.3 for the rest of data sets.
The methods used for experiments in the following sections are: BL (baseline), i.e., either Dirichlet prior smoothing or JM smoothing [42], TM-MI (translation language model with mutual information2 for word-to-word translation probabilities), TM-SYN (translation language model with synthetic queries), fb (pseudo-relevance feedback on baseline) and fb+TM(pseudo-relevance feedback combined with translation language model using mutual information).
4.2 Comparing Synthetic Queries with Mutual Information
We first look into the question whether mutual information (MI) can be an alternative way of estimating translation model. Table 2 shows the results for both TM-SYN and TMMI methods with both Dirichlet prior smoothing and JM smoothing, respectively. The results indicate that TM-MI method is able to better capture word relatedness. Indeed, statistical significance tests indicate that the difference between TM-MI and TM-SYN is statistically significant. In addition, estimating translation probabilities by mutual information for all data sets is more efficient than learning translation probabilities by synthetic queries. Table 1 shows a document word together with ten most probable query words that it will translate to by both synthetic queries and mutual information estimation methods. The table shows that the related words for word "everest" in case of mutual information are more specific than for words learned via synthetic queries.
Figure 2 shows the sensitivity of mutual information and synthetic queries to  parameter according to MAP measure (left) and Precision@ 10 (right). The difference indeed makes clearer that mutual information works better than synthetic queries. (Our results for synthetic queries are comparable to those reported in [2].)
According to these results, we can conclude that mutual
1http://www.lemurproject.org/ 2We use mutual information throughout the paper for simplicity but we mean the normalized mutual information described in section 3.1.

326

Table 2: Performance of Translation Language model with synthetic queries and mutual information estimation

according to Dirichlet prior smoothing (left) and JM smoothing (right), * means improvements over TM-SYN are

statistically significant with Wilcoxon signed-rank test. We only show the significance tests for MAP measure.

Data

MAP

Precision @10

Data

MAP

Precision @10

AP-90 SJMN

TM-MI 0.272
0.2

TM-SYN 0.251 0.195

TM-MI 0.423 0.28

TM-SYN 0.404 0.266

AP-90 SJMN

TM-MI 0.264* 0.197*

TM-SYN 0.25 0.189

TM-MI 0.381 0.252

TM-SYN 0.357 0.267

Table 3: Performance of Translation Language model on different datasets with Dirichlet Prior smoothing (left) and

JM smoothing (right), * means improvements over baseline are statistically significant with Wilcoxon signed-rank test.

We only show the significance tests for MAP measure.

Data
AP-90 SJMN TREC7

MAP

BL 0.248 0.195 0.183

TM-MI 0.272
0.2 0.187

Precision @10 BL TM-MI 0.398 0.423 0.266 0.28 0.412 0.404

Data
AP-90 SJMN TREC7

MAP BL TM-MI 0.246 0.264* 0.188 0.197* 0.165 0.172

Precision @10 BL TM-MI 0.357 0.381 0.252 0.267 0.354 0.362

TREC8 0.248 0.249 0.452 0.456

TREC8 0.236 0.244* 0.428 0.436

information works better than synthetic queries and it is also more efficient.
Because of the high computational complexity of synthetic queries, we cannot compare mutual information with it on larger collections, but later we will further experiment with mutual information on larger collections.
4.3 Comparing Translation Language Model with Standard Query Likelihood
We now look into how well a translation model with our mutual information-based estimation method performs as compared with the standard query likelihood method. Table 3 shows the results for BL and TM-MI methods according to two measures MAP and Precision @10.
Comparing the columns TM-MI with BL in both tables indeed indicates that the TM-MI outperforms method BL. Significant tests using Wilcoxon signed-rank test [37] show the difference between these two methods for cases marked in the tables are statistically significant. Comparing TM-MI with Dirichlet prior smoothing and TM-MI with JM smoothing shows that TM-MI with Dirichlet prior smoothing has higher MAP than TM-MI with JM smoothing.
Stress Tests: In order to have a better understanding of the translation language model, we applied some stress tests on AP90 data set3. This experiment is to help us understand when exactly the translation language model would be most beneficial. For the stress test, we gradually and randomly remove query words from relevant documents and compare the performance of BL method with TM-MI method. The results of MAP and Precision @10 are shown in Figure 3.
The results indeed indicate that the baseline method (BL) is purely based on exact matching and the performance will drop significantly if the exact matching does not happen. On the other hand, translation language model (TM-MI) is still able to find relevant documents by translating query words to semantically related words in the documents. This indicates that the translation language model works significantly better than the baseline when there is a vocabulary gap between queries and documents.
3We got the same trends on other data sets, but we only show the results for AP90 data set.

4.4 Effect of Smoothing on Translation Language Model
Understanding the influence of smoothing on translation language model is important and no previous work has looked into this. We have a good understanding of smoothing methods for basic language models [42], but it is not clear how smoothing affects the performance of statistical translation language models. In this section, we look into how statistical translation model behaves with the smoothing parameters.
We vary the smoothing parameters (both JM and Dirichlet prior smoothing) for both BL and TM-MI methods. Figure 4 (left and middle) shows the variation of the JM smoothing parameter and Dirichlet prior smoothing parameter on AP90, respectively (we do not show the results on other data sets since they are similar). The result of TM-MI with JM smoothing indicates that the translation model does need a very little smoothing. As shown, the optimal values for translation language model with Dirichlet prior smoothing is 1000 and with JM smoothing is 0.1. As a result, translation language model is less sensitive to the choice of smoothing parameter than the baseline method. And this is intuitively expected, as smoothing is implicitly gained by translating a document word to other semantically related words.
Please note that in the translation language model, we have one other parameter to tune, i.e., the number of words used for translation. Figure 4 (right) shows the sensitivity of the number of the words according to MAP measure. As shown in the figure, the translation language model is not so sensitive to the number of words used for translation.
4.5 Results with Pseudo-Relevance Feedback
Both statistical translation model and pseudo-relevance feedback are to capture word associations, so it would be interesting to see whether they are essentially taking advantage of the same associations or they can be combined to achieve even more improvement.
Table 4 shows the pseudo-relevance feedback results for baseline (fb) and when pseudo-relevance feedback is combined with translation language model (fb+TM). For fb+TM method, we first apply pseudo-relevance feedback on initial results (i.e., KL-divergence retrieval model [11]), and then this new query model from pseudo-relevance feedback is used with translation language model to score documents. The feedback parameters are fixed to extract 20 expanded

327

Precision at 10

0.12 Baseline - Precision @10 Tanslation - Precision @10
0.1

0.08

0.06

0.04

0.02

0

1

1.5

2

2.5

3

3.5

4

4.5

5

Number of query words removed

MAP

0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01
0 1

Baseline - MAP Translation - MAP
1.5 2 2.5 3 3.5 4 4.5 5 Number of query words removed

Precision

0.25 0.2
0.15 0.1
0.05 0 0

Baseline Translation Model

0.2

0.4

0.6

0.8

1

Recall

Figure 3: Stress Tests on AP90 Collection, Precision @10 (left) and MAP (middle). Precision-Recall curve when only "one query word" is removed from relevant documents (right)

MAP

0.3 0.25
0.2 0.15
0.1 0.05
0 0

Baseline Translation - Mutual information

0.2

0.4

0.6

0.8

1

JM Parameter

MAP

0.3 0.25
0.2 0.15
0.1 0.05
0 0

Baseline Translation - Mutual information

2000

4000

6000

8000

Dirichlet Prior Parameter

10000

MAP

0.3 0.25
0.2 0.15
0.1 0.05
0 0

AP90 SJMN TREC7 TREC8

20

40

60

80

100

Number of Words Used for Translation

Figure 4: JM parameter variation on AP90 (left), Dirichlet prior parameter variation on AP90 (middle) and Sensitivity of number of words used for translation to MAP (right).

words from the top 10 retrieved documents in the initial run. As shown in table 4, fb-TM method indeed outperforms fb method when used with JM smoothing. Statistical significant tests reveal that the difference is indeed statistically significant. However, fb+TM method does not significantly outperform fb method when used with Dirichlet prior smoothing. An interesting observation is that although the performance of pseudo-feedback (fb) method with JM smoothing is lower than pseudo-feedback with Dirichlet prior smoothing, when pseudo-feedback (fb) is combined with translation language model, i.e., fb+TM method, the better performance is gained with JM smoothing. In fact, the performance of fb+TM with JM smoothing is consistently better than the fb+TM with Dirichlet prior smoothing.
Figure 5 shows the P-R curves for BL, fb and fb+TM methods with JM Smoothing on AP904. This figure indeed indicates that the precision of fb+TM method at different recall points is higher than BL and fb methods. This is an interesting conclusion that translation language model brings in co-occurrence word knowledge that once combined with pseudo-relevance feedback, significant improvement is gained.
4.6 The Need for Self-Translation Regularization
A potential problem of the estimated translation probabilities is that it is possible that p(w|u) > p(w|w). This may lead to non-optimal retrieval performance because it is possible that a document that matches a query word exactly (p(w|w)) gets less score contribution from matching the query word exactly than a document that "matches" a
4We do not show other curves due to their similarity.

Precision

0.7

Baseline

0.6

Feedback Feedback&Translation

0.5

0.4

0.3

0.2

0.1

0

0

0.2

0.4

0.6

0.8

1

Recall

Figure 5: Comparison of Baseline with Translation Language model combined with pseudo-feedback and pseudo-feedback alone on AP90 data set with JM smoothing

query word through translation (p(w|u)). The interpolation formula (with ) can help alleviate this problem; indeed, if   0.5, we can always ensure that this constraint be satisfied. So, it would be interesting to see how  affects the performance. Figure 6 shows the sensitivity of  parameter according to MAP measure. We indeed observe that when  is very small (close to no interpolation) the performance is poor, suggesting that it is important to regulate the selftranslation probabilities to ensure that it is sufficiently large. In Figure 6, we can see that when 0.5    0.8 for most data sets, we can gain the optimal value. Note that when  = 1, we reach the baseline.
4.7 Findings
1. Translation language model is statistically significant bet-

328

Table 4: Performance of Translation Language model combined with pseudo-feedback with Dirichlet Prior smoothing

(left) and JM smoothing (right), * and + mean improvements over baseline and fb, respectively, are statistically

significant with Wilcoxon signed-rank test. We only show the significance tests for MAP measure.

Data
AP-90 SJMN TREC7 TREC8

BL 0.248 0.195 0.183 0.248

MAP fb 0.285 0.231 0.226 0.270

fb+TM 0.285 0.232 0.226 0.278

Precision @10

BL

fb fb+TM

0.3978 0.404 0.406

0.266 0.295

0.3

0.412 0.38

0.38

0.452 0.456 0.438

Data
AP-90 SJMN TREC7 TREC8

BL 0.246 0.188 0.165 0.236

MAP

fb

fb+TM

0.271 0.298*+

0.229 0.234*+

0.209 0.222*+

0.240 0.281*+

Precision @10

BL

fb fb+TM

0.357 0.383 0.411

0.252 0.316 0.313

0.354 0.38 0.384

0.428 0.4

0.452

MAP

0.35 0.3
0.25 0.2
0.15 0.1
0.05 0

AP90 SJMN TREC7 TREC8

0.2

0.4

0.6

0.8

1

Alpha

Figure 6: Sensitivity of  parameter to MAP measure

ter than the baseline query likelihood especially when there is a vocabulary gap. 2. Normalized mutual information can be used for wordto-word translation effectively and the results in the previous sections indicate that it is more accurate than synthetic queries. Synthetic queries are inefficient for a large collection such as TREC7 or TREC8. 3. The performance of translation language model combined with pseudo-relevance feedback outperforms pseudorelevance feedback alone; this indicates that translation language model brings in co-occurrence knowledge in addition. 4. Translation language model is less sensitive to the choice of smoothing parameter than the baseline. 5. Translation language model is robust as it improves over all individual queries.
5. RELATED WORK
Language modeling approaches received considerable attentions recently [22]. One of the most important challenges in language model-based information retrieval is to estimate a better document model. Smoothing is an important approach for document model estimation and has been shown to be critical for information retrieval [42]. To further improve the estimation of document models, different heuristics have been proposed in the past. For example, cluster or topic-model based approaches have been studied in [16, 36]. Tao et al. [33] proposed a document expansion approach to enrich document representation before estimating document models.
Statistical translation models were originally studied in machine translation with the goal of automatically translating sentences between different languages (e.g., French and English) [3] where authors proposed five different translation models. The simplest model (i.e., IBM 1) [3] ignores position information when learning word-to-word translation probabilities. This model has been adopted in information retrieval by Berger and Lafferty [2]. To train translation mod-

els, they synthetically generated (query, document) pairs. An alternative way of estimating the translation model is based on document titles [8]. In this work, the authors proposed to use (title, document) pairs as training data. These estimation methods are inefficient and the coverage of query words is low. Our proposed mutual information-based estimation is more efficient and has a better query words coverage.
Translation models have been naturally used in crosslingual information retrieval domain [20, 39]. For example, Nie et al. [20] used parallel corpus as training data to learn translation models. The work by Lavrenko et al. [12] has adapted the relevance model in two different ways based on KL-divergence retrieval models to perform cross-lingual information retrieval. The cluster-based query likelihood proposed in [10] can be regarded as a form of a translation model where the whole document is translated into the query. Recently, translation models have been applied in many applications including question answering, sentence retrieval and tracking information flow [18, 19, 40]. For example, Xue et al [40] has applied translation model on question-answer archives where question and answer pairs are used to train the translation model. In Contrary to all these works, we studied statistical translation model in ad hoc retrieval context.
Vocabulary gap has also been studied in the past. Many studies have tried to bridge the vocabulary gap between documents and queries both based on co-occurrence thesaurus [1, 9, 14, 21, 24, 31, 32, 38] and hand-crafted thesaurus [15, 35]. Some other works have considered to combine both approaches [4, 17]. In this paper, we considered word co-occurrence relationship based on mutual information and incorporated it into translation language model in a more principled way.
6. CONCLUSIONS AND FUTURE WORK
As a principled approach to capturing semantic relation of words in information retrieval, statistical translation models have been shown to outperform simple language models which rely on exact matching of words in the query and documents. In this paper, we propose a new simple way to estimate translation probabilities based on mutual information. Our experiment results indicate that the proposed mutual information estimation method is both more efficient and more effective than the existing synthetic query estimation method. We also proposed to regularize translation probability to ensure sufficient self-translation probability mass, which has been shown to be effective for both estimation methods we experimented with. Our results also show that the translation language model is not so sensitive to the effect of smoothing, and it can be combined with pseudorelevance feedback to further improve the performance.

329

For future, it would be interesting to propose some other efficient estimation methods. It would also be interesting to explore other ways of incorporating the translation probabilities into the retrieval formula. Another interesting direction is to study how to transfer the knowledge learned from one collection to another collection.
7. ACKNOWLEDGMENTS
We thank the anonymous reviewers for their useful comments. This material is based upon work supported by a Sohaib and Sara Abbasi Fellowship, Yahoo! Key Scientific Challenge Award, the National Science Foundation under Grant Numbers IIS-0347933, IIS-0713581, IIS-0713571, and CNS-0834709, and by NIH/NLM grant 1 R01 LM009153-01. Any opinions, findings, conclusions, or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsors.
8. REFERENCES
[1] J. Bai, D. Song, P. Bruza, J. Y. Nie, and G. Cao. Query expansion using term relationships in language models for information retrieval. ACM CIKM, pages 688­695, 2005.
[2] A. Berger and J. Lafferty. Information retrieval as statistical translation. ACM SIGIR, pages 222­229, 1999.
[3] P. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263­311, 1993.
[4] G. Cao, J. Y. Nie, and J. Bai. Integrating word relationships into language models. ACM SIGIR, pages 298­305, 2005.
[5] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. ACM SIGKDD, 39(B):1­38, 1997.
[6] N. Fuhr. Probabilistic models in information retrieval. The Computer Journal, 35(3):243­255, 1992.
[7] F. Jelinek. Statistical Methods for speech recognition. MIT Press., 1997.
[8] R. Jin, A. G. Hauptmann, and C. X. Zhai. Title language model for information retrieval. In ACM SIGIR, pages 42­48, 2002.
[9] Y. Jing and B. Croft. An association thesaurus for information retrieval. RIAO, pages 141­160, 1994.
[10] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. ACM SIGIR, pages 194­201, 2004.
[11] J. Lafferty and C. Zhai. Document language models, query models and risk minimization for information retrieval. ACM SIGIR, pages 111­119, 2001.
[12] V. Lavrenko, M. Choquette, and B. Croft. Cross-lingual relevance models. ACM SIGIR, pages 175­182, 2002.
[13] V. Lavrenko and B. Croft. Relevance-based language models. ACM SIGIR, pages 120­127, 2001.
[14] M. Lesk and B. Croft. Word-word associations in document retrieval systems. American Documentation, 20:20­27, 1969.
[15] S. Liu, F. Lin, C. Yu, and W. Meng. An effective approach to document retrieval via utilizing wordnet and recognizing phrases. ACM SIGIR, pages 266­272, 2004.
[16] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In ACM SIGIR, pages 186­193, 2004.
[17] R. Mandala, T. tokunaga, H. Tanaka, and K. Satoh. Ad hoc retrieval experiments using wordnet and automatically constructed thesauri. TREC-7, pages 475­481, 1998.
[18] D. Metzler, Y. Bernstein, B. Croft, A. Moffat, and J. Zobel. Similarity measures for tracking information flow. ACM CIKM, pages 517­524, 2005.

[19] V. Murdock and B. Croft. Simple translation models for sentence retrieval in factoid question answering. ACM SIGIR, pages 31­35, 2004.
[20] J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web. In ACM SIGIR, pages 74­81, 1999.
[21] H. J. Peat and P. Willett. The limitations of term co-occurrence data for query expansion in document retrieval systems. J. of Information science, 42(5):378­383, 1991.
[22] J. Ponte and W. B. Croft. A language modeling approach to information retrieval. ACM SIGIR, pages 275­281, 1998.
[23] M. Porter. An algorithm for suffix stripping. Program, 14(3), 1980.
[24] Y. Qiu and H. Frei. Concept based query expansion. ACM SIGIR, pages 160­169, 1993.
[25] C. J. V. Rijbergen. A theoretical basis for the use of co-occurrence data in information retrieval. Journal of Documentation, pages 106­119, 1977.
[26] C. J. V. Rijsbergen. Information retrieval. Butterworths, 1979.
[27] S. Robertson and K. Sparck. Relevance weighting of search terms. Journal of American Society for Information Science, 27:129­146, 1976.
[28] G. Salton. Automatic Text Processing: The Transformation, Analysis and Retrieval of Information by Computer. Addison-Wesley, 1989.
[29] G. Salton and M. McGill. Introduction to Modern Information Retrieval. McGraw-Hill., 1983.
[30] G. Salton, C. S. Yang, and C. T. Yu. A theory of term importance in automatic text analysis. Journal of American Society for Information Science, 26(1):33­44, 1975.
[31] H. Schutze and J. O. Pedersen. A co-occurrence based thesaurus and two applications to information retrieval. Information and processing management, 33(3):307­318, 1997.
[32] A. F. Smeaton and C. J. V. Rijsbergen. The retrieval effects of query expansion on a feedback document retrieval system. The Computer Journal, 26(3):239­246, 1983.
[33] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information retrieval with document expansion. In HLT-NAACL, pages 407­ 414, 2006.
[34] H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Transactions on Information Systems, 9(3):187­222, 1991.
[35] E. M. Voorhess. Query expansion using lexical-semantic relations. ACM SIGIR, pages 61­69, 1994.
[36] X. Wei and W. B. Croft. Lda-based document models for ad-hoc retrieval. In ACM SIGIR, pages 178­185, 2006.
[37] F. Wilcoxon. Individual comparisons by ranking methods. Biometrics, 1:80­83, 1945.
[38] J. Xu and B. Croft. Query expansion using local and global document analysis. ACM SIGIR, pages 4­11, 1996.
[39] J. Xu, R. Weischedel, and C. Nguyen. Evaluating a probabilistic model for cross-lingual information retrieval. ACM SIGIR, pages 105­110, 2001.
[40] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for question and answer archives. In ACM SIGIR, pages 475­482, 2008.
[41] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. ACM CIKM, pages 403­410, 2001.
[42] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. ACM SIGIR, pages 334­342, 2001.

330

Estimating Probabilities for Effective Data Fusion

David Lillis
School of Computer Science and Informatics
University College Dublin
david.lillis@ucd.ie
Rem W. Collier
School of Computer Science and Informatics
University College Dublin
rem.collier@ucd.ie

Lusheng Zhang
School of Computer Science and Informatics
University College Dublin
lu-sheng.zhang @ucdconnect.ie
David Leonard
School of Computer Science and Informatics
University College Dublin
david.leonard@ucd.ie

Fergus Toolan
School of Computer Science and Informatics
University College Dublin
fergus.toolan@ucd.ie
John Dunnion
School of Computer Science and Informatics
University College Dublin
john.dunnion@ucd.ie

ABSTRACT
Data Fusion is the combination of a number of independent search results, relating to the same document collection, into a single result to be presented to the user. A number of probabilistic data fusion models have been shown to be effective in empirical studies. These typically attempt to estimate the probability that particular documents will be relevant, based on training data. However, little attempt has been made to gauge how the accuracy of these estimations affect fusion performance. The focus of this paper is twofold: firstly, that accurate estimation of the probability of relevance results in effective data fusion; and secondly, that an effective approximation of this probability can be made based on less training data that has previously been employed. This is based on the observation that the distribution of relevant documents follows a similar pattern in most high-quality result sets. Curve fitting suggests that this can be modelled by a simple function that is less complex than other models that have been proposed. The use of existing IR evaluation metrics is proposed as a substitution for probability calculations. Mean Average Precision is used to demonstrate the effectiveness of this approach, with evaluation results demonstrating competitive performance when compared with related algorithms with more onerous requirements for training data.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Experimentation
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Keywords
information retrieval, probabilistic data fusion, results merging
1. INTRODUCTION
In the context of Information Retrieval (IR), many researchers have attempted to use data fusion to improve the quality of their results. This involves submitting a query to a number of distinct IR systems (known as "input systems", as they provide the inputs to the fusion process) that have access to the same document collection, and subsequently merging their outputs into a single result set to be presented to the user. This is related to, but distinct from, the concept of meta-search (or collection fusion), where the results being merged are from IR systems operating with disjoint (or partially overlapping) document collections [18].
Many techniques to tackle the data fusion task are available that use only the result sets that are actually being fused. These approaches vary from purely rank-based algorithms such as interleaving [18] to score-based techniques such as linear combinations [3, 14, 17] and the popular CombSum and CombMNZ algorithms [5, 6, 15]. Algorithms based on voting have also been popular [1, 13].
More recently, attempts have been made to take into account the past performance of input systems when performing fusion [1, 8, 10, 16]. These techniques make use of probabilities to calculate a score on which the final, fused result set will be ranked. Many, however, require detailed training data to be available, from which the probabilities are calculated. Typically, a number of training queries are run, with each of the input systems required to provide results for each. These results are then compared with relevance judgements so as to identify the positions in each result set where relevant documents have been returned. From this data, a model can be built up that predicts the probability of particular documents being relevant, based on the system that returned them and the positions in the respective result sets they occupy.
This position-level granularity of training data is an onerous requirement to have on a fusion process. Probabilistic data fusion with a minimal requirement for training would be preferable. The aim of this paper is to attempt to perform fusion based on probability of relevance, but without such

347

a reliance on detailed training data. In order to do this, we must firstly demonstrate that an accurate probability model is indeed beneficial to fusion performance. Following this we attempt to show that this probability of relevance can be approximated by a function of a documents position within a result set. We also outline one candidate function to achieve effective fusion.
This paper is organised as follows: Firstly, Section 2 outlines a number of considerations that must be taken into account when developing a data fusion solution. Section 3 motivates the work by considering some pre-existing probabilistic fusion models and examines how an accuratelyconstructed probability model can result in effective data fusion. Following from this, Section 4 shows how such a probability model can be estimated by reference to a singlevalue measure of the quality of the inputs to the fusion process. Having chosen the Mean Average Precision evaluation metric as this single-value measure, a set of experiments is outlined in Section 5 that demonstrates the effectiveness of our approach when compared with others. Finally, we outline our conclusions and present some ideas for future work in Section 6.
2. CHARACTERISTICS OF DATA FUSION
When performing effective data fusion, there are a number of "effects" that may be taken into account. These were initially outlined by Vogt and Cottrell in [17].
· The Skimming Effect is based on the observation that relevant documents are more likely to appear at the top of result sets (where an IR system would place those documents it estimates to be most relevant). Thus favouring early-ranked documents when compiling the final result set can result in improved fusion performance.
· The Chorus Effect argues that if multiple input systems agree on the relevance of a document (by including it in each of their result sets) then this is increased evidence of relevance. This is also consistent with Lee's observation that IR systems tend to return the same relevant documents but different nonrelevant ones [7]. Fusion algorithms that attach greater importance to documents that are returned by multiple input systems attempt to exploit this effect.
· The Dark Horse Effect refers to a situation where an input system returns an unusually high- or low-quality result set. In this situation, if a fusion technique was able to identify a "dark horse", it may opt to return only the result set of that input system, rather than performing any fusion. This effect is very difficult to detect and we are not aware of any techniques that attempt to make use of it.
3. PROBABILITY AS A STRATEGY FOR DATA FUSION
A number of data fusion algorithms have been proposed that use the probability of relevance as a method of assigning scores to documents. Aslam and Montague make use of a Bayesian model that uses both the probability of relevance and the probability of non-relevance to rank documents [1]. The probabilities are calculated by examining the precision

at a number of document levels. Result sets are divided into ranges between these document levels, with appropriate probability values being associated with each range. Manmatha et al. infer probabilities from the ranking scores given to documents by the various input systems [11].
Another group of probability-based fusion algorithms use training data to calculate a set of probabilities for each of the systems providing result sets to be fused. In this context, training data consists of result sets produced by the same input systems in response to queries for which relevance judgements are available. Having analysed where relevant documents tend to be returned by each inputs system, a probability model is built. For each of the input systems, this model maps a probability score on to each position in which a document may potentially be returned. For instance, System A may have a probability of 0.4 associated with position 1. This would imply that for any given document returned by System A at the top of its result set, there is an estimated probability of 0.4 that the document is relevant. Algorithms utilising this type of probability model include Lillis et al.'s ProbFuse [8] and SlideFuse [10] and the SegFuse algorithm developed by Shokouhi [16].
This approach to data fusion relies on two fundamental assumptions. Firstly, it is assumed that a system's performance in response to training queries is indicative of how it will perform when faced with different queries. It is also assumed that the construction of an accurate probability model will result in effective fusion. Although the empirical experiments presented in [9, 10, 16] demonstrate effective retrieval performance when compared against the baseline CombMNZ, the accuracy of the probability model is not tested.
The aim of this paper is to examine this second assumption in more detail. Establishing that an accurate model of the probabilities required results in effective fusion further motivates the examination of further methods of constructing such models.
In order to test this, we evaluated the effectiveness of using a perfect probability model for fusion. This perfect probability model was constructed by using the same queries (and consequently the same result sets) for training as for fusion. The consequence of this is that the probability model perfectly reflects the positions of the relevant documents in the result sets being used for fusion. Clearly, such an approach is not feasible from a practical point of view, as the relevant documents are not known at query time. However, the aim of this experiment is to demonstrate how effective fusion would be if an accurate approximation of the real probability distribution could be constructed.
The inputs for this experiment were taken from the TREC 2004 Web Track [4]. Five fusion runs were performed, using six input systems each time. The systems were chosen by their overall MAP score, with the six best systems being part of run1, the seventh to twelfth best systems in run2 etc. These inputs consisted of result sets relating to 225 distinct topics (queries).
The specific inputs used for each run are the same for all experiments presented in this paper, and are as follows:
· run1: MSRC04B2S, MSRC04C12, MSRC04B1S, MSRAx4, MSRAx2,MSRAmixed1
· run2: MSRAmixed3, MSRC04B1S2, MSRAx5, UAmsT04MSind, UAmsT04MWScb, UAmsT04MSinu

348

· run3: UAmsT04MWinu, uogWebSelAn, uogWebSelAnL, MSRC04B3S, THUIRmix045, THUIRmix041
· run4: uogWebCA, ICT04MNZ3, THUIRmix043, ICT04CIIS1AT, ICT04RULE, THUIRmix042
· run5: ICT04basic, ICT04CIILC, MeijiHILw1, uogWebSelL, UAmsT04LnuNG, MeijiHILw3
By way of comparison, the result sets were fused using the SlideFuse and CombMNZ fusion algorithms, which are described in detail in [10] and [5] respectively. SlideFuse is a probabilistic data fusion algorithm that estimates the probability of relevance at each position using training queries. It is chosen as a representative from the family of probabilistic algorithms to which it belongs (also including ProbFuse [9] and SegFuse [16]). In order to compensate for incomplete relevance judgements, where judgements of relevance or nonrelevance are not available for every document in the collection, SlideFuse smooths these probabilities using a sliding window approach. This means that the probabilities associated with each position also depends on the occurrence of relevant documents in neighbouring positions. In contrast, CombMNZ is a much simpler algorithm and has been chosen because it is frequently used as a baseline in fusion experiments. This does not use any training data, but rather uses the scores given to each document by the input systems to rank the fused result set. The details of how these were implemented are given in the following subsections.
3.1 PosFuse
The approach based on the perfect probability model is described here as "PosFuse" (as it is based on the probability at the position in which a document appears). Like SlideFuse, it is calculated in two stages: a training phase and a fusion phase.
In the training phase, P (dp|s) is calculated. This is the probability that a document d returned in position p of a result set is relevant, given that is has been returned by input system s. It is calculated by

P (dp|s) =

qQp Rdp,q Qp

(1)

where Qp is the set of all training queries for which at least p documents were returned by the input system and Rdp,q is the relevance of the document dp to query q (1 if the document is relevant, 0 if not). This is calculated for
each input system to be used in the fusion phase. Following this, the fusion stage requires that a ranking
score be assigned to each document (Rd). This is given by

Rd = P (dp|s)

(2)

sS

where S is the set of all input systems used and p is the position in which document d was returned by input system s. Although the use of probabilities would suggest that multiplication would be an obvious operator to use, the nature of data fusion makes addition more useful in this scenario. Adding the probability scores together results in a document's ranking score receiving a boost for every result set in which it appears (thus leveraging the Chorus Effect). Rd

is intended as a score on which to rank documents, rather than an accurate estimation of the probability of a document's relevance.
3.2 SlideFuse
SlideFuse is a probabilistic fusion algorithm that is also based on the probability of relevance in various positions in result sets [10]. For SlideFuse, this probability calculation is the same as described above in Equation 1.
However, SlideFuse does not use this probability alone in order to calculate scores. It also employs a smoothing of these probabilities based on the notion of a sliding window. The argument in favour of this smoothing is that in certain situations, some positions may be ultimately given a probability of zero. This occurs whenever no relevant documents are returned by an input system at that exact position during the training phase. There are two principal reasons why this may happen:
1. Few Training Queries: If the number of queries being used for training is very small, this reduces the overall number of relevant documents being returned by each input system. Because of this, it consequently increases the chance that a particular position may not contain a relevant document for any of the training queries.
2. Incomplete Relevance Judgements: When relevance judgements are "incomplete", not all documents have been judged for relevance to all the queries. This means that there three types of document: relevant, nonrelevant and unjudged. The lack of judged relevant documents appearing at any position may merely be as a result of documents being unjudged.
Whatever the reason, a probability of zero is undesirable. Firstly, it runs contrary to the Chorus Effect to neglect to take into account that a document was actually returned by an input system, regardless of its position. Secondly, it is counter-intuitive to give any document that was returned in a result set the same treatment as one that was not returned at all.
The sliding window is designed to reduce the likelihood of zero probabilities by also taking into account neighbouring positions. The start and end points (a and b respectively) of the sliding window surrounding each result set position p are given by

p-w

p - w >= 0

a=

(3)

0

p-w <0

p+w

p+w < N

b=

(4)

N -1

p + w >= N

where w is a parameter that indicates how many positions on either side of p should be included in the window and N is the total number of documents in the result set. In effect, the above definitions of a and b ensure that the window cannot begin before the first document in the result set and also cannot extend beyond the last document.
Once the boundaries of the window have been set, a probability must be associated with each. P (dp,w|s), the probability of relevance of document d in position p using a window

349

size of w documents either side of p, given that it has been returned by input system s is given by

P (dp,w|s) =

b i=a

P

(di

|s)

b-a+1

(5)

Finally, a ranking score is given to each document using a formula very similar to Equation 2, except that the probability associated with the window is used instead of the probability at a particular rank.

Rd = P (dp,w|s)

(6)

sS

3.3 CombMNZ
Although it is not a probabilistic model, we also include the CombMNZ fusion algorithm, as it has become a standard baseline against which other fusion algorithms are compared [2, 13, 19]. Originally proposed by Fox and Shaw in [5], CombMNZ is a score-based algorithm that does not rely on training. It has gained popularity as a baseline measure principally because it is easily implemented and its retrieval performance tends to be very strong, despite its simplicity. Our implementation of CombMNZ follows that of Lee [7], who carried out a number or experiments using a variety of techniques proposed by Fox and Shaw.
CombMNZ is run in two phases. Unlike PosFuse and SlideFuse, both of these are done at fusion time, with no training required. Because CombMNZ is based on the scores attributed to each document by each of the input systems, the first requirement is that these be normalised. This is intended to scale all of the scores into the same range, so as to avoid a situation where one input system attaches greater weight to documents merely because it calculates scores from 0 to 100 rather than from 0 to 1.
The normalisation formula used by Lee is known as "standard normalisation" [12] and is given by

normalised

sim

=

unnormalised sim - max sim - min

min sim

sim

(7)

where max sim and min sim are the maximum and minimum scores that are actually seen in the input result set. Once the scores have been normalised, CombM N Zd, the CombMNZ ranking score for any document d is given by

S

CombM N Zd = Ns,d × |Nd > 0|

(8)

s=1

where S is the number of result sets to be fused, Ns,d is the normalised score of document d in result set s and |Nd > 0| is the number of non-zero normalised scores given to d by any result set.

3.4 Initial Results
Table 1 shows the MAP score for a number of data fusion algorithms. For comparison purposes, the column labelled "MaxMAP" shows the highest overall MAP score achieved by any individual input. It can be argued that this is the baseline that all fusion algorithms should aim to beat. If a fusion algorithm cannot achieve this level of performance, then a superior approach would simply be to identify which of the input systems performs best, discarding the others.

In this table, the highest MAP score amongst the fusion algorithms is shown in bold.

MaxMAP PosFuse SlideFuse CombMNZ

run1 run2 run3 run4 run5

0.5389 0.5120 0.4589 0.4325 0.3976

0.5751 0.5679 0.5375 0.4791 0.4907

0.5697 0.5651 0.5223 0.4628 0.4640

0.3317 0.5249 0.1862 0.1740 0.4203

Table 1: MAP Scores when training on the actual result sets being fused. The highest MAP score for a fusion technique on each run is in bold.

From Table 1 it can be seen that the highest MAP scores are achieved on all runs by PosFuse. Additionally, these MAP scores are greater than the best performing individual input for each run. This is an interesting result in that it adds motivation to the pursuit of a probability distribution for the purposes of fusion.
Another interesting observation is that the PosFuse technique was able to achieve marginally greater MAP scores than SlideFuse. SlideFuse is based on probabilities that are initially calculated in the same way as for PosFuse, with the addition of the smoothing that is performed by the sliding window. It is, however, important to note that the principal motivation behind the use of sliding windows is to cater for situations where a small quantity of training queries combined with incomplete relevance judgements may cause some positions to be attributed a probability of zero. As the results shown in Table 1 are for fusion runs consisting of 225 training queries, this kind of situation does not arise to the same extent and so the motivation for this is lost. The performance of SlideFuse is still greater than the maximum individual MAP score, however.

4. MODELLING PROBABILITY
Having demonstrated the effectiveness of using accurate probability figures, we now investigate how this may be modelled, preferably without the necessity for large quantities of training data.

4.1 Curve Fitting

To do this, the probability distributions calculated for the

225-query run outlined in Section 3 were analysed. For each

distribution (each one related to one input system), a curve was fitted using the gnuplot graphing utility 1. In each case,

the probability of relevance was plotted on the y-axis, with

the result set position (starting at 1 for the top position in

the result set) on the x-axis. In each case, gnuplot fit a curve

of

the

form

y

=

a x

,

meaning

that

the

probability

of

relevance

would become a function of the result set position.

Figure 1 illustrates the process of fitting a curve for the

MSRC04B32S input system. The first graph in that figure

shows the results of fitting a curve only to the first 100 posi-

tions in each result set, whereas the second uses the entirety

of each result set (TREC results are truncated to at most

1http://gnuplot.info

350

1000 documents in each result set). The fitted a-value is shown at the top-right of each graph. Despite the large difference in the lengths of the result sets being used, there is less than a 1% difference between the a-values generated.

Figure 1: Curves fit to probability distribution for the MSRC04B2S input, showing 100 positions and 1000 positions respectively

Using the latter a-value (relating to 1000-document result sets), this leads to a modification of Equation 1 for calculating the probability of relevance. For the specific result sets used, the probability that a document d returned in position p in a result set created by the MSRC04B25 input system is represented by P (dp|M SRC04B25). Its value is given by

P (dp|M SRC04B25)

=

0.79929 p

(9)

This is shown for illustrative purposes: similar fitting was done for all of the other input systems available, with a variety of a-values being generated. Although interesting that such a function can be generated for a range of input systems, to do so requires even more training effort than what was needed for the PosFuse algorithm used in Section 3. In addition to the training data necessary to calculate the probability of relevance at each position, the curve fitting would also have to be performed.

The shape of the fitted curves is interesting in that it supports the reasoning behind the description of the Skimming Effect. The graph shown in Figure 1 shows that documents ranked in early positions in result sets are much more likely to be relevant than those further down the result set. It also supports the idea that probability scores (or approximations thereof) can be effectively used in the calculation of fusion scores.
4.2 Evalation of curve fitted approach
To gauge how effective this is in terms of fusion performance, a comparison is made with the results obtained for the experiment outlined in Section 3. Table 3 reproduces the figures shown in Table 1, with the addition of an extra column (marked "FitFuse"), which is based on the fitted curves.
For FitFuse, rather than using the probabilities of relevance calculated on a per-position basis, we use the formula described in Equation 9. The fitted a-values used for each input system is given in Table 2.

Input
run1 MSRAmixed1 MSRAx4 MSRC04B2S MSRAx2 MSRC04B1S MSRC04C12 run2 MSRAmixed3 MSRC04B1S2 UAmsT04MSinu MSRAx5 UAmsT04MSind UAmsTo4MWScb run3 MSRC04B3S THUIRmix041 THUIRmix045 UAmsT04MWinu uogWebSelAn uogWebSelAnL run4 ICT04CIIS1AT ICT04MNZ3 ICT04RULE THUIRmix042 THUIRmix043 uogWebCA run5 ICT04basic ICT04CIILC MeijiHILw1 MeijiHILw3 UAmsT04LnuNG uogWebSelL

a-value
0.806350 0.803393 0.799290 0.798310 0.786756 0.802797
0.774764 0.701004 0.469070 0.787904 0.454965 0.466055
0.649945 0.641529 0.661245 0.458516 0.469182 0.451617
0.685360 0.690526 0.653053 0.649099 0.638313 0.401074
0.643282 0.660100 0.374485 0.371387 0.670690 0.424656

Table 2: a-values used for FitFuse the various input systems.

The results shown in Table 3 are promising. Having moved away from the perfectly accurate probability distribution

351

MaxMAP FitFuse PosFuse SlideFuse CombMNZ

run1 run2 run3 run4 run5

0.5389 0.5120 0.4589 0.4325 0.3976

0.5773 0.5640 0.5140 0.4704 0.4724

0.5751 0.5679 0.5375 0.4791 0.4907

0.5697 0.5651 0.5223 0.4628 0.4640

0.3317 0.5249 0.1862 0.1740 0.4203

Table 3: MAP Scores when training on the actual result sets being fused. The highest MAP score for a fusion technique on each run is in bold.

used in PosFuse, FitFuse shows only a slight disimprovement in MAP score, despite it being merely an estimate of the probabilities involved. On the first run, it actually gains a marginally higher MAP score than PosFuse, which indicates that although estimating probability may not be expected to achieve the same quality results as a perfectlymodelled probability distribution, this is not necessarily the case.

4.3 Towards single-value training

Because of the onerous training needs, we are interested

in finding other values that can be substituted for a fitted

a-value in

a

y

=

a x

style

probability model.

In order for

a candidate value to be suitable for use in this way, it is

required to satisfy three criteria:

· Correlation: It must be shown to correlate to the fitted a-values.

· Training: It should require less exhaustive training calculations than methods such as PosFuse and SlideFuse.

· Results: It must be competitive in terms of the evaluation of fusion performance.

The first of these criteria is particularly important for selecting what value to use. Logically, a high a-value indicates that an input system is more likely to return relevant documents than one with a lower a-value. Clearly, this a-value is linked to the overall performance of an input system. As such, this motivates the use of established IR evaluation metrics for fusion.
Evaluation metrics have been a key focus of IR research for many years. Each is designed to measure the quality of a result set in some way, with different metrics having their own emphasis, strengths and weaknesses. The metric selected for investigation in this work is the widely-used Mean Average Precision (MAP) metric.
Figure 2 shows a graph of the fitted a-values (on the xaxis) plotted against MAP (on the y-axis). Although the two values are not shown to be directly proportional, a clear upward trend can be seen. Input systems with higher fitted a-values tend to also have higher MAP scores. Intuition would dictate that this is not a surprising result: a system with a greater tendency to return relevant documents would typically achieve a higher MAP score on evaluation (although the position of the relevant documents is also important).
A curve can be fitted for this graph also (and is indicated by the straight line in Figure 2). However, it important to

Figure 2: Correlation between MAP score and fitted a-value
bear in mind that a direct mapping from a MAP score to an a-value is not necessarily required. The aim of the fusion task is to rank the documents, rather than calculate an accurate a-value. Given the formula used below in Section 5, it can be shown that multiplying the MAP scores by a constant to better approximate a-values does not affect the ranking of the documents in the fused result set.
The second criterion required above is that the amount of training effort should be less than that of alternative techniques such as SlideFuse. SlideFuse requires knowledge of the exact positions in which relevant documents were returned in response to training queries. In contrast the proposed approach requires only a single-value estimation of the quality of the input system.
Finally, it is required to evaluate the effectiveness of using a function of MAP score and document position as a fusion strategy.
5. EVALUATION
In order to evaluate the effectiveness of the approach outlined above, it is necessary to carry out a number of experiments. These experiments involved running this new technique (which we shall call "MAPFuse") alongside a number of alternatives.
5.1 Experiment Setup
As with the initial experiments outlined in Section 3, five separate runs were performed. These use the same inputs as in the earlier experiments. Each input was divided into a set of training result sets and a set of fusion result sets. For this, an 20%/80% split was used (i.e. 45 training queries and 180 queries used for fusion).
Dividing query sets in this way alone may cause unrepresentative results being obtained. For instance, early queries (i.e. those used for training) may be disproportionately straightforward (or indeed difficult) when compared with those used for fusion. This may mean that differences between fusion techniques' performance may be a consequence of the training data rather than the algorithms themselves.
For this reason, each of the fusion runs was performed five separate times, with the queries being shuffled into a

352

MaxMAP MAPFuse PosFuse SlideFuse CombMNZ

different randomised order before each time. Thus the set of training queries was different each time. The evaluation results reported here for each run are the average of each of these five sets of shuffled inputs.
The baseline MAP score for each run is that achieved by the best-performing individual input system (denoted by MaxMAP). Training queries are ignored in this calculation, so the figures presented relate to the same query set for each technique.
Three data fusion algorithms were chosen for comparison. SlideFuse and CombMNZ are implemented as described in [10] and [5], respectively. Training queries are also ignored for CombMNZ, since that algorithm does not require a training phase. The implementation of PosFuse is as described in Section 3, with the exception that the probabilities are calculated on the training queries and then used to fuse the fusion queries at a later stage, rather than being calculated on the same result sets that are to be fused.

5.2 Defining MAPFuse

For the MAPFuse fusion algorithm, the training phase requires only that the MAP score for each input system on the training queries be calculated. This is performed by trec eval, which is a tool provided by TREC to calculate evaluation metrics for IR systems. Unlike the proofof-concept results shown in Section 3, relevance information for the actual result sets being fused is not required, as the MAP scores used for fusion are calculated using only the training queries.
Once the relevant MAP scores have been calculated, they are used in the fusion phase to calculate the scores on which the documents are ranked in the final, fused result set.
The ranking score Rd attributed to document d is given by

Rd

=

sS

M APs ps(d)

(10)

where S is the set of the input systems that returned document d somewhere in their result sets, M APs is the MAP score associated with system s and ps(d) is the position in which document d was ranked by system s.
The fact that the MAP score is divided by a document's position helps to leverage the Skimming Effect, whereas the fact that the scores are added to give the document's final ranking score boosts documents that have appeared in multiple result sets and so makes use of the Chorus Effect.

5.3 Results
The results of running these experiments are presented in Table 4. Values in bold face are the highest score achieved by a fusion algorithm on a particular run. Asterisks are used to indicate a statistically significant difference to the performance of MAPFuse when measured using the t-test.
With the exception of CombMNZ, each of the fusion algorithms achieves a higher MAP score than that of the best individual input system (again shown as "MaxMAP"). MAPFuse achieves comparable results to PosFuse and SlideFuse, with the highest MAP score for three of the five runs. It also shows a statistically significant improvement in MAP score over MaxMAP and CombMNZ on all runs. As an aside, it is of note that, unlike the situation in Section 3, the scores achieved by SlideFuse are consistently higher than those of PosFuse (with the sole exception of run5). This may pos-

run1 run2 run3 run4 run5

0.5468* 0.5120* 0.4555* 0.4357* 0.4084*

0.5767 0.5693 0.5132 0.4711 0.4858

0.5591* 0.5682* 0.5045*
0.4591 0.4777*

0.5728 0.5718 0.5171 0.4665 0.4681*

0.3361* 0.5404* 0.1842* 0.1785* 0.4277*

Table 4: MAP Scores From Fusion Runs. The highest MAP score for a fusion technique on each run is in bold. Asterisks indicate a statistically significant difference when compared to MAPFuse using the t-test.

sibly be explained by the lower quantity of training queries being used, with the sliding window beginning to show its advantages over the strictly position-based PosFuse.
The difference between the MAP scores of the three probabilistic techniques is quite small (MAPFuse's score is never more than 3% higher than PosFuse or 4% higher than that of SlideFuse). It is notable that despite this, the difference between MAPFuse and PosFuse is statistically significant in 4 of the 5 runs. Another observation is that for the two runs in which SlideFuse achieves a higher MAP score than MAPFuse, this difference is not statistically significant.
Despite these observations, the aim of the experiment is not necessarily to achieve significantly higher MAP scores. According to the third success criterion in Section 4, we merely require comparable performance with competing techniques. The principal advantage is that comparable retrieval results can be achieved by using only a single figure to represent the effectiveness of an underlying input system, rather than the detailed information about relevant documents' positions that is required by the other algorithms.

6. CONCLUSIONS AND FUTURE WORK

In this paper, we have examined the use of the probabil-

ity of relevance in performing data fusion. In this context,

we use "probability of relevance" to mean the probability

that a document returned by a particular input system in a

particular position in its result set is relevant.

Initially we showed that if a fully accurate model of the

probability of relevance at each position is available, positive

fusion results can be achieved using these probabilities to

calculate the ranking scores for documents. Following from

this, we have shown that these probabilities can be modelled

by

a

function

of

the

form

y

=

a x

.

Using the MAP score

of each input system on a number of training queries as

an substitute for a, we have shown that comparable MAP

scores to alternative fusion algorithms can be achieved.

The benefits of this approach are principally in the level

of training data that is required. Whereas algorithms like

SlideFuse required detailed training data on the specific lo-

cation of relevant documents within result sets, MAPFuse

requires only a single summary metric to represent the qual-

ity of each input system being used.

For the purposes of this paper, the common Mean Average

Precision (MAP) evaluation metric was used as the single-

value substitution for a. However, a range of alternative

353

metrics are available and so future work will concentrate on evaluating the impact of using alternative metrics.
Additionally, a more exhaustive study on a greater number of document collections will be necessary to demonstrate the wider applicability of this work. Such a study would also include a separation of the training and fusion phases so that each is carried out on a different document collection (although the retrieval systems generating each result set would not change). This would be an important stage in demonstrating that this type of fusion could be employed in a real-world information retrieval system.
7. ACKNOWLEDGEMENTS
This material is based upon works supported by the Science Foundation Ireland under Grant No. 08/RFP/CMS1183.
8. REFERENCES
[1] J. A. Aslam and M. Montague. Models for metasearch. In SIGIR '01: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 276­284, New York, NY, USA, 2001.
[2] S. M. Beitzel, E. C. Jensen, A. Chowdhury, D. Grossman, O. Frieder, and N. Goharian. Fusion of effective retrieval strategies in the same information retrieval system. J. Am. Soc. Inf. Sci. Technol., 55:859­868, 2004.
[3] J. P. Callan, Z. Lu, and W. B. Croft. Searching distributed collections with inference networks. In SIGIR '95: Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pages 21­28, New York, NY, USA, 1995.
[4] N. Craswell and D. Hawking. Overview of the TREC-2004 web track. In Proceedings of the Thirteenth Text REtrieval Conference (TREC-2004), 2004.
[5] E. A. Fox and J. A. Shaw. Combination of Multiple Searches. In Proceedings of the 2nd Text REtrieval Conference (TREC-2), National Institute of Standards and Technology Special Publication 500-215, pages 243­252, 1994.
[6] A. E. Howe and D. Dreilinger. SavvySearch: A Metasearch Engine That Learns Which Search Engines to Query. AI Magazine, 18:19­25, 1997.
[7] J. H. Lee. Analyses of multiple evidence combination. SIGIR Forum, 31:267­276, 1997.
[8] D. Lillis. ProbFuse: Probabilistic Data Fusion. Msc, University College Dublin, UCD, February 2006.

[9] D. Lillis, F. Toolan, R. Collier, and J. Dunnion. ProbFuse: A Probabilistic Approach to Data Fusion. In Proceedings of the 29th annual international ACM SIGIR Conference on Research and Development in information retrieval, pages 139­146, New York, USA, 2006.
[10] D. Lillis, F. Toolan, R. Collier, and J. Dunnion. Extending Probabilistic Data Fusion Using Sliding Windows. In Proceedings of the 30th European Conference on Information Retrieval (ECIR '08), volume 4956 of Lecture Notes in Computer Science, pages 358­369, Berlin, 2008. Springer.
[11] R. Manmatha, T. Rath, and F. Feng. Modeling score distributions for combining the outputs of search engines. In SIGIR '01: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 267­275, New York, NY, USA, 2001.
[12] M. Montague and J. A. Aslam. Relevance score normalization for metasearch. In CIKM '01: Proceedings of the Tenth International Conference on Information and Knowledge Management, pages 427­433, New York, NY, USA, 2001.
[13] M. Montague and J. A. Aslam. Condorcet fusion for improved retrieval. In CIKM '02: Proceedings of the eleventh international conference on Information and knowledge management, pages 538­548, New York, NY, USA, 2002.
[14] A. L. Powell, J. C. French, J. Callan, M. Connell, and C. L. Viles. The impact of database selection on distributed searching. In SIGIR '00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 232­239, New York, NY, USA, 2000.
[15] E. Selberg and O. Etzioni. The MetaCrawler Architecture for Resource Aggregation on the Web. IEEE Expert, pages 11­14, 1997.
[16] M. Shokouhi. Segmentation of Search Engine Results for Effective Data-Fusion. Advances in Information Retrieval, 4425, April 2007.
[17] C. C. Vogt and G. W. Cottrell. Fusion Via a Linear Combination of Scores. Information Retrieval, 1:151­173, 1999.
[18] E. M. Voorhees, N. K. Gupta, and B. Johnson-Laird. The Collection Fusion Problem. In Proceedings of the Third Text REtrieval Conference (TREC-3), pages 95­104, 1994.
[19] S. Wu and F. Crestani. Data fusion with estimated weights. In CIKM '02: Proceedings of the eleventh international conference on Information and knowledge management, pages 648­651, New York, NY, USA, 2002.

354

Interactive Retrieval Based on Faceted Feedback
Lanbo Zhang, Yi Zhang
School of Engineering UC Santa Cruz
Santa Cruz, CA, USA
{lanbo, yiz}@soe.ucsc.edu

ABSTRACT
Motivated by the commonly used faceted search interface in e-commerce, this paper investigates interactive relevance feedback mechanism based on faceted document metadata. In this mechanism, the system recommends a group of document facet-value pairs, and lets users select relevant ones to restrict the returned documents. We propose four facetvalue pair recommendation approaches and two retrieval models that incorporate user feedback on document facets. Evaluated based on user feedback collected through Amazon Mechanical Turk, our experimental results show that the Boolean filtering approach, which is widely used in faceted search in e-commerce, doesn't work well for text document retrieval, due to the incompleteness (low recall) of metadata assignment in semi-structured text documents. Instead, a soft model performs more effectively. The faceted feedback mechanism can also be combined with document-based relevance feedback and pseudo relevance feedback to further improve the retrieval performance.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Experimentation
Keywords
interactive retrieval, faceted feedback, relevance feedback, metadata-based retrieval
1. INTRODUCTION
A personalized search or filtering system usually suffers from the "cold start" problem, where the system performs poorly when it has little training data about new users. Researchers have proposed some approaches trying to alleviate
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

this problem. One direction is to borrow information from other users [20, 24]. For example, the idea in [24] is to learn a prior of user interests based on the behaviors (training data) of all users, and learn the user profile for a new user based on both the prior and the training data from this user. Another direction is to develop user interaction mechanisms to collect more information from users[18]. In this paper, we focus on the second direction. We aim to study a new interactive user feedback mechanism that helps retrieval systems learn more about user information needs with limited user interactions.
Faceted search has gained great success in e-commerce domain over the past years, and most popular online retailers, such as Amazon and eBay, now provide faceted search interfaces. On faceted-search-enabled websites, buyers can narrow down the list of products by putting constraints on a group of merchandize facets, such as category, price, brand, size, etc. Well designed faceted search has been shown to be understood by the average user [11]. This motivates us to explore whether we can adapt the faceted search idea to the general purpose document retrieval. In each domain, documents have their own facets, which might be manually assigned or generated automatically. These facets are usually stored in the form of faceted document metadata. Each metadata field corresponds to a facet type, and the specific value assigned to a field for a particular document is a facet value.
Users might have preferences for certain document facets. For example, Chinese readers prefer reading news written in Chinese; some students enjoy learning by reading slides which are usually in the "ppt" format rather than reading long papers which are usually in the "pdf" format; researchers are usually interested in papers within their own subjects; movie viewers might have preferences on movie genres, directors, or casts; online buyers might have preferences on brands, colors, etc. In all these cases, users have clear ideas about some facets of their interested documents, and this information might help the system learn users' preferences and interests. Ideally, users would provide structured queries to describe their information needs more accurately. However, INEX experiments on structured documents retrieval and previous research on log analysis found that people do not use structure in their queries frequently, or use them incorrectly and thus do not improve search effectiveness if they are forced to do so [8].
In this paper, we explore a simple interactive user feedback mechanism based on document facets, called faceted feedback. In this mechanism, instead of letting users pro-

363

vide relevance feedback on documents or create structured queries actively, the system suggests faceted constraints (in the form of facet-value pairs) and users can choose interesting facet-value pairs to improve the returned documents.
We study two major problems of designing a faceted feedback based retrieval system. First, how to recommend facetvalue pairs to users. In e-commerce domain, the candidates of facets and possible values for products are usually manually designed. To make it applicable in general purpose document retrieval, automatic facet recommendation is needed. In this paper, we investigate four approaches to recommending good facet-value pairs. Secondly, we study how to use user faceted feedback in retrieval. Existing e-commerce websites often use a Boolean filtering strategy while retrieving products. However, this may not be a good approach for all domains, since the document metadata is usually imperfect, and the rigid Boolean model may miss relevant documents and hurts the system recall. Thus we also propose a soft retrieval model. In this model, a document that meets a users elected faceted constraint gets a certain number of credits.
The proposed faceted feedback mechanism may have the following advantages. First, the suggested facet-value pairs are usually short and easy to understand. Compared with document-based feedback, this may reduce the cognitive overload of the user and thus is more likely to be adopted by the average user. Users can quickly select multiple facet-value pairs in a short time, so the system might get more user feedback. Second, it may help a user better understand the corpus, how the engine works, and train users in how to form better queries.
The rest of this paper is organized as follows. In section 2, we talk about the related work. Section 3 is the focus of this paper, and describes the faceted feedback mechanism. We propose four facet-value pair recommendation methods and two retrieval models in this section. In section 4, we describe the methodology of our experiments. Section 5 gives the experimental results and the corresponding analysis. Section 6 concludes this paper.
2. RELATED WORK
Many existing search engines equate user information needs with a keyword query, assuming that a user knows what words to use to best describe his or her information need. However, a user's information need is characterized by complex user criteria that are not included in a simple keyword query. Relevance feedback is a commonly used query refinement technique that can be traced back to 1960s. The basic idea is to rely on user interactions to better capture the user information need.
Document-based relevance feedback is one of the most widely used explicit feedback mechanisms. In this scenario, users are asked to provide feedback on the relevance of delivered documents. Many approaches have been proposed to incorporate document relevance feedback into retrieval. For example, Rocchio proposed to combine the original query vector with the center of relevant documents and the center of non-relevant documents [18]. Zhai et al. proposed to estimate a feedback topic model based on user feedback using Maximum Likelihood Estimation (MLE) in the language modeling approach [23]. Zhang et al. proposed to use the Bayesian logistic regression model combined with Rocchio algorithm [24]. Also, several approaches have been proposed to actively select good documents for users to pro-

vide relevance feedback. The simplest way is to choose the top ranked documents since they are most probably relevant. Others also tried other approaches, such as to choose documents with presumably good qualities (e.g., Wikipedia articles), or to choose a diversified set of documents based on document clustering or active learning [19].
A special type of document-based relevance feedback is pseudo relevance feedback. In this case, the top ranked documents are assumed to be relevant and used to modify the query based on the document feedback algorithms described above. Though the assumption is not true, pseudo relevance feedback has been proven effective in improving retrieval performances for short queries [12]. Kelly et al. found that pseudo-relevance feedback performs better for recall-oriented measures [13].
Term-based relevance feedback is to let users select relevant terms from a group of candidates suggested by query expansion techniques. However, research on term-based feedback have mixed results: some found it effectively improves retrieval performance [10, 22], while others found no obvious improvement [4].
Raghavan et al. proposed to use feedback on both instances and features and proposed a unified framework that can be used to combine document-based relevance feedback and feature-based relevance feedback [15].
Our work is motivated by early work in relevance feedback, and differs by focusing on retrieving semi-structured documents with faceted metadata. Anick et al. proposed to extract faceted terminologies automatically from the document text and let users provide relevance feedback on these faceted terminologies [5]. However, the facets in this paper refer to faceted terminologies, usually noun phrases. [9] proposed to get user feedback about controlled indexing vocabulary and got promising results on OHSUMED data set. However, existing research didn't provide detailed description about the algorithms or any quantitative evaluation with real users.
3. FACETED FEEDBACK
Unlike document-based relevance feedback mechanism which asks users to give feedback on the relevance of documents, faceted feedback allows users to give feedback on document metadata fields. In this paper, each metadata field is called a facet, and a facet (f ) with a specific value (v) is called a facet-value pair (f : v). Each facet-value pair represents a faceted constraint on returned documents, E.g., language:Chinese, format:ppt, subject:IR, genre:comedy.
3.1 Facet-value pair recommendation
To avoid overwhelming users with many facet-value pair candidates, the system needs to recommend a small number of facet-value pairs that are most probably interesting to a user. A good recommendation approach is crucial in the faceted feedback mechanism. Intuitively, the recommended facet-value pairs should be good in two respects: 1) they have a high probability of being relevant and thus chosen by the user; 2) they maximize the learning benefits if known to be relevant. Based on the first respect, we propose four facet-value pair recommendation methods. We will investigate the second respect in our future work.

364

3.1.1 Top Document Frequency (TDF)

normalization here:

The first approach is to select the most frequent facetvalue pairs occurring in the top N ranked documents returned by a baseline retrieval algorithm using the initial query. We calculate the frequency of each facet-value pair in the top N documents, which is called "Top N Document Frequency" (TDF). The top K most frequent facet-value pairs are chosen as candidates to present to the user. The underlying assumption is that the more frequently a facet-value pair appears in the top ranked documents, the more likely the user will like it.

N Sl(s)

=

s - minsiS (si) maxsiS (si) - minsiS (si)

(5)

where S is the set of scores of all considered facet-value pairs.

3.2 Incorporate faceted feedback into retrieval
We present two retrieval models to incorporate user faceted feedback in this section. Pu denotes the set of facet-value pairs chosen by the user.

3.2.1 Boolean Model

3.1.2 TDF-IDF
In the term-based feedback literature, researchers have concerns about using the most frequent terms from the top ranked documents, because a lot of common noisy terms are likely to be selected [23]. To avoid similar problems in faceted feedback, we consider another feature of facet-value pairs: the Inverse Document Frequency (IDF), which has a similar definition to the IDF of terms. When scoring a facet-value pair, we use the product of its top N document frequency (TDF) and IDF:
score(f : v, q) = tdf (f : v, q, N )  idf (f : v) (1)
where f : v is a facet-value pair, q is the initial query, and tdf (f : v, q, N ) is the top N document frequency of f : v for query q.
The motivation of using IDF is twofold: 1) a facet-value pair that appears rarely in the whole corpus while frequently in top ranked documents has a high probability to be relevant; 2) the retrieval system gets more benefits by knowing a rare facet-value pair covering a small number of documents being relevant than a frequent one.
3.1.3 Query Likelihood (QL)
Our third method is based on the language modeling approach. The query likelihood given each facet-value pair P (q|f : v) is estimated. The facet-value pairs with the largest query likelihoods are chosen as the candidates.

P (q|f : v) =

P (wj |f : v)c(wj ,q)

(2)

wj q

where c(wj, q) is the frequency of wj in the query q, and

P (wj|f : v) = P (wj|d)P (d|f : v)

(3)

dC

This is a "translation" model motivated by Berger et al. [6]. C is the whole corpus, P (wj|d) is the language model of document d, and P (d|f : v) is assumed to be uniform over all documents that contain f : v.

3.1.4 TDF-QL

The Boolean model filters documents with user faceted feedback. We can use the AND operation to require the retrieved documents contain all of the user-selected facet-value pairs. In practice, the AND operation might be too strict. One alternative is to use the OR operation to allow any document that contains at least one user-selected facet-value pair to pass. Another alternative is to use AND across different facets and OR within each facet. The Boolean model itself returns a document set instead of a ranked list. We can use any ranking methods, such as TFIDF, BM25 [21], etc., to rank the passed documents. We score documents by the Boolean model as follows:

 sm(d)  sbl(d) =
-

if d contains all(AND) / any one(OR) facet-value pair f : v  Pu otherwise

(6) where sm(d) is the score of document d computed using a baseline ranking method m.

3.2.2 Soft Model
Despite the fact that the Boolean model is commonly used in the e-commerce domain, it may not work well for semistructured text document retrieval. The Boolean model is based on two assumptions: 1) users are very clear about what they are looking for, and thus are able to select perfect facet-value pairs to restrict the returned documents; 2) document facets are accurate and complete so that no potentially relevant document is filtered out in retrieval due to meta data errors. These two assumptions may not hold in text document retrieval.
In a specific domain, some facets might be more informative than others. For example, for news articles, the information of time, locations, persons, and topics may be more important than publishers; for research papers, the subjects and keywords may be more informative than the file formats; For movies, the genres, casts and directors may be more informative than producers.
Based on the above motivations, we propose a soft retrieval model. In this model, we learn a weight for each type of facet, which is expected to reflect the quality of the facet. Here the quality may include user acquaintance, metadata accuracy, facet importance, etc. The soft model scores a document as follows:

TDF and QL capture the relationships between the user query and a facet-value pair from different aspects and may complement each other. We combine these two features to score a facet-value pair as follows:

ssm(d) =N Ss(sm(d))

+ f  N Sl(

(d, f : v)  idf (f : v)) (7)

f F

f :vPu

where

score(f : v, q) = N Sl(P (q|f : v))+(1-)N Sl(tdf (f : v, q, N ))

(4)

(d, f : v) = 1 if d contains f : v

(8)

where N Sl() is to normalize the features. We use linear

0 otherwise

365

f is the weight of facet f and is learned automatically. sm(d) is the original score of document d.
N Ss() is the standard normalization that converts the original document scores into a distribution with mean 0 and variance 1. The distributions of original document scores (sm(d)) across different queries and using different baseline retrieval models might be significantly different. We found the difference would badly hurt the retrieval performance in our experiment. So we chose to normalize the original document scores first. As defined in Equation 5, N Sl() is the linear normalization of the score part of a facet f .
4. EXPERIMENTAL METHODOLOGY
4.1 Datasets
To evaluate the proposed faceted feedback mechanism, we use two TREC filtering track datasets: the medical article collection OHSUMED and the news story collection RCV1 [14]. We choose these two corpora because they contain metadata, user queries/profiles, and relevance judgments.
OSHUMED dataset contains 348,566 medical articles selected from a subset of 270 medical journals covering years from 1987 to 1991. This dataset was used in the TREC 2000 filtering track [16], and we use the topics of this track to simulate user information needs in our experiment. The metadata field MeSH (Medical Subject Headline) is used as a document facet.
RCV1(Reuters Corpus Volume 1) dataset contains about 810,000 Reuters news stories published from 1996-0820 to 1997-08-19. There are three types of codes assigned to documents in this collection: topic, geographical region, and industry. These codes are generated with a process involved a combination of auto-categorization, manual editing, and manual correction. We use the three codes as document facets. RCV1 was used in the TREC 2002 filtering track [17], and the first 50 topics of this track are used to simulate user information needs in our experiment1.
4.2 Evaluation Based on Mechanical Turk
We use the Mechanical Turk [1] to collect user faceted feedback. Mechanical Turk is an online marketplace for work, where requesters can publish some tasks that require human intelligence, and workers can choose to work on the tasks to get paid. Comparing TREC assessors with Mechanical Turk workers, prior research shows Mechancial Turk workers are a good source for IR evaluation [3]. In our experiments, we ask workers to act as a real user to provide faceted feedback. For each query2, we design a question, in which the TREC topic statement (including the title and description) and a group of recommended facet-value pairs are shown (See figure 1). Mechanical Turk workers are asked to select good facet-value pairs to restrict the search results according to their understanding of the query. The topic statement helps them act as if they are the real search engine users with the information need. By configuring the Human Intelligence Task (HIT) properties, we make sure there are three workers work on each query to give faceted feedback. Note that those workers are all random workers on Mechanical Turk who happen to see our task and choose
1The prior research shows that the other topics do not match real user information needs well. 2Each query corresponds to a TREC topic.

to work on it. We also design some questions about their knowledge background related to the query topics in order to help us understand if there is a strong correlation between user knowledge levels and feedback quality.
4.3 Experimental settings
Our experiment is designed to answer the following questions:
· Is faceted feedback mechanism effective in improving retrieval performance?
· How does faceted feedback compare to other feedback mechanisms?
· Can faceted feedback be used together with other feedback mechanisms?
· Which facet-value pair recommendation methods are better?
To answer the first question, we compare the retrieval performance of faceted feedback to the baseline method BM25 without user feedback. In the baseline retrieval, only the title parts of TREC topics are used in order to simulate the short queries in real scenarios. To answer the second question, we compare faceted feedback with pseudo relevance feedback (PRF) and real document relevance feedback (RRF). We use the relevance judgments provided by TREC as simulated user feedback for RRF. To answer the third question, we use PRF and RRF respectively to calculate the original document scores (sm(d) in equation 6 and 7). The final retrieval performances will tell us whether faceted feedback complements existing feedback mechanisms and can be combined with them to further improve the retrieval performance. To answer the fourth question, we compare the retrieval performances of different faceted-value pair recommendation methods. Standard IR evaluation measures Mean Average Precision (MAP), Precision@N (P@N) and Recall@N (R@N) are used to evaluate the retrieval performances.
In our initial experiment, we found different users might choose different facet-value pairs given the same query and the same candidate set, which will lead to different retrieval performances. To avoid the influence of user difference, one possibility is to have the same user work on each of the candidate sets to be compared. However, a user's choice is influenced by his/her past experience and thus the order of how the candidate sets are presented will influence the results. To alleviate this problem, we combine the candidate sets recommended by four methods and present the large set to the user for feedback. When calculating the retrieval performance of a specific recommendation method, we only use the user-selected facet-value pairs that are included in the candidate set recommended by this method.
We set the number of facet-value pairs each recommendation method recommends (K) to 10, the number of top ranked documents used in the recommendation of facetvalue pairs (N in equation 1) to 100, and the weight of query likelihood ( in equation 4) to 0.5.

366

Figure 1: User interface on Mechanical Turk

5. EXPERIMENTAL RESULTS
5.1 Overall performances of faceted feedback
Table 1 shows the retrieval performances of the baseline (BM25), using faceted feedback (FF) from individual user (User1, 2 and 3 for OHSUMED dataset, User4, 5 and 6 for RCV1 dataset), and the average over three users (FF(Average)). P@10 is the precision of top 10 documents. All the performances reported here are obtained using the soft retrieval model3. The average MAP and P@10 of using faceted feedback on OHSUMED dataset are improved by 32.4% and 43.9% over the baseline (BM25) respectively. The average MAP and P@10 on RCV1 dataset are improved by 11.1% and 8.8% respectively. According to these results, we conclude that faceted feedback is effective in improving retrieval performance.
Table 1: Performances of Faceted Feedback (FF). "FF (User1|4)" means to use faceted feedback from User1 (on OHSUMED dataset) and User4 (on RCV1 dataset).

usually have disagreements about document relevance judgments. This is also consistent with our anticipation: users' feedback may be different due to their different backgrounds and different understandings about the same information need. For example, users majoring in medicine are very likely to give more accurate feedback than the average user, which will result in better performance on the OHSUMED dataset. However, this does not mean faceted feedback is only useful for smart or expert users. Table 1 shows that three users' feedback are all useful in improving retrieval performances.

Table 3: Performance comparison of different retrieval models on OHSUMED dataset. The feedback from User1 is used.

Retrieval model BM25 (baseline) Boolean model (AND) Boolean model (OR) Soft model

MAP 0.0921 0.0403 0.1120 0.1354

P@10 0.1397 0.1522 0.1758 0.2286

R@1000 0.4612 0.0935 0.4650 0.5301

Dataset Performance BM25 (baseline) FF (User1|4) FF (User2|5) FF (User3|6) FF (Average) Imprv over BM25

OHSUMED MAP P@10 0.0921 0.1397 0.1354 0.2286 0.1112 0.1873 0.1189 0.1873 0.1219 0.2010 32.4% 43.9%

RCV1 MAP P@10 0.2907 0.5680 0.3221 0.6180 0.3150 0.6120 0.3318 0.6240 0.3230 0.6180 11.1% 8.8%

5.2 User disagreement on faceted feedback
Given the same query and the same group of facet-value pair candidates, users may select different facet-value pairs, which lead to different retrieval performances. In Table 1, the performance using feedback from User1 and User6 (in bold) are better than other users. Table 2 gives two query examples for which users' faceted feedback are different from each other. Further analysis shows that there are very few queries that users gave exactly the same feedback. This is common in IR evaluation, as well trained TREC assessors
3The Boolean model will be discussed in a later section

Table 4: Performance comparison of different retrieval models on RCV1 dataset. The feedback from User6 is used. "Boolean model (A+O)" means to use AND operation across facets and OR operation within each facet.

Retrieval model BM25 (baseline) Boolean model (AND) Boolean model (A+O) Boolean model (OR) Soft model

MAP 0.2907 0.1046 0.2208 0.2912 0.3318

P@10 0.5680 0.3311 0.5102 0.5780 0.6240

R@1000 0.6658 0.1514 0.5062 0.6563 0.6954

5.3 Boolean model v.s. Soft model
Table 3 and 4 compare the performances of the Boolean models and the soft model. R@1000 is the recall of top 1000 documents. In Table 4, "Boolean (A+O)" means to use AND across facets and OR within each facet (Table 3 doesn't have this since only one facet is used on the OHSUMED dataset).

367

Table 2: Examples of user-selected facet-value pairs

Query "58 yo with cancer and hypercalcemia"
"Aborigine health"

User1 MeSH:Hypercalcemia MeSH:Diphosphonates MeSH:Calcium
Industry:Hospitals & Healthcare Topic:Health

User2 MeSH:Hypercalcemia MeSH:Carcinoma, Squamous Cell MeSH:Parathyroid Hormones
Region:Australia Topic:Health Topic:Welfare, Social Services

User3 MeSH:Hypercalcemia MeSH:Paraneoplastic Syndromes MeSH:Bone Neoplasms MeSH:Bone Resorption Region:Australia Topic:Health Topic:Government/Social

The Boolean model with AND operation works poorly on both datasets. It results in much lower Recall@1000 than other retrieval models. The Boolean OR operation works better than the baseline method on OHSUMED dataset and a little worse on RCV1 dataset. The Boolean A+O works better than Boolean AND while still worse than Boolean OR. This reveals that when we loosen the Boolean restriction, we are actually getting improved retrieval performances. In contrast to the general practice of using Boolean approach in faceted search, the Boolean model in our experiments doesn't work well for text document retrieval. We did some further analysis and figured out two reasons for that. First, document metadata assignments are not perfect. Many documents are not assigned with metadata that they should have (we call this case incompleteness of metadata assignment). Secondly, some users select ambiguous or inappropriate facet-value pairs, probably because they are not familiar with the current topic. When using the Boolean model, many potentially relevant documents are filtered out due to either incompleteness of metadata assignment or users' inappropriate feedback, and thus the system recall is hurt seriously.
Soft model works well since it uses user feedback as preferences instead of rigid requirements. We proposed to use the parameter f to capture the quality of a facet previously. The motivation is that the values of some facets are easy to determine by either human beings or algorithms, while for some other facets this might be hard. For example, the facet "Region" might be easier for human, while "topic" might be harder. Someone may think a news article talking about "resident health" should be categorized into the topic "Government/Social" while some others may not think so. We found that the feedback on the "Topic" facet are different across three users, and feedback on the "Region" facet are more consistent across the users. Besides, for easier facets, the metadata assignment tends to be accurate and complete, and thus trustable. While for harder facets, the metadata assignment might be inaccurate and incomplete, and thus less trustable. These observations further justified our motivation for introducing parameter f .
The proposed soft model requires training data to learn f for each facet, and we use the 3-fold cross validation in our experiment. The queries are randomly split into three equal-size sets. In each fold, two sets are used as training queries to learn the parameter (f ), and the last set is used for testing. The average performances over three folds are reported in Table 3 and 4. Table 5 shows the  values learnt

Table 5: The optimal f trained in each fold

Dataset

Fold

Facet Optimal 

Fold1

MeSH

8.5

OHSUMED Fold2

MeSH

7.5

Fold3

MeSH

9.5

Region

10

Fold1

Topic

2

Industry

1.5

Region

10

RCV1

Fold2

Topic

1.5

Industry

0.5

Region

2.5

Fold3

Topic

2

Industry

2

in each fold4. On RCV1 dataset, Region are consistently larger than T opic and Industry, which suggests that the "Region" facet is more trustable or easier for users than the other two facets.
It is worth mentioning that if a software such as those proposed by Herst et al.[11] is used to automatically generate facet values, we may prefer completeness/recall of metadata assignment instead of precision. Because metadata incompleteness hurts the system recall badly, while inappropriate facet value assignment hurts less, as the final ranking algorithm would rank a document low if it is non-relevant to the user information need.
5.4 Comparison of different facet-value pair recommendation approaches
Figure 2 compares the retrieval performances corresponding to four facet-value pair recommendation methods: TDF (Top N Document Frequency), TDFIDF (Combine TDF with IDF), QL (Query Likelihood), and TDFQL (Combine QL with TDF). The horizontal axis shows the number of facet-value pair candidates used, and the vertical axis shows the corresponding MAP. The performances shown in the figures are the average across three users. TDFIDF and TDFQL methods perform better than the other two methods on OHSUMED dataset. This is consistent with our expectation, since both methods combine two features of facet-value pairs.
Interestingly, on RCV1 dataset, TDFIDF and TDFQL perform worse than TDF. Further analysis shows that the
4The smallest scale we tried for f is 0.5, since smaller scales have no significant influences on retrieval performances.

368

facet-value pair "Region:USA" benefits retrieval performance a lot for several queries. Unfortunately, both TDFIDF and TDFQL rank it out of the top 10 since it appears frequently in the whole corpus, so users have no chance to see this candidate. One possible solution is to use facet weights (f ) as an extra feature in our facet-value pair recommendation methods. The motivation is that those facets more trustable for retrieval (with bigger f , such as the facet "Region" over the other two facets) should be boosted in recommendation. This is consistent with the second criterion of good facetvalue pairs we mentioned in section 3.1. We will evaluate this idea in our future work.
5.5 Comparison with other types of feedback
The retrieval performances of faceted feedback (FF), Pseudo Relevance Feedback (PRF) and Real document-based Relevance Feedback (RRF) are compared and shown in Table 6. The performances of combining PRF with FF (PRF@5+FF) and combing RRF with FF(RRF@5+FF) are also reported. The performance of FF is the average over three users. The top 5 ranked documents in BM25 are used for pseudo relevance feedback (PRF@5). The relevance judgments of top 5 are used in the BM25 relevance feedback algorithm as implemented in Lemur [2] (RRF@5). Table 6 shows that FF performs better than PRF, and closely to RRF on OHSUMED dataset; FF performs worse than PRF and RRF on RCV1 dataset, and 10% better than BM25.
Though FF might perform worse than RRF, FF is still very promising because of three major reasons. First, a retrieval system may get more faceted feedback than document feedback, as faceted search is commonly accepted by the average Internet user and faceted feedback seems very easy for a user to understand. Second, RRF and PRF often help little for hard queries when no relevant documents are retrieved in the top positions, while faceted feedback might help boost relevant documents in these cases. A major scenario where search engine fails is that an engine only focuses on one aspect of a query and ignores some other important aspects [7]. Faceted feedback provides the mechanism for users to put constraints on the important aspects to avoid this problem. Actually, in our experiment, we find there are a number of queries for which FF helps a lot when PRF and RRF help little or even hurt. These queries are mostly hard queries with poor initial retrieval performances. Take the query "Nuclear plants U.S." as an example, almost all the initially returned top documents in baseline ranking are about nuclear plants outside U.S., thus PRF and RRF hurt. FF helps since all users are able to identify the faceted restriction "Region:USA", which boosts those documents talking about events happening in U.S.. Third, different types of feedback are not exclusive and they could complement each other. We can easily combine FF with PRF or RRF and obtain better retrieval performances. This can be done by using PRF or RRF as the baseline method to calculate the original document scores (sm(d) in equation 6 and 7). Table 6 shows that the combininations of FF with PRF or RRF improve the performances further.
6. CONCLUSIONS
We researched the user feedback mechanism based on faceted document metadata. The results on a medical dataset and a news dataset show that faceted feedback is useful, though different users may give different feedback for the same query.

Table 6: Performance comparison of different types of feedback. FF: faceted feedback; PRF@5: pseudo relevance feedback using top 5 docs; RRF@5: real document-based relevance feedback using top 5 docs.

Dataset Performance BM25 (baseline) FF PRF@5 RRF@5 PRF@5+FF RRF@5+FF

OHSUMED MAP P@10 0.0921 0.1397 0.1219 0.2010 0.1096 0.1746 0.1240 0.2048 0.1269 0.1937 0.1473 0.2481

RCV1 MAP P@10 0.2907 0.5680 0.3230 0.6180 0.3711 0.6280 0.3887 0.6940 0.3899 0.6320 0.4025 0.7007

Directly using the Boolean model, which is commonly used in e-commerce, is inappropriate for metadata-based general purpose document retrieval, since the document metadata assignment is usually incomplete. The proposed soft model is shown consistently more effective on both datasets, as it automatically learns a weight for each facet, which captures the facet quality. The proposed facet-value pair recommendation methods are generally effective and can be improved in the future. Faceted feedback could be combined with pseudo relevance feedback and document relevance feedback. We tried one simple combining method and found better retrieval performance.
In the future, more research is needed to explore different facet-value pair recommendation algorithms, for example, incorporating facet weights (f ), considering the interaction among facet-value pairs and how user choices are affected by context. We also want to explore different ways to combine various feedback mechanisms.
7. ACKNOWLEDGMENTS
We thank Jessica Gronski and Yize Li for valuable discussions related to this research. The work was funded by National Science Foundation IIS-0713111, AFRL/AFOSR and UCSC/LANL Institute for Scalable Scientific Data Management. Any opinions, findings, conclusions or recommendations expressed in this paper are the authors', and do not necessarily reflect those of the sponsors.
8. REFERENCES
[1] Amazon mechanical turk. https://www.mturk.com. [2] The lemur toolkit for language modeling and
information retrieval. http://www.lemurproject.org/. [3] O. Alonso and S. Mizzaro. Can we get rid of trec
assessors? using mechanical turk for relevance assessment. In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation, 2009. [4] P. Anick. Using terminological feedback for web search refinement: a log-based study. In SIGIR '03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 88­95, New York, NY, USA, 2003. ACM. [5] P. Anick and S. Tipirneni. Interactive document retrieval using faceted terminological feedback. Hawaii

369

Figure 2: Performances of different facet-value pair recommendation approaches. The left figure: on OHSUMED dataset; the right figure: on RCV1 dataset

International Conference on System Sciences, 2:2036, 1999.
[6] A. Berger and J. Lafferty. Information retrieval as statistical translation. In SIGIR, 1998.
[7] C. Buckley. Why current ir engines fail. In SIGIR '04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 584­585, New York, NY, USA, 2004. ACM.
[8] B. Croft, D. Metzler, and T. Strohman. Search Engines: Information Retrieval in Practise, chapter 11 Beyond Bag of Words, page 463. Pearson, 2009.
[9] J. C. French, A. L. Powell, F. Gey, and N. Perelman. Exploiting a controlled vocabulary to improve collection selection and retrieval effectiveness. In CIKM '01: Proceedings of the tenth international conference on Information and knowledge management, pages 199­206, New York, NY, USA, 2001. ACM.
[10] D. Harman. Towards interactive query expansion. In SIGIR '88: Proceedings of the 11th annual international ACM SIGIR conference on Research and development in information retrieval, pages 321­331, New York, NY, USA, 1988. ACM.
[11] M. A. Hearst and E. Stoica. Nlp support for faceted navigation in scholarly collection. In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pages 62­70, Suntec City, Singapore, August 2009. Association for Computational Linguistics.
[12] D. Kelly, V. D. Dollu, and X. Fu. The loquacious user: a document-independent source of terms for query expansion. In SIGIR '05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 457­464, New York, NY, USA, 2005. ACM.
[13] D. Kelly and X. Fu. Elicitation of term relevance feedback: an investigation of term source and context. In SIGIR '06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 453­460, New York, NY, USA, 2006. ACM.
[14] D. D. Lewis, Y. Yang, T. Rose, and F. Li. Rcv1: A

new benchmark collection for text categorization research. 2004.
[15] H. Raghavan, O. Madani, and R. Jones. Active learning with feedback on features and instances. J. Mach. Learn. Res., 7:1655­1686, 2006.
[16] S. Robertson and D. Hull. The TREC-9 filtering track report. In The Ninth Text REtrieval Conference (TREC-9), pages 25­40. National Institute of Standards and Technology, special publication 500-249, 2001.
[17] S. Robertson and I. Soboroff. The TREC-10 filtering track final report. In Proceeding of the Tenth Text REtrieval Conference (TREC-10), pages 26­37. National Institute of Standards and Technology, special publication 500-250, 2002.
[18] J. J. Rocchio. Relevance feedback in information retrieval. 1971.
[19] X. Shen and C. Zhai. Active feedback in ad hoc information retrieval. In SIGIR '05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 59­66, New York, NY, USA, 2005. ACM.
[20] L. Si and R. Jin. Flexible mixture model for collaborative filtering. In ICML '03: Proceedings of the Twentieth International Conference on Machine Learning, 2003.
[21] S. J. M. H.-B. Stephen E. Robertson, Steve Walker and M. Gatford. Okapi at trec-3. 1994.
[22] B. Tan, A. Velivelli, H. Fang, and C. Zhai. Term feedback for information retrieval with language models. In SIGIR '07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 263­270, New York, NY, USA, 2007. ACM.
[23] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. pages 403­410, 2001.
[24] Y. Zhang. Using bayesian priors to combine classifiers for adaptive filtering. In SIGIR '04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 345­352, New York, NY, USA, 2004. ACM.

370

Mining the Blogosphere for Top News Stories Identification

Yeha Lee Hun-young Jung Woosang Song Jong-Hyeok Lee

Division of Electrical and Computer Engineering Pohang University of Science and Technology
Pohang, Gyungbuk, Republic of Korea
{sion, blesshy, woosang, jhlee}@postech.ac.kr

ABSTRACT
The analysis of query logs from blog search engines show that news-related queries occupy a significant portion of the logs. This raises a interesting research question on whether the blogosphere can be used to identify important news stories. In this paper, we present novel approaches to identify important news story headlines from the blogosphere for a given day. The proposed system consists of two components based on the language model framework, the query likelihood and the news headline prior. For the query likelihood, we propose several approaches to estimate the query language model and the news headline language model. We also suggest several criteria to evaluate the news headline prior that is the prior belief about the importance or newsworthiness of the news headline for a given day. Experimental results show that our system significantly outperforms a baseline system. Specifically, the proposed approach gives 2.62% and 10.19% further increases in MAP and P@5 over the best performing result of the TREC'09 Top Stories Identification Task.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Search and Retrieval ­ Retrieval models
General Terms
Algorithms, Experimentation, Performance
Keywords
Blog Retrieval, Blogosphere, Top News Stories Identification
1. INTRODUCTION
A blog, "web log", is a special type of website in which users (individuals or groups) express their opinions or thoughts on several subjects. Blog posts consist of a wide variety of topics. As the number of blog users increase, the popularity and the importance of blogs are growing, and several
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

commercial search engines such as Google1 and Technorati2 have provided blog search services.
Users' information needs for blog search differ from those for general web search. A large portion of the query logs from blog search engines are news-related queries [21, 22]. In other words, many users find information about news stories in the blogosphere. This implies that the blogosphere may be helpful when locating news stories.
A large number of news stories from various news channels are generated and updated day after day. However, a relatively few news among huge number of them receive attention from users. Therefore, it is one of the most important issues to evaluate the importance of news stories and rank them.
We investigate how to take advantage of the blogosphere for identifying top news stories. To this end, given a certain day, we retrieve and rank news headlines according to their importance or newsworthiness, using the blogosphere. Furthermore, this task is worthwhile in that it identifies the top news stories from blog users' point of view, instead of the news providers. The task is also called Top Stories Identification Task (TSIT) which was first introduced at the TREC 2009 Blog Track [21].
TSIT is a new pilot task that aims to "address the news dimension in the blogosphere" [21]. The task uses a date (day) as a query. For a date query, the system for the task ranks the news headlines in the order of their importance. Furthermore, for each news headline, the task requires a certain number of blog posts that capture diverse aspects relevant to the news headline.
TSIT has some characteristics that distinguish it from previous news-related studies such as Topic Detection and Tracking (TDT). First, the data given for TSIT contains only news headlines but no news contents. Therefore, the system for the task should rank news headlines utilizing the blogosphere (i.e. Blog08 corpus) instead of the contents of news articles. Second, unlike the corpus of news stories, blog posts are generally neither well-written articles nor topically coherent. They also include a lot of non-topical contents such as spam blogs and blog comment spam that advertise commercial products and services [16], making the task difficult.
In this paper, we present novel approaches to identify the top news stories in the blogosphere. The proposed approaches are based on the language model framework, which is widely used in information retrieval tasks. We propose a
1http://blogsearch.google.com/ 2http://www.technorati.com/

395

series of approaches to estimate a query language model and a news headline language model based on the blogosphere, and to rank the news headlines according to the distance between two language models. We also suggest several criteria to evaluate the prior probability that a news headline will be a top news story for a given day, and verify that these criteria are useful to identify top news stories. The experimental results show that our approach significantly improves the best performance submitted in the TREC 2009 Top Stories Identification Task.
The rest of the paper is organized as follows. In section 2, we briefly survey related work on new event detection. In section 3, we address the framework of our system, and propose several approaches to identify the top news headlines. In section 4, we conduct several experiments to evaluate the performances of our approach. Finally, we conclude the paper and discuss future work in section 5.
2. RELATED WORK
As a new pilot task, TSIT aims to identify top news stories in the blogosphere, and to provide a ranked list of news headlines. There are few researches for identifying and ranking top news stories in the blogosphere. One of the research directions closely related to TSIT may be the New Event Detection.
New Event Detection, one of the five tasks in TDT, aims to detect whether a given news story is concerned with already known events to a system or not. For the event detection problem, many approaches have been based on clustering or classification to estimate the similarity between the events and documents (e.g. the news stories); these approaches differ in the ways by which they evaluate the similarity [3, 5, 17, 25, 28, 29]. All of them compare each document with existing events. If the similarity between the document and the events is lower than some predefined criteria, the document is considered to address a new event. Otherwise, the document is assigned to the event to which it is most similar.
Various features have been proposed, including timeline analysis, burstiness and named entities. Chen et al proposed an aging theory to capture the life cycle of a news event, and improved the performance for event detection [7]. Chen et al used an aging theory and a sentence modeling to extract hot topics from news documents [8]. They analyzed the timeline to identify the key terms. The burstiness of terms was used by many researchers for the event detection [9, 12, 15, 24]. Kleinberg proposed an approach to identify the bursty features for the event detection from e-mail streams [15]. They used the infinite-state automaton to model the stream. He et al identified bursts of (a)periodic features using a Gaussian distribution, and then used them to detect (a)periodic events [12]. Kumaran and Allan used named entities for the event detection [17]. They showed that the usefulness of named entities can change according to certain situations. Kuo et al classified terms within news stories based on named entity type and parts-of-speech tags, and assigned a different weight to each term according to the type and class of news story [28].
The main difference between previous work for the event detection and our approach stems from the difference in source data for identifying events or news stories. In contrast with previous work, we identify the top news headlines

using only the unorganized blogosphere, not the well-defined contents of news articles.

3. TOP STORIES RANKING MODEL
To identify the top news stories, we rank them according to their importance or newsworthiness on a specific day. The newsworthiness of a news story can be decided by several criteria3 as follows:
· Timing News stories that is happening now are often more newsworthy than those that happened a week ago.
· Significance The number of people involved in a news story is important.
· Proximity News stories that occur near us are more important than distant ones.
· Prominence News stories about famous people are more newsworthy than stories about ordinary people.
· Human-Interest Human-interest stories are generally soft news. They appeal to emotions.

We assume that a news story mentioned in more blog posts or comments is more important or newsworthy on a specific day, because a top news story satisfying the above criteria may receive attention from many blog users, who express their thoughts or opinions about the news story in their blogs.
To measure the importance of a news story using the blogosphere, we adopt the language model framework, which is widely used in information retrieval tasks. Motivated by our assumption, we evaluate the importance of a news headline using the probability that blog posts published on a query day generate the headline. Let H be a news headline and let Qd and Qp be a given (date) query and a set of blog posts published on the query day Qd, respectively.
Score(Qd, H)  P (H|Qp)  P (Qp|H) P (H) (1)

Importance score of a news headline

Query Headline Likelihood Prior

3.1 The Query Likelihood
In the language model framework, the query likelihood means the probability that a document generates a given query. TSIT uses a date (day) as a query. Therefore, we regard the query likelihood as the probability that a news headline generates blog posts published on the query day (i.e. Qp).
To this end, we should estimate two language models, the Query Language Model (QLM) and the News Headline Language Model (NHLM). Both of the language models are estimated, based on blog posts.
3.1.1 Query Language Model
For a query day Qd, we estimate the QLM using blog posts Qp. However, the blog posts may discuss various topics from individual daily affairs to important events recently happened. If we model the blog posts using a single language model, the language model cannot correctly capture
3http://www.mediacollege.com/journalism/news/ newsworthy.html

396

the contents of the blog posts. As a result, we will get the

wrong QLM.

To solve this problem, we divide the documents into K

clusters. We assume that each cluster can accurately reflect

one of the various topics mixed in the blog posts. To esti-

mate the QLM, we first gather blog posts which are pub-

lished on a query day. Then, we cluster them using the

K-means algorithm. We represent each document using the

term vector d : wi = tfi × idfi, where tfi indicates the fre-

quency

of

term

wi

within

a

document

d,

and

idfi

=

log(

|T D| dfi

)

means inverse document frequency: |T D| is the total number

of documents in a collection and dfi is document frequency of a term wi. We use the cosine similarity as the distance function between two documents.

Similarity di, dj = di · dj

(2)

|di| × |dj |

where |di| and |dj | indicate the length of di and dj, respectively.
After clustering, we can generate the QLMs from the K document sets. Let Dk = {dk1 , dk2 , · · · , dkn } be the kth document set, QLMk be the kth QLM. The document set Dk contains information relevant to a topic of the document set, but also contains background information. We assume
that the documents are generated by a mixture model of
QLMk and the collection language model C that reflects the background information.

P (Dk) =

{(1 - )P (w|QLMk ) + P (w|C )}c(w;dki ) (3)

iw

where c(w; dki ) is the number of times term w occurred in

a document dki , P (w|C ) =

ctfw |C|

:

ctfw

is the number of

times term w occurred in the entire collection, |C| is the

length of the collection, and  is a weighting parameter. In

our experiments, we set  as 0.8.

Then, we can estimate QLMk using the EM algorithm [11]. The EM updates for p(w|QLMk ) are as follows:

tnw

=

(1 - )P n(w|QLMk ) (1 - )P n(w|QLMk ) + P n(w|C )

(4)

P n+1(w|QLMk ) =

n i=1

c(w;

dki

)tnw

w

n i=1

c(w

;

dki

)tn w

(5)

3.1.2 News Headline Language Model
To estimate the NHLM, for each news headline, we first retrieve blog posts relevant to its topic using the news headline itself as query. To this end, we evaluate the relevance between a news headline H and a blog post d using the the KL-divergence language model [18] with Dirichlet smoothing [27].

Score(H, d)  P (w|H) log P (w|d)

(6)

w

where P (w|H) is the maximum likelihood estimates of the

news headline, and P (w|d) =

: c(w;d)+dP (w|C ) |d|+d

c(w; d) is

the number of times a term w occurred in a blog post d, and

d is a smoothing parameter, and |d| is the length of the

document d.

Among the search results, we use only the blog posts

whose issued date is within a certain period from a query

day, because the time gap between the issued day of a blog

post and a query day often means that the blog post is mentioning an event different from those that happened on that day [25]. In other words, the blog post is likely to be relevant to a topically similar, but different news headline.
We gather only the blog posts between -3 and +28 days from a query day. Then, we choose 10 blog posts that can provide as diverse aspects about the news headline as possible. We call the 10 blog posts the supporting relevant posts of the news headline.
We propose two approaches to make the supporting relevant posts reflect the diverse aspects relevant to the news headline: Relevance-Based Selection (RBS), Feed-Based Selection (FBS).
RBS is an intuitive but naive approach to choose the supporting relevant posts. This approach selects the supporting relevant posts according to a relevance score of each blog post obtained from Eq. 6. We define this approach as a baseline for our experiments.
FBS chooses the supporting relevant posts based on blog feeds which belong to each of them. Individual blog users may have different interests and tendencies for the same events, and these differences can be represented through their blog feed [19]. That is, for a given news headline, blog posts from different blog feeds can provide different aspects, even if they address information on the same news story. Therefore, to increase the diversity of the supporting relevant posts, we select them from as wide a range of blog feeds as possible. In a similar way to RBS, FBS also chooses blog posts according to their relevance score, but FBS selects only one blog post from one blog feed.
We estimate the NHLM using the maximum likelihood estimate of the 10 supporting relevant posts and the Dirichlet smoothing [27]. Let NHLM and C be the NHLM and the collection language model, respectively, and let SRP be a set of the 10 supporting relevant posts.

P (w|NHLM )

=

c(w; SRP ) + hP (w|C ) |SRP | + h

(7)

where c(w; SRP ) is the number of times a term w occurred in SRP and h is a smoothing parameter.

3.1.3 Score Function
To evaluate the query likelihood, we use the KL-divergence language model [18], one of the-state-of-the-art information retrieval models, to rank news headlines in response to a given query. We use the maximum value among scores between the QLMs and the NHLM as the relevance score of a news headline.
Let ScoreQLH (Qd, H) be the relevance score of a news headline H with respect to a given query Qd. We define ScoreQLH (Qd, H) as follows:

ScoreQLH(Qd,H) = max k

P (w|QLMk) logP (w|NHLM) (8)

w

3.2 The News Headline Piror
We suggest two criteria to estimate the news headline prior P (H) that is the prior belief about the importance or newsworthiness of a news headline for a given day: Temporal Profiling and Term Importance. Although the proposed approaches depend on a date such as the query day, we re-

397

gard them as the priors of a news headline in that they are independent of the query language model.

3.2.1 Temporal Profiling
The Temporal Profiling criterion uses the temporal information of blog posts relevant to a news headline. We assume that if a news headline is important for a query day, many blog posts relevant to its topic will be posted on that day.
To generate the temporal profile of each news headline, we use a temporal profiling approach proposed in [14] with some modifications. The temporal profile of a news headline H is defined as follows:

P (t|H) = P (t|d) Score(H, d)

(9)

dR

d R Score(H, d )

where t is a date (day), and R is a document set that consists of 500 blog posts selected by an order of a relevance score Score(H, d) from Eq. 6, and

P (t|d) =

1 if t is equal to the document date 0 otherwise

We then smoothed the temporal profile P (t|H) using the background model as follows:

P (t|H) = (1 - )P (t|H) + P (t|C)

(10)

where

P (t|C)

=

1 |T D|

dC P (t|d): |T D| is the total num-

ber of documents in the collection, and  is a smoothing

parameter. In our experiments, we set  = 0.5.

This temporal profile is defined on each single day. How-

ever, if a news story is important for a query day Qd, the blog posts relevant to it may be published over a certain period

following the day due to the bursty nature [15]. Therefore,

we smooth the temporal profile model with the model for

adjacent days. Let ScoreT P (Qd, H) be a score of a news

headline estimated using the temporal profile of the news

headline.

ScoreT P (Qd, H)

=

1 Zw

w(t, Qd)P (t|H)
t

(11)

where  indicates a period from Qd, and Zw = t w(t, Qd). We define a weight function for w(t, Qd) using the Cosine (Hamming) kernel function [20] as follows:

w(t, Qd) =

1 2

1 + cos

|t-Qd |× 

0

t

(12)

otherwise

3.2.2 Term Importance
The Term Importance criterion uses term information of a news headline. We believe that each term has a different importance for a given day. If a news headline consists of important terms, it is likely to be a top news story and vice-versa. For example, a news headline that consists of common words or stopwords may not be a top news story.
We only consider named entities, not all terms in a news headline. Named entities were used by many event detection systems, improving the performance of the systems [17, 26, 28]. We extract named entities from each news headline using the Stanford Named Entity Recognizer4. Then, we gather all n-gram (n  3) from the named entities.
4http://nlp.stanford.edu/software/CRF-NER.shtml

We evaluate the importance of the n-gram terms based on the T F · IDF approach that is widely used for term weighting in many information retrieval tasks.
Let nt be the extracted n-gram term and T F (nt, Qd) be a term frequency of the term nt for a query day Qd. Intuitively, if a term nt occurs frequently within news headlines issued on a query day, it is likely to be important. In a similar way to the bursty nature of blog posts, news headlines relevant to important events that happen on the query day may be published over several subsequent days. Therefore, we define the term frequency T F (nt, Qd) as the number of a term nt within news headlines that are issued during a certain interval containing a query day Qd.

T F (nt, Qd) = c(nt; t)

(13)

t

where  indicates the period, and c(nt; t) means the number of a term nt occurring in news headlines issued on day t.
Let IDF (nt) be the inverse "date" frequency, and T N D be the total number of days that the news headline corpus spans. We define IDF (nt) as follows:

IDF (nt) = T N D

(14)

DF (nt) + 

where DF (nt) indicates the number of days on which nt occurs in news headlines, and  is a constant which controls the influence of DF (nt) on IDF (nt).
The inverse date frequency IDF (nt) corresponds to the inverse document frequency. In other words, a term nt with a high IDF (nt) value may be a keyword that distinguishes important events that happened on a query day from those that happened on other days.
Let ScoreT I (Qd, H) be the importance of a news headline H evaluated using the term importance.

ScoreT I (Qd, H) = max (T F (nt, Qd) × IDF (nt)) (15) ntH

3.3 Integration of Query Likelihood and News Headline Prior
We proposed several approaches for the query likelihood and the news headline prior in section 3.1 and 3.2. They capture the different characteristics of important news headlines. For the query likelihood, we analyze the contents of the blog posts, and model the dominant topics buried in them. Then, we rank the news headlines according to the probability that each headline generates one of the topics. For the news headline prior, we proposed two criteria to reflect the properties of important news headlines, Temporal Profiling and Term Importance.
To identify the top news headline, we integrate the query likelihood with the news headline prior. To achieve this, we first adjust each score from 0 to 1.

S corei (H )

=

Scorei(H) - mini maxi - mini

(16)

mini = min Scorei(H ) and maxi = max Scorei(H )

H

H

where Scorei(H) indicates one score of ScoreQLH (Qd, H), ScoreT P (Qd, H) and ScoreT I (Qd, H).

398

Finally, we define the ranking function as follows:
Score(Qd,H)=(1 - 1)ScoreQLH(Qd, H)
+1 (1 - 2)ScoreTI(Qd,H)+2ScoreTP(Qd,H) (17)
where 1 is the weighting parameter that adjusts the importance between the query likelihood and the news headline prior, and 2 is the parameter that controls the weights between two criteria for the news headline prior.
4. EXPERIMENTS
We conducted several experiments to evaluate our system for TSIT. We measured the performance of the query likelihood and the news headline prior, respectively. We also investigated the influence of the combination of two components on the performance of TSIT, with a varying weight parameter 1.
4.1 Setup
4.1.1 Data Set
The Blogs08 corpus and the news headline corpus from the New York Times (NYT) [21] were used for experiments. The Blogs08 corpus was created by monitoring 1 million blogs from January 14, 2008 to February 10, 2009, and consisted 808GB of feeds, 1445GB of permalink documents and 56GB of homepages. The news headline corpus consisted of headlines of articles published by NYT during the interval covered by the Blogs08 corpus.
Our experiments were performed using only Blog08 and the news headline corpus without resorting to any other resources. For the evaluation, we used the 55 topics and relevance judgments from the TREC 2009 Top Stories Identification Task.
We only used the permalinks (blog post) for the experiments. We discarded the HTML tags of each blog post, and applied the DiffPost algorithm [23] to remove non-relevant contents5 of each blog post. Each blog post was also processed by stemming using the Porter stemmer and eliminating stopwords using the INQUERY words stoplist [2].
4.1.2 Evaluation Method
In response to each query, we retrieved 100 news headlines according to their importance on that day, and provide 10 supporting relevant posts for each news headline, as in TSIT.
The evaluation consists of two phases. In the first phase, we assess the performances of the proposed approaches for identifying the top news headlines for a query day. For each query, we considered only the news headlines corresponding to Qd ± 1 days as ranking candidates, because of the time discrepancy between the day on which the headline was and the day Qd on which the news story actually happened [21]. We used the mean average precision (MAP) and the precision at rank 5 and 10 (P@5 and P@10) as the evaluation measures.
In the second phase, the supporting relevant posts are evaluated. The posts should provide diverse aspects relevant to their news headline. To assess the diversity of the supporting relevant posts, we used the -nDCG [10] and IA-Precision [1] measures.
5In [23], the non-relevant contents of a blog post means the

0.14

0.12

MAP of Query Likelihood

0.10

0.08

0.06 0.04

Query Likelihood Baseline (K=0)

0.02 30

300

600

900

1200

The number of clusters K

1500

Figure 1: The MAP scores of the query likelihood according to varying the number of clusters K. The NHLM is estimated using the RBS.

Table 1: The performances of the query likelihood estimated using the RBS and the FBS approaches for the NHLM. The number of clusters K is set to 500 for the QLM estimation.
Model MAP P@5 P@10
QLHRBS 0.1303 0.2291 0.2255 QLHF BS 0.1315 0.2473 0.2355

4.2 Results and Discussion
4.2.1 The Query Likelihood
We conducted several experiments to evaluate the performance of the query likelihood for TSIT. The aim of the experiments is to determine (1) how correctly the QLMs reflect the various topics buried in blog posts; and (2) how the proposed approaches for NHLM estimation affect the performance of the query likelihood.
The query likelihood has a few parameters, the number of clusters K for QLMs and the smoothing parameter h for NHLM. For our experiments, the smoothing parameter h is set to 2000 without further parameter tuning.
To determine how correctly the QLMs the reflect various topics buried in blog posts, we evaluated the performance of the query likelihood according to varying K values.
K : 1, 30, 50, 100, 300, 500, 1000, 1500
For the NHLM estimation, the supporting relevant posts were chosen using the RBS approach.
Figure 1 shows the MAP scores according to varying K values. We set the baseline using K = 1. This means that blog posts are modeled using a single QLM.
Compared with the baseline, the performances for all K > 1 were significantly improved, and the best performance was obtained when using K = 500. From these results, we can confirm that a single QLM cannot correctly capture the contents of the blog posts, because of the topical diversity of the blog posts. This weakness reduced its ability for identifying the top news headlines.
useless contents for the blog search, such as menu, banner and site description

399

Table 2: The performances of the news headline

prior estimated using Temporal Profiling, Term Im-

portance and their combination (2 = 0.8). The best performances are shown in bold.

Model Period MAP P@5 P@10

P RIT P

1

0.1410 0.2545 0.2691

2

0.1745 0.2982 0.3109

3 0.1800 0.3055 0.3273

P RIT I

1

0.0263 0.0400 0.0509

2

0.0448 0.1127 0.0873

3 0.0458 0.1273 0.0891

P RIT P +T I

3

0.1957 0.3673 0.3364

As the number of clusters K increased to 500, the respective topics buried in the blog post were captured by the K clusters. The QLM estimated using each cluster led to the improved performance of the query likelihood. When K  500, the clusters have been overfitted, and did not provide enough information relevant to each topic. As a result, the performance decreased.
To investigate how the proposed approaches for NHLM estimation affect the performance of the query likelihood, we measured the performances with two approaches to select the supporting relevant posts. For these experiments, we set K = 500 to estimate the QLMs.
Let QLHRBS and QLHF BS be the query likelihood using the NHLM estimated using RBS and FBS approaches, respectively. Table 1 shows the performances of QLHRBS and QLHF BS. The performances of QLHF BS are better than those of QLHF BS for all measures. These results means that the supporting relevant posts selected using FBS provide more diverse aspects of a news headline than those chosen using RBS. As a result, the performance of the query likelihood increased.
4.2.2 The News Headline Prior
We proposed two criteria to estimate the news headline prior: Temporal Profiling and Term Importance. We experimentally confirmed the usefulness of the proposed criteria to estimate the news headline prior.
First, we evaluated the performance of each approach according to varying the period . The approaches consider a certain period from a query day to gather evidence for the news headline prior. Generally, blog posts and news headlines related to events that happened on a query day are published on that day or in the following days. However, they can be published on preceding days, because of the time discrepancy described in section 4.1.2.
We defined several periods as follows:
· 1 :  is set between -1 and +1 days from Qd.
· 2 :  is set between -3 and +7 days from Qd.
· 3 :  is set between -3 and +14 days from Qd.
Let P RIT P and P RIT I be the news headline prior estimated using the Temporal Profiling and Term Importance, respectively. Table 2 shows the performances of each approach according to varying periods and those obtained from the combination of the two approaches. For the experiments, we set the parameters,  in Eq.12 and  in Eq.14, by maximizing the MAP using an exhaustive search in the following

MAP of News Headline Prior

0.20

0.18

0.16

0.14

0.12

0.10

0.08

0.06

0.04

0

0.2

0.4

0.6

0.8

1

The weighting parameter, 2

Figure 2: The Map scores of the news headline prior according to varying the parameter 2 (1 = 1).

Table 3: The performances of systems integrating

the query likelihood and the news headline prior,

QLHRBS+P RIT P+T I and QLHF BS+P RIT P+T I (1 = 0.8 and 2 = 0.8). uogTrTStimes: The best performance in TREC'09 Top Stories Identification Task,

QLHF BS: the best performance of the query likelihood, P RIT P +T I : the best performance of the news headline prior. The best performances are shown

in bold. Statistical significance at the 0.05 and 0.01

level is indicated by  and  for improvement from

the query likelihood, respectively, § and ¶ for im-

provement from the news headline prior, respec-

tively.

Model

MAP

P@5

P@10

uogTrTStimes

0.1862

0.3236

0.3127

QLHF BS P RIT P +T I QLHRBS+P RIT P+T I QLHF BS+P RIT P+T I

0.1315 0.1957 0.2081¶ 0.2124¶

0.2473 0.3673 0.4145¶ 0.4255¶

0.2255 0.3364 0.3455 0.3527§

values.
,  : 30, 40, 50, 60, 70
For both approaches, as we consider a longer period, we obtain better results. This observation confirms our assumption that if a news story is important for a given day, blog posts and news headlines relevant to it will be posted during several days. Although Temporal Profiling resulted in good performance, the performances of the Term Importance were relatively low. For event detection, the usefulness of the named entities can change depending on which circumstances they are used in [17]. We think that the poor performance of Term Importance is because we used the named entities without considering the circumstances. However, the combination of the two approaches led to the best performance ( = 50 and  = 40). Compared with the best performance of Temporal Profiling, we achieved 1.57% and 6.18% improvement in MAP and P@5, respectively. These results verify the usefulness of the named entities for identifying important news stories.
To explore the influence of two criteria when identifying the top news story, we measured the MAP score according

400

Table 4: The performances of the supporting relevant posts chosen using RBS and FBS.

Model

-nDCG@5 -nDCG@10 IA-P@5 IA-P@10

QLHRBS+P RIT P+T I QLHF BS+P RIT P+T I

0.479 0.485

0.486 0.492

0.169 0.171

0.145 0.147

to varying the weighting parameter 2 (1 = 1, i.e. we are utilizing only the news headline prior to evaluate the performance), in Figure 2. The weight parameter 2 controls the relative importance of Temporal Profiling and Term Importance as the news headline prior. The best performance was obtained when 2 = 0.8. From these results, we can again confirm that Temporal Profiling and Term Importance should be considered together to improve the performance.
4.2.3 Integration of the Query Likelihood and the News Headline Prior
Finally, we measured the performance of our system that integrates the query likelihood and the new headline prior. To integrate these components, we used QLHRBS and QLHF BS for the query likelihood, and P RIT P +T I for the news headline prior.
Table 3 shows the performances of our systems, QLHRBS+ P RIT P+T I and QLHF BS + P RIT P+T I. In addition, for comparison purposes, we reported the best performing results of the TREC-2009 Top Stories Identification Task [21], and the best performances of the query likelihood and the news headline prior. We performed the Wilconxon signed rank test to examine whether the improvement of the performance over that of each component was statistically significant.
Integrating the two components significantly improved the performance of TSIT. For the MAP, the best performance was 8.09% and 1.67% higher than those of the query likelihood and the news headline prior, respectively. Specifically, our system achieved 2.62%, 10.19% and 4.00% further increases in MAP, P@5 and P@10 over the best performance of TREC'09 TSIT.
This result implies that the performance can be improved by combining the query likelihood and the news headline prior. They reflect different characteristics that important news headlines should be satisfying. The query likelihood identifies the important news headlines based on modeling the dominant topics in blog posts, but the news headline prior identifies them using various features such as the number of relevant posts, and the importance of terms within news headlines.
Figure 3 shows the MAP scores of QLHF BS + P RIT P+T I according to varying the parameter 1 (2 = 0.8). The weight parameter 1 controls the relative importance of the query likelihood and the news headline prior. The best performance was obtained when 1 = 0.8. From these results, we can again verify that the performance for TSIT can be improved when integrating the query likelihood and the news headline prior. We do not show the graph for QLHRBS+P RIT P+T I, because it was almost identical to that of QLHF BS + P RIT P+T I.
4.2.4 Diversity of Supporting Relevant Posts
The supporting relevant posts should provide the diverse aspects relevant to a news headline. We proposed two approaches to choose the supporting relevant posts, the RBS and the FBS. We evaluated the diversity of the supporting

MAP of integrated system

0.22

0.21

0.20

0.19

0.18

0.17

0.16

0.15

0.14

0.13

0

0.2

0.4

0.6

0.8

1

The weighting parameter 
1

Figure 3: The Map scores of the integrated system according to varying the parameter 1 (2 = 0.8)

relevant posts selected by each approach and displayed the results in Table 4.
FBS performed better than RBS. We can verify that the supporting relevant posts of FBS provided more diverse aspects of a news headline than those of RBS. These results confirm that the use of blog feeds can improve the diversity of the supporting relevant posts. Furthermore, these results agree with the results from identifying the top news headlines in Table 3. That is, compared with RBS, FBS chose the supporting relevant posts that provide more correct and diverse aspects of a news headline. As a result, QLHF BS + P RIT P+T I outperformed QLHRBS + P RIT P+T I.
5. CONCLUSION AND FUTURE WORK
In this study, we presented several approaches for identifying top news stories in the blogosphere. Our system utilizes the query likelihood and the news headline prior, based on the language model framework. For the query likelihood, we proposed several approaches to estimate the QLM and the NHLM. The QLM can be estimated using blog posts issued on the query day. We divided the blog posts into K clusters so that each cluster can accurately contain one of the various topics buried in blog posts. Then, we estimated the K number of QLMs respective to their clusters. We also proposed two approaches to choose the supporting relevant posts for a news headline. The posts were also able to cover many different aspects of the news headline.
Furthermore, for the news headline prior, we suggested two criteria, Temporal Profiling and Term Importance. They measure the importance of a news headline in two different ways. Temporal profiling measures it using the temporal information of blog posts relevant to the news headline. Term Importance measures it using the meaningfulness of terms in the news headline.
We obtained the best performance for TSIT by considering the query likelihood and the news headline prior at

401

the same time. From experimental results, we can verify the the proposed approaches are effective in identifying top news headlines.
Many studies remain for future work. We used K-means clustering to model various topics buried in blog posts. It would be interesting to utilize several approaches such as PLSA [13] and LDA [4] to capture topics of blog posts. To improve the diversity of the supporting relevant posts, various ways such as MMR [6] are also worthy of research. Furthermore, we believe that various features such as comments or tags can be used to improve the performance when identifying top news stories.
6. ACKNOWLEDGMENTS
This work was supported in part by MKE & IITA through IT Leading R&D Support Project and also in part by the BK 21 Project in 2010.
7. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of WSDM 2009, pages 5­14. ACM, 2009.
[2] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. Inquery and trec-9. In Proceedings of TREC-9, pages 551­562, 2000.
[3] J. Allan, R. Papka, and V. Lavrenko. On-line new event detection and tracking. In Proceedings of SIGIR 1998, pages 37­45. ACM, 1998.
[4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993­1022, 2003.
[5] T. Brants, F. Chen, and A. Farahat. A system for new event detection. In Proceedings of SIGIR 2003, pages 330­337. ACM, 2003.
[6] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR 1998, pages 335­336. ACM, 1998.
[7] C. C. Chen, Y.-T. Chen, Y. Sun, and M. C. Chen. Life cycle modeling of news events using aging theory. In Proceedings of ECML 2003, pages 47­59, 2003.
[8] K.-Y. Chen, L. Luesukprasert, and S.-c. T. Chou. Hot topic extraction based on timeline analysis and multidimensional sentence modeling. IEEE Trans. on Knowl. and Data Eng., 19(8):1016­1025, 2007.
[9] H. L. Chieu and Y. K. Lee. Query based event extraction along a timeline. In Proceedings of SIGIR 2004, pages 425­432. ACM, 2004.
[10] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of SIGIR 2008, pages 659­666, New York, NY, USA, 2008. ACM.
[11] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1­38, 1977.
[12] Q. He, K. Chang, and E.-P. Lim. Analyzing feature trajectories for event detection. In Proceedings of SIGIR 2007, pages 207­214. ACM, 2007.
[13] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of SIGIR 1999, pages 50­57. ACM, 1999.

[14] R. Jones and F. Diaz. Temporal profiles of queries. ACM Trans. Inf. Syst., 25(3):14, 2007.
[15] J. Kleinberg. Bursty and hierarchical structure in streams. In Proceedings of SIGKDD 2002, pages 91­101. ACM, 2002.
[16] P. Kolari, A. Java, and T. Finin. Characterizing the splogosphere. In Proceedings of 3rd Annl. Workshop on Weblogging Ecosystem: Aggregation, Analysis and Dynamics, 15th Word Wide Web Conf., 2006.
[17] G. Kumaran and J. Allan. Text classification and named entities for new event detection. In Proceedings of SIGIR 2004, pages 297­304. ACM, 2004.
[18] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR 2001, pages 111­119. ACM, 2001.
[19] Y. Lee, S.-H. Na, and J.-H. Lee. An improved feedback approach using relevant local posts for blog feed retrieval. In Proceeding of CIKM 2009, pages 1971­1974. ACM, 2009.
[20] Y. Lv and C. Zhai. Positional language models for information retrieval. In Proceedings of SIGIR 2009, pages 299­306. ACM, 2009.
[21] C. Macdonald, I. Ounis, and I. Soboroff. Overview of the TREC-2009 Blog Track. In Proceedings of TREC 2009, 2010.
[22] G. Mishne and M. de Rijke. A study of blog search. In Proceedings of ECIR 2006, pages 289­301. Springer, 2006.
[23] S.-H. Nam, S.-H. Na, Y. Lee, and J.-H. Lee. Diffpost: Filtering non-relevant content based on content difference between two consecutive blog posts. In Proceedings of ECIR 2009, pages 791­795. Springer-Verlag, 2009.
[24] C. Wang, M. Zhang, L. Ru, and S. Ma. Automatic online news topic ranking using media focus and user attention based on aging theory. In Proceeding of CIKM 2008, pages 1033­1042. ACM, 2008.
[25] Y. Yang, T. Pierce, and J. Carbonell. A study of retrospective and on-line event detection. In Proceedings of SIGIR 1998, pages 28­36. ACM, 1998.
[26] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. Topic-conditioned novelty detection. In Proceedings of SIGKDD 2002, pages 688­693. ACM, 2002.
[27] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, 2004.
[28] K. Zhang, J. Zi, and L. G. Wu. New event detection based on indexing-tree and named entity. In Proceedings of SIGIR 2007, pages 215­222. ACM, 2007.
[29] Y. Zhang, J. Callan, and T. Minka. Novelty and redundancy detection in adaptive filtering. In Proceedings of SIGIR 2002, pages 81­88. ACM, 2002.

402

Proximity-Based Opinion Retrieval

Shima Gerani
University of Lugano Faculty of Informatics Lugano, Switzerland
shima.gerani@usi.ch

Mark J. Carman
University of Lugano Faculty of Informatics Lugano, Switzerland
mark.carman@usi.ch

Fabio Crestani
University of Lugano Faculty of Informatics Lugano, Switzerland
fabio.crestani@usi.ch

ABSTRACT
Blog post opinion retrieval aims at finding blog posts that are relevant and opinionated about a user's query. In this paper we propose a simple probabilistic model for assigning relevant opinion scores to documents. The key problem is how to capture opinion expressions in the document, that are related to the query topic. Current solutions enrich general opinion lexicons by finding query-specific opinion lexicons using pseudo-relevance feedback on external corpora or the collection itself. In this paper we use a general opinion lexicon and propose using proximity information in order to capture opinion term relatedness to the query. We propose a proximity-based opinion propagation method to calculate the opinion density at each point in a document. The opinion density at the position of a query term in the document can then be considered as the probability of opinion about the query term at that position. The effect of different kernels for capturing the proximity is also discussed. Experimental results on the BLOG06 dataset show that the proposed method provides significant improvement over standard TREC baselines and achieves a 2.5% increase in MAP over the best performing run in the TREC 2008 blog track.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Experimentation, Performance
Keywords
Opinion, Sentiment, Blog, Retrieval, Proximity
1. INTRODUCTION
Blog post opinion retrieval is the problem of finding blog posts that express opinion about a given query topic. This
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

problem was introduced in the Text REtrieval Conference (TREC) 2006 blog track and continued to 2008 [15, 12, 16]. The proposed approaches mostly follow a three step framework. In the first step, traditional IR is used to find documents that are relevant to the query. In the second step, opinion scores are generated for the relevant documents. Finally, a ranking method is used to rank documents according to their relevance and opinionatedness about the query.
Blog post opinion retrieval faces two main challenges. The first challenge is to find the best way to combine relevance and opinion scores to produce a single ranking. In previous work, researchers mostly used linear combinations of relevance and opinion scores [16]. We use a probabilistic approach and propose a simple model for combining probabilities of relevance and opinionatedness about a query.
The second challenge is assigning query-related opinion scores to documents. The problem is how to identify opinion expressions in the document that are directed at the concepts in the query. Simple averaging over the opinion weights of terms or sentences in a document to generate an opinion score is not an optimal approach. The reason is that documents can be relevant to many different topics at the same time but the opinion being expressed in them may be directed towards topics other than the query. So we need an opinion finding method that takes the query into account and ignores opinionated content that is not related to the query. In this paper we propose using proximity-based density functions to model the notion of query-relatedness for opinionated content. Our aim is to see how much improvement can be achieved using proximity information alone without the need for query-specific opinion-lexicon. Our contributions are:
· Presenting a novel probabilistic opinion retrieval model that is based on proximity between opinion lexicons and query terms.
· Investigating different ways of estimating the relevance probability of documents from their relevance scores.
· Investigating the impact of different types of proximity functions in our model.
We evaluate our model on the BLOG06 collection using five standard TREC 2008 baselines. We report significant improvements over these strong TREC baselines and over non proximity-based opinion retrieval scores in the experiments. The results show the effectiveness of utilizing the simple proximity information in enhancing opinion retrieval, compared to systems which utilize components such as query

403

expansion and query-specific opinion-lexicon refinement via pseudo-relevance feedback.
2. RELATED WORK
Research on opinion mining and sentiment analysis started mostly on review-type data with the intention to classify documents as expressing either a positive or negative opinion. The proposed approaches can be categorized into two main groups: lexicon-based [21, 20, 24] and classificationbased [17, 1, 13, 5]. Both of these approaches rely on word occurrences. The first approach (lexicon based), uses a manually or automatically built list of subjective words, such as `good' and `like', and assumes that the presence of these words in a document is the evidence of document opinionatedness. A term's opinion score can be used in different ways to assign an opinion score to the whole document. The second approach (classification-based) utilizes word occurrence and sometimes linguistic features and builds a classifier based on positive (opinionated) and negative (nonopinionated) documents using Machine Learning techniques. Nevertheless, most of the early research in this area neglected the problem of retrieving documents that are related to the topic of the user's interest. It also did not target the problem of ranking opinionated documents according to the degree with which they are opinionated (either in positive or negative way). Relevance of an opinion to a topic was considered for the first time in Yi et. al [24] and then in Hurst and Nigam's work [7] but they did not consider the ranking of documents. Instead they only classified documents as to whether they expressed an opinion about the topic. Opinion ranking was considered for the first time in Eguchi and Lavrenko's work [2].
In TREC 2006, the Opinion Retrieval task appeared in the Blog track. It considered the opinion mining problem from an Information Retrieval perspective and introduced the problem of retrieving and ranking blog posts that were relevant and opinionated about a given topic (query) [15]. There has been lots of research on blog opinion retrieval in TREC [15, 12, 16] and other conferences [26, 25, 19] in which people follow the opinion retrieval definition used in the TREC blog track. Following the categorization mentioned earlier, the proposed methods belong to two classes of lexicon-based [23, 25, 6, 9, 19] and classification-based [26, 8] approaches and usually follow the three-step framework mentioned earlier.
In this paper we follow the TREC opinion retrieval problem definition. We follow the lexicon-based approach in opinion finding and focus on the problem of finding topic related opinion expressions. In the rest of this section we explain in greater detail relevant previous work in handling the opinion retrieval challenge.
2.1 Capturing Topic Related Opinion Expression
Since the aim of opinion retrieval is to find documents that express an opinion about the query, unrelated opinion should not be considered in the scoring of a document. Therefore, the main challenge in opinion finding is to score documents by opinion expressions that refer to the query. In previous work, researchers followed two orthogonal approaches. In the first approach they built a query-specific opinion lexicon by starting from a general opinion lexicon and refining the opinion weights of terms in the lexicon via feedback style

learning on the top retrieved documents in response to the query [9, 14].
The second approach uses the proximity of subjective terms or sentences to the topic terms as a measure of relatedness [1, 26, 25, 19, 22]. Dave et al. [1] tried to capture proximity using higher order n-grams as units to represent text. However, n-grams cannot capture the dependency of nonadjacent terms. Although such dependencies can be captured by increasing the length of the n-gram, this can be impractical due to a lack of sufficient training data. Zhang et al. [25] calculate the proximity of opinion terms to query terms by computing the probability of query term and opinion term co-occurrence within a window. Vechtomova [22] considered the distance in the number of non-stopwords between a query term and subjective lexical units occurring within the window of n words around the query term. Although they considered proximity information in their models, Zhang et al. [25] did not find any advantage of using the proximity information while Vechtomova [22] did show some improvement in terms of opinion MAP. In this paper we introduce the proximity information in a more principled way and show that it can improve the performance over a non proximity-based opinion retrieval baseline.
Proximity information is also considered in [26, 19], with the difference being that they first find opinionated sentences and then consider proximity of opinionated sentences to the query term. Zhang et al. [26] use a SVM classifier to classify document's sentences as either opinionated or non-opinionated. They then apply a NEAR operator to classify an opinionated sentence as either relevant or not relevant to the query. Santos et al. [19] use a divergence from randomness proximity model to integrate the proximity of query terms to the opinionated sentences identified by a general opinion finding system. They further combine the proximity scored opinion sentences by the relevance score of the document using a linear combination. Our work is similar to this method in the sense that we also use a general opinion lexicon without refining it with query specific opinion terms, but our method differs in that we do not work on the sentence level but use the opinion weights and proximity of terms to the query directly. We also consider a proximity-based opinion density functions to capture the proximity information that has not been used in previous studies in opinion retrieval. The way we incorporate the relevance score in our model is also different from the previous studies in that we investigate different ways of estimating the relevance probability from the document's relevance score.
2.2 Combining Relevance and Opinion Scores
In order to produce a final ranking of documents by the degree of relevance and opinionatedness toward a query, previous works linearly combined the opinion and relevance scores without theoretical justification. In [25], Zhang et al. proposed a formal generative model for opinion retrieval that considers the relevant score as a weight for the opinion score of a document. Although their proposed model proved to be effective compared to previous work, it failed to take advantage of the component of the model that aimed to capture topic related opinion expression through proximity. The other shortcoming of their model is that it treats all opinion terms in the lexicon equally, while it is natural to think that some terms are more indicative of an opinion than others.

404

In this paper we propose a novel probabilistic method that considers the opinionatedness of terms in the lexicon together with its relatedness to the query. We will show the effectiveness of our proposed method in the experimental section.
3. TOPIC RELATED OPINION RETRIEVAL
Blog post opinion retrieval aims at developing an effective retrieval function that ranks blog posts according to the likelihood that they are expressing an opinion about a particular topic. We follow the typical generative model in Information Retrieval that estimates the likelihood of generating a document given a query, p(d|q). In opinion retrieval, we also need to estimate the probability of generating an opinion about the query. We introduce the random variable o which denotes the event that the document expresses an opinion about the query. Thus, for opinion retrieval we can rank documents by their likelihood given the query and opinion, p(d|o, q). We then factorize this probability as follows:
p(d|o, q)  p(d, o, q) = p(d)p(q|d)p(o|q, d) (1)
As was also mentioned in [25], we can see two components in this formula: p(d)p(q|d) which considers the relevance of document to the query, and p(o|q, d) which deals with its "opinionatedness". The relevance probability can be estimated using any existing IR method such as language models [18] or classical probabilistic models [4]. The difference in our model is in the second component, p(o|q, d), that is the opinion score of the document. We propose using a proximity-based estimate as a measure of opinion relatedness to the query.
In the remainder of this section we first explain the non proximity-based method for calculating the opinion score of the document. We then explain our proposed proximitybased opinion scoring.
3.1 Non-Proximity Opinion Score
The first studies on opinion retrieval assumed conditional independence between o and q given the document d. So, p(o|q, d) in those models was calculated as p(o|d). Such models assume that each document discusses only one topic and so if a document is relevant to a query, all opinion expressions in the document are about the query. In order to calculate a non proximity-based (general) opinion score for a document, we can simply calculate the average opinion score over all terms in the document:

X

p(o|q, d) = p(o|d) = p(o|t)p(t|d)

(2)

td

where p(t|d) = c(t, d)/|d| is the relative frequency of term t in document d and p(o|t) shows the probability of opinionatedness of the term.

3.2 Proximity Opinion Score
The assumption that a document is only relevant to a single topic and that all opinion expressions are about that topic is overly simplistic. In fact, a document can be relevant to multiple topics and just be opinionated about one of them. Therefore, for assigning opinion scores to documents, we need to identify opinion expressions that are directed toward the query topic. One possible approach is to find opinion lexicons that are mostly used to express opinion about the

opinion density

0.0018
0.0016
0.0014
0.0012
0.001
0.0008
0.0006
0.0004
0.0002
0 100 200 300 400 500 600 700 800 900
document positions
Figure 1: Example of Opinion density at different positions of a document
query topic. For instance, the word "delicious" may be used more for expressing opinion about a food type query than an electronic product. Having access to query-related opinion lexicons, we can either ignore the word delicious or give it a low weight if the query is about electronics. For example Na et al. [14] used an opinion lexicon refinement via pseudo relevance feedback in order to build a query-related opinion lexicon.
Another approach is to use the documents' structure. In this approach the distance of an opinion term to the query term is used as a measure of their relatedness. Accordingly, we assume that an opinion term refers with higher probability to the terms closer to its position. On the other hand, opinion terms can refer not only to the entities they proceed or follow, but also to the entities which may be a couple of words, or even sentences, before or after. Bi-gram or tri-gram models have limitations in capturing such dependencies between opinion and topic terms. In order to model this dependency, we propose considering proximitybased density kernels, centered at each opinion term, which favor positions closer to the opinion term's position. As a kernel we can use any non-increasing function of the distance between the position of an opinion term and any other position in a document [10]. We weight this kernel by the probability of opinionatedness of the term. Therefore, the opinion density at each position in the document is the accumulated opinion density from different opinion terms at that position. We define this accumulated probability to be the probability of the opinion expressed in the document about the term at that position. Figure 1 shows the opinion density at different positions in a sample document.
In order to present our model more formally, we first introduce some notation. We denote a document with the vector d = (t1, ..., ti, ..., tj, ..., t|d|) where the subscripts i and j indicate positions in the document and ti indicates the term occurring at the position i. To find the opinion probability at i, we calculate the accumulated opinion probability from all positions of the document at that position. So, for every position j in a document we consider the opinion weight of the term at that position which we denote by p(o|tj), and we

405

All Distributions

0.7

Gaussian

0.6

Laplace Triangular

Cosine

0.5

Circle Rectangular

0.4

0.3

0.2

0.1

0

-3

-2

-1

0

1

2

3

Distance

Figure 2: Proximity kernel functions with the same variance.

weight it by the probability that the term at that position j is about the query term at position i. We represent this probability by P (j|i, d) and calculate it as follows:

k(j, i)

p(j|i, d) =

P|d|
j =1

k(j

, i)

(3)

here k(i, j), is the kernel function which determines the weight of propagated opinion from tj to ti. Thus the probability of opinion at position i in the document can be estimated as:

|d|

X

p(o|i, d) = p(o|tj)p(j|i, d)

(4)

j=1

In the rest of this section we present the different kernels used in our experiments. We investigate the five different density functions used in [10], namely the Gaussian, Triangular, Cosine, Circle and Rectangular kernel. We also present Laplace kernel as an additional kernel in our experiments. Figure 2 shows the different kernels all with the same variance.
In the following formulas, we present normalized kernel functions with their corresponding variance formula.
1. Gaussian Kernel

1

» -(i - j)2 ­

k(i, j) =  exp

(5)

2

22

2. Laplace Kernel

k(i, j) = 1 exp » - |i - j| ­

2b

b

(6)

where 2 = 2b2

3. Triangular Kernel

k(i,

j)

=

(
1
a

" 1

-

|i-j| "
a

if |i - j|  a

0

otherwise

(7)

where 2 = a2 6

4. Cosine Kernel

k(i,

j)

=

(

1 2s

h 1

+

cos

"

|i-j|. s

"i

if |i - j|  s

0

otherwise (8)

where 2 = s2 ,, 1 - 2 « 3 2

5. Circle Kernel

k(i,

j)

=



2 r2

pr2

-

(i

-

j)2

if |i - j|  r

0

otherwise

(9)

where 2 = r2

4

6. Rectangular Kernel

1

k(i, j) =

2a
0

if |i - j|  a otherwise

(10)

where 2 = a2

3

As one baseline, we compare proximity kernels to the uniform kernel which gives the same importance to all positions in the document and simulates the non proximitybased opinion retrieval presented in section 3.1. Our aim is to investigate whether it is better to use kernels which favor opinion occurrence in close proximity of query term or not.

3.3 Probability of Document Opinionatedness about the Query
Now that we can compute the probability of opinion at each position in the document, we need to calculate an overall probability that the document is expressing an opinion about the query, p(o|d, q). In the following we will suggest different ways for calculating this probability.

|d|

X

p(o|d, q) =

p(o, i|d, q)

i=1

|d|

X

=

p(o|i, d, q)p(i|d, q)

(11)

i=1

We assume that o and q are conditionally independent given the position in the document, oq|(i, d). Thus p(o|i, d, q) reduces to p(o|i, d) which can be estimated using methods proposed in the section 3.2. Here we suggest different methods for estimating p(i|d, q), the probability of position i given the query q and the document. One method assumes that all query terms' positions in the document are equally important. Thus we have:

1 p(i|d, q) = |pos(q)|

if ti  q

(12)

0 otherwise

Here pos(q) is the set of all query terms' positions in the document. We can then calculate p(o|d, q) as follows:

1X

p(o|d, q) =

p(o|i, d)

(13)

|pos(q)|

ipos(q)

As an alternative, we can assume that only the query term position where p(o|i, d) is maximum is important. Thus:

p(o|q, d) = maxipos(q)p(o|i, d)

(14)

406

This approach is similar to first ranking the passages in the document by their degree of opinionatedness and then choosing the most relevant passage for scoring the document. We refer the first and second methods, avg and max in the experimental section.
We also considered placing a density kernel over each query term position in the document. Since the resulting formula for calculating p(o|q, d) was cubic, the approach was not efficient. It also didn't improved the performance greatly.

3.4 Smoothed Proximity Model
The proximity-based estimate can be further refined by smoothing it with the non proximity-based estimation as follows:

p(o|q, d) = (1 - )p(o|q, d) + p(o|d)

(15)

Smoothing the proximity model with the non-proximity score lets us capture the proximity at different ranges. This can be useful because there are some documents in which the exact query term occurs rarely. In such documents opinion expressions refer to the query indirectly through anaphoric expressions such as he, she, it, the film, etc. Since we don't do any query expansion or reference resolution in our model, we investigate whether smoothing the proximity model with the non-proximity score helps us capture further related opinion expressions in the document.

4. EXPERIMENTAL SETUP
In this section we explain our experimental setup for evaluating the effectiveness of the proposed methods.
Test Collection.
Our experiments are based on the BLOG06 collection and the set of 150 topics for the blog post opinion retrieval task in TREC 2006 through 2008 and their corresponding relevance assessments. The relevance assessments provide information about whether a given blog post is relevant to a topic and also reflects the post opinionatedness nature. Each topic contains three fields of title, description and narrative. We extracted query terms from the title field of a topic. Each permalink component was indexed as a retrieval unit. The preprocessing of the collection was minimal and involved only stopword removal.
In order to be able to compare with TREC 2008 participants, we used 100 topics from TREC 2006 and TREC 2007 numbered 851 to 950 as our training set and 50 topics from TREC 2008, numbered 1001 to 1050, for testing.
Opinion Lexicon.
In our experiments we used the opinion lexicon that was proposed in [9], since it has been shown to be effective in TREC 2008. This lexicon was made using sentiWordNet [3] and an automatically learned model from the Amazon.com product review and specification corpus. The opinion lexicon only contains words from the review corpus that are also present in sentiWordNet (i.e. the intersection of the two word sets). The opinion lexicon model gives us the probability of subjectiveness of each word, p(sub|w), which we use as the probability of opinionatedness (subjectivity) of the word in our model.

Retrieval Baselines.
In order to facilitate direct comparison between systems in TREC 2008 five relevance retrieval baselines were provided by the TREC organizers, selected from the best performing retrieval runs. Each of these baselines covers all 150 topics and contains a list of relevant documents to those topics. Note that the baseline TREC runs are purely relevance retrieval and not opinion retrieval systems. They score relatively well at opinion retrieval simply because the majority of blog posts that are relevant to a topic are also expressing an opinion about it. When evaluating opinion retrieval systems therefore, one must compare the Mean Average Precision (MAP) score of the opinion retrieval system with the MAP (for opinionated posts) of the baseline system. A recent study showed that it is very difficult to improve opinion retrieval performance over a strong baseline on the Blog06 collection[11]. In the following experiments we show the effectiveness of the proposed method in improving the opinion retrieval performance over the best baselines in TREC 2008.
Evaluation.
We used the opinion relevance judgements provided by TREC for evaluation. We report the MAP as well as RPrecision (R-Prec), binary Preference (bPref), and Precision at 10 documents (P@10) in terms of opinion finding.
Throughout our experiments we used the Wilcoxon signedrank matched pairs test with a confidence level of 0.01 level for testing statistical significance.

5. EXPERIMENTAL RESULTS
In this section we explain the experiments that we conducted in order to evaluate the usefulness of different setting of the proposed method.

5.1 Normalizing Relevance Scores
In order to use the standard TREC baselines in the relevance retrieval component of our model, we need to estimate the probability of relevance of each document to the query. The TREC baselines provide us with relevance score of the document, not the relevance probability. In order to estimate the probability of relevance of a document we investigated different normalization techniques for transforming the relevance score into a probability estimate. The easiest transformation is to use the relevance score directly (rank equivalent to dividing the score by the sum of all document scores for the same query). We also tried normalizing the relevance score using the minimum, min(score)q, maximum max(score)q, mean mean(score)q and standard deviation stdev(score)q of document's scores for the query q:

N 1 = score - min(score)q

(16)

max(score)q - min(score)q

N 2 = score - mean(score)q

(17)

stdev(score)q

In addition, we experimented with Logistic Regression for learning a transformation from relevance scores to probability estimates. We trained the model using the relevance judgements from the training set. We used variations on the score or rank of the documents in the TREC baselines as a feature for logistic regression. Thus the model we used to estimate the relevance probability of a document in each

407

8

16

32

64

128

Score 0.3262 0.3355 0.3364 0.3323 0.3304

N1 0.3641 0.3701 0.3708 0.3685 0.3662

N2 0.3713 0.3732 0.3740 0.3720 0.3709

LRS 0.3205 0.33 0.3307 0.3272 0.3254

LRLS 0.3216 0.3289 0.3285 0.3248 0.3228

LRN1 0.331 0.3402 0.3421 0.3395 0.3371

LRN2 0.2966 0.3048 0.3055 0.3012 0.2969

LRR 0.3324 0.33

0.3291 0.3297 0.3302

LRLR 0.3613 0.3672 0.3688 0.3682 0.3673

Table 1: MAP over TREC baseline4 using laplace kernel with different sigma values. Rows show MAP using different relevant probability estimation methods. An uparrow() and downarrow() indicate statistically significant increase and decrease over using the score directly.

8

16

32

64

128

avg 0.3526 0.3573 0.3617 0.3642 0.3668

max 0.3713 0.3732 0.3740 0.3720 0.3709

Table 2: MAP over TREC baseline4 with different opinion scoring method for laplace kernel with different sigma values. A star() indicates statistically significant improvement over avg method

0.38 0.375

laplace gaussian
triangle

cosine circle
square

uniform

0.37

MAP

0.365

baseline was the following:

e+.x

p(d is relevant|TREC baselinej ) = 1 + e+.x

(18)

Where x is one of the normalized scores, the rank or the log of the rank (or score) of document d. In order to estimate this probability, we learn values for  and . We used the logistic regression implementation provided in LingPipe1, the TREC 2006 topics, and the set of relevant and non-relevant documents for learning these parameters.
Table 1 shows the opinion retrieval performance of our proposed system, using different probability estimation methods on TREC baseline 4. We report the results for exponentially increasing values of sigma. In this table LRS, LRLS, LRN1 and LRN2, LRR and LRLR denote logistic regression using the score, log of the score, normalized score using equation 16 and 17, using rank of documents instead of score and log of rank as the explanatory variable respectively. As can be seen form Table 1, N1, N2 and LRLR have the highest MAP over all sigma values on TREC baseline 4 and the improvement over the score is statistically significant, but there is no statistically significant difference between these three methods. We chose N2 for TREC baseline 4 as it had the highest MAP over training topics. For the other baselines, we found that LRLR performed best on baseline 1,3 and 5, and N2 on baseline 2.

5.2 Probability of Document Opinionatedness about the Query
In section 3.3 we proposed two different techniques for calculating the relevant opinion probability. Table 2 shows the result of using avg and max techniques for different sigma values. The results show that the max is statistically better than the avg method. Therefore, we use the max method in the rest of our experiments.

5.3 Parameter Selection for the Proximity-based Relevant Opinion Model
In section 3.2 we proposed a proximity-based opinion propagation method in which a proximity kernel is considered around each opinion term occurrence position in the document. The opinion density at a query term position is then calculated by counting the accumulated opinion density from
1http://alias-i.com/lingpipe/

0.36

0.355 0

20 40 60 80 100 120 sigma

Figure 3: Parameter sweep for the kernel functions

different opinion terms at that position. In this way, a query term which occurs at a position close to many opinionated terms will receive high opinion density.
The proposed relevant opinion model has two parameters: the type of kernel function and its bandwidth parameter  which adjusts the scope of opinion propagation (referencing) over the document. Performance of different kernels on training topics using the best parameter for each kernel is reported in Table 3. The result shows that all proximity kernels improve significantly over the non-proximity baseline, but there is no statistically significant difference between different proximity kernels when using the best parameters for each kernel. Fig. 3 reports the sensitivity (in terms of MAP) of the different kernels to different values of  parameters ranging from 2 to 128. Although there was no statistically significant difference between kernels, the Laplace kernel has the most effective and stable MAP over different parameter settings. Thus, we used it as the proximity kernel for our system evaluation on the test query set.

kernel  MAP R-prec bPref p@10

Laplace 22 0.3744 0.4113 0.4305 0.6200

Gaussian 26 0.3730 0.4099 0.4305 0.6160

Cosine 24 0.3729 0.4095 0.4305 0.6170

Triangle 24 0.3728 0.4086 0.4302 0.6180

Square 28 0.3728 0.4100 0.4300 0.6130

Circle

16 0.3723 0.4080 0.4298 0.6120

Uniform  0.3606 0.4011 0.4231 0.6190

Table 3: The performance of proximity-based opinion retrieval for the best  for each kernel.  indicates statistically significant improvement over constant kernel.

408

kernel   MAP R-prec bPref p@10

Laplace 0.4 12 0.3775 0.4166 0.4325 0.6400

Gaussian 0.6 4 0.3772 0.4147 0.4317 0.6360

Triangle 0.5 4 0.3764 0.4121 0.4317 0.6420

Cosine 0.6 4 0.3762 0.4142 0.4318 0.6390

Circle

0.7 4 0.3764 0.4167 0.4333 0.6360

Square 0.4 18 0.3757 0.4092 0.4326 0.6230

Table 4: The performance of Opinion Mixture model for the best  and  for each kernel.

5.4 Parameter Selection for the Smoothed Proximity Model
We now report our experimental results in finding the best parameters for the smoothed proximity model presented in section 3.4. This model has three parameters: kernel type,  and the  parameter which is the interpolation coefficient. In order to find the best parameters, we tried different  values for each  value in the range of [2,128]. Table 4 reports the performance of different kernels using the best  and  parameter pairs for each kernel. It shows that, interpolating the proximity score with the no proximity opinion score improves the performance. It also shows that there is no statistically significant difference between kernels when the proximity score is interpolated with the general opinion score of the document. Here again, we choose the Laplace kernel for the evaluation on the test set.

basline1 noprox laplace laplaceInt baseline2 noprox laplace laplaceInt basline3 noprox laplace laplaceInt baseline4 noprox laplace laplaceInt baseline5 noprox laplace laplaceInt

MAP 0.3239 0.3751 0.3960 0.4020 0.2639 0.2791 0.2881 0.2886 0.3564 0.3819 0.3989 0.4043 0.3822 0.4129 0.4267 0.4292 0.2988 0.2918 0.3188 0.3223

R-prec 0.3682 0.4154 0.4369 0.4412 0.3145 0.3299 0.3401 0.3411 0.3887 0.4188 0.4369 0.4389 0.4284 0.4460 0.4545 0.4578 0.3524 0.3455 0.3732 0.3785

bPref 0.3514 0.4082 0.4291 0.4326 0.2902 0.3066 0.3166  0.3166 0.3677 0.4075 0.4207 0.4247 0.4112 0.4368 0.4472 0.4485 0.3395 0.3497 0.3698 0.3715

p@10 0.5800 0.6720 0.6860 0.6920 0.5500 0.5740 0.5820 0.5860 0.5540 0.6400 0.6600 0.6660 0.6160 0.6880 0.7080 0.7140 0.5300 0.5980 0.6080 0.6120

Table 5: Opinion finding MAP results over five standard TREC baselines using different proximity methods for TREC 2008 topics. A star() and dagger() indicate statistically significant improvement over the relevance and non-proximity opinion retrieval baselines respectively.

5.5 Experimental Results on the Test Query Set
In this section, we present the evaluation results of our approaches on the TREC 2008 query topics. Table 5 presents the retrieval performances of the proposed methods over the five standard TREC baselines in terms of opinion retrieval. We also compare the performance of the proposed techniques to the no proximity opinion retrieval method. In Table 5, we show results for the non-proximity method (noprox), the laplace kernel (laplace) and the smoothed model using the laplace kernel (laplaceInt). Table 5 shows that the proposed methods are consistently effective across all five standard TREC baselines.
We also performed per topic analysis of performance for 2008 topics. The results showed that the proposed opinion retrieval methods improves Average Precision of 40 out of 50 test topics over TREC baseline 4. Topics which achieved the highest improvement over the baseline are mostly single word topics such as topic 1001 (Carmax) and topic 1023 (Yojimbo). They performed poorly on topics with multiple terms such as topic 1013 (Iceland European Union). The reason is that topics with multiple terms usually define a concept which is different from each single term in the query. Thus a more precise model for capturing the occurrence of all query terms and their proximity is required to handle such queries.
Finally we compare our proposed approaches with the best runs at TREC 2008 blog track and report the comparison result in table 6 and table 7. Table 6 shows the performance of our proposed methods on the standard TREC baseline4, comparing to the best TREC run and the later proposed method in [22](KLD+dist-FD-FV-subj-b4) on the same baseline. Interestingly, both proposed methods outperform the best reported results in TREC (B4PsgOpinAZN)

Run laplaceInt laplace KLD+dist-FD-FV-subj-b4 B4PsgOpinAZN

Map 0.4292 0.4267 0.4229 0.4189

 MAP 12.30% 11.64% 10.65% 9.60%

Table 6: Opinion finding results for best runs on standard baseline 4, ranked by Mean  MAP using TREC 2008 new topics

and KLD+dist-FD-FV-subj-b4. B4PsgOpinAZN is based on a query specific lexicon which is built via feedback-style learning. KLD+dist-FD-FV-subj-b4 uses wikipedia for finding different facets in the query and query expansion. It then used Kullback-Leibler divergence to weight subjective units occurring near query terms. The distance of the query term to the subjective units is also considered in this model.
Table 7 reports the mean MAP and the mean of their relative improvements over the five standard baselines (MAP). We observe that the proposed methods have the highest mean of MAP and mean of MAP across the five standard baselines. This indicates that the proposed methods are effective and stable across different relevance retrieval baselines.
6. CONCLUSION AND FUTURE WORK
In this paper, we proposed a novel probabilistic model for blog opinion retrieval. We focused on the problem of opinion topic relatedness and we showed that using proximity information of opinionated terms to query terms is a good indicator of opinion and query-relatedness. We studied the parameter selection for our model and we have shown that when kernels are compared using the best parameter for

409

Run
laplaceInt laplace uicop1bl1r B1PsgOpinAZN

Map Mean 0.3693 0.3657 0.3614 0.3565

stdev 0.06 0.06 0.04 0.05

 MAP Mean stdev 13.41% 6.38% 12.33% 5.94% 11.76% 6.93% 9.67% 0.77%

Table 7: Opinion finding results for best runs using all five standard baselines, ranked by Mean  MAP using TREC 2008 new topics

each, there is no statistically significant difference between them. We proposed using Laplace kernel as it was more stable on different parameter values. We also analyzed the effect of normalizing the relevance score before applying it in the model. Our results show that normalization can be important, and that the best normalization strategy is dependent on the underling relevance retrieval baseline.
We have evaluated the proposed method on the BLOG06 collection. The proposed model was shown to be effective across five standard relevance retrieval baselines. It achieved the highest improvement over the best standard TREC baseline (baseline 4), comparing to other reported results on the same baseline.
For future work we plan to investigate the effect of using reference resolution techniques on the performance of the proposed method.
7. ACKNOWLEDGMENTS
We thank Seung-Hoon Na from the KLE group of Pohang University of Science and Technology for providing the opinion lexicon. This research was partly funded by the "Secr´etariat d'´etat a` l'E´ducation et a` la Recherche (SER)" and COST Action IC0702 "Combining Soft Computing Techniques and Statistical Methods to Improve Data Analysis Solutions".
8. REFERENCES
[1] K. Dave, S. Lawrence, and D. M. Pennock. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of WWW '03, pages 519­528, 2003.
[2] K. Eguchi and V. Lavrenko. Sentiment retrieval using generative models. In Proceedings of EMNLP'06, pages 345­354, 2006.
[3] A. Esuli and F. Sebastiani. Sentiwordnet: A publicly available lexical resource for opinion mining. In Proceedings of LREC '06, pages 417­422, 2006.
[4] N. Fuhr. Probabilistic models in information retrieval. Proceedings of Comput. J., 35(3):243­255, 1992.
[5] M. Gamon. Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis. In COLING '04, page 841, 2004.
[6] B. He, C. Macdonald, I. Ounis, J. Peng, and R. L. Santos. University of glasgow at TREC 2008: Experiments in blog, enterprise, and relevance feedback tracks with terrier. In Proceedings of TREC'08, 2008.
[7] M. Hurst and K. Nigam. Retrieving topical sentiments from online document collections. In Document Recognition and Retrieval XI, pages 27­34, 2004.

[8] L. Jia, C. T. Yu, and W. Zhang. UIC at TREC 2008 blog track. In Proceedings of TREC'08.
[9] Y. Lee, S.-H. Na, J. Kim, S.-H. Nam, H.-Y. Jung, and J.-H. Lee. KLE at TREC 2008 blog track: Blog post and feed retrieval. In Proceedings of TREC'08, 2008.
[10] Y. Lv and C. Zhai. Positional language models for information retrieval. In SIGIR '09, pages 299­306, 2009.
[11] C. Macdonald, B. He, I. Ounis, and I. Soboroff. Limits of opinion-finding baseline systems. In SIGIR '08, pages 747­748, 2008.
[12] C. Macdonald, I. Ounis, and I. Soboroff. Overview of the TREC-2007 blog track. In Proceedings of TREC'07, 2007.
[13] T. Mullen and N. Collier. Sentiment analysis using support vector machines with diverse information sources. In Proceedings of EMNLP'04, pages 412­418, 2004.
[14] S.-H. Na, Y. Lee, S.-H. Nam, and J.-H. Lee. Improving opinion retrieval based on query-specific sentiment lexicon. In ECIR '09, pages 734­738, 2009.
[15] I. Ounis, M. de Rijke, C. Macdonald, G. Mishne, and I. Soboroff. Overview of the TREC-2006 blog track. In Proceedings of TREC'06, 2006.
[16] I. Ounis, C. Macdonald, and I. Soboroff. Overview of the TREC-2008 blog track. In Proceedings of TREC'08, 2008.
[17] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of EMNLP '02, pages 79­86, 2002.
[18] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of SIGIR '98, pages 275­281, 1998.
[19] R. L. Santos, B. He, C. Macdonald, and I. Ounis. Integrating proximity to subjective sentences for blog opinion retrieval. In Proceedings of ECIR'09, pages 325­336, 2003.
[20] P. D. Turney. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In ACL '02, pages 417­424, 2002.
[21] P. D. Turney and M. L. Littman. Measuring praise and criticism: Inference of semantic orientation from association. ACM Trans. Inf. Syst., 21(4):315­346, 2003.
[22] O. Vechtomova. Facet-based opinion retrieval from blogs. Inf. Process. Manage., 46(1):71­88, 2010.
[23] K. Yang. WIDIT in TREC 2008 blog track: Leveraging multiple sources of opinion evidence. In Proceedings of TREC'08, 2008.
[24] J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In ICDM '03, page 427, 2003.
[25] M. Zhang and X. Ye. A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval. In SIGIR '08, pages 411­418, 2008.
[26] W. Zhang, C. Yu, and W. Meng. Opinion retrieval from blogs. In CIKM '07, pages 831­840, 2007.

410

A Content based Approach for Discovering Missing Anchor Text for Web Search

Xing Yi and James Allan
Center for Intelligent Information Retrieval Computer Science Department
University of Massachusetts, Amherst, MA, USA
{yixing,allan}@cs.umass.edu

ABSTRACT
Although anchor text provides very useful information for web search, a large portion of web pages have few or no incoming hyperlinks (anchors), which is known as the anchor text sparsity problem. In this paper, we propose a language modeling based technique for overcoming anchor text sparsity by discovering a web page's plausible missing anchor text from its similar web pages' in-link anchor text. We design experiments with two publicly available TREC web corpora (GOV2 and ClueWeb09) to evaluate different approaches for discovering missing anchor text. Experimental results show that our approach can effectively discover plausible missing anchor terms. We then use the web named page finding task in the TREC Terabyte track to explore the utility of missing anchor text information discovered by our approach for helping retrieval. Experimental results show that our approach can statistically significantly improve retrieval performance, compared with several approaches that only use anchor text aggregated over the web graph.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval­Search process,Retrieval models;H.3.5 [Information Storage and Retrieval]:Online Information Services­ Web-based services
General Terms: Algorithms, Experimentation
Keywords: anchor text, anchor text sparsity, language models, relevance models, content similarity, web search
1. INTRODUCTION
There are rich dynamic human generated hyperlink structures on the web. Most web pages contain some hyperlinks, referred to as anchors, that point to other pages. Each anchor consists of a destination URL and a short piece of text, which is called anchor text. Anchors play an important role in helping web users conveniently navigate to their interested web information. Although some anchor text only functions as a navigational shortcut which does not have direct semantic relation to the destination URL (e.g.,"click
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

# of web pages # of pages having inlinks # of pages having original
or enriched inlinks[14]

GOV2
25,205,179 376,121 (1.5%) 977,538 (3.9%)

ClueWeb09-T09B
50,220,423 7,640,585 (15.2%) 19,096,359 (38.0%)

Table 1: Summary of in-link statistics on two TREC web corpora used in our study.
here" and "next"), many times anchor text provides succinct description of the destination URL's content, e.g. "SIGIR 2010(Geneva, Switzerland)" is from an anchor linked to http://www.sigir2010.org/. Anchor text instances are usually reasonable queries that web users may issue to search for the associated URL and have been used to simulate plausible web queries relevant to the associated web pages in some web search research [15]. Therefore, anchor text is highly useful for bridging the lexical gap between user issued web queries and the relevant web pages. It is arguably the most important piece of evidence used in web ranking functions[14].
However, previous research has shown that the distribution of the number of inlinks on the web follows a power law [1], where a small portion of web pages have a large number of inlinks while most have few or no inlinks. Thus, most web pages do not have in-link associated anchor text, a problem originally referred to as the anchor text sparsity problem by Metzler et al. [14]. This problem presents a major obstacle for any web search algorithms that want to use anchor text to improve retrieval effectiveness. Table 1 shows the anchor text sparsity problem in two large TREC1 web corpora (GOV22 and ClueWeb09-T09B3). To address this problem, Metzler et al. [14] proposed aggregating, or propagating, anchor text across the web hyperlink graph so that web pages' lack of anchor text can be enriched with their linked web pages' associated anchor text. Table 1 shows that the number of URLs associated with some anchor text (original or propagated) in the two TREC web corpora is significantly increased by using their linked-based anchor text enrichment approach. Nevertheless, in Table 1 we notice that large portion of web pages still do not have any associated anchor text after having been enriched. This observation motivated us to consider another possible approach, which utilizes the content similarity between web pages, to alleviate anchor text sparsity.

1http://trec.nist.gov/ 2http://ir.dcs.gla.ac.uk/test_collections/ gov2-summary.htm 3http://boston.lti.cs.cmu.edu/Data/clueweb09/

427

Specifically, we hypothesize that the anchor text associated with a web page's inlinks typically has close semantic relations to the web page so that web pages that are similar in content may be pointed to by anchors having similar anchor text. Under this assumption, in this paper we propose a language modeling based technique for discovering a web page's plausible missing in-link anchor text by using its most similar web pages' in-link anchor text. We then test the effectiveness of our approach by using the discovered missing anchor text information for some TREC web search tasks. We find that even on the GOV2 data where a serious anchor text sparsity problem exists as shown in Table 1, our approach can significantly improve retrieval performance. Our content based approach can be combined with the hyperlink based approach to further reduce anchor text sparsity and benefit web search. Our enriched document and anchor text representations can also be used for many other tasks beyond web search, including estimating better document models and extracting advanced textual features for content match and document classification.
Our work has four chief contributions: 1) although content similarity has been used widely in other applications, we are the first to propose using web content similarity to address the anchor text sparsity problem. 2) We develop a language modeling based technique, which stems from ideas in one effective retrieval technique ­ relevance based language models [10], to effectively discover plausible missing anchor text information and use it for retrieval. 3) We empirically show that our approach performs better than Metzler et al.'s linked-based approach [14] in terms of discovering plausible missing anchor terms in two standard large TREC web corpora. 4) We show that our approach statistically significantly improves retrieval effectiveness, compared with several approaches that only use aggregated anchor text over the web graph, in the web named page finding task of the TREC Terabyte track [4].
We begin by reviewing related work in §2. Next, we describe different approaches of discovering missing anchor text to enrich document representations in §3. Then we describe the experimental setup and results of evaluating different approaches for anchor text discovery in §4. After that, we present how to use discovered anchor text information for retrieval in a language modeling approach and report the experimental results in §5. We conclude in §6.
2. RELATED WORK
Metzler et al. [14] first directly addressed the anchor text sparsity problem by using the web hyperlink graph and propagating anchor text over the web graph. Our work also addresses the same problem but using a different approach, which is based on the content similarity between web pages. Our approach is similar in nature to other similarity based techniques, such as cluster-based smoothing from the language modeling framework[8, 9, 11], except we focus on enriching web documents' anchor text representation by using their similar documents' associated anchor text.
Anchor text can be modeled in many different ways. Westerveld et al. [20] and Nallapati et al. [15] model anchor text in the language modeling approach [17] and calculate an associated anchor language model to update the original document model for retrieval. Fujii [6] further considers differently weighting each line of anchor text associated with the same page thus obtaining a more robust anchor language model. Here, we also adopt the language modeling approach

but focus on discovering a plausible associated anchor language model for web pages with no or few inlinks. Our approach can be easily used together with any language model based retrieval model (e.g., Ogilvie and Callan's model [16]) that takes document structure into account.
Our approach of overcoming anchor text sparsity stems from ideas in the relevance based language models(RMs), proposed by Lavrenko and Croft [10]. Their original work introduces the RMs to find plausible useful terms missing in the original query for query expansion. Here we adapt the RMs to compute a web content dependent associated anchor language model for discovering missing anchor terms and using anchor text for retrieval. Thus, our approach, although similar in spirit to, differs from document expansion [18] and graph-based document smoothing[13].
3. DISCOVERING MISSING ANCHOR TEXT
We now describe three different approaches for discovering plausible missing anchor text for web pages with few or no inlinks. The goal of each is to produce a ranked list of plausible anchor text terms for a page.
3.1 Aggregating Anchor Text
To overcome anchor text sparsity, Metzler et al.[14] originally proposed to augment web pages with auxiliary anchor text (denoted as ) that is derived by aggregating anchor text over the web graph. We first briefly review the procedure they have used to build , which is very important for our discussions and comparisons in this research. Given a web page 0's URL 0 , the procedure first collects all pages (0), within the same site (domain), that link to 0 . These links are known as 0 's internal inlinks. Then the procedure collects all anchor text  from pages (denoted as (0)) that are linked to any page in (0) from outside the site. The anchor text set  is known as external anchor text and is used as  for 0 .
Figure 1 illustrates the procedure by using a real-world example from the TREC GOV2 collection. We collect the auxiliary anchor text  for the page 0. 0's original anchor text (denoted as ), which comes from all pages (denoted as (0)) that are directly linked to 0 from outside the site, consists of lines including "Optima National Wildlife Refuge" and "Optima NWR". 0's  consists of lines including "Oklahoma Refuge Websites" and "Oklahoma National Wildlife Refuges".
Note that the above procedure does not use any anchor text associated with internal inlinks, because internal inlinks are typically generated by the owner of the site for navigational purposes and their associating anchor text tends to be navigational in nature (e.g., "home","next page", etc.; refer to [14] for more discussions on this issue). We emphasize that in the remainder part of this paper we follow Metzler et al. and do not use the anchor text associated with internal inlinks in any way.
In this paper we are specifically interested in the effectiveness of using  to serve as a surrogate for possibly missing original anchor text. In other words, we consider how effectively we may use  to discover plausibly missing original anchor text of the URL of the interest so that anchor text sparsity can be effectively reduced. Therefore, we focus on the discovered anchor terms themselves in the . We use two typical methods to rank the relative importance of each anchor term . The first method, denoted as AUX-TF, is to use each term 's term frequency () in the .

428

http://saltplains.fws.gov/index.html

Oklahoma Refuge Websites
... P5

auxiliary anchortext (aggregated)

Pages within

P2

the same site

P1 P0

http://saltplains.fws.gov/just4kid s.html

Oklahoma National Wildlife Refuges
P6

http://ifw2irm2.irm1.r2.fws.gov/texas.html
Buffalo Lake NWR
P7

Similar Pages:

P3

P4

Optima National Wildlife Refuge
P8

original anchortext

..... Optima NWR
..... P9

PIn(P0)={P1, P2} POrig (P0)={P8, P9} PAux (P0)={P5, P6} POrig (P4)={P7}

http://ifw2es.fws.gov/Oklahoma/refuges.htm l http://ifw2irm2.irm1.r2.fws.gov/toklahoma.html
P0: http://southwest.fws.gov/refuges/oklahoma/optima.ht ml. P1 :http://southwest.fws.gov/oklahoma.html . P2: http://southwest.fws.gov/refuges/okrefuges.html . P4 : http://southwest.fws.gov/refuges/texas/buffalo.html .

Figure 1: Illustration of how to aggregate anchor text over the web graph or use similar web pages' anchor text for discovering more anchor text for a web page (0 in this example). The page 0 is a GOV2 web page, whose DocID is GX010-01-9459902 and URL is http://southwest.fws.gov/refuges/oklahoma/optima.html.

The second method, denoted as AUX-TFIDF, is to use each term 's    () score, computed by multiplying () with 's  score in the web collection. The quality of the discovered anchor term rank lists produced from these two link based approaches implies the effectiveness of using auxiliary anchor text as a surrogate of missing original anchor text. We will compare these two approaches with our content based approach in §4.

3.2 Discovering Anchor Text through Finding Similar Web Pages

Note that in the link based approach, a web page 0 still

cannot obtain the auxiliary anchor text if it has no internal

inlinks or if all pages in its (0) have no external anchor

text. Indeed, Metzler et al. reported only 38% anchor text

sparsity reduction on a web sample with the link based ap-

proach[14]. Therefore, we propose a content based approach,

which does not have specific link structure requirements on

the target web page, to discover its plausible missing an-

chor text. Intuitively, our approach assumes that web pages

that are similar in content may be described by similar as-

sociated anchor text. For example, in Figure 1, the target

page 0, which is about Optima national wildlife refuge, is

similar in content with the page 4, which is about Buffalo

Lake national wildlife refuge. We observe that the anchor

term "NWR", which appears in 0's and 4's  but not

in 0's , can be used to partially describe both 0 and

4 although two pages are concerned about different places.

We consider a language modeling approach to better use

document similarity and anchor text information, based on

ideas from the relevance-based language models (RM)[10].

In brief, given a query , RM first calculates the posterior

() of each document  in the collection  generating

the query , then calculates a query dependent language

model ():



() =

() × (),

(1)

 

where  is a word from the vocabulary  of . Similarly, given an target page 0, our approach aims to calculate a relevant anchor text language model (RALM) (0) by:



(0) =

() × (0),

 

(2)

where  denotes the complete original anchor text that should be associated with  but may be missing,  denotes the complete original anchor text space for all pages, () is a multinomial distribution over the anchor text vocabulary . To compute (0) in Equation 2 where 0 and  information may be missing, we view each page 's content as its anchor text 's context and use 's document language model  = {()} as 's contextual model. Then we can calculate a translation model (0) by using 0 and 's contextual models and use (0) to approximate (0). This contextual translation approach is also used in Wang and Zhai's work [19].
When calculating a page 's document language model {()}, we employ Dirichlet smoothing on the maximum likelihood (ML) estimate of observing a word  in the page (()) with the word's collection probability ():

()

=

  +







(

)

+

 (),  + 

(3)

where  is the length of 's content and  is the Dirichlet smoothing parameter ( = 2500 in our experiments). Then
given two pages 0 and , we use the Kullback-Leibler divergence (KL) () between their document models
0 and  to measure their similarity and view that as the contextual similarity between the associated anchor text 0 and . Then the contextually based translation probability (0) is calculated by:

(0)

=

. exp(-(0  ))
 exp(-(0))

(4)

429

This (0) is then used to approximate (0) in Equation 2 to get:



(0) 

() × (0).

(5)

 

A few transformations of Equation 4 can obtain:

(0)   ()(0),

(6)



which is the likelihood of generating 0's context 0 from 's context 's smoothed language model and being normalized by 0's context length. This likelihood can be easily obtained by issuing 0 as a long query to any language model based search engine. In addition, we use the observed incomplete original anchor text language model () associated with  to approximate () in Equation 5, and let () = 0 if  has no . In this way, the RALM (0) can be computed.
In practice, for efficiency the RALM of the target page 0 is computed from 0's top- most similar pages'  (original anchor text) because (0) in Equation 4 is very small for the other pages. Due to the anchor text sparsity, we set  = 2000 in our experiments. Because some of these similar pages do not have associated , we use another parameter  to denote the number of most similar pages whose associated original anchor text is not missing and contributes information in the RALM, and we tune  in the experiments. Intuitively, increasing  can increase the number of anchor text samples to better estimate RALM but may also introduce more noise when the sample size is large.
The probability (0) of an anchor term  in the RALM directly reflects the goodness of the term  used as original anchor text for the page 0, thus we use the anchor terms that have the largest probabilities (0) in the RALM to evaluate the effectiveness of our content based approach. Theoretically our approach can associate any web page with some anchor term distribution information if there is some anchor text in the corpus, thus it can further reduce the anchor text sparsity.

3.3 Using Keywords as Anchor Text

The keyword based approaches come from the intuition
that important keywords in a web page may be good de-
scription terms for the page, thus may be arguably used as
anchor text. We use three typical term weighting schemes
to identify the keywords and rank the words in a web page's
content. The first method, denoted as DOC-TF, uses each
word 's term frequency 0 () in the page 0 for term weighting. The second method, denoted as DOC-TFIDF,
uses each word 's 0   () score, computed by multiplying 0 () with 's  score in the web collection. The third method, denoted as DOC-OKAPI, uses each word
's Okapi BM25 score  250 (), computed by:

 25() =

0 ()(1+1)

0

()+1

(1-+

0  

  (),

(7)

where  is the average document length of the pages in the collection. We use the typical setting 1 = 2,  = 0.75 in Equation 7 in our experiments.
The top ranked terms in a page 0 by three methods are used as the possible missing original anchor terms for 0. We will use three keyword based methods as baselines in §4.

4. EVALUATING DISCOVERY
We now compare the capability of discovering missing anchor text by different approaches described in §3, including two link based approaches (AUX-TF and AUX-TFIDF), our content based approach (RALM), and three keyword based approaches (DOC-TF, DOC-TFIDF and DOC-OKAPI).
4.1 Data and Methodology
We use two publicly available large TREC web collections (GOV2 and ClueWeb09-T09B). GOV2 is a standard TREC web collection [4] crawled from government web sites during early 2004. The ClueWeb09 collection is a much larger and more recent web crawl, which contains over 1 billion pages. ClueWeb09-T09B is a subset of ClueWeb09 and contains about 50 million English web pages. Compared with GOV2 crawled only from the gov domain, ClueWeb09T09B is crawled from the general web thus is a less biased web sample; in another aspect, GOV2 contains relatively high quality government web pages thus having less noise than ClueWeb09-T09B. Thus we use both GOV2 and ClueWeb09-T09B in our experiments to show how different approaches perform in web collections that have different characteristics. The Indri Search Engine4 was used to index both collections by removing a standard list of 418 INQUERY [2] stopwords and applying Krovetz stemmer. In a separate process, we run Indri Search Engine's harvestlinks utility on the two collections to collect web page inlinks and raw anchor text information where we do not perform stopping or stemming.
To evaluate the quality of discovered anchor text for a web page 0, we utilize the original anchor text  associated with all inlinks of 0. Specifically, we first hide the page 0's , apply different anchor text discovery approaches on 0, then compare the discovered anchor text with 0's . This procedure can be run automatically so that we can leverage large volumes of web pages to evaluate the performance of different approaches with no human labeling effort. More specifically, we consider each anchor term in a page 0's  as a good description term, or a relevant term, for 0 while terms not in  as non-relevant ones; in this way, we can generate term relevance judgments for 0. Then we employ each different approach to discover a ranked list of plausible missing anchor terms for 0 and then use the relevant judgments to evaluate the ranked anchor term list. Note that for fair comparison 0's  is not used in Equation 2 for calculating RALM in our approach. In the experiments, we perform slight stopping on the raw anchor text by removing a short list of 39 stopwords, which includes 25 common stopwords[12, pp.26] and 14 additional anchor terms5 that are either common navigational purposed words or part of URLs ­ it is common that anchor text contains some URL.
We calculate some typical TREC style evaluation measurements including Mean Average Precision (MAP), Mean Reciprocal Rank(MRR), Precision at the number of relevant terms(R-Prec), Precision at  (P@) and also normalized discounted cumulative gain (NDCG) [7]. In the experiments, we are specifically interested in the quality of top ranked discovered anchor terms; thus, we only use the
4http://www.lemurproject.org/indri/ 5The additional terms are: http, https, www, gov, com, org, edu, net, html, htm, click, here, next, home.

430

DOC-TF DOC-TFIDF DOC-OKAPI
AUX-TF AUX-TFIDF
RALM

MAP 0.3162 0.2936 0.2936 0.1969 0.1716 0.3183

NDCG 0.4585 0.4348 0.4348 0.2598 0.2423 0.4275

MRR 0.5441 0.5400 0.5400 0.3707 0.3442 0.5050

P@2 0.3833 0.3700 0.3700 0.2833 0.2433 0.3467

P@5 0.2800 0.2613 0.2613 0.1773 0.1720 0.2840

P@10 0.2060 0.1827 0.1827 0.1153 0.1140 0.1860

P@20 0.1333 0.1240 0.1240 0.0643 0.0647 0.1140

R-Prec 0.2716 0.2530 0.2530 0.1643 0.1428 0.3051

Discovered Rel. 400 372 372 193 194 342

Table 2: Performances on the GOV2 collection. There are 708 relevant anchor terms overall. Column 10 shows overall relevant anchor terms discovered by each different approach. RALM performs statistically significantly better than AUX-TF and AUX-TFIDF by each measurement in columns 2­9 according to the one-sided t-test ( < 0.005). There exists no statistically significant difference between each pair of RALM, DOC-TF, DOC-TFIDF and DOC-OKAPI by each measurement according to the one-sided t-test ( < 0.05).

DOC-TF DOC-TFIDF DOC-OKAPI
AUX-TF AUX-TFIDF
RALM

MAP 0.3517 0.3107 0.3107 0.1840 0.1634 0.2612

NDCG 0.4891 0.4388 0.4388 0.2507 0.2347 0.3615

MRR 0.5588 0.5145 0.5145 0.3309 0.3116 0.4630

P@2 0.3467 0.3133 0.3133 0.2248 0.2047 0.2833

P@5 0.2373 0.2213 0.2213 0.1463 0.1383 0.1733

P@10 0.1360 0.1173 0.1173 0.0729 0.0676 0.0911

P@20 0.1090 0.0983 0.0983 0.0577 0.0560 0.0770

R-Prec 0.2990 0.2608 0.2608 0.1675 0.1402 0.2398

Discovered Rel. 327 295 295 172 167 231

Table 3: Performances on the ClueWeb09-T09B collection. There are 582 relevant anchor terms overall. Column 10 shows overall relevant anchor terms discovered by each different approach. DOC-TF performs statistically significantly better than both RALM and AUX-TF by each measurement in columns 2­9 according to the one-sided t-test ( < 0.05). RALM performs statistically significantly better than AUX-TF and AUX-TFIDF by each measurement in columns 2­9 according to the one-sided t-test ( < 0.05).

top-20 terms in the discovered term rank lists by different approaches to calculate the measurements.
Note that web pages that can be used in our evaluation procedure need to satisfy two requirements: (1) they need to have some associated  and (2) they can collect some auxiliary anchor text from the web graph as described in §3.1. Thus, for each of two collections, we randomly sample 150 pages satisfying the two requirements for training and another 150 pages for testing. On both training sets, RALM's parameter  = 15 described in §3.2 achieves the highest MAPs.
4.2 Results and Analysis
The performance of discovering original anchor text by different approaches on the testing set of GOV2 and ClueWeb09-T09B are shown in Table 2 and Table 3, respectively. The results show that our approach (RALM) can effectively discover missing original anchor terms. On both collections RALM performs statistically significantly better than two link based approaches (AUX-TF and AUX-TFIDF). This indicates that, for discovering a page's missing anchor text, the anchor text associated with the similar pages provides more useful information than that associated with the linked web neighbors. The numbers of discovered relevant anchor terms by different approaches, shown in the last column of two tables, also indicate that only using auxiliary anchor text misses more original anchor text information than our content based approach.
Another observation is that RALM performs worse on ClueWeb09-T09B and not statistically significantly better on GOV2 than the keyword based approaches. This indicates that words having high IR utility like  or    scores are often good description terms for the page and used by human being as the anchor text. Removing a long list of stopwords from web page content has also helped the keyword based approaches to effectively select good descrip-

(AUX-TF, DOC-TF) (AUX-TF, RALM) (RALM, DOC-TF)

GOV2 30.5% 47.6% 26.0%

ClueWeb09-T09B 26.0% 46.3% 22.3%

Table 4: The average percentage (,  ) of the terms discovered by the  approach appearing in the ones discovered by the  approach.

tion words from the web content. One plausible reason that RALM performs relatively poorly on ClueWeb09-T09B is that, compared with the high quality GOV2 pages, ClueWeb pages are crawled from the general web, where the inlinks and anchor text may be generated in a more noisy way (e.g. spam), degrading RALM's performance. To better understand the performance of different approaches, in Table 5 and Table 6 we show the top-10 words of the anchor term rank lists discovered by different approaches for one evaluation web page in GOV2 and ClueWeb09-T09B, respectively.
Although using keyword information can discover some good anchor terms, the content-generated anchor terms do not help bridging the lexical gap between a web page and varied queries that attempt to search the page. Indeed, human generated anchor text is highly useful for reducing the word mismatch problem because the lexical gap between anchor text and queries is relatively small[14]. Here, we do some lexical gap analysis to show that our approach can also discover anchor terms similar in nature to human-generated ones but different from content-generated ones.
For each web page  in the testing set, we calculate the percentage (,  ) of the terms discovered by the  approach also appearing in the ones discovered by the  approach, then compute the average percent (,  ) with all the pages. We use the outputs from the keyword based DOC-TF, the link based AUX-TF, and the RALM in this analysis. Table 4 shows three average percentages (,  )

431

which we have specific interest in. We observe that AUXTF's discovered terms have much higher average per query overlap ratio with RALM's than with DOC-TF's. Moreover, RALM's discovered anchor terms have small overlap with DOC-TF's.

5. USING DISCOVERED ANCHOR TEXT FOR WEB SEARCH

We now describe how we use the discovered anchor text by different approaches for retrieval in a language modeling approach [17]. We point out that our focus here is not to evaluate different schemes to aggregate or combine anchor text [14]; instead, we focus on comparing the utility of RALM and auxiliary anchor text for helping retrieval.
5.1 Retrieval Models

We follow the typical language modeling based retrieval

approach[17] and score each web page  for a query  by the

likelihood of the page  's document language model ( )

generating the query :



( ) = ( ).

(8)



When using Dirichlet smoothing, the document language model ( ) can be calculated by Equation 3 and then used in Equation 8 for retrieval. We call this baseline QL. We only fix  = 2500 in Equation 3 for the document models used to calculate RALM, but tune the  for QL to achieve the best retrieval performance in our experiments in §5.2.
We follow the mixture model approach [15, 16] to use the discovered anchor text information for helping retrieval. In this approach, a web page  's document language model is assumed to be a mixture of multiple component distributions where each component is associated with a prior probability, or a mixture weight. Therefore, we can estimate a language model () from anchor text discovered by each different approach for the page  and use () as a component of  's document model thus obtaining a better document language model ~( ):

~( ) = ( ) + (1 - )(),

(9)

where ( ) is the original smoothed document model in the QL baseline. Then we can plug ~( ) into equation 8 for retrieval. We compare the retrieval performance of document language models updated by different discovered anchor text information.
We consider three different anchor text sources to update a web page  's document model: (1) the observed original anchor text  associated with  , (2) the auxiliary anchor text  of  , and (3) the RALM computed by our approach for  . We estimate the anchor text language model () and () by using the ML estimate of observing each word  in  and , respectively. Here, we design the following five retrieval methods that use the above three anchor text sources: 1. M-ORG, which only uses the observed original anchor text language (). 2. M-AUX, which only uses the auxiliary anchor text language (). 3. M-ORG-AUX, which uses both () and () to update the document model ( ) by:

~( ) = (( ) + (1 - )()) +(1 - )().

(10)

QL M-ORG M-AUX M-ORG-AUX M-RALM M-ORG-RALM

MRR 0.3132 0.3696 0.3187 0.3711 0.3388 0.3975

%Top10 49.7 57.5 50.8 57.5 53.6 59.7

Opt. Param.
 = 0.95  = 0.99  = 0.95,  = 0.99  = 20,  = 0.95 ,  = 0.95,  = 20

Table 7: Retrieval performance of different approaches with TREC 2006 NP queries. The star indicates statistically significant improvement over MRRs of M-ORG and M-ORG-AUX by one-sided t-test ( < 0.05). The triangle indicates statistically significant improvement over MRRs of QL and MAUX by one-sided t-test ( < 0.05).

4. M-RALM, which only uses the RALM (0) in Equation 2. The original anchor text of 0 is not used in Equation 2 for calculating RALM. 5. M-ORG-RALM, which uses both () and the RALM (0) in Equation 2 by:

~( ) = (( ) + (1 - )()) +(1 - )(0).

(11)

The original anchor text of 0 is not used in Equation 2 for calculating RALM.
Note that we can update each page's document model offline, thus this computationally expensive procedure has little impact on the online query processing time. Moreover, different from experiments in §4.1, we use all anchor terms instead of the top-20 most important terms discovered by different approaches.
5.2 Experiments
We use the TREC web named page finding tasks in Terabyte Track[4, 5] to evaluate the performance of different retrieval methods described in §5.1. The objective of the named page (NP) finding task is to find a particular page in the GOV2 collection, given a topic that describes it. We use the NP topics and their relevance judgments for our experiments. In this experiment, we used Porter stemmer and did not remove stopwords when indexing the GOV2 collection.
For each NP query, we first run it against the GOV2 collection to obtain the QL baseline; then we use five retrieval methods described in §5.1 to rerank the top-100 web pages returned by QL. The reranked lists are evaluated by two TREC measurements previously used for the task [5]: MRR which is the mean reciprocal rank of the first correct answer and the %Top10 which is the proportion of queries for which a correct answer was found in the first 10 search results. We use the TREC 2005 NP topics (NP601-872) for training and the TREC 2006 NP topics (NP901-1081) for testing. We first tune the Dirichlet parameter  = 500 for QL to achieve the highest MRR on the training set and obtain QL's top-100 web pages for reranking. We then fix  = 500 to calculate the smoothed document model component ( ) in the five retrieval methods but tune the mixture parameters  and  for them to achieve the highest MRRs with the training queries. For the two approaches that use RALM, the parameter  of RALM is also tuned. After that, we run different methods on the testing set.
Table 7 shows the retrieval performance of different methods and the tuned parameters in each method. We observe: (1) M-ORG-RALM performs statistically significantly bet-

432

"Optima National Wildlife Refuge", "Optima NWR", "Washita Optima National Wildlife Refuge near Butler OK"

DOC-TF
refuge wildlife oklahoma optima species hawk habitat
area prairie national

0 () 15 10 10 8 6 6 6 6 5 5

DOC-TFIDF
refuge optima hardesty hawk oklahoma wildlife guymon habitat species quail

0  () 79.69 74.30 47.48 36.20 36.03 31.98 29.35 26.42 23.70 21.74

DOC-OKAPI
refuge optima hardesty hawk oklahoma wildlife guymon habitat species quail

 250 () 153.76 143.37 91.63 69.86 69.53 61.71 56.63 50.98 45.73 41.95

AUX-TF
oklahoma wildlife refuge website u service s office national fish

() 6 2 2 1 1 1 1 1 1 1

AUX-TFIDF
oklahoma refuge wildlife fish u website office s national service

 () 21.62 10.62 6.40 3.11 3.03 2.36 1.54 1.29 1.22 1.09

RALM
nwr wildlife refuge national general brochure kansas
lake tear sheet

 (0) 0.1164 0.0834 0.0834 0.0834 0.0657 0.0657 0.0601 0.0522 0.0308 0.0308

Rel.
butler national
near nwr optima refuge washita wildlife

Table 5: Discovered missing anchor terms and their term weights by applying different approaches on one GOV2 web page (TREC DocID in GOV2: GX010-01-9459902) . The first row shows the original three pieces of anchor text associated with the page. The Rel column in bold font shows the term relevance judgments extracted from the first row. RALM can discover some term like "NWR", which may not appear in both the page and the auxiliary anchor text, thus may help to bridge the lexical gap between pages and web queries as using the original anchor text does.

"Weight Loss Resolutions", "Weight Loss New Year's Resolution to Lose Weight","Resolve to Lose Weight"

DOC-TF
weight loss lose new year
resolution time make goal diet

0 () 46 26 20 17 15 13 12 10 9 9

DOC-TFIDF
weight loss lose
resolution diet goal eat year
calorie pound

0  () 96.38 78.65 64.47 46.57 34.27 26.01 25.61 23.90 15.73 15.34

DOC-OKAPI
weight loss lose
resolution diet goal eat year
calorie pound

 250 () 112.53 91.83 75.28 54.38 40.02 30.37 29.90 27.90 18.36 17.91

AUX-TF
weight loss diet
weightloss guide scott jennifer contact site s

() 709 705 32 21 20 8 8 8 6 4

AUX-TFIDF
loss weight weightloss
diet guide jennifer scott guidesite
em mlibrary

 () 2132.63 1485.49 157.70 121.86 37.26 33.96 28.52 22.04 13.15 11.37

RALM
weight loss diet easy lose way myth warn ppa fda

 (0) 0.2245 0.1737 0.0550 0.0436 0.0422 0.0412 0.0396 0.0232 0.0232 0.0232

Rel.
lose loss new resolution resolve
s weight

Table 6: Discovered missing anchor terms and their term weights by applying different approaches on one ClueWeb09 web page (ClueWeb09 RecordID: clueweb09-en0004-60-01628). The first row shows the original three pieces of anchor text associated with the page. The Rel column in bold font shows the term relevance judgments extracted from the first row. The keyword approaches discovered "new year resolution", which may be hard to be discovered by using the page's web-graph neighbor pages' anchor text or using the page's similar pages' anchor text.

433

ter than M-ORG. This indicates that missing anchor text discovered by RALM provides additional information not in the original anchor text so that combining them can further improve the retrieval performance. (2) M-ORG-RALM and M-RALM performs statistically significantly better than MORG-AUX and M-AUX, respectively. This indicates that in GOV2 missing anchor text information discovered by our content based approach helps retrieval more effectively than the auxiliary anchor text.6
In Table 7, we observe that the auxiliary anchor text helps the performance very little in this task. There are two plausible reasons: first, TREC NP queries are short queries and Metzler et al. observed that auxiliary anchor text does not help or even hurts the performance of short navigational web queries[14]; second, the anchor text sparsity problem is serious on the GOV2, thus very small percentage of pages can collect some auxiliary anchor text as shown in Table 1 to benefit the search task. However, even when serious anchor text sparsity exists and queries are short, our content based approach still helps improving retrieval effectiveness.
We expect our technique can enhance the retrieval performance of general web search engines where there are large portion of short navigational queries. As is well known, in the general web search environment there are many lowquality web pages and spam; thus, we need to address issues about web page quality and noise filtering for better benefitting general web search. We leave this as future work.
6. CONCLUSIONS AND FUTURE WORK
In this paper, we proposed a language modeling based technique to overcome the anchor text sparsity problem by using web content similarity. Our approach computes a relevant anchor text language model, called RALM, from its similar web pages' associated anchor text to discover its plausible missing anchor text. Compared with a link based approach [14], our content based approach has no specific link structure requirements on the web page of interest and thus can further reduce anchor text sparsity.
We designed experiments with two TREC web corpora to evaluate the effectiveness of discovering missing anchor terms by three different approaches: the link based approach, the RALM approach, and the keyword based approach. Experimental results show that the RALM approach can effectively discover missing original anchor text and performs statistically significantly better than the two link based approaches on both collections. Moreover, RALM's discovered anchor text is similar in nature to auxiliary anchor text while different from the keywords in the web page.
By using the mixture model[15, 16], we used different discovered anchor text information within the language modeling framework for retrieval. We evaluated using different approaches for improving retrieval effectiveness with the TREC named page finding task. The results show that (1) RALM helps retrieval more than using the auxiliary anchor text collected over the web graph and (2) combining RALM and the original anchor text can statistically significantly improve the retrieval performance of only using the original anchor text. Furthermore, RALM can help improving retrieval effectiveness for short navigational queries even when serious anchor text sparsity exists. This makes RALM a promising technique for improving general web search engines.
6Our goal is not to compare ranking schemes, but to show the utility of the discovered anchor text. However, we note that these scores match or beat top-performing approaches [4].

There are several interesting directions of future work. Metzler et al. found that auxiliary anchor text can effectively help longer, informational queries [14]; we will explore how well RALM can help long informational queries. We also want to explore using RALM's discovered missing anchor text information beyond the language modeling based retrieval framework, e.g. using it to extract useful features for learning-to-rank retrieval approaches [3].
7. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
8. REFERENCES
[1] A. Broder et al. Graph structure in the web. Comput. Netw., 33(1-6):309­320, 2000.
[2] J. Broglio, J. P. Callan, and W. B. Croft. An overview of the INQUERY system as used for the TIPSTER project. Technical report, Amherst, MA, USA, 1993.
[3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proc. of ICML, pp. 89­96, 2005.
[4] S. Bu¨ttcher, C. L. A. Clarke, and I. Soboroff. The TREC 2006 Terabyte Track. In TREC, 2006.
[5] C. L. A. Clarke, F. Scholer, and I. Soboroff. The TREC 2005 Terabyte Track. In TREC, 2005.
[6] A. Fujii. Modeling anchor text and classifying queries to enhance web document retrieval. In Proc. of WWW, pp. 337­346, 2008.
[7] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422­446, 2002.
[8] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. In SIGIR, pp. 194­201, 2004.
[9] O. Kurland and L. Lee. Respect my authority!: Hits without hyperlinks, utilizing cluster-based language models. In SIGIR, pp. 83­90, 2006.
[10] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR, pp. 120­127, 2001.
[11] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In SIGIR, pp. 186­193, 2004.
[12] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge Univ. Press. 2008.
[13] Q. Mei, D. Zhang, and C. Zhai. A general optimization framework for smoothing language models on graph structures. In SIGIR, pp. 611­618, 2008.
[14] D. Metzler, J. Novak, H. Cui, and S. Reddy. Building enriched document representations using aggregated anchor text. In SIGIR, pp. 219­226, 2009.
[15] R. Nallapati, B. Croft, and J. Allan. Relevant query feedback in statistical language modeling. In Proc. of CIKM, pp. 560­563, 2003.
[16] P. Ogilvie and J. Callan. Combining document representations for known-item search. In SIGIR, pp. 143­150, 2003.
[17] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR, pp. 275­281, 1998.
[18] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information retrieval with document expansion. In Proc. of NAACL-HLT, pp. 407­414, 2006.
[19] X. Wang and C. Zhai. Mining term association patterns from search logs for effective query reformulation. In Proc. of CIKM, pp. 479­488, 2008.
[20] T. Westerveld, W. Kraaij, and D. Hiemstra. Retrieving web pages using content, links, urls and anchors. In Proc. of TREC, pp. 663­672, 2001.

434

EUSUM: Extracting Easy-to-Understand English Summaries for Non-Native Readers

Xiaojun Wan, Huiying Li and Jianguo Xiao
Institute of Computer Science and Technology, Peking University, Beijing 100871, China Key Laboratory of Computational Linguistics (Peking University), MOE, China
{wanxiaojun, lihuiying, xiaojianguo}@icst.pku.edu.cn

ABSTRACT
In this paper we investigate a novel and important problem in multi-document summarization, i.e., how to extract an easy-tounderstand English summary for non-native readers. Existing summarization systems extract the same kind of English summaries from English news documents for both native and nonnative readers. However, the non-native readers have different English reading skills because they have different English education and learning backgrounds. An English summary which can be easily understood by native readers may be hardly understood by non-native readers. We propose to add the dimension of reading easiness or difficulty to multi-document summarization, and the proposed EUSUM system can produce easy-to-understand summaries according to the English reading skills of the readers. The sentence-level reading easiness (or difficulty) is predicted by using the SVM regression method. And the reading easiness score of each sentence is then incorporated into the summarization process. Empirical evaluation and user study have been performed and the results demonstrate that the EUSUM system can produce more easy-to-understand summaries for non-native readers than existing summarization systems, with very little sacrifice of the summary's informativeness.
Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing ­ abstracting methods; I.2.7 [Artificial Intelligence]: Natural Language Processing ­ text analysis
General Terms
Algorithms, Experimentation, Design, Human Factors.
Keywords
EUSUM, multi-document summarization, reading easiness
1. INTRODUCTION
Document summarization is a task of producing a condensed version of a document or document set. A summary is usually required to be informative and fluent. Users can easily understand the main content of the document or document set by reading the summary.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

To date, various summarization methods and a number of summarization systems have been developed, such as MEAD, NewsInEssence and NewsBlaster. These methods and systems focus on how to improve the informativeness, diversity or fluency of the English summary, and they usually produce the same English summaries for all users, including native readers and nonnative readers. However, different users usually have different English reading levels because they have different English education backgrounds and learning environments. And native readers usually have higher English reading levels than non-native readers. In particular, Chinese readers usually have less ability to read English summaries than native English readers. For example, Chinese college students usually have passed the National English Test Band 4 (CET-4), and they have learned English for several years, but they still have more or less difficulty to read original English news and summaries. The difficulty lies in unknown or difficult English words (e.g. "seismographs", "woodbine"), or the complex sentence structure (e.g. "The chairman of the House Agriculture Committee says hearings are planned next year into how the U.S. Forest Service handled last summer's stubborn wildfires that scorched the West, including one-third of Yellowstone National Park."). Therefore, they have to slow down the reading speed in order to understand the news text or summary, or give up the reading process.
In this study, we argue that the English summaries produced by existing methods and systems are not fit for non-native readers (i.e. Chinese readers). We examine a new factor - reading easiness (or difficulty) 1 for document summarization, and the factor can indicate whether the summary is easy to understand by non-native readers or not. The reading easiness of a summary is dependent on the reading easiness of each sentence in the summary. And we propose a novel summarization system ­ EUSUM (Easy-toUnderstand Summarization) for incorporating the reading easiness factor into the final summary. The proposed system first predicts the reading easiness score for each sentence, and then incorporates the reading easiness score into the final sentence ranking process. Both informative and easy-to-understand sentences are selected into the summary. Both automatic evaluation and user study have been performed and the evaluation results verify the effectiveness of the proposed EUSUM system.
The contribution of this paper is summarized as follows: 1) We examine a new factor of reading easiness for document summarization. 2) We propose a novel summarization system ­ EUSUM for incorporating the new factor and producing easy-to-
1 In this paper, "reading easiness" and "reading difficulty" refer to the same factor, and we use them interchangeably.

491

understand summaries for non-native readers. 3) We conduct both automatic evaluation and user study to verify the effectiveness of the proposed system.
The paper is organized as follows: Section 2 introduces related work. Section 3 describes the details of the EUSUM system. Sections 4 and 5 present experimental results and discussions. Lastly we conclude our paper in Section 6.
2. RELATED WORK
2.1 Document Summarization
Document summarization methods can be generally categorized into extraction-based methods and abstraction-based methods. In this paper, we focus on extraction-based methods. Extractionbased summarization methods usually assign each sentence a saliency score and then rank the sentences in a document or document set.
For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, topic signature [19, 22]. The summary sentences can also be selected by using machine learning methods [1, 17] or graphbased methods [8, 23]. Other methods include mutual reinforcement principle [33].
For multi-document summarization, the centroid-based method [27] is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS [20] makes use of new features such as topic signature to select important sentences. Machine Learning-based approaches have also been proposed for combining various sentence features [34]. Themes (or topics, clusters) discovery in documents has been used for sentence selection [10]. The influences of input difficulty on summarization performance have been investigated in [25]. Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau [24] extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences [32]. Sentence ordering in summaries has been investigated in [2, 3] to improve the summary fluency.
Other summarization tasks include topic-focused (query-biased) document summarization [31], update summarization [18]. All these summarization tasks do not consider the reading easiness factor of the summary for non-native readers.
2.2 Reading Difficulty Prediction
A reading difficulty measure can be originally described as a function or model that maps a text to a numerical value corresponding to a difficulty or grade level [12]. And reading difficulty prediction can be viewed as a regression of difficulty grade level based on a set of features derived from the text. Earlier work on reading difficulty prediction is conducted for the purpose of education or language learning. For example, one purpose is to find appropriate reading materials of the appropriate difficulty level, in terms of both vocabulary and grammar, for English as a First or Second Language students. And almost all earlier work focuses on document-level reading difficulty prediction.
A variety of features have been investigated in reading difficulty measures. Average sentence length and word length are simple

proxies for grammatical and lexical complexity of a text, as in the Dale-Chall model [5]. The Flesch-Kincaid meaure [15] is probably the most common reading difficulty in use in earlier days. The Lexile Framework [29] uses individual word frequency estimates as a measure of lexical difficulty, and it uses a Rasch model based on the features of word frequency and sentence length. In recent years, more sophisticated features and models are used. Smoothed unigram language modeling has been developed to capture the predictive ability of individual words based on their frequency at each reading difficulty level [7]. A statistical approach is proposed to infer the distribution of a word's likely acquisition age automatically from authentic texts collected from the Web, and then an effective semantic component for predicting reading difficulty of news texts is provided by combining the acquisition age distributions for all words in a document [14]. Schwarm and Ostendorf [28] incorporate syntactic features derived from syntactic parses of text, and their system performs better than the Flesch-Kincaid and Lexile measures. The frequency of grammatical constructions has been used as a measure of grammatical difficulty, and the final prediction function is a linear function of the lexical and grammatical components [11, 12]. Pitler and Nenkova [26] combine lexical, syntactic and discourse features to produce a highly predictive model of text readability. In addition to English language, François [9] presents an approach to assessing the readability of French texts. More recently, a machine learning approach is used for predicting the readability of web search summaries or snippets [13].
In this study, we investigate the reading difficulty (or easiness) prediction of English sentences for Chinese readers, i.e. whether an English sentence is easy to understand by Chinese readers or not.
Note that sentence ordering in a long summary also has influences on the reading difficulty or readability of the summary, and proper order of extracted sentences can improve their readability [2]. However, sentence ordering is another research problem and we do not take into account this factor in this study.
3. THE EUSUM SYSTEM 3.1 System Overview
The main idea of the proposed EUSUM system is to incorporate the sentence-level reading easiness factor into the summary extraction process. Each sentence is associated with two factors: informativeness and reading easiness. The informativeness of a sentence is computed by using previous summarization methods. The reading easiness of a sentence is measured by an EU (easy-tounderstand) score, which is predicted by using statistical regression methods. The two scores are then combined and both informative and easy-to-understand sentences are chosen into the summary. The three steps of the EUSUM system will be described in details in next two sections.
As mentioned in Section 2.2, we do not consider the fluency factor of the whole summary in this study, which has been investigated in related research areas (e.g. sentence ordering [2, 3]).

492

3.2 Sentence-Level Reading Easiness Prediction
In this study, reading easiness refers to how easily a text can be understood by non-native readers. Reading easiness prediction is a task of mapping a text to a numerical value corresponding to a reading easiness. The larger the value is, the more easily the text can be understood. We focus on predicting the reading easiness score of an English sentence for Chinese college students.
As mentioned earlier, Chinese college students usually have studied English for several years and they usually have passed the CET-4 test2 or above, which means that they have some ability to read ordinary English articles. However, because of different English learning environments and different learning abilities, these students may have different English reading levels. Many students have some difficulty to read original English news or summaries. The two factors most influencing the reading process are as follows:
1) Unknown or difficult English words: for example, most Chinese college students do not know the words such as "seismographs", "woodbine".
2) Complex sentence structure: for example, a sentence with two or more clauses introduced by a subordinating conjunction is usually difficult to read.
As introduced in Section 2.2, various regression methods have been used for reading difficulty prediction. In this study, we adopt the -support vector regression (-SVR) method [30] for the reading easiness prediction task. The SVR algorithm is firmly grounded in the framework of statistical learning theory (VC theory). The goal of a regression algorithm is to fit a flat function to the given training data points.
In the experiments, we use the LIBSVM tool [6] with the RBF kernel for the regression task, and we use the parameter selection tool of 10-fold cross validation via grid search to find the best parameters with respect to mean square error (MSE), and then use the best parameters to train the whole training set.
We use the following two groups of features for each sentence: the first group includes surface features, and the second group includes parse based features.
The four surface features are as follows:
1) Sentence length: It refers to the number of words in the sentence. A long sentence may be more difficult to understand than a short sentence.
2) Average word length: It refers to the average length of words in the sentence. Usually, an English word with few characters is more easily recognized and remembered than that with many characters.
3) CET-4 word percentage: It refers to the percentage of how many words in the sentence appear in the CET-4 word list (690 words). As mentioned earlier, most Chinese college students have passed CET-4, and the words appearing in the CET-4 word list are likely to be recognized by the students.
2 CET-4 is College English Test Band 4, which is a national English level test in China, and all college students are required to pass this test before graduation.

4) Number of peculiar words: It refers to the number of infrequently occurring words in the sentence. We collect all words in the experimental corpus, and choose the top 2000 words with low frequency as the peculiar words. The frequency of each word is extracted from the Google Web 1T 1-gram database [4].
We use the Stanford Lexicalized Parser [16] with the provided English PCFG model to parse a sentence into a parse tree. The output tree is a context-free phrase structure grammar representation of the sentence. The four parse features are as follows:
1) Depth of the parse tree: It refers to the depth of the generated parse tree. Usually the higher the parse tree is, the more complex the sentence is.
2) Number of SBARs in the parse tree: SBAR is defined as a clause introduced by a (possibly empty) subordinating conjunction. It is an indictor of sentence complexity, especially for Chinese readers.
3) Number of NPs in the parse tree: It refers to the number of noun phrases in the parse tree.
4) Number of VPs in the parse tree: It refers to the number of verb phrases in the parse tree.
All the above feature values are scaled by using the provided svmscale program.
At this step, each sentence si can be associated with a reading easiness score EaseScore(si) predicted by the -SVR method. The larger the score is, the more easily the sentence is understood. The score is finally normalized by dividing by the maximum score.
3.3 Sentence-Level Informativeness Evaluation
In this study, we adopt two typical methods for evaluating the informativeness of each sentence in a document set. The two methods are described briefly in the following sections.
3.3.1 Centroid-Based Method
The centroid-based method is the algorithm used in the MEAD system. The method uses a heuristic and simple way to sum the sentence scores computed based on different features. In our implementation, the score for each sentence is a linear combination of the weights computed based on the following three features: 1) Centroid-based Weight. The weight C(si) of sentence si is calculated as the cosine similarity between the sentence text and the concatenated text for the whole document set D. The weight is then normalized by dividing by the maximal weight. 2) Sentence Position. The weight P(si) is calculated for sentence si to reflect its position priority as P(si)=1-(posi-1)/ni, where posi is the position number of sentence si in a particular document and ni is the total number of sentences in the document. Obviously, posi ranges from 1 to ni. 3) First Sentence Similarity. The weight F(si) is computed as the cosine similarity value between sentence si and the corresponding first sentence in the same document.
After all the above weights are calculated for each sentence, we sum the three weights and get the overall score InfoScore(si) for sentence si. After the scores for all sentences are computed, the

493

score of each sentence is normalized by dividing by the maximum score.

3.3.2 Graph-Based Method
The basic idea of the graph-based method is that of "voting" or "recommendation" between sentences. Formally, given a document set D, let G=(V, E) be an undirected graph to reflect the relationships between sentences in the document set. V is the set of vertices and each vertex si in V is a sentence in the document set. E is the set of edges. Each edge eij in E is associated with an affinity weight f(si, sj) between sentences si and sj (ij). The weight is computed using the standard cosine measure between the two sentences. Here, we have f(si, sj)=f(sj, si) and let f(si, si)=0 to avoid self transition.

We use an affinity matrix M to describe G with each entry corresponding to the weight of an edge in the graph. M =
~ (Mi,j)|V|×|V| is defined as Mi,j=f(si,sj). Then M is normalized to M to make the sum of each row equal to 1.
Based on matrix M~ , the saliency score InfoScore(si) for sentence si can be deduced from those of all other sentences linked with it and it can be formulated in a recursive form as in the PageRank algorithm:

 InfoScore(si

)

=

µ



all

ji

InfoScore(s j

)



M~

j,i

+

(1- µ) |V |

where µ is the damping factor usually set to 0.85, as in the PageRank algorithm.

After the scores for all sentences are computed, the score of each sentence is normalized by dividing by the maximum score.

3.4 Summary Extraction
After we obtain the reading easiness score and the informativeness score of each sentence in the document set, we linearly combine the two scores to get the combined score of each sentence.
Formally, let EaseScore(si)[0,1] and InfoScore(si)[0,1] denote the reading easiness score and the informativeness score of sentence si, the combined score of the sentence is:
CombinedSc ore(si ) = InfoScore (si ) +  × EaseScore (si )
where 0 is a parameter controlling the influences of the reading easiness factor. If  is set to 0, the summary is extracted without considering the reading easiness factor. Usually,  is not set to a large value because we must maintain the content informativeness in the extracted summary. Therefore, we choose the parameter value empirically in order to balance the two factors of content informativeness and reading easiness.
For multi-document summarization, some sentences are highly overlapping with each other, and thus we apply the same greedy algorithm in [31] to penalize the sentences highly overlapping with other highly scored sentences, and finally the informative, novel, and easy-to-understand sentences are chosen into the summary.
In the algorithm, the final rank score RankScore(si) of each sentence si is initialized to its combined score CombinedScore(si). And at each iteration, the highly ranked sentence (e.g. si) is selected into the summary, and the rank score of each remaining sentence sj is penalized by using the following formula:

RankScore(s j ) = RankScore(s j ) -   M~ j,i  CombinedScore(si )
where >0 is the penalty degree factor. The larger  is, the greater penalty is imposed to the rank score. If =0, no diversity penalty is imposed at all. The iteration is stopped after the summary length limit is reached.

4. EXPERIMENTS
4.1 Reading Easiness Prediction
4.1.1 Experimental Setup
In the experiments, we first constructed the gold-standard dataset in the following way.
DUC2001 provided 309 news articles for document summarization tasks, and the articles were grouped into 30 document sets. The news articles were selected from TREC-9. We chose five document sets (d04, d05, d06, d08, d11) with 54 news articles out of the DUC2001 test set. The documents were then split into sentences and there were totally 1736 sentences.
Two college students (one undergraduate student and one graduate student) manually labeled the reading easiness score for each sentence separately. The score ranges between 1 and 5, and 1 means "very hard to understand", and 5 means "very easy to understand", and 3 means "mostly understandable". The final reading easiness score was the average of the scores provided by the two annotators.
After annotation, we randomly separated the labeled sentence set into a training set of 1482 sentences and a test set of 254 sentences. We then used the LIBSVM tool for training and testing.
Two standard metrics were used for evaluating the prediction results. The two metrics are as follows:
Mean Square Error (MSE): This metric is a measure of how correct each of the prediction values is on average, penalizing more severe errors more heavily. Pearson's Correlation Coefficient (): This metric is a measure of whether the trends of prediction values matched the trends for human-labeled data.

4.1.2 Experimental Results
Table 1 shows the prediction results. For comparison, the result for the Flesch-Kincaid measure3 is also reported in the table. We can see that the overall results of our method are very promising. And the correlation is high. The results guarantee that the use of reading easiness scores in the summarization process is feasible.

Table 1. Reading easiness prediction results

Method Flesch-Kincaid SVR (Surface features) SVR (Parse features) SVR (Surface features + Parse features)

MSE 0.704 0.121 0.227 0.112

 0.377 0.929 0.853 0.931

3 The reading easiness scores based on the FK measure are directly obtained by accessing the following web service: http://www.standards-schmandards.com/exhibits/rix/index.php

494

We can also see that either the surface feature set or the parse feature set can achieve good prediction result, and the two feature sets can contribute to the overall prediction results. However, the Flesch-Kincaid measure does not perform well.
4.2 Document Summarization
4.2.1 Experimental Setup
In this study, we used the multi-document summarization task (task 2) in DUC2001 for evaluation. As mentioned in Section 4.1.1, DUC2001 provided 30 document sets. Because we have used five document sets (d04-d11) for training and testing in the task of reading easiness prediction, we used the remaining 25 document sets for summarization evaluation, and the average document number per document set is 10. The sentences in each article have been separated and the sentence information has been stored into files. A summary was required to be created for each document set and the summary length was 100 words. Generic reference summaries were provided by NIST annotators for evaluation.
We used the LIBSVM tool with the learned model to predict the reading easiness score for each sentence in the documents, and then used the scores for summary extraction.
Different from traditional summarization tasks, our task is to incorporate the reading easiness factor into multi-document summary, and the easy-to-understand summarization can be considered as a novel summarization task. Therefore, we evaluate a summary from the following two aspects:
Content Informativeness: This aspect is widely evaluated in almost all traditional summarization tasks. It refers to how much a summary reflects the major content of the document set. Usually, it can be measured by comparing the system summary with the reference summary.
We used the ROUGE-1.5.5 toolkit for automatic evaluation of the content informativeness, and the toolkit was officially adopted by DUC for automatic summarization evaluation. The toolkit measures summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary [21]. The ROUGE-1.5.5 toolkit reports separate F-measure scores for 1, 2, 3 and 4-gram, and also for longest common subsequence co-occurrences. In this study, we show four ROUGE F-measure scores in the experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), ROUGE-W (based on weighted longest common subsequence, weight=1.2), and ROUGE-SU* (based on skip bigram with unigram)4.
Reading Easiness: This aspect is not evaluated by previous summarization tasks. We aim to evaluate the reading easiness level of the whole summary. Because the reading easiness level of a summary is dependent on the reading easiness scores of the sentences in the summary, we use the average reading easiness score of the sentences in a summary as the summary's reading easiness level. The overall reading easiness score is the average across all 25 document sets.
4 We also used the option "-l 100" for truncating the summary and used the option "-m" for word stemming when using the ROUGE-1.5.5 toolkit.

In addition to the above automatic evaluation procedures, we also performed pilot user studies for evaluation. Four Chinese college students participated in the user studies. We have developed a user study tool for facilitating the subjects to evaluate each summary from the two aspects of content informativeness and reading easiness. Each subject can assign a score from 1 to 5 on each aspect for each summary. For reading easiness, 1 means "very hard to understand", and 5 means "very easy to understand". For content informativeness, 1 means "least informative", and 5 means "very informative". During each user study procedure, we compared two summarization systems' results. And the two summaries produced by the two systems for the same document set were presented in the same interface, and then the four subjects assigned scores to each summary after they read and compared the two summaries. The final score of a summary on one aspect was the average of the scores assigned by the four subjects. And the overall scores were averaged across all subjects and all 25 document sets.
4.2.2 Experimental Results
4.2.2.1 Automatic Evaluation Results
In this section, we report the automatic evaluation results of EUSUM from both two aspects. Though the combination weight  in EUSUM can be set to any non-negative value, it ranges from 0 to 1 in our experiments, because a much larger  will lead to a big sacrifice of the content informativeness for the summary. The penalty degree factor  for EUSUM is set to 10, as in [31]. Table 2 shows the ROUGE scores and the reading easiness score of the EUSUM system with the centroid-based method, which is denoted as EUSUM(Centroid). Table 3 shows the ROUGE scores and the reading easiness score of the EUSUM system with the graphbased method, which is denoted as EUSUM(Graph).
Seen from the tables, the ROUGE scores of EUSUM(Centroid) and EUSUM(Graph) are decreased with the increase of the combination weight , and the reading easiness scores of them are increased with the increase of . And we can see that with the increase of , the summary's reading easiness can be more quickly becoming significantly different from that of the summary with =0, while the summary's content informativeness is not significantly affected when  is set to a small value. Moreover, For EUSUM(Graph), even the ROUGE scores with =0.1 are better than that with =0. The results demonstrate that when  is set to a small value, the content informativeness aspect of the extracted summary are almost not affected, but the reading easiness aspect of the extracted summary can be significantly improved.
By comparing the performance values in the two tables, we can see that when  is fixed, the ROUGE-1, ROUGE-W and ROUGESU* scores of EUSUM(Graph) are higher than the corresponding scores of EUSUM(Centroid), which verifies the effectiveness of the graph-based summarization method. We can also see that when  is fixed, the reading easiness scores of EUSUM(Graph) are always higher than the corresponding scores of EUSUM(Centroid), which demonstrates that EUSUM(Graph) can extract more easy-to-understand summaries than EUSUM(Centroid). We explain the results by that the graph-based sentence extraction method tends to extract sentences with good feature values for indicating reading easiness. For example, sentence length is one of the important features for reading

495

easiness prediction, and a shorter sentence is more likely to be easy to understand. We compare the average sentence length (average word number per sentence) in the summaries extracted by EUSUM(Graph) and EUSUM(Centroid) in Figure 1. We can see that EUSUM(Graph) usually extracts shorter sentences than EUSUM(Centroid), which verifies the results from one perspective. Overall, the results show that EUSUM(Graph) is more suitable than EUSUM(Centroid) for extracting easy-tounderstand summaries5.

Table 2. EUSUM (Centroid) results vs. 



ROUGE-1 Average_F

ROUGE-2 ROUGE-W ROUGE-SU*

Average_F Average_F

Average_F

Reading Easiness
score

0

0.31682

0.05896

0.13532

0.09405

3.45681

0.1

0.31468

0.05529

0.13346

0.09335

3.53835

0.2

0.31419

0.05488

0.13367

0.09254

3.58633

0.3

0.31153

0.05379

0.13311

0.09085

3.65514

0.4

0.30938

0.05255

0.13203

0.08878

3.74286

0.5

0.30754

0.6

0.30402

0.7

0.30228

0.8

0.30227

0.9

0.29344

0.05029 0.04920 0.04792 0.04700 0.04233

0.13113 0.13011 0.12955 0.12959 0.12579

0.08802 0.08648 0.08506 0.08353 0.07808

3.82912 3.85643 3.96930 4.09463 4.22541

1

0.29242

0.04033

0.12514

0.07762

4.33986

Table 3. EUSUM (Graph) results vs. 



ROUGE-1 Average_F

ROUGE-2 ROUGE-W ROUGE-SU*

Average_F Average_F

Average_F

Reading Easiness
score

0

0.32010

0.05205

0.13701

0.09488

3.91532

0.1

0.32286

0.05364

0.13928

0.09612

3.98131

0.2

0.31928

0.05155

0.13693

0.09414

4.07323

0.3

0.31751

0.04920

0.13587

0.09235

4.16455

0.4

0.31598

0.04677

0.13518

0.09100

4.31125

0.5

0.30828

0.04544

0.13231

0.08773

4.40275

0.6

0.30723

0.04516

0.13308

0.08753

4.47232

0.7

0.30608

0.04525

0.13306

0.08656

4.56735

0.8

0.30197

0.04396

0.13165

0.08430

4.63991

0.9

0.29936

0.04343

0.13131

0.08324

4.67847

1

0.29498

0.04166

0.12993

0.07964

4.72482

(The bolded scores indicate that the difference between the scores and the corresponding scores when =0 is statistically significant by using t-test.)

In the above experiments, the penalty weight  is fixed to 10. We now take EUSUM(Graph) as an example to show how the penalty weight  influences the two aspects of the proposed summarization system. Figures 2 and 3 show the reading easiness score curves and the ROUGE-SU* F-score curves of EUSUM(Graph) with different settings, respectively. We can see that the reading easiness scores of EUSUM(Graph) with different settings have a tendency to increase with the increase of . And after  is larger than 10, the reading easiness scores for most settings do not change any more, which shows that the penalty weight has no significant influences on the reading easiness of the summaries when the weight is set to a moderately large value. The ROUGE-SU* scores are firstly increasing with the increase of

5 Actually, we can improve the centroid-based method by incorporating some useful features such as sentence length, but it is not the focus of this paper.

 and then decreased with the increase of , which demonstrates that less or much penalty will lower the performance instead of content informativeness.

30

Average sentence length

25

20

15

10

EUSUM(Centroid)

5

EUSUM(Graph)

0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 

Figure 1. EUSUM average sentence length (sentence word number) comparison.

Reading easiness score

5 4.8 4.6 4.4 4.2
4 3.8
0

=0 =0.8
24

=0.2 =1

=0.4

=0.6

6 8 10 12 14 16 18 20 

Figure 2. EUSUM(Graph) reading easiness vs. penalty weight.

ROUGE-SU* Average_F

0.1 0.095
0.09 0.085
0.08 0.075
0.07 0.065
0.06 0

=0 =0.8
24

=0.2 =1

=0.4

=0.6

6 8 10 12 14 16 18 20 

Figure 3. EUSUM(Graph) content informativeness vs. penalty weight.

4.2.2.2 User Study Results
In order to validate the effectiveness of the system by real nonnative readers, two user study procedures were performed:
User study 1: The summaries extracted by EUSUM(Centroid) (=0) and EUSUM(Graph) (=0.2) are compared and scored by subjects. Seen from Tables 2 and 3, most ROUGE scores of the two systems are very similar, and the reading easiness score of EUSUM(Graph) (=0.2) is higher than that of EUSUM(Centroid) (=0). Table 4 gives the averaged subjective scores of the two systems. The user study results verify that the summaries by EUSUM(Graph) (=0.2) are indeed significantly easy to understand by non-native readers, while the content informativeness of the two systems are not significantly different.
User study 2: The summaries extracted by EUSUM(Graph) (=0) and EUSUM(Graph) (=0.3) are compared and scored by subjects.

496

Seen from Tables 2 and 3, most ROUGE scores of the two
systems are not significantly different, and the reading easiness score of EUSUM(Graph) (=0.3) is higher than that of EUSUM(Graph) (=0). Table 5 gives the averaged subjective
scores of the two systems. The user study results verify that the summaries by EUSUM(Graph) (=0.3) is indeed significantly
easy to understand by non-native readers, while the content
informativeness of the two systems are not significantly different.

Table 4. Results for user study 1

EUSUM(Centroid) (=0) EUSUM(Graph) (=0.2)

Content Informativeness
3.47 3.63

Reading Easiness
3.33 4.01*

Table 5. Results for user study 2

EUSUM(Graph) (=0)

Content Informativeness
3.71

Reading Easiness
3.73

EUSUM(Graph) (=0.3)

3.52

4.02*

(* indicates that the performance difference is statistically significant by using ttest.)

4.2.2.3 Running Examples
In order to better compare the results, we give several typically extracted summaries for two document sets D14 and D59. The predicted reading easiness score of each sentence is also given in brackets.
EUSUM(Centroid)(=0) for D14:
A U.S. Air Force F-111 fighter-bomber crashed today in Saudi Arabia, killing both crew members, U.S. military officials reported. (3.97397) A jet trainer crashed Sunday on the flight deck of the aircraft carrier Lexington in the Gulf of Mexico, killing five people, injuring at least two and damaging several aircraft (3.182) U.S. Air Force war planes participating in Operation Desert Shield are flying again after they were ordered grounded for 24 hours following a rash of crashes. (3.41654) A U.S. military jet crashed today in a remote, forested area in northern Japan, but the pilot bailed out safely and was taken by helicopter to an American military base, officials said. (3.42433)
EUSUM(Graph)(=0) for D14:
The U.S. military aircraft crashed about 800 meters northeast of a Kadena Air Base runway and the crash site is within the air base's facilities. (3.84771) Two U.S. Air Force F-16 fighter jets crashed in the air today and exploded, an air force spokeswoman said. (4.35604) West German police spokesman Hugo Lenxweiler told the AP in a telephone interview that one of the pilots was killed in the accident. (3.79754) Even before Thursday's fatal crash, 12 major accidents of military aircraft had killed 95 people this year alone. (3.92878) Air Force Spokesman 1st Lt. Al Sattler said the pilot in the Black Forest crash ejected safely before the crash and was taken to Ramstein Air Base to be examined. (3.70656)
EUSUM(Graph)(=0.3) for D14:
Two U.S. Air Force F-16 fighter jets crashed in the air today and exploded, an air force spokeswoman said. (4.35604) The U.S. military aircraft crashed about 800 meters northeast of a Kadena Air Base runway and the crash site is within the air base's facilities. (3.84771) West German police spokesman Hugo Lenxweiler told the AP in a telephone interview that one of the pilots was killed in the accident. (3.79754) Even before Thursday's fatal crash, 12 major accidents of military aircraft had killed 95 people this year alone. (3.92878) However, suspension of training flights indicated otherwise. (4.97415) Listed as dead from the 433rd were Maj. (4.99479)
EUSUM(Centroid)(=0) for D59:

The Northwest Airlines jet that crashed Sunday in Detroit, killing at least 154 people, was involved in two incidents of engine failure in the past two years. (3.54385) A French DC-10 jetliner with 171 people aboard experienced a powerful highaltitude explosion, possibly from a terrorist bomb, before crashing in a remote desert region of Niger in northern Africa, officials in France said Wednesday. (2.60945) Freshman congressman Larkin Smith (R-Miss). died in a light plane crash in Mississippi, authorities said Monday, making him the second member of the House killed in an aviation accident in a week. (3.18245) Local news reporters quoted witnesses as saying that the plane appeared to nosedive into the earth. (3.9432)
EUSUM(Graph)(=0) for D59:
Sunday's crash was the first time in 24 years that passengers were killed in an accident involving a Northwest plane. (3.98315) FAA officials at the crash scene wouldn't speculate on the reasons for the crash or comment on the plane's engines. (4.16897) There were reports from passengers and observers that the plane's right-wing engine also failed before the crash. (3.94093) In July 1988, a United DC-10 crashed in Sioux City, Iowa, after an engine broke apart in flight, killing 112 people. (4.2899) The worst airline accident ever in the U.S. was the 1979 crash of an American Airlines jet in Chicago. (4.38547) FAA records show that besides those incidents that involved the plane that crashed, problems with the turbine sections of JT8D-200 series engines occurred on three Republic flights in the past four years. (2.97711)
EUSUM(Graph)(=0.3) for D59:
Sunday's crash was the first time in 24 years that passengers were killed in an accident involving a Northwest plane. (3.98315) FAA officials at the crash scene wouldn't speculate on the reasons for the crash or comment on the plane's engines. (4.16897) There were reports from passengers and observers that the plane's right-wing engine also failed before the crash. (3.94093) A team of National Transportation Safety Board investigators left Washington Wednesday night for Sioux City. (4.06095) The DC-10 operated by the French airline UTA crashed Tuesday after taking off from N'Djamena, Chad, on a flight that originated in Brazzaville, Congo. (3.63393) It can smash an airplane into the ground. (4.86026)
5. DISCUSSION
In this study, the experiments were performed by Chinese college students. Because college students in different countries may have different English reading levels, the experimental results may be slightly changed if we use non-native students in other countries for evaluation. Even for Chinese readers, college students and high school students may have different English reading levels, and thus the experimental results may be slightly changed if we use high school students for evaluation. That's to say, the reading easiness level of a summary should be adjusted with the particular reader. In practice, we can let readers to tune the combination weight  in the proposed EUSUM system, and they can select the best weight for extracting summaries best suitable for reading.
Similarly, for native English readers, different persons may have different English reading levels, and thus the framework proposed in this paper is also applicable. However, the reading easiness score of each sentence may be different because of the differences between the English reading abilities and behaviors of Chinese readers and native English readers.
6. CONCLUSION AND FUTURE WORK
In this study, we investigate the new factor of reading easiness for document summarization, and we propose a novel summarization system - EUSUM for producing easy-to-understand summaries for non-native readers. We performed automatic evaluation and user study to verify the effectiveness of the proposed system.

497

In future work, we will further improve the summary's reading easiness in the following two ways: 1) The summary fluency (e.g. sentence ordering in a summary) has influences on the reading easiness of a summary, and we will consider the summary fluency factor in the summarization system. 2) More sophisticated sentence reduction and sentence simplification techniques will be investigated for improving the summary's readability.
7. ACKNOWLEDGMENTS
This work was fully supported by NSFC (60873155), and partially supported by RFDP (20070001059), Beijing Nova Program (2008B03), NCET (NCET-08-0006) and National High-tech R&D Program (2008AA01Z421).
8. REFERENCES
[1] M. R. Amini, P. Gallinari. The Use of Unlabeled Data to Improve Supervised Learning for Text Summarization. In Proceedings of SIGIR2002, 105-112.
[2] R. Barzilay, N. Elhadad and K. McKeown, Inferring strategies for sentence ordering in multidocument news summarization, Journal of Artificial Intelligence Research 17, 2002.
[3] D. Bollegala, N. Okazaki and M. Ishizuka. A bottom-up approach to sentence ordering for multi-document summarization. In Proceedings of ACL2006.
[4] T. Brants, A. Franz. Web 1T 5-gram Version 1. Linguistic Data Consortium, Philadelphia, 2006.
[5] J. S. Chall and E. Dale. Readability revisited: the new DaleChall readability formula. Brookline Books. Cambridge, MA, 1995.
[6] C.-C. Chang and C.-J. Lin. LIBSVM : a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm
[7] K. Collins-Thompson and J. Callan. Predicting reading difficulty with statistical language models. Journal of the American Society for Information Science and Technology, 56(13), 2005.
[8] G. ErKan, D. R. Radev. LexPageRank: Prestige in MultiDocument Text Summarization. In Proceedings of EMNLP2004.
[9] T. L. François. Combining a statistical language model with logistic regression to predict the lexical and syntactic difficulty of texts for FFL. In Proceedings of the EACL2009 Student Research Workshop, 2009.
[10] S. Harabagiu and F. Lacatusu. Topic themes for multidocument summarization. In Proceedings of SIGIR-05.
[11] M. Heilman, K. Collins-Thompson, J. Callan and M. Eskenazi. Combining lexical and grammatical features to improve readability measures for first and second language texts. In Proceedings of HLT-2007.
[12] M. Heilman, K. Collins-Thompson and M. Eskenazi. An analysis of statistical models and features for reading difficulty prediction. In Proceedings of the 3rd Workshop on Innovative Use of NLP for Building Educational Applications, 2008.
[13] T. Kanungo and D. Orr. Predicting the readability of short web summaries. In Proceedings of WSDM2009.
[14] P. Kidwell, G. Lebanon and K. Collins-Thompson. Statistical estimation of word acquisition with application to readability prediction. In Proceedings of EMNLP2009.

[15] J. Kincaid, R. Fishburne, R. Rodgers and B. Chissom. Derivation of new readability formulas for navy enlisted personnel. Branch Report 8-75. Chief of Naval Training, Millington, TN, 1975.
[16] D. Klein and C. D. Manning. Fast Exact Inference with a Factored Model for Natural Language Parsing. In Proceedings of NIPS-2002.
[17] J. Kupiec, J. Pedersen, F. Chen. A.Trainable Document Summarizer. In Proceedings of SIGIR1995, 68-73.
[18] W. Li, F. Wei, Q. Lu and Y. He. PNR2: ranking sentences with positive and negative reinforcement for query-oriented update summarization. In Proceedings of COLING-08.
[19] C. Y. Lin, E. Hovy. The Automated Acquisition of Topic Signatures for Text Summarization. In Proceedings of the 17th Conference on Computational Linguistics, 495-501, 2000.
[20] C..-Y. Lin and E.. H. Hovy. From Single to Multi-document Summarization: A Prototype System and its Evaluation. In Proceedings of ACL-02.
[21] C.-Y. Lin and E.H. Hovy. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics. In Proceedings of HLT-NAACL -03.
[22] H. P. Luhn. The Automatic Creation of literature Abstracts. IBM Journal of Research and Development, 2(2), 1969.
[23] R. Mihalcea, P. Tarau. TextRank: Bringing Order into Texts. In Proceedings of EMNLP2004.
[24] R. Mihalcea and P. Tarau. A language independent algorithm for single and multiple document summarization. In Proceedings of IJCNLP-05.
[25] A. Nenkova and A. Louis. Can you summarize this? Identifying correlates of input difficulty for generic multidocument summarization. In Proceedings of ACL-08:HLT.
[26] E. Pitler and A. Nenkova. Revisiting readability: a unified framework for predicting text quality. In Proceedings of EMNLP2008.
[27] D. R. Radev, H. Y. Jing, M. Stys and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40: 919-938, 2004.
[28] S. Schwarm and M. Ostendorf. Reading level assessment using support vector machines and statistical language models. In Proceedings of ACL2005.
[29] A. J. Stenner. Measuring reading comprehension with the Lexile framework. Fourth North American Conference on Adolescent/Adult Literacy, 1996.
[30] V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.
[31] X. Wan, J. Yang and J. Xiao. Using cross-document random walks for topic-focused multi-documetn summarization. In Proceedings of WI2006.
[32] X. Wan and J. Yang. Multi-document summarization using cluster-based link analysis. In Proceedings of SIGIR-08.
[33] X. Wan, J. Yang and J. Xiao. Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction. In Proceedings of ACL2007.
[34] K.-F. Wong, M. Wu and W. Li. Extractive summarization using supervised and semi-supervised learning. In Proceedings of COLING-08.

498

The Effect of Assessor Errors on IR System Evaluation

Ben Carterette
Dept. of Computer and Information Sciences University of Delaware Newark, DE 19716
carteret@cis.udel.edu

Ian Soboroff
National Institute of Standards and Technology Gaithersburg, MD 20899
ian.soboroff@nist.gov

ABSTRACT
Recent efforts in test collection building have focused on scaling back the number of necessary relevance judgments and then scaling up the number of search topics. Since the largest source of variation in a Cranfield-style experiment comes from the topics, this is a reasonable approach. However, as topic set sizes grow, and researchers look to crowdsourcing and Amazon's Mechanical Turk to collect relevance judgments, we are faced with issues of quality control. This paper examines the robustness of the TREC Million Query track methods when some assessors make significant and systematic errors. We find that while averages are robust, assessor errors can have a large effect on system rankings.
Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation, H.3.5 Online Information Services
General Terms: Experimentation, Measurement
Keywords: assessor error, retrieval test collections
1. INTRODUCTION
Since TREC began in 1992 as the first large-scale use of pooling to create test collections, a great deal of research has focused on examining the quality of pooled test collections in spite of violations of the Cranfield assumptions [9, 17, 13, 14, 15] and on refining pooling to reduce costs and/or maximize quality [8, 12, 11, 3, 16, 5]. The TREC Million Query track [1] has emerged as a testbed for modern testcollection building methods, primarily those of Aslam and Pavlu [2] and Carterette et al. [6].
A primary motivation in many modern collection-building methods is to reduce the costs associated with making relevance judgments. Relevance assessors can be expensive to hire, train, and use, and particularly in the academic community where funding may not be available for collection building, low-cost (or zero-cost) methods have broad appeal, if not as yet broad application.
Recently, crowdsourcing and Amazon's Mechanical Turk (MTurk)1 have been used as sources of relevance judgments.
1https://www.mturk.com/mturk/welcome
Copyright 2010 Association for Computing Machinery. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of the U.S. Government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

These approaches have a very low cost per judgment, but they may have somewhat higher design costs and require additional cost and effort to control for assessor error. In particular, if an MTurk worker's only goal is to complete the task, the judgments may not look very different from random. Soboroff et al. has simulated a family of cases of random assessor errors: the assessors know roughly how many relevant documents there are (give or take a standard deviation or two), but they pay little to no attention to which documents they are judging relevant [12]. Soboroff et al. assigned relevance to documents in the pool randomly, creating a pseudo-rels that they used to evaluate systems. Surprisingly, evaluation results over the pseudo-rels correlated significantly to evaluation results over the "true" relevance judgments from the assessors.
That work had the relative luxury of deep pools of documents, which may have resulted in emergence of patterns. But even an assessor making judgments at random would take a fair amount of time to make, say, 12,000 relevance judgments. Recent work on test collections suggests that assessing more topics with fewer judgments each is a more cost-effective approach, since the topics are a larger source of variance than the missing judgments [11, 7]. But that work assumes the judgments are in some sense "perfect", i.e. that errors made by assessors have inconsequential variance and no bias. This is almost certainly not the case; assessor make mistakes due to misunderstandings of the task or documents, fatigue, boredom, and for many other reasons. These mistakes surely have a larger impact on evaluation when there are only a few relevance judgments to begin with.
Furthermore, most TREC collections are built using trained relevance assessors. Practitioners and researchers using modern collection building methods such as those pioneered in the TREC Million Query track may wish to create their own test collections using available sources of labor, such as Amazon's Mechanical Turk or other crowdsourcing methods. Bailey et al. [4] found that assessors of differing task and domain expertise could affect system rankings markedly; Kinney et al. [10] found that non-expert assessors judging domain-specific queries make significant errors affecting system evaluation. When assessors are not closely managed or highly trained, mistakes must be common.
Our goal is to investigate the effect of assessor errors by simulating different assessor "archetypes" in a low-cost largescale test collection scenario. We propose several models of how assessors might make errors, then use the models to simulate assessors going through the process of making judgments. We focus specifically on the effect in the TREC

539

Million Query track test collections, where there are many queries with very few judgments each.
2. ASSESSOR BEHAVIOR AND ERRORS
We mined a large log of assessor interaction data from the TREC 2009 Million Query (MQ) track for patterns of assessor behavior. The log contains a record for each judgment; each record consists of a timestamp, the query number, the document ID, the assessor ID, and the judgment itself. We examined the first 32 judgments per topic and excluded inter-judgment times in excess of 200 seconds. Based on this data, and further on our experience managing relevance assessors for various projects, we present some types of errors assessor might make, some ways we can model them, and hypothesized effects on evaluation in a MQ-type setting.
First we identify the following broad trends that can be used to model assessor behavior. These formed the basis for some of our models below.
1. Time between judgments decreases slightly from the first judgments for a topic to the last judgment for a topic (Fig. 1(a)).
2. It takes less time to judge a nonrelevant document than to judge a document with any degree of relevance (Fig. 1(b)).
3. Assessors can vary in their judging times (Fig. 1(c)) Our assessors do not display much variation but we might see more so in a larger set.
4. Measured across non-overlapping but large sets of topics, assessors vary in the proportion of documents they judge relevant (Fig. 1(d)).
These observations clearly indicate that assessors behave differently (i.e., there is variance due to the assessor), and moreover that there is interaction between assessor and document. There may also be interaction between assessor and topic. Because each topic was judged only once we will not find evidence of that in aggregate, but we looked for a particular type of interaction: topics that an assessor gave less attention to than normal, possibly due to unhappiness with the documents they were asked to judge. To do this, we modeled time between judgments as a function of assessor, document judgment, and where in the sequence the judgment fell. We then looked for topics for which the actual judgment times were lower than those predicted by the model across most of the sequence, specifically cases where at least 90% of the judgments were faster than expected. There were about 30 such topics, and for all 30 all documents were nonrelevant.
Finally, we looked for evidence of autocorrelation in judgments. We calculated the proportion of times an assessor judged a document relevant conditional on judging the previous document relevant, and contrasted that with the proportion conditional on judging the previous document nonrelevant. We used a subset of queries with roughly equal proportions of relevance so any effect would not be confounded by differing numbers of relevant documents. Assessors are in fact more likely to make the same judgment twice in a row: P (ji = 1|ji-1 = 1) = 0.22, while P (ji = 1|ji-1 = 0) is only 0.18. This difference is significant by a two-sample two-proportion test.
540

200

Time to judge (seconds)

150

100

50

0 0

10

20

30

Judgments in sequence

(a) Time to judge a document decreases slightly over the sequence of documents in a topic.

relevant

0

50

100

150

200

related

G

G

not relevant

highly relevant

G

G

0

50

100

150

200

Time to judge (seconds)

(b) Relevant, highly relevant, and related documents take longer to judge than nonrelevant ones.

200

Time to judge (seconds)

150

100

50

G

G

G

G

G

G

0

4

5

6

7

8

9

Assessor

(c) Our assessors vary only slightly in their judgments times.

not-rel

rel

high-rel

related

9

8

Assessor

7

6

5

4

0

1000

2000

3000

4000

5000

Judgments

(d) Assessors vary in the proportion of documents they judge relevant, as measured over large non-overlapping topic sets.

Figure 1: Interaction log observations.

2.1 Model Distributions and Priors
We will simulate different types of assessors using models of systematic errors based on observations above. Given a set of (binary) judgments for a topic, we convert nonrelevant judgments to relevant and vice versa according to a model.
If we are not careful, we can easily inject too much bias into the evaluation. For instance, if we model an assessor that has a tendency to overrate relevance by changing a judgment of nonrelevance to relevance with fixed probability p, the expected effect is to simply increase the number of relevant documents to pn, where n is the number of judged documents. Increasing the number of relevant documents in this way does not really address the question; some of those topics may very clearly have no relevant documents and even an optimistic assessor would not say they do. We therefore tied model parameters to the number of known relevant documents for each topic.
To do this, we define a model in terms of "background" parameters modeling average behavior; these parameters are then adjusted by topic. This is easy to do using discrete distributions such as Bernoulli and Poisson distributions. The parameters of both distributions have natural conjugate priors that allow updating based on existing judgments.
The Bernoulli distribution can be seen as modeling the probability p that a document is relevant; the parameter p can be modeled as having a Beta distribution. A Beta distribution is specified with two parameters ,  and has mean /( + ). The posterior of a Beta random variable also has a Beta distribution (hence it is conjugate); the posterior parameters are  + r,  + (n - r), where n is a number of observations and r is the number of positive observations. We can therefore view a Beta prior as a model of an assessor's average probability of judging a document relevant, and a Beta posterior using the number of relevant documents rq among nq judgments for a topic q as a model of an assessor's probability of judging a document relevant to q.
Similarly, the Poisson distribution can be seen as modeling the number of documents that will be judged relevant in some sequence length based on a rate parameter ;  can be modeled as having a Gamma distribution. Like a Beta distribution, a Gamma distribution is specified with two parameters , , but its mean is /. The posterior of a Gamma random variable has a Gamma distribution with parameters  + r,  + n. We can therefore view a Gamma prior as a model of an assessor's rate of judging documents relevant, and a Gamma posterior using rq and nq as the rate adjusted for the topic.
2.2 Assessor Models
Our baseline model is that an assessor makes judgments randomly. Real assessors of course do not make random relevance judgments; this model only serves as comparison to the more informed models below. Using the Beta distribution as described above, the probability that a document will be judged relevant to a topic q will be ( + rq)/( +  + nq). So, for example, an assessor modeled by prior parameters  = 2,  = 8 (expected to judge 20% of documents relevant) confronted by a topic for which all 32 documents have been judged nonrelevant will have a 2/42 = 0.05 probability of judging each of those documents relevant. The effect of this is that the number of simulated relevant documents for each topic can be expected to be "near" the actual number of

relevant documents, but with enough noise that evaluation results will change.
Our first realistic model is an unenthusiastic assessor, that is, the assessor is not interested in reading or understanding documents and simply wants to complete the job. Judgments by this assessor may be characterized by a pattern such as judging everything nonrelevant, or alternating judgments in some pattern. Our discovery of topics that were completed faster than usual and with all nonrelevant judgments lends support to this model. For the unenthusiastic assessor we do not suppose that there is any particular probability model. The assessor just follows a fixed pattern, such as judging everything nonrelevant or alternating between relevant and nonrelevant judgments. This is in some sense the most biased model, in that the simulated judgments have nothing to do with the actual judgments.
Our second model is the optimistic assessor; he or she takes an overly-broad view of the topic and ends up judging things relevant that are not. It is well-known from work by Harman [9] and Voorhees [13] that assessors do differ reasonably in their judgment of relevance; in this model and the one following we presume a view of the topic which most observers would consider beyond reasonable, for example judging relevance solely by the presence of certain terms or not correctly identifying a spam document as not relevant. We model optimistic assessors as being more likely to judge a nonrelevant document relevant. The model parameters are again Beta parameters , , and the probability that a document will be judged relevant to a particular topic is ( + rq)/( +  + nq). However, for this model we can only change nonrelevant judgments to relevant; we will not change any of the relevant documents to nonrelevant.
Conversely, we model a pessimistic assessor as taking an overly-narrow view of the topic and judging documents nonrelevant that should be considered relevant. The model is identical to the optimistic model, except that relevant documents become nonrelevant with probability ( + (nq - rq))/( +  + nq) (which comes from treating a nonrelevant judgment as the "positive" outcome).
Another model we explore we call topic-disgruntled. This assessor chose a query for some reason (interest in topic, seemed easy), but the documents turned out to be something else (different topic, harder than expected). Disgruntled by the topic, the assessor begins to click through rapidly after the first few judgments. Again, the presence of topics completed faster than usual lends support to this model. The model is time-based. After k judgments, the assessor becomes disgruntled and judges the remaining documents nonrelevant. The parameter is a "patience parameter" ; after nq judgments (which are identical to the "true" judgments), the assessor judges everything else nonrelevant. This assessor has a Gamma prior specified by parameters ,  (resulting in prior patience  = /), and their posterior patience for a given topic is based on how many relevant documents there are:  = ( + rq)/( + nq).
Similar in execution to the disgruntled model is the lazy/ overfitting assessor. The assessor sees a few very nonrelevant (or a few very relevant) documents at the start of judging and unjustly assumes all subsequent documents are likely to be the same. He or she begins rapidly entering judgments conforming to his early judgments. The model is implemented roughly the same as the topic-disgruntled

541

model, except that it only kicks in if the first nq judgments are all nonrelevant (or all relevant).
Another model is that the assessor is fatigued. The assessor starts each day alert and attentive, but tires as time passes. Judgments become more random as a result. This is also a time-based model. We again begin with a Beta prior. For this model, however, there is an assessor model with parameters ,  as well as separate priors for each judgment; we will denote the parameters i, i. The posterior for a given judgment will be (i +rqi)/(i +i +i), where rqi is the number of documents judged relevant to q up to judgment i. For i = 0 we will set i, i to zero. The first judgment for a topic, then, will be the same. For each subsequent judgment, the Beta parameters will grow with assessor parameters , : after each judgment, i and i increase by  and  respectively. The effect is that after k judgments, the posterior probability of judging a document relevant will be (k + rqk)/(k + k + k). As k increases, the assessor converges to judging every document according to their prior probability /( + ).
Our final model is Markovian, that is, the assessor's judgments are conditional on previous judgments. This could simulate an assessor who "feels bad" about judging too many nonrelevant documents in a row and thus takes a broader view of the topic over time, or one who takes a narrower view after judging many relevant documents in a row. The observation that assessors are more likely to make the same judgment twice in a row supports this model.
3. ASSESSOR SIMULATION
To analyze the effects of particular types of systematic error, we simulated assessors judging documents in a TREClike setting: an assessor is given a topic description and reads documents to judge whether they are relevant to the topic. Since we are particularly interested in test collections with very many lightly-judged topics, we used the TREC 2009 Million Query track data as the starting point for our simulations. Before describing the simulation procedure and results, we briefly describe the track.
3.1 TREC Million Query Track
The Million Query track was designed to study the use of low-cost evaluation methods in the TREC setting. It produces test collections that consist of a large number of lightly-judged topics. Judgments in the Million Query track are either not relevant, related (but not relevant), relevant, or highly relevant. In 2009, 638 topics received a total of 34,534 judgments (54 per topic on average), of which 26% were either relevant or highly relevant. There were 95 topics for which no relevant documents were found.
The low-cost methods used by the track attempt to target judgments that are going to be more useful in evaluation. The Million Query track uses two methods to select documents to judge. One (statAP) is an approach based on statistical sampling, in which each judged relevant documents is taken to be representative of some population of relevant documents in the same "region" from which it was sampled [2]. The other (MTC) is an algorithmic approach that weighs documents according to how informative a judgment to them is expected to be; after each judgment, it recomputes the weights given the new information and presents the top-weighted document for judging [6].
Errors in judging can have unpredictable effects in both

methods. In statAP, an erroneous judgment of relevance can have a major impact on the estimated number of relevant documents for the topic, particularly if the judgment is to a document that has a low probability of being sampled. Conversely, an erroneous judgment of nonrelevance will cause the number of relevant documents to be underestimated. In MTC, an erroneous judgment can result in the algorithm taking an entirely different path.
The two different approaches to selecting judgments have different approaches to evaluation that are based on different assumptions. The sampling approach takes each judgment of relevance as representative of a set of relevant documents; this set is used to calculate an unbiased estimator of average precision. The algorithmic approach can take one of two tacks: it can either bound the differences in average precision between pairs of systems or it can compute a probability that the difference in average precision is less than zero over the space of possible judgments that could be made to unjudged documents. The probabilistic approach is generally more useful, and it has the advantage of being able to produce an estimate of average precision called "expected average precision" (EAP). Unlike the statMAP estimate, EAP is highly biased, but because it is meant for pairwise comparisons the bias can be expected to cancel out. Plots of EAP typically have very low values compared to plots of statAP.
We have decided to limit our focus to the effects on statAP. There are two reasons: first, because statAP estimates "look like" standard average precision, it is easy to see how errorful judgments are causing errors in the estimates. EAP estimates look very different from average precision, and because it is already very biased, changing the judgments will not necessarily cause obvious differences in their values. The second reason to prefer statAP is that MTC cannot effectively be simulated in the Million Query track data. If one judgment changes, it is likely that some future document selected for judging will be one that we do not already have a judgment on.

3.1.1 statAP
Some understanding of statAP is necessary to understand how an errorful judgment affects the estimate. statAP is a method for sampling a set of documents S to be judged, then using those judgments to estimate average precision. The statAP estimate is calculated as:

statAP

=

1 R

dS

xdprec@r(d) d

where xd is the relevance of document d (1 for relevant, 0 for not relevant), d is an inclusion probability calculated
for sampling, R is an estimate of the number of relevant
documents, and prec@r(d) is an estimate of the precision at the rank at which document d appears. The estimates of precision and the number of relevant documents are:

R=

xd ,

dS d

prec@k = 1

xd

k dS,r(d)k d

The ratio xd/d can be thought of as the number of relevant documents that xd is representative of in the same "region" of documents with similar inclusion probabilities. A lower d gives greater weight to a relevant document, increasing the estimated numbers of relevant documents compared to a relevant document with a higher d.

542

statMAP statMAP

0.3 true statMAP
simulated statMAP
0.25

0.2

0.15

0.1

0.05 0

5

10

15

20

25

30

35

System number (ordered by statMAP)

(a) Judgments from a random assessor give a small (but significant) rank correlation;  = 0.35.

0.28

true statMAP

0.26

simulated statMAP

0.24

0.22

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06 0

5

10

15

20

25

30

35

System number (ordered by statMAP)
(b) Judgments from an unenthusiastic assessor give a small (but significant) rank correlation;  = 0.33.

Figure 2: statMAP system scores after applying the two simplest assessor models.

3.2 Simulation Procedure
The simulation proceeds as follows: for a given topic, we start with the sequence of judgments in the same order they were originally made (this information is provided in the "fullrels" file distributed with the Million Query track data). We alter the judgment according to each of the models above. For those models that involve random sampling, we perform 25 trials on each judgment for each topic. We used increasing powers of two for parameter values, i.e.  and  ranged from 1 to 1024 independently. When complete, we have an errorful "prels" file that we can use to evaluate the Million Query track systems with statAP.
After re-evaluating Million Query track systems, the simplest approach to determining the effect of errors is to measure how well the new evaluation correlates to the "true" evaluation resulting from using the original relevance judgments. Kendall's  rank correlation is widely-used for this. Kendall's  is a function of the number of system pairs that swap between two rankings. The more swaps, the lower  is; when  = 1 the rankings are identical.
3.3 Simulation Results
System evaluation results based on the judgments from the first two assessor models--random judging with prior parameters  = 1,  = 8 and an unenthusiastic assessor that alternates between nonrelevant and relevant judgments to stay amused--are shown in Figure 2. In both cases the Kendall's  correlation to the official ranking is around 0.34, and remains consistently around 0.34 no matter what the prior parameters are and no matter what judging pattern is used (among those we tried). This can be thought of as a baseline for an assessor who is not actively malicious but is not interested in making an effort.
Figure 3 illustrates altered system rankings based on the other models for selected parameter values. For models with random sampling, error bars indicate the distribution of MAP estimates observed over 25 trials.
The optimistic and pessimistic models give very different results even when the prior parameter give equal probability of changing the judgment. Rank correlations based on optimistic judgments quickly degrade, while rank correlations

based on pessimistic judgments degrade much more slowly. Figures 3(a) and 3(b) demonstrate this for  = 1,  = 16 (for the optimist) and  = 16,  = 1 (for the pessimist). Roughly the same number of judgments changed in both cases, but the effect on performance is much worse when those changes create more relevant documents than when they create more nonrelevant documents. With the pessimistic model, in fact, the correlation is nearly perfect; the scores have simply shifted downward.
The disgruntled and lazy models are similar to the pessimistic model in that they result in fewer relevant documents than exist in the "true" judgments. However, they produce worse results in general. In the disgruntled case (Fig. 3(c), despite labeling roughly the same number of documents relevant as the pessimist, the  correlations are on average 10% lower. The lazy assessor (Fig. 3(d)) actually found many more relevant documents than either the pessimist or the disgruntled assessor with the same prior parameters, but apparently found "worse" relevant documents than the pessimist, as its  correlation is lower.
The fatigued and Markovian models are rather similar to each other in how they rerank systems. The Markovian model produces somewhat more pronounced effects on some of the systems, resulting in a lower  correlation. Both result in more documents being judged relevant.
One conclusion we draw from these results is that it is generally better to underestimate relevance than to overestimate it. The models that result in fewer documents being judged relevant--the pessimist, the disgruntled, and the lazy--generally produce more accurate rankings of systems than those that result in more documents being judged relevant. This suggests that low-cost evaluation methods are sensitive to noise in the relevant documents. Among models that result in fewer relevant documents, the pessimist produces the best rankings overall, though the system scores are strongly biased downward.
Of course, we do not conclude from this that assessors should be trained to be pessimists. This is an abstract model; the altered judgments had no relationship to any properties of the actual documents apart from their original relevance judgments.

543

0.28

true statMAP

0.26

simulated statMAP

0.24

0.22

0.2

statMAP

0.18

0.16

0.14

0.12

0.1

0.08

0.06 0

5

10

15

20

25

30

35

System number (ordered by statMAP)
(a) Optimistic assessor ( = 1,  = 16) judges many more documents relevant.  = 0.72

0.3 true statMAP
simulated statMAP
0.25

0.2

statMAP

0.15

0.1

0.05

0

0

5

10

15

20

25

30

35

System number (ordered by statMAP)

(b) Pessimistic assessor ( = 16,  = 1) judges many fewer documents relevant.  = 0.92

0.3 true statMAP
simulated statMAP
0.25

0.35 true statMAP
simulated statMAP
0.3

statMAP

statMAP

0.25 0.2
0.2
0.15 0.15

0.1

0.1

0.05 0

5

10

15

20

25

30

35

System number (ordered by statMAP)

(c) Disgruntled assessor ( = 1,  = 16) gives up early.  = 0.81

0.05 0

5

10

15

20

25

30

35

System number (ordered by statMAP)

(d) Lazy assessor ( = 1,  = 16) assumes first few judgments indicate the rest.  = 0.9

0.28

true statMAP

0.26

simulated statMAP

0.24

0.22

0.2

statMAP

0.18

0.16

0.14

0.12

0.1

0.08

0.06 0

5

10

15

20

25

30

35

System number (ordered by statMAP)
(e) Fatigued assessor ( = 0.05,  = 1) becomes more random over time.  = 0.9

0.28

true statMAP

0.26

simulated statMAP

0.24

0.22

0.2

statMAP

0.18

0.16

0.14

0.12

0.1

0.08

0.06 0

5

10

15

20

25

30

35

System number (ordered by statMAP)
(f) Markov assessor ( = 1,  = 16) makes each judgment based on the previous one.  = 0.84

Figure 3: Comparison between "true" statMAP system scores calculated over all Million Query 2009 topics+judgments and statMAP scores after each assessor model is applied to all topics with the specified parameters. The new rankings are evaluated by Kendall's  rank correlation (averaged over 25 trials when appropriate).

544

1000 800 600

 1.0 0.9 0.8 0.7

1000 800 600

 1.0 0.9 0.8 0.7

 

0.6

0.6

400

400

0.5

0.5

200

200

0.4

0.4

0.3

200

400

600

800

1000

0.3

200

400

600

800

1000





Figure 4: Contour maps illustrating the change in  with prior parameters ,  in the optimistic model (left) and pessimistic model (right). Lighter areas indicate higher values of  .

3.4 Worst Case Analysis
The results presented above represent relatively good parameter settings for each of the models. Depending on the prior parameters, the results can become quite bad. Figure 4 shows contour maps demonstrating the change in  with prior parameters  and  in the optimistic and pessimistic models. The optimist is best (indicated by lighter shading) when  is low and  is very high, which is when it is least likely to incorrectly judge a document relevant. Its performance quickly degrades from there. The pessimist is best when  is low and  is high, which is when it is least likely to incorrectly judge a document nonrelevant, but it maintains good performance until  is high and  is low. Note that the optimistic is much darker in much more of the space than the pessimistic, indicating substantially lower  correlations for any parameter settings.
Other models are similar to these two. Those that produce more relevant documents than originally existed in the relevance judgments tend to exhibit a faster drop-off in performance when parameters move away from the low-probability regions. Those that produce fewer relevant documents than originally existed tend to exhibit a slower drop-off.
4. ADJUSTING FOR ASSESSOR ERRORS
The effect of assessor errors is to add unplanned variance and bias into the evaluation. This increases the cost indirectly--though the judgments can be made for the same cost, the cost of the errors they introduce adds up. Thus it may be worth expending some extra cost to ensure that errors made by assessors cannot cause too much damage in the aggregate. Here we consider some simple approaches to adjust or correct their errors.
Since we are interested in cases where the assessors may be distributed around the world rather than present in person, and cases with many more assessors judging fewer topics each, we do not want to spend too much time on solutions that involve a great deal of interaction with the assessors.
4.1 Multiple Judgments
One possible solution is to have some documents judged multiple times. The cost clearly depends in part on how many rejudgments are made and how documents are chosen for rejudging, but it also depends on how the extra judgments are incorporated into the evaluation. Some of the

differences observed in the extra judgments will be due to reasonable disagreements about relevance rather than errors. While such differences could possibly be resolved by adjudication, this essentially adds another assessor--one who must be able to make a decision based on conflicting evidence--to the process, and that carries significant cost.
One alternative is to use a simple process like majority vote. If rejudgments converge on a particular decision, it is more likely that the original judgment was in error. This requires more duplicated effort, though, especially since rejudgments themselves are not immune to error.
Along similar lines, since pessimistic models seem to hurt performance less, we could require a supermajority of positives to call a rejudged document relevant. Thus it would take two of two judgments, or two of three judgments, being relevant before we are confident in concluding that a document really is relevant. This would only apply in the cases we actually decide to have a document rejudged; because of that and the additional cost in duplicated effort, the choice of documents to have rejudged must be made very carefully.
We hypothesize that for statAP evaluation, documents with lower inclusion probabilities are better candidates for rejudgment. These documents, if erroneously judged relevant, can have a much greater effect on the evaluation than documents with higher inclusion probabilities. The simulations bear this out: those models that resulted in worse ranking performance had lower inclusion probabilities on average among the judgments that changed. For example, the average inclusion probability among documents that the optimist in Figure 3(a) judged relevant was 0.09, while the average inclusion probability among documents the pessimist in Figure 3(b) judged relevant was 0.12.
To test the effect of rejudging low probability documents, we ran a second simulation to rejudge a few documents with low inclusion probabilities from a prior simulation. In this case, the simulated assessor uses the same model as the original, but only judges documents with inclusion probability less than 0.01. The new judgments are then merged with the existing judgments using the supermajority approach: any document that has been judged relevant twice is considered relevant, while the rest are nonrelevant. Since 90% of the judgments have inclusion probabilities greater than 0.01, most will not change, and most of the judged relevant documents will stay relevant.

545

rank correlation

1.05 1
0.95 0.9
0.85 0.8
0.75 0.7
0.65 0

100

200

300

400

500

600

number of errorful queries

Figure 5: Kendall's  decreases linearly (within the given error bars) as the number of errorful topics among in an evaluation increases. Average  does not fall below 0.9 until 232 of the original judged topics have been replaced with errorful versions.

The effect of this on the optimist is a small improvement in the  correlation from 0.72 to 0.75, which may not be worth the cost of the extra judgments. However it seems that this could potentially be a useful starting point for selecting documents for rejudging and incorporating the rejudgments into the evaluation.
4.2 Quality Assurance
Another approach to handling erroneous errors is to treat relevance judgments as a quality-assurance problem. Given an estimate of the permissible number of badly-judged topics, we can sample the topics that have been judged and check whether those seem to be errorful in order to estimate the total number of problem cases. If the number is above what is permissible, we can impose tighter controls for a brief time until judging seems to be going smoothly again.
We investigated the permissible number of bad topics by starting with the full evaluation over the original judgments and gradually replacing topics with their errorful doubles from the models above. The goal was to see how many "bad" topics we could inject into the evaluation before we reached a  correlation below 0.9, the threshold at which we might feel uncomfortable with the ranking.
The result is shown in Figure 5. The decrease in  is roughly linear in the number of errorful topics, but it does not drop below 0.9 on average until 232--over 40% of the total number of topics--have been replaced. This suggests that statAP is actually fairly robust to errors in judgments, at least in terms of its ability to rank systems. An evaluation could proceed for a fairly long time before tighter controls would need to be enforced.
5. CONCLUSION
We argue that as test collection construction continues to take lower-cost routes away from well-trained, managed assessors to crowdsourcing or cheaper, faster assessors, the errors in evaluation estimates will have to be quantified and potentially adjusted for the errors that will almost certainly occur in judging. We presented eight models of possible errors and showed how each affects an estimate of average precision. We proposed two possible means to adjust for errors: 1) have certain documents selected for rejudging, then

use a voting algorithm to combine the judgments; 2) estimate how many problem cases there seem to be to determine whether judging needs to be more strictly observed.
As a next step, we plan to undertake a true crowdsourcing experiment using Mechanical Turk to investigate the degree to which the behaviors we posit actually occur in that population and the effect resulting errors have on evaluation. Beyond that, future work must consider that these errors will seldom happen independently. Most evaluations will be affected by some mixture of errors, and the parameters of that mixture could have a substantial effect on both the evaluation and adjustments.
6. REFERENCES
[1] James Allan, Javed A. Aslam, Ben Carterette, Virgil Pavlu, and Evangelos Kanoulas. Overview of the TREC 2008 million query track. In Proceedings of TREC, 2008.
[2] Javed A. Aslam and Virgil Pavlu. A practical sampling strategy for efficient retrieval evaluat ion, technical report.
[3] Javed A. Aslam, Virgil Pavlu, and Emine Yilmaz. A statistical method for system evaluation using incomplete judgments. In Proceedings of SIGIR, pages 541­548, 2006.
[4] Peter Bailey, Nick Craswell, Ian Soboroff, Paul Thomas, Arjen P. de Vries, and Emine Yilmaz. Relevance assessment: Are judges exchangeable and does it matter? In Proceedings of SIGIR, pages 667­674, 2008.
[5] Ben Carterette. Robust evaluation of information retrieval systems. In Proceedings of SIGIR, 2007.
[6] Ben Carterette, James Allan, and Ramesh K. Sitaraman. Minimal test collections for retrieval evaluation. In Proceedings of SIGIR, pages 268­275, 2006.
[7] Ben Carterette, Virgil Pavlu, Evangelos Kanoulas, Javed A. Aslam, and James Allan. Evaluation over thousands of queries. In Proceedings of SIGIR, pages 651­658, 2008.
[8] Gordon V. Cormack, Christopher R. Palmer, and Charles L.A. Clarke. Efficient construction of large test collections. In Proceedings of SIGIR, pages 282­289, 1998.
[9] Donna Harman. Overview of the fourth Text REtrieval Conference. In Proceedings of the Fourth Text REtrieval Conference (TREC-4), pages 1­24, 1995. NIST Special Publication 500-236.
[10] Kenneth A. Kinney, Scott Huffman, and Juting Zhai. How evaluator domain expertise affects search result relevance judgments. In Proceedings of CIKM, pages 591­598, 2008.
[11] Mark Sanderson and Justin Zobel. Information retrieval system evaluation: Effort, sensitivity, and reliability. In Proceedings of SIGIR, pages 186­193, 2005.
[12] Ian Soboroff, Charles Nicholas, and Patrick Cahan. Ranking Retrieval Systems without Relevance Judgments. In Proceedings of SIGIR, pages 66­73, 2001.
[13] Ellen Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. In Proceedings of SIGIR, pages 315­323, 1998.
[14] Ellen M. Voorhees. The philosophy of information retrieval evaluation. In CLEF '01: Revised Papers from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems, pages 355­370, London, UK, 2002. Springer-Verlag.
[15] Ellen M. Voorhees and Donna K. Harman, editors. TREC: Experiment and Evaluation in Information Retrieval. MIT Press, 2005.
[16] Emine Yilmaz and Javed Aslam. Estimating average precision with incomplete and imperfect relevance judgments. In Proceedings of CIKM, pages 102­111, 2006.
[17] Justin Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In Proceedings of SIGIR, pages 307­314, 1998.

546

Reusable Test Collections Through Experimental Design

Ben Carterette, Evangelos Kanoulas, Virgil Pavlu, Hui Fang carteret@cis.udel.edu, e.kanoulas@sheffield.ac.uk, vip@ccs.neu.edu, hfang@ece.udel.edu
 Department of Computer & Information Sciences, University of Delaware, Newark, DE  Information Studies Department, University of Sheffield, Sheffield, UK
 College of Computer and Information Science, Northeastern University, Boston, MA Department of Computer & Electrical Engineering, University of Delaware, Newark, DE

ABSTRACT
Portable, reusable test collections are a vital part of research and development in information retrieval. Reusability is difficult to assess, however. The standard approach-- simulating judgment collection when groups of systems are held out, then evaluating those held-out systems--only works when there is a large set of relevance judgments to draw on during the simulation. As test collections adapt to larger and larger corpora, it becomes less and less likely that there will be sufficient judgments for such simulation experiments. Thus we propose a methodology for information retrieval experimentation that collects evidence for or against the reusability of a test collection while judgments are being made. Using this methodology along with the appropriate statistical analyses, researchers will be able to estimate the reusability of their test collections while building them and implement "course corrections" if the collection does not seem to be achieving desired levels of reusability. We show the robustness of our design to inherent sources of variance, and provide a description of an actual implementation of the framework for creating a large test collection.
Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval] Performance Evaluation
General Terms: Experimentation, Measurement
Keywords: information retrieval, test collections, reusability, evaluation
1. INTRODUCTION
Test collections are a vital part of research and development in information retrieval. They enable rapid development of new approaches to retrieval. They allow us to identify subtle distinctions between retrieval methods that could not be identified by users but that can add up to improved user experience over time. They support feature selection and parameter tuning by allowing us to efficiently test many possible combinations and values.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Unfortunately, test collections are expensive. They require judgments of the relevance of individual documents to topics in a sample. To properly control for variance, a test collection must have many topics and many judgments, and these require a great deal human effort. This expense makes reusability desirable: the cost of a test collection can be justified by the fact that it is amortized over many uses.
Constructing reusable test collections is difficult. The relevance judgments must be complete enough that future users of that collection can have confidence that their systems will be accurately evaluated. The majority of reusable test collections in the field exist as a result of the efforts of the organizers and participants of TREC (the Text REtrieval Conference), CLEF (the Cross Language Evaluation Forum), NTCIR (NII Test Collections for IR), and INEX (INitiative for the Evaluation of XML retrieval). These test collections arose by conducting an experiment to evaluate different approaches to a particular retrieval problem, and their reusability is a function of their large size and the diversity of approaches that were included in the experiment.
The standard experimental design for IR evaluation is a simple repeated-measures design, in which experimental units are topics/queries, treatments are systems, and each system provides ranked results for each query. This is the design that has been used for virtually every TREC, CLEF, NTCIR, and INEX track that has resulted in the release of a test collection. Measurements on experimental units are evaluation measures such as average precision (AP) calculated over relevance judgments; judging a pool of documents retrieved by the participating systems ensures that the measurements will be as accurate as possible.
Reusability emerges as a result of using a large and diverse set of retrieval systems and making sure they are judged to a substantial depth using pooling: there are simply so many judgments that it is unlikely any new system will ever be developed that does not retrieve many of the same documents that were judged as part of the original experiment. But as test collections grow larger and larger, pooling becomes more infeasible. Furthermore, recent work suggests that, for an experiment like that described above, it is actually more cost-effective to use many queries with very few judgments each. Thus TREC has begun adopting alternatives to pooling [15]: statistical sampling, which attempts to pick out the judgments that will result in a low-variance unbiased estimator [2], or algorithmic approaches that try to pick out the judgments that will reduce variance regardless of bias introduced [6]. But while these are more cost-effective for answering the original evaluation question, it is not at all

547

clear that they are cost-effective in the sense of producing test collections that can be reused many times. Because there are many fewer judgments, it is much more likely that a future system will retrieve many documents that were not judged, and therefore much more likely that we will not be able to accurately measure the performance of that system.
Our goal is to elevate reusability to a basic consideration along with evaluation. We do that by proposing an experimental design that collects evidence for or against reusability while judgments are being collected. The method that is used to select judgments does not matter, but the design is tied to the notion that more queries with fewer judgments is the correct way to build a test collection. It relies on having a large number of queries that can be partitioned into a combinatorial number of blocks.
In Section 2 we define what it means for a test collection to be reusable and discuss previous work on the topic. In Section 3, the main body of this work, we describe our design and the statistical analyses that it supports, and anticipate and answer some questions about its validity. In Section 5 we demonstrate the use of the design and analysis in the construction of an actual large test collection.
2. TEST COLLECTION REUSABILITY
A test collection consists of a corpus of documents, a set of topics that are representative of a particular task, and judgments of relevance of documents to topics. These judgments are generally taken from a set of retrieval systems performing the task. We define reusability as follows: A test collection is reusable if and only if we can use it for precise measurements of the performance of systems that did not contribute to its judgments. By "precise" we mean that the measurements fall within some given error intervals with high probability. By "systems that did not contribute judgments" we mean systems that are likely to be developed with current technology--it is always possible that new, unforeseen technology could produce retrieval systems that are both good and unlike anything seen before; since we will never be able to predict such cases, we do not want to tie reusability to them too much.
Test collections are used for many purposes beyond simple evaluation. Furthermore, evaluation comes in many different flavors. Below we discuss some aspects of reusability and previous work on this topic.
2.1 Applications of Test Collections
In addition to evaluation, test collections are used for training and optimization, including model selection, feature selection, and parameter tuning, for failure analysis, for data exploration, and many other purposes. While our focus is on evaluation, these other uses are important. Some can be seen as being related to evaluation: optimization uses an objective function based on an evaluation measure; the goal of failure analysis is to find reasons for an evaluation measure being different than expected.
2.2 Evaluating Reusability
The question of reusability has been studied primarily in the context of depth pooling. TREC and other fora form pools from the top documents retrieved by each submitted run for each topic; under the assumption that documents not highly ranked could be considered nonrelevant, test collections based on such pools are likely very reusable.

Harman [11] tested this by examining a pool formed by the documents in ranks 101-200 over the TREC-2 and TREC3 collections. Her study showed up to 21% more relevant documents could be found. Along the same line, Zobel [17] extrapolated the number of relevant documents found by depth to suggest that there could be up to twice as many relevant documents in the collection as there are in the pool. To examine the effect of the missing relevant documents on new systems that had not contributed any documents to the formation of the pool, he performed a leave-one-runout simulation. For each participating run, he removed all the documents it uniquely retrieved from the judgments and compared the evaluation over this reduced set of judgments to the evaluation with the full set. His study showed that the effect of the missing documents was minimal.
Voorhees adapted the leave-one-out methodology to leave out all of the runs contributed by a particular site at a time, under the assumption that runs submitted by the same participating site are similar enough that they retrieve very similar documents [14]. This leave-sites-out simulation has since become the standard approach to evaluating reusability.
Bu¨ttcher et al. [3] employed the leave-one-site-out over the TREC 2006 Terabyte collection and confirmed Zobel's conclusion. Further, the reusability of the collection by leaving out all manual runs was also tested. Given that manual runs are usually among the best performing ones, this did lead to somewhat different evaluation results.
Sakai [13] employed leave-one-site-out, take-just-one-site, and take-just-three-sites over TREC and NTCIR data. His goal was to identify the effects of missing judgments on a number of different evaluation metrics. He considered all pairs of runs over the full judgment set and found the number with statistically significant differences, then repeated the process with judgments obtained by one of the three aforementioned methods and counted the errors. The results demonstrate that while the rankings of systems over the full and reduced set of judgments are similar, missing relevant documents leads to many errors of commission, i.e. finding differences significant even though they are not.
Carterette et al. proposed that reusability should be evaluated in terms of the ability of the test collection to produce high confidence in evaluation results, specifically pairwise comparisons between systems [4] or width of confidence interval on an evaluation measure [7]. The former work used judgments from two systems to evaluate a larger set of 10 systems; the latter employed the leave-sites-out methodology discussed above to predict confidence interval width when evaluating new systems.
The simulation approaches above depend on having a fairly large number of judgments in the first place: any document that is selected for judging in the simulation phase must already have an actual judgment made by a human assessor. Without a fairly complete set of judgments it is likely that documents selected for judging will not actually have judgments; it is not possible to apply simulation to evaluate the reusability of TREC Million Query collections, for instance, because holding systems out would result in different documents being selected for judging than were originally judged for the track.
2.3 Types of Reusability
Based on the work above, we identify three types of analysis that test collections are used for in evaluation:

548

1. "within-site" analysis, in which a research/development site is conducting an experiment to determine which of several possible (internally-developed) systems to publish or deploy. We believe this is the most common use of test collections.
2. "between-site" analysis, in which one research/development site compares their results to those of another site, possibly relying on published results.
3. "participant comparison" analysis, in which a research/development site compares their results to those of the systems that are on record as participating in a particular track or task.
"Site" is TREC terminology, but it can be defined loosely; within a particular setting, any group of systems that are similar in some sense could be considered a "site". We use the term in that general sense throughout this work.
Our goal is to develop a methodology that can be used to test all three types of reusability when simulation is impossible due to the process used to select documents to judge.
3. EXPERIMENTAL DESIGN
As discussed above, the standard design used in systembased IR evaluations is the repeated-measures design. This is appropriate for drawing conclusions about differences between systems, but it does not tell us anything about reusability. Furthermore, as discussed in Section 2.2, post-hoc evaluations of reusability are impossible when refinements to the implementation of the repeated-measures design such as statistical sampling or algorithmic selection were used.
In our design, each system is held out from actual judgment collection for some queries. After the judging is complete, "new" systems are constructed by putting together all the queries from which a system was held out and evaluating it with the judgments contributed by the non-held-out systems for those same queries. Note that this means the reusability experiment can be performed only once.
Our design is meant to serve two ends: to draw conclusions about differences between systems, and to draw conclusions about the future reusability of the test collection that will result. It is meant to be "fair" in the sense that each system contributes judgments to the same number of queries. Since it can introduce bias or variance depending on which systems are held out from which queries, it attempts to minimize/control that as much as possible by ensuring that no two systems are held out of the same queries consistently. The complete description follows.
3.1 Description of Design
We partition N topics into b + 1 sets T0, T1, . . . , Tb. The first set, T0, consists of n topics to which all systems contribute judgments. This is the standard repeated-measures design to ensure that we can answer questions about differences between these systems. It provides a baseline for answering questions about reusability.
In each subsequent set, a subset of systems are held out during judgment collection for each topic. The held-out set is different for each topic. Choosing which systems to held out can be done by site (if multiple sites have contributed systems): if there are m sites, k are held out from each query in the set; which k to hold out can be determined using round robin. The total number of queries must be a

subset topic S1 S2 S3 S4 S5 S6

T0

t1 + + + + + +

all-site · · ·

baseline tn + + + + + +

T1

tn+1 + + + + ­ ­

tn+2 + + + ­ + ­

tn+3 + + ­ + + ­

tn+4 + ­ + + + ­

tn+5 ­ + + + + ­

tn+6 + + + ­ ­ +

tn+7 + + ­ + ­ +

tn+8 + ­ + + ­ +

tn+9 ­ + + + ­ +

tn+10 + + ­ ­ + +

tn+11 + ­ + ­ + +

tn+12 ­ + + ­ + +

tn+13 + ­ ­ + + +

tn+14 ­ + ­ + + +

tn+15 ­ ­ + + + +

T2

tn+16 + + + + ­ ­

··· ···

tn+30 ­ ­ + + + +

T3

···

Table 1: Illustration of proposed experimental design at the site level with m = 6 sites and k = 2 held out from each topic. Each column shows which topics a site contributed to. A + indicates that all of the sites' runs contributed judgments to the topic; ­ indicates that the sites' runs did not contribute judgments. Each subset T1 . . . Tb has the same contribution pattern as subset T1.

multiple of

m k

to ensure that each site is held out of the

same number of queries.

This design is essentially a standard randomized, repeated-

measures block design in which blocks are defined by which

sites have been held out; there are

m k

blocks and b ob-

servations in each block. Statistical tools such as mixed-

effects ANOVA can be applied directly to answer questions

about differences between individual systems. Answering

questions about reusability will require some additional tools

that we describe in the next section.

The design is illustrated in Table 1 to give a sense of how

it provides data for each of our three types of reusability:

1. "within-site": Within each subset Ti, each site con-

tributes to

m-1 k

topics and is held out from

m-1 k-1

topics. Thus in addition to the n topics that all sites

contribute

to,

each

site

contributes

to

b

m-1 k

topics

that

can

be

used

as

a

site

baseline,

and

to

b

m-1 k-1

top-

ics that can be used for testing reusability by compar-

ing results on those topics to results on the site base-

line topics. In Table 1, for instance, the within-site

reusability set for site S6 includes the first five topics

in each subset, e.g. topics numbered n+1 through n+5

in subset T1. The within-site baseline includes the first

n all-site baseline topics along with the last 10 in each

subset, e.g. those numbered n + 6 through n + 15 in

subset T1.

2. "between-site": Within each subset Ti, each pair of

sites contributes to the same

m-2 k

topics and is held

out of the same

m-2 k-2

topics.

The

n

+b

m-2 k

total

549

topics those two sites contribute to form a baseline for

comparisons between those sites.

The

b

m-2 k-2

topics

they were both held out from can be used to deter-

mine the between-site reusability. In Table 1, the first

topic in each subset can be used for testing reusability

between sites S5 and S6 against the last six that both contributed to, along with the first n in the baseline.

3. "participant comparison": Within each subset Ti, there

are

m-2 k-1

topics that one site contributes to and an-

other site does not. These topics can be used to eval-

uate comparing the non-contributing site to the con-

tributing site. In Table 1, if S5 is the "participant baseline" and S6 is the "new system", topics numbered n + 2 through n + 5 are part of the set used to test

reusability.

The values b, n, k are parameters that need to be set by the researchers. Suppose we have an idea of how many total topics (N ) will be judged and how many total judgments there will be. This may be based on budget constraints, power analysis, previous work, or most likely a combination of all three. We can express the total number of queries as:

m

N =b

+ n.

k

Let us further suppose that we want to guarantee that at least n0 topics are part of the baseline set that all systems contribute to. Then:

N b m k

+ n0



b



N

- n0
m

k

For a given m and k, we can set

b=

N - n0
m

k

and

n=N -b m

k

Determining k is then a matter of creating a table of values and determining which produces the best distribution of topics among the three types of reusability for answering the questions important to the researchers. Note that larger k provides more topics for between-site experiments, but requires more total topics; smaller k provides more topics for within-site experiments. All design parameters and their relationships to each other are summarized in Table 2.

3.2 Statistical Methods for Analysis
We need to be able to determine whether the evaluation results over "new systems" (restricted to the held-out topics) match the evaluation results over the same systems when they contribute to the judgments. If we were using this design for the TREC Robust track in 2004, for example, we might like to know whether the runs submitted by Johns Hopkins' Applied Physics Lab (APL) are ranked the same when evaluated over 210 topics they contributed judgments to as when evaluated over 39 topics they did not contribute to. This is a statistical question: even if the collection is perfectly reusable, we are evaluating systems over two different sets of topics with two different sample sizes, and we therefore must expect that some evaluation results will change due to chance alone. This must therefore have a statistical answer, i.e. a p-value that will allow us to reject reusability if the evidence is against it.
More specifically, there are three questions of interest:

number of sites

m

total number of topics

N

min. size of baseline set

n0

number of held-out sites

k

number of topic subsets

b

size of all-site baseline set n

size of within-site baseline

size of between-site baseline

size of within-site reuse set

size of between-site reuse set

size of participant-comparison set

fixed by researchers

fixed by budget

fixed by researchers

variable

b= n=

(N - n0)/

N

-b

m k

m k

n+b

m-1 k

n+b

m-2 k

b

m-1 k-1

b

m-2 k-2

b

m-2 k-1

Table 2: A summary of parameters of the experimental design and how they relate to each other. Some parameters can be treated as fixed values. At least one is a variable that must be chosen in consideration of certain tradeoffs. The rest are functions of those.

1. Are systems that are significantly different over topics they contributed to also significantly different over topics they did not contribute to? (Likewise with nonsignificant differences.)
2. Is the relative ordering of systems over topics they contributed to the same as the relative ordering over topics they did not contribute to?
3. Do the system scores averaged over the topics it contributed to match the scores averaged over the topics it did not contribute to?

The first--agreement in statistical significance--is the most important but also the most difficult to discern, so we focus on that. If the first fails, the second--relative orderings being the same--still provides some reusability. The third is a sufficient but not necessary condition for the second; we care about the measures being the same to the extent that they have some extrinsic meaning that we want to keep.

3.2.1 Agreement in statistical significance

Testing for agreement in statistical significance is somewhat complicated. The first step is simple: use some significance test to determine whether pairs of systems are significantly different. We recommend a t-test, possibly adjusting the p-values to account for the family-wise error rate growing with the number of pairwise comparisons (the so-called "multiple comparisons problem" [12]). After performing two sets of pairwise tests (one set for all pairs of systems over the baseline topics, one for the same pairs over the reusability topics), we can form a contingency table showing the agreement in significance between the two sets of tests. The five runs submitted by APL to the TREC 2004 Robust track provide an example:

reuse tests p < 0.05 p  0.05

baseline tests

p < 0.05 p  0.05

6

0

3

1

Among the 10 pairwise comparisons, six resulted in a significant difference being found over both the baseline and reusability topics. Three had a significant difference over the baseline topics but not over the reusability topics. One had no significant difference in either set.

550

So we can see that there are three errors of omission and

none of commission. The question is whether these errors

are outside the realm of what is expected. Note that we must

expect some errors just because of the difference in topic set

sizes between the two experiments. Thus the next step is to

construct a contingency table of expected agreement between

the two sets of tests, then compare our observed values to

the expected in a statistically sound way.

We will use power analysis to construct the expected con-

tingency table. Power analysis is a very deep topic, and we

unfortunately do not have space to go into details. For more

information we suggest Cohen's book [10] or two recent pa-

pers in the IR literature [9, 16]. The high-level view is that

the power of a test is equivalent to the probability that the

p-value would be deemed significant for any sample of the

same size. Power is a function of the effect size, the sample

size, and the significance level. Effect size is a measure of the

degree of difference between two systems over the hypotheti-

cal population of topics; for the t-test effect size is estimated

as the mean difference in average precisions divided by the

standard deviation of the differences. Power monotonically

increases with both effect size and sample size.

We can estimate the power of a given pairwise test by

estimating the effect size and plugging that along with sam-

ple size and significance level (usually 0.05) into a power

function (available for most widely-used statistical software

packages). This power estimate can then be treated as the

expectation that the test would be found significant at the

0.05 level. The power of the comparison over the reusability

topics uses the same process, only with the smaller sample

size instead of the baseline topic sample size.

For example, the mean difference in average precision be-

tween APL runs rsTs and rsDw is 0.046 over the 210 baseline

topics, and the standard deviation is 0.176. The effect size

is 0.046/0.176 = 0.260, which would be considered a mod-

erate effect. The power of a test comparing those two runs

over 210 topics is 0.964, i.e. there is a 96% chance that a

significant difference between them would be found for any

set of 210 topics. If the sample size is reduced to 39 (the

size of the reusability set), the power drops to 0.354.

Now the expectation that both tests come out significant

is simply the product of their estimated powers. For the two

runs above, that is 0.964 · 0.354 = 0.341; we add 0.341 to the

number of expected positive agreements. The probability

that the first comes out significant but the second does not

is .964 · (1 - 0.354) = 0.623, so we add that to the number

of expected errors of omission. We add (1 - 0.964) · 0.354 =

0.013 to the number of expected errors of commission, and

(1 - 0.964) · (1 - 0.354) = 0.023 to the number of expected

negative agreements. Continuing in the same way for all 10

pairs of APL's runs produces the table of expected values:

baseline expectation

reuse expect. p < 0.05 p  0.05

p < 0.05

7.098

0.073

p  0.05

2.043

0.786

By inspection this table is not very different from the observed values. The final step is to verify that statistically. We do that using a 2 goodness-of-fit test for whether the observed values match the expected1. In this case they do: the p-value of the 2 test is 0.88, meaning we cannot
1In this case, because the number of observations is small, we actually use a randomized "exact" version of the 2 test.

conclude that the tables are different, and therefore cannot conclude that the collection is not reusable for this site--we tentatively would say that other sites that are creating runs "like" APL's can trust in the reusability of this collection.
To test between-site reusability, we use the same process, but only test significance between pairs of runs from different sites. For example, if the two sites are APL and IBM, we would only look at significant differences between each APL run and each IBM run (over the intersection of topics they contributed to or were held out from), but not between two APL runs or two IBM runs. Apart from that consideration, the analysis proceeds in exactly the same way. Likewise, participant-comparison uses the same process but uses the topics that one site contributed to and the other did not.
3.2.2 Relative ordering of systems
There are many well-known rank correlation statistics that can be used to determine whether the systems are ordered the same between the two sets of topics. Kendall's  is the most frequently used; it is calculated by subtracting the number of pairs of systems that have been swapped between two rankings from the number in the same order. Like our significance test procedure above, it calculates counts over pairs of systems; to adapt it to between-site and participantcomparison reusability, we can count pairs that are different between sites while ignoring those from the same site.  does not have a notion of the expected number of errors that is meaningful for reusability.
Carterette introduced an alternative measure of rank similarity called drank that takes into account similarity of systems amongst themselves [5]. If the systems are more similar, some reordering is expected, and the measure is smaller. drank can provide a p-value for the reusability ranking being similar to the baseline ranking.
3.2.3 Agreement in system scores
To determine whether the system scores agree, we can calculate a point estimator such as root mean square error: RM SE = 1/n ni=1(M APi - M APi )2, where M APi is the baseline M AP and M APi is the reusability MAP. The larger this is, the more error is present between the two sets of scores. However, RMSE does not have a known distribution that can be used to determine a p-value, so its interpretation is somewhat subjective.
4. VALIDATION
We present three validation experiments. The first simply demonstrates that it is indeed possible to use our analysis in Section 3.2.1 to disprove reusability. The next two show conversely that if a collection is reusable the p-value is not likely to be low.
4.1 Disproving reusability
A very simple way to validate that reusability will be rejected when it is not true is to simulate evaluation over a non-reusable collection. For example, we can use random number generation to simulate evaluation measures for m systems and show that the 2 p-value will be low when the simulation is explicitly set up so that the evaluation measures differ between the baseline and reusability sets. We drew measures from beta distributions (ensuring they would be between 0 and 1) such that the measures drawn for reusability topics for one run would be lower than those

551

reuse p < 0.05
p  0.05

baseline p < 0.05 O = 196 E = 189.5 O = 57 E = 62.1

p  0.05 O=2 E = 4.3 O = 45
E = 44.1

Table 3: Observed versus expected agreement in significance results for within-site reusability aggregated over all Million Query 2008 sites. The 2 p-value is 0.58, indicating no evidence to reject reusability.

drawn for the baseline topics for the same run and as a result the significance tests involving those runs would not agree. The result is that as the number of reusability topics increases, the p-values decrease, with 50 reusability topics in this scenario producing a p-value less than 0.01.
4.2 Robustness to differences in topic samples
By robustness to differences in topic samples, we mean that conclusions about reusability are not expected to be confounded by the fact that the tests are based on different size samples of different topics (as in Section 3.2.1 above). To show this, we set up an idealized scenario in which the test collection must be reusable and show that our analysis will not reject reusability.
Our data is the 2008 TREC Million Query (MQ) track data consisting of 564 topics with 15,000 total judgments collected from 25 systems submitted by 9 different sites [1]. Every run contributed judgments to every topic; none were held out. We chose n0 = 200 topics to be the baseline that all systems "contribute" to. For each of the remaining 364 topics, we "held out" k = 2 sites. Plugging into the formula above results in b = 10 topic sets.
The AP value for each system/topic is simply that calculated for the track. This makes a 100% reusable collection: the AP estimates on the "held-out" topics are exactly the same as they were when the systems actually contributed judgments. This is (intentionally) highly artificial, but note that it is not meant to be a simulation of judgment collection or of evaluation. It is a boundary case to demonstrate that our conclusions will not be confounded by variance in the topic samples when reusability is true. There are other sources of bias and variance that this test does not address.
Rather than apply the procedure described in Section 3.2.1 to each site individually (running the risk of multiple comparisons problem), we aggregated the contingency tables and expected contingency tables across sites to obtain two tables representing all within-site comparisons. They are shown together in Table 3. The 2 p-value is 0.58, indicating no evidence to suggest the difference in topic samples is causing a problem. We did the same for between-site reusability and participant-comparison reusability; the 2 p-values are 0.54 and 0.36, respectively.
4.3 Robustness to held-out systems
Another possibility is that holding certain systems out will inject bias into topic evaluations. For example, if a very good system that retrieves many relevant documents is held out, evaluation results for the other systems may not be as accurate, even when reusability holds in other cases. To test this we use simulation in the Robust 2004 data described

reuse p < 0.05
p  0.05

baseline p < 0.05 O = 130 E = 135.4 O = 127 E = 121.6

p  0.05 O = 17 E = 13.9 O = 160 E = 163.1

Table 4: Observed versus expected agreement in
significance results for within-site reusability aggregated over all Robust 2004 sites. The 2 p-value is
0.74, indicating no evidence to reject reusability.

within site bseline queries (average 211 quereis) site reusability queries (average 38 queries)

site baseline depth=100 0.45
JUR

0.4

NLP

SAB

0.35

APL

FUB

0.3

HUM

ICL

0.25

MPI

PIR

0.2

POL

UIC

0.15

UOG

VTU

0.1

WDO

0.05

0 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45
allsites baseline (all 249 queries)

site reusability depth=100 0.45
JUR

0.4

NLP

SAB

0.35

APL

FUB

0.3

HUM

ICL

0.25

MPI

PIR

0.2

POL

UIC

0.15

UOG

VTU

0.1

WDO

0.05

0 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45
all sites baseline (all 249 queries)

Figure 1: Robust 2004 simulation with 2 sites held out per topic. Judgments were based on a pool of depth 100. The left plot compares MAP over the 210 site baseline topics to true MAP calculated with all judgments. The right compares MAP over the 39 site reusability topics to true MAP.

above. Note that Robust 2004, despite being large by TREC standards, is fairly small for this design; because there are only 249 topics, we have no all-site baseline set, and we cannot hold more than k = 2 sites out. The number of topics we have for each of the three types of tests is limited (39 for within-site, only 3 for between-site). The advantage is that Robust 2004 has very many relevance judgments, so we can simulate pools of any depth.
Once again, this is validation that the design works when reusability is true. To ensure reusability to the greatest degree possible, we simulated a depth-100 pool. That is, for each topic, runs submitted by two sites were held out of simulated judging; the other 12 sites had their top 100 ranked documents judged according to the existing judgments in the TREC qrels file. We then evaluated all runs using that pool and separated them into systems that contributed and systems that were held out.
We only have enough topics for within-site analysis. The observed and expected significance results are shown in Table 4; the p-value is 0.74, indicating no evidence to reject reusability. We performed the same test on shallower pools; for pools of depth 10, 20, and 50, the p-values are 0.63,0.58, and 0.60, respectively. Figure 1 shows the comparison of evaluation results on different topic sets in the depth-100 pool. Note that the fact that we have only 39 topics for reusability testing is somewhat limiting, however.
5. IN SITU REUSABILITY EXPERIMENT
The analysis above provides evidence that our design is correct. We next observe it in a real experimental setting: judgment collection for the 2009 TREC Million Query (MQ) track [8]. Eight participating sites submitted a total of 35

552

runs over 1,000 queries. The corpus was the Category B subset of the new ClueWeb09 web collection. 638 of the 1,000 queries were converted to full topics and judged; of those, 146 formed the all-site baseline to which every run contributed judgments. The remaining 492 topics had two sites held out during judging. Held-out sites were selected by round-robin scheduling. Assessors did not know whether they were judging a reusability topic or not, and topic order was randomized, so there is no reason to suppose that the reusability topic sample is biased compared to the baseline sample. Assessors made a total of 34,534 judgments (54 per topic on average), of which 26% were either relevant or highly relevant. There were 95 topics for which no relevant documents were found.
The Million Query track uses two official evaluation measures, statMAP and MTC's "expected" MAP. Both are estimates of average precision, but they are designed for different purposes. statMAP is an unbiased estimator of average precision. MTC EMAP is a biased estimator meant to provide good comparative evaluation.
Our goal in this section is to determine the extent to which this test collection is reusable.
5.1 Results
Reusability results for MQ are illustrated in Figure 2, which shows statMAP (top) and MTC EMAP (bottom) scores of runs over (a) 145 baseline against 170 site baseline topics (left), (b) 170 site baseline against 160 site reuse topics (center), and (c) 145 baseline against 160 site reuse topics (right). Each run was evaluated over all the topics it contributed to and all the topics it was held out from, but since different sites contributed to different topics, no two sites were evaluated over exactly the same set of topics.
As mentioned in Section 4, differences in mean scores over baseline topics and mean scores over reusability topics for a given site may be due to a number of different effects: (1) the baseline and reuse topics are two different topic sets of different size; (2) apart from the site under study there are two other sites that did not contribute documents to each reusability topic; (3) the site under study itself did not contribute documents to the reuse topics (this is the actual effect we would like to quantify); and finally, (4) for this particular study the fact that both methods evaluate runs with a very small number of documents introduces some variability even in the baseline topics.
The plots in Figure 2 attempt to separate the second and third effects. Essentially, the comparison of the mean scores between the 145 baseline topics and the 160 site reuse topics (right) summarizes the results of the reusability experiment, and it is what an actual new site would observe by using the MQ 2009 collection. StatMAP scores over the reuse topics are positively correlated with the statMAP scores over the baseline topics, though the correlation is rather weak. MTC EMAP scores over these two sets of topics are well correlated. One can consider the other two plots as the decomposition of the effects seen in the right plot. The left plot illustrates the effect of holding out sites other than the site under study. For the statMAP case this has a rather strong effect on the scores computed, though it is minimal for the MTC scores. The middle plots try to isolate the effect of holding out the site under study. As can be seen, this also has a strong effect on the statMAP scores, while the effect is mild in the case of the MTC scores.

reuse p < 0.05
p  0.05

baseline p < 0.05 O = 257 E = 302.5 O = 133 E = 85.1

p  0.05 O = 41 E = 26.2 O = 100 E = 117.2

Table 5: Observed versus expected agreement in significance results for between-site reusability aggregated over all Million Query 2009 sites. The 2 p-value is 0, indicating sufficient evidence to reject reusability.

The plots give a visual sense of reusability, suggesting within-site may be acceptable at the level of rank agreement if not score agreement, but between-site is likely not acceptable. To quantify this, we computed three correlation statistics as described in Section 3.2.2. First we computed the overall Kendall's tau between the ranking induced by the scores in the two topic sets. This is a rough estimate of the between-site reusability. For statMAP scores this is 0.7643, while for MTC EMAP scores this is 0.8350, both of which are rather low. Next we computed the Kendall's  among the runs of each individual site to estimate withinsite reusability; Table 6 shows these. Note that the values are not comparable across sites since the number of runs compared affects the Kendall's  values. Finally, we computed a  -like correlation to quantify the ability to compare "new" runs to contributing participants. For each site, we count the number of its reusability runs that are correctly ordered against the baseline runs and the number that have been swapped with a baseline run. Every comparison involves exactly one run for that site; for this measure we do not compare two runs from the same site or two runs from a different site. The final value is determined identically to Kendall's  ; the set of values can be seen in Table 6.
The significance test agreement procedure, when applied to this data, suggests that there is not enough evidence to reject within-site reusability (p > 0.5), but there is more than enough to reject between-site reusability (p < 0.01). To explain how within-site reusability holds despite some of the low  correlations in Table 6, we note that  is not able to capture anything about whether swaps are "reasonable". The lowest  is -0.6 for UIUC, but by inspection (Fig. 2) UIUC's systems are all very close to each other. It is perfectly reasonable that they would be ordered differently over another set of topics, and thus the low  is not a concern. For between-site reusability, however, we have seen that it is unlikely; that the 2 test confirms this is a point in its favor. The full contingency table for between-site reusability is shown in Table 5.
6. CONCLUSIONS
We have proposed an experimental design that can be used during construction of large test collections to collect evidence for or against the future reusability of the collection. It is appropriate for when the set of judgments is too small to be able to evaluate reusability through simulation; since test collections are moving in this direction, some framework will be necessary for determining whether these collections can be reused. We presented tools for statistical analysis and demonstrated their use in artificial data, a

553

statAP over site reusability queries (average 160)

statAP over site reusability queries (average 160)

statAP over within site baseline queries (average 170)

0.25

0.2

0.15 0.1 0.08

iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr
0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.26 0.28 statAP over all sites baseline queries (all 145 queries)

0.25

0.2

0.15 0.1 0.08

iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr
0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.26 0.28 statAP over within site baseline queries (average 170)

0.25

0.2

0.15 0.1 0.08

iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr
0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.26 0.28 statAP over all sites baseline queries (all 145)

MTC over site reusability queries (average 160)

MTC over site reusability queries (average 160)

MTC over within site baseline queries (average 170)

0.12

0.1

0.08 0.06 0.04
0.03

iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr
0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11 0.12 0.13 MTC over all sites baseline queries (all 145 queries)

0.12

0.1

0.08 0.06 0.04
0.03

iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr
0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11 0.12 0.13 MTC over within site baseline queries (average 170)

0.12

0.1

0.08 0.06 0.04
0.03

iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr
0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11 0.12 0.13 MTC over all sites baseline queries (all 145 queries)

Figure 2: StatMAP and MTC EMAP scores of systems over (a) 145 baseline against 170 site baseline topics, (b) 170 site baseline against 160 site reuse topics, and (c) 145 baseline against 160 site reuse topics.

within-site  participant comparison

statAP MTC statAP MTC

iiith 0.333 0.333 0.750 0.938

IRRA 1.000 0.800 0.547 1.000

NEU 0.200 1.000 1.000 1.000

Sabir 0.333 1.000 0.987 0.840

UDel 0.800 0.600 0.573 0.933

ECEUdel 0.800 0.800 0.773 0.707

UIUC -0.600 0.800 0.773 0.947

uogTr 1.000 1.000 0.939 0.909

Table 6: Rank correlations based on Kendall's  for site baseline to site reusability (top) and for comparison of site reusability to the "original" TREC runs excluding those treated as new (bottom).

simulation experiment, and a real-life implementation of the design; in general their results confirm our intuitions about the evaluation.
Clearly there is much more and much deeper analysis we could do. For this work we chose to present some of the topics we felt were most important in presenting this methodology, but we certainly intend to continue investigating other tools for analysis, more sophisticated statistical methods, and of course IR-centric implications for the failure (or lack of failure) of reusability when it happens.
Acknowledgements
The authors gratefully acknowledge support by the European Commission who funded parts of this research within the Accurat project (FP7-ICT-248347) and by the Marie Curie IIF (FP7-PEOPLE-2009-IIF-254562).
7. REFERENCES
[1] J. Allan, J. A. Aslam, B. Carterette, V. Pavlu, and E. Kanoulas. Overview of the TREC 2008 million query track. In Proceedings of TREC, 2008.
[2] J. A. Aslam and V. Pavlu. A practical sampling strategy for efficient retrieval evaluat ion, technical report.
[3] S. Bu¨ttcher, C. Clarke, P. Yeung, and I. Soboroff. Reliable information retrieval evaluation with incomplete and biased judgements. In Proceedings of SIGIR, pages 63­70, 2007.
[4] B. Carterette. Robust test collections for retrieval evaluation. In Proceedings of SIGIR, pages 55­62, 2007.
[5] B. Carterette. On rank correlation and the distance between rankings. In Proceedings of SIGIR, 2009.
[6] B. Carterette, J. Allan, and R. K. Sitaraman. Minimal test collections for retrieval evaluation. In Proceedings of SIGIR, pages 268­275, 2006.

[7] B. Carterette, E. Gabrilovitch, V. Josifovsky, and D. Metzler. Measuring the reusability of test collections. In Proceedings of WSDM, 2009.
[8] B. Carterette, V. Pavlu, H. Fang, and E. Kanoulas. Overview of the TREC 2009 million query track. In Notebook Proceedings of TREC, 2009.
[9] B. Carterette and M. D. Smucker. Hypothesis testing with incomplete relevance judgments. In Proceedings of CIKM, pages 643­652, 2007.
[10] J. Cohen. Statistical Power Analysis for the Behavioral Sciences. Lawrence Erlbaum, 2nd edition, 1998.
[11] D. Harman. Overview of the second text retrieval conference (trec-2). Inf. Process. Manage., 31(3):271­289, 1995.
[12] J. P. A. Ionnidis. Why most published research findings are false. PLoS Med., 2(8), 2005.
[13] T. Sakai. Comparing metrics across trec and ntcir: the robustness to system bias. In Proceedings of CIKM, pages 581­590, 2008.
[14] E. M. Voorhees. The philosophy of information retrieval evaluation. In CLEF '01: Revised Papers from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems, pages 355­370, London, UK, 2002. Springer-Verlag.
[15] E. M. Voorhees. Overview of trec 2009. In Proceedings of TREC, 2009. Notebook draft.
[16] W. Webber, A. Moffat, and J. Zobel. Statistical power in retrieval experimentation. In Proceedings of CIKM, pages 571­580, 2008.
[17] J. Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In Proceedings of SIGIR, pages 307­314, 1998.

554

Do User Preferences and Evaluation Measures Line Up?
Mark Sanderson, Monica Lestari Paramita, Paul Clough, Evangelos Kanoulas
Department of Information Studies, University of Sheffield Regent Court, 211 Portobello St, Sheffield, S1 4DP, UK
+44 114 22 22648
m.sanderson@shef.ac.uk

ABSTRACT
This paper presents results comparing user preference for search engine rankings with measures of effectiveness computed from a test collection. It establishes that preferences and evaluation measures correlate: systems measured as better on a test collection are preferred by users. This correlation is established for both "conventional web retrieval" and for retrieval that emphasizes diverse results. The nDCG measure is found to correlate best with user preferences compared to a selection of other well known measures. Unlike previous studies in this area, this examination involved a large population of users, gathered through crowd sourcing, exposed to a wide range of retrieval systems, test collections and search tasks. Reasons for user preferences were also gathered and analyzed. The work revealed a number of new results, but also showed that there is much scope for future work refining effectiveness measures to better capture user preferences.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]
General Terms
Measurement, Experimentation.
Keywords
Mechanical Turk, User Experiment, Evaluation Measures
1. INTRODUCTION
There is a long tradition of encouraging conducting, and researching evaluation of search systems in the IR community. A test collection and an evaluation measure are together used as a tool to make a prediction about the behavior of users on the IR systems being measured. If measurement using the collection reveals that system A is more effective than system B, it is assumed that users will prefer A over B in an operational setting. One of the striking aspects of almost all the early work in test collections is that the predictions about users implied from such measurements were rarely, if ever, validated. Given that test collections are used to simulate users, that so little validation took place is perhaps surprising.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.

In the last ten years a series of papers employing a range of methods conducted such validation. The papers produced contradictory results, some failing to find any link between test collection measures and user preferences, performance, or satisfaction; others finding links, but only when differences between IR systems were large.
Much of the past work involved a small number of topics, systems, and users; and/or introduced some form of artificial manipulation of search results as part of their experimental method. There was also a strong focus on test collections and not on the relative merits of different evaluation measures.
Therefore, it was decided to examine, on a larger scale, if test collections and their associated evaluation measures do in fact predict user preferences across multiple IR systems, examining different measures and topic types. The study involved 296 users, working with 30 topics, comparing user preferences across 19 runs submitted to a recent TREC evaluation. The research questions of the study were as follows
1. Does effectiveness measured on a test collection predict user preferences for one IR system over another?
2. If such a predictive power exists, does the strength of prediction vary across different search tasks and topic types?
3. If present, does the predictive power vary when different effectiveness measures are employed?
4. When choosing one system over another, what are the reasons given by users for their choice?
The rest of this paper starts with a literature review, followed by a description of the data sets and methods used in the study. Next, the results of experiments are described, the methods are reflected upon, conclusions are drawn, and future work is detailed.
2. PAST LITERATURE
The past work described here is grouped into two sections, based on the methods used to measure users. Contradictions between the results of the two groups are then discussed.
2.1 Measures rarely predict users
The power to predict user preferences using a test collection and evaluation measure was first examined in the work of Hersh et al [15] who used the 14 topics and qrels of TREC 6 and 7's interactive track to determine which of two retrieval systems was significantly better. They then conducted an experiment involving 24 searchers, retrieving over six topics of TREC-8: three topics on one system, three on the other. The researchers reported that there was no significant difference in the effectiveness of the searchers when using the different systems. This work was repeated on another test collection [25] drawing the same conclusion.
Allan et al [4] created artificial document rankings from TREC data each with controlled levels of effectiveness. Users were

555

shown selections of the generated rankings and asked to identify relevant information. Unlike the work described above, a correlation between user behavior and test collection based evaluation measures was found, but mainly when measured differences were large. Turpin & Scholer [26] repeated the artificial document ranking method, getting thirty users to examine fifty topics. No significant difference in the time users took to find the first relevant document was found. A small significant difference in the number of relevant documents identified was observed for large differences in the MAP of the artificial ranks.
Inspired by Hersh and Turpin' s method Al-Maskari et al [3] measured how well groups of users performed on two IR systems. Fifty six users searched from a selection of 56 topics. The researchers showed that test collection based measures were able to predict user behavior, and to some extent a user's level of satisfaction, however only when measured differences between the systems were large.
Although test collection based work is relatively recent, there is a longer tradition of correlating user outcomes with effectiveness measures calculated on actual searching systems. Tagliacozzo [22] showed that 18% of ~900 surveyed MEDLINE users appeared unsatisfied with search results despite retrieving a large number of relevant documents. As part of a larger study, Su [21] examined correlations between precision and user satisfaction; finding no significant link. Hersh et al [14] examined medical students' ability to answer clinical questions after using a medical literature search engine. No correlation between search effectiveness measures and the quality of the student's answers was found. Huuskonen et al [16] conducted a similar medical searching experiment reporting the same lack of correlation.
Smith and Kantor [20] engaged 36 users to each search 12 information gathering topics on two versions of a web search engine: one, the normal searching system and the other, a degraded version which displayed results starting from rank 300. Users weren't aware they were being shown the different versions. Although no actual effectiveness measures were taken, it is reasonable to assume that there was a significant difference in precision between the versions. However, there was no significant difference in user success in finding relevant items. Smith and Kantor reported that users of the poorer system issued more queries, which appeared to mitigate the smaller number of relevant documents retrieved in each search.
2.2 Measures predict user behavior
Measuring users through an analysis of query logs, Joachims [17] described an experiment showing users different sets of search results; as with previous work although there were measurable differences between the quantity and rank of relevant documents, Joachims saw little difference in users' click behavior. Users given poorer search results still choose top ranked documents. He proposed an alternative approach, which was to interleave the retrieval outputs of the two systems into a single ranking and observe if users tended to click on documents from one ranking more often than the other. The results showed users consistently chose documents from the better part of the interleaved ranking. This method of giving users (unknowingly) a choice and observing their preference was repeated [18] producing similar results. In this work, small, but measurable changes in document rankings were compared, and significant differences in user

behavior were observed. Further analysis of query logs to model user click behavior was conducted by many researchers, e.g. [10].
Thomas et al [24] described another preference methodology where two sets of search results were presented side-by-side to users who were then asked which of the two they preferred. The method was used to compare the top 10 results of Google and the (presumably worse) Google results in ranks 21-30. They reported a clear preference for the top ranked results over the lower ranked.
2.3 Lessons drawn from past work
After reading the first set of research results, one might question the value of all test collection based research, as the only time users show any difference in behavior, success in their work, or preference for searching systems is when large differences in effectiveness between IR systems are measured. In direct contradiction to this, is the smaller body of work in the following section measuring clear preferences by users even for subtle differences in retrieval results. What might be the cause of this apparent contradiction?
Smith and Kantor's work appears to be the clearest in demonstrating that if it is important for users to locate relevant documents they can cope with the burden of a poorer search engine by re-formulating their query. In addition, Joachims' work appears to show that users will often make do with poorer results. The work in Section 2.1 could be failing to observe differences across users because these two traits simply make human searchers hard to measure.
As can be seen, there is only limited work using the preference based approach and to the best of our knowledge there is no work using this method to test the correlations between users and evaluations based on test collections. Further, none of the past work has addressed the more nuanced questions of whether certain evaluation measures or search tasks show better prediction of user behavior over others. Although there are a plethora of papers comparing different evaluation measures, almost without exception they report cross-measure correlations or use some form of stability statistic to imply which might be better. The only exception is Al-Maskari et al who examined correlations between user satisfaction and evaluation measures [2] finding that Cumulative Gain (CG) correlated better with user preferences than P(10), DCG and nDCG, but the experiment was based on a small sample of people.
Because examination of different measures is almost unexplored, we addressed it here. With a growth of interest in search systems supporting diversity, there is as yet little research examining the predictive power of test collections in relation to diverse queries. Therefore, this paper conducted such a broad investigation into the predictive power of test collections and evaluation measures.
3. METHOD
The experiment required six components: a test collection with diverse topics and QRELS; multiple IR systems; a population of users; a method of measuring them; the selection of effectiveness measures; and a method of selecting which systems to show to users. These components are now described.
3.1 The test collection
The 50 million document Category B set of the ClueWeb09 collection was chosen as it was used in a diversity task for

556

...

...

Figure 1 - Screen shown to MTurkers: containing query, subtopic, instructions, paired rankings, input buttons, and text box

TREC's 2009 Web track. Given a short ill specified query, the goal of the diversity task was for participating groups to build IR systems that returned a ranked list of documents that collectively fulfilled the multiple information needs represented by the query. For the diversity track, each topic was structured as a set of subtopics, each related to a different user need [13]. The documents returned in the submitted runs were judged with respect to each subtopic. For each retrieved document, TREC assessors made a binary judgment as to whether or not the document satisfied the subtopic's information need.
Each one of the subtopics was categorized as being either navigational or informational (from Broder [9]). The query was also classified as either ambiguous or faceted, with ambiguous queries having multiple distinct interpretations while faceted queries had a single interpretation but with many aspects.
The structuring of subtopics judged in their own right into aggregated diverse topics, allowed (in this paper) both an experiment on diverse search and on non-diverse search: the first using the aggregated topics, the second treating the subtopics as a large set of ordinary topics.
3.2 IR systems
A source of different outputs was needed against which user preferences could be measured. Al Maskari et al in their experiments drew from a pool of three live searching systems, however, the researchers often found that the systems performed very differently from each other, which unsurprisingly resulted in large differences in user preference. In the design of the experiments here, it was judged desirable to have more explicit control over the differences between the systems being compared. Allan et al and others achieved this by artificially creating search results; we judged it preferable to use actual search output.
Arni et al [7] used the runs of an evaluation exercise as a source of search outputs to draw from to show users. From that pool of runs the researchers were able to select those runs that had similar effectiveness scores. For the category B ClueWeb09 collection, 19 diversity runs were submitted from ten research groups, these were the pool of search outputs used. Their use is detailed in 3.5.

3.3 Measuring user preference
To measure user preferences between different search results, the side-by-side method from Thomas et al [24] was chosen. For a particular topic, a pair of runs was selected from the pool and the top ten results (showing title, snippet and URL) were shown to users along with the topic title that generated the search and the subtopic description (referred to as an "aspect" in the interface) that expressed the information need behind the search request (example in Figure 1). The snippets were generated using a web service from the Bing search engine. Not all ClueWeb09 collection URLs still exist, which meant that 35% of results did not have a snippet. A post hoc analysis of data showed that missing snippets did not appear to influence user preferences.
Users were asked to indicate which of the two results they preferred. Using QREL data from the web track, effectiveness was measured on the two rankings and the agreement between users and the measures was assessed.
The aim of the diversity track was to promote searching systems that retrieved documents covering multiple interpretations of the same query, thereby ensuring that the search output was of value to the widest possible range of users. In a pilot experiment, an attempt was made to elicit user preferences for one IR system over another by asking individual users to indicate their preference for a ranking based on the ambiguous topic title alone. The expectation was that users would judge the value of search results relative to the multiple interpretations of a topic. However, it was found that the users were not able to do this reliably.
Therefore, in the experiments reported here, users were asked to focus on a particular subtopic and judge pairs of rankings in that context. They were asked to imagine they were searching for the subtopic using the query title text. The instructions were worded avoiding terms such as "diversity", so as not to bias choices. No other information about the experiment was given to the users. Users could indicate that the left or right result was better, both were equally good, or none of them were relevant (the ordering of paired systems was randomized). They were also asked to write a reason for their choice.
Different users were given the different subtopics of a topic and their preferences were aggregated to form a judgment on the diverse topic as a whole.

557

3.4 Population of users
The goal of the research work was to examine the preferences of a large number of users across many IR systems searching on a wide range of topics. It was decided to use the crowd sourcing system Mechanical Turk [5] to provide the large population.
Mechanical Turk users (MTurkers) were asked to judge a set of paired rankings for a set of subtopics. As it was assumed that there could be some disagreement amongst MTurkers, each pairing was seen on average by eight. A "trap question" was shown in an attempt to identify those who were not conducting the experiment in good faith. For every five comparisons shown to an MTurker one was a trap, which was built by pairing a run relevant to the required subtopic with a run for an entirely different topic. MTurkers who did not answer such pairings correctly had all of their answers rejected from the study (example in Figure 2). In total 342 MTurkers were used, 46 were rejected for failing a trap question (13%), which left 296 whose responses contributed to the results. We did not gather any demographic information from them. MTurkers were paid 8¢ for each block of five pairs they were shown. Many MTurkers worked on more than one block. The median time taken to complete the five pairs was just over 6 minutes. The total cost of the study including initial pilot studies was just under $60.
3.5 Selecting measures
The aim of the work was to examine how well evaluation measures predicted user preferences. Measures for both diversity and conventional IR were examined in this experiment.
3.5.1 Diversity measures
With the growth of interest in diversity, a number of evaluation measures were proposed. These measures include Cluster Recall (CR) used in ImageCLEFPhoto 2008 [7], Clarke et al's -nDCG [11], Agrawal et al's intent aware Precision (IA-PC) [1], and Clarke et al.'s [12] novelty- and rank-biased precision (NRBP).
Cluster Recall (CR) is based on the subtopic recall (or S-Recall) proposed by Zhai et al. [28] to assess diversity. The CR at a cutoff rank k, CR(k), is defined as the percentage of subtopics covered by the first k documents in a ranked list. This is a pure diversity measure, i.e. it is not affected by the number of documents covering each cluster, or by their position in the ranked list. Further, it does not incorporate any notion of document novelty within a given subtopic.
Contrary to CR, both -nDCG and NRBP consider both the number of relevant documents and their rank position over the subtopics of a query. For both measures, each document is assigned a gain value that is a function of the number of subtopics

the document covers, and for each subtopic, the number of documents ranked above the given document that cover the same subtopic. The variable  is used to control how important diversity is in the measure. The -nDCG metric is based on the traditional nDCG metric utilizing the aforementioned gain function, while the NRBP metric is based on Rank-Biased Precision (RBP). It is defined by replacing the traditional binary relevance of a document with the aforementioned gain. Thus, for a given subtopic, for =0 the two metrics do not assess the novelty of the subsequent documents that cover this subtopic, while for =1 they only consider relevant the first document that covers the given subtopic, ignoring all the subsequent ones.
Finally, intent aware Precision at rank k accounts for diversity and the number of relevant documents. Given a query, the precision value at cut-off k is computed for each subtopic separately (i.e. only the documents that cover the subtopic under consideration are considered relevant ­ called aspect precision) and the weighted average of the precision values is computed, with weights being the popularity of each subtopic. In the Web Track data the subtopics of a query were assumed to be equally popular.
3.5.2 Conventional evaluation measures
By considering each of the subtopics in the test collection as individual topics with their own QRELS, it was possible to examine differences across alternate conventional evaluation measures. Here nDCG, Mean Reciprocal Rank (MRR) and Precision measured at rank 10, P(10), were the measures chosen.
3.6 Selecting the pairs to show users
As seen in Section 2.1, existing research showed that differences in user performance could be measured on IR systems with large differences in search effectiveness. The challenge was in measuring user preference when searching on IR systems with far smaller differences. Therefore, the selection of run pairs to show the MTurkers focused on finding pairs that were similar to each other. There was a concern that using runs with low effectiveness could result in confusion when choosing between rankings. Therefore, topics where all runs had two or fewer relevant documents in the top ten were removed. This left thirty topics in the dataset.
A search was conducted across the remaining topics to locate pairs of runs that had the same number of relevant documents in the top ten, done to ensure that the rankings were similar. To enable diversity measures to be tested, runs were only paired when there was more than a minimum difference in subtopic coverage: CR(10) and -nDCG(10) 0.1. Runs submitted by the same research group were not paired together.

Figure 2 ­ Partial screen shot of a trap question shown to MTurkers 558

In total, 79 system pairs matching the search criteria were found. Each system pair was shown to, on average, eight MTurkers for each of a topic's subtopics. The MTurker judgments for one of the 79 pairs were gathered as follows. Each system pair displayed the two retrieval results for a search based on the query text of a particular topic. The MTurkers were asked to indicate their preference for one of the paired systems in relation to a particular subtopic. Multiple MTurkers were shown the same system/subtopic pair; although if an MTurker failed a trap question, their preference judgments were removed. MTurker preferences were treated as votes for one system or another normalized by the number of MTurkers who examined the system/subtopic pair.
This process was repeated for each of a topic's subtopics and the mean of the resulting normalized majority values was taken. The system that the majority of MTurkers preferred across the subtopics was selected as the best system for that topic. At the same time a diversity measure was calculated for the two system rankings, the best was the one with the highest effectiveness score. Across the 79 pairs, the number of times that MTurkers agreed/disagreed with the diversity measure was counted. If there was a tie, the MTurkers were judged to have said that the ranks from the systems were equal.
4. RESULTS
The predictive power of test collections/measures was examined on both the diverse web search topics (section 4.1) and their component subtopics (section 4.2).
4.1 User preferences in diversity search
The results of the initial experiment are shown in Table 1. As can be seen, there was a preference amongst users for systems that were measured to be more diverse. Assuming a null hypothesis that MTurkers saw no difference between the paired systems, and the level of agreement was simply due to chance; using a t-test1 it was found that p<0.05; the null hypothesis was rejected and the level of agreement in Table 1 was found to be significant.

Users

-nDCG Small  Large 

Agreed

54 68% 26 60% 28 78%

Ranks equal

3 4% 3 7% 0 0%

Disagreed

22 28% 14 33% 8 22%

79

43

36

Table 1 ­ Agreement differences in small and large -nDCG

Next, the 79 pairs were placed into one of two bins: respectively those with a small and large difference in -nDCG. The pairs were sorted by their difference; those pairs greater than the mean of the differences were placed in the large  bin; the others in the small  bin. The figures for user agreement are also shown in Table 1. Although the agreement appeared to grow as the size of difference between the two rankings increased, a significance test between large and small  showed p>0.05.

The range of different cluster evaluation measures described above were also examined, see Table 2. In this test no significant difference between any pair of measures was found. It is notable that the measure CR provided as effective a prediction of user preference as the other measures. Cluster Recall is simply

1 A 2 tailed, 2 sample unequal variance test was used throughout.

counting the percentage of topic interpretations that are covered in the ranking.

Users

-nDCG CR NRBP IA-PC

Agreed

54

55

54

51

Ranks equal

3

3

3

2

Disagreed

22

21

22

26

79

79

79

79

Table 2 ­ MTurkers' agreements to the diversity measures

Kendall's  -nDCG@10 CR@10 NRBP

CR@10 0.7956

NRBP 0.8523 0.7159

IA-PC@10 0.8424 0.7219 0.7010

AP-correl. -nDCG@10 CR@10 NRBP

CR@10 0.6719

NRBP 0.8736 0.6282

IA-PC@10 0.7867 0.5492 0.6839

Table 3 ­ Correlations between diversity measures
Given that we have observed similar degrees of correlation between different diversity measures and user preferences, we next investigated how these different measures correlated with each other. Given that these measures assess somewhat different aspects of system effectiveness, a strong correlation would indicate that better systems are good in all aspects of effectiveness assessed by these measures. A weak correlation will indicate that different users prefer different qualities of the ranked lists and the sets of users whose preferences agree with each individual measure do not fully overlap even though it so happens to be of similar size.
For each measure, we considered the mean values for all systems/runs submitted to the TREC track and over all 50 queries were calculated. For each two measures, we calculated Kendall's , and the AP-correlation [27], see Table 3. Kendall's  is a function of the minimum number of pair wise adjacent interchanges needed to convert one ranking into the other. The AP-correlation is a similar metric, which however mostly accounts for the swaps towards the top of the system rankings, i.e. the disagreements over the top ranked systems. It can be seen that, there is a positive correlation among all measures, the strength of which however differs among different measures. In particular, the most correlated measures are -nDCG and NRBP. IA-PC and -nDCG are also well correlated, however, they mostly agree on the poorly performing systems as indicated by lower AP-correl. Further, there is a positive correlation between CR and -nDCG; however it also concerns the bottom performing systems. Finally, CR and IA-PC correlate well regarding the bottom performing systems but they rank the top performing systems differently.
Therefore, the weak correlation among several of these measures indicates that indeed they assess different aspects of system performance. However, given the results in Table 2 it seems that all of these aspects are important for an average user.

4.2 User preferences in traditional search
If one treats each subtopic of the test collection as a distinct test collection topic, with its own QRELS, one can compare user

559

preferences against traditional test collection measures. In total there were 252 subtopic/system pairs shown to MTurkers.
Three standard evaluation measures ­ nDCG, MRR, and P(10) ­ were applied to the pairs and a prediction of which ranking users would prefer was made based on each measure. The measures were selected as exemplars of particular features in evaluation: P(10) is a simple count of the number of relevant documents in the top 10; MRR measures the rank of the highest relevant; nDCG combines both number of relevant documents and their rank; nDCG's ability to handle degrees of relevance was not exploited as the diversity track QRELS contained binary judgments only.
If the effectiveness measure for the two rankings were the same, user preferences were not examined. Therefore the number of pairs considered differed across the three measures. The results of this analysis are shown in Table 4.

Users

nDCG

MRR

P(10)

Agreed

160 63% 127 53% 106 50%

Ranks equal

28 11% 27 11% 24 11%

Disagreed

64 25% 87 36% 84 39%

252

241

214

Table 4 ­ MTurkers's agreement with traditional measures

Use of the t-test showed that nDCG was significantly more in agreement with user preferences than the other two measures. There was no significant difference between MRR and P(10).

We considered the P(10) result in more detail. Because system pairs that were measured to be the same were removed from the figures in Table 4, in all of the 214 system comparisons shown to MTurkers, we can infer that one of the runs had retrieved more relevant documents than the other. Yet for only 50% of the pairs did MTurkers agree that the system with more relevant documents was the better one. In 39% of the cases MTurkers preferred the system with fewer relevant documents and in 11% they judged the system retrieving more relevant to be no better than the system returning fewer. The number of relevant documents retrieved is not the only factor that influences users.

The figures for MRR indicated that just focusing on the position of the top ranked document provided slightly more user agreement with user preferences. However, combining the two features into the nDCG measure appeared to be the best of the three strategies.

Focusing on nDCG, as in Section 4.1, the pairs were split into two bins: one for pairs with a large  and one for a small . The split was defined by the mean difference between the 252 pairs. The figures for user agreement are shown in Table 5.

Users

nDCG

Small  Large 

Agreed

160 63% 93 60% 67 68%

Ranks equal

28 11% 20 13% 8 8%

Disagreed

64 25% 41 27% 23 23%

252

154

98

Table 5 ­ Agreement differences in small and large nDCG

It can be seen that users agree more when there was a large difference in the evaluation measure than when the difference is small. However, as with the comparison in Table 1 no significance was found.

An alternate way to split the 252 pairs was on whether one of the two rankings contained no relevant documents. Table 6 and Table

7 show the agreement figures based on this split. Contrasting the strength of user agreement between Table 6 and Table 7, for all three columns MTurkers agreed more strongly when one pair of runs had nDCG=0. This was confirmed with a statistically significant difference being found between the figures in the first columns (nDCG) of the two tables.
If there was a larger difference between nDCG values in the system pairs in Table 6 compared to the pairs in Table 7 that could explain the differences in agreement level. However, an examination of mean  in nDCG between each of the 126 pairs in Table 6 and Table 7 showed little difference, respectively 0.164 and 0.169. The best explanation for the difference was that it was due to the presence of a zero nDCG in one of the pairs. The results suggest a need for evaluation measures, e.g. GMAP [19] which penalize systems that fail to return any relevant documents for particular topics.

Users

nDCG

Small  Large 

Agreed

88 70% 54 66% 34 77%

Ranks equal

13 10% 11 13% 2 5%

Disagreed

25 20% 17 21% 8 18%

126

82

44

Table 6 ­ Analysis of pairs which had nDCG=0 in one result

Users

nDCG

Small  Large 

Agreed

72 57% 39 54% 33 61%

Ranks equal

15 12% 9 13% 6 11%

Disagreed

39 31% 24 33% 15 28%

126

72

54

Table 7 ­ Analysis of pairs with nDCG>0 in both results

The final analysis of this data was to examine different types of topic. Within the TREC Web collection, a small number of the subtopics were navigational, most were informational [9]. User agreement was measured split across these 2 topic types (see Table 8).

Users

nDCG

Informational Navigational

Agreed

160 63% 146 62%

14 82%

Ranks equal 28 11%

28 12%

0 0%

Disagreed

64 25%

61 26%

3 18%

252

235

17

Table 8 ­ Analysis on different aspect types

For the small number of navigational topics, there was a strong agreement between users and the predictions made by the evaluation measures. No significance was found between the columns in this table. Given the small number of navigational topics, it would be valuable to repeat this experiment with a even balance in the number of navigational and informational topics.

4.3 MTurker comments on differences
In addition to indicating their preferences, MTurkers could also provide comments about their choices. In total, 96% of the judgments had associated comments that often indicated the reason(s) behind, or affecting, a decision. These often highlighted factors beyond the results simply having more relevant documents on a topic (informational) or a link to a required webpage (navigational). There were 11.6 words per comment, on average, and using an inductive approach to data analysis [23] comments in which the users made a specific preference (54% of those submitted, 1,307) were categorized. Fifteen classes were derived

560

and in 88 cases, comments were assigned multiple categories, e.g. "the left one has more useful results higher in the search" was assigned the classes 'position' and 'number' indicating that the number of results and their position in the ranking would have likely influenced their preference judgment, see Table 9.
Although these are factors which researchers often highlight as affecting relevance [8], we see these mentioned unprompted by the MTurkers in this study, again highlighting the benefit of using MTurk to gather data for this kind of study, beyond implicit feedback strategies such as query logs.
5. REFLECTIONS ON METHOD
Here we discuss using MTurk as a source of user preferences; and using preference as a means of determining the impact of search on users.
5.1 Quality of the MTurk data
With an anonymous monetized crowd sourcing system, there is always a concern that data gathered will be overwhelmed with noise from spammers. However, evidence in our analysis such as time taken to complete tasks (median ~6 min.) of this set indicated that the majority of data was created in good faith. Indeed this gathering from hundreds of users of not only quantitative data, but also qualitative data gave this set a value that query/click logs do not have.
Nevertheless there are collections of data points in the set which we do not fully understand. Unexpected user responses to search results is not uncommon: Tagliacozzo [22], surveying ~900 MEDLINE users, described how nearly 18% declared dissatisfaction with search results despite earlier indicating that a large number of relevant documents were returned in the search they were asked to judge. Alonzo described how the development of MTurk experiments required multiple iterations to ensure that MTurkers were given clear instructions on what to do and to be certain that the responses from them are given in good faith [6].
The data set yielded a set of significant results on evaluation measures, but we view the method used here as a first step towards developing a more refined approach. Improvements will come not only from avoiding erroneous output from MTurkers, but also from building a more refined model of how users determine their preference for one searching system over another.
5.2 Does preference imply satisfaction?
The ultimate aim of this and past research was to understand if differences found between IR systems based on test collections were real: with a better system, would users be able to search more effectively, achieve their goals quicker, ultimately be more satisfied? Results from past work indicated that measuring such broader outcomes were challenging as users were adept at adapting either by searching more intensively or by managing to exploit relevant but poorer output.
The past work showed significant differences measured in a test collection did not necessarily show practical differences in user behavior. Was this simply because there was no practical difference to measure or was there simply a challenge in the way we measure users?
This and previous work showed that a preference based methodology can measure significant differences in users for relatively small differences in retrieval effectiveness. However, it

is worth remembering that the side-by-side method is a simulation of search and the measurement of preference says nothing about whether users are more satisfied with their search or will achieve their information seeking tasks more effectively. One might wish to hypothesize that such a link exists, but testing that hypothesis is for future work.

Category Example

#

On topic "All the results about secret garden" /

332

"Contains work information in Michigan."

Specific "the Wikipedia link for Hoboken will have most 265 links of the information I would be looking for" / "Right side has a link to the ESPN home page as asked for." / "Right links to the desired information, the left does not."

Not "Each result would be helpful, but the left was

181

classifiable easier." / "I thought the left side was better."

Irrelevant "More non-relevant results in right column." /

144

"#5 on the left side is not for flame design at

all"

More "more relevant" / "more relevant results on the 132 relevant right"

Number "right has more map links" / "There are more

123

results for finding houses or apartments in

Hoboken"

Position "Both lists include a link to reviews, but it's

69

higher on the list on the left than on the right." /

"Top results more closely align to the request."

Range of "The right column has a broader range of

66

results relevant topics." / "seemed to include more of a

variety of dino sites that would have what the person was looking for"

Presentation "Results on the left are clearer and easier to

36

read." / "right results are more descriptive"

Quality/ "right had a porn result, left is better" / "Left

16

authority side has more relevant results, as well as listing

more credible webpages"

Spam/ "Left seems to be legit. Right is more of junk

15

adverts and ads" / "Most of the right results are

advertisements" / "less spammy"/ "Less

commercial, more focused on the real subject results." / "The right column just lists pet

adoption classified ads"

Duplication "right results has three repeated listings of

11

Verizon wireless and broad band services" /

"Almost all of the right results were just Wikipedia pages."

Availability "we can download the maps" / "more free data"

6

Language "Left results have more relevancy, every link

6

has something about pampered chef or related

equipment in English"

Dead links "'The right had dead links" / "The left had a few

5

links which did not work but the majority did

and returned results on appraisals."

Table 9 ­ Analysis of 1,307 MTurk comments

6. CONCLUSIONS AND FUTURE WORK
The research questions posed at the start of the paper were answered through the gathering and examination of a large dataset

561

of user preferences for a wide range of different IR systems tested over many queries and query types.
Clear evidence was found that effectiveness measured on a test collection predicted user preferences for one IR system over another. The strength of user prediction by test collection measures appeared to vary across different search tasks such as navigational or informational queries.
For diverse queries, little difference between diversity measures was found. A conventional analysis of correlation between the measures was conducted confirming that they are similar. When comparing nDCG, MRR and P(10) it was found that nDCG most effectively modeled user preferences. However, user preferences between pairs of systems where one had failed to retrieve any relevant documents were notably stronger than when both rankings had at least one relevant document, which suggests a need for adjustments to these measures to allow for this user view.
Finally, an examination and grouping of the written reasons that users provided for choosing one system ranking over another was outlined. Here it was shown that although relevance, rank, and information content of the documents was an important factor when users chose one system over another, a wide range of other reasons was also provided by users. Specific web sites were sought by some; avoidance of commercial sites or documents in other languages was desired by others. Such information from the MTurkers suggested that the test collections, relevance judgments and evaluation measures could be improved to provide a more effective model of user preferences than is currently available.
Such ideas are left to future work, where we will also continue to analyze our gathered data set as well as consider how to refine our collection methods to gather larger more informative sets in the future.
7. ACKNOWLEDGMENTS
This work was co-funded by the EU FP7 project ACCURAT contract FP7-ICT-248347 and Marie Curie Fellowship FP7PEOPLE-2009-IIF-254562.
8. REFERENCES
[1] Agrawal, R., Gollapudi, S., Halverson, A., & Ieong, S. 2009. Diversifying search results. ACM WSDM, 5-14.
[2] Al-Maskari, A., Sanderson, M. & Clough, P., 2007. The relationship between IR effectiveness measures and user satisfaction. ACM SIGIR, 773-774.
[3] Al-Maskari, A., Sanderson, M., Clough, P., & Airio, E. 2008. The good and the bad system: does the test collection predict users' effectiveness? ACM SIGIR, 59-66.
[4] Allan, J., Carterette, B., & Lewis, J. 2005. When will information retrieval be "good enough"? ACM SIGIR, 433440.
[5] Alonso, O., Rose, D. E., & Stewart, B. 2008. Crowdsourcing for relevance evaluation. SIGIR Forum 42, 2, 9-15.
[6] Alonso, O. & Mizzaro, S., 2009. Can we get rid of TREC assessors? Using Mechanical Turk for relevance assessment.
In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation. 15­16. [7] Arni, T., Tang, J., Sanderson, M., Clough, P. 2008. Creating a test collection to evaluate diversity in image retrieval. Workshop on Beyond Binary Relevance, SIGIR 2008.

[8] Barry, C. L. 1994. User-defined relevance criteria: an exploratory study. J. Am. Soc. Inf. Sci. 45, 3, 149-159.
[9] Broder, A. 2002. A taxonomy of web search. SIGIR Forum 36(2) 3-10.
[10] Chapelle, O. and Zhang, Y., 2009. A dynamic bayesian network click model for web search ranking. Proc. 18th WWW Conf, 1-10
[11] Clarke, C., Kolla, M., Cormack, G., Vechtomova, O., Ashkan, A., Büttcher, S., MacKinnon, I. 2008. Novelty & diversity in information retrieval evaluation. ACM SIGIR, 659-666.
[12] Clarke, C., Kolla, M., & Vechtomova, O. 2009. An Effectiveness Measure for Ambiguous and Underspecified Queries. Advances in Information Retrieval Theory, 188-199.
[13] Clarke, C., Craswell, N., and Soboroff, I. 2009. Preliminary Report on the TREC 2009 Web Track. TREC 2009 Notebook.
[14] Hersh, W.R. et al., 2002. Factors associated with success in searching MEDLINE and applying evidence to answer clinical questions, Am Med Inform Assoc.
[15] Hersh, W., Turpin, A., Price, S., Chan, B., Kramer, D., Sacherek, L., & Olson, D. 2000. Do batch and user evaluations give the same results? ACM SIGIR, 17-24.
[16] Huuskonen, S. & Vakkari, P. 2008. Students' search process and outcome in Medline in writing an essay for a class on evidence-based medicine. Journal of Documentation, 64(2), 287-303.
[17] Joachims, T., 2002. Evaluating retrieval performance using click through data. Workshop on Mathematical/Formal Methods in IR, 12­15.
[18] Radlinski, F., Kurup, M., Joachims, T. 2008. How does click through data reflect retrieval quality? ACM CIKM, 43-52.
[19] Robertson, S. 2006. On GMAP: and other transformations, ACM CIKM, 78-83
[20] Smith, C.L. & Kantor, P.B., 2008. User adaptation: good results from poor systems. ACM SIGIR, 147-154.
[21] Su, L.T., 1992. Evaluation measures for interactive information retrieval. IP&M, 28(4), 503-516.
[22] Tagliacozzo, R., 1977. Estimating the satisfaction of information users. Bulletin of the Medical Library Association, 65(2), 243-249.
[23] Thomas, D.R. 2006. A General Inductive Approach for Analyzing Qualitative Evaluation Data. American Journal of Evaluation, 27(2), 237-246
[24] Thomas, P. & Hawking, D., 2006. Evaluation by comparing result sets in context. ACM CIKM, 94-101.
[25] Turpin, A.H., Hersh, W. 2001. Why batch and user evaluations do not give the same results. ACM SIGIR, 225231.
[26] Turpin, A. & Scholer, F. 2006. User performance versus precision measures for simple search tasks. ACM SIGIR, 1118.
[27] Yilmaz, E., Aslam, J., Robertson, S. 2008. A new rank correlation coefficient for information retrieval. ACM SIGIR, 587-594.
[28] Zhai, C.X., Cohen, W.W. & Lafferty, J., 2003. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. ACM SIGIR, 10-17.

562

Positional Relevance Model for Pseudo-Relevance Feedback

Yuanhua Lv
Department of Computer Science University of Illinois at Urbana-Champaign
Urbana, IL 61801
ylv2@uiuc.edu
ABSTRACT
Pseudo-relevance feedback is an effective technique for improving retrieval results. Traditional feedback algorithms use a whole feedback document as a unit to extract words for query expansion, which is not optimal as a document may cover several different topics and thus contain much irrelevant information. In this paper, we study how to effectively select from feedback documents those words that are focused on the query topic based on positions of terms in feedback documents. We propose a positional relevance model (PRM) to address this problem in a unified probabilistic way. The proposed PRM is an extension of the relevance model to exploit term positions and proximity so as to assign more weights to words closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic. We develop two methods to estimate PRM based on different sampling processes. Experiment results on two large retrieval datasets show that the proposed PRM is effective and robust for pseudo-relevance feedback, significantly outperforming the relevance model in both document-based feedback and passage-based feedback.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models, Relevance feedback, Query formulation
General Terms
Algorithms
Keywords
Positional relevance model, pseudo-relevance feedback, positional language model, proximity, passage-based feedback, query expansion
1. INTRODUCTION
Pseudo-relevance feedback (or blind feedback) is an important general technique for improving retrieval accuracy [26, 24, 27, 3, 25, 16, 32]. The basic idea of pseudo-relevance
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

ChengXiang Zhai
Department of Computer Science University of Illinois at Urbana-Champaign
Urbana, IL 61801
czhai@cs.uiuc.edu
feedback is to assume that a small number of top-ranked documents in the initial retrieval results are relevant and select from these documents related terms to the query to improve the query representation through query expansion, which generally leads to improvement of retrieval performance.
Most existing feedback algorithms (e.g., [26, 24, 27, 3, 25, 16, 32]) use a whole feedback document as a unit for selecting expansion terms, which, however, is non-optimal when the content of a document is incoherent (i.e., covering several different topics) and thus may contain much irrelevant information as often happens in Web search. The existence of multiple topics and irrelevant information would lead to a noisy feedback model as potentially harmful terms from non-relevant topics may be picked up to include in the feedback model. As a result, the use of pseudo feedback may not improve or even decrease the retrieval performance. Thus a critical challenge in improving all feedback methods is to effectively select from feedback documents those terms that are most likely relevant to the query topic.
In this paper, we solve this challenge by exploiting the position and proximity information of terms as cues to assess if a term is related to the query topic. Since topically related content is usually grouped together in text documents, terms closer to the occurrences of query words are, in general, more likely relevant to the query topic, thus a good feedback model should intuitively place higher weights on such terms.
Based on this intuition, we propose a novel positional relevance model (PRM) to incorporate the cues of term positions and term proximity in a probabilistic feedback model based on statistical language modeling. The key idea is to extend the relevance model [16] to aggregate the associations between a term and query words at the position level via the positional language model (PLM) [19]. An important advantage of estimating a relevance model based on PLM is that it can model the "relevant positions" in a feedback document with probabilistic models so as to assign more weights to terms at more relevant positions in a principled way, thus leading naturally to selection of expansion terms more likely relevant to the query topic.
Since PRM estimates a relevance model at the level of term positions, it incorporates individual term positions directly into a probabilistic model. This is in contrast with virtually all the existing pseudo feedback techniques which have only made use of term statistics at the document level [26, 24, 27, 3, 25, 16, 32, 18], or at the best, at the level of passages [2, 30, 17, 21] without distinguishing every different position.

579

Analogously to the two methods proposed for estimating the relevance model [16], we also derive two methods for estimating PRM, leading to two different ways to aggregate term information based on positions. We evaluate the proposed PRM on two large TREC datasets. Experimental results demonstrate that PRM is effective in exploiting term proximity for pseudo feedback and significantly outperforms the relevance model in both document-based feedback and passage-based feedback.
2. RELATED WORK
2.1 Pseudo-Relevance Feedback
Pseudo-relevance feedback has been shown to be effective with various retrieval models [26, 24, 27, 3, 25, 16, 32, 18].
In the vector space model, feedback is usually done by using the Rocchio algorithm, which forms a new query vector by maximizing its similarity to pseudo-relevant documents [26]. The feedback method in classical probabilistic models is to select expansion terms primarily based on Robertson/Sparck-Jones weight [24].
Several query expansion techniques have been developed in the language modeling framework, including, e.g., the mixture-model feedback method [32] and the relevance model [16]. The basic idea is to use feedback documents to estimate a better query language model. Both the mixture model and relevance model have been shown to be very effective, but the relevance model appears to be more robust [18].
In the mixture-model feedback [32], the words in feedback documents are assumed to be drawn from two models: (1) background model and (2) topic model. The mixturemodel feedback finds the topic model that best describes the feedback documents by separating the topic model from the background model. The topic model is then interpolated with the original query model to form the expanded query.
Much like mixture-model feedback, the relevance model also estimates an improved query language model. Given a query Q, a relevance model is a multinomial distribution P (w|Q) that encodes the likelihood of each term w given the query as evidence. To estimate the relevance model, the authors first compute the joint probability of observing a word together with the query words in each feedback document and then aggregate the evidence by summing over all the documents. It essentially uses the query likelihood P (Q|D) as the weight for document D and takes an average of the probability of word w given by each document language model.
All these pseudo feedback algorithms use a whole feedback document as a unit, and thus term position and proximity evidences are largely ignored. Our work is an extension of the relevance model to estimate a feedback model based on individual term positions.
2.2 Passage Feedback
There have been several studies to exploit passage-level evidence of documents for feedback, e.g., [2, 30, 17], which can potentially address the heterogeneous topical structure of documents to some degree. However, these approaches usually take a traditional feedback model as a black box to handle sub-document units as if they were regular documents. For example, Liu and Croft's work [17] estimates a relevance model based on the best matching passage of each feedback document, where fixed-length arbitrary passages

that resemble overlapped windows but with an arbitrary starting point [12] can often be used due to its effectiveness and efficiency [12, 17]. A limitation of this approach is that term positions are not directly incorporated into the feedback model. As we will show later in the paper, the proposed PRM outperforms such a passage feedback approach.
Some other approaches, e.g., [31, 4], make use of visual cues or eye tracker to improve passage feedback for web search: on the server side of a search engine, documents can be decomposed into topically different components via visual cues [31], while on the client side of users, gaze-based attention feedback [4] can go down to the sub-document level by exploiting evidence about which document parts the user looks at. However, such approaches face the same problems as general passage feedback without being able to model each individual position.
In fact, these passage-based or sub-document level feedback models are orthogonal to the proposed PRM in the sense that PRM can be applied to passages to model proximity inside a passages in the same way as it can be applied to whole documents. Moreover, the underlying positional language model [19], which can capture passage-level evidence in a soft way in model estimation, has been shown to work better than imposing a "hard" boundary of passages.
Recently, Metzler and Croft's work on Latent Concept Expansion [21] also indirectly captures term position and proximity evidence through the use of appropriate passages. Their work provides a more general model which is complementary with our ideas in that we can use PRM as an effective feature defined on their graph, so our PRM scores can then be combined with other features explored in [21] to further improve its performance.
2.3 Term Proximity Heuristic in IR
The term proximity heuristic, which rewards a document where the matched query terms occur close to each other, has been previously studied in [13, 14, 11, 9, 23, 20, 5, 6, 28, 19, 10, 34]. Keen's work [13, 14] is among the earliest efforts, in which, a "NEAR" operator was introduced to address proximity in Boolean retrieval model. The shortest interval containing a match set was first used as a measure of proximity in [9, 11]. Recent work has attempted to heuristically incorporate proximity into an existing retrieval model (often through score combinations) [22, 23, 5, 6, 28]. For example, a variety of proximity measures were proposed and explored in [28]. Another work [10] used a learning approach to combine various proximity measures to obtain an effective proximity-based retrieval function. Recently in our previous work [19], we proposed a positional language model (PLM) for information retrieval, which not only captured term proximity information but also covered passage retrieval in a unified language modeling approach. However, in all these studies, term proximity has been solely used for ranking documents in response to a given query rather than improving pseudo feedback.
There has been relatively little work done in the area of formally modeling term proximity heuristic in the context of pseudo feedback. However, there have been several attempts to simply combine term proximity with other feedback heuristics to select good expansion terms. In [29], several distance functions were evaluated for selecting query expansion terms from windows or passages surrounding query term occurrences; however, no improvement was observed as

580

compared to existing feedback methods. Cao et al. [7] used a supervised method to classify whether an individual expansion term is good or not, in which term proximity is one of their features. Their method only loosely combined term proximity with traditional feedback heuristics; in contrast, we incorporate term position and proximity into a probabilistic feedback model with more meaningful parameters.

3. POSITIONAL RELEVANCE MODEL
In this section, we describe the proposed positional relevance model (PRM) which incorporates term position information into the estimation of feedback models so that we can naturally reward terms close to query terms in the feedback documents and avoid including irrelevant terms in the feedback model.
The proposed PRM can be regarded as an extension to the relevance model (RM) [16]. We thus first give a brief introduction to the relevance model.
3.1 Relevance Models
As a pseudo feedback method, the relevance model [16] has proven to be not only effective, but also robust in a recent study [18]. The basic idea is to use the query likelihood score of a feedback document as the weight and estimate a query language model (for feedback) based on weighted aggregation of term counts in the feedback documents.
Formally, let Q = {q1, q2, · · · , qm} be a query and  represent the set of smoothed document models for the pseudo feedback documents. One of the most robust variants of the relevance model (RM1) [18] is computed as follows [16]:

m

P (w|Q) 

P (w|D)P (D) P (qi|D)

(1)

D 

i=1

where p(D) is a prior on documents and is often assumed to be uniform without any additional prior knowledge about document D. Thus, the estimated relevance model is essentially a weighted combination of individual feedback document model with the query likelihood score of a document as its weight.
After the relevance model is estimated, the estimated P (w|Q) can then be interpolated with the original query model Q to improve performance [1].

P (w|Q) = (1 - )P (w|Q) + P (w|Q)

(2)

where  is a parameter to control the amount of feedback. In the rest of the paper, we will refer to this instantiation of relevance model as RM3.

3.2 Positional Relevance Models
In the relevance model, the count of a term is computed over an entire feedback document. The main idea of the proposed positional relevance model (PRM) is to further distinguish different positions of a term and discount the occurrences of a term at positions that are far away from a query term in a feedback document.
Similarly to RM, a PRM is also a multinomial distribution P (w|Q) that attempts to capture the probability that term w is seen in a relevant document. However, PRM goes beyond RM to estimate the conditional probability P (w|Q) in terms of the joint probability of observing w with the query

Figure 1: Dependence networks for two methods, i.e., method 1 (left) and method 2 (right), of estimating positional relevance models.

Q at every position in every feedback document. Formally,

P (w|Q) = P (w, Q)  P (w, Q) =

|D|
P (w, Q, D, i) (3)

P (Q)

DF i=1

where i indicates a position in document D, and F is the set of feedback documents (assumed to be relevant).
The challenge now lies in estimating the joint probability P (w, Q, D, i). Inspired by the two estimation methods proposed in [16] for estimating relevance models, we derive two methods similarly for estimating P (w, Q, D, i). The first method assumes that w is sampled in the same way as Q, while the second method assumes that w and Q are sampled using two different mechanisms.

3.2.1 Method 1: i.i.d. sampling
In this method, we first compute the joint probability of observing a word together with the query words at each position and then aggregate the evidence by summing over all the possible positions. Specifically, we factor the joint probability P (w, Q, D, i) for each pseudo-relevant document D as follows:

P (w, Q, D, i) = P (D)P (i|D)P (w, Q|D, i)

(4)

Intuitively, we have assumed a generative model in which

we would first pick a document according to P (D), then

choose a position i in document D with probability P (i|D),

and finally generate word w and query Q conditioned on D

and i, with probability P (w, Q|D, i).

P (D) can be interpreted as a document prior and set to

a uniform distribution with no prior knowledge about doc-

ument D. While it is possible to estimate P (i|D) based on

document structures, here we assume that every position is

equally

likely,

i.e.,

P (i|D)

=

1 |D|

.

Improving

the

estimation

of p(D) and P (i|D) would be an interesting future work.

An illustration of the dependencies between the variables

involved in the derivation is shown on Figure 1 (left side).

After making these assumptions and a further assump-

tion that the generation of word w and that of query Q are

independent, we have

P (w, Q, D, i)  P (w, Q|D, i) = P (Q|D, i)P (w|D, i) (5)

|D|

|D|

Plugging Equation 5 into Equation 3, we obtain the following estimate of the PRM:

P (w|Q)  P (w, Q) 

|D| P (Q|D, i)P (w|D, i)

(6)

DF i=1

|D|

581

In the above equation, P (w|D, i) is the probability of sampling word w at position i in document D. To improve the efficiency of PRM, we simplify P (w|D, i) as:

P (w|D, i) =

1.0 0.0

if w occurs at position i in D otherwise

(7)

The term P (Q|D, i) in Equation 6 is the key component in estimating the positional relevance model. It is the query likelihood at position i of document D, and we will discuss how to estimate it based on the positional language model [19] in Section 3.2.3. Additionally, there is a third term |D| in the equation, which penalizes long documents to prevent them from dominating the feedback model (long documents naturally have more positions).
Thus, Equation 6 essentially combines all terms in feedback documents by assigning different weights to each term: (1) P (Q|D, i) serves as a relevance-based weight for each position in each document so that a position with many query terms nearby would have a higher weight. Thus as an intra-document weight, P (Q|D, i) can measure the relative weights of positions within a document: a position closer to query words would more likely generate the query, and as a result a term that occurs at this position would naturally receive a higher weight. (2) |D| comes into the formula because of the assumption about uniform distribution over all the positions in a document and can be interpreted as an inter-document weight: it penalizes a long document which is reasonable since a longer document by nature has more positions and more occurrences of terms to contribute.

3.2.2 Method 2: conditional sampling
In this method, we consider the following different way to decompose the joint probability distribution:

P (w, Q, D, i) = P (Q)P (D|Q)P (i|Q, D)P (w|D, i) (8)

The assumed generative model is as follows. We first pick a query according to some prior P (Q). We then generate a document D with probability P (D|Q). Finally, we select a position i in D with probability P (i|Q, D) and generate word w according to P (w|D, i). An illustration of this sampling process is given on the right side of Figure 1.
For the purpose of estimating P (w|Q), we can clearly ignore the term P (Q) as it is a query-specific constant. Using Bayes Rule and assuming both P (D) and P (i|D) to be uniform (as we have assumed in the first estimation method), we have

P (D|Q) =

P (Q|D)P (D)

=

P (Q|D)

(9)

DF P (Q|D)P (D)

DF P (Q|D)

P (i|D, Q) =

P (Q|D, i)P (i|D)

|D| i=1

P

(Q|D,

i)P

(i|D)

=

P (Q|D, i)

|D| i=1

P

(Q|D,

i)

(10)

Plugging Equations 8, 9, and 10 into Equation 3, we ob-

tain the following estimate of PRM:

|D|
P (w|Q) 
DF i=1

P (Q|D) DF P (Q|D)

P (Q|D, i)

|D| i=1

P

(Q|D,

i)

P

(w|D,

i)

(11)

where P (Q|D) is the query likelihood score of document D,

which can be computed using either the positional language

models [19] or the standard document language model. In

our experiments, we use the latter, i.e., P (Q|D) =

m j=1

P

(qj

|D).

As in the first estimation method, we compute P (w|D, i) using Equation 7.
Similarly to the first estimation method, this second estimate of PRM also essentially combines all terms in feedback documents by assigning different weights to each term: the first weighting term in Equation 11 is seen to be the normalized query likelihood score of the document, which assigns more weights to documents that are more likely to be relevant, while the second weighting term is the normalized query likelihood of each positional language model, which assigns more weights to terms that are closer to query words.
Compared to the first estimation method, the document length normalizer |D| is missing, but a comparable effect is now achieved by normalizing the query likelihood of each positional language model P (Q|D, i). Indeed, the effect of intra-document weighting and inter-document weighting can now be seen even more clearly, i.e., the normalized P (Q|D) can be interpreted as the inter-document weight favoring a document matching the query well, while the normalized P (Q|D, i) clearly achieves intra-document weighting to place more weight on terms closer to query terms in document D.

3.2.3 More Estimation Details
This section provides the final estimation details for our positional relevance model (Equation 6 and 11), i.e., how to estimate P (Q|D, i). We adapt the positional language model [19] to do that.
The key idea of PLM is to estimate a language model for each position of a document. Specifically, we let each word at each position of a document propagate the evidence of its occurrence to all other positions in the document so that positions closer to the word would get more share of the evidence than those far away. The PLM at each position can then be estimated based on all the propagated counts of all the words to the position as if all the words had appeared actually at the position with discounted counts. This new family of language models is intended to capture the content of the document at a position level, which is roughly like a "soft passage" centered at this position but can potentially cover all the words in the document with less weight on words far away from the position.
Formally, the PLM at position i of document D can be estimated as:

P (w|D, i) =

c (w, i)

(12)

w V c (w , i)

where c (w, i) is the total propagated count of term w at position i from the occurrences of w in all the positions. Following [19], we estimate c (w, i) using the Gaussian kernel function:

|D|
c (w, i) = c(w, j) exp
j=1

-(i - j)2 22

(13)

where i and j are absolute positions of the corresponding terms in the document, and |D| is the length of the document; c(w, j) is the real count of term w at position j. With the approximation method proposed in [19], the following estimation of P (w|D, i) is obtained:

P (w|D, i) =  22 ·



c (w, i)

|D|-i 

-

1-i 

(14)

582

where (·) is the cumulative normal distribution and the denominator is essentially the length of the "soft" passage centered at position i.
However, there is one issue with the above estimation: the length of "soft" passages around the boundaries of a document would be smaller than that in the middle of the document; as a result, boundary positions tend to unfairly receive more weights. This may not raise problems in PLMs for retrieval [19], but it is a more serious concern for PRM, where the relative weights of terms are more important. So we decide to use a fixed length for all "soft" passages in feedback documents to estimate their corresponding positional language models as follows:

P (w|D, i) = c(w, i)

(15)

22

This strategy has shown to be better than the original implementation in [19] for estimating PRM.
The distribution P (·|D, i) needs to be smoothed. Now that all "soft" passages have equal length, we use JelinekMercer smoothing method to smooth PLM, which is shown to work as well as the Dirichlet prior smoothing method and is relatively insensitive to the setting of  in our experiments.

P(w|D, i) = (1 - )P (w|D, i) + P (w|C)

(16)

where   [0, 1] is a smoothing parameter and p(w|C) is the collection language model. Now we can compute the positional query likelihood score P (Q|D, i) for position i.

m

P (Q|D, i) = P(qj |D, i)

(17)

j=1

Plugging Equation 17 into Equations 6 and 11, we would be able to compute the two estimation methods directly. Interestingly, if we set  = 1 or  = , Method 2 will degenerate to the general relevance model (see Equation 1).
The computation of positional query likelihood is the most time-consuming part in estimating PRM. Fortunately, there is no serious efficiency concern even with an unoptimized implementation. The reason is because we only need to traverse each position of a document twice: during the first pass, the positions of query terms are recorded; in the second, we compute a positional query likelihood for each position directly based on the position information of query terms collected in the first pass. Therefore, the efficiency is comparable to the estimation of the relevance model.
Finally, the estimated positional relevance model P (w|Q) will also be interpolated with the original query model Q using Equation 2 to improve performance with a similar parameter  to that used in the mixture-model feedback [32] and RM3 [1].

4. EXPERIMENTS
4.1 Experimental Setup
We used two standard TREC datasets in our study: Terabyte (i.e., the Gov2 collection) and ClueWeb09 Category B. They represent two very large web text collections in English. Queries were taken from the title field of the TREC topics. We used the Lemur toolkit (version 4.10) and Indri search engine (version 2.10) 1 to implement our algorithms. For both datasets, the preprocessing of documents
1http://www.lemurproject.org/

queries #qry(with qrel)
#documents mean(dl)

Terabyte05 Terabyte06

751-800

801-850

50

49

25, 205, 179

931

ClueWeb09 Cat. B 20001-21000 358 50, 220, 423 875

Table 1: Document set characteristic

and queries included stemming with the Porter stemmer and stopwords removing using a total of 418 stopwords from the standard InQuery stoplist. Table 1 shows some basic statistics about the datasets.
We evaluated seven methods. (1) The basic retrieval model is the KL-divergence retrieval model [15], and we chose the Dirichlet smoothing method [33] for smoothing document language models, where the smoothing parameter µ was set empirically to 1500. This method was labeled as "NoFB". (2) The baseline pseudo feedback method is the relevance model "RM3" described in Section 3.1 [1], which is one of the most effective and robust pseudo feedback methods under language modeling framework [18]. (3) Another baseline pseudo feedback method is a standard passage-based feedback model, labeled as "RM3-p", which estimates the RM3 relevance model based on the best matching passage of each feedback document [17]. (4) We have two variations of PRM, i.e., "PRM1" and "PRM2", which are based on the two estimation methods described in Section 3.2, respectively. (5) In addition, we also used PRM1 and PRM2 for passage feedback in a way as RM3-p does. Specifically, we first computed a PLM for each position of the document, and then we estimate a PRM based on a passage of size 2 centered at the position with the maximum positional query likelihood score (see Equation 17). These two runs are labeled as "PRM1-p" and "PRM2-p" respectively.
There are several parameters in these pseudo feedback algorithms. We fixed the number of feedback documents to 20 and the number of terms in feedback model to 30. Other parameters, including the feedback interpolation coefficient , the two additional parameters  and  in PRM, the passage size, and the passage smoothing parameter in RM3-p, were all tuned on Terabyte05 dataset.
We used Terabyte06 and ClueWeb09 for testing. The topranked 1000 documents for all runs were compared in terms of their mean average precisions (MAP) (for Terabyte06) or eMAP [8] (for ClueWeb09). In addition, other performance measures, such as Pr@10, Pr@30 and Pr@100 for Terabyte06 and eP@10, eP@30 and eP@100 for ClueWeb09, were also considered in our evaluation.
4.2 Feedback Effect
We first examine the overall retrieval precision of the pseudo feedback models for document-based feedback. The results are summarized in Table 2, where the best result for each row is highlighted. As we see, both PRM1 and PRM2 significantly outperform the basic KL-divergence retrieval model in terms of MAP. In addition, PRM1 and PRM2 are also significantly better than RM3 across data sets. For example, the relative improvements of PRM1 over NoFB are 9.0% on Terabyte06 and 13.5% on ClueWeb09 in terms of average precision, which are much larger than the corresponding improvements achieved by RM3 (only 2.8% and 5.9% respectively). RM3 improves Pr@10 over NoFB in neither dataset; however both PRM1 and PRM2 often improve Pr@10, though not significantly. Besides, comparing PRM1

583

Collection Terabyte06
ClueWeb09

Metric
MAP Pr@10 Pr@30 Pr@100
eMAP eP@10 eP@30 eP@100

NoFB
0.3047 0.5367 0.4653 0.3547
0.0713 0.2371 0.2433 0.2216

RM3
0.3131 0.5041 0.4660 0.3576
0.0755 0.2307 0.2486 0.2283

PRM1 0.3322+
0.5306 0.4884+ 0.3671+ 0.0809+ 0.2418+ 0.2536+ 0.2356+

PRM2 0.3319+ 0.5490+ 0.4871+ 0.3741+ 0.0786+ 0.2377+ 0.2525+ 0.2325+

Table 2: Comparison of different pseudo feedback models for document-based feedback. `*' and `+' mean the corresponding improvements over NoFB and RM3 are significant respectively.

Collection Terabyte06 ClueWeb09

RM3-p 0.3077 0.0781

PRM1 0.3322 0.0809

PRM2 0.3319 0.0786

PRM1-p 0.3331 0.0800

PRM2-p 0.3290 0.0798

Table 3: MAP/eMAP comparison of passage-based feedback methods. `*' means the corresponding improvement over RM3-p is significant.

and PRM2, we find that PRM1 is slightly more effective than PRM2.
We are also interested in evaluating if a heuristic passagebased feedback (i.e., RM3-p) can work as well as PRM, since both PRM1 and PRM2 essentially can be regarded as achieving a soft effect of passage feedback. Moreover, we can also use PRM1 and PRM2 for "hard" passage feedback in a way as RM3-p does, which leads to PRM1-p and PRM2-p respectively. So we further compare the average precision of PRM1, PRM2, PRM1-p, PRM2-p, and RM3-p in Table 3. From the table, it is clear that PRM1, PRM2, PRM1-p and PRM2-p all outperform RM3-p significantly in most cases, suggesting that our model does not only have sound statistical foundation but also works effectively. In addition, we also observe that RM3-p behaves quite differently in two datasets: it beats RM3 on ClueWeb09 but loses to RM3 on Terabyte06. However, all the four variations of PRM perform better than RM3 consistently. Finally, it is also interesting to see that PRM1 and PRM2 work similarly to PRM1-p and PRM2-p respectively, which may mean that PRM1 and PRM2 have already achieved successfully an effect of passage-based feedback by assigning weights to different positions, so it does not bring too much additional benefit to apply PRM to passages explicitly.
Next we examine the robustness to the parameter setting in PRM on the Terabyte06 collection.
4.3 Robustness Analysis
In PRM1 and PRM2, there is a parameter  inherited from the positional language model to control the propagation range, which would influence the effect of term position and term proximity. Specifically, if we increase  to infinity, the effect of term position and proximity will be disabled. However, if we decrease this parameter to a finite value, term position and proximity will play an important role in PRM. We fix other parameters to their default values as trained on Terabyte05 and focus on understanding how  affects the retrieval performance of PRM1 and PRM2. From Figure 2 (left), we can see that, as long as  is in the range of [100, 1000], both PRM1 and PRM2 outperform RM3 clearly. Indeed, by setting  around 200, we can often obtain the optimal performance for both PRM1 and PRM2. This result confirms the observation in previous work [19]. In addition,

comparing PRM1 and PRM2, PRM2 seems to be less sensitive to .
Next, the positional language model is smoothed using Jelinek-Mercer method to estimate PRM. The smoothing is controlled by a parameter . When  = 0, we are using the pure positional language model, while if  = 1, we completely ignore the position and proximity evidence so that every position will receive the same weight. Again, we fix other parameters and show in Figure 2 (right) how the average precision changes under different . The experiment results indicate that when  is set to around 0.1, both PRM1 and PRM2 achieve their optimal performance. However, PRM1 and PRM2 always outperform RM3 with  < 1. Comparing PRM1 and PRM2, we see again that PRM2 seems to be more robust.
Recall that we interpolate the feedback model with the original query model. The interpolation is controlled by a coefficient . When  = 0, we are only using the original query model (i.e., no feedback), while if  = 1.0, we completely ignore the original query model and use only the estimated feedback model. We fix other parameters and show in Figure 3 (left) how the average precision changes according to the value of . We can see that both PRM1 and PRM2 are clearly better than RM3 with different  values. And the optimal  for all the methods seems to be in a range around 0.5. Besides, it is also interesting to observe that the pure feedback model results ( = 1.0) of PRM1 and PRM2 are much better than that of RM3, suggesting that the positional relevance model can lead to a more accurate query model. Finally, comparing PRM1 and PRM2, the former seems to be slightly more effective.
We further compare the robustness of different methods w.r.t. the number of feedback documents. We change the number of feedback documents from 1 to 200. The MAP results are shown in Figure 3 (middle). We notice that PRM1 and PRM2 are more robust to the number of feedback documents as compared to RM3. It is also interesting to see there is almost no performance decrease of PRM1 and PRM2 even when we set the parameter to 200, suggesting that the proposed positional relevance model works better in tolerating noisy information. Moreover, with only 1 feedback document, PRM1 and PRM2 have already been able to outperform RM3, no matter how many feedback documents RM3 uses, which may indicate that our methods can identify good feedback terms more accurately by assigning position-dependent weights.
Additionally, we also compare the sensitivity of different methods to the number of expansion terms in Figure 3 (right). We vary the number of terms from 5 to 100, and observe that both PRM1 and PRM2 can achieve a very effective performance with only 10 expansion terms, while

584

0.34

0.34

0.335

0.335

0.33

0.33

0.325

0.325

MAP MAP

0.32

0.32

0.315

0.315

0.31

0.31

0.305 0.3 0

RM3 PRM1 PRM2

500

1000

1500

2000



0.305

RM3 PRM1

PRM2 0.3

0

0.2

0.4

0.6

0.8

1



Figure 2: Sensitivity to the propagation range  (left) and the smoothing parameter  (right) of PRM.

MAP MAP

0.34

0.33

0.32

0.31

0.3

0.29

0.28

RM3

0.27

PRM1 PRM2

NoFB 0.26

0

0.2

0.4

0.6

0.8

1

Feedback coefficient 

0.34

0.335

0.33

0.325

0.32

0.315

0.31

0.305

RM3 PRM1

PRM2 0.3

0

20

40

60

80

100

Number of feedback documents

MAP

0.34 0.335
0.33 0.325
0.32 0.315
0.31 0.305
0.3 0

RM3 PRM1 PRM2

20

40

60

80

100

Number of expansion terms

Figure 3: Sensitivity to the feedback interpolation coefficient  (left), the number of feedback documents (middle), and the number of expansion terms (right) of different pseudo feedback methods

PRM1 0.0 0.2 0.4 0.6 0.8
PRM2 0.0 0.2 0.4 0.6 0.8

0.0

0.2

0.4

0.6

0.8

RM3

0.0

0.2

0.4

0.6

0.8

RM3

Figure 4: MAP Plot of PRM1 (left) and PRM2 (right) as compared to RM3 on Terabyte06

RM3 needs 70 terms, but even so, its performance is still not as good as our methods with 10 terms. This would be another advantage of our methods since fewer expansion terms mean higher efficiency, which is very important for retrieval systems.
To further see the robustness of our methods on individual queries, we plot the MAP of PRM1 versus RM3 and PRM2 versus RM3 on Terabyte06 in Figure 4. It is interesting that the proposed methods, particularly PRM2, are quite robust; they improve most of the queries clearly with only a small number of queries decreased slightly.

5. CONCLUSIONS
We proposed a novel positional relevance model (PRM) for pseudo-relevance feedback. The PRM exploits term position and proximity evidence to assign more weights to words closer to query words based on the intuition that words closer to query words are more likely to be consistent with

the query topic. Specifically, PRM generalizes the relevance model to aggregate the associations between a word and query words at the position-level in a probabilistic way. We also developed two methods to estimate the PRM based on different generative models.
Experiment results on two large web data sets show that the proposed PRM is quite effective and robust and performs significantly better than the state of the art relevance model in both document-based feedback and passage-based feedback. Compared to the relevance model, the proposed models are also less sensitive to the setting of various parameters, such as feedback coefficient, number of feedback documents, and number of expansion terms. Comparing the two estimation methods of PRM, the first method (PRM1) appears to be more effective, while the second (PRM2) tends to be more robust. Both methods achieve its optimal retrieval performance when setting the  value in a range around 200 and  to around 0.1.
There are many interesting future research directions to explore. One of the most interesting directions is to further study whether setting a term-specific and/or query-specific  can further improve performance. Another interesting direction is to study how to optimize  automatically based on the layout of web pages. Improving the estimate of other components in PRM (e.g., the probability of choosing a position in a document) would also be interesting.
6. ACKNOWLEDGMENTS
We thank the anonymous reviewers for their useful comments. We also thank Wan Chen for helping improve the English in this paper. This material is based upon work supported by the National Science Foundation under Grant

585

Numbers IIS-0347933, IIS-0713581, IIS-0713571, and CNS0834709.
7. REFERENCES
[1] Nasreen Abdul-Jaleel, James Allan, W. Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Donald Metzler, Mark D. Smucker, Trevor Strohman, Howard Turtle, and Courtney Wade. Umass at trec 2004: Novelty and hard. In TREC '04, 2004.
[2] James Allan. Relevance feedback with too much data. In SIGIR '95, pages 337­343, 1995.
[3] Chris Buckley, Gerard Salton, James Allan, and Amit Singhal. Automatic query expansion using smart: Trec 3. In TREC '94, pages 69­80, 1994.
[4] Georg Buscher, Andreas Dengel, and Ludger van Elst. Query expansion using gaze-based feedback on the subdocument level. In SIGIR '08, pages 387­394, 2008.
[5] Stefan Buttcher and Charles L. A. Clarke. Efficiency vs. effectiveness in terabyte-scale information retrieval. In TREC '05, 2005.
[6] Stefan Buttcher, Charles L. A. Clarke, and Brad Lushman. Term proximity scoring for ad-hoc retrieval on very large text collections. In SIGIR '06, pages 621­622, 2006.
[7] Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen Robertson. Selecting good expansion terms for pseudo-relevance feedback. In SIGIR, pages 243­250, 2008.
[8] Ben Carterette, James Allan, and Ramesh Sitaraman. Minimal test collections for retrieval evaluation. In SIGIR '06, pages 268­275, 2006.
[9] Charles L. A. Clarke, Gordon V. Cormack, and Forbes J. Burkowski. Shortest substring ranking (multitext experiments for trec-4). In TREC '95, pages 295­304, 1995.
[10] Ronan Cummins and Colm O'Riordan. Learning in a pairwise term-term proximity framework for information retrieval. In SIGIR '09, pages 251­258, 2009.
[11] David Hawking and Paul B. Thistlewaite. Proximity operators - so near and yet so far. In TREC '95, pages 500­236, 1995.
[12] Marcin Kaszkiel and Justin Zobel. Effective ranking with arbitrary passages. Journal of the American Society for Information Science and Technology, 52(4):344­364, 2001.
[13] E. Michael Keen. The use of term position devices in ranked output experiments. The Journal of Documentation, 47(1):1­22, 1991.
[14] E. Michael Keen. Some aspects of proximity searching in text retrieval systems. Journal of Information Science, 18(2):89­98, 1992.
[15] John D. Lafferty and Chengxiang Zhai. Document language models, query models, and risk minimization for information retrieval. In SIGIR '01, pages 111­119, 2001.
[16] Victor Lavrenko and W. Bruce Croft. Relevance-based language models. In SIGIR '01, pages 120­127, 2001.
[17] Xiaoyong Liu and W. Bruce Croft. Passage retrieval based on language models. In CIKM '02, pages 375­382, 2002.

[18] Yuanhua Lv and ChengXiang Zhai. A comparative study of methods for estimating query language models with pseudo feedback. In CIKM '09, pages 1895­1898, 2009.
[19] Yuanhua Lv and ChengXiang Zhai. Positional language models for information retrieval. In SIGIR '09, pages 299­306, 2009.
[20] Donald Metzler and W. Bruce Croft. A markov random field model for term dependencies. In SIGIR '05, pages 472­479, 2005.
[21] Donald Metzler and W. Bruce Croft. Latent concept expansion using markov random fields. In SIGIR '07, pages 311­318, 2007.
[22] Christof Monz. Minimal span weighting retrieval for question answering. In Rob Gaizauskas, Mark Greenwood, and Mark Hepple, editors, SIGIR Workshop on Information Retrieval for Question Answering, pages 23­30, 2004.
[23] Yves Rasolofo and Jacques Savoy. Term proximity scoring for keyword-based retrieval systems. In ECIR '03, pages 207­218, 2003.
[24] Stephen E. Robertson and Karen Sparck Jones. Relevance weighting of search terms. Journal of the American Society of Information Science, 27(3):129­146, 1976.
[25] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. Okapi at trec-3. In TREC '94, pages 109­126, 1994.
[26] J. J. Rocchio. Relevance feedback in information retrieval. In In The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313­323. Prentice-Hall Inc., 1971.
[27] Gerard Salton and Chris Buckley. Improving retrieval performance by relevance feedback. Journal of the American Society of Information Science, 41(4):288­297, 1990.
[28] Tao Tao and ChengXiang Zhai. An exploration of proximity measures in information retrieval. In SIGIR '07, pages 295­302, 2007.
[29] Olga Vechtomova and Ying Wang. A study of the effect of term proximity on query expansion. Journal of Information Science, 32(4):324­333, August 2006.
[30] Jinxi Xu and W. Bruce Croft. Query expansion using local and global document analysis. In SIGIR '96, pages 4­11, 1996.
[31] Shipeng Yu, Deng Cai, Ji-Rong Wen, and Wei-Ying Ma. Improving pseudo-relevance feedback in web information retrieval using web page segmentation. In WWW '03, pages 11­18, 2003.
[32] ChengXiang Zhai and John D. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In CIKM '01, pages 403­410, 2001.
[33] ChengXiang Zhai and John D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In SIGIR '01, pages 334­342, 2001.
[34] Jinglei Zhao and Yeogirl Yun. A proximity language model for information retrieval. In SIGIR '09, pages 291­298, 2009.

586

Human Performance and Retrieval Precision Revisited

Mark D. Smucker
Department of Management Sciences University of Waterloo
msmucker@uwaterloo.ca

Chandra Prakash Jethani
David R. Cheriton School of Computer Science University of Waterloo
cpjethan@cs.uwaterloo.ca

ABSTRACT
Several studies have found that the Cranfield approach to evaluation can report significant performance differences between retrieval systems for which little to no performance difference is found for humans completing tasks with these systems. We revisit the relationship between precision and performance by measuring human performance on tightly controlled search tasks and with user interfaces offering limited interaction. We find that human performance and retrieval precision are strongly related. We also find that users change their relevance judging behavior based on the precision of the results. This change in behavior coupled with the well-known lack of perfect inter-assessor agreement can reduce the measured performance gains predicted by increased precision.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]
General Terms: Experimentation, Human Factors, Performance
Keywords: Precision, user studies, human performance, Cranfield, evaluation metrics, interaction
1. INTRODUCTION
The Cranfield approach to information retrieval (IR) evaluation measures the ranking quality of a retrieval system given a test collection of documents, search topics, and relevance judgments [4]. Popular evaluation metrics used as part of the Cranfield-style of evaluation are typically some measure reflecting the precision or recall of the ranking produced by the retrieval system.
In the past decade, various studies have offered conflicting reports on the value of the Cranfield approach and its associated metrics for users of interactive retrieval systems. Most notable is perhaps the work of Hersh et al. [7] who asked "Do improvements in system performance demonstrated by batch evaluations confer the same benefit for real users?" Hersh et al. found no statistically significant gains for users
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

of systems with better batch evaluation performance as measured by mean average precision (MAP). In a followup work, Turpin and Hersh [13] reached a similar conclusion. Subsequent studies have offered conflicting reports on the value of batch evaluation for predicting human performance or satisfaction [1, 2, 11, 12].
In all of these studies, human performance on some task has been compared to measures of precision. From the beginning, it is clear that the attention to recall and precision has met with resistance. Responding to criticism, Cleverdon et al. [5] wrote about recall and precision saying "The unarguable fact, however, is that they are fundamental requirements of the users, and it is quite unrealistic to try to measure how effectively a system or a subsystem is operating without bringing in recall and precision." Voorhees [14] stresses that performing well on the abstract task represented by a Cranfield-style evaluation is "assumed to be a necessary but not sufficient prerequisite for performing well on real search tasks."
In this paper, we revisit the connection between retrieval precision and human performance by first noting that most Cranfield-style retrieval metrics actually make little to no attempt to include a model of a user or user interface (UI). At most, the precision based metrics have a model of a user as someone who reads the ranked documents, in order, one after the other. As this hypothetical user reads documents, the user takes the time to make a relevance judgment for each document. This user takes a constant amount of time to evaluate the relevance of each document regardless of the document's length, style, or the search topic. Given what most evaluation metrics do not attempt to do, we do not find it too surprising that they lack strong predictive power for human performance with interactive retrieval systems.
If we are to create evaluation metrics that are predictive of human performance, we must first better understand the relationship between retrieval precision and human performance. Our approach is to first try to understand the simplest of systems. We can then gradually increase the complexity of the systems studied as our understanding grows.
As such, we conducted a two phase user study that carefully controlled the precision of ranked lists and also the level of user interaction allowed. For both phases, we investigated human performance at two different levels of precision -- low and high. In the first phase of the study, the users alternated between judging the relevance of full documents and summaries of documents. This interface forced users to proceed down the ranked list of documents one at a time. This first phase of the experiment models the common as-

595

sumptions of Cranfield-style precision-based metrics minus the assumption that documents take a uniform amount of time to judge. We alternated between summaries and documents to obtain performance data on both while avoiding a learning effect that could be caused by doing either all documents or all summaries first for a given topic. Our interest in summaries comes from their use in modern search interfaces and their use in phase 2.
In the second phase of the study, the interface showed users the ranked list of documents in a fashion similar to modern web search engines that show 10 query-biased document summaries per page. Clicking on a summary allowed the user to save the document if the user thought it was relevant to the given search topic. While similar to web search interfaces, this interface did not provide any means for the user to reformulate queries, or even enter a query. Here the user had to search the provided results list. While we still limited interaction in phase 2, we hypothesized that any difference in human performance at the two levels of precision will be reduced compared to phase 1. In other words, precision as a metric should not predict human performance in phase 2 as well as in phase 1. Phase 2 offered considerably more freedom for behavior outside of what traditional Cranfield-style metrics attempt to predict.
From this experiment, we found that:
· For both interfaces, human performance was significantly greater with the higher level of retrieval precision.
· The difference in human performance between the two levels of retrieval precision was greater in the phase 1 interface that more closely matched Cranfield-style evaluation assumptions.
· Users change their behavior depending on the precision of the results list:
­ When judging high precision lists, users were less likely to judge NIST relevant documents as relevant compared to when judging the lower precision lists. This effect was stronger for phase 2 than for phase 1.
­ In phase 2, when searching low precision lists, users take more care to avoid clicking on summaries of NIST non-relevant documents as compared to when searching the higher precision lists.
We next describe the experiment details.
2. METHODS AND MATERIALS
We created a two phase, within-subjects study with each phase utilizing a different user interface. The first phase tightly controlled the behavior of the study participants while the second phase offered more freedom. Participants in both phases were the same. For each phase, the participants performed four tasks. Two of the tasks were with good retrieval results and two were with bad retrieval results. We next describe the experiment in more detail.
2.1 Collection and Search Topics
We selected 8 topics from the 2005 TREC Robust track. Table 1 shows the topics and their titles. Our criteria for the topics were that they be something that was not too dated

Number 310 336 362 367 383 426 427 436

Topic Title Radio Waves and Brain Cancer Black Bear Attacks Human Smuggling Piracy Mental Illness Drugs Law Enforcement, Dogs UV Damage, Eyes Railway Accidents

Relevant 65 42
175 95
137 177
58 356

Table 1: Topics used in the study and the number of NIST relevant documents for each topic.

and that would be of possible interest to the the study participants. For each topic, we took the topic's description and narrative and rewrote them as a single description of what the participants should consider to be a relevant document.
The 2005 TREC Robust track used the AQUAINT document collection. This collection contains 1,033,461 newswire documents from the New York Times, Associated Press, and Xinhua News Agency.
For the 2005 TREC Robust track, NIST assessors judged documents as non-relevant, relevant, and highly relevant. To simplify analysis, we treat relevant and highly relevant documents as simply relevant documents.
2.2 Construction of Results Lists
The primary goal of this study is to measure how human performance changes with different levels of precision. To achieve this goal, we had to carefully control the precision of the results.
In both phases of the study, participants view an ordered list of documents. To understand how different levels of precision affect human behavior, it was important that the precision of this list be uniform. In other words, we decided to create an artificial, ranked list that controlled precision so that precision at rank N would remain approximately constant. This is in contrast to operational retrieval systems where it is well-known that, on average, precision decreases with increasing rank. Understanding how humans respond to ranked lists with decreasing levels of precision is a more difficult task that we leave for future work.
Even though our ranked lists are artificial, we carefully constructed the lists with two additional goals in mind. First, the ranking needed to seem plausible to the participants, i.e. the ranking should look like one produced by a real retrieval system. Second, choosing between relevant and non-relevant documents should remain approximately as difficult as it is for users of real systems. We believe that we achieved both of these goals as we next describe.
2.2.1 Uniform Precision
We used the following procedure for each topic to create ranked results of near uniform precision at rank N , where precision is one of the possible precisions at 10 (P10) greater than zero, i.e. 0.1, 0.2, . . . , 1.0.
We first created a ranking of documents by performing a reciprocal rank fusion [6] on all of the runs submitted to the 2005 TREC Robust track minus the 4 lowest performing runs. From this ranking, we separated the known relevant documents from the non-relevant documents while maintaining the order of each set of documents. We used the NIST

596

provided relevance judgments (qrels). We treated unjudged documents as non-relevant. Unjudged documents made up a small fraction of the seen documents in both phases (2% in phase 1 and 6% in phase 2).
With a ranked list of relevant documents, and a ranked list of non-relevant documents in hand, we then recombined them as follows to achieve near-uniform precision. For a given P10 value, from the top of the ranked lists we remove the number relevant and non-relevant documents needed to obtain the desired P10. For example, if P10 is 0.3, we remove the top 3 ranked relevant documents and the top 7 non-relevant documents. We then randomly permute these 10 documents and append them to a final ranked list. We repeat this process 10 documents at time until we have a ranked list of 1000 documents.
2.3 Good and Bad Levels of Precision
Given our limited resources, we decided to only investigate two levels of precision, and thus we needed to create, for each search topic, 2 lists of ranked documents. We refer to the lists with higher precision as having a quality of good and the other as having a quality of bad. While our good and bad labels refer to specific precision levels, Al-Maskari et al.'s good and bad labels referred to composite retrieval systems [1].
In choosing the two levels of precision, we wanted to have the levels be different enough that if precision does affect human performance, we could be expected to see the difference given a reasonable number of participants. At the same time, the levels of precision had to be within of the range of plausibility for real retrieval systems. We believe we achieved these goals as follows.
To determine what the good and bad precision levels should be, we took the automatic, title-only runs from the 2005 TREC Robust track and measured their precision at 10 (P10). Over all 49 topics, the best system had an average P10 of 0.592 and the bottom of the "okay" runs was 0.304. Three runs were considered non-okay with P10's of 0.120 and lower; these results are typically buggy or the result of operational mistakes. We decided that a P10 of 0.6 was to be the good quality and that 0.3 was to be the bad quality.
Because our constructed ranked lists have a near uniform precision at N , for mean average precision (MAP) and Rprecision (precision at the number of known relevant documents, R), our good results are approximately 0.6 for both of these metrics (0.3 for bad results). P10, P20, and so forth are all exactly 0.6 and 0.3 up till the number of relevant documents is exhausted.
While a precision of 0.6 is a 100% relative improvement over 0.3, a uniform precision of 0.3 produces a mean average precision of approximately 0.3. The best performing automatic title-only run in the 2005 TREC Robust track had a MAP of 0.332. A uniform precision of 0.3 is by no means a ranked list of such low quality that it would prevent participants from finding many relevant documents.
These precision levels are similar to, but also different from previous investigations. For example, our bad level of precision at 0.3 is similar to the systems Hersh et al. investigated [7] while our good level of precision at 0.6 is more similar to the work of Turpin and Scholer [12]. Hersh et al.'s two systems had MAPs of 0.2753 and 0.3239 on the 6 topics from the TREC 8 interactive track used in their study [7]. Turpin and Scholer [12] used controlled result

lists with APs of 0.55 to 0.95. In contrast to Hersh et al. and Turpin and Scholer, our precision levels are such that one has a majority of non-relevant documents in the ranked list and the other has a majority of relevant documents.
2.4 Performance Measures
The aspect of human performance that we focus on in this paper is the number of relevant documents that a human can find using a retrieval system in a given amount of time. This is similar to but not the same as instance recall, which was used by Hersh et al. [7]. Instance recall requires that the instances found be novel. The example that Hersh et al. give is a search for Hubble telescope discoveries. If two documents report the same discovery, both count towards number of relevant documents found but only the instance of the specific discovery counts towards instance recall.
We are not measuring document recall. As Moffat and Zobel [9] have pointed out, there are numerous issues with attempts to measure recall since it requires the correct estimation of the number of relevant documents in the collection. We are in effect measuring cumulated gain [8] at a time t rather than a rank N .
To measure the number of relevant documents found, we need to determine when we consider a document relevant or not. Cleverdon et al. list four choices [5], of which the first three appear relevant to our study. Relevance can be determined:
1. "By the questioner."
2. "By the consensus of opinion of a group of people."
3. "By an individual, not the questioner."
In this paper we measure relevance both by items 1 and 3. In future work, we plan to look and see if item 2 can be used for our study by using the judgments of our participants to create consensus relevance judgments.
Item 1 corresponds to equating the NIST relevance judgments (qrels) to be the relevance judgments of the questioner. Measuring performance by the qrels means that documents judged relevant by a participant only count toward the number found if the NIST assessor said that the documents are relevant. Item 3 corresponds to accepting on a per participant basis the participant's judgments as correct.
With our experimental design, it is important to report measures based on item 3 as well as item 1. We report using item 1 since it has been common practice, but if a participant decides to behave in a random fashion or decides to judge everything relevant, the participant will show gains when documents are counted based only on NIST relevance. The same random participant will not show gains when the participant's judgments are accepted as correct (item 3).
2.5 Study Design
As mentioned above, we conducted a two phase, withinsubjects study. Each phase of the study utilized a different user interface, but the underlying result lists (Section 2.2) were the same for both phases.
Phase 1 started with participants filling out consent forms and answering a basic demographic and experience questionnaire. For both phases, participants then completed a 10-15 minute tutorial describing the phase's tasks and giving participants practice with the user interfaces.

597

Figure 1: The left screenshot shows the user interface (UI) for phase 1 with a full document. Participants in phase 1 also judged document summaries in the same way. The right screenshot shows the phase 2 UI with query-biased document summaries shown. Clicking on a summary took the user to page with the full document. This page, similar to the phase 1 document judging UI on the left, allowed the user to save the document as relevant, but did not require a relevance judgment be made.

In each phase, a participant completed 4 search tasks. Each search task lasted 10 minutes. Two of the four search tasks had good result lists and two had bad result lists. Each search task corresponded to 1 of the 8 TREC search topics.
Before and after completing a search task, participants answered a short questionnaire on the topic. We used the same 4 pre- and post-task questions as Bailey et al. [3]. On the post-task questionnaire, we also provided the participants with a free-form textbox to allow them to alert us to any issues they encountered during the task. One possible effect of the pre-task questionnaire is that it may have forced the participant to study the search topic and its description of what is and is not relevant before beginning the actual search task.
We offered no incentives to participants to encourage them to work fast or with high accuracy. From a participant perspective there was no end to each task except for an eventual notice that 10 minutes had past and the task was over.
We did adopt language similar to that used by Smith and Kantor [11] and instructed the participants in phase 1 to "try to find as many relevant documents as possible in the 10 minutes while still making as few mistakes in judging the documents' relevance as possible." In phase 2, we asked participants to "try to find and save as many relevant documents as possible in the 10 minutes while saving as few non-relevant documents as possible."
In phase 1, we received several questions from participants about seeing duplicate documents and summaries. In phase 2, we explicitly instructed participants to judge duplicate documents the same and that the duplicate documents were there by design.
2.5.1 User Interfaces
In this section, we describe the user interfaces for phases 1 and 2 and the associated tasks for each phase. Figure 1 shows both interfaces.

The phase 1 interface asks users to make binary relevance judgments. The interface shows the user one document or one document summary at a time. To see the next document or summary, the user must make a relevance judgment. The interface alternates between summaries and full documents. The interface instructs users to judge summaries based on what they think the full document's relevance to be. Both summaries and documents have the search topic's title terms highlighted. The search topic's title and description remained visible for the entire task.
For the document summary, we displayed the document's title, if known, as well as a query-biased snippet. To construct the query-biased snippets for each document, we used the topic's title as a bag-of-words query and then retrieved the top two scoring sentences from the document. Snippets had a maximum length of 50 words.
All participants in phase 1 worked down their given ranked list. For example, a participant would start with judging the document or summary for the document at rank 1 of the assigned results list. If given a full document to judge, the participant would next judge a document summary of the document at rank 2 of the result list. We hid the navigation bar (back button, etc.) for phase 1. For both phase 1 and 2 we disabled the ability to right click on the web page.
We modeled the phase 2 interface after the standard style of modern web search engines. We presented the user with 10 document summaries per page. Clicking on a summary allowed the user to view the full document. The user could then decide to save a document if the user considered the document to be relevant. The user could undo their save operation if needed. Both saving and unsaving were implemented with client side scripts and dynamic HTML, which did not require a page reload. To go back to the list of search results, the user could use the browser's back button. At the bottom of the page of search results were links to take the user to the next 10 results or the previous 10 results. As

598

with the phase 1 UI, the search topic's title and description remained visible on every page for the entire task.
Note that the phase 2 interface lacks a query search box. There was no way for users to reformulate the query and produce a new ranked list. Again, like phase 1, the idea was to minimize the flexibility in the interface to control the experiment and determine how precision affects human performance.
In phase 2, when viewing the list of search results, the participants viewed 10 query-biased document summaries per page. With the uniform precision result lists, each page has the same number of relevant documents ­ 6 for good and 3 for bad results.
2.5.2 Balanced Design, Averages, and Significance
We built Graeco-Latin squares to balance topics and precision quality across the four tasks of each phase. A participant worked on 4 of the 8 topics in phase 1 and the remaining 4 topics in phase 2. A balanced block including whether participants started with a summary or a document in phase 1 required 16 participants. For each block of 16, we randomly permuted the topics and assigned the first four to one phase and the last four to the other phase. Half of the participants used one half of the topics for phase 1 while the other participants used the other half of the topics for phase 1. We randomly permuted the rows and columns of the squares and then randomly assigned the treatments to the identifiers used for building the squares.
In both phases, we have balanced topics across users and tasks. The goal of the balanced design is to eliminate the effect of topic on the results. We have utilized a withinsubjects experiment so that we can block the data by user. This design allows us to measure statistical significance with matched pairs of data at the user level to compare the good and bad precision levels.
For all measures, we produce an average for each participant's good tasks and for their bad tasks. We take the mean of these averages across all participants to obtain overall averages for the good and bad levels of precision. In the few cases where we could not compute a measure for a participant, we excluded the participant from the average. In addition to the mean, we report the standard deviation of the mean (SDOM, also known as the standard error, or standard error of the mean). For example, an average of 7.4 with a SDOM of 0.6 is reported as 7.4 ± 0.6.
We measure statistical significance with the paired Student's t-test in all cases. Pairing is by participant. In the case of the measures for which a priori we predict that increased precision will result in increased human performance (Table 2), we report the one-sided t-test's p-value as well as the two-sided p-value. In all other cases (Table 3) we report only the two-sided p-value.
2.6 Cleaning of Data
At the initial close of the experiment, we had filled three blocks of 16 participants and had started to fill a fourth block of 16. From this data, we eliminated cases where we had technical issues affecting data collection, or where it was apparent that the participant did not attempt to accurately complete the task, or where we had other concerns about the data collection that would affect the integrity of the experiment. We then recruited additional participants to replace the deleted slots. We successfully filled 3 blocks of

16. Our results are based on this fully balanced set of 48 participants.
2.7 Participants
After having our study approved by our university's office of research ethics, we recruited participants via posters and an email announcement to a university-wide graduate student email list. While we did not keep records on how participants learned of the study, the posters appeared to attract a diversified set of participants.
We paid participants $10 for each phase of the study. We did not require participants to do both phases. For the majority of participants, phase 1 was conducted several weeks prior to phase 2. Other participants had several hours or days between phase 1 and phase 2. Only a few participants did phase 1 and phase 2 back to back. Some participants found phase 1 too boring and tedious and dropped out of the study after completing phase 1. We replaced participants who did not return for phase 2 with new participants.
Our results are based on the 48 participants that remained after data cleaning (Section 2.6).
All participants had used web search engines before. The majority reported using search engines several times a day, enjoying using search engines, and felt that they were experts who did not have trouble finding information on the Internet. Three participants reported receiving training in searching or information retrieval (none of these participants were known to us as IR students or researchers).
Hersh et al. [7] note that there have been unpublished observations that performance on recall tasks is related to reading speed. As such, we also asked participants about their self-perceived reading speed. 21 of the participants felt they were fast readers, 23 were neutral, and only 4 felt they were not.
The participants consisted of 23 females and 25 males. All but 2 of the participants were students. Of the students, 12 were undergraduates and 34 were graduate students. For the students, 34 (27 grad, 7 ugrad) were science, technology, engineering, or mathematics students. The other 12 students identified themselves as "arts" or "other". The median age was 24.5, minimum was 18, and maximum was 56.
All participants considered themselves fluent speakers of English, but for many participants we observed that they did not seem to be native speakers of English, but we did not collect survey data on native language.
3. RESULTS AND DISCUSSION
Table 2 shows the main results of the study. For both phase 1 and 2, the good level of precision produced statistically significant improvements in human performance over the bad level of precision. We set good precision to 0.6 and bad precision to 0.3 (Section 2.2). Figure 2 graphically shows the results on a per-participant and per-topic basis. Table 3 shows additional measures of interest.
As explained in Section 2.4, we use two ways to count the number of relevant documents found by users in 10 minutes. The first way takes a user's judgment as truth. When a user judges a document as relevant, we count that as finding a relevant document regardless of the NIST assessor's judgment. The second way only counts a relevant document as found by the user when the user judges the document to be relevant and the NIST assessor agrees that the document is relevant.

599

Phase 1 1 1 1 2 2

Average Measure Summaries Judged Relevant by User Summaries Judged Relevant by User and NIST Documents Judged Relevant by User Documents Judged Relevant by User and NIST Documents Saved as Relevant by User Documents Saved as Relevant by User and NIST

Precision Quality

Bad

Good

3.6 ± 0.4 5.6 ± 0.7

2.3 ± 0.3 4.6 ± 0.6

6.0 ± 0.6 8.8 ± 1.0

3.9 ± 0.4 7.4 ± 0.8

10.8 ± 0.9 12.6 ± 1.1

7.4 ± 0.6 10.9 ± 1.0

Diff. 2.1 2.3 2.8 3.4 1.9 3.6

Rel. Gain 59% 101% 46% 86% 17% 48%

1-sided p-value
0.004 < 0.001 < 0.001 < 0.001
0.042 < 0.001

2-sided p-value
0.008 0.001 0.001 < 0.001 0.083 0.001

Table 2: Main Results. Section 2.5.2 explains the calculation of averages and statistical significance.

Phase 1 - Study Participants

Phase 2 - Study Participants

40

40

30

30

User Saved User Saved & NIST Rel.

20

Good Quality - Average Number of Documents

20

Good Quality - Average Number of Documents

10

10

0

User Judged Relevant User Judged Rel. & NIST Rel.

0

10

20

30

40

Bad Quality - Average Number of Documents

Phase 1 - Topics

User Judged Relevant User Judged Relevant & NIST Relevant

30

0

0

10

20

30

40

Bad Quality - Average Number of Documents

Phase 2 - Topics

User Saved User Saved & NIST Relevant

30

25

20

15

Good Quality - Average Number of Documents

25

20

15

Good Quality - Average Number of Documents

10

10

5

5

0

0

0

5

10

15

20

25

30

Bad Quality - Average Number of Documents

0

5

10

15

20

25

30

Bad Quality - Average Number of Documents

Figure 2: Average number of documents judged relevant (phase 1) and saved as relevant (phase 2) for good vs. bad precision levels. The top two plots show per-user averages. For these plots, points above the dashed line are study participants (users) who had better performance with the good level of precision. The bottom two plots show per-topic averages. For these two plots, each point is a topic and is the average of 12 study participants (users) for bad and 12 other participants for good.

600

Phase 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2

Average Measure
Summaries Judged by User Documents Judged by User
Time in Seconds to Judge Summary Time in Seconds to Judge Document
User Summary Accuracy (Agreement with NIST) User Document Accuracy (Agreement with NIST) P( user judges rel | NIST non-rel, summary ) P( user judges rel | NIST rel, summary ) P( user judges rel | NIST non-rel, document ) P( user judges rel | NIST rel, document ) P( user clicks sum | NIST non-rel, first 10 results ) P( user clicks sum | NIST rel, first 10 results ) P( user saves doc | NIST non-rel, first 10 results ) P( user saves doc | NIST rel, first 10 results ) Fraction of Documents Viewed that are NIST Non-Relevant Fraction of Documents Viewed that are NIST Relevant
Fraction of Viewed NIST Non-Rel. Documents that are Saved Fraction of Viewed NIST Relevant Documents that are Saved
Time in Seconds Viewing Summaries before Action Time in Seconds Viewing Document before Action
Maximum Rank Viewed Number of Documents Viewed

Precision Quality

Bad

Good

16.6 ± 2 17.6 ± 2

16.4 ± 2 17.4 ± 2

15.6 ± 1 15.4 ± 1

49 ± 6

49 ± 7

0.72 ± 0.02 0.62 ± 0.02

0.76 ± 0.03 0.75 ± 0.02

0.17 ± 0.03 0.18 ± 0.03

0.47 ± 0.05 0.47 ± 0.03

0.24 ± 0.03 0.23 ± 0.03

0.78 ± 0.03 0.73 ± 0.03

0.58 ± 0.04 0.66 ± 0.04

0.84 ± 0.03 0.80 ± 0.03

0.20 ± 0.03 0.23 ± 0.03

0.70 ± 0.04 0.63 ± 0.04

0.56 ± 0.02 0.32 ± 0.01

0.44 ± 0.02 0.68 ± 0.01

0.29 ± 0.03 0.26 ± 0.03

0.77 ± 0.03 0.70 ± 0.03

10.1 ± 1 8.0 ± 0.8

27 ± 2

26 ± 2

50 ± 6

35 ± 3

20.5 ± 1 21.5 ± 2

Rel. Change
6% 6% -1% -1% -14% -1% 7% 0% -3% -6% 14% -4% 11% -10% -43% 54% -11% -9% -21% -4% -29% 5%

t-test p-value 0.463 0.461 0.906 0.950 0.007 0.913 0.779 0.997 0.899 0.295 0.019 0.307 0.520 0.117 < 0.001 < 0.001 0.390 0.080 0.007 0.730 0.009 0.412

Table 3: Additional Measures of Interest. The t-test is paired by user and is two-sided.

In phase 1, when user judgments are counted as truth, the relative gain of good over bad is 59% for summaries and 46% for documents. When we only count documents judged relevant by both the user and NIST, the relative gain is 101% for summaries and 86% for documents. More documents are judged as relevant than summaries, for users are less likely to judge summaries as relevant. It seems that if in doubt, users make a decision of non-relevant.
In phase 2, the relative gains are less than in phase 1; the gap in human performance between the good and bad lists has shrunk. The relative gain for documents saved is 17% and it is 48% for documents saved that are NIST relevant. The one-sided t-test is appropriate for all the results in Table 2, and for these results, all are statistically significant (p < 0.05). Figure 2 shows that for the majority of topics, users performed better with the higher precision results. On a per-topic basis, we obtain statistically significant results for a one-sided, paired t-test with only 8 topics (p < 0.05).
While we measured statistically significant increases in human performance, the relative gain from bad to good varies considerably. An obvious question to ask is why are so many of the human performance improvements so much less than the 100% difference in precision?
There are several possibilities that need to be investigated to understand these differences. One possibility is that users may take more time to judge a document relevant than they do to judge a document non-relevant. With a low precision list of documents, the user will encounter many more non-relevant documents than when the user views a higher precision list. The difference in judging times would allow for more documents to be judged in the low precision list than in the high precision list in a given amount of time.
We found no support for users taking different amounts of time for the judging of summaries or documents in phase 1.

Nor did we find any evidence that users were able to judge more documents when judging the lower precision list in phase 1. For phase 2, users did spend less time viewing summaries when the list quality was good ­ 8.0 seconds vs. 10.1 seconds. This difference in time did not result in a statistically significant difference in the number of documents viewed (20.5 vs. 21.5 documents).
Another possibility is that users may change how they judge relevance if they detect that the results are good or bad on average. Scholer and Turpin have shown users can vary significantly in their relevance criteria [10].
In effect, the user could learn a prior probability of relevance for the results list either consciously or subconsciously. This added knowledge, or something else related to the precision of the list, could lead the user to change what they consider to be relevant to the search topic. The user might change their relevance criteria by either tightening or loosening it. Changing the relevance criteria does not change the rate of judging but it does change the rate at which documents are judged to be relevant by the user.
We do see some evidence that the users change their relevance criteria given the precision of the results list. Table 3 shows the measured probabilities that a user would judge a document as relevant. For documents judged by NIST to be non-relevant documents, we see no evidence of users changing their relevance criteria given the precision of the results list. In both phases, users save as relevant 20 to 24 percent of documents judged by the NIST assessor to be non-relevant.
On the other hand, for documents judged relevant by NIST, it appears that users change their relevance criteria. While there is only suggestive evidence for phase 1, for phase 2 the evidence is stronger. In phase 1, when precision is bad, the users have a probability of 0.78 of judging NIST

601

relevant documents as relevant, but when precision is good, the probability drops to 0.73 (p = 0.295). For phase 2, we have two measures of how users judge NIST relevant documents. The first measure estimates this probability based on the user behavior for the first page of 10 results under the assumption that the users view all 10 of these documents. In this case, for bad the probability is 0.70, and for good the probability is 0.63 (p = 0.117). The second measure is the fraction of NIST relevant documents viewed, i.e. the user clicks on a summary and views the full document, that are saved as relevant. For bad this fraction is 0.77, and for good it is 0.70 (p = 0.080). Taken together, it appears that when users are searching a low precision list, they are more likely to save a NIST relevant document as relevant than when they are viewing a high precision list.
How does this affect the measured human performance? The reduction in predicted performance for documents saved as relevant comes about from users saving NIST non-relevant documents as relevant and also from the difference in the probability of saving relevant documents. When users view NIST non-relevant documents in both the bad and good results, they will save them as relevant about 20 to 24 percent of the time, but when viewing a low precision list, the user sees many more NIST non-relevant documents. For relevant documents, the story is different. The user viewing the low precision list is more likely to save the NIST relevant documents as relevant than when the user views the high precision list.
In phase 2, there is another way that users change their behavior given the precision of the results. Users are less likely to click on summaries of documents judged by NIST to be non-relevant. For bad, users click on summaries of non-relevant documents with a probability of 0.58 while for good, the probability is 0.66 (p = 0.019). This goes in hand with users taking longer to view summaries when the list precision is bad. The end result is that users go considerably deeper into the bad ranked list than they do in the good list. For bad, users on average reach rank 50, while for good, they reach rank 35 (p = 0.009). With what appears to be a small overhead of about 2 seconds per view of the querybiased summaries, users of the lower precision results can avoid wasting time viewing full documents not likely to be relevant.
4. CONCLUSION
We ran a two phase, within-subjects user study in which we controlled both the retrieval precision and the amount of interaction allowed in the search interface. By doing so, we were able to observe that the users change their behavior given the precision of the results list. When viewing lower precision results, users are more careful to avoid clicking on non-relevant summaries and are more likely to judge relevant documents as relevant. When viewing higher precision results, the same users spend less time viewing summaries, are more likely to click on non-relevant summaries, and are less likely to judge relevant documents as relevant. These behaviors, combined with the lack of perfect inter-assessor agreement, reduce the measured gains expected from increases in precision.
For both phases of the study, we found that retrieval precision and human performance are strongly related. The higher precision result lists produced statistically significant gains in human performance. We found that when the user

task and user interface better match the Cranfield-style evaluation metric, the metric better predicts human performance.
5. ACKNOWLEDGMENTS
Special thanks to Gordon Cormack for his helpful advice, and thanks to the anonymous reviewers for their helpful feedback. This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC), in part by an Amazon Web Services in Education Research Grant, and in part by the University of Waterloo. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.
6. REFERENCES
[1] A. Al-Maskari, M. Sanderson, P. Clough, and E. Airio. The good and the bad system: does the test collection predict users' effectiveness? In SIGIR'08, pages 59­66. ACM, 2008.
[2] J. Allan, B. Carterette, and J. Lewis. When will information retrieval be "good enough"? In SIGIR'05, pages 433­440. ACM, 2005.
[3] E. W. Bailey, D. Kelly, and K. Gyllstrom. Undergraduates' evaluations of assigned search topics. In SIGIR'09, pages 812­813. ACM, 2009.
[4] C. Cleverdon. The Cranfield tests on index language devices. In Aslib Proceedings, volume 19, pages 172­192, 1967.
[5] C. Cleverdon, J. Mills, and M. Keen. Aslib Cranfield research project - factors determining the performance of indexing systems; volume 1, design; part 1, text. Technical report, Cranfield University, 1966. URI: http://hdl.handle.net/1826/861.
[6] G. V. Cormack, C. L. A. Clarke, and S. Buettcher. Reciprocal rank fusion outperforms Condorcet and individual rank learning methods. In SIGIR'09, pages 758­759. ACM, 2009.
[7] W. Hersh, A. Turpin, S. Price, B. Chan, D. Kramer, L. Sacherek, and D. Olson. Do batch and user evaluations give the same results? In SIGIR'00, pages 17­24. ACM, 2000.
[8] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. TOIS, 20(4):422­446, 2002.
[9] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. TOIS, 27(1):1­27, 2008.
[10] F. Scholer and A. Turpin. Relevance thresholds in system evaluations. In SIGIR'08, pages 693­694. ACM, 2008.
[11] C. L. Smith and P. B. Kantor. User adaptation: good results from poor systems. In SIGIR'08, pages 147­154. ACM, 2008.
[12] A. Turpin and F. Scholer. User performance versus precision measures for simple search tasks. In SIGIR'06, pages 11­18. ACM, 2006.
[13] A. H. Turpin and W. Hersh. Why batch and user evaluations do not give the same results. In SIGIR'01, pages 225­231. ACM, 2001.
[14] E. M. Voorhees. I come not to bury Cranfield, but to praise it. In HCIR'09, pages 13­16, 2009.

602

Extending Average Precision to Graded Relevance Judgments

Stephen E. Robertson Evangelos Kanoulas

Emine Yilmaz

ser@microsoft.com

e.kanoulas@sheff.ac.uk eminey@microsoft.com

Microsoft Research 7 JJ Thomson Avenue Cambridge CB3 0FB, UK

Department of Information Studies University of Sheffield Sheffield S1 4DP, UK

ABSTRACT
Evaluation metrics play a critical role both in the context of comparative evaluation of the performance of retrieval systems and in the context of learning-to-rank (LTR) as objective functions to be optimized. Many different evaluation metrics have been proposed in the IR literature, with average precision (AP) being the dominant one due a number of desirable properties it possesses. However, most of these measures, including average precision, do not incorporate graded relevance.
In this work, we propose a new measure of retrieval effectiveness, the Graded Average Precision (GAP). GAP generalizes average precision to the case of multi-graded relevance and inherits all the desirable characteristics of AP: it has a nice probabilistic interpretation, it approximates the area under a graded precision-recall curve and it can be justified in terms of a simple but moderately plausible user model. We then evaluate GAP in terms of its informativeness and discriminative power. Finally, we show that GAP can reliably be used as an objective metric in learning to rank by illustrating that optimizing for GAP using SoftRank and LambdaRank leads to better performing ranking functions than the ones constructed by algorithms tuned to optimize for AP or NDCG even when using AP or NDCG as the test metrics.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]
General Terms: Experimentation, Measurement, Performance
Keywords: information retrieval, effectiveness metrics, average precision, graded relevance, learning to rank
We gratefully acknowledge the support provided by the European Commission grants FP7-ICT-248347 and FP7PEOPLE-2009-IIF-254562.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1. INTRODUCTION
Evaluation metrics play a critical role both in the context of comparative evaluation of the performance of retrieval systems and in the context of learning-to-rank (LTR) as objective functions to be optimized. Many different evaluation metrics have been proposed and studied in the literature. Even though different metrics evaluate different aspects of retrieval effectiveness, only a few of them are widely used, with average precision (AP) being perhaps the most commonly used such metric. AP has been the dominant systemoriented evaluation metric in IR for a number of reasons:
· It has a natural top-heavy bias. · It has a nice probabilistic interpretation [25]. · It has an underlying theoretical basis as it corresponds
to the area under the precision recall curve. · It can be justified in terms of a simple but moderately
plausible user model [16]. · It appears to be highly informative; it predicts other
metrics well [2]. · It results in good performance ranking functions when
used as objective in learning-to-rank [27, 24].
The main criticism to average precision is that it is based on the assumption that retrieved documents can be considered as either relevant or non-relevant to a user's information need. Thus, documents of different relevance grades are treated as equally important with relevance conflated into two categories. This assumption is clearly not true: by nature, some documents tend to be more relevant than others and intuitively the more relevant a document is the more important it is for a user. Further, when AP is used as an objective metric to be optimized in learning to rank, the training algorithm is also missing this valuable information.
For these reasons, a number of evaluation metrics that utilize multi-graded relevance judgments has appeared in the literature (e.g. [15, 8, 9, 19, 17]), with nDCG [8, 9] being the most popular among them, especially in the context of learning-to-rank as most learning to rank algorithms are designed to optimize for nDCG [6, 5, 22, 24].
In the framework used to define nDCG, a relevance score is mapped to each relevance grade, e.g. 3 for highly relevant documents, 2 for fairly relevant documents and so on. The relevance score of each document is viewed as the gain returned to a user when examining the document (utility of the document). To account for the late arrival of relevant documents gains are then discounted by a function of the rank. The discount function is viewed as a measure of the

603

patience of a user to step down the ranked list of documents. The discounted gain values are then summed progressively from rank 1 to k. This discounted cumulative gain at rank k is finally normalized in a 0 to 1 range to enable averaging the values of the metric over a number of queries, resulting in the normalized Discounted Cumulative Gain, nDCG.
The nDCG metric is thus a functional of a gain and a discount function and thus it can accommodate different user search behavior patterns on different retrieval task scenarios. As it has been illustrated by a number of correlation studies different gain and discount functions lead to radically different rankings of retrieval systems [23, 12, 11].
Despite the great flexibility nDCG offers, defining gain and discount functions in a meaningful way is a difficult task. Given the infinite number of possible discount and gain functions, the vast differences in users search behavior, the many different possible retrieval tasks and the difficulty in measuring user satisfaction, a complete and rigorous analysis of the relationship between different gain and discount functions and user satisfaction under different retrieval scenarios is prohibitively expensive, if at all possible.
For this reason, in the past, the selection of the gain and discount functions has been done rather arbitrarily, based on speculations of the search behavior of an average user and speculations of the correlation of the metric to user satisfaction. For instance, Burges et al. [5], introduced an exponential gain function (2rel(r) - 1, where rel(r) is the relevance score of the document at rank r) to express the fact that a highly relevant document is very much more valuable than one of a slightly lower grade. Further, the logarithmic discount function (1/log(r + 1)) dominated the literature compared to the linear one (1/r) based on the speculation that the gain a user obtains by moving down the ranked list of documents does not drop as sharply as indicated by the linear discount.
Despite the reasonable assumptions behind the choice of the gain and discount function that dominates nowadays the literature, recent work [1] demonstrated that cumulative gain without discounting (CG) is more correlated to user satisfaction than discounted cumulative gain (DCG) and nDCG (at least when computed at rank 100). This result not only strongly questions the validity of the aforementioned assumptions but mostly underlines the difficulty in specifying gain and discount functions in a meaningful manner.
Due to the above difficulties associated with the current multigraded evaluation metrics, even when multigraded relevance judgments are available, average precision is still reported (together with the multigraded metrics) by converting the relevance judgments to binary [4, 3]. Thus, despite the invalid assumption of binary relevance, average precision remains one of the most popular metrics used by IR researchers (e.g. in TREC [3]).Furthermore, even though AP is wasting valuable information in the context of learningto-rank, since it ignores the swaps between documents of different positive relevance grades, it has been successfully used as an objective metric [27]. Therefore, we believe that a direct extension of the metric to the multigraded case in a systematic manner is needed and it will become a valuable tool for the community both in the context of evaluation and in the context of LTR.
In this paper, we generalize average precision to the multigraded relevance case in a systematic manner, proposing a

new metric, the graded average precision (GAP). The GAP metric is a direct extension of AP and thus it inherits all the desirable properties that average precision has:
· It has the same natural top-heavy bias average precision has.
· It has a nice probabilistic interpretation. · It has an underlying theoretical basis as it corresponds
to the area under the "graded" precision-recall curve. · It can be justified in terms of a simple but moderately
plausible user model similarly to AP · It appears to be highly informative. · When used as an objective function in learning-to-rank
it results in good performance retrieval systems (it outperforms both AP and nDCG).
The incorporation of multi-graded relevance in average precision becomes possible via a simple probabilistic user model which naturally dictates to what extend documents of different relevance grades account for the effectiveness score. This user model corresponds to one of the approaches briefly discussed in Sakai and Robertson [20]. This model offers an alternative way of thinking about graded relevance compared to the notion of utility employed by nDCG and other multi-graded metrics.
Sakai [19] for instance has previously introduced a multigraded measure (the Q-measure) which has been shown to behave similarly to AP for ranks above R (where R is the number of relevant documents in the collection). Nevertheless, the incorporation of graded relevance by the Q-measure follows the same model with nDCG. GAP on the other hand is based on the well-trusted notions of precision and recall as is AP.
In what follows, we first describe the user model on which GAP is based and define the new metric. We then describe some desirable properties GAP possesses. In particular, we describe a probabilistic interpretation of GAP, generalize precision-recall curves for the multigraded relevance case and show that GAP is an approximation to the area under the graded precision-recall curves. Further, we evaluate GAP in terms of informativeness [2] and discriminative power [18]. Finally, we extend two popular LTR algorithms, SoftRank [22] and LambdaRank [6], to optimize for GAP and test the performance of the resulting ranking functions over different collections.

2. GRADED AVERAGE PRECISION (GAP)

2.1 User Model

We start from a rudimentary user model, as follows: as-

sume that the user actually has a binary view of relevance,

determined by thresholding the relevance scale {0..c}. We

describe this model probabilistically ­ we have a probabil-

ity gi that the user sets the threshold at grade i, in other

words regards grades i, ..., c as relevant and the others as

non-relevant. We consider this probability to be defined over

the space of users. These should be exclusive and exhaustive

probabilities:

Pc
j=1

gj

= 1.

2.2 Definition of GAP
Now, we want some form of expected average precision, the expectation being over this afore-defined probabilistic event space. Simple interpretation of this (just calculate

604

average precision separately for each grade and take a proba-

bilistically weighted combination) has problems; for instance,

in the case of an ideal ranked list, when there are no docu-

ments in some grades, the effectiveness score returned is less

than the optimal value of 1. So, instead, we extend the non-

interpolated form of AP; that is, we step down the ranked

list, looking at each relevant document in turn (the "pivot"

document) and compute the expected precision at this rank.

With an appropriate normalization at the end, this defines

the graded average precision (GAP).

In particular, suppose we have a ranked list of documents,

and document dn at rank n has relevance in  {0..c}. If

in > 0, dn, as pivot document, will contribute a precision

value to the average precision calculations for each grade j,

0 < j  in, since for any threshold set at grades less than or

equal to in, dn is considered relevant. The binary precision

value

for

each

grade

j

is,

1 n

(|dm

:

m



n, im



j|),

while

the expected precision at rank n over the aforementioned

probabilistic user space can be computed as,

in
X

,,

1

«

E[P Cn] =

n |dm : m  n, im  j| · gj

j=1

Let I(i, j) be an indicator variable equal to 1 if grade i is

larger than or equal to grade j and 0 otherwise. Then, the

expected precision at rank n can also be written as,

in
X

,,

1

«

E[P Cn] =

n |dm : m  n, im  j| · gj

j=1

1

in
X

n
X

= n

gj

I(im, j)

j=1 m=1

1

n min(in,im)
XX

= n

gj if im > 0

m=1 j=1

By observing the new form of calculation of E[P Cn], we can compute the contribution of each document ranked at m  n to this weighted sum for those grades j  im. Thus we define a contribution function:

 m,n =

Pmin(im ,in )
j=1

gj

0

if im > 0 otherwise

Now the contribution from the pivot document can be

defined

as,

E[P Cn]

=

1 n

Pn
m=1

m,n.

The maximum possible E[P Cn] depends on the relevance

grade in, it is the probability that this document is regarded

as

relevant

by

the

user,

Pin
j=1

gj

.

We must take account

of this when normalizing the sum of E[P Cn]'s. Suppose

we have Ri total documents in grade i (for this query);

then the maximum possible value of cumulated E[P Cn]'s is,

Pc
i=1

Ri

Pi
j

=1

gj

,

which

corresponds

to

the

expected

num-

ber of documents considered relevant in the collection, with

the expectation taken over the space of users, as above.

The graded average precision (GAP) is then defined as:

GAP

=

P
n=1

1 n

Pn
m=1

m,n

Pc
i=1

Ri

Pi
j=1

gj

Remark on thresholding probabilities: The user model that GAP is based on dictates the contribution of different relevance grades to the GAP calculation by considering the probability of a user thresholding the relevance scale at a certain relevance grade (the g values). This allows a better understanding and an easier mechanism to determine the

relative value of different relevance grades to an average user than the underlying model for the current multi-graded evaluation metrics. For instance, given the relevance grades of documents, click through data can be utilized to conclude relative preferences of users among documents of different relevance grades [10, 14]. Assuming that the user only clicks on the documents he finds relevant, the g values correspond to the probability that a user clicks on a document of a particular relevance grade, given all the documents clicked by the user. In this paper, given that our goal is to develop a good system-oriented metric, we propose an alternative way of setting the g values by considering which g = {gi} makes the metric most informative (see Section 4.1).
3. PROPERTIES OF GAP
In this section we describe some of the properties of GAP that make the metric understandable and desirable to use.
First, it is easy to see that GAP generalizes average precision ­ it reverts to average precision in the case of binary relevance. With respect to the model described in Section 2.1, binary relevance means that all users find documents with some relevance grade t > 0 relevant and the rest non-relevant (i.e., gj = 1 if j = t, for some relevance grade t > 0 and 0 otherwise).
Furthermore, GAP behaves in the expected way under document swaps. That is, if a document is swapped with another document of smaller relevance grade that appears lower in the list, the value of GAP decreases and vice-versa. As a corollary to this property, GAP acquires its maximum value when documents are returned in non-increasing relevance grade order.
In the following sections, we describe a probabilistic interpretation of GAP and show that GAP is an approximation to the area under a graded precision-recall curve.
3.1 Probabilistic interpretation
In this section we define GAP as the expected outcome of a random experiment, which is a generalization of the random experiment whose expected outcome is average precision [25], for the case of graded relevance. This offers an intuition behind the new measure.
3.1.1 Probabilistic interpretation of AP
Yilmaz and Aslam [25] have shown that AP corresponds to the expected outcome of the following random experiment:
1. Select a relevant document at random. Let the rank of this document be n.
2. Select a document at or above rank n, at random. Let the rank of that document be m.
3. Output 1 if the document at rank m, dm, is relevant.
In expectation, steps (2) and (3) effectively compute the precision at a relevant document. Then step (1), in combination with steps (2) and (3), effectively computes the average of these precisions. Hence, average precision corresponds to the probability that a document retrieved above a randomly picked relevant document is also relevant.
3.1.2 Probabilistic interpretation of GAP
Consider the case where graded relevance judgments are available. We claim that GAP corresponds to the expected outcome of the following random experiment:

605

1. Select a document that is considered relevant by a user (according to the afore-defined user model), at random. Let the rank of this document be n.
2. Select a document at or above rank n, at random. Let the rank of that document be m.
3. Output 1 if the document at rank m, dm, is also considered relevant by the user.

Hence, GAP can be seen as the probability that a document retrieved above a randomly picked "relevant" document is also "relevant", where relevance is defined according to the user model previously described.
We compute the expectation of the above random experiment to show that it corresponds to GAP. In expectation, step (3) corresponds to the conditional probability of document dm being considered as relevant given that document dn is also considered as relevant. To calculate this probability, let's consider all possible cases of the relative ordering of the relevant grades for documents dn and dm.

· (in  im) : Since the relevance grade of dn is smaller than or equal to the one for dm, if dn is considered relevant then dm will also be considered as relevant.

P r(dm = rel|dn = rel) =

=1=

Pin
j=1

gj

Pin
j=1

gj

=

Pmin(in ,im )
j=1

gj

Pin
j=1

gj

since min(in, im) = in. · (in > im) : By applying the Bayes' Theorem,

P r(dm = rel|dn = rel) =

= P r(dn = rel|dm = rel) · P r(dm = rel) P r(dn = rel)

=

1

·

Pim
j=1

gj

Pin
j=1

gj

=

Pmin(in ,im )
j=1

gj

Pin
j=1

gj

since min(in, im) = im

In expectation, steps (2) and (3) together, correspond to the value the "pivot" document dn will contribute to GAP,

1 n

·

n
X
m=1

Pmin(in ,im )
j=1

Pin
j=1

gj

gj

In step (1), the probability that a document dn is consid-

ered

relevant

is

Pin
j=1

gj

.

Thus,

the

probability

of

selecting

this document out of all documents that are considered rel-

evant is,

pdn

=

Pin
j=1

gj

Pc
i=1

Ri

Pin
j=1

gj

Therefore, step (1) in combination with steps (2) and (3) effectively computes the average of the contributed values, which corresponds to GAP,

GAP

=


X
n=1

1 n

n
X
m=1

Pmin(in ,im )
j=1

gj

Pin
j=1

gj

·

Pin
j=1

gj

Pc
i=1

Ri

·

Pin
j=1

gj

=

P
n=1

1 n

Pn
m=1

Pmin(in ,im )
j=1

gj

Pc
i=1

Ri

Pi
j=1

gj

3.2 GAP as the area under the graded precisionrecall curves

In this section we first intuitively extend recall and preci-

sion to the case of multi-graded relevance, based on the prob-

abilistic model defined in Section 2.1. Then we define the

graded precision-recall curve, and finally show that GAP ap-

proximates the area under the graded precision-recall curve,

as AP approximates the area under the binary precision-

recall curve.

Precision-recall curves are constructed by plotting pre-

cision against recall each time a relevant document is re-

trieved. In the binary relevance case, recall is defined as the

ratio of relevant documents up to rank n to the total number

of relevant documents in the query. In the graded relevance

case, a document is considered relevant only with some prob-

ability. Therefore, recall at a relevant document at rank n

can be defined as the ratio of the expected number of rele-

vant documents up to rank n to the expected total number

of relevant documents in the query (under the independence

assumption between numerator and denominator).

In particular, according to the user model defined in Sec-

tion 2.1, documents of relevance grade im are considered rel-

evant

with

probability

Pim
j=1

gj

,

and

thus,

the

expected

num-

ber

of

relevant

documents

up

to

rank

n

is,

Pn
m=1

Pim
j=1

gj ,

while the expected total number of relevant document is,

Pc
i=1

Ri

Pi
j=1

gj

.

Hence, the graded recall at rank n can be computed as,

graded

Recall@n

=

Pn
m=1

Pim
j=1

gj

Pc
i=1

Ri

Pi
j=1

gj

The recall step, i.e. the proportion of relevance information

acquired when encountering a "relevant" document at rank n

to

the

total

amount

of

relevance,

is,

Pin
j=1

gj

/

Pc
i=1

Ri

Pi
j=1

gj

.

This corresponds to the expected outcome of step (1) of the

random experiment described in Section 3.1 and expresses

the probability of selecting a "relevant" document at rank n

out of all possible "relevant" documents.

In the binary case, precision at a relevant document at

rank n is defined as the fraction of relevant documents up

to that rank. In the multi-graded case, precision at a "rel-

evant" document at rank n can be defined as the expected

number of documents at or above that rank that are also

considered as "relevant" This quantity corresponds to the

expected outcome of steps (2) and (3) of the random exper-

iment

in Section 3.1, graded Precision@n

=

1 n

n
X ·
m=1

Pmin(in ,im )
j=1

gj

Pin
j=1

gj

Therefore, graded average precision can be alternatively defined as the cumulated product of graded precision values and graded recall step values at documents of positive relevance grade, as average precision can be defined as the cumulated product of precision values and recall step values at relevant documents.
Given the definitions of graded precision and graded recall, one can construct precision-recall curves. Now it is easy to see that GAP is an approximation to the area under the non-interpolated graded precision-recall curve as AP is an approximation to the area under the non-interpolated binary precision-recall curve.
Note that Kek¨al¨ainen and J¨arvelin [13] have also proposed a generalization of precision and recall. The way they generalized the two statistics is radically different than the one we

606

propose; in their work precision and recall follow the nDCG framework where gain values are assigned to each document.
4. EVALUATION OF GAP
There are two important properties that a system-oriented evaluation metric should have: (1) it should be highly informative [2] ­ that is it should summarize the quality of a search engine well, and (2) it should be highly discriminative ­ that is it should identify the significant differences in the performance of the systems. We evaluated GAP in terms of both of these properties. We used nDCG as a baseline for comparison purposes. Given that our goal is to propose a good system-oriented metric that can be used as an objective function to optimize for in LTR, in what follows we mostly focus on the informativeness of the metric since it has been shown to correlate well with the effectiveness of the trained ranking function [26].
In particular, when a ranking function is optimized for an objective evaluation metric, the evaluation metric used during training acts as a bottleneck that summarizes the available training data. At each training epoch, given the relevance of the documents in the training set and the ranked list of documents retrieved by the ranking function for that epoch, the only information the learning algorithm has access to is the value of the evaluation metric. Thus, the ranking function will change on the basis of the change in the value of the metric. Since more informative metrics better summarize the relevance of the documents in the ranked list and thus better capture any change in the ranking of documents, the informativeness of a metric is intuitively correlated with the ability of the LTR algorithm to "learn" well.
4.1 Informativeness
To assess the informativeness of the evaluation metrics we use the Maximum Entropy Method (MEM) as proposed in Aslam et al. [2].
Similar to Aslam et al. we make the assumption that the quality of a list of documents retrieved in response to a given query is strictly a function of the relevance of the documents within that list (as well as the total number of relevant documents for the given query). Then, the question that naturally arises is how well does a metric capture the relevance of the output list and consequently the effectiveness of a retrieval system? In other words, given the value of a metric, for a given system on a given query, how accurately can one predict the relevance of documents retrieved?
Suppose that you were given a list of length N corresponding to output of a retrieval system for a given query, and suppose that you were asked to predict the probability of seeing a relevant document at some rank. Since there are no constraints, all possible lists of length N are equally likely, and hence the probability of seeing a relevant document at any rank is 1/2. Suppose now that you are also given the information that the expected number of relevant documents over all lists of length N is R. The most natural answer would be a R/N uniform probability for each rank. Finally, suppose that you are given the additional constraint that the expected value of a metric is v. Under the assumption that our distribution over lists is a product distribution, i.e. p(r1, r2, ..., rN ) = p(r1) · p(r2) · ... · p(rN ) (Aslam et al. call this probability-at-rank distribution), we can solve the problem by using MEM. That is, we find the most random probability-at-rank distribution (by maximizing the entropy

of p) that satisfies the following constraints: (a) the expected value of the metric over the probability-at-rank distribution is v, and (b) the expected number of relevant documents in each grade  is R).
To apply the maximum entropy method we derive the expected GAP and nDCG over the probability-at-rank distribution. The derivations are omitted due to space limitations. The maximum entropy formulations are shown in Figure 1. Both of them are constraint optimization problems and numerical methods were used to determine their solutions.
The result of the above optimization is a maximum entropy probability-at-rank distribution (over all relevance grades). Using this probability-at-rank distribution, we can infer the maximum entropy precision-recall curve. If a metric is very informative then the maximum entropy precision-recall curve should approximate well the actual precision-recall curve.
We then test the performance of GAP and nDCG using data from TRECs 9 and 10 Web Tracks (ad-hoc task) and TREC 12 Robust Track (only the topics 601-650 that have multi-graded judgments). Using the setup described above, we first infer the probability-at-rank distributions given the value of each metric and then calculate the maximum entropy precision-recall curves when only highly relevant documents are considered as relevant and when both relevant and highly relevant documents are considered as relevant (the graded PR-curves described in Section 3.2 are not used due to their bias towards GAP). As in Aslam et al. [2], for any query, we choose those systems that retrieved at least 5 relevant and 5 highly relevant documents to have a sufficient number of points on the precision-recall curves. We use different values for g1 and g2 to investigate their effect on the informativeness of GAP.
The mean RMS error between the inferred and the actual precision-recall curves, calculated at the points where recall changes, is illustrated in Figure 2. The x-axis corresponds to different pairs of threshold probabilities, g1 and g2. The blue solid line corresponds to the RMS error between the actual and the inferred precision-recall curves subject to GAP, while the red dashed line indicates the RMS error of the inferred precision-recall curves subject to nDCG.
As it can be observed (1) the choice of g1 and g2 appears to affect the informativeness of GAP; when g1 is high GAP appears to summarize well the sequence of all relevant documents independently of their grade, while when g2 is high GAP appears to summarize well the sequence of all highly relevant documents, (2) choosing g1 and g2 to be relatively balanced (around 0.5) seems to be the best compromise between summarizing well the sequence of all relevant documents independent of their grade and highly relevant documents only, and (3) with g1 and g2 to relatively balanced GAP appears to be more informative than nDCG in most of the cases1. Finally, note that when the thresholding probability g1 = 1 (the right-most point for GAP curve in all plots), GAP reduces to average precision since relevant and highly relevant documents are conflated in a sin-
1Different gain (linear vs. exponential) and discount (linear vs. log) functions used in the definition of nDCG were tested. The ones that utilized the log discount function appeared to be the most informative, while the effect of the gain function on informativeness was limited. The nDCG metric used here utilizes an exponential gain and a log discount function.

607

N
X Maximize: H(p) = H(pn)
n=1
Subject to:

1.

Nc
XX
n=1 =0

P r(in = n

)

0

X · @ gj
j=1

n-1 c 00min(,) 1

XX

X

+

@@

gj A P r(im

m=1 =0

j=1

=

11 )AA

/

"Pc
i=1

Ri

Pi
j=1

" gi

=

gap

N

X

2.

P r(in = ) = R

 : 1    c

n=1

c

X

3.

P r(in = ) = 1

n : 1  n  N

=0

N
X Maximize: H(p) = H(pn)
n=1
Subject to:

1.

N
X

c
X

(eg()

-

1)

·

P r(in

=

) / (optDCG)

=

ndcg

lg(n + 1)

n=1 =0

N

X

2.

P r(in = ) = R

n=1

c

X

3.

P r(in = ) = 1

=0

 : 1    c n : 1  n  N

RMS Error

Figure 1: Maximum entropy setup for GAP and nDCG, respectively.

0.2 0.18 0.16 0.14 0.12
0[.01,1]
0.24 0.22
0.2 0.18 0.16 0.14 0.1[02,1]

TREC 9 : relevant and highly relevant
GAP nDCG

[0.25,0.75] [0.5,0.5] [0.75,0.25]

[1,0]

TREC 9 : only highly relevant

[0.25,0.75]

[0.5,0.5]

GAP nDCG

[0.75,0.25]

[1,0]

RMS Error

RMS Error

TREC 10 : relevant & highly relevant 0.13
GAP nDCG
0.12

0.11

0.1

0.0[09,1]
0.15 0.14 0.13 0.12 0.11
0.1 0.0[09,1]

[0.25,0.75] [0.5,0.5] [0.75,0.25]

[1,0]

TREC 10 : only highly relevant

GAP nDCG

[0.25,0.75] [0.5,0.5] [0.75,0.25]

[1,0]

RMS Error

RMS Error

0.24 0.22
0.2 0.18 0.16 0.14 0.1[02,1]
0.21 0.2
0.19 0.18 0.17 0.16 0.1[05,1]

TREC 12 : relevant and highly relevant
GAP nDCG

[0.25,0.75] [0.5,0.5] [0.75,0.25]

[1,0]

TREC 12 : highly relevant

[0.25,0.75]

[0.5,0.5]

GAP nDCG

[0.75,0.25]

[1,0]

RMS Error

Figure 2: Mean RMS error between inferred and actual PR curves when only highly relevant documents are considered as relevant and when both relevant and highly relevant documents are considered as relevant.

gle grade. Therefore, one can compare the informativeness of GAP with the informativeness of AP by comparing the right-most point on the GAP curve with any other point on the same curve. For instance one can compare GAP with equal thresholding probabilities (g1 = g2 = 0.5) with AP by comparing the point on the blue line that corresponds to the [0.5,0.5] on the x-axis with the point on the blue line that corresponds to the [1,0] on the x-axis. This way we can test whether graded relevance add any value in the informativeness of the metric on the top of binary relevance. What is striking about Figure 2 is that in TREC 9 and 10 GAP (with g1 = g2 = 0.5) appears more informative than AP when relevant and highly relevant documents are combined (top row plots). That is, the ability to capture the sequence of relevance regardless the relevance grade is benefited by differentiating between relevant and highly relevant documents.
4.2 Discriminative Power
A number of researchers have proposed the evaluation of effectiveness metrics based on their discriminative power.That is, given a fixed set of queries, which evaluation metric can better identify significant differences in the performance of systems? By utilizing the framework proposed by Sakai [18], based on the Bootstrap Hypothesis Testing and using data from TREC 9, 10 and 12, we observed that the GAP metric appeared to outperform nDCG over TREC 12 data while

the opposite was true for TREC 9 and 10. When limiting our experiments to the best performing systems (top 15 by both metrics), GAP consistently outperformed nDCG in all three data sets. The results for TREC 9 are illustrated in Figure 3. Due to space limitations we omit the figures from TREC 10 and 12. In the figure the more towards the origin of the axes the curve is the more discriminative the metric is. The inner plot corresponds to the test over the best performing systems.

achieved significance level (ASL)

TREC9

0.1

0.1

gap

0.08

ndcg

0.06

0.05

0.04

0 150

200

0.02

8000 1000 1200 1400 1600 1800 2000 2200 system pair sorted by ASL

Figure 3: Discriminative power based on bootstrap hypothesis tests for TREC 9.

5. GAP FOR LEARNING TO RANK
Finally, we employed GAP as an objective function to optimize for in the context of LTR. For comparison pur-

608

Opt nDCG SoftRank Opt GAP
Opt AP Opt nDCG LambdaRankOpt GAP Opt AP

Test Metric

nDCG AP

PC(10)

0.6162 0.6084 0.5329

0.6290 0.6276 0.5478

0.6129 0.6195 0.5421

0.6301 0.6158 0.5355

0.6363 0.6287 0.5388

0.6296 0.6217 0.5360

Table 1: Test set performance for different metrics when SoftRank and LambdaRank are trained for nDCG, GAP, and AP as the objective over 5K Web Queries from a commercial search engine.

Opt nDCG SoftRank Opt GAP
Opt AP Opt nDCG LambdaRankOpt GAP Opt AP

Test Metric

nDCG AP

PC(10)

0.4665 0.4452 0.4986

0.4747 0.4478 0.5001

0.4601 0.4448 0.4900

0.4585 0.4397 0.5005

0.4665 0.4432 0.5042

0.4528 0.4408 0.4881

Table 2: Test set performance for different metrics when SoftRank and LambdaRank are trained for nDCG, GAP, and AP as the objective over the OSHUMED data set.

poses we also optimized for AP and nDCG. In our experiments we employed two different learning algorithms, (a) SoftRank [22] and (b) LambdaRank [6] over two different data sets, (a) a Web collection with 5K queries and 382 features taken from a commercial search engine, and (b) the OHSUMED collection provided by LETOR [21]. The relevance judgments in the both data set are in a 3 grade scale (non-relevant, relevant and highly relevant). Five-fold cross validation was used in the case of OHSUMED collection.
Since the informativeness of the metric is well correlated with the effectiveness of the constructed ranking function, we select g1 and g2 based on the criterion of informativeness. As we observed in Section 4.1, the values of gi that result in the most informative GAP variation is g1 = g2 = 0.5. Intuitively, these values of gi indicate that highly relevant documents are "twice as important as relevant documents.
LTR algorithms: SoftRank [22] is a neural network based algorithm that is designed to directly optimize for nDCG, as most other learning to rank algorithms. Since most IR metrics are non-smooth as as they depend on the ranks of documents, the main idea used in SoftRank to overcome the problem of optimizing non-smooth IR metrics is based on defining smooth versions of information retrieval metrics by assuming that the score sj of each document j is a value generated according to a Gaussian distribution with mean equal to sj and shared smoothing variance s. Based on this, Taylor et al. [22] define ij as the probability that document i will be ranked higher than document j. This distribution can then be used to define smooth versions of IR metrics as expectations over these rank distributions.
Based on these definitions, we extend SoftRank to optimize for GAP by defining SoftGAP, the expected value of Graded Average Precision with respect to these distributions and compute the gradient of SoftGAP.
Given the probabilistic interpretation of GAP defined earlier and the distribution ij, the probability that document i will be ranked higher than document j, SoftGAP can be computed as follows:
Let P Cn be:

P Cn

=

Pin
j=1

gj

+

PN
m=1

mn

Pmin(im ,in )
j=1

gj

PN
m=1,m=n

mn

+

1

then

Sof tGAP

=

N
X
n=1

Pc
i=1

P Cn

Ri

Pi
j=1

gi

Optimizing for an evaluation metric using neural networks and gradient ascent requires computing the gradient of the objective metric with respect to the score of an individual

document s¯m. To compute the gradients of SoftGAP, we use a similar approach as the one Taylor et al. [22] used to compute the gradients of nDCG. Detailed derivations for the computation of the gradients are omitted due to space limitations.
LambdaRank [6] is another neural network based algorithm that is also designed to optimize for nDCG. In order to overcome the problem of optimizing non-smooth IR metrics, LambdaRank uses the approach of defining the gradient of the target evaluation metric only at the points needed.
Given a pair of documents, the virtual gradients ( functions) used in LambdaRank are obtained by scaling the RankNet [5] cost with the amount of change in the value of the metric obtained by swapping the two documents [6].
Following the same setup, in order to optimize for GAP, we scale the RankNet cost with the amount of change in the value of GAP metric when two documents are swapped. This way of building gradients in LambdaRank is shown to find the local optima for the target evaluation metrics [7]. Detailed derivations for the computation of the virtual gradients for LambdaRank are also omitted due to space limitations.
Results: Tables 1 and 2 show the results of training and testing using different metrics. In particular the rows of the table correspond to training for nDCG, GAP and AP, respectively. The columns correspond to testing for nDCG at cutoff 10, AP and precision at cutoff 10. As it can be observed in the table training for GAP outperforms both training for nDCG and AP, even if the test metric is nDCG or AP respectively. The differences among the effectiveness of the resulting ranking functions are not large, however, (1) most of them are statistically significant, indicating that the fact that GAP outperforms AP and nDCG is not a results of any random noise in training data, (2) GAP consistently leads to the best performing ranking function over two radically different data sets, and (3) GAP consistently leads to the best performing ranking function over two different LTR algorithms. Thus, even if the differences among the constructed ranking functions are not large, optimizing for GAP can only lead to better ranking functions.
These results strengthen the conclusion drawn from the discussion about the informativeness of the metrics. First, it can be clearly seen that even in the case that we care about a binary measure (AP or PC at 10) the utilization of multi-graded relevance judgments is highly beneficial. Furthermore, these results suggest that even if one cares for nDCG at early ranks, one should still train for GAP as opposed to training for nDCG.

609

6. CONCLUSIONS
In this work we constructed a new metric of retrieval effectiveness (GAP) in a systematic manner that directly generalizes average precision to the multi-graded relevance case. As such, it inherits all desirable properties of AP: it has a nice probabilistic interpretation and a theoretical foundation; it estimates the area under the non-interpolated grade precision-recall curve. Furthermore, the new metric is highly informative and highly discriminative. Finally, when used as an objective function for learning-to-rank purposes GAP consistently outperforms AP and nDCG over two different data sets and over three different learning algorithms even when the test metric is AP or nDCG itself.
7. REFERENCES
[1] A. Al-Maskari, M. Sanderson, and P. Clough. The relationship between ir effectiveness measures and user satisfaction. In SIGIR '07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 773­774, New York, NY, USA, 2007. ACM.
[2] J. A. Aslam, E. Yilmaz, and V. Pavlu. The maximum entropy method for analyzing retrieval measures. In G. Marchionini, A. Moffat, J. Tait, R. Baeza-Yates, and N. Ziviani, editors, Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 27­34. ACM Press, August 2005.
[3] P. Bailey, N. Craswell, A. P. de Vries, I. Soboroff, and P. Thomas. Overview of the trec 2008 enterprise track. In Proceedings of the Seventeenth Text REtrieval Conference (TREC 2008), 2008.
[4] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P. de Vries, and E. Yilmaz. Relevance assessment: are judges exchangeable and does it matter. In SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 667­674, New York, NY, USA, 2008. ACM.
[5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In ICML '05: Proceedings of the 22nd international conference on Machine learning, pages 89­96, New York, NY, USA, 2005. ACM Press.
[6] C. J. C. Burges, R. Ragno, and Q. V. Le. Learning to rank with nonsmooth cost functions. In B. Sch¨olkopf, J. C. Platt, T. Hoffman, B. Sch¨olkopf, J. C. Platt, and T. Hoffman, editors, NIPS, pages 193­200. MIT Press, 2006.
[7] P. Donmez, K. M. Svore, and C. J. Burges. On the local optimality of lambdarank. In SIGIR '09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 460­467, New York, NY, USA, 2009. ACM.
[8] K. J¨arvelin and J. Kek¨al¨ainen. Ir evaluation methods for retrieving highly relevant documents. In SIGIR '00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 41­48, New York, NY, USA, 2000. ACM Press.
[9] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems, 20(4):422­446, 2002.
[10] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In SIGIR '05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 154­161, New York, NY, USA, 2005. ACM.
[11] E. Kanoulas and J. A. Aslam. Empirical justification of the gain and discount function for ndcg. In To appear in CIKM

'09: Proceedings of the 18th ACM international conference on Information and knowledge management, 2009.
[12] J. Kek¨al¨ainen. Binary and graded relevance in ir evaluations: comparison of the effects on ranking of ir systems. Inf. Process. Manage., 41(5):1019­1033, 2005.
[13] J. Kek¨al¨ainen and K. J¨arvelin. Using graded relevance assessments in ir evaluation. J. Am. Soc. Inf. Sci. Technol., 53(13):1120­1129, 2002.
[14] T. Minka, J. Winn, J. Guiver, and A. Kannan. Infer.net user guide : Tutorials and examples.
[15] M. S. Pollock. Measures for the comparison of information retrieval systems. American Documentation, 19(4):387­397, 1968.
[16] S. Robertson. A new interpretation of average precision. In SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 689­690, New York, NY, USA, 2008. ACM.
[17] T. Sakai. Ranking the NTCIR Systems Based on Multigrade Relevance, volume 3411/2005 of Lecture Notes in Computer Science. Springer Berlin / Heidelberg, February 2005.
[18] T. Sakai. Evaluating evaluation metrics based on the bootstrap. In SIGIR '06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 525­532, New York, NY, USA, 2006. ACM.
[19] T. Sakai. On penalising late arrival of relevant documents in information retrieval evaluation with graded relevance. In First International Workshop on Evaluating Information Access (EVIA 2007), pages 32­43, 2007.
[20] T. Sakai and S. Robertson. Modelling a user population for designing information retrieval metrics. In The Second International Workshop on Evaluating Information Access (EVIA 2008) (NTCIR-7 workshop) Tokyo, December 2008, 2008.
[21] J. X. Tao Qin, Tie-Yan Liu and H. Li. Letor: A benchmark collection for research on learning to rank for information retrieval. Information Retrieval Journal, 2010.
[22] M. Taylor, J. Guiver, S. E. Robertson, and T. Minka. Softrank: optimizing non-smooth rank metrics. In WSDM '08: Proceedings of the international conference on Web search and web data mining, pages 77­86, New York, NY, USA, 2008. ACM.
[23] E. M. Voorhees. Evaluation by highly relevant documents. In SIGIR '01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 74­82, New York, NY, USA, 2001. ACM.
[24] J. Xu and H. Li. Adarank: a boosting algorithm for information retrieval. In SIGIR '07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 391­398, New York, NY, USA, 2007. ACM.
[25] E. Yilmaz and J. A. Aslam. Estimating average precision with incomplete and imperfect judgments. In P. S. Yu, V. Tsotras, E. Fox, and B. Liu, editors, Proceedings of the Fifteenth ACM International Conference on Information and Knowledge Management, pages 102­111. ACM Press, November 2006.
[26] E. Yilmaz and S. Robertson. Deep versus shallow judgments in learning to rank. In SIGIR '09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 662­663, New York, NY, USA, 2009. ACM.
[27] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support vector method for optimizing average precision. In SIGIR '07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, New York, NY, USA, 2007. ACM Press.

610

Properties of Optimally Weighted Data Fusion in CBMIR

Peter Wilkins

Alan F. Smeaton

CLARITY: Centre for Sensor CLARITY: Centre for Sensor

Web Technologies

Web Technologies

Dublin City University

Dublin City University

Ireland

Ireland

pwilkins@computing.dcu.ie alan.smeaton@dcu.ie

Paul Ferguson
CLARITY: Centre for Sensor Web Technologies
Dublin City University Ireland
pferguson@computing.dcu.ie

ABSTRACT
Content-Based Multimedia Information Retrieval (CBMIR) systems which leverage multiple retrieval experts (En) often employ a weighting scheme when combining expert results through data fusion. Typically however a query will comprise multiple query images (Im) leading to potentially N × M weights to be assigned. Because of the large number of potential weights, existing approaches impose a hierarchy for data fusion, such as uniformly combining query image results from a single retrieval expert into a single list and then weighting the results of each expert. In this paper we will demonstrate that this approach is sub-optimal and leads to the poor state of CBMIR performance in benchmarking evaluations. We utilize an optimization method known as Coordinate Ascent to discover the optimal set of weights (|En| · |Im|) which demonstrates a dramatic difference between known results and the theoretical maximum. We find that imposing common combinatorial hierarchies for data fusion will half the optimal performance that can be achieved. By examining the optimal weight sets at the topic level, we observe that approximately 15% of the weights (from set |En| · |Im|) for any given query, are assigned 70%-82% of the total weight mass for that topic. Furthermore we discover that the ideal distribution of weights follows a log-normal distribution. We find that we can achieve up to 88% of the performance of fully optimized query using just these 15% of the weights. Our investigation was conducted on TRECVID evaluations 2003 to 2007 inclusive and ImageCLEFPhoto 2007, totalling 181 search topics optimized over a combined collection size of 661,213 images and 1,594 topic images.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms
Measurement, Experimentation
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Keywords
Data Fusion, Multimedia Fusion, Content-Based
1. MOTIVATION
Content-based multimedia information retrieval (CBMIR) systems often combine multiple sources of evidence to answer an information need. These systems typically employ multiple `retrieval experts' whose outputs are combined to create a response. This problem can be phrased as a combination of experts problem and is a case of Data Fusion. CBMIR is particularly characterized by the use of multiple `noisy' signals such as the color of an image, or the textures it contains, and through combining multiple sources of noisy information, reasonable performance can be achieved [17]. However this makes the role of weighted data fusion paramount to the success of many CBMIR systems.
Given that the performance of any such retrieval system is dependant upon the optimal generation of weights, and the manner in which those weighted documents1 are combined, it is essential that we know something about what the ideal distribution of weights for a query is, and the manner in which to combine these documents.
In order to gain significant insights into what the ideal form of weighting for CBMIR data fusion is, and indeed how ranked lists should be combined, we need to deviate from the traditional empirical model typically used to evaluate new algorithmic advances. Traditionally we have some form of training data, either topics, data or both, some proposed model which we want to test and some form of parameters which require tuning and a set of evaluation metrics and relevance assessments. Also included in this model is a test set from which final results will be reported. The common sequence of events is that a model is first optimised on training data, then the optimised model is used on the test data. The final result typically reported is the outcome of the evaluation metrics run on the output of the model on the test data, where presumably the model has not been overfitted.
The major problem with the established empirical model is that of evaluation, where what we want to determine is not the comparison of competing models and their associated performance, but rather given a maximally performing model, what parameters were associated with it? In data fusion tasks we will have a range of input sources of evidence, which we will then combine in some manner in order to compute a final response. The fundamental problem is that using the established empirical model, we could eval-
1The term `document' will refer to any multimedia artifact.

643

uate two different fusion systems, and after executing both having first trained on the training collection we can make the observation that system `a' outperforms system `b' by 15%. On the surface this seems fine, system `a' has achieved a good performance improvement over system `b'. However, this 15% is a relative increase, it is only meaningful when comparing the two systems under observation, and far less important when we do not know when given a fixed set of inputs, what the maximum achievable performance is. For instance, if system `a' scored a MAP of 0.2, but the theoretical maximum attainable given the same inputs was 0.8, then the relative importance of system `a's 15% improvement is diminished as there is clearly room for greater improvement. However, if the maximum was determined to be 0.21, then that 15% improvement is very significant.
A better use when determining what the maximum performance is for a fixed set of inputs, is to study the properties of this maximally performing model. Rather than being primarily concerned with the maximum performance MAP value, to flip this around such that given we have a model which achieves excellent performance, what are the properties of this model that led to this performance. In order to achieve this it necessitates optimisation directly on the test data. We are in-fact not proposing any particular model for data fusion in this paper, but rather we have created and observed the optimal model for this retrieval problem, such that we can report on the properties of this model which future systems should seek to leverage.
The objective of this paper therefore is to study what are the variables which generate a maximally performing CBMIR data fusion system, and if there are any commonalities to these that can be discovered so as to inform the development of new data fusion algorithms and systems. The performance of these models is in themselves immaterial, it is these factors which we wish to identify and examine. In particular in this paper, we will be examining the impact of combinational hierarchies upon performance, and the distribution that an ideal form of weighting takes.
No doubt at this point after mention of optimizing directly on the test set, alarm bells are ringing in several readers minds, however we believe we have good justification for doing this. Whilst we have laboured the point as to why we are examining models optimized on the test data, we believe this is necessary as the approach is unorthodox, and inadequately justified can lead to the the results presented in this paper being dismissed out of hand.
The paper is organized as follows. Section 2 presents related work in data fusion and evaluation, with reference to CBMIR applications. Section 3 details our experimental environment including retrieval experts, data sets, and a brief discussion of the optimization technique. Section 4 presents the results of our optimization on TRECVID 2003-2007 and ImageCLEFPhoto 2007 and discusses observations on this data set. Section 5 tests these observations to determine if they can be exploited by existing data fusion approaches. We finish with our conclusions in Section 6.
2. RELATED WORK
Early data fusion research began with experimentation into the combination of different retrieval models, document representations and query representations [13, 6, 16]. Research by Belkin et al. [2, 3] noted that varying these different factors produced different sets of relevant documents, yet

exhibited no major changes in performance metrics. Croft [5] notes that observations from these early studies suggested that it was beyond the capabilities of a single system to retrieve all the relevant documents for a given query. According to Croft this resulted in two streams of IR systems being developed, one stream was to create single models which can combine multiple sources of evidence such as the INQUERY system based on an inference network. The alternative stream is the development of systems which effectively combine the outputs of multiple searches from different retrieval models [7]. Croft notes for the task of multimedia retrieval, as different modalities are combined this requires the development of systems which combine the ranking from multiple subsystems (what we would term experts) [5].
Investigations into the behaviour of data fusion began with observations about low overlaps of the documents returned by different ranking models [13, 6, 16]. Belkin et al. [2] found "Different representations of the same query, or of the documents in the database, or different retrieval techniques for the same query, retrieve different sets of documents (both relevant and nonrelevant)". Lee [9] examined this research but contrasted it against the findings of Turtle et al. and Saracevic et al. [18, 16], where Turtle et al. in experiments combining probabilistic and Boolean retrieval results found that the relevant documents retrieved were shared by both approaches, whilst Saracevic & Kantor found that different query formulations found different documents, but that a document's odds of being judged as relevant increased monotonically as a document appeared in multiple result sets. Lee took these findings to formulate a new hypothesis for the effectiveness of data fusion: "different runs might retrieve similar sets of relevant documents but retrieve different sets of nonrelevant documents" [9].
Testing this hypothesis Lee introduced two evaluation metrics to measure the degree of overlap between relevant documents and nonrelevant documents, termed Roverlap and N Roverlap. Lee [9] finds that the best result from data fusion was achieved when result sets were combined in which relevant documents had high overlap and low overlap for non-relevant documents. The work of Vogt et al. [19] confirms Lee's observations by conducting pairwise experiments combining 61 TREC submissions. Vogt and Cottrell term the relevant overlap as the `Chorus Effect', that multiple retrieval systems return the same relevant documents.
Croft [5] interprets the findings of the work of Lee and of Vogt and Cottrell as being the result of combination of uncorrelated classifiers. Assuming that the retrieval systems being combined are good, that as the result lists being combined are truncated to 1000 results, and that for a given TREC query there are typically only 100-200 relevant documents, that most good systems will return within the 1000 results the 100-200 relevant documents, but as the `classifiers' (search systems) are uncorrelated, they will return different sets of nonrelevant documents. Furthermore this emphasizes earlier observations that combinations of independent good search systems, produce gains in performance when fused [5].
The data fusion hypothesis of Lee was critically examined by both McCabe et al. [11] and Beitzel et al. [1]. Both conducted approaches where various system parameters were held constant whilst varying one aspect, such as the ranking model, stemming, stopping, relevance feedback etc. The work of McCabe et al. found that when systemic parame-

644

ters are held constant, that the combination of vector, probabilistic and Boolean retrieval models did not improve performance of retrieval, contrary to previous accepted wisdom. This was further demonstrated by a lack of performance improvement when combining results from TREC-6, 7 and 8 queries which produced high overlaps in both Roverlap and Noverlap, meaning that each of the approaches were returning very similar content. Nevertheless this work found that the overlap coefficients were a good predictor of the potential for performance improvement with data fusion, particularly when systems were combined with weights, such that a poor performing system could be discounted. The combination of a poor system with a good system, using weights where the good system was weighted highly, produced performance increases, lending support to the application of weights for expert combination [11].
Beitzel et al. [1] like McCabe also conducted experiments where system parameters are held constant to measure the impact of combination of different aspects of retrieval systems. The work of Beitzel et al. specifically examined the combination of "highly effective retrieval strategies". Assuming this, Beitzel et al. hypothesize that combination of highly effective systems through voting mechanisms like CombMNZ are more likely to harm performance, as the highly effective systems have already been optimized and will rank relevant documents highly, therefore the candidates for promotion up a ranked list are lower ranked common nonrelevant documents as the relevant documents are already highly ranked. They further hypothesize that as constants such as the query and stemming for each retrieval model are held constant, that different models will produce approximately the same set of documents for a query, only the relative ranks of these sets are likely to be different. For highly effective systems Beitzel et al. found that the combination of retrieval models (e.g. vector space and probabilistic) hurts performance, rather than helps, whilst the overlap coefficients defined by Lee [9] provide a poor indicator of potential for improvement through data fusion [1].
These two results however give credence to the application of weighted data fusion to the task of CBMIR. Given that CBMIR is characterized by the combination of multiple poor retrieval experts [17], we are unlikely to be combining multiple experts that actually perform consistently well for any set of queries. Furthermore as the work of McCabe shows, weighted combination of poor retrieval experts can lead to significant performance improvements.
Within the multimedia research community, several fusion approaches have been investigated. Yan et al. propose the use of `Query-Class' dependent weights [22], where a set of predefined query classes are assigned feature weights learned from the training data. This approach is extended by Kennedy et al. [8] to automatically discover query classes from training data. These approaches, however, typically weight entire features (retrieval experts).
Two previous investigations into the role of feature combination for multimedia retrieval have been completed, by McDonald and Smeaton [12] and by Yan and Hauptmann [21]. McDonald and Smeaton empirically compare combination approaches for score, rank and probability techniques. Their work evaluated these approaches by optimizing Mean Average Precision (MAP) on a training collection with multiple topics, then applying these generalized optimized parameters to a test set. Our work differs as our optimizations

Eval. TRECVID 03 TRECVID 04 TRECVID 05 TRECVID 06 TRECVID 07 ImageCLEF 07

Keyframes 72,462 48,818 78,206 146,497 295,350 20,000

Topic Images 138 160 228 169 719 180

Topics 25 24 24 24 24 60

Table 1: Details of corpora used

occur at the topic level (Average Precision), rather than the topic set level (MAP). Furthermore the fusion approaches detailed in [12] use a hierarchical approach which is likely to obscure the effect individual query images have on performance. Yan and Hauptmann [21] conduct experiments with TRECVID 2002 data to construct a theoretical framework for studying the upper bounds of combination functions. They found that linear forms of combination may be too restrictive for large numbers of experts to be combined effectively. However, like McDonald and Smeaton, this work examined combination at the expert level and as such did not delve down to the granularity of pairs Ii, Ej .
3. EXPERIMENTAL SETUP
The task we explore is an ad-hoc search task, where a system is given an expression of an information need and is required to return as many relevant matches as possible. For our investigation we performed `fully automatic' retrieval which processes a query with no human intervention.

We used six different multimedia corpora, five of which came from TRECVID [17] and one from ImageCLEF [4]. These two campaigns share similar objectives as both seek to promote research in content-based retrieval by utilizing common test collections and open, metrics-based evaluations. As previously noted, for all corpora we only consider the visual information provided. In the case of video, we use the extended keyframe set provided, meaning that in many cases we index more than one keyframe per shot. For query descriptions we make use of all visual data provided. In the case of videos used as part of the topic description, we sample keyframes from this video and add it to the topic image set. The six corpora we used are described in Table 1.
We make use of six global visual features defined in the MPEG-7 specification [10]: Scalable Color (SC), Color Structure (CS), Color Layout (CL), Color Moments (CM), Edge Histogram (EH) and Homogeneous Texture (HT). To compute an answer to a visual query, we take the topic images and we query them against each retrieval expert, producing for each pair Ii, Ej a ranked list of results. For our experiments we produced ranked lists of 1000 results per pair Ii, Ej . Each ranked list is normalized using MinMax [7], then weighted and linearly combined using CombSUM [7]. We would note here that we deliberatly choose CombSUM over CombMNZ, as through the processes discussed in the paper, we have empirically shown that CombSUM with linear weighting offers superior performance to that of weighted CombMNZ (due to space constraints we cannot explore these results, see: [20]). The ranking metric for each expert is implemented as defined by the MPEG-7 standard, typically a variation on Euclidean distance.

645

The optimization method we use in this work is known as Coordinate Ascent (also known as Alternating Variables Method ). It is a method which is able to optimize directly on Average Precision (AP), by randomly initializing the linear weights, then finding a local maxima based on AP. The method is then repeated multiple times so that a global maxima can be found. This approach has been used to good effect by Metzler and Croft, and a complete explanation of this method can be found in their work [14].
3.1 Hierarchical Combination Approaches
There are three basic levels of combination available to CBMIR designers whose systems utilize multiple retrieval experts and multiple query components: combination at the `query' level [12], combination at the `expert' level [22] and direct combination. Figure 1 illustrates these variations.
The elements for weighting can be formally defined as follows. A CBMIR search topic will contain multiple example visual images, Images I = {queryimagei ... queryimagen} where 1  i  n. A CBMIR system will have at its disposal multiple retrieval experts, E = {expertj, ... expertm} where 1  j  m. Therefore, we can define the pair Ii, Ej , which is a unique coupling of every example query image to every visual retrieval expert. This will generate n × m pairs.
We can now further define the set of weights to be tuned at each of the three different levels of combination. For "Query" level combination, the results of every retrieval expert for a specific query image (Ii) are linearly combined with uniform weight into a single ranked list which represents a given image. The merged results lists for every image are then weighted and combined, meaning for this level we need to optimize n weights (i.e. |I|), giPving Image Weights IW = {wi} where w is the weight and wi = 1.
At an "Expert" level of combination we execute the opposite. For a specific expert (Ej) we query against it all query images for the topic, merging the results to produce for each expert a single ranked list. We then weight the result list of each expert, and combine to form our final ranked list. In this level, we are required to optimize m wPeights (i.e. |E|), formally Expert Weights EW = {wj} and wj = 1.
Finally we have the "direct" level of combination which specifies weights for every coupling of an example image and retrieval expert. That is, for every pair Ii, Ej we are required to set a weight. This will produce a weight set of size n×m (i.e. |I|·|E|) where IEW = {wij}, where i rP efers to the query image, j refers to the retrieval expert and wij = 1.
4. OPTIMIZATION RESULTS
We performed the optimization of CBMIR on TRECVID 2003 to 2007 and on ImageCLEFPhoto 2007. The results are presented in Table 2 (where TV is TRECVID and IC is ImageCLEFPhoto).

Eval. MAP Uniform BR

TV03 0.122 0.059 N/A

TV04 0.108 0.029 N/A

TV05 0.141 0.065 0.126

TV06 0.056 0.016 0.087

TV07 0.130 0.042 0.087

IC07 0.216 0.128 0.189*

Table 2: Optimized Results compared to `Best Reported' (BR). `Uniform' represents using all pairs Ii, Ej with no weighting. *IC07 BR is visual only

Figure 1: Levels of combination for a single search topic, with 2 retrieval experts (E) and 2 example query images (I), giving 4 ranked lists (pairs Ii, Ej ). Three levels are available, combination at the `Query' Level, combination at the `Expert' level and direct combination without any hierarchy.

646

were assigned values > 1. The implications of this are that overall the initial observations would suggest that a minority of the pairs Ii, Ej received the majority of a topic's weight.
Without other evidence there remains the possibility that the effect presented is a corpora-specific event and that the weights are indeed more normally distributed. To account for this we present in Figure 3 a corpora-specific plot of the weight distribution in the form of quantile-quantile (Q-Q) plot. In this figure, the x-axis represents a theoretical normal distribution of weights, whilst the y-axis is the actual weight which was assigned. The dashed line displays the trend line of the weights if they were normally distributed.

Figure 2: Standard Scores for assigned weights across all corpora

TV 2005: Normal Q-Q Plot

8

This optimization generated automatic retrieval runs which achieved excellent performance with the use of no semantic information or text. For comparison, the row `BR' shows the best reported automatic system MAP from that year's activity. These figures are actually a bit startling, as the very high levels of performance achieved run contrary to expectations from previous experiments [17]. This is particularly apparent if we compare the optimized MAP to the `Uniform' MAP. The `Uniform' map demonstrates a retrieval run where all pairs Ii, Ej are equally weighted, i.e. there is no weighting at all. We can see that the optimal weights applied to pairs Ii, Ej can produce up to a 300% increase in performance. The impact of these figures is such that people question if such performance is achievable with low-level visual MPEG-7 features and no text, as it runs contrary to previous experimental knowledge.
We show the comparison to the best reported runs in that year's evaluation, as it demonstrates the effectiveness of our optimization, producing retrieval runs which achieve excellent performance. The comparison highlights the maximum of what can be achieved with data fusion and global low-level visual features, particularly when compared against the top performing runs which made use of multiple evidence modalities including text and semantic information. We note that this comparison to published retrieval runs (`BR') is not a fair comparison as we optimized on the test data, however the intention of this work is to demonstrate the gains achievable with optimized weights, even when compared against retrieval runs that used high quality signals such as text.
We analyzed the optimal weight topic sets IEW generated for each topic and calculated the standard score (also known as Z-Score). The standard score allows us to express for any given pair Ii, Ej weight wij how far from the topic mean weight it is in terms of standard deviations. This provides us with a measure which can be used across topics reliably. Figure 2 is a histogram of the distribution of standard scores across all topics and corpora.
We can infer multiple insights from the presented distribution and measures of central tendency. Firstly, that whilst the distribution of weights has some properties of that of a normal distribution, such as a majority of the data points clustered around the mean and within the range ±3, there does exist a very definitive positive skew. Secondly, as part of this positive skew approximately 10%-11% of the weights

Sample Quantiles

0

2

4

6

-3 -2 -1 0 1 2 3
Theoretical Quantiles
Figure 3: Weight Distribution for TRECVID 2005
Examining the Q-Q plot, we can see the same distributional pattern, as it demonstrates a significant departure from a normal distribution, particularly once the normalised weights values exceed 1. The pattern shown in the plot is similar to what would be expected if the distribution of the weights was log-normal, again we can also see demonstrated in each plot a positive skew. Whilst the data presented in this Figure is only for 2005, this pattern was repeated for all of our experimental corpora [20]. Based on this evidence this indicates that within topics, a minority of Ii, Ej weights are assigned a majority of the topic weight mass.
To explore this, we examined each corpus and its topics to determine where topic weight was assigned. For each topic we set a threshold of +1 and calculated the total amount of weight mass which was more than +1 from the mean weight, and what percentage of Ii, Ej were assigned these weights over this threshold, i.e. wij > 1. The results of our analysis are presented in Figure 4 which show two columns for each topic. The first column in blue (dark), represents the total amount of weight allocated in that topic which was +1 greater than the mean weight. The second column in yellow (light) represents how many of the Query-Terms value of wij was more than +1 from the mean. For example, the

647

0,149 0,150 0,151 0,152 0,153 0,154 0,155 0,156 0,157 0,158 0,159 0,160 0,161 0,162 0,163 0,164 0,165 0,166 0,167 0,168 0,169 0,170 0,171 0,172

first graph in Figure 4 represents topics from TRECVID 2005. In topic `0149' we can observe a blue bar at 70%, and a yellow bar at 6%. This means that for topic '0149', 6% of the pairs Ii, Ej used for that topic were allocated 70% of the weight and the remaining 94% of Ii, Ej had only 30% of the topic weight.
Weight Assigned Percentage of Pairs <I,E> receiving this weight TRECVID 2005 High Performing Pairs <I,E> 1
0.8
0.6
0.4
0.2
0
Topics
Figure 4: Evaluation Campaign Weights
We can see from this graph that for all topics across TRECVID 2005, achieving the maximum average precision possible is dependant upon specific pairs Ii, Ej being allocated the bulk of the weight for that topic, rather than specific experts EW being correctly weighted. Whilst we only show TRECVID 2005 here, the patterns expressed in this graph are replicated for all evaluations examined.
The possibility exists that we are inadvertently seeing one retrieval expert for a topic performing strongly, and thus all pairs Ii, Ej which utilize that expert are up-weighted. We examined the distribution of each of the six experts within the set of highly weighted Ii, Ej to determine if there was a bias towards any particular expert, shown in Table 3.
CL CM CS SC EH HT 15% 17% 13% 13% 24% 18%
Table 3: Distribution of Retrieval Experts in Ii, Ej with wij > 1 There is a slight bias towards EH and to a lesser extent the HT experts, however as there are only two texture but four color experts, this bias can be accounted for. The data presented in Figure 4 and Table 3 shows that highly weighted Ii, Ej are distributed across different experts.
We observe that the key to maximizing AP is to correctly identify salient pairs Ii, Ej and ensure that these are highly weighted, rather than weighting the overall performance of any given retrieval expert. To test this observation, we devise a series of experiments that utilize only highly weighted pairs Ii, Ej to see if we still achieve good performance. A highly-weighed pair Ii, Ej is a pair whose weight wij is greater than +1 of the mean weight for that topic.
5. EXPERIMENTS
To test our observations we devised three experiments in order to (1) determine to what extent the highly-weighted pairs Ii, Ej impact upon performance; (2) to determine if the weighting of these pairs needs to be exact or if merely

identification is enough; and finally, (3) to determine the impact the remainder of the pairs Ii, Ej which do not have much weight allocated to them have upon performance. As a comparison we have also included two optimizations, "Query" level and "Expert" level optimizations. These represent the best performance achievable if we utilize existing data fusion methods (such as Query-Class, single feature machine learning [22][15]), and allow us to determine if our suggested strategies of targeted weighting of pairs Ii, Ej rather than expert level weighting offers improvement.
· (1) 1: For each topic, only use highly-weighted pairs Ii, Ej (i.e. pairs Ii, Ej whose assigned value from optimization was +1 for the mean weight). The value of wij will be the value determined during optimization (Section 4). This test will examine the impact of precisely weighted high-performing pairs Ii, Ej . It can be thought of as a high-precision experiment as for each topic we will be using only 5%-20% of the available ranked lists for that topic.
· (1U) 1 Uniform: Using only the highly-weighed pairs Ii, Ej , assign each a uniform weight. This will examine if just the identification of high-performing pairs Ii, Ej is sufficient to yield performance increases, specifically determining if accurate weighting of pairs is required, or if they can be assigned a binary weight [0,1]. As the task of determining the optimal set wij is realistically only viable post-experiment, this experiment tests if realistic fusion approaches can be developed, as it does not require perfect weights, only identification of likely high performing pairs Ii, Ej .
· (1U-T) 1 & Tail: We extend experiment 1, by taking the remaining weight mass that isn't assigned to high-performing pairs and allocate it uniformly amongst the remaining pairs in IEW . This experiment complements the previous, we assign a large weight to the high-performing pairs, whilst a low weight to the remainder. As the high-performing pairs constitute only 5%-20% of available pairs for a topic, this experiment is testing the impact of recall, i.e. can we include the remainder of the data without accurate weighting so as to increase our recall.
· Expert Optimized: We implement the "Expert"level of combination as described in Section 3.1 and as is implemented by several data fusion approaches. Here we utilize the optimization approach as described in Section 4 so as to determine the near-optimal set of weights EW for "Expert" level combination, i.e. we optimize weights wj. This demonstrates the best performance that can be expected using the same query images and experts as the previous experiments if we impose a combination hierarchy at the "Expert" level.
· Query Optimized: This experiment is as for "Expert" optimized, except that the weight set we are optimizing is IE, i.e. weights wi, and demonstrates the best performance that can be achieved if we combine at the "Query" level.
For each experiment we include the minimum and maximum achieved for that corpus. The minimum is a `Uniform' run, where all pairs Ii, Ej are equally weighted, demonstrating

648

the performance achieved if no weighting scheme at all is employed. The maximum is the fully optimized result as shown in Section 4, demonstrating the best performance that can be achieved. These two figures provide a lower and upper bound for data fusion performance comparisons, allowing us to make decisions using absolute observations with regard to the bounds, rather than relative observations by comparing only to existing data fusion approaches.
Our results are presented in Figure 5. Each table presents the minimum (Uniform All), maximum (All Optimized) and results of the 5 experiments using MAP, recall and P10. For every experiment's MAP, we show in brackets how close that approach came to achieving the optimal performance. The MAP of each of the experiments, along with the maximum MAP, is graphed in Figure 6. For each of our 5 runs we ran significance tests (partial randomization) with  0.05. For the TRECVID benchmarks we found no significant difference between the `Query' and `Expert' levels of hierarchical combination, indicating that if hierarchical combination is employed and optimally weighted, there is no difference in between them. However for ImageCLEF `Expert' was significantly different. For benchmarks TRECVID 2003-2006, all runs using highly weighted (1) pairs performed significantly better than the hierarchical combination approaches. For TRECVID 2007, only run 1 was significantly different.
The graph presents a clear stratification of the results, particularly for benchmarks TRECVID 2003 - 2006. We can clearly see the very large discrepancy in performance between the hierarchical fusion approaches (at the bottom of the graph) versus the targeted weighting approaches in the middle. This separation illustrates the performance gains achievable by moving away from hierarchical combinations. Of exception is TRECVID 2007 and ImageCLEF 2007, where there is less of a difference in performance. These two benchmarks exhibit the greatest ratio of topic images to collection images ­ in the case of ImageCLEF one topic image for every 112 collection images. This indicates that recall plays a more prominent role in these evaluations, and that the selection of highly-weighted pairs may have been too restrictive to provide adequate topic coverage. This is reinforced by the run 1U-T, which included all pairs Ii, Ej : it performed the best even though it used non-specific weights.
The run 1 highlights that, using a subset of pairs Ii, Ej from IEW , very good performance can be achieved despite a reduction in potential recall by not using all pairs. Far more encouraging is the performance of runs 1U and 1UT. Whilst run 1 had value as an illustrative run, it is hard to conceptualize a data fusion algorithm that would create the exact optimal weights for these pairs. However, as runs 1U and 1U-T did not use the optimal weights, but rather only identified what the high-performing pairs 1U and 1U-T were (essentially a binary weighting), and still achieved excellent performance, it provides a clear direction for development of data fusion algorithms. These runs demonstrate that if methods can be developed to identify pairs Ii, Ej that are likely to be highly weighted, then exact weighting is not required to obtain performance superior to that of methods which employ hierarchies.
6. CONCLUSIONS
In this paper we have demonstrated that the application of a data fusion hierarchy severely limits the performance that a CBMIR retrieval run can possibly achieve. We propose

Full Optimization
Uniform with tail
0.25

Optimized Expert Optimized (Wj)

Uniform Query Optimized (Wi)

0.2

0.15

MAP

0.1

0.05

0 TV2003

TV2004

TV2005

TV2006

Evaluation

TV2007

IC2007

Figure 6: MAP Values for Data Fusion experiments

that rather than weighting combinations at the "Expert" or "Query" level, data fusion algorithms will achieve far greater performance by optimizing specific instances of an example query image and retrieval expert. Furthermore, we have demonstrated through our optimization process, that the ideal distribution of weights for data fusion in CBMIR is that of a log-normal distribution. Our observations are robust, as they occur within a fixed frame of reference, i.e. the lower and upper bounds achievable with data fusion, such that we determined the absolute effectiveness of particular approaches without having to make relative comparisons. Of practical concern however is how these observations may be interpreted to further aid CBMIR performance, and what methods may be used to weight at the direct level. Potential avenues for exploration involve looking for some form of correlation between documents which attract a large weight and content-analysis techniques such as entropy measures.
Acknowledgements
This work is supported by Science Foundation Ireland under grant 07/CE/I1147.
7. REFERENCES
[1] S. M. Beitzel, E. C. Jensen, A. Chowdhury, D. Grossman, O. Frieder, and N. Goharian. Fusion of effective retrieval strategies in the same information retrieval system. Journal of the American Society for Information Science and Technology, 55(10):859­868, 2004.
[2] N. J. Belkin, C. Cool, W. B. Croft, and J. P. Callan. Effect of multiple query representations on information retrieval system performance. In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '93), pages 339­346, Pittsburgh, PA, USA, 1993.
[3] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw. Combining the evidence of multiple query representations for information retrieval. Information Processing and Management, 31(3):431­448, 1995.
[4] P. Clough, M. Grubinger, A. Hanbury, and H. Mu¨ller. Overview of the imageclef 2007 photographic retrieval task. In Proceedings of the CLEF 2007 Workshop, LNCS, Budapest, Hungary, 2008.

649

Legend Run Uniform All Expert Level Query Level 1 Uniform 1 Uniform & Tail 1 All Optimized

TRECVID 2003

MAP

Recall

0.0593

0.238

0.0752 (61%) 0.265

0.0776 (63%) 0.273

0.0966 (79%) 0.279

0.0958 (78%) 0.283

0.0989 (80%) 0.281

0.1224

0.303

P10 0.108 0.196 0.228 0.292 0.272 0.308 0.376

TRECVID 2004

MAP

Recall

0.0288

0.144

0.0519 (47%) 0.168

0.0543 (50%) 0.202

0.0738 (68%) 0.227

0.0764 (70%) 0.225

0.0770 (71%) 0.225

0.1084

0.232

P10 0.100 0.200 0.187 0.287 0.278 0.309 0.391

TRECVID 2005

MAP

Recall

0.0646

0.114

0.0827 (59%) 0.134

0.0850 (60%) 0.146

0.1037 (74%) 0.148

0.1109 (79%) 0.151

0.1108 (79%) 0.157

0.1407

0.173

P10 0.233 0.363 0.346 0.492 0.496 0.554 0.658

Legend Run Uniform All Expert Level Query Level 1 Uniform 1 Uniform & Tail 1 All Optimized

TRECVID 2006

MAP

Recall

0.0164

0.093

0.0299 (53%) 0.114

0.0262 (47%) 0.115

0.0460 (82%) 0.133

0.0453 (80%) 0.138

0.0496 (88%) 0.139

0.0563

0.149

P10 0.079 0.221 0.163 0.358 0.342 0.417 0.488

TRECVID 2007

MAP

Recall

0.0422

0.201

0.0655 (56%) 0.250

0.0700 (60%) 0.261

0.0680 (58%) 0.284

0.0762 (65%) 0.286

0.0772 (66%) 0.262

0.1175

0.312

P10 0.142 0.258 0.275 0.346 0.359 0.375 0.508

ImageCLEF 2007

MAP

Recall

0.1283

0.402

0.1648 (76%) 0.413

0.1544 (71%) 0.415

0.1404 (65%) 0.381

0.1715 (80%) 0.413

0.1439 (68%) 0.381

0.2156

0.438

P10 0.347 0.473 0.445 0.415 0.457 0.445 0.590

Figure 5: Experimental Results, all corpora

[5] W. B. Croft. Combining approaches to information retrieval. Advances in Information Retrieval, pages 1­36, 2000.
[6] P. Das-Gupta and J. Katzer. A study of the overlap among document representations. SIGIR Forum, 17(4):106­114, 1983.
[7] E. A. Fox and J. A. Shaw. Combination of Multiple Searches. In Proceedings of the 3rd Text REtrieval Conference (TREC-2), Gaithersburg, MD, USA, 1994.
[8] L. S. Kennedy, A. P. Natsev, and S.-F. Chang. Automatic discovery of query-class-dependent models for multimodal search. In Proceedings of the 13th annual ACM international conference on Multimedia (MULTIMEDIA '05), pages 882­891, Singapore, Singapore, 2005.
[9] J. H. Lee. Analyses of multiple evidence combination. In Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '97), pages 267­276, Philadelphia, Pennsylvania, USA, 1997.
[10] B. Manjunath, P. Salembier, and T. Sikora, editors. Introduction to MPEG-7: Multimedia Content Description Language. Wiley, 2002.
[11] M. McCabe, A. Chowdhury, D. Grossman, and O. Frieder. System fusion for improving performance in information retrieval systems. In Proceedings of International Conference on Information Technology: Coding and Computing (ITCC 2001), Las Vegas, NV, USA, 2001.
[12] K. McDonald and A. F. Smeaton. A comparison of score, rank and probability-based fusion methods for video shot retrieval. In Proceedings of the 4th ACM international Conference on Image and Video Retrieval (CIVR '05), Dublin, Ireland, 2005.
[13] M. McGill, M. Koll, and T. Noreault. An evaluation of factors affecting document ranking by information retrieval systems. Technical Report NSF-IST-78-10454 to the National Science Foundation (USA), Syracuse University, 1979.

[14] D. Metzler and W. B. Croft. Linear feature-based models for information retrieval. Information Retrieval, 10(3):257­274, 2007.
[15] A. P. Natsev, M. R. Naphade, and J. Tesic. Learning the semantics of multimedia queries and concepts from a small number of examples. In Proceedings of the 13th annual ACM international conference on Multimedia (MULTIMEDIA '05), pages 598­607, Singapore, Singapore, 2005.
[16] T. Saracevic and P. Kantor. A study of information seeking and retrieving, iii: Searchers, searches, overlap. Journal of the American Society for Information Science and Technology (JASIST), 39:177­196, 1988.
[17] A. F. Smeaton, P. Over, and W. Kraaij. Evaluation Campaigns and TRECVid. In Proceedings of the 8th ACM SIGMM International Workshop on Multimedia information retrieval (MIR 2006), 2006.
[18] H. Turtle and W. Croft. Evaluation of an Inference Network-based Retrieval Model. ACM Transactions on Informaion Systems, 9(3):187­222, 1991.
[19] C. C. Vogt and G. W. Cottrell. Fusion Via a Linear Combination of Scores. Information Retrieval, 1(3):151­173, 1999.
[20] P. Wilkins. An Investigation Into Weighted Data Fusion for Content-Based Multimedia Information Retrieval. PhD thesis, Dublin City University, Glasnevin, Dublin, Ireland, September 2009.
[21] R. Yan and A. G. Hauptmann. The combination limit in multimedia retrieval. In Proceedings of the eleventh ACM international conference on Multimedia (MULTIMEDIA '03), pages 339­342, Berkeley, CA, USA, 2003.
[22] R. Yan and A. G. Hauptmann. Probabilistic latent query analysis for combining multiple retrieval sources. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2006), pages 324­331, Seattle, Washington, USA, 2006.

650

Efficient Partial-Duplicate Detection Based on Sequence Matching

Qi Zhang, Yue Zhang, Haomin Yu, Xuanjing Huang
School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, P.R.China
{qi_zhang, 09210240052, 09210240086, xjhuang}@fudan.edu.cn

ABSTRACT
With the ever-increasing growth of the Internet, numerous copies of documents become serious problem for search engine, opinion mining and many other web applications. Since partial-duplicates only contain a small piece of text taken from other sources and most existing near-duplicate detection approaches focus on document level, partial duplicates can not be dealt with well. In this paper, we propose a novel algorithm to realize the partial-duplicate detection task. Besides the similarities between documents, our proposed algorithm can simultaneously locate the duplicated parts. The main idea is to divide the partial-duplicate detection task into two subtasks: sentence level near-duplicate detection and sequence matching. For evaluation, we compare the proposed method with other approaches on both English and Chinese web collections. Experimental results appear to support that our proposed method is effectively and efficiently to detect both partial-duplicates on large web collections.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Information Search and Retrieval; H.3.7 [Digital Libraries]: Collection, Systems Issues
General Terms
Algorithms, Experimentation.
Keywords
Partial-Duplicate Detection, Sequence Matching, MapReduce
1. INTRODUCTION
Because of the explosion of Internet and the fact that digital documents can be easily replicated, enormous duplicated web pages and mirrored documents cause serious problem
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

for search engine, product review, and many other Web applications. Along with the increasing requirements, nearduplicate detection has received much attentions in recent years [24, 25, 11, 26, 20].
Existing studies on near-duplicate detection usually focus on the whole document level to figure out web pages that have the same content but only differ in the framing, navigation bar, advertisements, footer, and so on. Thus there are several factors that can not be well processed by existing methods.
Collection: Figure 1 shows a pair of Web pages1 2 which both of contain the article "Droid is No. 2 in Android traffic: Admob". Besides this article, the page in Figure 1.(a) contains another nine related ones. Thus, the similarity between the pages in Figure 1.(a) and (b) is low in the document level.
Multiple-page: In order to facilitate user's browsing, some articles are divided into multiple pages. Websites may use different strategies to split articles. Moveover, a number of websites may display the article in one page according to their own styles. It also leads to the similarities between the pages are low in document level.
Threads in Forum: Millions of people contribute more than 10 gigabytes content everyday through forums, blogs and other consumer-generated mediums [21]. However, user generated content often contains a couple of sentences/pragraphs copied from news sites or other users [14]. Since the duplications are usually only a small piece of text, they can not be effectively detected by existing methods.
Besides the factors listed above, there are a number of problems like, plagiarize sentences, non-cleaned web pages, sentences/paragraphs quotation, can also be generalized to partial duplicate. If a pair of documents are partial-duplicate with each other, it means they contain a number of sentences or paragraphs with similar content. With requirements of applications such as plagiarism detection, information flow tracking, opinion mining, and so on, partial-duplicate detection task is proposed and studied in this paper. Local text reuse detection [23] can be used to partially address this task. However, we argue that only similarities and category types do not provide sufficient information for all applica-
1http://iphandroid.com/ 2http://www.chinapost.com.tw/business/companyfocus/2009/11/25/234147/Droid-is.htm

675

tions and are not convenient enough for user to easily find the duplications in dozens of lines.

Type here your search

Home

DroidisNo. 2in Andoridtraffci: Amdo b
Posted Novemb er24,2009 ­ 9:39pm in:Android Updates

MotorolaInc.'s Droid handled the snedc-o largest share of traffciamong mo bile phones equippde withGoogleInc.'s Android operating systemtwo weeks after the nhdaset's introduction, according toma rket researcherAdmo b Inc.

Share

Droid,introduced on Nov.6 to comp ete againstApple In.c's iPhone and Research InMotionLtd.'s BalckBerry,had 24 percentofallrequestsfrom Android phonesasofNov.18,according toestima tesby teh San Mateo, Calfiorni-abased resaerch firm.

That's second toHTC Corp.'sDream phone taht handled 36 percent oAfndroid traffci, according to Amdo b,which cmop iels thedatafrom requestsfor asdon its neotrwk ofmo bile Web sitesand iPhone and Android applications.

Motorolaintroduced theDroid tomeetdema nd for smart phones tahtallowuserstobrowse the

(a)

Subcrbie vaiRSS

Subcrbie via Em ail

Blac k Fr id ay Amaz in g Dea l Hu rry and check out these am azing Black Friday deals for Ch ristma s ww w.am azon.com
Make Mo ne y Fr om 3G Ap ps Profitfrom the 3G APPS trend, create apps & ma ke mo ney today www. appleipho neapps.co.uk
3G Millio na ir es Club EasilyJointhem Today CreatingSelling3G apps Huge Cash Income 3 g-appcash.co m
Yo ur Smartp ho ne = Ho ts po t Turn your Wind ow s Mob ilephone intoan internet Ho tspot.Try for free! ww w.WM WifRio uter.com

Categoreis
AndroidUpdates iPhandroid News iPhone Updates
Monthly archives
November 2009

News Opinion Taiwan Living Learn English The China Post Subscribe

RSS Feeds

" Search

Business
Updated Wednesday, November 25, 2009 11:00 am TWN, By Jason Clenfield, Bloomberg
Droid is No. 2 in Android traffic: Admob

Motorola Inc.'s Droid handled the second-largest share of traffic among mobile phones equipped with Google Inc.'s Android operating system two weeks after the handset's introduction, according to market researcher Admob Inc.

Le ar n Chin es e On li ne 1,300+ audio and video lessons Speaking practice by phone Ch inesePo d.co m

~¢...

g

 ´,,·­ ~ R ,, ... ¨,,¨

¤

www .MarsEnglihs.cn

In du ct io n Ligh ti ng SO LARA Probably themo st environm entallamp inthe wo rld www .amk o.com.tw/SOL AR A

Droid, introduced on Nov. 6 to compete against Apple Inc.'s iPhone and Research In Motion Ltd.'s BlackBerry, had 24 percent of all requests from Android phones as of Nov. 18, according to estimates by the San Mateo, California-based research firm.
That's second to HTC Corp.'s Dream phone that handled 36 percent of Android traffic, according to Admob, which compiles the data from requests for ads on its network of mobile Web sites and iPhone and Android applications.

Motorola introduced the Droid to meet demand for smart phones that allow users to browse the Internet, send e-mails and download software, the fastest-growing part of the mobile-phone industry.

(b)

Global Markets Asia Americas Europe Middle East Africa
Company Focus Breaking News Updated Wednesday, November 25, 2009 11:40 am TWN
Toyota said to plan moving some U.S. jobs from Calif. Cadbury shares rise on report of Nestle interest

Figure 1: Examples of partial-duplicate web pages

In this paper, we present an efficient algorithms for detecting partial-duplicates and locating their positions. Figure 2 shows an example on partial-duplicates. As shown in the graph, a sequence of sentences in Page A are similar with a number of sentences in Page B. Page A and C also contains duplicated text. From these pairs, we try to get the following results:
· Page A (Seni to Senj) Page B (Senk to Senl).
· Page A (Senm to Senn) Page C (Senp to Senq).
Since the proposed method can not only detect duplicates but also locate their positions, the near-duplicates of the whole document level can also be precisely detected. As the Web collections contain hundreds of millions pages, the algorithm is explored with MapReduce [8], which is a framework for large-scale distributed computing. We implement our method and compare it with the state-of-the-art approaches on four web collections and one manually constructed evaluation corpus. The experimental results show that it achieves good performance, both effectiveness and efficiency are significantly improved.
The contributions of this work are as follows: 1) We convert the partial-duplicate detection task into sentence level near-duplicate detection task and sequence matching task. 2) In order to handle hundreds of millions documents, the algorithm is designed and implemented under the MapReduce framework. 3) Shingles, I-Match, and Spotsigs are compared and evaluated in experiments, and experimental analyses of the signatures for sentences are provided. 4) Evaluations on manually labeled "Oracle Set" and four large web collections are used to measure the effectiveness and efficiency.
The remaining of the paper is organized as follows: In section 2, we review a number of related work and the stateof-the-art approaches in related areas. Section 3 provides an brief introduction of MapReduce. Section 4 presents the proposed method. Experimental results in test collections

2=CA)

2=CA*

2=CA+

Figure 2: Partial-duplicate content

and analyses are shown in section 5. Section 6 concludes this paper.
2. RELATED WORK
Near-duplicate detection has received considerable attentions over the past several years. Previous studies on duplicate and near-duplicate detection can be roughly divided into two research directions: document representation and efficient detection. The first one focuses on representing documents with or without linguistic knowledge. Since collection contains hundreds of millions of documents, the second one, efficiency, has also received lots of attentions. This section introduces related approaches briefly.
Broder [3] defined the resemblance and containment between two documents. He used shingles to represent documents and Jaccard overlap to calculate the similarity between documents. In order to reduce the complexity of shingling, Broder [4] proposed to use meta-sketches for this task.
Indyk and Motwani[15] proposed the notion of localitysensitive hashing and applied it to sublinear-time similarity searching. LSH maintains a number of hash tables, which each of is parameterized by the number of hashed dimensions. Points close to each other in some metric space have the same hash value with high probability. Gionis et al. [11] also used LSH for approximate similarity search.
I-Match [7] hinges on the premise that removal of very infrequent terms and very common terms results in good document representations for the near-duplicate detection task. They filter the input document based on collection statistics and compute a single hash value for the remainder text. The documents with same hash value are duplicates.
Schleimer et al. [22] proposed a local document fingerprinting algorithm, which is called winnowing. They described and analyzed the winnowing algorithm for selecting fingerprints from hashes of k-grams. They also presented the complexity of any local document fingerprinting algorithm and gave the non-trivial lower bound.
Henzinger [13] performed an evaluation of Border et al.'s [4] shingling and Charikar's [6] random projection near-duplicate algorithms on 1.6B web pages. The results showed that neither of the algorithms works well for detecting near-duplicate pairs on the same site, while both of them achieve high precision for near-duplicate pairs on different sites.
Manku et al. [19] proposed an approach for both online and batch types near-duplicate detection. They used Charikar's fingerprinting technique [6] and demonstrated it's effectiveness. They also presented an algorithmic technique for identifying existing f-bit fingerprints that differ from a given fingerprint in at most k bit-positions, for small k.
Theobald et al. [26] presented their work SpotSigs, which combine stopword antecedents with short chains of adjacent content terms. Through demonstrating the upper bounds of Jaccard similarity, they also proposed several pruning conditions, which could ignore all pairs of documents safely during

676

the matching process when SpotSig vectors exceed a certain difference in length.
Besides the approaches focused on Web pages or documents, Muthmann et al. [20] proposed their work to identify threads with near-duplicate content and to group these threads in the search results. They incorporated text-based features, features based on extracted entities for products, and structure-based features to capture the near-duplicate threads.
Local text reuse detection proposed by Seo and Croft [23] is also related to our method. Different from duplicate detection, text reuse tries to capture the loose restatements of the information from the previous sources [2]. They defined six categories of text reuse and a general framework for text reuse detection. Several fingerprinting techniques for the framework were evaluated under the framework.
Lin [18] explored the problem of pairwise similarity on large document collections and introduced three MapReduce algorithms to solve this problem, which are based on brute force, large-scale ad hoc retrieval, and the Cartesian product of postings lists. Different with us, the granularity of this work is also document level.
Kolak and Schilit [16] described an approach to mine popularly quoted passages and add links among them on a digital library. They use shingle table method to find repeated sequences between different books. Since the storage complexity of shingle methods is huge and extracting shared shingles is timing consuming tasks, the method can not be directly used for partial-duplicate detection task.
In order to handle hundreds of millions web collections, we also use MapReduce framework in this work, which is introduced by Dean and Ghemawat [8]. It is used an associated implementation for processing and generating large data sets. The MapReduce programming model has been successfully used at Google for many different purposes.
3. MAPREDUCE
As number of data such as web pages, web request logs, and so on grows rapidly, applications have to be distributed across thousands of machines in order to finish in time. Bulk-synchronous parallel (BSP) model [27] and some higherlevel abstractions(MPI [12]) have been supported programmers to write parallel programs. However, because of its higher-level abstractions, programmers usually spend too much time on details. MapReduce [8], which is difference from these systems, exploits a restricted programming model to parallelize the user program automatically. And the transparent fault-tolerance and load balancing are also provided, because of the restrictions.
The key concept behind MapReduce is inspired by the map and reduce primitives present in many functional languages. Dean and Ghemawat [8] presented the observation that most information processing computations share the same two-stage structure, which contains map and reduce operations. The map operation is applied to every logical "record" of input to compute a set of intermediate key/value pairs. Then the reduce operation is applied to all the values that shared the same key, in order to combine the derived data. Figure 3 shows the two-stage structure.
Under this framework, the computation takes a set of input key/value pairs, and produces a set of output key/value pairs. A programmer only needs to implement two opera-

input input input input

map map map map

key-value pairs
key-value pairs
key-value pairs
key-value pairs

Group &
Sort

reduce reduce reduce

output output output

Figure 3: The basic structure of MapReduce

tions: map and reduce. The intermediate key/value pairs will be grouped and sorted by the key automatically.
Many different implementations of MapReduce interface are available now. Google's MapReduce implementation is coupled with Google File System (GFS) [10], a kind of distributed file system. Apache's MapReduce implementation, Hadoop3, which follows the same architecture, uses a distributed file system named Hadoop Distributed File System (HDFS) to store data and the intermediate results. Hadoop tries to schedule the MapReduce computation tasks to the node where the data locates in order to reduce the overall network I/O. Besides Hadoop, MapReduce has also been implemented by many corporations, such as Greenplum, GridGain, Cell Broadband Engine, and so on.
In this paper, we implement our algorithms under the open-source implementation Hadoop 0.20. HDFS is used to provide the distributed storage.
4. OUR APPROACH
A partial-duplicate is a pairwise relationship. Given a pair of documents, we need to identify and locate the duplicated parts between them. To make questions simple, we limit granularity to sentence level. Based on this assumption, we propose the algorithm PDC-MR, which converts the partial-duplicate detection task into three MapReduce jobs (illustrated in Figure 4 and Figure 5).
1) Indexing: We use a MapReduce job to build a standard inverted index [9] for collections. Signatures used as terms in the inverted index are extracted from each sentences in map procedure. The map procedures emit the signature as the key, and a tuple consists of the document id and sentence id. After grouping and sorting, the reduce procedures take the tuples as input and write out the inverted index to the disk. Since signatures would highly impact the final result, a detail description of it will be given in the Section 4.1.
2) Sentence Duplication Detecting: Jaccard coefficient is used to measure the similarities between sentences. If the Jaccard similarity between a sentence pair is over a threshold, they are considered duplicates. Another MapReduce job is used to detect the sentence duplicates. The map procedures read the inverted index from disks and emit a pair of sentences which both contain a same signature as the key. After grouping and sorting, all signature ids belonging to the same sentence pair are brought together. The reduce procedures take them as inputs, and emit the sentence duplications. The procedure is shown in the right of Figure 4.
3http://hadoop.apache.org/

677

d 1

map

d 2

map

d 3

map

ds 11
ds 12
ds 13
ds 14

f ff 1 23
ff 3 4
fff 1 4 8
f 2

ds 21
ds 22
ds 23

ff 5 6
f ff 2 37
ff 1 4

ds 31
ds 32
ds 33
ds 34

f 8
f ff 3 79
ff 3 4
ff 2 10

Group &
Sort

f

ds ds 

1

11 13

f

ds ds 

2

11 14

f

ds ds 

3

11 12

f

ds ds 

4

12 13

f ds

5

21

f ds

6

21

f ds ds

7

22 32

f ds ds

8

31 13

f ds

9

32

f ds

10

34

reduce reduce reduce reduce reduce

f ds ds 

1

11 13

f

ds ds 

2

11 14

f

ds ds 

3

11 12

f

ds ds 

4

12 13

f ds

5

21

f ds

6

21

f ds ds

7

22 32

f ds ds

8

31 13

f ds

9

32

f ds

10

34

Indexing

map map map map map map map map map map

(d s d s ) 11 13
(d s d s ) 11 22
(d s d s ) 13 22
(d s d s ) 11 14
(d s d s ) 11 34
(d s d s ) 14 34
(d s d s ) 11 12
(d s d s ) 11 22
(d s d s ) 11 32 


Group &
Sort

(d s d s ) 1 11 12
(d s d s ) 1 11 13
(d s d s ) 1 11 14
(d s d s ) 0 11 21
(d s d s ) 2 11 22
(d s d s ) 1 11 23
(d s d s ) 0 11 31
(d s d s ) 1 11 32
(d s d s ) 1 11 33
(d s d s ) 1 11 34




reduce reduce

Sentence Duplication Detecting

Figure 4: Detecting sentence duplication of a toy collection of 3 documents.

d 2
d 1
d 3
d 1
d 3
d 2

dj
S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S1
S2
S3
S4
di S5
S6
S7
S8
S9
Figure 5: The sequence matching strategy
3) Sequence Matching: With the results of sentence duplicate detection, matrixes representing sentence duplicates for each pair of documents are generated. Figure 5 shows an example of the sentence duplicates between page di and page dj. The dot plots in the figure represents duplicated sentence pairs. The sequences of duplicated sentences are partial duplications we try to extract and locate. Based on that, the problem can be straightly converted to the sequence matching task, which aims to find all diagonals in the matrix. We also use a MapReduce job to do that. The outputs of the job include partial duplicates among documents and their locations. Since numerous of document pairs are needed to be processed, Section 4.3 gives detail descriptions about the efficient sequence matching method.
4.1 Signatures
As described in the Section 2, a number of signature extraction methods have been proposed for document level near-duplicate detections. Since the average number of words per sentence is much fewer than document, we introduce several signature methods in this section.
4.1.1 Shingles
Shingles is the simplest method, which is proposed by Broder et al. [5]. It tokenizes documents into a list of words and extracts all word sequences of adjacent words to represent the document. "n-shingles" represents the number of n adjacent words in a shingle. As the shingles uses all chunks, it might not be suitable for large collections because of too many signatures.

4.1.2 I-Match
I-Match [7] uses SHA1 hash function over concatenation of terms filtered by stopwords and infrequent terms. It hinges on the assumption that removal of very infrequent terms and stop words results in good document representations for the near-duplicate detection task. Although the computationally of I-Match is attractive, it usually unstable even to small perturbations of content.

4.1.3 SpotSigs
SpotSigs [26] combines stopword antecedents with short chains of adjacent content terms. A spotsig si of a location in a document consists of a chain of words that follow an antecedent word ai at a fixed spot distance di. Antecedent words are predefined and typically chosen to be stop words. Experimental results in [26] show that SpotSigs with five common terms as antecedent achieve better result than a full stopword list. However, we observe that signatures can not be extracted from more than 15.2% sentences in English collection with the five common terms. The experimental results about selecting the number of antecedents are shown in Section 5.3.

4.2 Sentence Duplication Detection
As shown in Figure 4, the sentence duplicate detection algorithm, which is implemented by a MapReduce job, extracts near duplicated sentence pairs whose Jaccard similarity are higher than a threshold. Sentences are represented by a group of signatures. The upper bounds for Jaccard similarity [26] is

J(A, B)

=

|A |A

B| B|



min (|A|, |B|) max (|A|, |B|)

(1)

For |A|  |B|, we can get:

J(A, B)



|A| |B|

(2)

With the upper bound and vector representation of docu-

ments, we observe that only similar length sentence pairs

can be near duplicate. If we set the threshold to  , sen-

tence

pairs

where

|A| |B|

  can be safely removed.

Based

on that, the pseudo-code of this method is show in Algo-

rithm 1. The input of the procedure map is the signature

id (sigi) and associated postings list ([d1s1, d2s2, ...], where disj represents document id and sentence id). Inside each mapper, all candidate sentence pairs, which follow the upper

678

bound of the Jaccard similarity, are emitted to the key-value pair ( disj, dksl , sigi). After grouping and sorting, all signature ids belonging to the same sentence pair are brought together. With the list, Jaccard similarity can be easily calculated. The procedure reduce takes the sentence pair and corresponding list as input and emit the duplication judgments based on the Jaccard similarity and predefined threshold  .

Algorithm 1 Pseudo-code of sentence duplication detection algorithm in MapReduce

MAP(sigi, [d1s1, d2s2, ...])

1: for all disj  [d1s1, d2s2, ...] do

2: for all dksl  [d1s1, d2s2, ...] do

3:

if disj = dksl then

4:

if

(

|disj |  |dksl|

and

|dk sl| |disj |



)

or

(

|disj |  |dksl|

and

|disj | |dk sl|



)

then

5:

EMIT( disj , dksl , sigi)

6:

end if

7:

end if

8: end for

9: end for

REDUCE( disj , dksl , [sig1, sig2, ...])

1: if

|di sj |di sj

dk sl| dk sl|

<

then

2: EMIT( di, dk , sj , sl )

3: end if

4.3 Sequence Matching
As described in the previous sections, the sequence matching procedure aims to find all diagonals in the matrix. Algorithm 2 shows the pseudo-code of the MapReduce job. Inputs to the procedure map consists document pairs (keys, di, dj ) and a corresponding list of duplicated sentence pairs between these documents (values, [ sk, sl , sp, sq , ...]). For each duplicated sentence pair, the longest diagonal whose root is the pair is extracted and emitted. Extracted sentence pairs will be eliminated.  is used as the threshold for the diagonal length. The final output, which contains document pair, respective start positions, and length, are generated in the procedure reduce. In practical, the reducer can also be merged into the mapper to trim the intermediate data.
5. EXPERIMENTS
5.1 Collections
We evaluate our methods with four corpora WT10g, TREC Blogs064, SogouT 2.05, and ClueWeb09-T09B6. Table 1 shows the statistics of the four collections. WT10g is used by TREC Web tracks, which contains more than 1.6 million documents collected from about 11,000 servers. Besides that, BLOGS06 corpus, which is used by TREC 2006 and TREC 2007 blog tracks, is also selected to evaluate systems. It is a big sample of the blogsphere, and contains more than 3.2 millions documents including spam as well as
4http://ir.dcs.gla.ac.uk/test collections 5http://www.sogou.com/labs/dl/t.html 6http://boston.lti.cs.cmu.edu/Data/clueweb09/

Algorithm 2 Pseudo-code of sequence matching algorithm in MapReduce

MAP( di, dj , [ sk, sl , sp, sq , ...])

1: P  [ sk, sl , sp, sq , ...]

2: 3:

forDall DsiIsAj GinOPNAdLoEXTRACT(sisj )

4: if |D| >  then

5:

EMIT( di, dj , D)

6:

P P -D

7: end if

8: end for

DIAGONALEXTRACT(sisj )
1: while sisj in P do 2: D  D sisj 3: si  si+1 4: sj  sj+1 5: end while

REDUCE( di, dj , [D1, D2, ...])
1: for all D  [D1, D2, ...] do 2: EMIT( di, dj , Start.di, Start.dj, D.Length ) 3: end for

possibly non-blogs. SogouT 2.0 corpus is made up of 24.8M Chinese Web pages and crawled from all domains. TREC Category B dataset(ClueWeb09-T09B), which is a subset of the ClueWeb09, contains 50 million English pages and has been used in various TREC tracks.

Table 1: Statistics of the evaluation corpora

Corpus WT10g Blogs06 SogouT 2.0 ClueWeb09-T09B

Language English English Chinese English

#Docs 1,692,096 3,215,171 24,833,521 50,220,423

Size 11GB 88.8GB 372.5GB 490.4GB

5.2 Implementation and Setup
All the MapReduce jobs were implemented in Java for Hadoop framework. HDFS was used to provide the distributed storage. All experiments were evaluated on a 16 machines cluster. Each machine contains two Xeon quad core CPUs (2.0GHz), and 32GB RAM. Software stack of the experiments used Java 1.6 and Hadoop version 0.20. For web page cleaning, we just removed all HTML markup tags from the collections. Since the impact of sentence boundary detection's performance would not be heavy and a number of manually written rules can achieve good result [1] with little attractive computational consumption, we used around 50 rules to do that in our experiment.
5.3 Comparison of Signatures
In order to compare the performances of different signatures, we manually select 2000 documents, which contain 57135 sentences totaly, from ClueWeb09-T09B (Oracle Eng is used to represent the corpus in the following section for simple). For Chinese corpus SogouT, we also constructed a manually labeled corpus (Oracle Chn), which contains

679

F1 Score F1 Score
F1 Score
5 10 20 30 40 50 60 70 80 90 100 200 500
F1 Score
10 100 500 1K 2K 5K 10K 15K 20K

Oracle Chn
1

0.9

0.8

0.7

0.6

0.5

0.4

2-Shingles

0.3

3-Shingles

4-Shingles 0.2

0.1 1

0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1


Oracle Eng
1

0.9

0.8

0.7

0.6

0.5

0.4

2-Shingles

0.3

3-Shingles

4-Shingles 0.2

0.1 1

 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1

Figure 6: Shingles' performances of varying the threshold  for corpora Oracle Eng and Oracle Chn

F1 Score F1 Score

Oracle Chn
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0

Oracle Eng
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0

0.0-1.0 0.1-1.0 0.2-1.0 0.3-1.0 0.4-1.0 0.5-1.0 0.6-1.0 0.7-1.0 0.8-1.0 0.9-1.0 0.0-1.0 0.1-1.0 0.2-1.0 0.3-1.0 0.4-1.0 0.5-1.0 0.6-1.0 0.7-1.0 0.8-1.0 0.9-1.0

IDF

IDF

Figure 7: I-Match' performances of varying IDF for corpora Oracle Eng and Oracle Chn

80516 sentences extracted from 2000 documents. Six in-

dividuals were asked to label them. The average Kappa

statistic among them is around 91.6%, which shows good

agreement.

Figure 6 shows the performances comparison of 2-Shingles,

3-Shingles, and 4-Shingles. We observe that 4-Shingles con-

sistently performs better than 2-Shingles and 3-Shingles in

both English and Chinese collections. Different with results

in document level [18, 26], threshold  = 0.9 achieves the

best performance in both of the collections. The reason is

that around 91% of duplicated sentences in Oracle Chn and

89% of them in Oracle Eng are exactly same with each other

in our evaluation collections. However, this kind of factor is

rare in the document level.

Figure 7 shows the performances of I-Match with different

IDF ranges. Tokens exceeding IDF range were filtered. We

use

idfi

=

log(N/dfj ) log(N )

to

calculate

the

IDF

value

for

token

i,

where N is the corpus size, dfj is the document frequency

of the token. Since the similarities calculated by I-Match

are either 0 or 1, the threshold  does not need to adjusted.

The best result is achieved by [0.1, 1.0] in both Oracle Chn

and Oracle Eng. It means that most of the tokens should

be kept and used to calculate the hash result. The main

reason is that sentences usually contain a small number of

tokens and most of the duplicated sentences are same with

each other. When tokens whose IDF is lower than 0.4 are

filtered, most of the sentences have less than 2 tokens left

in Oracle Chn. Because of that, the recall for [0.4, 1.0] is

almost perfect 100%, but the precision is only 1.4%.

The impacts of the number of antecedents for Spotsig are

shown in Figure 8. The x-axis represents the number of an-

tecedents and varies from 5 to 500 in Oracle Chn and 10 to

20K in Oracle Eng. The numbers below each point represent

the average number of signatures per sentences with corre-

sponding antecedents. It shows that the antecedents' num-

Oracle Chn
1
3.8 4.2 4.5 4.8 5.0 6.9 9.0 0.9

2.6 3.0 3.5

0.8

2.1 1.4

0.7 0.9
0.6

# Antecedent

Oracle Eng

1

0.9

3.4 3.7 3.8 3.9

0.8

3.0

0.7

2.6

0.6

2.2

0.5 0.7

1.5

0.4

0.3

0.2

0.1

# Antecedent

Figure 8: Spotsigs' performances of varying the number of antecedents for corpora Oracle Eng and Oracle Chn

F1 Score

Oracle Chn
1

0.9

0.8

0.7

0.6

0.5

0.4

# Antecedent = 5

0.3

# Antecedent= 60

# Antecedent = 100

0.2

0.1 1

 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1

F1 Score

Oracle Eng
1

0.9

0.8

0.7

0.6

0.5

0.4

# Antecedent = 1K

0.3

# Antecedent = 5K

# Antecedent= 10K

0.2

0.1 1

 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1

Figure 9: Spotsigs' performances of varying the threshold  for corpora Oracle Eng and Oracle Chn

ber would highly impact the performance. We think that the main reason is that sentences cannot be well represented by a small number of signatures. By trading-off between efficiency and effectiveness, we choose # antecedent = 60 to achieve 95.2% F1 score in Chinese collection. For English one, we choose # antecedent = 10K. We observe that the number of antecedents is much different between English and Chinese collections. However the best results are both achieved at the similar average number of signatures per sentence. It shows that a sentence can be well described by around 4 signatures. Figure 9 shows the performances with different thresholds. Comparing with shingles, spotsigs show the similar trends. We achieve the best result with  = 0.9 in Oracle Chn and Oracle Eng.
In summary, 4-Shingles achieve the best result in the sentence level duplicate detection. However, the performances of 2-Shingles, 3-Shingles, Spotsigs, and I-Match are comparable. The parameters used for sentence level are much different with document level ones. We think that it is caused by the characters of sentence collections, such as length, standard for labeling and so on. We also observe that although all three signature extraction methods are highly tunable, the results prove to be robust for a large variety of parameters.
5.4 Effectiveness Evaluation
After evaluating three different methods to extract duplicated sentences, we now consider the impact of sequence matching. Table 2 summaries the sequence matching results with different signatures. The configurable parameters IDF range, similarity threshold  , and # antecedent are selected by the previous experiments and listed in the brackets. We use Precision, Recall, and F1-Score as our choice of evaluation metric to measure how accurately the dupli-

680

Table 2: Summary of sequence matching results with Shingles, I-Match and Spotsigs for Oracle sets

Corpus Oracle Chn
Oracle Eng

Signature
2-Shingles( = 0.9) 3-Shingles( = 0.9) 4-Shingles( = 0.9) I-Match(IDF=[0.1,1.0]) Spotsigs(#A=60,  = 0.9)
2-Shingles( = 0.9) 3-Shingles( = 0.9) 4-Shingles( = 0.9) I-Match(IDF=[0.1,1.0]) Spotsigs(#A=10K,  = 0.9)

P
0.936 0.937 0.942 0.935 0.938
0.987 0.987 0.987 0.985 0.981

R
0.937 0.937 0.942 0.938 0.930
0.966 0.966 0.967 0.960 0.965

F1
0.936 0.937 0.942 0.937 0.934
0.977 0.977 0.977 0.972 0.973

cation is located. From analyzing the Oracle collections, we observe that lengths of most duplications are bigger than three. Hence, , which is the threshold of diagonal length, is set to 3 in all the experiments. We observe that the final results are heavily related to the performances of sentence duplicate detection. Since the performances of 2-Shingles, 3-Shingles, 4-Shingles, I-Match and Spotsigs are similar, the final F1-scores do not have significant difference. In order to evaluate the impact of , we also evaluate the performances at  = 1. In Oracle Eng, the F1-score of 2-Singles is only 0.952, which is significantly 7 different from the results shown in the Table 2. 3-Shingles, 4-Shingles, I-Match and Spotsigs have the similar results. By trading-off efficiency and effectiveness, we determine to use I-Match method to extract signatures in our method.
Figure 10 summarizes our results of PDC-MR versus the document level near-duplicate detection. For convenient comparison among copra, the top one million documents of each corpus are used in this experiment. For document level near-duplicate detection, the state-of-the-art method Spotsig is used, whose parameters are set up based on [26]. The y-axis represents the number of unique documents. The bottom parts of each bar represent results of Spotsig. The top parts represent the number of documents which can be detected by our PDC-MR method but can not be detected by document level Spotsig. In WT10g, Spotsig extracts around 31K documents which contain duplications in the same corpus. They compose more than 1.89 million duplication pairs. Besides those documents, through our method, another 94K documents which contain partial-duplicates are detected. In Blogs06, ClueWeb09-T09B, and Chinese corpus SogouT2.0, we get similar results. It shows that partial-duplications are common in web collections and our proposed method can effectively detect them.
In order to evaluate the validity of the extracted partial duplicates, we random select 200 documents from the detection results of each corpus and manually classify them into four types as listed in the Table 3. "News Collection" and "Multiple Page" are described in the Section 1. "Partial Quotation" represents all types of short piece of text quotation. Banner, copyright notice, navigation bar, and other non-content parts are classified into "Other". The results show that "Partial quotation" account for the majority of all instances. The average length of this kind of duplications
7The paired  -test (<0.05) is used to measure the significance.

Table 3: Partial duplicates in the web collections

Corpus
WT10g Blogs06 SogouT 2.0 ClueWeb09-T09B

News Collection
4% 2.5% 10% 3%

Multiple Page
8.5% 5% 18% 7%

Partial Quotation
80% 79% 60% 58%

Other
7.5% 13.5% 12% 32%

140K 120K 100K
80K 60K 40K 20K
0K

21,397 31,486 WT10g

70,930

12,261

44,862

58,134

60,814

13,657

Blogs06

SogouT 2.0

Spotsig(Doc Level)

PDC-MR

ClueWeb09-T09B

Figure 10: Summary of PDC-MR vs. document level Spotsig in four web collections

is around 6 sentences. While the average length of document is more than 23 sentences in SogouT 2.0 and 26 sentences in WT10g. Thus those partial duplications can not be easily detected by the existing document level detection methods. The results show that most of extracted partial duplications are useful and meaningful. Except ClueWeb09T09B, the percentages of "Other" type in other collections are less than 15%. While, there are 32% instances belonging to this type in ClueWeb09-T09B. We think the main reason is that ClueWeb09-T09B is not well cleaned and contains lots of advertisements.
5.5 Efficiency Evaluation
Figure 11 plots the running times of spotsigs based nearduplicate detection and our proposed PDC-MR method for different corpus size. ClueWeb09-T09B is used in this experiment. All Hadoop jobs in the efficiency experiments were configured with 60 mappers and 60 reducers. The graph suggests that although the number of sentence is huger than the number of documents, our proposed method is more efficient than Spotsig. We think that it makes sense since I-match is efficient and its performance is also comparable in sentence level.
6. CONCLUSIONS
This paper presents our work on partial-duplicate detection task. A number of factors like news collection, multiple pages, threads in forums, plagiarize sentences, non-cleaned web pages, and sentences/paragraphs quotation belong to it. In order to address this problem, we propose a novel MapReduce algorithm, which converts the task into three MapReduce jobs. Except for the similarities between documents, the algorithm can simultaneously output the positions where the duplicated parts occur. The contributions of the work include both empirical analysis of signatures for

681

Runing TIme (seconds )

16K

14K 12K

Spotsig(Doc Level) PDC-MR

10K

8K

6K

4K

2K

0K

1

2

3

4

5

# DOC (millions)

Figure 11: Running time of the PDC-MR and Spotsig with different corpus size

sentence and algorithm design. Experimental results in four real-world web collections show that the proposed method can be effectively and efficiently used to detect partial- and near-duplicate.
7. ACKNOWLEDGMENTS
The author wishes to thank the anonymous reviewers for their helpful comments. This work was partially funded by 973 Program (2010CB327906), Shanghai Leading Academic Discipline Project (B114), Doctoral Fund of Ministry of Education of China (200802460066), and Shanghai Science and Technology Development Funds (08511500302).
8. REFERENCES
[1] J. Aberdeen, J. Burger, D. Day, L. Hirschman, P. Robinson, and M. Vilain. Mitre: description of the alembic system used for muc-6. In Proceedings of MUC6, pages 141­155, Morristown, NJ, USA, 1995.
[2] M. Bendersky and W. B. Croft. Finding text reuse on the web. In WSDM '09, pages 262­271, New York, NY, USA, 2009. ACM.
[3] A. Z. Broder. On the resemblance and containment of documents. In Proceedings of SEQUENCES 1997, page 21, Washington, DC, USA, 1997. IEEE Computer Society.
[4] A. Z. Broder. Identifying and filtering near-duplicate documents. In Proceedings of COM 2000, pages 1­10, London, UK, 2000.
[5] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. Syntactic clustering of the web. Comput. Netw. ISDN Syst., 29(8-13):1157­1166, 1997.
[6] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of STOC 2002, pages 380­388, New York, NY, USA, 2002. ACM.
[7] A. Chowdhury, O. Frieder, D. Grossman, and M. C. McCabe. Collection statistics for fast duplicate document detection. ACM Trans. Inf. Syst., 20(2):171­191, 2002.
[8] J. Dean and S. Ghemawat. Mapreduce: Simplified data processing on large clusters. In Proceedings of OSDI 2004, San Francisco, CA, USA, 2004.
[9] W. B. Frakes and R. A. Baeza-Yates. Information Retrieval: Data Structures & Algorithms. Prentice-Hall, 1992.

[10] S. Ghemawat, H. Gobioff, and S.-T. Leung. The google file system. SIGOPS Oper. Syst. Rev., 37(5):29­43, 2003.
[11] A. Gionis, P. Indyk, and R. Motwani. Similarity search in high dimensions via hashing. In VLDB '99, pages 518­529, San Francisco, CA, USA, 1999.
[12] W. Gropp, E. Lusk, and A. Skjellum. Using MPI: portable parallel programming with the message-passing interface. MIT Press, Cambridge, MA, USA, 1994.
[13] M. Henzinger. Finding near-duplicate web pages: a large-scale evaluation of algorithms. In SIGIR '06, pages 284­291, New York, NY, USA, 2006. ACM.
[14] S. C. Herring, L. A. Scheidt, I. Kouper, and E. Wright. A longitudinal content analysis of weblogs: 2003-2004. Blogging, Citizenship and the Future of Media, pages 3­20, 2006.
[15] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In STOC '98, pages 604­613, New York, NY, USA, 1998. ACM.
[16] O. Kolak and B. N. Schilit. Generating links by mining quotations. In Proceedings of HT 2008, pages 117­126, New York, NY, USA, 2008. ACM.
[17] A. Kolcz, A. Chowdhury, and J. Alspector. Improved robustness of signature-based near-replica detection via lexicon randomization. In Proceedings of SIGKDD 2004, pages 605­610, New York, NY, USA, 2004. ACM.
[18] J. Lin. Brute force and indexed approaches to pairwise document similarity comparisons with mapreduce. In Proceedings of SIGIR '09, pages 155­162, New York, NY, USA, 2009. ACM.
[19] G. S. Manku, A. Jain, and A. Das Sarma. Detecting near-duplicates for web crawling. In WWW '07, pages 141­150, New York, NY, USA, 2007. ACM.
[20] K. Muthmann, W. M. Barczyn´ski, F. Brauer, and A. L¨oser. Near-duplicate detection for web-forums. In IDEAS '09, pages 142­151, New York, NY, USA, 2009. ACM.
[21] R. Ramakrishnan and A. Tomkins. Toward a peopleweb. Computer, 40(8):63­72, 2007.
[22] S. Schleimer, D. S. Wilkerson, and A. Aiken. Winnowing: local algorithms for document fingerprinting. In SIGMOD '03, pages 76­85, New York, NY, USA, 2003. ACM.
[23] J. Seo and W. B. Croft. Local text reuse detection. In SIGIR '08, pages 571­578, New York, NY, USA, 2008. ACM.
[24] N. Shivakumar and H. Garcia-Molina. Scam: A copy detection mechanism for digital documents. In Digitial Library, 1995.
[25] N. Shivakumar and H. Garcia-Molina. Finding near-replicas of documents and servers on the web. In Proceedings of WebDB 1998, pages 204­212, London, UK, 1999. Springer-Verlag.
[26] M. Theobald, J. Siddharth, and A. Paepcke. Spotsigs: robust and efficient near duplicate detection in large web collections. In SIGIR '08, pages 563­570, New York, NY, USA, 2008. ACM.
[27] L. G. Valiant. A bridging model for parallel computation. Commun. ACM, 33(8):103­111, 1990.

682

Discriminative Models of Integrating Document Evidence and Document-Candidate Associations for Expert Search

Yi Fang
Department of Computer Science Purdue University
West Lafayette, IN 47907, USA
fangy@cs.purdue.edu

Luo Si
Department of Computer Science Purdue University
West Lafayette, IN 47907, USA
lsi@cs.purdue.edu

Aditya P. Mathur
Department of Computer Science Purdue University
West Lafayette, IN 47907, USA
apm@cs.purdue.edu

ABSTRACT
Generative models such as statistical language modeling have been widely studied in the task of expert search to model the relationship between experts and their expertise indicated in supporting documents. On the other hand, discriminative models have received little attention in expert search research, although they have been shown to outperform generative models in many other information retrieval and machine learning applications. In this paper, we propose a principled relevance-based discriminative learning framework for expert search and derive specific discriminative models from the framework. Compared with the state-ofthe-art language models for expert search, the proposed research can naturally integrate various document evidence and document-candidate associations into a single model without extra modeling assumptions or effort. An extensive set of experiments have been conducted on two TREC Enterprise track corpora (i.e., W3C and CERC) to demonstrate the effectiveness and robustness of the proposed framework.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval; H.3.4 Systems and Software
General Terms
Algorithms, Design, Experimentation
Keywords
Expert search, enterprise search, discriminative models
1. INTRODUCTION
With vast amount of information available within large organizations, the key challenge is to harness existing knowledge and expertise in a timely and effective manner. In consequence, enterprise information retrieval systems are increasingly demanded to return people with specific knowledge and skills in response to a user's query. A class of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

vertical search engines known as expert finder have emerged for enterprise organizations.
As an important IR application, expert search (also known as expert finding) has received substantial attention in the IR research community. Rapid progress has been made in modeling and evaluation since the launch of TREC Enterprise Track in 2005 [12]. A notable observation is that probabilistic generative models have dominated the literature of expert search. In particular, many statistical language modeling techniques were proposed to model the relationship between a candidate expert and a query. These models usually characterize a generative process of how a query is generated from supporting documents of an expert. The key ingredient in these methods is to determine associations between people and documents because the associations are ambiguous in the TREC scenarios as well as in many realistic settings. Previous works have investigated different metrics or a combination of them to measure the associations, but the way of choosing or combining them is rather often heuristic and lacks of a clear justification. Furthermore, document evidence such as document or expert authority information, internal and external document structures, global evidence and so on is shown to be able to significantly improve expert retrieval performance, but to incorporate these features often requires many modeling assumptions and is often unwieldy.
On the other hand, discriminative models, another important class of probabilistic models with solid statistical foundation, are nearly absent in the research of expert search, especially on the TREC evaluations. In fact, discriminative models have been preferred over generative models in the recent past in many machine learning applications, partly because of their attractive theoretical properties. In the domain of IR, various discriminative models have also been applied to many retrieval problems (e.g., [23]). However, very limited research has been conducted to design discriminative models for expert search.
In this work, we present a relevance-based discriminative learning framework for expert search and derive specific discriminative models from the framework. Similar to some prominent language models, the proposed models aggregate document evidence and document-candidate associations through supporting documents. Unlike the language models, we directly model the conditional probability of relevance given a query and an expert. As a result, heterogeneous or even arbitrary features can be naturally included into a single model. The parameters associated with the features are automatically learned from training data. We

683

report an extensive set of experiments on two TREC corpora to evaluate the effectiveness and robustness of the proposed discriminative framework.
The next section discusses related work. Section 3 introduces the state-of-the-art generative language models for expert search. Section 4 presents our proposed approaches. In section 5, we discuss the advantages of discriminative models in the context of expert search. Section 6 explains our experimental methodology and Section 7 presents the experimental results. Section 8 concludes and points out some future work.
2. RELATED WORK
The early work on expert finding systems was initiated in the Knowledge Management community, usually in the form of yellow pages [9]. These systems relied on experts to judge and input their skills by themselves against a predefined set of keywords, and thus the task was time-consuming. More recent techniques locate experts in an automatic fashion. An overview of early automatic expert finding systems is provided in [36]. The task of expert search has received a significant amount of attention as the task had been included in the TREC Enterprise track from 2005 to 2008 [12, 32, 1, 7]. The TREC Enterprise tracks provided a common platform for researchers to empirically evaluate methods for expert search. They demonstrated the feasibility of expert search on heterogeneous data collections. In the TREC corpora, the relationship between documents and experts is ambiguous and thus to model the document-candidate associations is a key issue in expert search research.
Most of the recent work on expert search generally falls into two categories: profile-centric and document-centric approaches. Balog et al. [3] formalizes the two methods by proposing two generative language models. Their Model 1 directly models the knowledge of an expert from associated documents, which is equivalent to a profile-centric approach, and their Model 2 first locates documents on the topic and then finds the associated experts, which is a documentcentric approach. It has been shown in [3] that Model 2 is generally more effective than Model 1 and since then it becomes one of the most prominent language models for expert search. In [8], a two-stage language model combining a document relevance and co-occurrence model is proposed, which is essentially equivalent to Model 2. An attempt to further improve their models is made by proposing a proximitybased document representation for incorporating sequential information in text [25]. There are many other generative probabilistic models proposed for expert finding. For example, Serdyukov and Hiemstra [30] propose an expert-centric language model. Fang and Zhai [14] derive two families of generative models by applying probability ranking principle. Probabilistic topic models are also proposed to simultaneously model the topical distribution of expertise evidence and experts [34].
Some alternative approaches to expert search exist beyond language modeling. One effective approach is to treat the problem of ranking experts as a voting problem based on data fusion techniques [21]. Eleven different voting strategies were proposed to aggregate over the documents associated to an expert. Another approach is to model the process of expert finding by probabilistic random walks on so-called expertise graphs [31]. Many other expert finding methods were proposed during TREC Enterprise tracks.

Besides the models, some researchers have shown that suitable features can help significantly boost the performance of expert finding. These features include document authority information such as the PageRank, indegree, and URL length [38], graph-based expert authority [10], internal document structures that indicate the experts' associations with the content of documents [6], non-local evidence [2], and the evidence that can be acquired outside of an enterprise [29]. Additional evidence can be integrated by identifying home pages of candidate experts and clustering relevant documents [20]. Proximity features that characterize the cooccurrence of query and expert mentions in the document are also shown indicative by the top runs in the TREC evaluations [16]. This led to several window-based approaches including [25, 4, 20].
On the other hand, the early work of applying discriminative models in IR can date back to the early 1980s in which the maximum entropy approach was investigated to get around term independence assumptions in probabilistic generative models [11]. More recently, Nallapati [23] compared the performance of the maximum entropy model and support vector machines with that of language modeling in ad hoc retrieval and homepage finding, and argued that SVMs are preferred over language models because of their ability to learn arbitrary features automatically. Furthermore, it has been shown that feature-based discriminative models can consistently and significantly outperform current state of the art retrieval models with the correct choice of features [22]. Discriminative models have received increasing attention in IR, as another related area, learning to rank for IR, sparked genuine interest among researchers in the community [18]. Most of the learning to rank models are discriminative in nature and they have been shown improvements over their generative counterparts in ad hoc retrieval. Benchmark data sets such as LETOR [19] are also available for research on learning to rank. Although valuable work has been done on discriminative models for ad hoc retrieval and other IR domains, very limited research has been conducted to design discriminative models for expert search. The only relevant work that we are aware of is [15], which addressed the issue of differentiating heterogeneous sources according to specific queries and experts by learning associated weights from data, but the work did not model document-candidate relationship nor address how to incorporate new document evidence, which are two key issues in expert search.

3. GENERATIVE MODELS
To predict a class  given an observation , the desired choice of  is given by the conditional class probabilities  (). Depending on how to compute  (), the existing classification techniques can be broadly classified into two major categories: generative models and discriminative models. In a discriminative approach, a parametric model is introduced for  (), and the values of the parameters are inferred from a set of labeled training data. In contrast, the generative approach attempts to capture the manner in which an observation  is generated from given classes  by specifying a prior distribution  () over classes and a class-conditional distribution  () over the observation. The posterior  () is obtained from Bayes' Theorem as

 ()   () ()

(1)

In the context of expert search, the task is to find out what

684

is the probability of a candidate  being an expert given a query topic . In other words, we want to know  () in order to rank candidate  according to this probability. Similarly, by invoking Bayes' Theorem, we have:

 ()   () ()

(2)

where  () is the prior probability of a candidate, which is generally assumed uniform. Thus, the key quantity to estimate in the generative models is the probability of a query given the candidate,  (). Many language modeling techniques are proposed to estimate this quantity. One of the most prominent and effective one was called document models (often referred as Model 2) [3] where documents act as a hidden variable in the process which accumulates expertise evidence. Formally, it is expressed as



 () =  () ()

(3)

=1

where  () is the probability of the document  to generate the query  and can be calculated using a standard language model.  () is the probability of association between the document  and the candidate .  is the number of documents in the collection. Model 2 mimics the process one might use to find experts using a document retrieval system. Here, relevant documents are retrieved for the expertise requested, and they are used as evidence to indicate whether the associated candidates are experts. After aggregating all such evidence, the experts can be identified. As  () is relatively easy to determine in language models, the key ingredient in this model (and also in many other language models for expert search) is to estimate the document-candidate associations:  (), or  () if  () is assumed to be uniform.  () can be estimated by various methods. The simplest form is the boolean model where associations are binary decisions:  () = 1 if the candidate appears in the document; otherwise,  () = 0. More sophisticated methods are frequency based which consider the number of times that a candidate appears in the document. A set of heuristic combinations of all these metrics are also compared and investigated in [6].

4. DISCRIMINATIVE MODELS FOR
EXPERT SEARCH
4.1 Discriminative Learning Framework for Expert Search
For the text-based retrieval, conventional relevance-based probabilistic models rank documents by sorting the conditional probability that each document would be judged relevant to the given query [17]. The underlying principle using probabilistic models for information retrieval is called probability ranking principle [26]. The Binary Independence Model (BIM) [27] is a realization of this principle. In the domain of expert search, the similar principle can be used where experts are ranked according to the descending order of the conditional probability of relevance given an expert and a query. Fang and Zhai [14] applied this principle in studying expert search problem. Both BIM and [14]'s models are generative and they use Bayes' theorem to reverse the original conditional probability.
We propose a discriminative learning framework to directly model the conditional probability of relevance by a

parametric probability function. We cast expert search into a binary classification problem that treats the relevant queryexpert pairs as positive data and irrelevant pairs as negative data. Formally, we use a relevance variable   {1, 0} to denote whether two entities are relevant or not and thus the conditional probability of relevance (, ) represents the extent to which the expert  is relevant to the query . In our framework, (, ) can take any function form with parameter  that needs to estimate from training data. Based on different forms of , the resulting discriminative models are different. Given the relevance judgment  for the training expert-query pair (, ) which is assumed independently generated, the conditional likelihood  of the training data is as follows

   

=

( = 1, ) ( = 0, )1- (4)



where  is the number of queries and  is the number of experts. The parameters can then be estimated by maximizing the following log likelihood function

    (

 = arg max

 log ( = 1, )

(5)




(

))

+ (1 - ) log 1 - ( = 1, )

The estimated parameters can then be plugged back in ( = 1, ). According to the probability ranking principle, the experts are presented to users in the descending order of ( = 1, ). In the next section, we propose a specific discriminative model by defining the form of ( = 1, ).

4.2 A Discriminative Model
According to the previous work, Model 2 turned out to be one of the most effective formal models for expert search. The success of the model lies in its effective process to collect expertise evidence from documents. Our discriminative model builds on the same process in which the supporting document  serves as a bridge to connect expert  and query . Given a document , whether  and  are relevant depends on two factors: document evidence and document-candidate associations. More specifically, we consider: 1) whether the document  is relevant to the query ; 2) whether the expert  is relevant to the document . The final relevance decision for (, ) is made by averaging over all the documents. Formally, this can be expressed as

 ( = 1, ) =  (1 = 1, ) (2 = 1, ) ()

=1
(6)

where  (1 = 1, ) allows us to model the probability that a document  matches a topic , which indicates the document evidence.  (2 = 1, ) allows us to model the probability that a supporting document  mentions a candidate , which indicates the document-candidate associations. A

document  with higher values on both probabilities would contribute more to the value of  ( = 1, ). The prior

probability of a document,  (), is generally assumed uni-

form

(i.e.,

 () =

1 

).

We

model

both

 (1

= 1, )

and

 (2 = 1, ) by logistic functions on a linear combination

685

of features. Formally, they are parameterized as follows:

(  

)

 (1 = 1, ) = 

(, )

(7)

=1

(  

)

 (2 = 1, ) = 

  (, )

(8)

=1

where () = 1/(1 + exp (-)) is the standard logistic function.  is the weight for the  query-document feature (, ) and  is the weight for the  document-candidate feature (, ). Specifically, (, ) is the document evidence such as document retrieval scores that indicates how relevant the document is to the query. (, ) is the feature such as the boolean associations that describe the strength of associations between a document and a candidate.  denotes the number of document evidence features and  denotes the number of document-candidate association features. The weight parameters can be learned by maximizing the conditional log-likelihood of the data (i.e., Eqn. 5). Because there is no analytical solution, we use the BFGS Quasi-Newton for the optimization [13]. The method requires the objective function and its gradients. The partial derivatives of the log-likelihood  with respect to  and  are given as

 

=

 


 


(

  (1

- -

  )



)

(1 - ) (, )

=1

 

=

 


 


(   (1

- -

  )


=1

 (1

-

 ) (,

) )

where ,  and  denote the probabilities of Eqn. 6, Eqn. 7, and Eqn. 8, respectively. The main computation of the gradient method is evaluating the log likelihood function and its gradients against param( eters. Both of )them have computational complexity of   ( + ) . In practice, we only have a small number of relevance judgments for training and thus  is relatively small. In addition, the number of documents associated with each expert and the number of features used are also usually relatively small. Therefore, the training procedure can be efficient.
We can see that both Model 2 and this discriminative model try to aggregate document evidence and documentcandidate associations through the bridge of documents, but they are different in how to estimate these two probabilities. In Model 2, the document evidence (i.e.,  ()) is calculated by standard language models and the documentcandidate associations (i.e.,  ()) are estimated by a heuristic combination of document-candidate association features. In our proposed discriminative model, both quantities are modeled by logistic functions with arbitrary features and the parameters are automatically determined from training data. From Eqn. 6, we can see that ( = 1, ) is essentially the arithmetic mean of  ( = 1, , ) with respect to . Thus we refer the model as the arithmetic mean discriminative (AMD) model.

4.3 An Alternative Discriminative Model with Geometric Mean
It has been shown that in certain cases geometric mean (the product rule) is better than arithmetic mean (the sum rule) in combining evidences [35]. This observation mo-

tivates an alternative discriminative model which we refer as the geometric mean discriminative (GMD) model where ( = 1, ) is modeled by the geometric mean as follows:

 (

=

1, )

=

1 

 (  (1

=

1, ) (2

=

)1 
1, )

=1

(9)

where  is the normalization factor that scales the geometric

mean to be a proper probability distribution as follows



 (

)1 

=

 (1, ) (2, )

(10)

1{0,1},2{0,1} =1

Both  (1 = 1, ) and  (2 = 1, ) here take the same form with Eqn. 7 and Eqn. 8. By plugging them and Eqn. 10 into Eqn. 9, we can get

 (

=

1, )

=

1

+ exp(-) +

1 exp(- )

+ exp(-)

(11)

where



=

 



(

1 



)

(, ) , 

=

 



(

1 



)

 (, )

=1

=1

=1

=1



=

 

( 

1 



(,

) )

+

 

( 

1 



 (,

) )

=1

=1

=1

=1

We can notice that in Eqn. 11 there are three exponential terms in the denominator, which means that either querydocument features (, ) or document-candidate features (, ) alone cannot dominate the final relevance  ( = 1, ). The parameters of the model can also be estimated by maximizing the conditional log-likelihood function using BFGS. The GMD model has the same computational complexity with AMD.

4.4 Advantages of Discriminative Models for Expert Search
Some theoretical results show that discriminative models tend to have a lower asymptotic error [24]. Besides the theoretical considerations, we believe there are specific reasons for the domain of expert search that make discriminative models a suitable choice. First of all, the proposed discriminative models can effortlessly incorporate features. As shown in Section 2 and prior research, expert search can benefit from including various types of features. Language modeling approaches often require many modeling assumptions and extra modeling effort to include new features especially when the heterogeneous features are present. Secondly, discriminative models typically make fewer model assumptions than their generative counterparts. For example, many state-of-the-art generative models, including Model 2, the candidate-generation model [14] and the twostage language model approach [8], assume that the query  and candidate  are independent given the document , i.e., (, ) = (). It requires extra modeling effort for these models to overcome the assumption [4]. In contrast, our proposed discriminative models can easily get around it. For example,  (2 = 1, ) in Eqn. 6 can be replaced by  (2 = 1, , ) where no independence assumption is made on  (2 = 1, , ). Thirdly, the discriminative models directly and naturally characterize the notion of relevance. In Model 2 and many other language models, there

686

is no explicit reference to the class variable that denotes whether an expert is relevant or not. We use  ( = 1, ) instead of  () to make it explicit that the relevance of an expert is measured with respect to a query. This explicit notion of relevance can help quantify the extent to which a user's information need is satisfied.
5. EXPERIMENTS
5.1 Data Collections
Our experiments are carried out in the setting of the Expert Search task of the TREC Enterprise tracks from 2005 to 2008. For TREC 2005 and 2006, the document collection was a crawl of the World Wide Web Consortium (W3C) [12, 32]. For TREC 2007 and 2008, a different and more realistic corpus was introduced, which is a crawl of the website of Commonwealth Scientific and Industrial Research Organization (CSIRO). The corpus is known as the CSIRO Enterprise Research Collection (CERC) [1, 7]. Table 1 gives detailed statistics of the collections and query sets. The W3C data is supplemented with a list of 1092 candidate experts represented by their full names and email addresses while the CERC data do not contain a predefined list of candidates. Based on the observation that most CSIRO employees have a CSIRO email address following the pattern "firstname.lastname@csiro.au", we extract a list of candidates with email addresses matching this pattern from text. We also use heuristic rules to filter non-personal addresses (e.g. education.act@csiro.au). The total number of candidates extracted is 3,482. In 2005, 50 queries were created based on the working groups in W3C (there were 10 training topics also available in 2005). In 2006, 49 queries were developed by the track participants collectively using the provided list of supporting documents for each candidate. The 50 queries used in 2007 were created with the help of CSIRO's Science Communicators, while the judgments of 77 queries in 2008 were made by participants.
To evaluate the proposed models on W3C, we use the TREC 2006 topics plus the 10 available TREC 2005 training topics for training and test the models on the TREC 2005 topics. Similarly on CERC, we use TREC 2008 topics for training and TREC 2007 topics for testing. Although different years have different ways of topic assessments, we will see in the experiments that the discriminative models can still gain significant improvements from the training data. Our decision of choosing the training and testing configurations is mainly based on the number of relevance judgments available. We need a reasonable amount of training data for the discriminative models and there are relatively more relevance judgments in 2006 for W3C and in 2008 for CERC. Because the two test collections have very different characteristics, we do not evaluate the models across the corpora. To obtain a balanced training set, we randomly select the same number of negative instances with the number of positive instances for each training query, by following the undersampling method in [23]. To acquire negative instances for the queries without non-relevance judgments (i.e., 10 TREC 2005 training topics), we use the Base method introduced in Section 6.1 to identify a list of unjudged/irrelevant experts for each query. Evaluation measures are mean average precision (MAP), R-precision (R-Prec), mean reciprocal rank (MRR), and precision@5 (p@5) and precision@10 (p@10).

Table 1: Statistics of the W3C and CERC testbeds

W3C

CERC

# Documents

331,037

370,715

# People

1,092

3,482

Avg. Doc Length in Token

983.4

354.8

Avg. # Rel Experts/Topic 51.5 (2006) 10.4 (2008)

(TREC Year)

30.2 (2005)

3.0 (2007)

Training Queries

2006 (49)

2008 (77)

2005 (10)

Testing Queries

2005 (50)

2007 (50)

5.2 Research Questions
An extensive set of experiments were designed to address the following questions of the proposed research:
 Can the discriminative trained model perform better than its generative counterpart when the same set of features are available for use? (Section 6.1)
 Can integration of additional features into the discriminative model improve the performance? (Section 6.1)
 What features are likely more important in terms of the relative values of the learned weights in the discriminative model? (Section 6.1)
 What is the effect of only retrieving a subset of documents on the proposed model? (Section 6.2)
 How robust is the proposed discriminative model with respect to the underlying document retrieval methods? (Section 6.3)
 How robust is the proposed discriminative learning framework with respect to specific discriminative models? (Section 6.4)
In all the sections except Section 6.4, we only use the arithmetic mean discriminative (AMD) model to assess the discriminative learning approach, since we care less about the difference between discriminative models than about the difference between generative and discriminative models.
5.3 Experimental Setup
In all our experiments, we have done minimal preprocessing in which both queries and documents are stemmed using Krovetz stemmer. We only use the "title" or "query" fields in the topics without using extra information (e.g., "narrative"). No query expansion nor external resource is utilized. As shown in Section 4, each query-expert pair is characterized by two feature vectors, i.e., document evidence (, ) and document-candidate associations (, ). Table 2 summarizes the features used in the discriminative models.
These features include the score from the standard document language model (1), document features (2 - 5), external document structure features (6 - 9), basic association features (1 -5), internal document structure features (6 - 9), and proximity features (10 - 13). Here the external document structure features are the boolean variables to represent whether a document (in W3C) comes from specific types of documents (e.g., 8 = 1 means the document is either from "www" or "esw"). The evaluations on W3C use all the features, while the features 6 - 9 and 6 - 9 are not applied to CERC, as the CERC dataset does not

687

Table 2: Features used in the discriminative models.

"B" denotes the feature takes boolean values and

"N" represents numerical values

Feature

Description

Type References

1

LM

N

[37]

2

PageRank

N

[38]

3

URL length

N

[38]

4

Anchor text

N

[38]

5

Title

N

[38]

6

From lists

B

[12]

7

From people

B

[12]

8

From www+esw

B

[12]

9

From other+dev

B

[12]

1

Exact name match

B

[3]

2

Name match

B

[3]

3

Last name match

B

[3]

4

Email match

B

[3]

5

LM score

N

[6]

6

EMAIL FROM

B

[5]

7

EMAIL TO

B

[5]

8

EMAIL CC

B

[5]

9

EMAIL CONTENT

B

[5]

10  13

Proximity

B

-

contain explicit document types nor many emails with internal structure information useful for expert search [38]. The 1 feature is the document retrieval score by LM using the topic as the query. The smoothing method of LM is Jelinek-Mercer with the parameter  = 0.5 (we use the same smoothing for other LMs). The 5 feature is the retrieval score by LM using the candidate identifier as the query [6]. The "Proximity" features (6 - 9) are the boolean variables indicating whether the candidate identifier co-occurs with the query term in a window with various sizes. We use 20, 50, 100 and 250 as the window sizes (in number of words), approximated to the sizes of sentence, passage, paragraph and section, respectively. The details about these features can be found in the corresponding reference. To normalize the features, we use query-based normalization for each feature as suggested in [19].
Many of these features have been shown useful for expert search. Because of the generative nature of language models, it is difficult for them to incorporate such heterogeneous features in a unified modeling framework, but discriminative models can effortless include all the features and many more. Since the focus of this study is on the probabilistic models rather than feature engineering, we do not intend to choose a complete set of features, but they are one of the most comprehensive and diverse feature sets in a single work among the existing expert search research.
6. RESULTS
6.1 Discriminative Model vs. Model 2
In this section, we compare the proposed discriminative model with its generative counterpart: Model 2. The proposed model is evaluated on four different feature configurations, which are presented in Table 3. The Base method is the implementation of Model 2 by following [3], which includes 4 types of document-candidate associations. The R1 configuration uses these 4 association features plus 1 as document evidence. Thus, the identical information is

Table 3: Experimental configurations

Base Balog et al's Model 2 (candidate-centric) with 4

association features (i.e., 1 - 4) [3]

R1

Discriminative model with 4 association features

(1 - 4) and LM document evidence feature (1)

R2

Discriminative model with full document evi-

dence features and 4 association features (1 -4)

R3

Discriminative model with full association fea-

tures and one document evidence feature (1)

R4

Discriminative model with full document evi-

dence features and full association features

Table 4: Comparison of the discriminative model (AMD) with the Base mehod on W3C and CERC. Best results on each collection are highlighted. The symbol indicates statistical significance at 0.95 confidence interval against Base

MAP R-Prec MRR

P5

P10

W3C

Base R1 R2 R3 R4

0.1909 0.2001 0.2282 0.2412 0.2598

0.2445 0.2552 0.2764 0.2904 0.3035

0.5081 0.5300 0.5624 0.6232 0.6196

0.3760 0.3820 0.3960 0.4020 0.4130

0.3120 0.3310 0.3370 0.3560 0.3680

CERC

Base R1 R2 R3 R4

0.4039 0.4123 0.4453 0.4569 0.4604

0.3514 0.3569 0.3854 0.3879 0.3938

0.5389 0.5593 0.5924 0.5886 0.6143

0.2240 0.2280 0.2390 0.2610 0.2520

0.1540 0.1540 0.1650 0.1660 0.1770

available for R1 and Base to use. The weights in Base are set by following the choice of the best run in [3]. R4 is the configuration with full applicable features for the discriminative model (the R4 configuration is the default setting in all the experiments except explicitly noted). Table 4 contains the evaluation results on the two test collections. We can see that the discriminative model consistently performs better than Base across all the feature configurations on all measures. With the full set of features (i.e., R4 vs Base), all the differences are statistically significant by two-tailed Student's t-test at 0.95 confidence level. In R1 vs Base, although their differences are not significant, the discriminative model outperforms the Base method on all the evaluation metrics.
Since all the features are normalized, the weight associated with each feature can reflect the importance of the feature in some degree. Table 5 reports the top 3 features with the largest weights in  and  respectively in the learned AMD model. These features are ordered alphabetically in the table since their weights are not very distinct from each other. We find that the features listed for the two testbeds are generally different with the exception of 1 and 2, showing the importance of these two features across the corpora. An interesting observation is that the 8 feature when used on W3C has a large weight among all the document-candidate association features. This is intuitive in the sense that the person who is in the email cc field is likely an authoritative of the topics of the email, which is also consistent with what was reported in [5]. Another observation is that the "Proximity" features have large weights for both testbeds (i.e., 13

688

Table 5: The top 3 features with the largest weights

in AMD (R4) learned from training data

Doc evidence Doc-candidate associations

W3C CERC

1, 2, 6 1, 2, 5

1, 8, 13 4, 5, 11

MAP

0.25 0.2
0.15 0.1 102 0.5 0.4 0.3 0.2 101

W3C

103 Number of Documents Retrieved
CERC

102

103

Number of Documents Retrieved

Base AMD
104
Base AMD
104

MAP

Figure 1: Impact of varying the number of documents retrieved ( ) on the discriminative model. Top: impact on W3C; Bottom: impact on CERC.

for W3C and 11 for CERC), but with different window sizes: i.e., larger size on W3C. This may come from the fact that these two collections have very different average document lengths.
6.2 The Effect of the Size of Retrieved Documents
Similar to Model 2, the learned discriminative model can be efficiently used on top of an existing document search engine as follows: 1) Perform a standard document retrieval run using the topic as a query and retrieve the top  documents; 2) For each candidate associated with the relevant documents, calculate the probability of relevance using Eqn. 6 on these  documents. In this section, we aim to investigate the effect of the size of documents retrieved on the performance of the discriminative model. We use LM as the document retrieval run. Figure 1 shows the MAP results by varying  on the two test collections. Note that the scales on the x-axis and y-axis differ per plot. From the figure, we can see that as  increases, the discriminative model has a similar trend with the baseline: increasing, achieving a maximum, and then flattening. On W3C, the MAP value tops after 300 documents retrieved, fewer than what the baseline needs (i.e., 400). For CERC, both models need around 50 documents for best performance. Therefore, using a subset of documents could speed up the process of expert search as the best performers use much less documents than the whole set of relevant documents. At the same time, the retrieval performance can be improved although their differences are not found statistically significant.
6.3 Experiments by Using Different Document Retrieval Methods
As shown in Section 6.1 as well as in prior work, the doc-

Table 6: Evaluation of AMD with different docu-

ment retrieval methods on W3C and CERC

MAP R-Prec MRR P5

P10

W3C

LM BM25 Indri

0.2598 0.2658 0.2562

0.3035 0.3141 0.3066

0.6196 0.6238 0.6149

0.4130 0.4060 0.4090

0.3680 0.3700 0.3640

CERC

LM BM25 Indri

0.4604 0.4551 0.4667

0.3938 0.3895 0.4086

0.6143 0.5877 0.6000

0.2520 0.2470 0.2550

0.1770 0.1740 0.1780

Table 7: Comparison of the geometric mean discrim-

inative model with Base and AMD (R4) on W3C

and CERC. The symbol indicates statistical signif-

icance at 0.95 confidence interval for GMD against

Base

MAP R-Prec MRR

P5

P10

W3C

Base AMD GMD

0.1909 0.2598 0.2512

0.2445 0.3035 0.3010

0.5081 0.6196 0.6266

0.3760 0.4130 0.4110

0.3120 0.3680 0.3640

CERC

Base AMD GMD

0.4039 0.4604 0.4669

0.3514 0.3938 0.4030

0.5389 0.6143 0.6274

0.2240 0.2520 0.2500

0.1540 0.1770 0.1790

ument retrieval score 1 is an important feature to show document evidence for expert search. In this experiment, we assess the extent to which the performance of the discriminative model is affected by the choice of the underlying document retrieval model. Besides LM, another two different document retrieval methods are used (i.e., BM25 [28] and Indri [33]). Specifically, the 1 feature is replaced by these two retrieval scores respectively in the R4 configuration. Table 6 shows the MAP results of the proposed model across the three retrieval models. From the table, we can see that the results are quite similar and they are all significantly better than the baseline. This indicates that the discriminative model is robust to the underlying document retrieval method.
6.4 The Alternative Discriminative Model vs. Base and AMD
In this section, we conduct the experiment to evaluate the alternative discriminative model (GMD). The aim is to investigate the robustness of the proposed discriminative framework with respect to the choice of specific discriminative models derived from the framework. Table 7 contains the results. From the table, we can see that all the results achieved by GMD significantly outperform the baseline. Furthermore, these results are quite similar with those achieved by the AMD (R4) model. In particular, the GMD model is generally better than AMD on CERC and worse on W3C, but the differences between GMD and AMD are not statistically significant. These results demonstrate that the proposed discriminative framework generates accurate and robust results with both types of discriminative models.

689

7. CONCLUSIONS AND FUTURE WORK
In this work, we propose a discriminative learning framework and derive specific models for expert search. The main advantage of the proposed approaches is their ability to integrate a variety of document evidence and documentcandidate association features. The evaluations on two TREC Enterprise track testbeds have shown the effectiveness and robustness of the proposed framework.
There are several possibilities to extend the research in this paper. We chose "out-of-order" training in the experiments because more training data are available in 2006 and 2008. It would be interesting to perform the "in-order" experiments (i.e., training on 2005 or 2007), which would allow fair comparisons with the TREC submitted runs. The relevance judgments in 2005 and 2007 seem also more likely to be obtained in a real enterprise. In fact, lack of training data hinders the applicability of many discriminative models. On the other hand, generative models may be able to effectively utilize abundant unlabeled data. It is desirable to develop a hybrid of discriminative and generative models to obtain the best of both for expert search. In addition, in certain scenarios, pairwise comparisons between experts might be more easily collectible than the pointwise judgment for each expert. We will explore to extend the proposed discriminative learning framework to handle this type of training data.
8. ACKNOWLEDGMENTS
We thank the anonymous reviewers for many valuable comments. This research was partially supported by a grant from the Indiana Economic Development Company, the NSF research grant IIS-0749462, and a grant from Purdue University. Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors', and do not necessarily reflect those of the sponsor.
9. REFERENCES
[1] P. Bailey, N. Craswell, A. De Vries, and I. Soboroff. Overview of the trec-2007 enterprise track. In TREC-15, 2007.
[2] K. Balog. Non-local evidence for expert finding. In CIKM, 2008.
[3] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert finding in enterprise corpora. In SIGIR, 2006.
[4] K. Balog, L. Azzopardi, and M. de Rijke. A language modeling framework for expert finding. Information Processing & Management, 45(1):1­19, 2009.
[5] K. Balog and M. de Rijke. Finding experts and their details in e-mail corpora. In WWW, page 1036. ACM, 2006.
[6] K. Balog and M. De Rijke. Associating people and documents. In ECIR, 2008.
[7] K. Balog, I. Soboroff, P. Thomas, N. Craswell, A. de Vries, and P. Bailey. Overview of the trec-2008 enterprise track. In TREC-16, 2008.
[8] Y. Cao, J. Liu, S. Bao, and H. Li. Research on expert search at enterprise track of TREC 2005. In TREC-13, 2005.
[9] P. Carlile. Working knowledge: how organizations manage what they know. Human Resource Planning, 21(4):58­60, 1998.
[10] H. Chen, H. Shen, J. Xiong, S. Tan, and X. Cheng. Social network structure behind the mailing lists: Ict-iiis at trec 2006 expert finding track. In TREC-14, 2006.
[11] W. Cooper. Exploiting the maximum entropy principle to increase retrieval effectiveness. JASIST, 34(1):31­39.

[12] N. Craswell, A. de Vries, and I. Soboroff. Overview of the trec-2005 enterprise track. In TREC-13, 2005.
[13] J. Dennis and R. Schnabel. Numerical Methods for Unconstrained Optimization and Nonlinear Equations. Society for Industrial Mathematics, 1996.
[14] H. Fang and C. Zhai. Probabilistic models for expert finding. In ECIR, 2007.
[15] Y. Fang, L. Si, and A. Mathur. Ranking experts with discriminative probabilistic models. In SIGIR Workshop on Learning to Rank for Information Retrieval, 2009.
[16] Y. Fu, W. Yu, Y. Li, Y. Liu, M. Zhang, and S. Ma. THUIR at TREC 2005: Enterprise track. In TREC-14, 2006.
[17] N. Fuhr. Probabilistic models in information retrieval. The Computer Journal, 35(3):243, 1992.
[18] T. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225­331, 2009.
[19] T. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR Workshop on Learning to Rank for Information Retrieval, 2007.
[20] C. Macdonald, D. Hannah, and I. Ounis. High quality expertise evidence for expert search. In ECIR, 2008.
[21] C. Macdonald and I. Ounis. Voting for candidates: adapting data fusion techniques for an expert search task. In CIKM, 2006.
[22] D. Metzler and W. Bruce Croft. Linear feature-based models for information retrieval. Information Retrieval, 10(3):257­274, 2007.
[23] R. Nallapati. Discriminative models for information retrieval. In SIGIR, 2004.
[24] A. Ng and M. Jordan. On discriminative vs. generative classifiers: a comparison of logistic regression and naive bayes. NIPS, 2002.
[25] D. Petkova and W. Croft. Proximity-based document representation for named entity retrieval. In CIKM, 2007.
[26] S. Robertson. The probability ranking principle in IR. Journal of documentation, 33(4):294­304, 1977.
[27] S. Robertson and K. Jones. Relevance weighting of search terms. JASIST, 27(3):129­146, 1976.
[28] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-4. In TREC-4, 1996.
[29] P. Serdyukov and D. Hiemstra. Being omnipresent to be almighty: The importance of the global web evidence for organizational expert finding. In SIGIR Workshop on Future Challenges in Expertise Retrieval, 2008.
[30] P. Serdyukov and D. Hiemstra. Modeling documents as mixtures of persons for expert finding. In ECIR, 2008.
[31] P. Serdyukov, H. Rode, and D. Hiemstra. Modeling multi-step relevance propagation for expert finding. In CIKM, 2008.
[32] I. Soboroff, A. de Vries, and N. Craswell. Overview of the trec-2006 enterprise track. In TREC-14, 2006.
[33] T. Strohman, D. Metzler, H. Turtle, and W. Croft. Indri: A language model-based search engine for complex queries. In International Conference on Intelligence Analysis, 2004.
[34] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. Arnetminer: Extraction and mining of academic social networks. In SIGKDD, 2008.
[35] D. Tax, M. Van Breukelen, R. Duin, and J. Kittler. Combining multiple classifiers by averaging or by multiplying? Pattern recognition, 33(9):1475­1485, 2000.
[36] D. Yimam-Seid and A. Kobsa. Expert finding systems for organizations. Sharing Expertise: Beyond Knowledge Management, 2003.
[37] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. TOIS, 22(2):214, 2004.
[38] J. Zhu, X. Huang, D. Song, and S. Ruger. Integrating multiple document features in language models for expert finding. Knowledge and Information Systems, pages 1­26.

690

Blog Snippets: A Comments-Biased Approach

Javier Parapar
Information Retrieval Lab Dept. of Computer Science
University of A Coruña
javierparapar@udc.es

Jorge López-Castro
Information Retrieval Lab Dept. of Computer Science
University of A Coruña
irlab@udc.es

Álvaro Barreiro
Information Retrieval Lab Dept. of Computer Science
University of A Coruña
barreiro@udc.es

ABSTRACT
In the last years Blog Search has been a new exciting task in Information Retrieval. The presence of user generated information with valuable opinions makes this field of huge interest. In this poster we use part of this information, the readers' comments, to improve the quality of post snippets with the objective of enhancing the user access to the relevant posts in a result list. We propose a simple method for snippet generation based on sentence selection, using the comments to guide the selection process. We evaluated our approach with standard TREC methodology in the Blogs06 collection showing significant improvements up to 32% in terms of MAP over the baseline.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: search process General Terms: Experimentation Keywords: Blogs, Comments, Snippets.
1. INTRODUCTION AND BACKGROUND
In the last years the rise of the blogs has induced a new branch within the field of Information Retrieval: blog searching. Indeed, a new TREC Track has been created in order to advance in this area, the Blog Track starting in 2006 [7] with a new standard data-set, the Blogs06 collection [5].
Basically in blog search we can distinguish the post-page, the individual web-page that contains a post; the post by itself, the text entry that has been written by the blog's author; and the comments, opinions of the readers about the post. For instance, in the opinion retrieval task in the TREC Blog Track the retrieval unit is the post-page. Although most of the information should be present in the post, several works have already presented results demonstrating the utility of the readers' comments in the post-pages.
In [6] the authors demonstrated that by indexing the comments and the post, the recall of blog search is increased. It has to be remarked that recall is more important in blog than in web search because searchers are usually interested in all the recent posts about a specific topic. Although the precision values are basically maintained, highly discussed relevant post-pages seem to appear before in the rank. These results were obtained in a small corpus of 225MB where the 15% of the posts are commented. Another way of exploiting the comments information is in opinion finding. In [6] the comments are also used as indicators of discussion, estimat-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

ing the presence of comments and using this information to formulate query independent document priors.
Post comments have also been used in order to improve blog's summaries as a way of capturing readers feedback to the post entries. In [3] several original approaches are presented exploiting comments' content to summarize a post. In this work the evaluation was done only over 100 posts of two different blogs and using the criteria of four human evaluators under the ROUGE methodology [4]. The conclusion is that the summaries guided by the comments are more accurate than the generic summaries.
The objective of this poster is to demonstrate that the post snippets biased by the readers' comments can be used to improve the accessibility to relevant posts. The quality of the snippets in web search has been already demonstrated as a determinant factor to improve the user access to relevant information [8]. We present a simple sentence extraction technique to generate the snippet for a post, introducing the comments' information to select good sentences from the post text. We propose an indirect evaluation of the snippets quality. We will compare the performance of our approach against generic summaries in terms of retrieval effectiveness. We assume that better snippets can be more effective in a retrieval task. This assumption allows a large scale evaluation that can be performed with TREC methodology, comparing the effectiveness of using or not the comments in terms of topic relevance, as defined in the TREC Blog Track.
In the next section approaches to generate generic and comments-biased snippets are presented. In section 3 the evaluation and results of both methods are reported. Finally conclusions and future work are commented in section 4.
2. COMMENTS-BIASED SNIPPETS
The way in which we decided to exploit the comments is using them to guide the snippet generation in a similar way of how a query is used in a query-biased summary. We designed a simple algorithm based on sentence extraction from the post text to generate Comments-Biased Snippets (CBS) that works as follows:
1. The comments are splitted in sentences and these are sorted according to their relevance to the post text.
2. Given the comments sentences in that relevance order, a novelty detector is applied to avoid redundant sentences, the most novel sentences (S) are selected until reach the 30% of the size of the comments size.
3. The post text is splitted in sentences and these are ranked according to their relevance to S.

711

4. The most relevant and novel sentences of the post are selected until reaching 30% of the post original size.
In order to score the relevance of a sentence with respect to a piece of text we used the formulation of R(s|q) as defined in [1], acting the piece of text as q. Cosine distance also as in [1] was used to filter redundant sentences. We have to remark that in no case comments' text is added to the summaries, the comments are only used to guide the selection of important post's sentences.
As baseline we created Generic Snippets (GS) without the use of comments information. To produce these snippets we followed exactly the same steps as presented before but the post text itself was used instead of the comments to guide the sentence selection process.
To test our approach, we had to automatically extract from the collection the posts and comments from the postpages. This is indeed a messy task because this information is not standardly tagged in the collection. We applied a hybrid approach [2] with static templates for common blogging software platforms and a set of heuristics for the non commonly structured blogs.
3. EVALUATION AND RESULTS
3.1 Settings and Methodology
In order to compare both approaches we decided to use the standard TREC methodology allowing in this way a largescale evaluation without depending on expensive evaluators' work. As we are dealing with blog snippets we decided to use the Blog Track approach evaluating the results under the topic relevance criteria. We used the Blogs06 collection [5] and topics 851-900 and 901-950 with "title only".
We created two different indices: one with the generic snippets and another one with the comments-biased snippets. Higher retrieval effectiveness using the CBS index should indicate that the relevant documents are more accessible using comments-biased snippets. We decided to use a high performance state-of-the-art retrieval model: BM25. The b value was trained for each index in queries 851 - 900 optimizing M AP , for the other BM25 parameters recommended values were used. Topics 901-950 were used for test.
With the objective of performing a fair evaluation we decided to evaluate both approaches only over the subset of the collection where both post and comments were detected by our extraction approach (we have to remark than extraction accuracy can be still improved). Therefore from 3, 215, 171 permalinks (post-pages) in the Blogs06 collection we selected 1, 754, 334 post-pages. Note than doing this, the effectiveness values are lower than in the whole collection because many relevant documents are not considered.
3.2 Results
The evaluation results are summarized in Table 1. Our approach (CBS) achieves significant improvements for every setting and measure over the generic snippets (GS). In the test topics the improvement in terms of MAP, that was the measure optimized in training, achieved the 32%.
We also tested, in an additional experiment, the combination of both the post and comments to guide the sentence extraction process in the snippet generation. The effectiveness of this approach was only slightly better than using only the post (GS). This is explained because most of the

Table 1: Comparison between GS and CBS for train-

ing and test topics. Statistical significant improve-

ments according with Wilcoxon Signed Rank test

(p - value < 0.01) are starred, best values are bolded.

Topics

Measure

GS

CBS

851 - 900 T 901 - 950 T

M AP R - prec bP ref
M AP R - prec bP ref

0.0935 0.1744 0.1813 0.0756 0.1333 0.1360

0.1115(+19.25%) 0.1957(+12.21%) 0.2150(+18.59%) 0.1005(+32.94%) 0.1612(+20.93%) 0.1703(+25.22%)

sentences selected to guide the snippet construction were obtained from the post text (they were more relevant).
In additional experiments we indexed the posts' text and the posts' text plus the comments, confirming the preliminary results of Mishne and Glance [6]. For instance, the M AP values in the testing topics were 0.1638 when only indexing the posts against 0.1971 when indexing posts and comments, and 0.1737 when indexing the full text of the web-page. Of course these values are higher than the ones achieved using snippets, but we have to remark that our objective was not to replace the posts as retrieval units but showing that better snippets are more relevant and therefore can improve the user access to the relevant posts.
4. CONCLUSIONS AND FUTURE WORK
In this poster we presented the use of readers' comments in blog search to generate better snippets an thus to improve the accessibility to relevant post in blog search results lists. The evaluation confirmed the suitability of the approach significantly improving the baseline method with gains up to 32%. As future work we would like to combine both the comments and the user queries to guide the snippet generation and perform an user involved evaluation with DUC methodology or crowdsourcing evaluation [9].
Acknowledgments: This work was funded by FEDER, Ministerio de Ciencia e Innovacio´n and Xunta de Galicia under projects TIN2008-06566-C04-04 and 07SIN005206PR.
5. REFERENCES
[1] J. Allan, C. Wade, and A. Bolivar. Retrieval and novelty detection at the sentence level. In Proc. of ACM SIGIR'03, pp. 314­321, 2003.
[2] G. Attardi and M. Simi. Blog mining through opinionated words. In TREC, Special Publication 500-272. NIST, 2006.
[3] M. Hu, A. Sun, and E. Lim. Comments-oriented document summarization: understanding documents with readers' feedback. In Proc. of ACM SIGIR'08, pp. 291­298, 2008.
[4] C.-Y. Lin and E. Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proc. of NAACL'03, pp. 71­78, 2003.
[5] C. Macdonald and I. Ounis. The TREC Blogs06 collection: Creating and analysing a blog test collection. DCS TR, University of Glasgow, 2006.
[6] G. Mishne and N. Glance. Leave a reply: An analysis of weblog comments. In Third annual workshop on the Weblogging ecosystem, Edinburgh, Scotland, 2006.
[7] I. Ounis, M. de Rijke, C. Macdonald, G. A. Mishne, and I. Soboroff. Overview of the TREC-2006 blog track. In TREC, Special Publication 500-272. NIST, 2006.
[8] R. W. White, J. M. Jose, and I. Ruthven. Using top-ranking sentences to facilitate effective information access. JASIST, 56(10): pp. 1113­1125, 2005.
[9] O. Alonso, R. Baeza-Yates, and M. Gertz. Effectiveness of Temporal Snippets. In Proc. of WWW'09, pp. 1113­1125, 2005.

712

Effective Query Expansion with the Resistance Distance Based Term Similarity Metric

Shuguang Wang
Intelligent Systems Program University of Pittsburgh Pittsburgh, PA 15260
swang@cs.pitt.edu
ABSTRACT
In this paper, we define a new query expansion method that relies on term similarity metric derived from the electric resistance network. This proposed metric lets us measure the mutual relevancy in between terms and between their groups. This paper shows how to define this metric automatically from the document collection, and then apply it in query expansion for document retrieval tasks. The experiments show this method can be used to find good expansion terms of search queries and improve document retrieval performance on two TREC genomic track datasets.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithm, Performance
Keywords: Information Retrieval, Query Expansion, Term Similarity
1. INTRODUCTION
A fundamental challenge of information retrieval (IR) is to find documents that are relevant to user queries. The search queries usually consists of only few terms, which barely describe the information that users request. A widely used approach to deal with this problem is to expand the original query with relevant terms [4, 5]. In this study, we tackle the query expansion problem by defining new term-similarity metric that is based on the electric resistant network. In particular, this metric is derived from the effective resistance distances in between pairs of vertices in an undirected weighted graph. In this graph, nodes represent terms and they are linked together based on their co-occurrences. The edge weights represent the strength of term co-occurrences and are interpreted as electric resistances. Based on the resistance distances between pairs of terms, we demonstrate how to derive the similarity between terms and groups of terms. In this paper, we will discuss how to build the metric from document collection and apply it in query expansion for document retrieval tasks. We then present some of the evaluation results on two TREC Genomic Track data. Finally we will conclude the paper and suggests some future work.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Milos Hauskrecht
Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260
milos@cs.pitt.edu
2. METHODOLOGY
Our objective is to define a metric in the term space that would reflect how likely the terms are to be associated (or cooccur) in the document. We define the metric with the help of a weighted graph representing direct associations among terms and their strength. More formally, our model consists of an undirected weighed graph G = (V, E, w) where nodes V represent terms in the document, edges E represent pairwise association relations in between them, and weights w on the edges measure the strength of associations in between the connected pairs of nodes. In general, the association in between any two terms is calculated by considering all association paths and cumulative weights connecting them. This defines a metric on the term space.
Building an Association Graph We propose to build the graph from the (training) corpus
of documents by parsing each document and by extracting the pairwise associations among terms on the sentence level. If these two concepts co-occur in the same sentence, a direct link in between the concepts is included in the graph. Let j and k represent two distinct terms. If the two terms cooccur in n > 0 different documents, a link in between j and k with weight n is added to the graph (See Figure 1).

Figure 1: Building an association network from documents

Electric Resistance Network

To define the metric for any pairs of nodes (terms or con-

cepts) in the association graph, we propose to interpret the

weighted graph as a resistance network. Figure 2 illustrates

the resistance network obtained from a weighted association

network. In this case, the links and their weights in the

graph are replaced with connections with resistances corre-

sponding to their weights. More specifically, a weight wj,k

in between nodes j, k in the original weighted graph defines

the electric conductance cj,k of the connection that is the re-

ciprocal

of

its

electric

resistance

rj,k

=

1 cj,k

=

1 wj,k

.

We

can

use the electric resistance network to calculate the effective

resistance in between any two nodes in the network. This

effective resistance is the basis of our distance (similarity)

715

metric. The metric is also referred as resistance distance and comes with an intuitive random walk interpretation [1].

Figure 2: Building a resistance network from an association network
Calculating Effective Resistances In general, the calculation of resistances (or conductances)
in between any two nodes in an electric network is more complex and requires us to consider all serial and parallel path connections in between them. Also in order to define a proper metric we should define the distance for all possible pairs. We calculate the resistances with the help of graph Laplacian (L) [2], where L = A - D and A is the adjacency matrix and D is the degree matrix of the graph. This approach is also used to defined the spectral transformation kernel function [6].
The effective resistance in between nodes vj and vk can be calculated as: rj,k = L+j,j + L+k,k - L+j,k - L+k,j where L+ is the pseudo-inverse of the graph Laplacian. In general, the pseudo-inverse of a matrix A can be calculated from the singular value decomposition of A = U V  as A+ = U +V where + is the pseudo-inverse of .

3. USING THE DISTANCE METRIC IN IR

The effective resistance calculations define a distance met-

ric in between nodes (terms or concepts) that can in turn

be used to support various inferences in the term space. We

extend this metric to define the distance in between a set of

(seed) terms S and a target t as the average of the distances

between

nodes

in

S

and

t:

rS,t

=

1 |S|

siS rsi,t.

With the above metric, we can find all relevant terms

to the original query terms. However, the metric may not

differentiate well the relevant terms that are specific to the query from the rest of the relevant terms. To deal with it, we borrow the idea from TF-IDF[3] and re-normalize the distances between terms based on their relative distances:

rQn ,x =

1

|X -Q|

rQ,x

,

yX-Q rx,y

(1)

where Q is a query, x is a term, and X is the set of all terms. rQ,x is the resistance distance computed using the proposed metric and it is normalized by the average distance between x and all other non-query terms in the graph.

4. EXPERIMENTS
We evaluate our method using TREC Genomic Track 2003 & 2004 datasets, which are consist of abstracts from Medline. Test queries in 03 data contains gene names, their associated products (e.g., proteins), and their symbols and synonyms. Test queries in 04 data are sentences and they cover more general topics and involve more genomic concepts. We define our metric over only important terms: gene/protein names for 03 data and 5000 terms with highest TF-IDF scores for 04 data. We use 30% of 03 data and

25% of 04 data to extract the association networks respectively. We choose Lemur/Indri and its internal Pseudo

Table 1: TREC genomic track data statistics

Year #Abstracts #Test Queries

2003

525,932

50

2004 4,591,008

50

Relevance Feedback (PRF) query expansion module as the baselines. We use the Mean Average Precision (MAP), to measure the retrieval performance of various methods. All query terms are connected by "#combine" and the weights of expanded terms are assigned according to distance measures as w(x) = e-rQ,x . x is a expanded term, Q is the set of original query terms, and rQ,x defines the resistance distance between them.
We report results of query expansion with two proposed metrics, rQ,x and its normalized version rQn ,x. We first combine our metrics with Lemur/Indir and compare them with two baselines (See Tables 2). We use 5 expanded terms in this experiment. Both proposed metrics perform much (about 20%) better than the original Indri. More importantly, our metrics are much (over 9%) better than the PRF expansion approach and the normalized metric is the best.

Table 2: MAP of various methods

Methods

03 04

Indri

0.243 0.216

Indri+PRF 0.258 0.228

Indri+rQ,x Indri+rQn ,x

0.282 0.251 0.291 0.261

5. CONCLUSION AND FUTURE WORK
We have presented a new term similarity metric that can be easily defined using the document collection and applied it successfully in query expansion for document retrieval tasks. To the best of our knowledge this is the first study that attempts to define the term similarity metric based on electric resistance networks. In our evaluation, we defined the similarity metrics on important concepts because the data is from genomic domain. We would extend our study to define the similarity on all terms and experiment it on general document retrieval tasks.
6. REFERENCES
[1] P. G. Doyle and J. L. Snell. Random Walks and Electrical Networks. The Mathematical Association of America, Washington DC, 1984.
[2] D. J. Klein and M. Randi´c. Resistance distance. Journal of Mathematical Chemistry, 12:81­95, 1993.
[3] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Information Processing and Management, 5:513­523, 1988.
[4] J. Xu and B. W. Croft. Query expansion using local and global document analysis. In Proceedings of the 19th ACM SIGIR conference, pages 4­11. ACM, 1996.
[5] Y. Xu, G. J. Jones, and B. Wang. Query dependent pseudo-relevance feedback based on wikipedia. In Proceedings of the 32th ACM SIGIR conference, pages 59­66. ACM, 2009.
[6] X. Zhu, J. Kandola, J. Lafferty, and Z. Ghahramani. Graph kernels by spectral transforms. Semi-Supervised Learning, 2006.

716

The Impact of Collection Size on Relevance and Diversity
Marijn Koolen Jaap Kamps
University of Amsterdam, The Netherlands
{m.h.a.koolen, kamps}@uva.nl

ABSTRACT
It has been observed that precision increases with collection size. One explanation could be that the redundancy of information increases, making it easier to find multiple documents conveying the same information. Arguably, a user has no interest in reading the same information over and over, but would prefer a set of diverse search results covering multiple aspects of the search topic. In this paper, we look at the impact of the collection size on the relevance and diversity of retrieval results by down-sampling the collection. Our main finding is that we can we can improve diversity by randomly removing the majority of the results--this will significantly reduce the redundancy and only marginally affect the subtopic coverage.
Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--performance evaluation (efficiency and effectiveness)
General Terms: Experimentation, Measurement, Performance
Keywords: Diversity, Relevance, Collection size
1. INTRODUCTION
Hawking and Robertson [2] established that precision at a certain rank cutoff increases as the collection grows in size. Other things being equal, a larger collection will contain more relevant documents making it intuitively easier to find a fixed number of them. Hence we postulate that:
1. The amount of relevant information increases with collection size.
However, adding documents to the collection will lead to diminishing returns: since more and more information is already covered by the collection, it is increasingly hard to add new information. Hence we postulate that:
2. The amount of redundant information increases with collection size.
The TREC 2009 Web Track's Diversity Task [1] addresses the issue of redundancy by penalising systems that return the same information over and over again. Diversity puts the impact of collection size on precision in an interesting perspective. On the one hand, being topically relevant is a prerequisite for the desired results, which according to our first postulate would make a larger collection size beneficial. On the other hand, redundancy of information is harmful, which according to our second postulate would make a larger
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

collection potentially detrimental. We will try to determine the relative importance of these two opposing forces. Hence our main research question is:
What is the impact of collection size on the diversity of search results?
We use ClueWeb09 category B, consisting of the first 50 million English pages of the full ClueWeb09 collection and the Diversity Task's topics and relevance judgements. We indexed the collection using Indri 4.10. Common stop words are removed and the remaining terms are stemmed using Krovetz. The retrieval model is a standard language model with Jelinek-Mercer smoothing ( = 0.15) and a linear length prior (proportional to the length of the document). This run is not optimised for diversity, but merely serves as a way to illustrate the phenomena under consideration. We randomly down-sample the collection, using collection samples ranging between 5% and 100% of the full collection, and repeat this experiment five times. All sample statistics and scores are averages over these five iterations. Random sampling will make the expected probability of relevance of a document the same in the sample and in the full collection. This is helpful for our analysis, but in a realistic setting collections are unlikely to grow in an unbiased way.
2. RELEVANCE AND COVERAGE
We will first analyse the effect of reducing the collection size on the number of relevant documents, and on the number of topics or subtopics with at least one relevant result. There are 50 Diversity topics with 180 subtopics having at least one relevant page in the ClueWeb09 category B collection. In total, there are 3,777 positive relevance judgments for 2,783 distinct pages (some pages are relevant for multiple subtopics). Figure 1 shows the fraction of relevant pages in each sample and the fraction of subtopics for which there is at least one relevant page in the sample (averaged over the five samples). What is the impact of collection size on the number of relevant documents? Obviously, with random sampling the fraction of the relevant pages increases proportionally with the collection. Our first postulate holds.
What is the impact on the number of topics or subtopics with at least one relevant result? Here we see a very different pattern. Starting at 5%, the sample already contains over 40% of the subtopics. At a sample size of 30%, the collection contains relevant pages for over 80% of the subtopics. The fractions for the overall topics are even higher.
Our analysis shows that the small samples already cover the vast majority of subtopics with a relatively small fraction of the relevant documents. The larger samples, in contrast, contain many more relevant documents but only few additional subtopics.

727

1

0.8

0.6

0.4

0.2

Relevant pages

Total sub-topics

Total topics 0

0

0.2

0.4

0.6

0.8

1

Sample size

Figure 1: Impact of collection size on the fraction of relevant pages and subtopics with relevance.

Table 1: Redundancy and diversity of relevant information

Sample 10% 20% 30% 40% 50% 60% 70% 80% 90%
100%

Rel.docs/

Topic Subtopic

9.33

3.88

16.41

5.75

23.70

7.87

31.37

9.90

39.12

11.86

46.32

13.73

53.60

15.41

61.46

17.17

69.18

19.00

76.71

20.88

# Subtopics Top 10 Inf. Nav. All 41.6 2.2 43.8 47.4 2.0 49.4 47.6 2.4 50.0 47.8 2.6 50.4 46.8 2.4 49.2 46.2 1.6 47.8 44.8 2.4 47.2 43.4 2.4 45.8 40.2 2.0 42.2 39.0 2.0 41.0

3. REDUNDANCY AND DIVERSITY
We now analyse the effect of reducing the collection size on the redundancy of relevant information, and on the diversity or coverage of subtopics in the top of a retrieval run. Table 1 shows the number of relevant pages per topic and subtopic (columns 2 and 3). What is the impact of collection size on the redundancy of relevant information? The number of relevant pages and hence the redundancy steadily increases with the sample size. Eventually the collection contains many relevant document per topic and subtopic. Our second postulate also holds.
What is the impact on the diversity or coverage of subtopics in the top of the ranking? Table 1 shows the number of informational, navigational and total subtopics covered by a relevant document in the top 10 of our full-text run (columns 4, 5 and 6 respectively) when restricted to the sample. We see that initially the number of subtopics is increasing due to the increasing coverage for the smallest samples, but then peaks and tapers off due to the increasing redundancy for the larger samples.
Our analysis shows that collection size has a larger impact on redundancy than on the coverage of topics. This implies that the diversity at a fixed depth decreases with collection size, except for the smallest samples where coverage is still increasing noticeably.

4. RETRIEVAL EFFECTIVENESS
Finally, we analyse the effect of reducing the collection size on the performance on the TREC 2009 Web Track's Diversity Task's test collection. Figure 2 shows the impact of collection size on di-

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

IA-P10 IA-P20

0.02

a-nDCG10

a-nDCG20

0

0

0.2

0.4

0.6

0.8

1

Sample size

Figure 2: Impact of collection size on result diversity.

versity performance. The top two lines show the -nDCG measure at cutoffs 10 and 20, the bottom two show the IA-P measure at cutoffs 10 and 20. We show the variance over the 5 sample iterations only for the -nDCG@10 and IA-P@10 to keep the figure readable. The variance is similar at rank 20. variance is largest between 20% and 30% of the collection. We see an initial increase of performance at sample sizes below 15% of the collection. After that, however, the performance doesn't increase further and remains relatively stable across sample sizes of 30% and above. In fact, the performance at rank 10 is actually decreasing. This is in line with the results in Table 1, supporting the validity of the measures.
Our analysis leads to the remarkable conclusion that when result diversity is of importance, we can improve performance by randomly removing more than two-thirds of the results from the collection or from a retrieval run.
5. CONCLUSIONS
We analysed the impact of collection size on relevance, coverage, redundancy and diversity. We found that the number of relevant documents increases, but the coverage of subtopics quickly saturates. As a result the redundancy of information steadily increases leading to a lower diversity of results. This leads to the remarkable conclusion that, when result diversity is of importance, we can improve performance by randomly removing the majority of the results--this will significantly reduce the redundancy and only marginally affect the subtopic coverage.
Our results are based on a standard full-text run--which does not do a very good job at retrieving diverse results--and an ideal diverse ranking would suffer from removing random results. However, it also makes a call to caution to any claim for a technique to diversify results. Any such techniques might improve in whole or in part due to an arbitrary reduction of the result-list.
In future research we investigate the impact of information redundancy, study better ways of reducing the collection than random sampling, and address the notion of an optimal collection size.
REFERENCES
[1] C. L. A. Clarke, N. Craswell, and I. Soboroff. Overview the TREC 2009 web track. In The Eighteenth Text REtrieval Conference (TREC 2009) Notebook. National Institute for Standards and Technology, 2009.
[2] D. Hawking and S. Robertson. On collection size and retrieval effectiveness. Information Retrieval, 6:99­150, 2003.

728

Where to Start Filtering Redundancy? A Cluster-Based Approach

Ronald T. Fernández1, Javier Parapar2, David E. Losada1, Álvaro Barreiro2
1Department of Electronics and Computer Science, University of Santiago de Compostela, Spain
{ronald.teijeira, david.losada} @usc.es
2Information Retrieval Lab, Department of Computer Science, University of A Coruña, Spain
{javierparapar, barreiro} @udc.es

ABSTRACT
Novelty detection is a difficult task, particularly at sentence level. Most of the approaches proposed in the past consist of re-ordering all sentences following their novelty scores. However, this re-ordering has usually little value. In fact, a naive baseline with no novelty detection capabilities yields often better performance than any state-of-the-art novelty detection mechanism. We argue here that this is because current methods initiate too early the novelty detection process. When few sentences have been seen, it is unlikely that the user is negatively affected by redundancy. Therefore, re-ordering the first sentences may be harmful in terms of performance. We propose here a query-dependent method based on cluster analysis to determine where we must start filtering redundancy.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Information Filtering, Clustering, Retrieval Models
General Terms: Experimentation
Keywords: Novelty Detection, Sentence Clustering
1. INTRODUCTION
Novelty detection (ND) consists of filtering out redundant material from a ranked list of texts. This is an important task that has recently become of interest in many scenarios, such as text summarization, web information access, etc. However, the performance of current ND methods is not satisfactory.
In ND at sentence level, current mechanisms are based on filtering out redundancy starting at the beginning of the rank. There is often little overlapping among the first sentences seen by the user. In fact, we demonstrate here that the performance of these methods is poor because, usually, it is better to leave the ranking as it is (i.e. do not apply redundancy altogether). Redundancy that affects severely to the user comes likely at later stages (e.g. when the user has already seen a bunch of sentences). Therefore, filtering out information from the top ranked positions may be harmful. We propose here an approach based on starting the ND process only when there is strong evidence about redundancy. Moreover, because different queries may pro-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Table 1: Comparison of performance between differ-

ent state-of-the-art novelty detection methods and

the baseline (do nothing). Best values are bolded.

DN (basel.) NW SD CD

TREC 2003

P@10

.8760

.8800 .8460 .8060

MAP

.7411

.8188* .7902 .8046*

TREC 2004

P@10

.7640

.6760 .5900 .6460

MAP

.6103

.6086 .5574 .5865

duce document ranks with diverse redundancy levels, it is necessary to do this process in a query-dependent way. To this aim, we propose here a method based on clustering.
2. THE METHOD
First, we study the performance of state-of-the-art ND methods, i.e. NewWords, SetDif and CosDist [1] and show that they are often outperformed by a do nothing (DN) baseline (leave the relevance ranking as is). NewWords (NW) counts the number of terms of a sentence that have not been seen in any previous sentence. SetDif (SD) counts the number of different words between a current sentence and the most similar previously seen sentence. CosDist (CD) is a vector-space measure where the novelty score of a sentence si is the negative cosine of the angle between si and the most similar sentence in the history. These measures have shown their merits in past evaluations of ND [1]. However, when we compare these methods against a DN baseline, i.e. no novelty detection, we find that the application of ND is not justified. This is illustrated in Table 1, where we report the results found for Task 2 of TREC Novelty Tracks 2003 and 2004 (given the relevant sentences in 25 retrieved documents, identify all novel sentences). Statistical significant improvements between the performance of these ND methods and the baseline (t-test with 95% confidence level) are marked with .
The ND mechanisms are only helpful in a couple of cases and, furthermore, most of the ND methods lead to a performance that is worse than the baseline's performance. This indicates that current ND methods produce a strong reordering of the information presented to the user, which is problematic in terms of performance. We claim that this poor performance comes from initiating ND too early.
We propose here a method that drives the process so that,

735

Table 2: Comparison of performance between our

approach over the three ND methods and their orig-

inal formulations. Statistical significant differences

of each version w.r.t. the corresponding original

method are marked with  (at 95% of confidence

level). Best values are bolded.

NW NWr SD SDr CD CDr

testing: TREC 2003 (training: TREC 2004)

P@10 .8800 .8980 .8460 .8920* .8060 .8940*

%

(+2 .05 )

(+5 .44 )

(+10 .92 )

MAP .8188 .8237 .7902 .8074* .8046 .8182*

%

(+0 .60 )

(+2 .18 )

(+1 .69 )

testing: TREC 2004 (training: TREC 2003)

P@10 .6760 .7840* .5900 .7640* .6460 .7760*

%

(+15 .98 )

(+29 .49 )

(+20 .12 )

MAP .6086 .6389* .5574 .6169* .5865 .6318*

%

(+4 .98 )

(+10 .67 )

(+7 .72 )

depending on the query, ND is triggered starting at a given position r in the ranking of sentences (the sentences in previous positions preserve their order). To determine the value of r we propose a cluster-based approach. The intuition behind this idea is that ND should only be started when we find some evidence about redundancy, i.e. a sentence is strongly thematically related to a previous one, and this can be detected using clustering. The k-NN clustering algorithm was widely used for cluster-based document retrieval, see [2] for instance. Here we use a variant of the k-NN algorithm: instead of setting the number k of neighbors for a sentence we set the minimum similarity threshold t for the given metric (in our case cosine distance). Given a sentence si, its neighborhood is the set of sentences sk such that sim(si, sk)  t. The method works as follows: first, for each query we cluster all its relevant sentences using tNN. Next, we scan sequentially the ranking of sentences (as provided by the task) and fix r to the position of the first sentence whose cluster (neighborhood) contains a sentence already seen before. This means that positions from 1 to r - 1 are frozen, while sentences starting at the r position are re-ranked using the ND methods described above.
3. EXPERIMENTS
In our evaluation we considered the TREC 2003 [4] and 2004 [3] novelty datasets. These test collections supply relevance and novelty judgments at sentence level for each topic. Sentences were clustered by applying the t-NN clustering algorithm. We considered values for t between 0 and 0.95 (in steps of 0.05). In order to assess the parameter stability we performed double cross-evaluation by swapping training and testing collections. Runs are compared using precision at 10 (P@10) and mean average precision (MAP) computed with the novelty judgments. In the training stage we obtained the optimal t (in terms of MAP) for the best novelty technique, i.e. NewWords, (t = 0.5 and t = 0.4 training in TREC 2003 and 2004, respectively) and this value was fixed for all novelty methods in the test collection.
Table 2 shows a comparison between the original ND methods (NW, SD and CD) and our variants (NWr, SDr and CDr, respectively). The modified ND methods outperform significantly the original ones. Only when NWr is evaluated against the TREC 2003 dataset significant differences w.r.t. NW are not obtained but, anyway, performance does not decrease. Therefore, the new methods are promising.

0.82 0.80 0.78 0.76 0.74
0

TREC 2003 MAP
DN NW NWr
0.2 0.4 0.6 0.8 t

0.68 0.66 0.64 0.62 0.60
0

TREC 2004 MAP DN NW NWr
0.2 0.4 0.6 0.8 t

Figure 1: Performance of our approach considering t = 0 . . . 0.95, NW and DN with TREC 2003 and TREC 2004.

Observe also that the new performance figures are substantially higher than the values yielded by the DN baseline (Table 1). This means that the ND techniques are still useful (provided that they are executed from lower rank positions).
Figure 1 shows the impact of t on NWr, in terms of MAP (trends are similar for the rest of methods). With low t values the performance is equivalent to the original NW (clusters are large and, therefore, the ND process is initiated at early positions in the ranking). As t increases, we obtain better performance and t around 0.5 seems a good configuration.
We also tested a simple approach based on training r in one collection (same r for all queries) and using the learnt value in the the testing collection. This query-independent approach performs worse than our cluster-based method.

4. CONCLUSIONS
In this poster we analyzed the performance of current state-of-the-art novelty detection methods at sentence level. We showed that, usually, these methods perform worse than doing nothing. This happens because, when the user has seen few sentences, the information tends to be novel and, therefore, applying a novelty detection process that re-orders these sentences may be harmful. Therefore, we proposed a mechanism that consists of starting the novelty detection process from a given rank position. To this aim, we followed a query-dependent cluster-based method that predicts a good ND starting position. We showed that statistically significant improvements between this variant and the stateof-the-art ND methods were obtained. As future work, we propose to study the performance of our approach for novelty detection at document level.
Acknowledgments: This work was partially supported by FEDER, Ministerio de Ciencia e Innovaci´on and Xunta de Galicia under projects TIN2008-06566-C04-04, 2008/068 and 07SIN005206PR.

5. REFERENCES
[1] J. Allan, C. Wade, and A. Bolivar. Retrieval and novelty detection at the sentence level. In Proceedings of the 26th ACM SIGIR, pp. 314­321, Canada, 2003.
[2] K. S. Lee, W. B. Croft, and J. Allan. A cluster-based resampling method for pseudo-relevance feedback. In Proceedings of the 31st ACM SIGIR, pp. 235­242, USA, 2008.
[3] I. Soboroff. Overview of the TREC 2004 Novelty Track. In Proceedings of the 13th TREC, USA, 2004.
[4] I. Soboroff and D. Harman. Overview of the TREC 2003 Novelty Track. In Proceedings of the 12th TREC, USA, 2003.

736

Scalability of Findability: Effective and Efficient IR Operations in Large Information Networks

Weimao Ke
School of Information and Library Science University of North Carolina at Chapel Hill
Chapel Hill, NC 27599-3360, USA
wke@unc.edu
ABSTRACT
It is crucial to study basic principles that support adaptive and scalable retrieval functions in large networked environments such as the Web, where information is distributed among dynamic systems. We conducted experiments on decentralized IR operations on various scales of information networks and analyzed effectiveness, efficiency, and scalability of various search methods. Results showed network structure, i.e., how distributed systems connect to one another, is crucial for retrieval performance. Relying on partial indexes of distributed systems, some level of network clustering enabled very efficient and effective discovery of relevant information in large scale networks. For a given network clustering level, search time was well explained by a polylogarithmic relation to network size (i.e., the number of distributed systems), indicating a high scalability potential for searching in a growing information space. In addition, network clustering only involved local self-organization and required no global control ­ clustering time remained roughly constant across the various scales of networks.
Categories and Subject Descriptors
H.3.4 [Information storage and retrieval]: Systems and Software--Distributed systems, Information networks
General Terms
Algorithms, Performance, Experimentation
Keywords
distributed IR, scalability, network clustering, decentralized search, weak tie, strong tie, clustering paradox, connectivity
1. INTRODUCTION
In today's digital environments, there exist a variety of information networks where information is distributed among dynamic systems. On the Web, for example, individual web
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10 July 19­23, 2010. Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

Javed Mostafa
School of Information and Library Science University of North Carolina at Chapel Hill
Chapel Hill, NC 27599-3360, USA
jm@unc.edu
sites host diverse information topics and form a network by means of hyperlinks. Likewise, digital libraries interoperate with one another and serve information distributed across collections in a network. For reasons such as copyright and privacy, lots of information cannot be fully collected and indexed in advance for retrieval purposes. In addition to this is the dynamics of many environments such as the deep web and peer-to-peer networks, in which it is not only difficult to gather information but also challenging to keep an index up to date.
Centralized IR solutions can hardly survive the continued growth of today's information spaces ­ they are vulnerable to scalability demands [3]. A distributed architecture is desirable and, due to many constraints, is often the only choice. Distributed (federated) IR research is a response to the challenge of retrieving information from distributed sources. Recent distributed IR research has focused on intra-system retrieval fusion/federation, cross-system communication, and distributed information storage and retrieval algorithms [9, 23].
Classic distributed information retrieval has shown some potential of efficiently and effectively bringing distributed information together. However, the reliance on centralization of a metasearch server will continue to suffer from critical problems such as scalability, single point failure, and fault tolerance. Further decentralization of meta search models will involve issues beyond the main focus of federated IR research.
Research has been done under the theme of peer-to-peer information retrieval (P2P-IR) and, more recently, large scale distributed systems for IR (LSDS-IR) [5, 10, 16, 11]. While classic distributed IR often focuses on tens, if not hundreds, of distributed collections, P2P- or LSDS-IR usually envisions an IR problem situated in thousands and even millions of distributed, dynamic systems. The magnitude, distribution, and dynamics of information in such an environment remain a great challenge in IR. Applications of this research include not only search in peer-to-peer environments but also information retrieval in digital libraries, intelligent information discovery on the deep web, distributed desktop search, and agent-assisted web surfing etc.
Finding relevant information in distributed networked environments transforms into a problem concerning information retrieval and complex networks. In this study, we focus on how relevant information can be effectively and efficiently found in large scale information networks, where no centralized index can possibly be built. We investigate the impact of network structure/topology on the effectiveness and effi-

74

ciency of decentralized IR operations relying on distributed indexes. We test the proposed retrieval methods in a growing information space and examine the scalability potential.
2. RELATED WORK
While traditional IR and distributed IR research provides basic tools for attacking decentralized search problems, the evolving dynamics and heterogeneity of today's networked environments challenge the sufficiency of classic methods and call for new innovations [3]. Whereas peer-to-peer offers a new type of architecture for application-level questions and techniques to be tested, research on complex networks studies related questions in their basic forms [2, 23].
2.1 P2P Information Retrieval
In an open, dynamic information space such as a peerto-peer network, people, information, and technologies are all mobile and changing entities. Identifying where relevant collections are for the retrieval of information is essential. Without global information, decentralized IR methods have to rely on individual indexes in distributed nodes and their limited local intelligence to collectively construct paths to desired information.
Recent years have seen growing popularity of peer-to-peer (P2P) networks for large scale information sharing and retrieval [17]. There have been ongoing discussions on the applicability of existing P2P search models for IR, the efficiency and scalability challenges, and the effectiveness of traditional IR models in such environments [23]. Some researchers applied Distributed Hashing Tables (DHTs) techniques to structured P2P environments for distributed retrieval and focused on building an efficient indexing structure over peers [7, 18, 21].
Others, however, questioned the sufficiency of DHTs for dealing with high dimensionality of IR (e.g., a large number of terms for document representation) in dynamic P2P environments [5, 17, 16]. For retrieval with a large feature space, which often requires frequent updates to cope with a transient population, it is challenging for distributed hashing to work in a traffic- and space-efficient manner. Unstructured overlay systems work in an nondeterministic manner and have received increased popularity for being fault tolerant and adaptive to evolving system dynamics [17].
2.2 Decentralized Search in Networks
Research on complex networks provides valuable principles for searching/navigation in distributed systems. Not only do many information networks such as the Web share the common phenomenon of small world but they also appear to be searchable [2]. Particularly, studies showed that without global information about where targets are, members of a very large network are able to collectively construct short paths (if not the shortest) to destinations [15, 22, 8].
The implication in IR is that relevant information, in various networked environments, is very likely a small number of connections/links away from the one who needs it and is potentially findable. This indicates potentials for decentralized retrieval algorithms to traverse an information network to find relevant information efficiently. However, this is not an easy task because not only relevant information is a few degrees/connects away but so is all information.
To find relevance in a densely-packed "small world" network remains very challenging. Nonetheless, research has

demonstrated how nodes connect to one another and the structure of the network they thus form have critical impacts on how searches function. Network clustering, sometimes by means of semantic overlay, can significantly improve effectiveness and efficiency of IR operations in an information network.
Clustering, the process of bringing similar entities together, is useful for information retrieval. Traditional IR research utilized document-level clustering to support exploratory searching and to improve retrieval effectiveness. In large scale distributed IR, topical clustering techniques such as semantic overlay networks (SONs) have been widely used, in which systems containing similar information form semantic groups for efficient searches [5, 10, 16].
Research indicated that a proper degree of network clustering with some presence of remote connections has to be maintained for efficient searches [15, 20]. Clustering reduces the number of "irrelevant" links and aids in creating topical segments useful for orienting searches. With very strong clustering, however, a network tends to be fragmented into local communities with abundant strong ties but few weak ties to bridge remote parts [12]. Although searches might be able to move gradually toward targets, necessary "hops" become unavailable. We refer to this phenomenon as the Clustering Paradox, in which neither strong clustering nor weak clustering is desirable. The Clustering Paradox has received attention in complex network research and requires further scrutiny in a decentralized IR context [15, 14].

3. EXPERIMENTAL SYSTEM
We have developed a multi-agent decentralized search architecture named TranSeen for finding relevant information distributed in networked environments. We illustrate the conceptual model in Figure 1 (a) and major components in Figure 1 (b). The TranSeen system is an implementation in Java, based on two well-known open-source platforms: 1) JADE, a multi-agent system/middle-ware that complies with the FIPA (the Foundation for Intelligent Physical Agents) specifications [6], and 2) Lucene, a high-performance library for full-text search [13].

v d

c

?

b

u

(a) Global View

QQuuerey ry

Neighbor Prediction
Neighbor Representation

Query Representation

Neighbor Representation

Document Representation

Local Retrieval

Doc
document

Doc
Document
Doc
Document

(b) Agent Internal View

Figure 1: Conceptual Framework. (a) Global View of agents working together to route a query in the network space. (b) Agent Internal View of how components function within an agent.

Assume that agents, representatives of distributed information systems, reside in an n dimensional (hypersphere) space. An agent's location in the space represents its information topicality. Therefore, finding relevant sources for an information need is to route the query to agents in the relevant topical space. To simplify the discussion, assume all

75

agents can be characterized using a two-dimensional space. Figure 1 (a) visualizes a 2D circle (1-sphere) representation of the information space. Let agent Au be the system that receives a query from the user whereas agent Av has the relevant information. The problem becomes how agents in the connected society, without global information, can collectively construct a short path to Av so that relevant information can be retrieved from there. In Figure 1 (a), the query traverses a search path Au  Ab  Ac  Ad  Av to reach the target. While agents Ab and Ad help move the query toward the target gradually (through strong ties), agent Ac has a remote connection (weak tie) for the query to "jump."
3.1 Decentralized Search
When an agent receives a query, it first conducts local search operations to retrieve relevant information from its individual document collection. If local results are unsatisfactory, e.g., relevance/similarity scores do not reach a predefined threshold, the agent will contact its neighbors for help. Therefore, there requires a mechanism for matching query representation with potential good neighbors ­ either the neighboring agent is more likely to have relevant information to answer the query directly or more likely to connected with relevant targets. Agents explore their neighborhoods through interactions (e.g., query-based sampling), develop some knowledge about neighbors' topicality and connectivity, and serve as local decision makers in the search process. They are essentially metasearch systems for one another.
3.2 Network Structure & Local Clustering
As discussed earlier, network structure plays an important role in decentralized search. We used a parameter called the clustering exponent  to guide network clustering for decentralized search: the probability pr of two nodes being connected/linked is proportional to r-, where r is the pairwise topical distance and  the clustering exponent.
Figure 2: Function of Clustering Exponent 
The clustering exponent , as shown in Figure 2, describes a correlation between the network (topological) space and the search (topical) space [15, 8]. When  is small, connectivity has little dependence on topical closeness ­ local segments become less visible as the network is built on increased randomness. As shown in Figure 3 (c), the network is a random graph given a uniform connectivity distribution

at  = 0. When  is large, weak ties (long-distance connections) are rare and strong ties dominate [12]. The network becomes highly segmented. As shown in Figure 3 (a), when   , the network is very regular (highly clustered) given that it is extremely unlikely for remote pairs to connect. Given a moderate  value, as shown in Figure 3 (b), the network becomes a narrowly defined small world, in which both local and remote connections present.

 (a) Segmented

 = 2.5 (b) Small World

=0 (c) Random

Figure 3: Network Clustering: Impact of Clustering Exponent .

The clustering exponent  influences the emergence of local segments and overall network clustering. In complex network research, it has been shown that only with some particular value of , search time (i.e., search path length) is optimal and bounded by a poly-logarithmic function of network size [15]. One important aspect of this research is to study the impact of network structure on decentralized IR effectiveness and efficiency.

4. ALGORITHMS
This section elaborates on specific algorithms used in the research. Section 4.1 presents the basic functions for information representation, neighbor representation, and similarity measurement. Section 4.2 describes four search (neighbor selection) algorithms based on neighbor similarity and/or connectivity. Section 4.3 elaborates on the function for agent rewiring (clustering) based on the clustering exponent .
4.1 Basic Functions
4.1.1 TF*IDF Information Representation
We used the Vector-Space Model (VSM) for information (document and query) representation [4]. Given that information was highly distributed, a global term space was not assumed. Instead, each agent processed information it individually had and produced a local term space, which was used to represent each information item using the TF*IDF (Term Frequency * Inverse Document Frequency) weighting scheme. An information item was then converted to a numerical vector where a item t was computed by:

W (t) = tf (t) · log( N )

(1)

df (t)

where tf (t) is the frequency of the term t of the term space

in the information item, N is the total number of informa-

tion items (e.g., documents) in an agent's local collection,

and df (t) is the number of information items in the set con-

taining

the

term

t

of

the

term

space.

We

refer

to

log(

N df (t)

)

as IDF. IDF values were computed within the information

space of an agent given no global information.

76

4.1.2 DF*INF Agent Representation
Following a simple federated IR model, we allowed agents to collect document frequency (DF) information from neighors (distributed systems) and to use it to create metadocuments for neighbor representation [19]. Treating each metadocument as a normal document, it was then straightforward to calculate neighbor frequency (NF) values of terms, i.e., the number of metadocuments (neighbors) containing a particular term. A metadocument (neighbor) was then represented as a vector where a term t was computed by:

W

(t)

=

df

(t)

·

log(

N nf (t)

)

(2)

where df (t) is the frequency of the term t of the term space in the metadocument, N is the total number of an agent's neighbors (metadocuments), and nf (t) is the number of neighbors containing the term t. We refer to this function as DF*INF, or document frequency * inverse neighbor frequency.

4.1.3 Similarity Scoring Function
Based on the TF*IDF (or DF*INF) values obtained above, pair-wise similarity values can be computed. Given a query q, the similarity score of a document d matching the query was computed by :

X tf (t) · idf 2(t) · coord(q, d) · queryN orm(q) (3)
tq
where tf (t) is term frequency of term t in document d, idf(t) the inverse document frequency of t, coord(q, d) a coordination factor based on the number of terms shared by q and d, and queryN orm(q) a normalization value for query q given the sum of squared weights of query terms. The function is a variation of the well-known cosine similarity measure. Additional details can be found in [13, 4].

4.2 Search Methods
When an agent found no sufficiently relevant information from its local collection, it forwarded the query to another agent. We proposed the following four neighbor selection strategies, i.e., search methods, to be tested and compared in experiments.

4.2.1 RW: Random Walk
The Random Walk (RW) strategy ignores knowledge about neighbors and simply forwards a query to a random neighbor. Without any learning module, Random Walk is presumably neither efficient nor effective. Hence, the Random Walk served as the search performance lower-bound.

4.2.2 SIM: Similarity-based Search
Let k be the number of neighbors an agent has and S = [s1, .., sk] be the vector about neighbors' similarity scores to a query. The SIM method sorts the vector and forwards the query to the neighbor with the highest score. We assumed that agents were cooperative ­ that is, they shared with one another document frequency (DF) values of key terms in their collections, based on which a meta document were created as representative of a neighbor's topical area. A query was then compared with each meta document, represented by DF*INF (see Equation 2), to generate the similarity vector S.

4.2.3 DEG: Degree-based Search
In the degree-based strategy, information about neighbors' degrees, i.e., their numbers of neighbors, was known to the current agent. Let D = [d1, .., dk] denote degrees of an agent's neighbors. The DEG method sorts the D vector and forwards the query to the neighbor with the highest degree, regardless of what a query is about [1].
4.2.4 SimDeg: Similarity*Degree Search
The SimDeg method combines information about neighbors' relevance to a query and their degrees. [20] reasoned that a navigation decision relies on the estimate of a neighbor's distance from the target, or the probability that the neighbor links to the target directly, and proposed a measure based on the product of a degree term (d) and a similarity term (s) to approximate the expected distance. Following the same formulation, the SimDeg method used a combined measure SD = [s1 · d1, .., sk · dk] to rank neighbors, given neighbor relevance vector S = [s1, .., sk] and neighbor degree vector D = [d1, .., dk]. A query were forwarded to the neighbor with the highest sd value.
4.3 Agent Rewiring and Network Clustering
We used the clustering exponent  to guide agent selforganization and network clustering. For each agent, the first step was to determine how many neighbors it should have. Given the web collection (Section 5.1) used in this study, we obtained each agent's (i.e., a web domain) indegree based on hyperlink analysis and normalized the degree to a value d  [30, 60]. Once agent u determined its degree du, a number of random agents were selected for u such that the total number of random neighbors dT du (dT  150 in this study). Then, the current agent (u) used its metadocument to query each of the dT neighbors (v) to determine their topical distance ruv. Finally, the following probability function was used by the agent to decide who should remain as neighbors (overlay): puv  ru-v, where  is the clustering exponent and ruv pairwise topical distance.
5. EXPERIMENTAL SETUP
5.1 Data Collection
We used the ClueWeb09 Category B collection created by the Language Technologies Institute at CMU for IR experiments, which contains a crawl of 50 million English pages during Jan - Feb 2009. Analysis of the hyperlink graph produced Figures 4 (a) in-degree frequency distribution and (b) Site size (#pages per site) distribution based on 50, 221, 776 pages extracted from 2, 777, 321 unique domains (treated as sites) (on log/log coordinates).
5.2 Task and Queries
Given the large size of the data collection, it is nearly impossible to manually judge the relevance of every document and to establish a complete relevance base. While previous research on large scale distributed information retrieval mainly relied on similarity thresholds to do automatic relevance judgment, such an approach was rather arbitrary and was biased by the centralized IR system that served as the gold standard [5, 16].

77

1e+06

1e+08

Degree frequency f(k)

1e+00

1e+02

1e+04

1e+06

1

10

100

10000

In-degree (k)

(a) In-degree distribution

1e+00

1e+02

1e+04

1e+06

Web site size (# pages) (s)

(b) Site size distribution

Figure 4: ClueWeb09 Category B Statistics

5.2.1 Documents as Queries
Serving diverse users in an open, dynamic environment, implies that some queries are likely to be narrowly defined. We reasoned that relevant information is rare when a query is very specific. In this study, we used documents (web pages with title and content) as queries to simulate decentralized searches. We obtained a set of query documents by sampling documents from the 100 most popular web domains. Removing queries that were too broad or vague resulted in 85 queries.

5.2.2 Task: Exact/Rare Item Search
To make searches more realistic/challenging and automatic evaluation more objective, we considered extreme rarity of relevant documents given very specific information needs. We decided that, given each query, there was only one relevant document among all documents distributed in the network and the task was to find that exact document. When a query document was issued to a random system/site in the network, the task involved finding the system who hosted it. The strength of this task is that relevance judgment was more objective provided the relative unambiguity of a "hosting" relationship. The extreme rarity, however, posed a great challenge on the proposed decentralized search methods.

5.3 Evaluation Metrics
This study focused on effectiveness and efficiency of IR operations in networks and scalability of decentralized search. We emphasized the finding of exact/rare information in large distributed environments and proposed the use of the following evaluation metrics.

5.3.1 Effectiveness

Of various evaluation metrics used in TREC and IR, pre-

cision and recall are the basic forms. Whereas precision P

measures the fraction of retrieved documents being relevant,

recall R evaluates the fraction of relevant documents being

retrieved. The harmonic mean of precision and recall, known

as

F1,

is

computed

by

F1

=

2·P ·R P +R

[4].

5.3.2 Efficiency
In experiments, we measured the search path length L (i.e., the number of agents involved) and actual time  taken to find relevant information for each query. The average search length L¯ of all queries was calculated to measure efficiency. When fewer agents are involved, the entire dis-

Size frequency f(s)

1e+00

1e+02

1e+04

tributed system is considered to be more efficient. Likewise, average search time ¯ was calculated to evaluate efficiency.
5.3.3 Scalability
One important objective of this research was to learn how decentralized IR systems can function and scale in very large information network. For scalability, we ran experiments on different network size scales N  [102, 103, 104]. First, we used the 100 most highly linked web domains to form a 100agent network and conducted experiments on it. Then, we extended the network to 1, 000 and 10, 000 systems/sites for additional experiments. Table 1 shows the total number of documents on each network scale. After experiments, we analyzed the functional relationships of effectiveness and efficiency to network size.
Network Size N 102 103 104 # Documents 0.5M 1.7M 4.4M
Table 1: Network Size and Total # Docs

5.4 Simulation Procedures and Setup
Pseudo code in Algorithm 1 illustrates how different experimental parameters were combined for the simulations. Experiments were conducted on a Linux cluster of 10 PC nodes, each having Dual Intel Xeon e5405 (2.0 Ghz) Quad Core Processors (8 processors), 8 GB fully buffered system memory, and a Fedora 7 installation. The computer nodes were connected internally through a dedicated 1Gb network switch. Agents were distributed among the 80 processors. The Java Runtime Environment version was 1.6.0 07.

Algorithm 1 Simulation Experiments

1: for each Network Size  [102, 103, 104] do 2: for each   [0, .., 15] do

3: rewire network with the  value

4: for each Search Method do

5:

for each Query do

6:

assign query to a random agent

7:

repeat

8:

forward query from one another

9:

until relevant agent found OR search path L 

Lmax

10:

if sufficient relevant information found then

11:

send the results back

12:

else

13:

send failure message back

14:

end if

15:

end for

16:

measure effectiveness P , R, and F1

17:

measure efficiency ¯ and L¯

18: end for

19: end for

20: end for

6. RESULTS
We conducted experiments on networks of 102, 103, and 104 systems. We set the max search length length Lmax to 20% of network population so that even less effective/efficient
methods will be able to persist in searches. Figures 5 and

78

Search Path Length (#hops)

6 present results on IR effectiveness (recall, precision, and F1) while Figures 7, 8, and 9 report on efficiency (search path length and time) against different network clustering conditions (guided by clustering exponent ).
6.1 Effectiveness

10

15

Network Size: 100 Similarity Search Similarity*Degree Degree Search Random Walk

20

Search Time (s)

400

600

800

Network Size: 100 Similarity Search Similarity*Degree Degree Search Random Walk

5

200

1.0

1.0

0

0.8

0.8

0.6

RECALL

Network Size: 100 Similarity Search Similarity*Degree Degree Search Random Walk

PRECISION

0.4

0.6

Network Size: 100 Similarity Search Similarity*Degree Degree Search Random Walk

0

5

10

15

20

Clustering Exponent (ALPHA)
(a) Search Length

0

5

10

15

20

Clustering Exponent (ALPHA)
(b) Search Time

Figure 7: Efficiency on Network 100.

0.4

0.2

0.2

0.0

0

5

10

15

20

Clustering Exponent (ALPHA)
(a) Recall

0

5

10

15

20

Clustering Exponent (ALPHA)
(b) Precision

Figure 5: Effectiveness on Network 100

As shown in Figures 5 and 6, the similarity-based search (SIM) and similarity*degree (SimDeg) method performed very well in terms of effectiveness, showing a very large advantage in recall over the degree-based (DEG) and randomwalk (RW) methods. When the network was under some proper clustering conditions (e.g., with   10 for network 10,000), the SIM and Sim*Deg methods achieved nearly 100% recall. Precision was 1.0 for all conditions because a document was retrieved only when it exactly matched the query.

0.0

Search Path Length (#hops)

40

60

80

5 hops and 150 milliseconds to find exact match for each query, it took RW more than 15 hops and 400 milliseconds to reach 20% of targets (a 3-time difference in efficiency). When the network size increased to 10, 000, RW search took 50 seconds and traversed about 1, 500 nodes on average to reach a < 0.4 recall whereas SIM search took less than 4 seconds and roughly 110 nodes to achieve a 1.0 recall ­ a more than 10-time difference in efficiency.

Network Size: 1000 Similarity Search Similarity*Degree Degree Search Random Walk

Network Size: 1000 Similarity Search Similarity*Degree Degree Search Random Walk

100

Search Time (s) 500 1000 1500 2000 2500 3000 3500 4000

20

1.0

1.0

0.6

0.8

F1

Network Size: 1000 Similarity Search Similarity*Degree Degree Search Random Walk

F1

0.6

Network Size: 10000 Similarity Search Similarity*Degree Degree Search Random Walk

0.4

0.4

0.2

0.0

0

5

10

15

20

Clustering Exponent (ALPHA)
(a) F1 on network 103

0

5

10

15

20

Clustering Exponent (ALPHA)
(b) F1 on network 104

Figure 6: Effectiveness on Larger Networks

The DEG search method, biased toward highly linked (popular) sites in the searches, achieved moderate performance between SIM and RW methods and had improved performance in larger networks, e.g., a roughly 0.7 recall in the 10,000-system network. Random walk (RW) consistently performed below a 0.4 recall across all network sizes and  conditions.
6.2 Efficiency
Figures 7, 8, and 9 show very high efficiency of the SIM and SimDeg search methods across the network sizes, especially under stronger clustering conditions. The efficiency gap between the SIM/SimDeg and RW/DEG methods increased dramatically as network size increased. For example, in the 100-node network, while SIM searched roughly

0.8

0

5

10

15

20

Clustering Exponent (ALPHA)
(a) Search Length

0

5

10

15

20

Clustering Exponent (ALPHA)
(b) Search Time

Figure 8: Efficiency on Network 1,000.

Figures 7 - 9 demonstrate that network structure had a great impact on decentralized IR performance, particularly on efficiency in larger networks. While search efficiency (in terms of search path length and search time) under different clustering conditions only differed slightly in the 100-agent network, the difference was much larger in the 10, 000-agent network (Figure 9). For example, the average search path length for the SIM method decreased from 6 to 5 (a 20% difference) when the clustering exponent was changed from 0 (random network) to 10 (strong clustering) in the 100 network. In the 10, 000-agent network, however, the same degree of change in network clustering led to a roughly 200% difference in search efficiency. Statistical tests indicated that SIM search achieved significantly better results with a balanced level of network clustering (i.e., at  = 10) than with over- or weak-clustering networks. The significant differences not only appeared in the 10, 000-system network but also in the 100- and 1000-system networks.
6.3 Scalability
For each network size, we identified network clustering conditions under which superior performance was observed (i.e., at  = 10) and plotted average search path length (efficiency) against network size in Figure 10. As discussed ear-

79

Search Path Length (#hops)

1000

500

Network Size: 10000 Similarity Search Similarity*Degree Degree Search Random Walk

200

100

0

5

10

15

20

Clustering Exponent (ALPHA)

Figure 9: Efficiency on Network 10, 000. Y is log transformed.

lier, SIM and SimDeg searches consistently achieved nearly 1.0 recall and precision across the various network sizes, much better than DEG and RW methods. DEG search tended to perform slightly better in larger networks than in smaller ones. However, as shown in Figure 10, search path length for RW and DEG dramatically increased in larger networks, while the increases for SIM and SimDeg were relatively moderate.
Similarity Search Similarity*Degree Degree Search Random Walk

1500

1000

Efficiency: Search Path Length (#hops)

500

0

100 200

500 1000 2000 Network Size: log(N)

5000 10000

Figure 10: Scalability of all search methods with  = 10. X denotes network size and is log transformed.

6.3.1 Scalability of SIM Search
SIM and SimDeg methods appeared to be much more scalable than RW and DEG methods. To better understand the scalability of SIM search and to predict how it could perform in even larger networks (e.g., a network of millions of nodes), we conducted further analysis on the relationship of its efficiency to network size.
Previous research on complex networks suggested that optimal network clustering supports scalable searches, in which search time is a poly-logarithmic function of network size [15]. We relied on a generalized regression model that modeled search path length L (and search time  ) against log-

transformed network size N . The model was specified to
reach the origin (0, 0) because, when log(N ) = 0 (i.e., N =
1), there is only one node and no effort is needed to search a network. The best fit for search path length L was produced by the model in Table 2, in which L = 0.0275 · log160(N ) with a nearly perfect R2 = 0.9971.

Search Length: L  0 +  log610(N ), where N is network size.

Estimate Standard Error t P r(> |t|)

 0.0275

0.0042

65.73 < 2E-16 ***

R2 = 0.997 (adj. 0.9968), F = 4320 on 1 and 13 DF

Table 2: Search path length vs. Network size

The same model was also applied to identify a poly-logarithmic function of search time  and network size N with a smaller R2 = 0.752. Apparently, search time involves other factors such as machine load fluctuation and is less predictable than search path length.
Overall, the scalability analysis supports search time as a poly-logarithmic function of network size ­ so that when an information network continues to grow in magnitude, it is still promising to conduct effective IR and search operations within a manageable time limit. Although we found the order of the poly-logarithmic relationship to be roughly 6 in this study, a smaller exponent can be expected when other factors on network structure and search methods can be optimized.
6.3.2 Scalability of Network Clustering
Our search methods relied on local indexes and a structure self-organized by distributed systems in the network. Without global information and centralized control, network clustering was performed locally ­ distributed systems formed the network structure in terms of their limited opportunities to interact and individual preferences and constraints on building indexes for others. This local mechanism for clustering demonstrated a high level of scalability. As shown in Figure 11, average clustering time c remained relatively constant, < 1 sec, across all network size scales N  [102, 103, 104, 105].

7. CONCLUSION
We conducted experiments on decentralized IR operations on various scales of information networks and analyzed effectiveness, efficiency, and scalability of proposed search methods. Results showed network structure, i.e., how distributed systems connect to one another, is crucial for retrieval performance. With a balanced level of network clustering under local topical guidance, similarity-based search functions (i.e., SIM and SimDeg) were found to perform very efficiently while maintaining a high level of effectiveness even in very large networks. For example, in searches for single unique documents among the 4.4 million documents distributed among 10, 000 agents/systems, selectively involving only 110 agents within 4 seconds yielded 100% precision and 100% recall with a guiding clustering exponent  = 10. Under these conditions, more importantly, search time was well
1Each of the three X levels has multiple data points. Future work will integrate whether the relationship can be used to predict search efficiency on larger scales.

80

Clustering Time (milliseconds)

100

1000

3000

50

1e+02

Individual clustering time Average clustering time

1e+03

1e+04

Network Size (N)

1e+05

Figure 11: Scalability of Network Clustering

explained by a poly-logarithmic function of network size, suggesting high scalability of the proposed methods.
In addition, the network clustering function that supported very high effectiveness and efficiency of IR operations in large networks is itself scalable. Clustering only involved local self-organization and required no global control ­ clustering time remained roughly constant across the various network sizes N  [102, 103, 104, 105].
This study provides guidance on how IR operations can function and scale when today's information spaces continue to grow in magnitude. Particularly, we have found that connectivity among distributed systems, based on local network clustering, is crucial to the scalability of decentralized methods. The clustering paradox on decentralized search performance appears to have a scaling effect and deserves special attention for IR operations in large scale networks.
Acknowledgment
We appreciate valuable discussions with Gary Marchionini, Munindar P. Singh, Diane Kelly, Jeffrey Pomerantz, Jos´e R. P´erez-Agu¨era, and Simon Spero, and constructive comments from SIGIR'10 reviewers. We thank the NC Translational and Clinical Sciences (TraCS) Institute for support.
8. REFERENCES
[1] L. Adamic and E. Adar. How to search a social network. Social Networks, 27(3):187 ­ 203, 2005.
[2] R. Albert and A.-L. Barab´asi. Statistical mechanics of complex networks. Reviews of Modern Physics, 74(1):47­97, 2002.
[3] R. Baeza-Yates, C. Castillo, F. Junqueira, V. Plachouras, and F. Silvestri. Challenges on distributed web retrieval. ICDE 2007: Data Engineering 2007., pages 6­20, April 2007.
[4] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison Wesley Longman Publishing, 2004.
[5] M. Bawa, G. S. Manku, and P. Raghavan. Sets: search enhanced by topic segmentation. In SIGIR '03, pages 306­313, 2003.

[6] F. L. Bellifemine, G. Caire, and D. Greenwood. Developing Multi-Agent Systems with JADE (Wiley Series in Agent Technology). John Wiley & Sons, 2007.
[7] M. Bender, S. Michel, P. Triantafillou, G. Weikum, and C. Zimmer. Improving collection selection with overlap awareness in p2p search engines. In SIGIR '05, pages 67­74, 2005.
[8] M. Bogun~a´, D. Krioukov, and K. C. Claffy. Navigability of complex networks. Nature Physics, 5(1):74 ­80, 2009.
[9] J. Callan, F. Crestani, and M. Sanderson. SIGIR 2003 workshop on distributed information retrieval. SIGIR Forum, 37(2):33­37, 2003.
[10] A. Crespo and H. Garcia-Molina. Semantic overlay networks for p2p systems. In Agents and Peer-to-Peer Computing, pages 1­13, 2005.
[11] C. Doulkeridis, K. Norvag, and M. Vazirgiannis. Peer-to-peer similarity search over widely distributed document collections. In LSDS-IR '08, pages 35­42, 2008.
[12] M. S. Granovetter. The strength of weak ties. American Journal of Sociology, 78(6):1360­1380, May 1973.
[13] E. Hatcher, O. Gospodneti´c, , and M. McCandless. Lucene in Action. Manning Publications, second edition edition, March 2010.
[14] W. Ke and J. Mostafa. Strong ties vs. weak ties: Studying the clustering paradox for decentralized search. In LSDS-IR'08, pages 49­56, Boston, USA, July 23 2009.
[15] J. M. Kleinberg. Navigation in a small world. Nature, 406(6798), August 2000.
[16] J. Lu and J. Callan. User modeling for full-text federated search in peer-to-peer networks. In SIGIR '06, pages 332­339, 2006.
[17] E. K. Lua, J. Crowcroft, M. Pias, R. Sharma, and S. Lim. A survey and comparison of peer-to-peer overlay network schemes. IEEE Communications Surveys and Tutorials, 7:72­93, 2005.
[18] T. Luu, F. Klemm, I. Podnar, M. Rajman, and K. Aberer. Alvis peers: a scalable full-text peer-to-peer retrieval engine. In P2PIR '06, pages 41­48, 2006.
[19] A. L. Powell and J. C. French. Comparing the performance of collection selection algorithms. ACM Transactions on Information Systems (TOIS), 21(4):412­456, October 2003.
[20] O. Simsek and D. Jensen. Navigating networks by using homophily and degree. Proceedings of the National Academy of Sciences, 105(35):12758­12762, 2008.
[21] G. Skobeltsyn, T. Luu, I. P. Zarko, M. Rajman, and K. Aberer. Web text retrieval with a p2p query-driven index. In SIGIR '07, pages 679­686, 2007.
[22] D. J. Watts, P. S. Dodds, and M. E. J. Newman. Identity and Search in Social Networks. Science, 296(5571):1302­1305, 2002.
[23] I. P. Zarko and F. Silvestri. The CIKM 2006 workshop on information retrieval in peer-to-peer networks. SIGIR Forum, 41(1):101­103, 2007.

81

Estimating Interference in the QPRP for Subtopic Retrieval
Guido Zuccon1, Leif Azzopardi1, Claudia Hauff2, and C. J. "Keith" van Rijsbergen1
1Dept. of Computing Science, University of Glasgow, Scotland (UK) 2Human Media Interaction, University of Twente, Netherlands
{guido, leif, keith}@dcs.gla.ac.uk, c.hauff@ewi.utwente.nl

ABSTRACT
The Quantum Probability Ranking Principle (QPRP) has been recently proposed, and accounts for interdependent document relevance when ranking. However, to be instantiated, the QPRP requires a method to approximate the "interference" between two documents. In this poster, we empirically evaluate a number of different methods of approximation on two TREC test collections for subtopic retrieval. It is shown that these approximations can lead to significantly better retrieval performance over the state of the art.
Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval - Retrieval Models General Terms: Theory, Experimentation Keywords: Quantum Probability Ranking Principle, Interference estimation, Diversity

1. INTRODUCTION

The Probability Ranking Principle (PRP) provides a the-

oretically sound ranking strategy, that assumes the inde-

pendence between document relevance [7]. To move beyond

independence, a number of ranking strategies have been pro-

posed which account for interdependent document relevance

assessments [6, 8, 9]. A theoretically motivated development

that also accounts for this limitation is the recently proposed

Quantum Probability Ranking Principle (QPRP) [11]. How-

ever, to instantiate a retrieval model that upholds the QPRP

the "interference" between documents needs to be estimated

(see [10] for more details). This ranking principle prescribes

that documents should be ranked according to:

! X

di = arg max P (d) +

Id,d

(1)

d RA

!

Xp p

= arg max P (d) +

P (d) P (d ) cos d,d

d RA

where Id,d is the interference between documents d and d , P (d) is the estimated probability of relevance of document d, and RA is the list of documents already ranked. The angle d,d is the difference between the phases of the probability amplitudes associated to documents d and d . The interference term arises because in quantum probability
theory, the total probability obtained from the composition

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

of the probabilities associated to two events is the sum of the probabilities of the events and their "interference" (i.e. pAB = pA + pB + IAB)1 [4]. Under the QPRP, the documents in the ranking share relationships at relevance level, i.e. they interfere with each other, and this interference has to be taken into account when ranking documents.
An open problem in the development of the QPRP is: how to effectively estimate/approximate this interference term? In other words: how can the interference between documents be approximated, and does this translate into effective retrieval performance for tasks, such as subtopic retrieval, where it is imperative that interdependent document relevance is considered? In this poster, we empirically investigate several strategies to approximate the interference term in the QPRP on two TREC collections for subtopic retrieval.

2. APPROXIMATING INTERFERENCE

In this section we propose a number of approaches to ap-

proximate the interference term of an instantiation of the

QPRP in the context of subtopic retrieval. We set cos d,d = -fsim(d, d ), where fsim(d, d ) is a similarity function be-
tween the (normalised) term vectors of documents d and

d .This results in demoting similar documents in the rank-

ing while diverse and novel documents are promoted. With

this substitution, the ipnterferpence term becomes:

Id,d = - P (d) P (d )fsim(d, d )

(2)

In the empirical investigation proposed in our study, the

components of the term vector of a document are the Okapi

BM25 weights corresponding to each term occurring in the

document. We test two different working hypothesis when

computing fsim between the documents already present in the ranking (e.g. d1, . . . , di-1) and the current candidate document d:

1. pairwise: the user judges the interest of the cur-

rent document by comparing it to each of the previous

ranked documents: in this case, the current candidate

and the documents already ranked are compared using

fsim in a pairwise fashion;

2. surrogates: the user judges the interest of the cur-

rent document by comparing it to the knowledge he ac-

quired from documents d1, . . . , di-1: the current candi-
date is then compared against a surrogate of the documents already ranked, which is obtained interpolating2

d1, . . . , di-1.

1As opposed to what happens in Kolmogorovian probability theory, i.e. pAB = pA + pB , when A and B are mutually exclusive events. 2In this work, we linearly interpolate the documents' term vectors to

741

Clueweb TREC 678

Measure -ndcg@10
NRBP IA-P@10
s-r@10
-ndcg@10 NRBP
IA-P@10 s-r@10

PRP .416 .123 .058 .379
.093 .032 .033 .151

PT .424 .126 .062 .384
.105 .029 .041* .178*

Pear. .418 .127 .063 .387
.094 .035 .035 .180*

Pairwise L2 Cos. Jac. .431 .427 .419 .128 .127 .124 .063 .064 .061 .385 .388 .381

.099 .029 .046 .173*

.099 .034 .040 .168*

.097 .035 .038 .165*

KL .413 .117 .062 .389 .115* .043 .047* .190*

Pear. .415 .127 .060 .379
.106* .039 .038 .185*

Surrogates L2 Cos. Jac. .426 .424 .412 .128 .126 .120 .059 .060 .053 .375 .380 .360

.100 .029 .043 .175

.094 .034*
.037 .160

.106* .037 .044 .184*

KL .433 .135 .067 .402
.092 .032 .024 .113

Table 1: Overview of the results obtained over two TREC test collections. Each similarity or correlation function indi-
cates an instantiation of the QPRP where the corresponding function is employed to estimate interference. Statistical significance over the PRP is indicated by *, while  indicates statistical significant improvements over PT.

3. EMPIRICAL STUDY AND RESULTS
To study the effectiveness of the strategies proposed in the previous section, we evaluate the QPRP in the context of subtopic retrieval, employing the TREC 678 interactive collection with the subtopics judgements described in [9], and the recent ClueWeb collection (part B only), along with the topics defined for the Web diversity track. Using the Lemur Toolkit 4.10 we indexed these collections, where stemming was applied and stop words removed. For the QPRP, we test both pairwise and surrogate comparisons, and examine a number of similarity functions to act as fsim: L1 and L2 norms, cosine similarity, Jaccard coefficients, Pearson's correlation, Bayesian correlation score, KL, JSD, skew divergences [5]. The best performing approximations are reported here. Each of the different QPRP approximations were compared against the default PRP baseline, i.e. Okapi BM25 and against the state of the art Portfolio Theory (PT) [8]3. For PT, we tuned the parameters in each collection to maximise -ndcg@10. All the models have been implemented in C++/Lemur and code is available on request. We evaluated each model employing the diversity measures suggested in [1, 2, 3, 9].
In Table 1 we report the results for the PRP (i.e. BM25), PT and a subset of the approximations we tried for the QPRP models. First of all we note that the KL based QPRP model performs the best on each collection when surrogates are used in the TREC 678 and pairwise comparisons are used on ClueWeb. These differences might be due to the limited number of topics available for the TREC 678 collection (20 in total), but also because of the different kind of documents in these collections (newswire articles vs. Web pages). Furthermore, while no significance can be calculated for the improvements on TREC 678 due to the number of topics, improvements over PRP and PT obtained on ClueWeb are statistically significant. The Pearson based QPRP consistently provides excellent retrieval performance regardless of the comparison method - and while this is not always better than the optimised PT, it is not significantly worse, and in
form the surrogate. Alternative approaches might perform weighted interpolations of these vectors, in order to simulate user's memory effects (e.g. documents retrieved at early ranks are weighted less than documents at ranks close to i) or estimated importance of documents (e.g. documents ranked at early positions contribute more in generating the surrogate than lower ranked ones). 3Note, we have treated the variance of a document, 2, as a parameter of the PT model and conducted a grid search of the parameter space b (the user propensity to risk) by 2 to select the optimal run of PT on each employed collection.

fact is significantly better on several measures. It should also be noted that since the QPRP based methods do not require extensive parameter tuning like PT, the KL and Pearson instantiations of the QPRP are highly competitive, simple and attractive alternatives.
4. CONCLUSIONS AND FUTURE WORKS
In this poster we have investigated a number of strategies to approximate the interference term in the QPRP. Our results show that excellent retrieval performances can be consistently obtained when employing the Pearson based QPRP, while, the KL based QPRP provides the best subtopic retrieval performances overall. Future work will examine what type of comparison (i.e. surrogate or pairwise) should be employed given the data collection, along with incorporating other types of approximations of interference within the QPRP.
Acknowledgments. This work is partially funded by EPSRC EP/F014384/.
5. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In WSDM '09, pages 5­14, 2009.
[2] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR '08, pages 659­666, 2008.
[3] C. L. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. In ICTIR '09, pages 188­199, 2009.
[4] R. P. Feynman. The Concept of Probability in Quantum Mechanics. In Proc. 2nd Berk. Symp. on Math. Stat. and Prob., pages 533­541, 1951.
[5] L. Lee. Measures of distributional similarity. In Proc. ACL'99, pages 25­32, 1999.
[6] F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse rankings with multi-armed bandits. In ICML '08, pages 784­791, 2008.
[7] S. E. Robertson. The probability ranking principle in IR. Journal of Documentation, 33:294­304, 1977.
[8] J. Wang and J. Zhu. Portfolio theory of information retrieval. In SIGIR '09, pages 115­122, 2009.
[9] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR '03, pages 10­17, 2003.
[10] G. Zuccon and L. Azzopardi. Using the Quantum Probability Ranking Principle to Rank Interdependent Documents. In ECIR '10, pages 357­369, 2010.
[11] G. Zuccon, L. Azzopardi, and C. J. van Rijsbergen. The quantum probability ranking principle for information retrieval. In ICTIR '09, pages 232­240, 2009.

742

Query Quality: User Ratings and System Predictions

Claudia Hauff Franciska de Jong
University of Twente
Enschede, The Netherlands
{c.hauff,f.m.g.dejong} @ewi.utwente.nl

Diane Kelly
University of North Carolina Chapel Hill, NC, United States
dianek@email.unc.edu

Leif Azzopardi
University Glasgow Glasgow, United Kingdom
leif@dcs.gla.ac.uk

ABSTRACT
Numerous studies have examined the ability of query performance prediction methods to estimate a query's quality for system effectiveness measures (such as average precision). However, little work has explored the relationship between these methods and user ratings of query quality. In this poster, we report the findings from an empirical study conducted on the TREC ClueWeb09 corpus, where we compared and contrasted user ratings of query quality against a range of query performance prediction methods. Given a set of queries, it is shown that user ratings of query quality correlate to both system effectiveness measures and a number of pre-retrieval predictors.
Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval
General Terms: Human Factors, Performance
Keywords: Query Performance Prediction
1. QUERY PERFORMANCE PREDICTION
Estimating the quality (or difficulty) of a query is an important task [3, 4, 8, 9], which can aid in the development of adaptive Information Retrieval (IR) systems. For instance, if an IR system can determine which queries will perform poorly, then actions can be taken to ameliorate performance. The evaluation of Query Performance Prediction (QPP) methods has usually been performed in the context of system effectiveness (e.g., average precision); when a query results in low effectiveness, the query is considered poor or difficult, and conversely when the effectiveness is high, the query is considered good or easy. However, if we wish to develop better adaptive IR systems, it is also important to consider query quality from the user's perspective and to determine whether the user thinks a query is hard or easy. This could be very useful when suggesting or ranking queries, by enabling the system to appropriately respond to the user's perception. And since QPP methods are generally based on "rules of thumb" about how a user might rate a query's performance, it is also interesting to examine whether QPP methods actually reflect the intuitions of human assessors. In this poster, we investigate whether users judge the quality of queries in accord with QPP methods, and if such methods can be used as a proxy for user query ratings.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Related to the study conducted in this poster are two lines of research that have been investigated: (1) user ratings vs. performance and (2) inferred ratings vs. performance. Of the first line, in an experiment in the late 1990's [6], a number of IR researchers were asked to classify TREC topics as either easy, medium or hard for a newswire corpus they were familiar with. The researchers were given the TREC topic statements, though not the search results. It was found that they were largely unable to predict the topics' quality correctly and, surprisingly, they could also not agree among themselves on how to classify the topics. Of the second line, in [5, 7] initial experiments were performed that compared a user based measure (the median time to find a relevant document) with Clarity Score [3] and a range of pre-retrieval QPP methods [7]1. In [5], no significant correlation was found for Clarity Score, while in [7], the best preretrieval predictor achieved a Kendall's Tau rank correlation of  = 0.2. However, these experiments were conducted in limited contexts, i.e. IR researchers on a small data set [6] or using time as an implicit user rating of query quality [5, 7]. Here, we conduct an investigation on a large test web collection, with users who regularly use search engines, and compare their explicit ratings of query quality against system predictions from a range of pre and post-retrieval QPP methods.
2. EMPIRICAL STUDY AND RESULTS
Following on from the previous experiments [5, 6, 7], we performed a similar study but with eighteen post graduate computer science students as assessors using the most recent TREC test corpus: ClueWeb09 (cat. B) [2], a 50 million document crawl of the Web from 2009. We utilized the fifty topics of the TREC 2009 Web adhoc retrieval task2 which consist of a query part (to be submitted to the IR system) and a description (the information need). In this study, we provided the assessors with the queries and descriptions and instructed them to judge on a scale from 1 (poor quality / hard) to 5 (high quality / easy), what they expect the search result quality to be, if the queries would be submitted to a Web search engine. Note, that the queries were not actually submitted to a search engine.
On the system side, we indexed the corpus with the Lemur Toolkit3, with Porter stemming and stopword removal applied. For retrieval, we used a Language Model with Dirich-
1A pre-retrieval QPP method estimates a query's quality before the retrieval stage (it relies on corpus statistics), while a post-retrieval QPP method exploits the ranked lists of results to form an estimate. 2One topic (wt09-20) has no relevant documents and is thus ignored. 3 http://www.lemurproject.org/

743

let Smoothing (µ = 1000). The retrieval effectiveness was measured by estimated average precision (AP) and estimated precision at 30 documents (P@30), two new TREC measures [1]. For system effectiveness predictions we used three pre-retrieval QPP methods: Max. Inverse Document Frequency (MaxIDF ), Summed Term Weight Variability (SumVAR) [8] and Summed Collection Query Similarity (SumSCQ) [8], as well as three post-retrieval methods: Clarity Score [3], Query Feedback [9] and Query Commitment [4]. Note that the pre-retrieval predictors are parameter-free, while the post-retrieval predictors were evaluated over a range of parameters4. Assessor Ratings: To investigate how well ratings of query quality matched system performance, we examined the correlation between the assessors' ratings and AP5. We found that on average across all assessors, the rank correlation was  = 0.33, while the worst/best correlation between ratings and AP was  = 0.20 and  = 0.48, respectively6. To examine this more deeply, we split the set of queries into five equal partitions given the system measures (ordered from high to low). We then averaged all assessor ratings for the queries within each partition. Table 1 shows that the assessors tended to rate the better performing queries higher than the poorly performing queries for both AP and P@30. This indicates that on average assessor ratings were in line with system measures. However, the ratings of query quality among assessors varied considerably, leading to a rather low inter-rater agreement. When comparing all pairs of assessors, we observed a maximum  = 0.54 (linearly weighted Cohen's kappa); the average agreement between all pairs of assessors reached  = 0.36.

Query Partitions
Top Ten
To
Bottom Ten

Performance AP P@30
0.414 0.629 0.298 0.470 0.099 0.272 0.032 0.133 0.005 0.038

Assess. Ratings vs.

AP

P@30

3.87 (1.07) 4.00 (1.01)

3.72 (1.09) 3.53 (1.20)

3.24 (1.37) 3.31 (1.29)

2.79 (1.20) 2.89 (1.33)

2.51 (1.48) 2.40 (1.34)

Table 1: Avg. performance given partitions based on AP and P@30 respectively (columns 2&3); average (std. dev.) assessor ratings given partitions based on AP and P@30 respectively (columns 4&5).
QPP - System Predictions: Table 2 reports the correlation between the system predictions made by each QPP method and system performance (columns 2&3). The most striking result is that the pre-retrieval predictors (SumSCQ and SumVAR) obtained the highest correlations with system performance. This contrasts previous findings obtained on older test collections [3], where it is post-retrieval QPP methods that exhibit higher correlations. We suspect that the post-retrieval methods are adversely affected by the content of the web pages in ClueWeb09 (i.e. they contain a lot of non-informative content, like ads, links, menus, etc.). Ratings vs. Predictions: Finally, we compared the assessor ratings against the QPP system predictions (Table 2, columns 4-6 show these correlations.). Due to the low level
4For post-retrieval predictors, we report the highest correlations obtained given the parameters. 5Results were similar for P@30, where the min., avg. and max. correlations were  = 0.23, 0.36, and 0.47, respectively. 6Based on Kendall's Tau's rank correlation coefficient, significant correlations (p < 0.01) are marked with .

Pre/Post Ret. Predictors MaxIDF SumSCQ SumVAR
Clarity Score Query Feedback Query Commit.

Performance AP P@30
0.35 0.19 0.39 0.35 0.42 0.38
0.27 0.18 0.37 0.29 0.26 0.11

Assessor Ratings Min Avg Max
-0.09 0.09 0.29 0.20 0.31 0.49 0.17 0.28 0.43
-0.10 0.02 0.19 0.12 0.28 0.44
-0.15 0.01 0.18

Table 2: Kendall's Tau correlations: QPP methods vs. performance, and vs. assessor ratings (shown are minimum, average and maximum correlation).

of inter-rater agreement between the assessors, we report the minimum, average and maximum correlation between ratings and predictions. The highest correlations were observed between assessor ratings and the pre-retrieval predictions by SumSCQ. This predictor assigns higher quality scores to more specific queries and was the best indicator of assessor ratings of query quality among all the predictors we evaluated. Although SumSCQ yields significant correlations with most assessors, the correlations are only moderate, at best. Of the evaluated post-retrieval predictors, only Query Feedback resulted in significant correlations when the best parameter was selected.
3. DISCUSSION AND FUTURE WORK
In this poster, we explored the relationship between explicit user ratings by assessors and the system predictions of a number of QPP methods. We found that assessor ratings of query quality are significantly correlated to the predictions of pre-retrieval predictors, but not consistently to post-retrieval predictors. However, while some QPP methods provide a better explanation of user ratings than others, the relationship is still quite weak (with moderate correlations at best). This suggests that current QPP methods are unlikely to be adequate proxies of user ratings. Since most QPP methods only utilize system side information, perhaps there are gains to be had by developing more sophisticated methods/models of query performance prediction that include the user and their state of knowledge in the process. In future work, we will investigate these findings in more detail and also consider how the amount of information provided to the user and the QPP method affects their ability to accurately predict the quality of a query.

4. REFERENCES
[1] J. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In SIGIR '06, pages 541­548, 2006.
[2] C. L. Clarke, N. Craswell, and I. Soboroff. Prelim. report on the TREC 2009 Web Track. In TREC 2009 Notebook Papers, 2009.
[3] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In SIGIR '02, pages 299­306, 2002.
[4] A. Shtok, O. Kurland, and D. Carmel. Predicting query performance by query-drift. In ICTIR '09, pages 305­312, 2009.
[5] A. Turpin and W. Hersh. Do clarity scores for queries correlate with user performance? In ADC '04, pages 85­91, 2004.
[6] E. Voorhees and D. Harman. Overview of the sixth text retrieval conference. In Proceedings of the TREC 6, 1997.
[7] Y. Zhao and F. Scholer. Predicting query performance for user-based search tasks. In ADC '07, pages 112­115, 2007.
[8] Y. Zhao, F. Scholer, and Y. Tsegay. Effective pre-retrieval query performance prediction using similarity and variability evidence. In ECIR '08, pages 52­64, 2008.
[9] Y. Zhou and W. B. Croft. Query performance prediction in web search environments. In SIGIR '07, pages 543­550, 2007.

744

Multi-field Learning for Email Spam Filtering

Wuying Liu
College of Computer National University of Defense Technology
410073 Changsha, Hunan, CHINA

Ting Wang
College of Computer National University of Defense Technology
410073 Changsha, Hunan, CHINA

wyliu@nudt.edu.cn

tingwang@nudt.edu.cn

ABSTRACT
Through the investigation of email document structure, this paper proposes a multi-field learning (MFL) framework, which breaks the multi-field document Text Classification (TC) problem into several sub-document TC problems, and makes the final category prediction by weighted linear combination of several subdocument TC results. Many previous statistical TC algorithms can be easily rebuilt within the MFL framework via turning binary result to spamminess score, which is a real number and reflects the likelihood that the classified email is spam. The experimental results in the TREC spam track show that the performances of many TC algorithms can be improved within the MFL framework.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ information filtering
General Terms
Algorithms, Experimentation, Performance
Keywords
Spam Filtering, Multi-field Learning, Text Feature Selection
1. INTRODUCTION
Currently email spam filtering is normally considered as an online binary Text Classification (TC) task, and many robust statistical TC algorithms have been proposed [1]. In these algorithms, email is often treated as a single plain-text document, and text feature is also extracted within this single document. Actually a full email (often including five natural text fields: Header, From, ToCcBcc, Subject, and Body) is a multi-field text document. Feature extraction from full email document makes many text features disturb each other, and text feature from one field is often noise to other fields.
In statistical TC algorithms, a document is normally represented as a text feature vector. The dimension of feature vector space, the total number of text features, reflects the representational granularity of vector space model. Previous research has shown that overlapping word-level k-grams model can achieve promising results [2]. For email document, single plain-text model (SPTM) and multi-field model (MFM) are two representations. The SPTM ignores the field information of text feature, regarding the same string occurrence in different fields as single text feature, while the MFM treats it as distinct text features. The dimension of
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

feature vector space for trec07p email set is showed in Table 1. For the two email representations, four overlapping word-level models are applied respectively. For MFM, the five natural text fields' information is considered.
Table 1. Dimension of Feature Vector Space.
1-grams 2-grams 3-grams 4-grams SPTM 1,037,395 4,189,054 9,447,962 13,869,560 MFM 1,258,491 4,906,594 10,390,571 14,880,647 Table 1 shows the dimension of MFM is larger than that of SPTM for each k-grams model. For instance, this obvious difference between two representations reaches 1,011,087 for 4-grams model. The result from Table 1 indicates that text feature noises exist indeed in SPTM. Because more finely granular text feature can reduce the noises and increase the TC accuracy, this paper proposes a multi-field learning (MFL) framework, which is an alignment technique of text feature sources. In MFL framework, text features are enhanced by field information, and the disturbances among text features from different fields are expected to be reduced.

2. MULTI-FIELD LEARNING
In order to reduce the text feature noises of multi-field document, the proposed MFL framework makes use of multi-field structural feature by the divide-and-conquer strategy. Figure 1 shows the MFL framework for multi-field document binary TC. The framework includes a Splitter, a Combiner, and several Scorers. The Splitter analyses a multi-field document, and splits it to several sub-documents according to the natural field structure or some explicit rules. The text feature extracting, the scorer training and updating, and the sub-document predicting are only localized in the sub-documents from the same field. Each scorer calculates a spamminess score (SS) for its corresponding sub-document, and sends the SS to the Combiner. The Combiner combines multiscorer's SSs to form the final SS, which is a real number in [0, 1]. If the final SS is in [0, 0.5], then the document is predicted as a ham, otherwise, if the final SS is in (0.5, 1], it is predicted as a spam.

Multi-field Document
Splitter

Field1 Sub-document Scorer1
Field2 Sub-document Scorer2

...

...

Fieldn Sub-document

Scorern

SS1 SS2 CombinerSS
SSn

Figure 1. Multi-field Learning Framework.

745

In MFL framework, the weighted linear combination method is designed for combining n scorer's output scores to form the final SS. The formula of calculating SS is SS=iSSi, (i=1, 2, ... , n), where SSi indicates the ith scorer's output SS, and the weight i indicates the historical classification ability of the ith scorer. The normalized TC accuracy rates are used to estimate the weights.
Except five natural sub-documents from the five natural text fields of email, some artificial sub-documents can be extracted by some explicit rules. For instance, the regular expression can be applied to extract all IP addresses in email Header to form an artificial sub-document. This artificial method can generate a new field sub-document which does not exist in actual multi-field document, which is equivalent to increasing the statistical weight for some attributed texts, and such texts often have an explicit optimal TC rule.
It is nearly a supervised online binary TC process that the scorer receives a sub-document and calculates a SS according to its TC model. Previous supervised online binary TC algorithms can be rebuilt into these scoring algorithms by changing a binary output to a continuous SS output. So, MFL framework is a general frame for ensemble previous TC algorithms.
3. EXPERIMENT
Email is a typical multi-field document, so this paper verifies the validity of MFL framework through the email spam filtering experiment of Immediate Full Feedback defined in the TREC2007 spam track [3]. The TREC spam filter evaluation toolkit and the associated evaluation methodology are applied. Experiment corpus is trec07p email set. The running hardware environment is a PC with 1GB memory and 2.80GHz Pentium D CPU.
A MFL framework of seven sub-documents for email document is implemented, in which the Splitter extracts five sub-documents (Header, From, ToCcBcc, Subject, and Body) by natural field structure and extracts two sub-documents (H.IP, H.EmailBox) by regular expressions. The H.IP contains IP address text and H.EmailBox contains Emailbox address text within email Header. Each scorer's historical SS outputs can be drawn to a receiver operating characteristic (ROC) curve. The percentage of the area below the ROC curve (ROCA%) indicates the historical classification ability, and the ROCA% is reasonable to estimate the classification accuracy rate of a scorer. So, before an email classified, the MFL framework normalize current seven ROCA% values to estimate the weights of scorers.
To verify that MFL framework's effect on improving the performance of previous TC algorithms, two typical online TC algorithms are run in MFL framework. The bogo filter (bogo0.93.4) is a classical implementation of online Bayesian statistical algorithm [4], while the tftS3F filter is based on relaxed online SVMs algorithm and has gained several best results in the TREC2007 spam track [5]. We report (1-ROCA)% overall performance, where 0 is optimal. Table 2 shows the overall performance of filters affected by this paper proposed approaches in the rank reference of top three filters in the TREC2007 spam track whose font is italic. In Table 2, the (.mfl) postfix indicates running in MFL framework. The experimental results show that the bogo filter's (1-ROCA)% is optimized from original 0.1558 to mfl's 0.0103, and the tftS3F filter's (1-ROCA)% is also optimized from original 0.0093 to mfl's 0.0083.

Table 2. Overall Performance of Email Spam Filtering.

wat3 tftS3F.mfl tftS3F bogo.mfl fdw4 bogo

(1-ROCA)%0.0055 0.0083 0.0093 0.0103 0.0109 0.1558

TREC Rank 1

2

3

Table 2 shows that the performance of the online Bayesian and

relaxed online SVMs algorithms can be improved within the MFL

framework, which demonstrates the advantage of MFL framework.

The improvement of MFL framework can be explained in two

main reasons: (1) The MFL framework can reduce the

disturbances among text features from different fields; (2) Multi-

field ensemble learning has statistical, computational and

representational advantages [6].

4. CONCLUSION
This paper elucidates that the structural feature of multi-field document is very useful for statistical TC algorithm. The proposed MFL framework represents more finely granular text feature with field information, and takes advantage of the structural feature. The experiment shows that MFL framework can improve the performance of many TC algorithms. Moreover, MFL framework is suitable to parallel running environment, if it is applied on the reduplicate hardware for multiple scorers, the theoretical computational time of MFL framework to classify a document is nearly equal to the lowest scorer's running time.
Further research will concern semi-supervised learning, active learning, and personal learning for spam filtering within MFL framework. We will apply large-scale unlabeled emails, select effective samples for training by mining differences among multiple scorers of MFL framework, and improve the TC model for both global and personal filtering.

5. ACKNOWLEDGMENTS
The research is supported by the National Natural Science Foundation of China (No.60873097, No.60933005) and Program for New Century Excellent Talents in University (No.NCET-060926). Many thanks to Dr. D. Sculley for his tftS3F filter code.

6. REFERENCES
[1] Gordon V. Cormack. Email spam filtering: a systematic review. Foundations and Trends in Information Retrieval, 1(4):335-455, 2008.
[2] H. Drucker, D. Wu, V. N. Vapnik. Support vector machines for spam categorization. IEEE Transactions on Neural Networks, 10(5):1048-1054, 1999.
[3] Gordon V. Cormack. TREC 2007 spam track overview. In TREC2007: Proceedings of the 16th Text REtrieval Conference, National Institute of Standards and Technology, Special Publication 500-274, 2007.
[4] Paul Graham. Better bayesian filtering. In Proceedings of the 2003 Spam Conference, January 2003.
[5] D. Sculley, Gabriel M. Wachman. Relaxed online SVMs for spam filtering. In SIGIR'07: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 415-422, 2007.
[6] Thomas G. Dietterich. Ensemble methods in machine learning. In MCS2000: Proceedings of the Multiple Classifier Systems, pages 1-15, 2000.

746

Semi-Supervised Spam Filtering using Aggressive Consistency Learning

Mona Mojdeh and Gordon V. Cormack
Cheriton School of Computer Science University of Waterloo
Waterloo, Ontario, Canada
{mmojdeh,gvcormac}@uwaterloo.ca

ABSTRACT
A graph based semi-supervised method for email spam filtering, based on the local and global consistency method, yields low error rates with very few labeled examples. The motivating application of this method is spam filters with access to very few labeled message. For example, during the initial deployment of a spam filter, only a handful of labeled examples are available but unlabeled examples are plentiful. We demonstrate the performance of our approach on TREC 2007 and CEAS 2008 email corpora. Our results compare favorably with the best-known methods, using as few as just two labeled examples: one spam and one non-spam.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Filtering
General Terms
Experimentation, Measurement
Keywords
Spam, Email, Filtering, Classification
1. INTRODUCTION
Semi-supervised methods are of special interest when there are very few training samples available. In many machine learning applications, there is always great human effort involved in labeling samples, while obtaining unlabeled data is fairly simple. This is the case for spam filters. During the initial deployment of spam filters, a normal user may be willing to provide only a few labeled examples for training but will still expect correct classification of a large number of emails. Another application is personalized spam filtering with low label cost, using per-user semi-supervised filters with few labeled examples to augment a global filter.
In this paper we address the problem of email spam filtering with very few correct training samples using graph based semi-supervised learning methods. Previous semisupervised methods such as Transductive SVM and Logistic Regression and Dynamic Markov Compression with self training for spam filtering have yielded mixed results [4]. In this paper we are focused on the special situation in which
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

the first handful of messages are labeled and used to filter the rest.
We present an aggressive graph-based iterative solution modeled after the local and global consistency learning method of Zhou et al. [5]. The same method is applied for detecting web spam in [3]. Local consistency guarantees that the nearby points are likely to have the same label; while the global consistency guarantees that the points on the same structure are likely to have the same label. We have also applied Single Value Decomposition to find the most informative terms. Our experiments show a comparatively high performance of our method in the presence of very few training samples.
2. AGGRESSIVE CONSISTENCY LEARNING METHOD
Given a sequence of n email messages and labels denoting the true class ­ spam or nonspam ­ of each of the first nlabeled n, we consider the problem of finding the class of the remaining n - nlabeled messages. Algorithm 1 demonstrates the details of our method. The input matrix Xnm represents the feature vector of the messages; n is number of messages and m is the number of terms, and Yn1 is the labels of messages; {yi  {-1, 1} for i  nlabeled and yi = 0 for i > nlabeled}. The output of the algorithm is {yi  {-1, 1} for i > nlabeled}.
The n × n symmetric Gaussian affinity matrix A captures the similarity between each pair of messages xi and xj, where xi - xj 2 is the Euclidian distance between messages xi and xj. A is then normalized by constructing L = D-1/2AD-1/2 [5]. The   (0, 1) parameter in line 4 of the algorithm, determines the relative amount of information that each node in the graph receives from its neighbors. It is worth mentioning that self-reinforcement is avoided since the diagonal elements of the affinity matrix are set to zero in the first step.
The main contribution of this algorithm is the aggressive approach in updating the affinity matrix. A large number of elements in the affinity matrix are approximately zero due to the large Euclidean distances between messages meaning that the messages do not share many terms. In our aggressive definition of affinity matrix, for all zero rows or columns in equation (1), a "1" (equivalently, a link in the graph) is inserted where the distance between the two corresponding messages is minimum in that column or row. Although adding a link in this case may seem too "aggressive", the simulation results show the improved performance.
Moreover, in order to better handle the sparsity of the

751

Algorithm 1 Aggressive Consistency Learning Method (ACLM)
Input: X, Y , ,  1: Compute Affinity matrix

Aij =

e-

xi -xj 22

2

for i = j

(1)

0

for i = j,

2: For all j such that i Ai,j  0 : Arj = 1 where r = arg minj xr - xj
3: Compute L = D-1/2AD-1/2 where

n

Dii = Aij .

(2)

j=1

4: Y = (1 - )(I - L)-1L

affinity matrix A, we also propose to reduce the dimensionality of matrix X. By applying Singular Value Decomposition (SVD)[2] on matrix X; we find the most informative terms in X and replace X with its approximate. In other words, X = U  V -1, we only keep the rank highest singular values of X; so {i,i = 0  i > rank}.
3. EXPERIMENTS AND RESULTS
We compare the effectiveness of ACLM with the supervised and transductive modes of SV M light [1] (denoted SVM and TSVM). We have compared these methods on two email corpora, TREC 2007 Public Corpus 1 and CEAS 2008 Public Corpus 2. From each corpus we have selected the first 10, 000 from which the first 1000 were used for tuning purposes to figure out the three main parameters , , and rank.
For the actual experiment, we divided the remaining 9000 messages into batches of 1000, getting 9 batches. For each batch we used the first 100 messages to select a balanced training set (same number of spam and non-spam) and the remaining 900 messages as the test set. We report mean error rate, as average over all batches.
Each message was abstracted as a binary feature vector representing word occurrences within the whole email, including headers. We removed terms with document frequency of less than 5 in the training and test sets combined. Binary term frequency was then used for the terms. Raw term frequency was also investigated, but did not provide better results than binary weights.
For parameters of SVM and TSVM, several values were adjusted but no improvement over their default values was observed. The p parameter in TSVM, representing the proportion of spam messages to be expected, was tuned using our tuning set of emails.
Fig. 1 shows the results of the methods on CEAS08 and TREC07 corpora. ACLM with SVD gives best performance of all methods between 4 and 32 labeled examples, mostly having less than 0.01 error rate. TSVM only performs best with fewer than 4 examples. We have previously seen similar results in [4] where TSVM was performing better than SVM only when the train and test sets were from two completely different sources. SVM does not give best performance on CEAS08 even with 30 labeled examples.
1trec.nist.gov/data/spam.html 2www.ceas.cc/challenge

Figure 1: Error rate for ACLM (with SVD and without), SVM, TSVM on CEAS08 (up) and TREC07 (bottom) corpora

0.35

ACLM no SVD

0.3

ACLM with SVD

SVM 0.25
TSVM

0.2

Error Rate

0.15

0.1

0.05

0

2

4

8

16

32

Number of Labeled Examples (log scale)

0.3 ACLM no SVD
ACLM with SVD 0.25
SVM
TSVM 0.2

Error Rate

0.15

0.1

0.05 2

4

8

16

32

Number of Labeled Examples (log scale)

4. REFERENCES
[1] SVM Light. http://svmlight.joachims.org/.
[2] O. Alter, P. Brown, and D. Botstein. Singular value decomposition for genome-wide expression data processing and modeling. In Proc Natl Acad Sci, USA, 2000.
[3] C. Castillo, D. Donato, V. Murdock, and F. Silvestri. Know your neighbors: Web spam detection using the web topology. In 30st ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2007), Netherlands, 2007.
[4] M. Mojdeh and G. Cormack. Semi supervised spam filtering: Does it work? In 31st ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008), Singapore, 2008.
[5] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Scholkopf. Learning with local and global consistency. In Advances in Neural Information Processing Systems 16 (NIPS 2003), pages 321­328. MIT Press.

752

Re-examination on Lam% in Spam Filtering

Haoliang Qi*, Muyun Yang**, Xiaoning He**, Sheng Li**

*Department of Computer Science and Technology,

**School of Computer Science and Technology,

Heilongjiang Institute of Technology

Harbin Institute of Technology

No.999, Hongqi Street, Harbin, China

No.92,West Dazhi Street, Harbin, China

Email:haoliang.qi@gmail.com, ymy@mtlab.hit.edu.cn, nxnh@qq.com, lisheng@hit.edu.cn

ABSTRACT
Logistic average misclassification percentage (lam%) is a key measure for the spam filtering performance. This paper demonstrates that a spam filter can achieve a perfect 0.00% in lam%, the minimal value in theory, by simply setting a biased threshold during the classifier modeling. At the same time, the overall classification performance reaches only a low accuracy. The result suggests that the role of lam% for spam filtering evaluation should be re-examined.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Information filtering
General Terms
Algorithms, Measurement, Performance, Theory
Keywords
Spam Filtering; Lam%; Measurement
1. INTRODUCTION
The spam filtering is generally regarded as a binary classification task to identify spams from normal e-mails (i.e. hams). To evaluate the performance of spam filters, the overall classification accuracy (or the total proportion of misclassified messages) is not a good choice because all errors are treated on equal footing. Nevertheless, the logistic average misclassification percentage (lam%), a single quality measure based only on the filter's binary ham/spam classification accuracy, is proposed as: lam% = logit-1 ((logit(hm%)+logit(sm%))/2), where logit(x)=log(x/(1-x)), in which hm% is ham misclassification percentage, and sm% is spam misclassification percentage[1]. As the geometric mean of the odds of ham and spam misclassification[1], lam% is widely adopted together with 1-AUC by open spam filtering competitions including TREC Spam Filtering Track and CEAS(Conference on Email and Anti-Spam) Spam-filter Challenge. The lam% is designed to impose no a priori relative importance on either ham or spam misclassification, rewarding equally an improvement in the odds of either. However, this paper demonstrates that lam% is inherently defected in allowing a 0.00% by a biased threshold, whereas maintaining a very high error rate. Therefore, it is not reliable enough to observe only lam% as the measure for spam filtering evaluation.
Copyright is held by the author/owner(s). SIGIR 2010, July 19-23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. OPTIMIZATION OF LAM%
The optimization of lam% evolves its definition as the following:
lam% = logit-1 logit(hm%) + logit(sm%) 2
= logit-1((log hm% + log sm% ) / 2) 1 - hm% 1 - sm%
= logit-1((log hm%sm% ) / 2) (1 - hm%)(1 - sm%)
where logit(x)= log(x/(1-x)), and logit-1(x) , the inverse function of logit(x), is defined by logit-1(x) = ex .
1+ ex
In order to minimize lam%, reviewing the monotonic property of
the function lam%, i.e. logit-1(x) , is necessary. Because
[logit-1(x)]' = ex > 0 , logit-1(x) is a monotone increasing (1+ ex )2
function. Consequently, lam% can be minimized by the minimum (log hm%sm% ) / 2 .With either hm% approaching 0
(1 - hm%)(1 - sm%) while sm% unequal to 1, or sm% approaching 0 while hm% unequal to 1, (log hm%sm% ) / 2 can be minimized.
(1 - hm%)(1 - sm%) Therefore a threshold of a spam filter could be set toward either side of hm% or sm%, and this will minimize (log hm%sm% ) / 2 . However an over biased threshold
(1 - hm%)(1 - sm%) such as 1 in probability model will lead to hm%=0, resulting to compute log(0) or to be divided by 0 when computing lam%. Therefore, the threshold should be well determined in order to make a desired hm% and sm%.
Each message with its score above the threshold is labeled as spam, otherwise ham. Through setting a biased threshold, i.e. setting the threshold very high or very low, hm% or sm% could approach zero, and so lam% will tend eventually to be zero. In this sense, the way to optimize lam% via a biased threshold trick is independent of specific filtering models. And Section 3 presents the details of optimizing lam% under the framework of Logistic Regression.
3. BIASED LOGISTIC REGRESSION
Logistic Regression (LR), the state of art spam filtering model [2][3], is used as an example to optimize lam% to zero. In LR model, the prediction of a message is calculated according to logistic function which is described in Equation 1.

757

wph(eyrie=xrsipaismth| exr

i ) = exp( vector of

xr i · wr )/(1+exp(xr i · wr )) the message's features,

vector, and yi is the prediction of the message

wr is xr i .

1 the weight By using

Equation 1, the prediction value is converted between 0 and 1

from a real number ranging from - to +. Algorithm 1 in the

following presents the pseudo code of LR.

((A(123l)))gfoworrripet=ha=m0che;1x/x:r/pii(nL,xryiotii'iga·/il/swirztyie)ci'/(wRi1se+eaigegrgxheoptssl(sdxitroeoinn·0lwarb)e)l

(4) if ( p >0.5 ) predict spam;

(5) else

predict ham;

(6)

if

(

y i'

== uur

1) // uur

y i'

is a spam uur

(7)

w= uur

w + (1-p) ×

uur

uur

xi × TRAIN_RATE

(8) else w = w - p × xi × TRAIN_RATE

where TRAIN_RATE is the learning rate, i.e. the learning speed,

yi' in Line 2 and Line 6 is the golden judgment of xi.

Usually, the prediction is spam if p > 0.5, or ham otherwise (see Line 4 and 5). As mentioned in Section 2, lam% can be optimized by setting the biased threshold. The threshold can be set nearly to 1 or 0, which can decrease hm% or sm% to 0. LR with the biased threshold is referred as biased LR. For biased LR, pseudo codes are the same to the traditional LR except that Line 4 is replaced by "if ( p > threshold ) predict spam". Note that the weight updating in biased LR is not changed, and thus the final prediction scores of messages are not biased (i.e. not changed).

TONE (Train On or Near Error) is adopted to train the spam filter in the actually modeling process. Two types of samples are trained: (1) misclassified samples by the filter; and (2) correctly classified samples if falling within a predefined classification boundary.

Character 4-grams, the first 3,200 features of each message and binary feature scoring are used similar to ref. [2].

4. EXPERIMENTAL RESULTS
We evaluated the lam% optimization method on TREC public spam filtering datasets. The basic information of the datasets is shown in Table 1.
Table 2 shows the experimental results which are acquired by TREC Spam Filter Evaluation Toolkit. hm%, sm% and 1-AUC
are also reported. The parameters of LR and the biased LR are the
same except the threshold: TRAIN_RATE = 0.002, TONE = 0.45, and threshold = 0.999999 for biased LR.
By analyzing the experimental results, we can see that the proposed method decreases lam% to 0.00%, i.e. the minimal value in theory, across all datasets. Although lam% is perfect, the accuracy of the filters for most cases is below 50%, i.e. the performance of random guess. Therefore, there is an obvious defect in lam% as the measure for spam filtering evaluation.
It is very interesting that 1-AUC (area under the ROC curve), the other measure of spam filtering, is left untouched when biasing the threshold. This means that optimizing lam% method proposed

in this paper does not hurt 1-AUC. 1-AUC can be computed as 1AUC = SwappedPairs/m/n [4][5], where m and n are the number of golden spam messages and that of golden ham messages, respectively. Swapped pairs are defined as pairs that are ranked in the wrong order. Hence, ROC area also has a probabilistic interpretation: the probability that a random ham will receive a lower score than a random spam[1]. Because of this pair-wise correct measure [4] and unchanged prediction score of the biased LR, 1-AUC is unchanged too.

Table 1. Statistics of experimental corpora

Corpus Language Ham Spam Total

TREC05p English

39399 52790 92189

TREC06p English

12910 24912 37822

TREC07p English

25220 50199 75419

TREC06C

Chinese

21766 42854

Table 2. Experimental results

64620

Corpus TREC 05p
TREC 06p
TREC 07p
TREC 06C

Filter
LR Biased LR LR Biased LR LR Biased LR LR Biased LR

hm%
0.37 0.00 0.76 0.00 0.36 0.00 0.11 0.00

sm%
0.54 87.25 0.45 98.10 0.06 89.03 0.10 93.17

lam % 0.44 0.00 0.58 0.00 0.15 0.00 0.10 0.00

Accura cy (%)
99.54 50.04 99.44 35.38 99.84 40.74 99.90 38.21

1-AUC (%) 0.0124 0.0124 0.0308 0.0308 0.0054 0.0054 0.0013 0.0013

5. CONCLUSION
As a popular measure for the spam filtering performance, lam% is widely applied. However, this paper discloses that lam% fails when setting biased threshold. The suggestion is that other measures instead of the lam% should be focused for the spam filtering evaluation.

6. ACKNOWLEDGMENT
The research is supported by the Key Project of the National Natural Science Foundation of China (No. 60736044) and National Natural Science Foundation of China (No. 60873105).

7. REFERENCES
[1] G. Cormack, T. Lynam. TREC 2005 Spam Track Overview. TREC 2005. 2005
[2] J. Goodman, W. Yih. Online Discriminative Spam Filter Training. CEAS 2006. 2006
[3] G. Cormack. University of Waterloo Participation in the TREC 2007 Spam Track. TREC 2007. 2007.
[4] T. Joachims. A Support Vector Method for Multivariate Performance Measures. ICML 2005. 2005.
[5] H.B. Mann and D.R. Whitney. On a Test of Whether One of Two Random Variables Is Stochastically Larger Than the Other. Ann. Math. Statist., 18, 1947.

758

Unsupervised Estimation of Dirichlet Smoothing Parameters

Jangwon Seo jangwon@cs.umass.edu

W. Bruce Croft croft@cs.umass.edu

Center for Intelligent Information Retrieval Department of Computer Science
University of Massachusetts, Amherst Amherst, MA 01003

ABSTRACT
A standard approach for determining a Dirichlet smoothing parameter is to choose a value which maximizes a retrieval performance metric using training data consisting of queries and relevance judgments. There are, however, situations where training data does not exist or the queries and relevance judgments do not reflect typical user information needs for the application. We propose an unsupervised approach for estimating a Dirichlet smoothing parameter based on collection statistics. We show empirically that this approach can suggest a plausible Dirichlet smoothing parameter value in cases where relevance judgments cannot be used.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms: Algorithms, Measurement, Experimentation
Keywords: Dirichlet smoothing, unsupervised approach, parameter estimation
1. INTRODUCTION
Dirichlet smoothing is known to be one of the most effective smoothing techniques for the language modeling-based retrieval framework [5]. This smoothing technique has a free parameter, i.e. the Dirichlet smoothing parameter. A standard approach for determining this parameter is to choose a value which maximizes a retrieval performance metric using relevance judgments. We call this supervised approach metric-based estimation of Dirichlet smoothing parameters.
We do not, however, always have relevance judgments as given by TREC standard test collections. For example, we may use new document collections where there are no relevance judgments. Even when we have relevance judgments for a collection, we may be addressing different search tasks from those for which relevance judgments are made. Furthermore, the characteristics of actual user queries can be different from the queries associated with relevance judgments used for training the smoothing parameter. For example, if most queries used in relevance judgments are long, while real queries are short, then the trained value may not work well because the smoothing parameter is sensitive to query lengths as well as document lengths [2]. In such cases, we cannot use metric-based estimation.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

To tackle these situations, we propose an unsupervised estimation approach. This method estimates a Dirichlet smoothing parameter from collection statistics, specifically, a variance of multinomial parameters associated with each term. Therefore, this estimation is independent of specific queries or relevance judgments. Note that if a test collection with relevance judgments is available, we cannot say that our unsupervised approach can produce a better smoothing parameter than the supervised approach. In this work, we intend to introduce an estimation technique which can be used when the supervised approach cannot be used.
There are few formal studies for determining Dirichlet smoothing parameters for retrieval models in an unsupervised manner. However, the average document length of a collection is sometimes used as the parameter value [1, 6, 4]. Also, in the Machine Learning literature, Minka [3] has presented maximum likelihood estimation for Dirichlet distributions.

2. UNSUPERVISED ESTIMATION
Dirichlet smoothing assumes that a document can be represented by a multinomial distribution, Multi(1, 2, · · · , N ), where N is the size of vocabulary of collection C. Introducing a Dirichlet prior, Dir(1, · · · , N ), we choose the mean of the posterior distribution as a smoothed document representation given by p(i|D) = (tfi,D + i)/(|D| + 0), where D is a document, i is an index corresponding to a unique term, and 0 = j j . A typical choice for 's is i =  · mi, where mi = cfi/|C|. Then, the mean of the Dirichlet prior, E[i] = i/ j j = mi, is independent of . On the other hand, the variance of the Dirichlet prior, Var[i] = [i(0 - i)]/[20(0 + 1)] = mi(1 - mi)/( + 1), is closely related to the choice of . Therefore, the variance can be parameterized by .
Assuming that a smoothing parameter should reflect col-
lection statistics well, we choose  which minimizes the following squared error of variances.

e() =
i

V¯i - Var[i]

2
=

Var[i]

i

V¯i( + 1) mi(1 - mi)

-

1

2

where V¯i is the sample variance.

Via

de(µ) dµ

=

0,

a

closed

form

solution

is

obtained

by

=

V¯i i mi(1 - mi)

/

V¯i2 i m2i (1 - mi)2

(1)

V¯i can be computed by DC(pML(i|D) - mi)2, where

759

Avg.#terms of short queries Avg.#terms of long queries
short long avgdl est

AP 2.5 5.1 4000 1900 464 2560

WSJ 2.5 5.1 2300 1200 449 1563

GOV2 2.4 3.8 3700 800 949 1011

Table 1: Average query lengths of split topic sets and four Dirichlet smoothing parameters. short and long are parameters trained for short queries and long queries, respectively. avgdl is the average document length. est is estimated by our proposed method.

short long avdl est

AP Short Long 0.1359 0.1097 0.1344 0.1114 0.1304 0.1030 0.1344 0.1109

WSJ Short Long 0.2255 0.1840 0.2206 0.1853 0.2107 0.1769 0.2235 0.1847

GOV2 Short Long 0.1532 0.1367 0.1456 0.1479 0.1466 0.1479 0.1477 0.1477

Table 2: Retrieval results for short queries and long queries according to different Dirichlet smoothing parameters. A number is a MAP score.

pML(i|D) is the maximum likelihood estimator of a language model, i.e. tfi,D/|D|. However, since computations crossing all terms and all documents are required, this is practically infeasible in case of large collections. Therefore, we use a sampling and approximation approach. First, we randomly sample T terms from a collection and consider only these terms instead of all terms in vocabulary. Then, we exploit the fact that each term occurs very sparsely in documents. That is, in many cases, tfi,D = 0. Accordingly, we consider an approximation, V¯i  m2i . Using this approach, Equation (1) can be easily computed. We call this unsupervised approach variance-based estimation of Dirichlet smoothing parameters.
3. EXPERIMENTS
We conducted experiments to evaluate our unsupervised estimation method. We used three standard TREC collections: AP (topic 51-150), WSJ (topic 51-150) and GOV2 (topic 701-800). Each document is stemmed by the Krovetz stemmer and stopped by a standard stopword list. To simulate situations where the characteristics of training queries are different from those of test queries, we split the topics into two subsets with the same size according to the number of terms in the topic titles, i.e. short queries and long queries.

4000 3000 2000 1000
0

1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

Figure 1: Estimated Dirichlet smoothing parameters (y-axis) according to the numbers of sample terms (x-axis) on the AP collection.

For each collection, we considered four Dirichlet smoothing parameters. Two of them are values which maximize mean average precision (MAP) for short queries and long queries, respectively. To find the values, we swept [500, 4000] with stepsize 100. Another is the average document length of each collection that is often used as an unsupervised heuristic for Dirichlet smoothing parameters. The last one is a value computed by our proposed method (with T = 3000). Table 1 shows these values. As you see, even though relevance judgments are built on the same collection, there is a substantial divergence between the Dirichlet smoothing parameters trained for different types of queries. While the average document length does not appear close to the trained values, a parameter estimated by our unsupervised approach appears between two trained values. That is, this method seems to produce a plausible value.
We evaluated retrieval performance of these smoothing parameters for short queries and long queries. Table 2 shows the results. The average document length produces consistently poor performance. Also, parameters trained with a specific type of query (short and long) do not generalize well to different types of queries. This shows that when making relevance judgments, accurate prediction of the characteristics of actual user queries is necessary so that the supervised approach is effective. On the other hand, parameters estimated by our unsupervised method, while not the best, do produce reasonable (i.e., the second best) performance regardless of the type of query for all collections.
To see how our method depends on the number of sample terms T , we tried various T 's as shown in Figure 1. This shows that the Dirichlet smoothing parameter value appears stable after T = 3000. That is, the dependence on T is not substantial when a sufficient number of terms are used.
4. CONCLUSIONS
We proposed an unsupervised estimation approach for determining Dirichlet smoothing parameters. This method was shown empirically to be able to produce a plausible parameter. Furthermore, this method is relatively stable and robust in that it is independent of the characteristics of queries and relevance judgments. Therefore, it can be applied to cases that relevance judgments cannot be used or are not applicable.
Acknowledgments: This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS-0534383. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
5. REFERENCES
[1] H. Fang and C. Zhai. An exploration of axiomatic approaches to information retrieval. In SIGIR '05, 2005.
[2] D. E. Losada and L. Azzopardi. An analysis on document length retrieval trends in language modeling smoothing. Information Retrieval, 11(2), 2008.
[3] T. Minka. Estimating a Dirichlet distribution. [4] D. Petkova and W. B. Croft. Hierarchical language models
for expert finding in enterprise corpora. In ICTAI '06, 2006. [5] C. Zhai and J. Lafferty. A study of smoothing methods for
language models applied to Ad Hoc information retrieval. In SIGIR '01, 2001. [6] J. Zheng and Z. Nie. Language models for web object retrieval. In NISS '09, 2009.

760

On Performance of Topical Opinion Retrieval

Giambattista Amati
Fondazione Ugo Bordoni Rome, Italy gba@fub.it

Giuseppe Amodeo
University of L'Aquila L'Aquila, Italy
gamodeo@fub.it

Valerio Capozio
University "Tor Vergata" Rome, Italy
capozio@mat.uniroma2.it

Carlo Gaibisso
IASI - CNR Rome, Italy carlo.gaibisso@iasi.cnr.it

Giorgio Gambosi
University "Tor Vergata" Rome, Italy
gambosi@mat.uniroma2.it

ABSTRACT
We investigate the effectiveness of both the standard evaluation measures and the opinion component for topical opinion retrieval. We analyze how relevance is affected by opinions by perturbing relevance ranking by the outcomes of opiniononly classifiers built by Monte Carlo sampling. Topical opinion rankings are obtained by either re-ranking or filtering the documents of a first-pass retrieval of topic relevance. The proposed approach establishes the correlation between the accuracy and the precision of the classifier and the performance of the topical opinion retrieval. Among other results, it is possible to assess the effectiveness of the opinion component by comparing the effectiveness of the relevance baseline with the topical opinion ranking.
Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Performance evaluation (efficiency and effectiveness)
General Terms: Theory, Experimentation
Keywords: Sentiment Analysis, Opinion Retrieval, Classification
1. INTRODUCTION
Opinion mining aims to classify sentences or documents by polarity of opinions. The application of opinion mining to IR (named Topical Opinion Retrieval) deals with ranking documents according to both topic relevance and opinion content. Topical Opinion Retrieval goes back to the novelty track of TREC 2003 [11] and the Blog tracks of TREC [7, 4, 8].However, there is not yet a comprehensive study of the interaction and the correlation between relevance and sentiment assessments. For example, the best runs based on the best official topic relevance baseline (baseline4) in the blog track of TREC 2008 (short topics 1001-1050) [8] achieve the MAPR value equal to 0.4724, that drops to the MAPO|R of opinion equal to 0.4189, and to MAP equal to 0.1566 and 0.1329 for the polarity tasks (positive and negative opinionated rankings respectively). Performance degradation is intuitively expected because any variable which is additional to relevance, for example opinion, deteriorates system performance.
There is no way to separate and evaluate the effective-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

ness of the opinion detection component, or to determine whether and to which extent the relevance and opinion detection components are influenced by each other. It seems evident that an evaluation methodology or at least some benchmarks are needed to assess how much effective the opinion component is. At the moment, the only way to assess MAPO|R after opinionated re-ranking is to compare the increment of MAPO|R with respect to the relevance baseline, that is to assume the relevance baseline as a random ranking of opinionated documents about a given topic. Continuing with the same example, the MAPO|R of the relevance baseline is 0.3822, so that the actual MAPO|R increment is 9.6% after opinion re-ranking. Changing baselines and thus initial MAPR, one would have different increment rates even if the same polarity or opinion mining and re-ranking techniques were used. It is also a matter of fact that opinion MAPO|R seems to be highly dependent on the initial relevance MAPR of the first-pass retrieval [7, 4, 8]. To exemplify: how effective is the performance value of opinion MAPO|R 0.4189 when we start from an initial relevance MAPR of 0.4724? What would be the MAPO|R and P@10O|R by filtering documents as in a binary classification approach, and what would be the accuracy of such opinion classifier?
In conclusion, can absolute values of MAP be used to compare different tasks, such as topical opinion and the ad hoc retrieval, and to compare and assess the state of the art of different techniques on opinion finding? At this aim, we introduce a completely novel methodological framework to: - provide best achievable topical opinion MAPO|R, for a given relevance document ranking MAPR; - predict the performance of topical opinion retrieval given the performance of topic retrieval and opinion classification; - reciprocally, assess the opinion detection component accuracy from the overall topical opinion retrieval performance; - study the robustness of standard evaluation measures (MAP, P@10 etc,) for opinion retrieval; - study best re-ranking or filtering strategies on top of opinion classifiers independently from the adopted ad hoc relevance model.
This paper focuses on a few of these issues.
2. METHODOLOGY
In the first step all documents that are relevant to the test queries R = qQR(q) are pooled. From the subset O = qO(q)  R of opinionated and relevant documents one obtains the conditional probability of occurrence of opin-

777

0.55 0.5
0.45 0.4
0.35 0.3
0.25 0.2
0.15 0.1 50%
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45 50%

60% 60%

70% 70%

80% 80%

90%

100%

90%

100%

Figure 1: MAPO|R and P@10O|R by classifier accuracy. Opinionated document filtering with Monte Carlo sampling from the five TREC official baselines.

ions with respect to relevance, P (O|R). Assuming that each document has an unknown topic as a hidden variable, O is a sample of opinionated documents of the whole collection, and the prior for opinionated but not relevant documents, i.e. P (O|R), is provided by P (O|R) (i.e. it is de facto postulated the independence between relevance and opinion content). The second step consists in constructing a Monte Carlo sampling of opinionated documents from test data, and in obtaining thus opinion-only classifiers with different accuracy k. The MAPO|R is averaged on rankings built with all classifiers with the same accuracy k. The relevance rankings are modified according to:
· filtering, that is not opinionated documents are removed from the relevance baseline;
· re-ranking, that is opinionated documents receive a "reward" in their relevance ranking.
Topical opinion retrieval by Monte Carlo sampling is easily conducted with the filtering approach. Re-ranking approach requires the combination of two different scores, related to content and to opinion (e.g. [2, 9]). Opinion scores can be easily obtained with a lexicon-based approach [5, 10, 6, 3, 1]. The official relevance baselines and the topical opinion scores are provided by the blog TREC, while the opinion scores are not available, so that we use the lexiconbased scores and the re-ranking methodology that can be found in [1]. This lexicon-based approach has a good performance and achieves the MAPO|R of 0.4006 with respect the same baseline (baseline4 of the blog TREC 2008). Monte Carlo sampling consists in assigning the lexicon-based score to opinionated documents and a null score to non opinionated ones, assuming an accuracy 0  k  1 of the classifier.
3. CONCLUSIONS
To improve M APO|R with respect to its baseline requires a high accuracy of the opinion-only classifiers (at least around 80% with the filtering approach and 70% with the re-ranking technique of [1] as shown in Figures 1 and 2). However, smaller accuracy even close to that of the relevance baseline improves P@10 more easily than for MAP. The best value of M APO|R with the filtering approach achieves an empirical value around the M APR of the relevance baseline. There is almost a linear correlation between the M APO|R and the ac-

0.55 0.5
0.45 0.4
0.35 0.3
0.25 0.2
0.15 0.1 50%
0.85 0.8
0.75 0.7
0.65 0.6
0.55 0.5
0.45 50%

60% 60%

70% 70%

80% 80%

90%

100%

90%

100%

Figure 2: MAPO|R and P@10O|R by classifier accuracy. Relevant document re-ranking of the five TREC official baselines with lexicon-based scores and Monte Carlo sampling.

curacy k of the opinion-only classifier, both in filtering and re-ranking approaches. Finally, interpolating values of Figures 1 and 2 one can show that the best run of TREC 2008 based on the re-ranking approach must have an opinion-only classifier accuracy greater than or equal to 78%, while using the lexicon-based classifier of [1] an accuracy of 74%.
Within this evaluation framework we can now compare performances of different opinion mining techniques, investigate how relevance and opinion are influenced by each other, and assess effectiveness of re-ranking strategies.
4. REFERENCES
[1] G. Amati, E. Ambrosi, M. Bianchi, C. Gaibisso, and G. Gambosi. Automatic construction of an opinion-term vocabulary for ad hoc retrieval. In Proc. 30th ECIR, vol. 4956, LNCS, pages 89­100, 2008.
[2] K. Eguchi and V. Lavrenko. Sentiment retrieval using generative models. In Proc. ACL-EMNLP Conf., pp. 345­354, 2006.
[3] X. Huang and W. B. Croft. A unified relevance model for opinion retrieval. In Proc. 18th ACM-CIKM, pp. 947­956, 2009.
[4] C. Macdonald, I. Ounis, and I. Soboroff. Overview of the TREC-2007 Blog Track. In Proc. 16th TREC, 2007.
[5] G. Mishne. Multiple ranking strategies for opinion retrieval in blogs. In Proc. 15th TREC, 2006.
[6] S. H. Nam, S. H. Na, Y. Lee, and J. H. Lee. Diffpost: Filtering non-relevant content based on content difference between two consecutive blog posts. In Proc. 31st BCS-ECIR, vol. 5478, LNCS, pp. 791­795, 2009.
[7] I. Ounis, M. de Rijke, C. Macdonald, G. A. Mishne, and I. Soboroff. Overview of the TREC-2006 Blog Track. In Proc. 15th TREC, 2006.
[8] I. Ounis, C. Macdonald, and I. Soboroff. Overview of the TREC-2008 Blog Track. In Proc. 17th TREC, 2008.
[9] B. Pang and L. Lee. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1­2):1­135, 2008.
[10] J. Skomorowski and O. Vechtomova. Proc. 29st BCS-ECIR, vol. 4425, LNCS, pp. 405­417, 2007.
[11] I. Soboroff and D. Harman. Overview of the TREC-2003 Novelty Track. In Proc. 12th TREC, pp. 38­53, 2003.

778

Improving Sentence Retrieval with an Importance Prior

Leif Azzopardi
Department of Computing Science University of Glasgow, United Kingdom
leif@dcs.gla.ac.uk

Ronald T. Fernández, David E. Losada
Dept. of Electronics and Computer Science University of Santiago de Compostela, Spain
{ ronald.teijeira, david.losada } @usc.es

ABSTRACT
The retrieval of sentences is a core task within Information Retrieval. In this poster we employ a Language Model that incorporates a prior which encodes the importance of sentences within the retrieval model. Then, in a set of comprehensive experiments using the TREC Novelty Tracks, we show that including this prior substantially improves retrieval effectiveness, and significantly outperforms the current state of the art in sentence retrieval.
Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval
General Terms: Experimentation, Performance
Keywords: Sentence Retrieval, Language Models
1. INTRODUCTION
Sentence retrieval (SR) is a challenging problem area that has received a significant amount of attention recently [1, 4, 5, 7]. The main SR task consists of finding relevant sentences from a document base given a query. This task is very useful in a wide range of Information Retrieval (IR) applications, such as summarization, question answering, and opinion mining. However, the task has usually been approached by taking a document retrieval model and adapting it for SR. In fact, the model that is the state of the art in SR is known as term frequency-inverse sentence frequency (TF.ISF), which is analogous to the traditional TF.IDF method used in document retrieval [1, 4]. While, numerous attempts to develop more sophisticated models that employ techniques such as Natural Language Processing and Clustering have been proposed [2, 3, 8], they have failed to significantly and consistently outperform the TF.ISF method. Consequently, little progress has been made in terms of improving sentence retrieval effectiveness.
In this poster we posit that a relevant sentence needs to be indicative of the query, but also representative and important within the context of the document; i.e. we assume that key statements within a document are more likely to be relevant, if they are on topic. With this aim, we adopt the Language Modeling framework and include a sentence based prior to encode the importance of a sentence in a document within the model. In a set of experiments performed over several TREC test collections, we compare the proposed models against existing SR models and show that using an importance prior within a LM framework delivers retrieval performance that significantly outperforms the current state of the art.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. SENTENCE RETRIEVAL MODEL
The SR task consists of estimating the relevance of each sentence s in a document d in a given document set D, and supply the user with a ranked list of sentences which satisfy his/her need (expressed as a user query q). Using a language modeling framework to address this problem has been previously performed by Murdock [5] and Losada and Fernández [4]. The standard Language Modeling approach to SR estimates the probability of a query given a sentence language model (for specific details see [4, 5]). However, an unexplored extension is the inclusion of a sentence prior encoding the importance of the sentence within the context of the document.
To include a prior of importance of a sentence in a document, here we explicitly include the document in the sentence model and treat SR as a problem of estimating the probability of the query and the document given the sentence i.e. p(q, d|s). This probability tells us how likely the sentence is to produce both the query and the document, i.e. is it relevant to the query and central to the document? Using Bayes' Theorem, we can re-write it to become:

p(q, d|s) p(q|s, d)p(d|s)

(1)

where p(d|s) is the probability of the document given the sentence and p(q|s, d) is the probability of the query given the sentence and
document:

p(q|s, d) = a p(t|s) + b p(t|d) + gp(t) c(t,q)

(2)

tq

where a + b + g = 1. In [5] Eq. 2 is used (we shall refer to this as 3MM), while in [4] either Jelinek-Mercer (JM) or Dirichlet (DIR) smoothing is employed by setting the parameters appropriately1. These three models provide the standard sentence language modeling baselines. For the proposed extension shown in Eq. 1 we need to estimate p(d|s) which can be regarded as the importance of a sentence in a document2. To facilitate the estimation, Bayes Theorem can be employed, and then the components can be expressed as language models, so that:

p(d|s)

=

p(s|d ) p(d ) p(s)

p(s|d) p(s)

=

ts p(t|d)c(t,s) ts p(t)c(t,s)

(3)

where p(s|d) is the probability of a sentence given a document, the p(s) the probability of a sentence, p(d) is the prior probability of a document, p(t|d) is the probability of generating t from the maximum likelihood estimator of the document, c(t, ·) is the
number of times the term appears in the sentence/document/query.
Here, we assume that there is no a priori preference towards any

1For JM, b = 0. For DIR, b = 0, g = m/(c(d) + m) and a = 1 - g, where c(d) is the
number of terms in the document. 2In the standard models p(d|s) is assumed to be constant and is thus ignored.

779

of the documents, and treat p(d) as a constant. The p(s|d) represents how likely the sentence is to be generated from the document, whereas p(s) represents how likely the sentence is to be generated randomly. The ratio between the two expresses the importance of the sentence. Observe that p(d|s) will give preference to those sentences that are central to the document's topics (i.e. high p(s|d)) but also rare within the collection (i.e. low p(s)). It should also be noted that this prior will implicitly tend to favor longer sentences because p(t|d) is greater than p(t)3. With the importance prior, in our experiments we shall refer to the extended Language Models as 3MM.IP, JM.IP, and DIR.IP.
3. EMPIRICAL STUDY AND RESULTS
In this paper, we adopt the same definition of the sentence retrieval problem as proposed in the TREC Novelty Tracks. Although these tracks are mostly focused on researching redundancy filtering, they also involve a SR task that enables research into how to retrieve sentences that are relevant to a given query. The SR problem is framed as follows: given a textual query that represents an information need, a ranked set of documents is supplied and the systems have to process this ranking to extract the sentences that are estimated as relevant to the information need.
Data: Along with this definition we used all three TREC Novelty Track collections 2002, 2003 and 20044. Each collection was indexed using the Lemur toolkit5, where standard stop words were removed but stemming was not applied. The corresponding set of topics for each collection was used, where short queries were constructed taking the title field of the TREC Topic6. The TREC 2002 collection was used to train and estimate the parameters of each model used, while the TREC 2003 and 2004 collections were used to test the sentence retrieval models.
Models: In this work, we used a number of baseline models: (i) the current state of the art, TF.ISF [1], (ii) BM25 [6], which closely matches the performance of TF.ISF but is parameterized [4], and (iii) the standard sentence language models, JM and DIR, as well as 3MM [4, 5]. These are compared against the extended sentence language models, JM.IP, DIR.IP and 3MM.IP.
Measures: For all of our experiments, we report the performance of each method using Mean Average Precision (MAP) and R-Prec. To compare the differences in performance between the different methods, statistical significance tests were applied using the t-test with a 95% confidence level. Here, we show the statistical comparisons between each model and TF.ISF and DIR (see Table 1).
Results: Table 1 shows the performance obtained for each of the different models tested. Firstly, we note that the standard sentence language models do not outperform the state of the art TF.ISF or BM25. And in fact, TF.ISF and BM25 are significantly better than DIR. However, when the prior on sentence importance is incorporated within the language modeling framework, we note that these models all significantly outperform both TF.ISF and DIR, with improvements of up to 20% in some cases. The model that performed the best overall was DIR.IP which resulted in gains of 5-8% over TF.ISF. This is a substantive gain making these extended models an attractive and stronger baseline.
3So in the product in Eq. 3 the ratio for each term in the sentence is greater than one, and the more terms the greater the influence. 4See http://trec.nist.gov for track descriptions and reports. 5 http://www.lemurproject.org 6It should be noted that most teams participating in the TREC novelty tracks used the whole topic, so our results are not directly comparable to the official TREC results, but instead are based on a more realistic scenario.

Model TF.ISF BM25 JM 3MM DIR JM.IP 3MM.IP DIR.IP

TREC 2003

MAP

R-Prec

0.3851 0.4581

0.3852 0.4580

0.3474 0.4406

0.3513 0.4419

0.3638 0.4457

0.4137 0.4800

0.4104 0.4802

0.4144 0.4802

TREC 2004

MAP

R-Prec

0.2358 0.3298

0.2368 0.3300

0.2131 0.3010

0.2195 0.3060

0.2240 0.3146

0.2548 0.3520

0.2527 0.3504

0.2549 0.3522

Table 1: The Mean Average Precision (MAP) and R-Precision (R-Prec) for each model on TREC 2003 and 2004.  and  de-
note that the model is significantly better than TF.ISF and DIR, respectively, (p < 0.05). Parameters estimated on TREC 2002.

4. DISCUSSION AND FUTURE WORK
In this poster, we proposed and empirically evaluated an extension of the LM framework for SR to include sentence importance through a prior. It was found that by including the importance prior substantial improvements were obtained for all the different Language Models which resulted in significantly better performance. However, as the importance prior implicitly tends to favor longer sentences, it may be the case that the improvements witnessed are due to better length normalization (if longer sentences are more likely to be relevant). This work also suggests that the naive application of document retrieval models to other task may lead to nonoptimal performance. This will be the focus of future investigation along with examining how the vector space and other probabilistic models can be extended to also incorporate sentence importance and potentially better length normalization.
Acknowledgments: This work was partially supported by FEDER and Xunta de Galicia under projects 07SIN005206PR and 2008/068.
5. REFERENCES
[1] J. Allan, C. Wade, and A. Bolivar. Retrieval and novelty detection at the sentence level. In Proceedings of the 26th ACM SIGIR , pages 314­321, Toronto, Canada, 2003.
[2] S. Kallurkar, Y. Shi, R. S. Cost, C. K. Nicholas, A. Java, C. James, S. Rajavaram, V. Shanbhag, S. Bhatkar, and D. Ogle. UMBC at TREC 12. In Proceedings of the 12th TREC 2003, pages 699­706, 2003.
[3] X. Li and W. B. Croft. Novelty detection based on sentence level patterns. In Proceedings of the 14th CIKM 2005, pages 744­751, Bremen, Germany, 2005.
[4] D. E. Losada and R. T. Fernández. Highly frequent terms and sentence retrieval. In Proceedings of the 14th SPIRE 2007, pages 217­228, Chile, 2007.
[5] V. G. Murdock. Aspects of sentence retrieval. PhD thesis, University of Massachusetts Amherst, September 2006.
[6] S. E. Robertson, S. Walker, and M. Beaulieu. Okapi at TREC-7: automatic ad hoc, filtering, VCL and interactive track. In Proceedings of the 7th TREC , pages 253­264, Gaithersburg, USA, 1999.
[7] R. W. White, J. M. Jose, and I. Ruthven. Using top-ranking sentences to facilitate effective information access. American Society for Information Science and Technology, 56(10):1113­1125, 2005.
[8] M. Zhang, C. Lin, Y. Liu, L. Zhao, and S. Ma. THUIR at TREC 2003: Novelty, robust and web. In Proceedings of the 12th TREC, pages 556­567, Gaithersburg, USA, 2003.

780

Hierarchical Pitman-Yor Language Model for Information Retrieval

Saeedeh Momtazi, Dietrich Klakow
Spoken Language Systems Saarland University, Saarbrücken, Germany
{saeedeh.momtazi,dietrich.klakow}@lsv.uni-saarland.de

ABSTRACT
In this paper, we propose a new application of Bayesian language model based on Pitman-Yor process for information retrieval. This model is a generalization of the Dirichlet distribution. The Pitman-Yor process creates a power-law distribution which is one of the statistical properties of word frequency in natural language. Our experiments on Robust04 indicate that this model improves the document retrieval performance compared to the commonly used Dirichlet prior and absolute discounting smoothing techniques.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]:Information Search and Retrieval
General Terms: Theory, Algorithm, Experimentation
Keywords: information retrieval, language modeling, PitmanYor process, smoothing methods
1. INTRODUCTION
Statistical language modeling has successfully been used in speech recognition and many natural language processing tasks. Language models for information retrieval have been the topics of intense research interest in recent years. The efficiency of this approach, its simplicity, the state-ofthe-art performance it provides, and straightforward probabilistic interpretation are the most important factors which contribute to its popularity [3].
Smoothing plays an essential role when estimating a language model for retrieving relevant documents. A large number of smoothing methods have been proposed for language modeling; among them, three different techniques-- namely Jelinek-Mercer, Bayesian smoothing with Dirichlet priors, and absolute discounting--have shown significant improvements in information retrieval performance [6].
A hierarchical Bayesian language model based on PitmanYor processes has been recently proposed by Teh [5]. This model which is a nonparametric generalization of the Dirichlet distribution [5] has been shown to produce results superior to the state-of-the-art smoothing methods. Hierarchical Pitman-Yor language model has also been applied in speech recognition task and improved the system performance significantly [1]. However, to the best knowledge of the authors this method has not been used for language model-based information retrieval.
In this work, we propose using the hierarchical Pitman-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Yor language model for the document retrieval task, and compare this approach with the state-of-the-art smoothing methods widely studied for language model-based information retrieval.

2. METHOD
In language model-based document retrieval, P (Q|d) is estimated by the probability of generating each query term:

P (Q|d) =

P (qi|d)

(1)

i=1...M

where M is the number of terms in the query, qi denotes the ith term of query Q = {q1, q2, ..., qM }, and d is the document model. Therefore, the goal is to estimate P (w|d) which can
be simply calculated by the maximum likelihood estimation:

Pml(w|d) =

c(w, d) w c(w, d)

(2)

However, having the problem of unseen words, we need to use a smoothing technique to give a non-zero probability to the unseen words. We hypothesized that Bayesian smoothing based on Pitman-Yor process can be used as a new approach to solve the zero probability problem in document retrieval.
Pitman-Yor process is a nonparametric Bayesian model which recursively placed as prior for predicting probabilities in language model. Considering P (w|d) as the probability of word w given the observation of document d to be estimated, the Pitman-Yor process can be defined as:

P (w|d)  P Y (, , PBG(w))

(3)

where  is a discount parameter,  is a strength parame-

ter, and PBG is the prior/background probability of word w

before observing any document.

The procedure of drawing word probabilities from the

Pitman-Yor process can be described using the "Chines restau-

rant" analogy. Imagine a Chinese restaurant with an infinite

number of tables, each with an infinite number of seats. Cus-

tomers, which correspond to word tokens, enter the restau-

rant and seat themselves at a table. Each customer can sit at

an

occupied

table

k

with

probability

ck -   + c.

where

ck

is

the

number of customers already sitting there and c. = k ck;

the customer can also sit at a new unoccupied table with

probability

 + t.  + c.

where

t.

is

the

current

number

of

occu-

pied tables. It is necessary to mention that all customers

that correspond to the same word type w can sit at different

793

tables, in which tw denotes the number of tables occupied by customers w.
One of the advantages of Pitman-Yor process is improving the Dirichlet prior by using a discounting parameter  (0 <  < 1) deriving from absolute discounting method. Another key advantage of Pitman-Yor process is generating a power-law distribution in the language model, which is one of the statistical properties of word frequencies in natural language. This property, which is based on the scenario of rich-get-richer, implies that in the statistical property of word counts, words with low frequency have a high probability and words with high frequency occur with low probability. Benefiting from this idea in the document smoothing can help us to have different discounting value for each word based on the frequency of that word in the document.
Given the seating arrangement of customers as described above, the estimated probability of word w having the observation of document d is given by:

P (w|d)

=

c(w, d) - tw + ( + t.)PBG(w) w c(w, d) + 

(4)

If we set the discounting parameter  = 0, then the model reduces to the Dirichlet process. If we set the strength parameter  = 0 and limit tw = 1, then the model reverts to the absolute discounting method.
Although this formula is based on unigram model, the hierarchical behavior of the Pitman-Yor process allows us to use this model for higher level n-grams as well.
The most important and computationally expensive part of the above formula is calculating tw for each word which should have a relation to the word count c(w, d). Towards this end, we use the power-law discounting model proposed by Huang and Renals [2]:

tw = 0 tw = f (c(w, d)) = c(w, d)

if c(w, d) = 0 if c(w, d) > 0

(5)

They showed that the above formula is a near optimum estimate for tw, which can be obtained without a computationally expensive training procedure.

3. EXPERIMENTAL RESULTS
To evaluate our methods, we used TREC ad hoc testing collections from disk 4 and 5 minus CR which includes Financial Times (1991-1994) and Federal Register (1994) from disk 4 and Foreign Broadcast Information Service (1996) and Los Angeles Times (1989-1990) from disk 5. The total number of documents are 528,155.
We used Robust04 topics for our experiment such that topics 301-450 have been used as development set and topics 601-700 for test set. For each of the topics, the set of top 1000 documents retrieved by Indri [4] was selected and then the documents are ranked with LSVLM, the language modeling toolkit developed by our chair, in the second step.
Table 1 shows the results of our experiments in which Mean Average Precision (MAP) and Precision at 10 (P@10) serve as the primary metrics, and results are marked as significant* (p < 0.05), highly significant** (p < 0.01), or neither according to 2-tailed paired t-test. This table presents our main results evaluating the accuracy of Bayesian smoothing with Dirichlet prior, absolute discounting and our proposed Bayesian smoothing based on Pitman-Yor process.
As shown by the tabulated results, the Pitman-Yor language model significantly outperforms both Dirichlet prior

Table 1: Retrieval results with different smoothings.

Significant differences with absolute discounting and

Dirichlet prior are marked by a and d respectively.

Model

MAP P@10

Absolute Discounting Dirichlet Prior
Pitman-Yor Process Pitman-Yor Process ( = 0)

0.3138 0.3147 0.3271da 0.3222a

0.4484 0.4518 0.4657a 0.4566

and absolute discounting. As mentioned, the major features of the Pitman-Yor process are generalizing Dirichlet prior and generating power-law distribution by having different discounting parameters for each word based on its frequency. We believe that the power-law distribution is the main contribution of the Pitman-Yor language model which causes such an improvement in retrieval performance. We also applied Pitman-Yor language model while setting  = 0; i.e. the model became more similar to absolute discounting, but it still creates power-law distribution by benefiting from tw parameter. The results are presented in the last raw of the table. From the results we can see that although setting  = 0 decreases the performance, the reduction is not significant; and the simplified version of Pitman-Yor smoothing which only has one parameter still beat the other smoothing methods.
4. CONCLUDING REMARKS
We proposed a new smoothing method for language modelbased document retrieval, named Bayesian smoothing based on Pitman-Yor process, and verified that this language model provides better performance than other state-of-the-art smoothing techniques. The key advantage of Pitman-Yor language model is generating a power-law word distribution, which is the primary reason for its superior performance.
Acknowledgments
Saeedeh Momtazi is funded by the German research foundation DFG through the International Research Training Group (IRTG 715).

5. REFERENCES
[1] S. Huang and S. Renals. Hierarchical Pitman-Yor language models for ASR in meetings. In Proceedings of IEEE ASRU International Conference, pages 124­129, 2007.
[2] S. Huang and S. Renals. Power law discounting for n-gram language models. In Proceedings of IEEE ICASSP International Conference, 2010.
[3] J. Ponte and W. Croft. A language modeling approach to information retrieval. In Proceedings of ACM SIGIR International Conference, pages 275­281, 1998.
[4] T. Strohman, D. Metzler, H. Turtle, and W. Croft. Indri: A language model-based search engine for complex queries. In Proceedings of International Conference on Intelligence Analysis, 2005.
[5] Y. Teh. A hierarchical Bayesian language model based on Pitman-Yor process. In Proceedings of ACL International Conference, 2006.
[6] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of ACM SIGIR International Conference, 2001.

794

Entity Summarization of News Articles

Gianluca Demartini
L3S Research Center Appelstrasse 9a
30167 Hannover, Germany
demartini@L3S.de

Malik Muhammad Saad Missen
IRIT Toulouse, France
missen@irit.fr

Roi Blanco, Hugo Zaragoza
Yahoo! Research Diagonal 177
08018 Barcelona, Spain
{roi,hugoz}@yahooinc.com

ABSTRACT
In this paper we study the problem of entity retrieval for news applications and the importance of the news trail history (i.e. past related articles) to determine the relevant entities in current articles. We construct a novel entitylabeled corpus with temporal information out of the TREC 2004 Novelty collection. We develop and evaluate several features, and show that an article's history can be exploited to improve its summarization.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Measurement, Experimentation
Keywords: Entity Summarization, Time-aware Search
1. INTRODUCTION
Entity retrieval is becoming a major area of interest in IR research and it is quickly being adopted in commercial applications. One of the promising areas applying entity retrieval models in the commercial world is news search. News retrieval has also been the focus of much attention in the IR research community, but to our knowledge there have been no entity ranking tasks defined for news.
Consider the following user scenario: a user types a query (or topic) into a news search engine and obtains a list of relevant results, ordered by time. Furthermore, the user subscribes to this query so in the future she will continue to receive the latest news on this query. We are interested in entity ranking tasks related to this user scenario. For instance, standard entity ranking could be used to show the most interesting entities for the query. In practice, the temporal dimension is not needed here. However, if the user is observing a current document, we may want to show the most relevant entities of the document for her query taking into account features extracted from previous documents. This prompts the Entity Summarization (ES) task definition: given a query, a relevant document and possibly a set of previous related documents (the history of the document),
Work performed while intern at Yahoo! Research. This work is partially supported by the EU Large Scale Integrated Project LivingKnowledge (contract no. 231126).
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

retrieve a set of entities that best summarize the document. This is a newly defined task that can be useful, for example, in vertical search for presenting the user more than just a ranked list of documents.
2. TIME-AWARE ENTITY SUMMARIZATION
More formally, we define a "news thread" relevant to a query as the list of relevant documents D = [d1 . . . dn] chronologically ordered. Then, given a document di we define its history as the list of relevant documents H = [d1 . . . di-1] chronologically ordered pre-dating the document di. Given an entity e, we note as de,1 the first document in which the entity occurred in the news thread. Note that such a document is not necessarily the first document in D as entities may appear only in subsequent documents. Moreover, we note as de,-1 as the last document in H which contains e.
For addressing this task, we propose features both from the local document as well as from H. The first feature we consider is the frequency of an entity e in a document d, noted F (e, d). In the following we will use this feature as our baseline. It is possible to consider if an entity appears as a subject of a sentence as this is generally the person or thing carrying out an action (after running a dependency parsing over the sentence collection). Hence, we define the Fsubj(e, d) as the number of times an entity e appears as subject of a sentence in the document d.
Additionally, we propose two position-based features that take into account where in document d an entity e appears. Let F irstSenLen(e, d) be the length of the first sentence where e appears in document d and F irstSenP os(e, d) be the position of the first sentence where e appears in d (e.g, the fourth sentence in the document).
We now introduce a number of features that take into consideration the document history H. Let F (e, H) be the frequency (i.e., the number of times it appears) of the entity e in the history H. Instead of counting each entity occurrence a simpler variation considers the number of documents in which the entity e has appeared so far. We thus define DF (e, H) as the document frequency of e in H.
Furthermore, it is possible to examine single documents from the past to extract more features; we then define F (e, de,-1) as the frequency of entity e in the previous document where the entity appeared and F (e, de,1) as the frequency of entity e in the first document where the entity appeared.
We can also compute CoOcc(e, H), the number of other entities with which the entity co-occurred in a sentence in the set of past documents H.

795

3. EXPERIMENTAL EVALUATION
We selected the 25 event topics of the latest TREC Novelty collection (2004) consisting of news articles. We annotated the documents associated with those topics using state of the art NLP tools1 in order to extract entities of type person, location, organization, and product based on WSJ annotations. The system detected 7481 entity occurrences in the collection: 26% persons, 10% locations, 57% organizations, and 7% products. Human judges assessed the relevance of the entities in each document with respect to the topic grading each entity on the 3-points scale: Relevant, Related, Not Relevant. An additional category was used, i.e., 'Not an entity', to mark entities which had been wrongly annotated by the NLP tool. A total of 21213 entitydocument-topic judgements were obtained in the collection2.
We compare the effectiveness of different features and some feature combinations using several performance metrics. We report values for Precision@3 (P@3), Precision@5 (P@5), and Mean Average Precision (MAP) considering Related entities as non-relevant and using tie-aware metrics [2].

Feature

P@3 P@5 MAP

All Ties

.34

.34

.42

Individual Features (Local and History)

F(e,d)

.65

.56

.60

FirstSenLen .37

.36

.45

FirstSenPos .31

.31

.43

Fsubj

.49

.44

.50

F (e, de,1)

.58

F (e, de,-1) .64

DF (e, H) .63

F (e, H)

.66

CoOcc(e, H) .62

.53
.56 .57 .59
.57

.56 .62 .65 .66 .65

Features combined with F(e,d)

F irstSenLen .65 F irstSenP os .67

.57 .58

.62 .62

Fsubj F (e, de,1) F (e, de,-1) F (e, H)

.65
.65 .68 .70

.56 .57 .60 .62

.61 .61 .65 .68

CoOcc(e, H) .68 .61 .67

DF (e, H)

.69 .61 .68

Table 1: Effectiveness of individual features and of features when combined with F (e, d). Bold values indicate the best performing runs. * (**) indicates statistical significance w.r.t. F(e,d) and () w.r.t. F(e,H) with paired t-test p<0.05(0.01).

Individual Features. The upper part of Table 1 shows effectiveness values obtained when ranking entities in a document according to individual features. For comparison, a feature that assigns the same value to each entity would obtain a MAP value of 0.42. The feature F (e, d) obtains the best MAP value (0.60) among features from the local article. In general, history features perform better than local features and the highest performance is obtained by ranking entities according to their frequency in the past documents. Interestingly, when identifying relevant entities for a docu-
1http://sourceforge.net/projects/supersensetag/ 2The evaluation collection we have created is available for download at: http://www.l3s.de/~demartini/deert/

ment, the frequency of the entity in the previous document in the story F (e, de,-1) is a better evidence than the frequency in the current document. This may be an indication of how people read news: some entities become relevant to readers after repeated occurrences. If an entity appears also in the previous documents it is more likely to be relevant.
Given these results we conclude that the evidence from the past is very important for ranking entities appearing in a document. We expect effectiveness of methods that exploit the past to improve as the size of H grows. That is, the more history is available the better we can rank entities for the current news. For |H|  20 the average effectiveness of F (e, H) grows together with |H| up to values of 0.7 MAP.
Combined Features. So far we have presented different features for ranking entities that appear in a document. Combining them in an appropriate manner yields a better ranking of entities; however, because the probability distribution of relevance given a feature is different among features we need a way for combining them. The following experiments rank entities in a document according to a score obtained after combining several features together. We consider linear combination of features (transformed with a function as explained in [1]).

Let the score for an entity e and a vector f of n features

be score(e, f ) =

n i=1

wig(fi,

i)

,

where

wi

is

the

weight

of each feature and g is a transformation function for the

feature fi using a given parameter i. In this paper we

employ a transformation function of the form: g(x, ) =

x x+

as suggested in [1], where x is the feature to transform

and  is a parameter. We also tried a linear transformation

but it did not perform as well (more complex non-linear

transformations could also be explored). In order to combine

features we then need to find a parameter i for the function

g and a weight wi for each feature fi. We tested two and

three features combinations, where the variables i, and the

combination weights wi have been tuned with 2-fold cross

validation of 25 topics training to optimize MAP. In order to

find the best values we used a optimization algorithm that

performs a greedy search over the parameter space [3].

Combining F (e, d) with another feature is able to outper-

form the baseline for some range of the weight w that can be

learned on a training set. The best effectiveness is obtained

when combining F (e, d) and F (e, H) obtaining an improve-

ment of 13% in terms of average precision. Other features,

when combined with the baseline, also obtain high improve-

ments performing as good as the combination with F (e, H)

(CoOcc(e, H) having 12% and DF (e, H) having 13% im-

provement in terms of MAP).

As future work, besides testing our features on different

time-aware document collections, we aim at adopting ma-

chine learning techniques to combine the proposed features.

4. REFERENCES
[1] N. Craswell, S. Robertson, H. Zaragoza, and M. Taylor. Relevance weighting for query independent evidence. In SIGIR '05, USA. ACM.
[2] F. McSherry and M. Najork. Computing information retrieval performance measures efficiently in the presence of tied scores. In ECIR, 2008.
[3] S. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval, (4), 2009.

796

Feature Subset Non-Negative Matrix Factorization and its Applications to Document Understanding

Dingding Wang
School of Computer Science Florida International University
Miami, FL 33199
dwang003@cs.fiu.edu

Chris Ding
CSE Department University of Texas at Arlington
Arlington, TX 76019
chqding@uta.edu

Tao Li
School of Computer Science Florida International University
Miami, FL 33199
taoli@cs.fiu.edu

ABSTRACT
In this paper, we propose feature subset non-negative matrix factorization (NMF), which is an unsupervised approach to simultaneously cluster data points and select important features. We apply our proposed approach to various document understanding tasks including document clustering, summarization, and visualization. Experimental results demonstrate the effectiveness of our approach for these tasks. Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval. General Terms: Algorithms, Experimentation. Keywords: Feature subset selection, NMF.
1. INTRODUCTION
Keyword (Feature) selection enhances and improves many IR tasks such as document categorization, automatic topic discovery, etc. Many supervised keyword selection techniques have been developed for selecting keywords for classification problems. In this paper, we propose an unsupervised approach that combines keyword selection and document clustering (topic discovery) together.
The proposed approach extends non-negative matrix factorization (NMF) by incorporating a weight matrix to indicate the importance of the keywords. This work considers both theoretically and empirically feature subset selection for NMF and draws the connection between unsupervised feature selection and data clustering.
The selected keywords are discriminant for different topics in a global perspective, unlike those obtained in co-clustering, which typically associate with one cluster strongly and are absent from other clusters. Also, the selected keywords are not linear combinations of words like those obtained in Latent Semantic Indexing (LSI): our selected words provide clear semantic meanings of the key features while LSI features combine different words together and are not easy to interpret. Experiments on various document understanding applications demonstrate the effectiveness of our proposed approach.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. FEATURE SUBSET NON-NEGATIVE MA-

TRIX FACTORIZATION (FS-NMF)
Let X = {x1, · · · , xn) contains n documents with m keywords (features). In general, NMF factorizes the input nonnegative data matrix X into two nonnegative matrices,

X  F GT ,

where G  Rn+×k is the cluster indicator matrix for clustering columns of X and F = (f1, · · · , fk)  R+m×k contains k cluster centroids. In this paper, we propose a new objective

to simultaneously factorize X and rank the features in X as

follows:



W

min
0,F 0,G0

||X

-

F GT

||2W

,

s.t.

Wj = 1

(1)

j

where W  Rm + ×m which is a diagonal matrix indicating the weights of the rows (keywords or features) in X, and  is a
parameter (set to 0.7 empirically).

2.1 Optimization
We will optimize the objective with respect to one variable while fixing the other variables. This procedure repeats until convergence.

2.1.1 Computation of W

Optimizing Eq.(1) with respect to W is equivalent to op-

timizing







J1 = Wibi - ( Wi - 1), bi = (X - F GT )2ij .

i

i

j

Now,

setting

J1 Wi

= bi - Wi-1

= 0,

we

obtain

the

fol-

lowing updating formula



1

Wi

=


i

1

bi-1




1
bi-1

(2)

2.1.2 Computation of G
Optimizing Eq.(1) with respect to G is equivalent to optimizing

J2 = T r(XT W T X - 2GF T W T X + F T W T F GT G).

Setting

J2 G

=

-2XT W F

+ 2GF T W T F

=

0,

we

obtain

the

following updating formula

Gik



Gik

(XT W F )ik (GF T W F )ik

(3)

The correctness and convergence of this updating rule can

be rigorously proved. Details are skipped here.

805

2.1.3 Computation of F
Optimizing Eq.(1) with respect to F is equivalent to optimizing
J3 = T r[W XXT - 2W XGF T + W F GT GF ].

Setting

J3 F

=

-2W XG + W F GT G + (GT GF T W )T

= 0,

we

obtain the following updating formula

Fik



Fik

(W XG)ik (W F GT G)ik

(4)

3. EXPERIMENTS

3.1 Document Clustering
First of all, we examine the clustering performance of FS-NMF using four text datasets as described in Table 1, and compare the results of FS-NMF with seven widely used document clustering methods: (1) K-means; (2) PCA-Km: PCA is firstly applied to reduce the data dimension followed by the K-means clustering; (3) LDA-Km [2]: an adaptive subspace clustering algorithm by integrating linear discriminant analysis (LDA) and K-means; (4)Euclidean coclustering (ECC) [1]; (5) minimum squared residueco-clustering (MSRC) [1]; (6) Non-negative matrix factorization (NMF) [5]; (7) Spectral Clustering with Normalized Cuts (Ncut) [9]. More description of the datasets can be found in [6]. The accuracy evaluation results are presented in Figure 2.

Datasets CSTR Log Reuters
WebACE

# Samples 475 1367 2900 2340

# Dimensions 1000 200 1000 1000

# Class 4 8 10 20

Table 1: Dataset descriptions.

K-means PCA-Km LDA-Km
ECC MSRC NMF Ncut FS-NMF

WebACE
0.4081 0.4432 0.4774 0.4081 0.4432 0.4774 0.4513 0.5577

Log
0.6979 0.6562 0.7198 0.7228 0.5655 0.7608 0.7574 0.7715

Reuters
0.4360 0.3925 0.5142 0.4968 0.4516 0.4047 0.4890 0.4697

CSTR
0.5210 0.5630 0.5630 0.5210 0.5630 0.5630 0.5435 0.6996

Table 2: Clustering accuracy on text datasets.
From the results, we clearly observe that FS-NMF outperforms other document clustering algorithms in most of the cases, and the effectiveness of FS-NMF for document clustering is demonstrated.
3.2 Document Summarization
In this set of experiments, we apply our FS-NMF algorithm on document summarization. Let X be the documentsentence matrix, which can be generated from the documentterm and sentence-term matrices, and now each feature (column) in X represents a sentence. Then the sentences can be ranked based on the weights in W . Top-ranked sentences are included into the final summary. We use the DUC benchmark dataset (DUC2004) for generic document summarization to compare our method with other state-ofart document summarization methods using ROUGE evaluation toolkit [7]. The results are demonstrated in Table 3.

Systems
DUC Best Random Centroid [8] LexPageRank [3] LSA [4] NMF [5] FS-NMF

R-1
0.382 0.318 0.367 0.378 0.341 0.367 0.388

R-2
0.092 0.063 0.073 0.085 0.065 0.072 0.101

R-L
0.386 0.345 0.361 0.375 0.349 0.367 0.381

R-W
0.133 0.117 0.124 0.131 0.120 0.129 0.139

R-SU
0.132 0.117 0.125 0.130 0.119 0.129 0.134

Table 3: Overall performance comparison on DUC2004 data using ROUGE evaluation methods.
From the results, we observe that the summary generated by FS-NMF outperforms those created by other methods, and the scores are even higher than the best results in DUC competition. The good results benefit from good sentence feature selection in FS-NMF.
3.3 Visualization
In this set of experiments, we calculate the pairwise document similarity using the top 20 word features selected by different methods, and Figure 1 demonstrates the document similarity matrix visually. Note that in the document dataset (CSTR dataset), we order the documents based on their class labels.

50

100

150

200

250

300

350

400

50

100

150

200

250

300

350

400

50

100

150

200

250

300

350

400

450

50

100 150 200 250 300 350 400 450

50

100

150

200

250

300

350

400

450

50

100 150 200 250 300 350 400 450

(a) FS-NMF

(b) NMF

(c) LSI

Figure 1: Visualization results on the CSTR Dataset. Note that CSTR has 4 clusters.

From Figure 1, we have the following observations. (1) Word features selected by FS-NMF can effectively reflet the document distribution. (2) NMF tends to select some irrelevant or redundant words thus Figure 1(b) shows no obvious patterns at all. (3) LSI can also find meaningful words, however, the first two clusters are not clearly discovered in Figure 1(c).
Acknowledgements: The work is partially supported by an FIU Dissertation Year Fellowship and NSF grants DMS-0915110 and DMS-0915228.

4. REFERENCES
[1] H. Cho, I. Dhillon, Y. Guan, and S. Sra. Minimum sum squared residue co-clustering of gene expression data. In Proceedings of SDM 2004.
[2] C. Ding and T. Li. Adaptive dimension reduction using discriminant analysis and k-means c lustering. In ICML, 2007.
[3] G. Erkan and D. Radev. Lexpagerank: Prestige in multi-document text summarization. In EMNLP, 2004.
[4] Y. Gong and X. Liu. Generic text summarization using relevance measure and latent semantic analysis. In SIGIR, 2001.
[5] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS, 2000.
[6] T. Li and C. Ding. The relationships among various nonnegative matrix factorization methods for clustering. In ICDM, 2006.
[7] C.-Y. Lin and E.Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In NLT-NAACL, 2003.
[8] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, pages 919­938, 2004.
[9] S. X. Yu and J. Shi. Multiclass spectral clustering. In ICCV, 2003.

806

Learning to Rank Query Reformulations

Van Dang, Michael Bendersky and W. Bruce Croft
Center for Intelligent Information Retrieval Department of Computer Science University of Massachusetts Amherst, MA 01003
{vdang, bemike, croft}@cs.umass.edu

ABSTRACT
Query reformulation techniques based on query logs have recently proven to be effective for web queries. However, when initial queries have reasonably good quality, these techniques are often not reliable enough to identify the helpful reformulations among the suggested queries. In this paper, we show that we can use as few as two features to rerank a list of reformulated queries, or expanded queries to be specific, generated by a log-based query reformulation technique. Our results across five TREC collections suggest that there are consistently more useful reformulations in the first positions in the new ranked list than there were initially, which leads to statistically significant improvements in retrieval effectiveness.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query Formulation
General Terms
Algorithms, Measurement, Performance, Experimentation.
Keywords
Query reformulation, query expansion, query log, query performance predictor, learning to rank.
1. INTRODUCTION
Query logs have become an important resource for many tasks including query reformulation [3, 6]. Most log-based reformulation techniques, however, are evaluated using nonstandard approaches and proprietary query logs, making it hard to compare one to another. A more recent study [2] compares different techniques using TREC collections and finds that when intial queries have relatively high quality, query expansion is much more reliable than substitution.
Although the log-based expansion technique [2] can generate some good reformulations for high-quality TREC queries, it also produces many bad reformulations and it does not generate a reliable ranking of the reformulations by quality.
In this paper, we show that we can effectively rerank the list of reformulated queries obtained with this log-based expansion approach. By using as few as two features, SCQ
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

(Similarity Collection Query) [8] and query clarity [1], we can substantially improve the ranking of reformulated queries in terms of the quality of the reformulations in the top two ranks (measured by NDCG@2 ), which then leads to significant improvements in retrieval effectiveness.

2. METHOD

2.1 Log-based Query Expansion
The log-based query expansion method [2] (referred to as LQE) is a slight modification of the query substitution method proposed by Wang and Zhai [6]. It first estimates a context distribution for terms occuring in a query log. It then constructs a translation model that can suggest similar words based on their distributional similarity. Given any query, the expansion model will try to expand it with candidates suggested by the translation model for each query term. The model decides whether to expand the query based on how similar the candidate is to the query term and how appropriate it is to the context of the query. For more details, see [2].

2.2 The Reranking Approach
Query quality predictors aim to predict a query's quality without explicit relevance judgements. Thus, given a ranked list of reformulated queries, it is intuitive to think about reorganizing this list based on the "quality" score given by some predictor.
We tried some of the top-performing predictors that Kumaran and Carvalho [4] used in a similar task and found that
[8] and clarity score [1] are the most effective for our problem. Therefore, we rerank the list of expanded queries by

( )= 1×

( )+ 2×

()

where 1 and 2 are weight of the two predictors.

Table 1: Statistics of queries used for reformulation

AP WSJ Robust-04 WT10G Gov-2

Title Q. 133 133

200

66

119

Desc. Q. 150 150

246

94

134

807

Table 3: Evaluation of retrieval effectiveness in terms of MAP.  and  indicate significant difference to the

original query and LQE's ranked list respectively. Best result in each column is marked in bold.

Title Query

Description Query

AP

WSJ

RBT-04 WT10G Gov-2

AP

WSJ RBT-04 WT10G Gov-2

Orig-Q 0.1694 0.2594

0.2247

0.1904 0.2829

0.1660 0.2358 0.2519 0.1770 0.2518

LQE

0.1741 0.2563

0.2297

0.1911 0.2559

0.1694 0.2391 0.2538

0.1775

0.2497

Rerank 0.1749 0.2663 0.2382 0.1962 0.2901 0.1820 0.2374 0.2584 0.1836 0.2579

Table 2: Our approach ("Rerank") consistently out-

performs LQE in NDCG@2. All differences are signif-

icant at < 0.05

Collection

Title Query

Desc. Query

LQE Rerank LQE Rerank

AP

0.2434 0.4805 0.2307 0.3728

WSJ

0.2318 0.5040 0.2250 0.3296

Robust-04 0.2905 0.5559 0.2138 0.3687

WT10G 0.2673 0.5499 0.1680 0.3847

Gov-2 0.1933 0.5830 0.2059 0.4093

3. EVALUATION
3.1 Experiment Settings
In this section, we evaluate the performance of our reranking technique. Evaluation is done on five TREC collections: AP, WSJ, Robust-04, WT10G and Gov-2, with both title and description queries. We use the language modeling framework and remove all stop words at indexing time. We adopt the parameter settings for LQE from the authors [2].
Due to the limited coverage of the available query log [5], we use only a subset of TREC queries where the LQE can generate at least one reformulation. Information about these subsets is given in Table 1.
On each collection, we first use LQE to generate a list of expanded queries ( = 30) for each original query. We append to this list the original query - in the case when all generated reformulations are bad, the reranking approach has a chance to choose not to reformulate. We then use our approach to rerank this list and compare its performance with that of the intial list as well as original query.
3.2 Training Data
We run LQE with the MSN log to obtain a list of reformulations for each original query. We use all these queries to do retrieval and record their MAP and use them to create our dataset. Training and testing are done using 5-fold cross validation on this dataset. 1 and 2 are learned using AdaRank [7] to maximize the average NDCG@2. The algorithm ends up choosing either ( 1 = 1, 2 = 0) or ( 1 = 0, 2 = 1) depending on the collection.
3.3 Reranking Effectiveness
We use NDCG@2 to measure the quality of the ranked list of reformulations given by our approach. Reformulations are graded on a scale from zero to four with respect to the improvement they provide over the original query. In particular, improvement larger than 0.03 corresponds to a 4, or ( > 0.03)  4. Similarly, (0.01 <  0.03)  3, (0 <  0.01)  2, ( = 0)  1 and ( < 0)  0.
Table 2 summarizes the result: the list of reformulations ranked by our approach has a much higher average NDCG@2

than the initial list. All improvements are statistically significant at < 0.05 using a two-tailed t-test.
3.4 Retrieval Effectiveness
We define the MAP of a ranked list of reformulations as the best MAP observed among its top queries. In this section, we compare the MAP obtained by (i) the original query, (ii) the list of reformulations generated by LQE, and (iii) the list reranked by our method.
As can be seen in Table 3, the best of the top two reformulated queries ranked by our approach is almost always significantly better than the original query. This is not the case in LQE. In many cases, our method also provides significant improvements over LQE. This result suggests that the reranking can push better reformulations to the first two positions in the ranked list.
4. CONCLUSIONS
In this paper, we have shown that by reranking the list of reformulations generated by the log-based query expansion technique [2] with only two features, we can push more good reformulations into the first two positions in the list. This is reflected in the huge gain of NDCG@2 and statistically significant improvement in retrieval effectiveness. In the future, we will investigate more features. We hope this will lead to greater improvement in NDCG@1, helping retrieval systems to reformulate queries implicitly without user involvement.
5. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #IIS0711348, and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
6. REFERENCES
[1] S. Cronen-Townsend, Y. Zhou, and W.B. Croft. Predicting Query Performance. In Proc. of SIGIR, pages 299-306, 2002.
[2] V. Dang and W.B. Croft. Query Reformulation Using Anchor Text. In Proc. of WSDM, pages 41-50, 2010.
[3] R. Jones, B. Rey and O. Madani. Generating Query Substitutions. In Proc. of WWW, pages 387-396, 2006.
[4] G. Kumaran and V.R. Carvalho. Reducing Long Queries Using Query Quality Predictors. In Proc. of SIGIR, pages 564-571, 2009.
[5] Proc. of the 2009 workshop on Web Search Click Data, Barcelona, Spain. ACM New York, NY, USA, 2009.
[6] X. Wang and C. Zhai. Mining Term Association Patterns from Search Logs for Effective Query Reformulation. In Proc. of CIKM, pages 479-488, 2008.
[7] J. Xu and H. Li. AdaRank: A Boosting Algorithm for Information Retrieval. In Proc. of SIGIR, pages 391-398, 2007.
[8] Y. Zhao, F. Scholer, and Y. Tsegay. Effective Pre-retrieval Query Performance Prediction Using Similarity and Variability Evidence. In Proc. of ECIR, pages 52-64, 2008.

808

Many are Better Than One: Improving Multi-Document Summarization via Weighted Consensus

Dingding Wang Tao Li
School of Computer Science Florida International University
Miami, FL 33199
{dwang003,taoli}@cs.fiu.edu

ABSTRACT
Given a collection of documents, various multi-document summarization methods have been proposed to generate a short summary. However, few studies have been reported on aggregating different summarization methods to possibly generate better summarization results. We propose a weighted consensus summarization method to combine the results from single summarization systems. Experimental results on DUC2004 data sets demonstrate the performance improvement by aggregating multiple summarization systems, and our proposed weighted consensus summarization method outperforms other combination methods.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval.
General Terms: Algorithms, Experimentation.
Keywords: Weighted consensus, summarization.
1. INTRODUCTION
Various multi-document summarization methods base on different strategies and usually produce diverse outputs. A natural question arises: can we perform ensemble or consensus summarization by combining different summarization methods to improve summarization performance? In general, the terms of "consensus methods" or "ensemble methods" are commonly reserved for the aggregation of a number of different (input) systems. Previous research has shown that ensemble methods, by combining multiple input systems, are a popular way to overcome instability and increase performance in many machine learning tasks, such as classification, clustering and ranking. The success of ensemble methods in other learning tasks provides the main motivation for applying ensemble methods in summarization. To the best of our knowledge, so far there are only limited attempts on using ensemble methods in multi-document summarization.
As a good ensemble requires the diversity of the individual members, here we study several widely used multi-document summarization systems based on a variety of strategies, and evaluate different baseline combination methods for obtaining a consensus summarizer to improve the summarization performance. Motivated from [5], we also propose a novel weighted consensus scheme to aggregate the results from
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

individual summarization methods, in which, the relative contribution of an individual summarizer to the consensus is determined by its agreement with other members of the summarization systems. Note that usually a high degree of agreements does not automatically imply the correctness since the systems could agree on a faulty answer. However, each of the summarization systems has shown its effectiveness individually, so the agreement measure can be used in the consensus summarization.

2. WEIGHTED CONSENSUS SUMMARIZATION (WCS)

2.1 Notations
Suppose there are K single summarization methods, each of which produces a ranking for the sentences containing in the document collection. Then we have K ranking lists {r1, r2, · · · , rK} and ri  RN , i = 1, · · · , K, where N is the total number of sentences in the documents. The task is to find a weighted consensus ranking of the sentences r with a set of weights {w1, w2, · · · , wK } assigning to each of the individual summarization methods.

2.2 Formulation
Our goal is to minimize the weighted distance between r and all the ri. Let w = [w1, w2, . . . , wK ]T  RK . The problem can be formulated as follows.

arg min
w
s.t.

K
(1 - ) wi r - ri 2 +  w 2
i=1
K
wi = 1; wi  0 i,
i=1

where 0    1 is the regularization parameter which spec-

ifies the tradeoff between the minimization of the weighted

distance and the smoothness enforced by w. In experiments,

 is set to 0.3 empirically. For simplicity, we use Euclidean

distance to measure the discordance of the consensus rank-

ing r and each of individual sentence rankings ri.

We

initialize

wi

=

1 K

,

and

this

optimization

problem

can

be solved by iterating the following two steps:

Step 1: Solve for r while fixing w. The optimal solution

is

the weighted average r = Step 2: Solve for w while

i wi fixing

ri. r

.

Let

d = [ r - r1 2, r - r2 2, · · · , r - rK 2]  RK .

809

Note that

K
(1 - ) wi r - ri 2 +  w 2 = (1 - )d w + w w

i=1

=

w-

-1 2 d

2-

( - 1)2 4

d2

For fixing r, the optimization problem becomes

arg min
w

w

-



-

1 d

2

s.t.

2

K
wi = 1;
i=1

wi  0,

i

This is a quadratic function optimization problem with

linear constraints with K variables. This is a problem of

just about tens of variables (i.e., weights for each input sum-

marization system) and thus can be computed quickly. It

can

also

be

solved

by

simply

projecting

vector

-1 2

d

onto

(K - 1)-simplex. With step 1 and 2, we iteratively update

w and r until convergence. Then we sort r in ascending

order to get the consensus ranking.

3. EXPERIMENTS
In the experiments, we use four typical multi-document summarization methods as individual summarizers and compare our WCS method with other eight aggregation methods. The four individual summarization methods are: (a) Centroid [7], (b) LexPageRank [1], (c) LSA [2], and (d) NMF [4]. And the baseline aggregation methods are: (1) average score (Ave Score), which normalizes and averages the raw scores from different summarization systems; (2) average rank (Ave Rank), which averages individual rankings; (3) median aggregation (Med Rank); (4) Round Robin (RR); (5) Borda Count (BC); (6) correlation-based weighting (CW), which weights individual systems by their average Kendall's Tau correlation between the ranking list they generated and all the other lists; (7) ULTRA [3], which aims to find a consensus ranking with the minimum average Spearman's distance [8] to all the individual ranking lists; (8) graph-based combination (Graph), the basic idea of which is similar to the work proposed in [9], however, we use cosine similarity so that we can compare this method with other combination methods fairly. We conduct experiments on DUC benchmark data for generic multi-document summarization and use ROUGE [6] toolkit (version 1.5.5) to measure the summarization performance.
3.1 Overall Summarization Performance
Table 1 show Rouge-1, Rouge-2, and Rouge-SU scores of different individual and combination methods using DUC2004 data sets (intuitively, the higher the scores, the better the performance). From the results, we observe that (1) Most of the combination summarization systems outperform all the individual systems except the round robin combination. The results demonstrate that in general consensus methods can improve the summarization performance. (2) Weighted combinations (e.g., CW, ULTRA, and WCS) outperform average combination methods which treat each individual system equally. (3) Our WCS method outperforms other weighted combination methods because WCS optimizes the weighted distance between the consensus sentence ranking to individual rankings and updates the weights and consensus ranking iteratively, which is closer to the nature of consensus summarization than other approximation based weighted methods.

Systems
DUCBest Centroid LexPageRank
LSA NMF Ave Score Ave Rank Med Rank RR BC CW ULTRA Graph WCS

R-1
0.382 0.367 0.378 0.341 0.367 0.388 0.385 0.385 0.364 0.378 0.378 0.392 0.379 0.398

R-2
0.092 0.073 0.085 0.065 0.072 0.089 0.087 0.087 0.072 0.085 0.085 0.090 0.086 0.096

R-SU
0.132 0.125 0.130 0.119 0.129 0.132 0.131 0.131 0.126 0.129 0.131 0.133 0.132 0.135

Table 1: Overall performance comparison on DUC2004. Remark: DUCBest shows the best results from DUC 2004 competition.

3.2 Diversity of Individual Summarizers
In this set of experiments, we further examine if the four individual summarization methods are complementary to each other. We use our WCS method to aggregate any three of the four summarization methods and compare the results with the aggregation utilizing all the four methods. Table 2 shows the comparison results. From the results, we observe that adding any of the four individual methods improves the summarization performance. This is because these individual summarization methods are diverse and their performance is data dependant.

Systems
Centroid+LexPageRank+LSA Centroid+LexPageRank+NMF
Centroid+LSA+NMF LexPageRank+LSA+NMF
All

R-1
0.383 0.385 0.376 0.382 0.398

R-2
0.088 0.090 0.082 0.087 0.096

R-SU
0.132 0.133 0.131 0.132 0.135

Table 2: WCS results on DUC2004.

Acknowledgements: The work is partially supported by an FIU Dissertation Year Fellowship and NSF grants IIS0546280 and DMS-0915110.

4. REFERENCES
[1] G. Erkan and D. Radev. Lexpagerank: Prestige in multi-document text summarization. In EMNLP, 2004.
[2] Y. Gong and X. Liu. Generic text summarization using relevance measure and latent semantic analysis. In SIGIR, 2001.
[3] A. Klementiev, D. Roth, and K. Small. An unsupervised learning algorithm for rank aggregation. In ECML, 2007.
[4] D. Lee and H. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, pages 788­791, 1999.
[5] T. Li and C. Ding. Weighted consensus clustering. SIAM Data Mining, 2008.
[6] C.-Y. Lin and E.Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In NLT-NAACL, 2003.
[7] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, pages 919­938, 2004.
[8] C. Spearman. The proof and measurement of association between two things. Amer. J. Psychol., 1904.
[9] V. Thapar, A. A. Mohamed, and S. Rajasekaran. A consensus text summarizer based on meta-search algorithms. In Proceedings of 2006 IEEE International Symposium on Signal Processing and Information Technology, 2006.

810

Investigating the Suboptimality and Instability of Pseudo-Relevance Feedback

Raghavendra Udupa
Microsoft Research India, Bangalore 560080, India
raghavu@microsoft.com

Abhijit Bhole
Microsoft Research India, Bangalore 560080, India
v-abhibh@microsoft.com

ABSTRACT
Although Pseudo-Relevance Feedback (PRF) techniques improve average retrieval performance at the price of high variance, not much is known about their optimality1 and the reasons for their instability. In this work, we study more than 800 topics from several test collections including the TREC Robust Track and show that PRF techniques are highly suboptimal, i.e. they do not make the fullest utilization of pseudo-relevant documents and under-perform. A careful selection of expansion terms from the pseudo-relevant document with the help of an oracle can actually improve retrieval performance dramatically (by > 60%). Further, we show that instability in PRF techniques is mainly due to wrong selection of expansion terms from the pseudo-relevant documents. Our findings emphasize the need to revisit the problem of term selection to make a break through in PRF.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation
1. INTRODUCTION
Pseudo-relevance feedback (PRF) uses terms from the top ranking documents of the initial unexpanded retrieval for expanding the query [5]. Although PRF improves average retrieval performance, improvements have been incremental despite years of research [1, 3]. It is not known whether PRF techniques are under-performing or have already given their best by making the fullest use of pseudo-relevant documents in Query Expansion. Further, there is no satisfactory explanation of instability of PRF techniques. It is commonly believed that PRF is unstable because the initial unexpanded retrieval brings many non-relevant documents in the top for some topics and therefore, query expansion produces topic drift. However, for a good number of queries, retrieval performance does not change significantly. It is not known why PRF techniques fail to make a difference to such queries.
It is important that the twin issues of optimality and instability be addressed to decide whether to continue investment on new research in PRF and to devise effective ways of combating instability. In this work, we take the first steps towards understanding the optimality and instability of PRF
1By optimality of PRF techniques, we mean their use of pseudo-relevant documents in such a way as to maximize retrieval performance.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

techniques by studying more than 800 topics from several test collections including the TREC Robust Track.
We develop DEX, an oracle method for extracting a set of expansion terms from the pseudo-relevant documents using discriminative learning. Being an oracle method, DEX can be viewed as a good approximation to the ideal PRF technique that we can hope to design. As state-of-the-art PRF techniques and DEX extract expansion terms from the same set of pseudo-relevant documents, the gap in their retrieval performance indicates the future potential for improvement in retrieval performance of PRF techniques.
2. THE DEX ORACLE
DEX is an oracle for extracting a set of useful expansion terms from the pseudo-relevant documents by using the knowledge of relevant documents. DEX first extracts a set of candidate expansion terms2 t1, ..., tN from the pseudorelevant documents and then partitions this set into a set of useful terms and a set of non-useful terms using statistical learning [6]. It treats relevant documents for the topic as +ve instances and top scoring non-relevant documents as ve instances3. It learns a linear discriminant function w to discriminate the +ve instances from the -ve instances. The linear discriminant function classifies a vector x as +ve if wT x > 0 and as -ve if wT x  0. Therefore, DEX treats terms ti : wi > 0 as useful terms and the rest as non-useful. Finally, DEX picks the largest weighted k > 0 terms for expansion.
3. EXPERIMENTAL STUDY
We employed a KL-divergence based retrieval system with two stage Dirichlet smoothing as our baseline [4]. We used model-based feedback technique (Mixture Model) as a representative PRF technique [3]. For expanded retrieval, we interpolated the feedback model with the original query model with  set to 0.5. For estimating the feedback model, we used the top 10 documents fetched by the initial retrieval. Topics as well as documents were stemmed using the well known Porter stemmer and stop-words were removed. We compared model-based feedback with DEX-based PRF. We used the DEX algorithm (Section 2) to extract k = 5 expansion terms from the top 10 documents of the unexpanded retrieval. We formed a feedback model from the expansion
2Candidate expansion terms are those terms from the pseudo-relevant documents whose idf > ln10 and collection frequency  5 [2]. 3Each labeled instance D has an associated feature vector x whose dimensions i = 1, ..., N correspond to the candidate expansion terms t1, ..., tN respectively.

813

Table 1: Comparitive Retrieval Performance

Collection
CLEF 00-02 CLEF 03,05,06
AP WSJ SJM Robust03 Robust04

LM MAP P@5 0.43 0.49 0.38 0.42 0.28 0.47 0.27 0.48 0.21 0.34 0.11 0.32 0.25 0.49

MF MAP P@5 0.44 0.50 0.41 0.43 0.33 0.50 0.30 0.52 0.24 0.36 0.13 0.34 0.28 0.49

DEX MAP P@5 0.74* 0.74 0.66* 0.72 0.48* 0.73 0.43* 0.72 0.43* 0.63 0.29* 0.64 0.40* 0.71

terms by assigning equal probability mass to the DEX terms. As with model-based feedback, we interpolated our feedback model with the query model with  set to 0.5. We call unexpanded retrieval, model-based feedback, and DEX-based PRF as LM, MF and DEX respectively.
3.1 Test Collections
We used the CLEF (LATimes 94, Glasgow Herald 95) and TREC (Associated Press 88-89, Wall Street Journal, San Jose Mercury, Disks 4&5 minus the Congressional Record) document collections in our experiments. We studied retrieval performance on the following sets of topics: CLEF Topics 1 - 140 (CLEF 2000-2002), Topics 141-200 (CLEF 2003), Topics 251-350 (CLEF 2005-2006), TREC Topics 51 - 200 (TREC Adhoc Tasks 1, 2, 3), Assorted Topics (TREC Robust 2003, Hard), Topics 301-450, 601-700 (TREC Robust 2004). There were totally 821 unique topics. Some topics were used for retrieval on multiple document collections.
3.2 Retrieval Performance
We used MAP and P@5 as the average performance measures to compare the three retrieval models. We say that a topic is hurt by expanded retrieval if the average precision decreases by 0.01 or more relative to the unexpanded retrieval. Similarly, we say that a topic is improved (benefitted) by a retrieval model if the average precision increases by 0.01 or more. We compare the performance of LM, MF, and DEX on all topics in Table 1. We see that MF fares better than LM overall but the improvement in retrieval performance is modest. In contrast, DEX gives dramatic improvement in retrieval performance relative to both LM and MF on all test sets despite using the same set of documents for estimating the feedback model. DEX improves MAP by > 60% over LM and by > 42% over MF in general and on Robust03, DEX improves MAP by 123% over MF. Not only the MAP has improved dramatically P@5 has also improved. The huge gap between the average retrieval performance of MF and DEX highlights a very important fact: PRF techniques are highly suboptimal. Their average retrieval performance is much lower than what can be potentially achieved using the same set of feedback documents.
Table 2 shows the percentage of topics which benefitted from MF and DEX, topics which got hurt by MF and DEX and topics which remained indifferent to MF and DEX. We observe that nearly 25% of topics in all collections are indifferent to MF whereas a smaller percentage of topics are hurt by MF. DEX reduces the percentage of topics in these two categories substantially. In the Robust03 track (hard), we see that the percentage of topics which got hurt reduced to 6% from 14% and the percentage of topics which remained indifferent from 28% to 4%. The relatively high robustness

Table 2: Effect of MF and DEX on individual topics

Collection Name
CLEF 00-02 CLEF 03,05,06
AP WSJ SJM Robust03(Hard) Robust04

% Topics improved MF DEX 54 91 56 89 68 97 61 93 56 98 52 90 55 85

% Topics indifferent MF DEX 23 8 25 8 24 1 27 2 29 1 34 4 28 4

% Topics

hurt

MF DEX

23 1

19 3

8

2

11 5

15 1

14 6

17 11

of DEX gives hope for PRF techniques to achieve a higher degree of robustness while not sacrificing the gain in average retrieval performance. Wrong selection of terms is at the root of instability and PRF techniques will need to relook term selection strategies [1, 2].
3.3 Discussion
To understand why MF is suboptimal and unstable, we computed the average rank of the DEX terms in the rank list of the MF expansion terms (ranked according to p(.|F )). We suspected that DEX terms would not be at the top of the rank list. Because otherwise MF would not be comparatively so worse. Our suspicion turned out to be true. For a large majority of the topics, DEX terms were deep down in the MF rank list. For instance, the average rank of DEX terms for Topic 193 from TREC 3 was 114 and for TREC Topics 350 and 190 it was 229 and 735 respectively. It is clear that MF fails to recognize the importance of DEX terms and ranks them poorly. As a consequence of not choosing the right terms, expanded retrieval fails to improve the retrieval performance of these topics.
4. CONCLUSION
Our study shows that current PRF techniques are highly suboptimal and also that wrong selection of expansion terms is at the root of instability of current PRF techniques. A careful selection of expansion terms from the pseudo-relevant documents with the help of an oracle can actually improve retrieval performance dramatically (by > 60%). We believe our findings will motivate PRF researchers to revisit the issue of term seleaction in PRF. It might be worthwhile to selectively extract expansion terms from the feedback documents. Further, term interactions may prove crucial in addressing the problems of suboptimality and instability[2].
5. REFERENCES
[1] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting good expansion terms for pseudo-relevance feedback. In Proceedings of SIGIR '08.
[2] R. Udupa, A. Bhole, and P. Bhattacharyya. On selecting a good expansion set in pseudo-relevance feedback. In Proceedings of ICTIR '09.
[3] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval.In Proceedings of CIKM '01.
[4] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst.(2)
[5] E. N. Efthimiadis. Query expansion. Annual Review of Information Systems and Technology.
[6] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning.

814

High Precision Opinion Retrieval using Sentiment-Relevance Flows

Seung-Wook Lee swlee@nlp.korea.ac.kr

Jung-Tae Lee jtlee@nlp.korea.ac.kr

Young-In Song yosong@microsoft.com

Hae-Chang Rim rim@nlp.korea.ac.kr

 Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea  Microsoft Research Asia, Beijing, China

ABSTRACT
Opinion retrieval involves the measuring of opinion score of a document about the given topic. We propose a new method, namely sentiment-relevance flow, that naturally unifies the topic relevance and the opinionated nature of a document. Experiments conducted over a large-scaled Web corpus show that the proposed approach improves performance of opinion retrieval in terms of precision at top ranks.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms
Algorithms, Measurement, Experimentation
Keywords
opinion retrieval, sentiment analysis, sentiment-relevance flow
1. INTRODUCTION
Opinion retrieval is a new retrieval task which involves locating documents that express opinions about a topic of interest. With the rapid growth of user-centric media such as blogs and forums, opinion retrieval has been gaining considerable attention in recent years from academia and industry motivated by the huge business opportunities.
A key to success in opinion retrieval is to leverage both the topical relevance and the opinionated nature of documents simultaneously in ranking. However, most opinion retrieval systems separate the two components independently by adopting a re-ranking approach [1]. This approach involves finding as many relevant documents with regard to a given topic as possible regardless of their opinionated nature at first and then re-ranking them by combining the topical relevance scores with the opinion scores computed using some opinion detection techniques. Although a few approaches have shown attempts to unify the two components, for example, by using the proximity between topic words and opinion words within a document [3], the results are not yet conclusive and require more investigation.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Illustration of various flows.
In this paper, we present a new opinion retrieval method that adopts a very recently proposed technique involving relevance flow graphs [2]. A relevance flow graph of a document is a graphical plot of the topic relevance degree of individual sentences (with regard to a query) versus their positions in the document. The line graph labeled "Relevance" in Figure 1 illustrates an example; it visually shows the fluctuation of topic relevance levels within a document in regard to the query. [2] demonstrates that topically relevant documents have distinguishable flows from non-relevant ones in terms of the variance of relevance levels or the positions of high relevance levels (namely peaks), by designing a regression model based on such information and applying it to re-rank the retrieved results of conventional document ranking. Such method has shown promising results in improving the accuracy at top ranks.
Motivated by this achievement, we hypothesize that a truly relevant document in opinion retrieval not only has opinion sentences regarding the topic but does have them in predictable patterns within the document. Our idea is to create a new flow graph, namely sentiment flow, that plots the opinionated nature of individual sentences using some opinion scoring method, and then merge it with the topic relevance flow to generate a whole new flow graph, called sentiment-relevance flow. The dotted line graph labeled "Sent-Rel" in Figure 1 illustrates an example. The following section will elaborate on such technique.
2. SENTIMENT-RELEVANCE FLOW
The sentiment-relevance flow of a document (SRF) is a sequence of scores that reflect both the topic relevance with regard to the query and the opinionated nature of individual

817

sentences, ordered by the sentence positions. Given a query Q, we calculate the score of a sentence si at position i as follows:

score(Q, si) = topic(Q, si) · opinion(si)

(1)

where topic(Q, si) refers to the topic relevance score of si for Q calculated using some conventional relevance scoring function, such as the BM25 function. opinion(si) represents the opinion score of si computed as follows:

i+W |O|

opinion(si) =

f req(sj, owk)

(2)

j=i-W k=1

where W is the half size of the context window (empirically set to 15), O is a set of opinion word lexicon, and f req(sj, owk) is a function that returns the frequency of opinion word owk in sentence sj. When computing this score, we look at not only the sentence at hand but also its context, because we have observed that opinions often appear not in the same sentence where topic words occur but in preceding or succeeding sentences.
We normalize the individual sentence scores and the sentence positions in range 0 to 1 as in [2]. The graphical plot of such SRF, as shown in Figure 1, indicates the fluctuation of both topic relevance and opinionated nature within the document in a comprehensive way. As in [2], we refer to sentences that have scores higher than 0.5 as peaks.
In order to infer document relevance from SRFs, we use maximum entropy modeling to train a regression model that is able to predict the relevance of a document based on its SRF. The main features extracted from the SRFs for regression are the ones found useful in [2], which include the following: the variance of sentence scores, the fraction of peaks, and the first peak position.
For opinion retrieval, we rank documents by linearly combining the topic relevance scores with the new relevance scores inferred from their SRFs as follows:

score(Q, D) =  · topic(Q, D) + (1 - ) · srf (Q, D) (3)

where topic(Q, D) is the topic relevance of document D with regard to Q, and srf (Q, D) is the prediction score of the classifier for D.  is a weight parameter where 0    1.

3. EXPERIMENT
We conduct our experiments over Blogs06, a large-scaled Web blog collection. We use the title queries of 150 topics used in 2006, 2007, and 2008 TREC Blog Tracks. We validate our method in a re-ranking scheme. In other words, we initially retrieve the top n (=15) documents using two popular ranking models, the BM25 model and the query likelihood (QL) language model with dirichlet smoothing1, and then re-rank the results using Equation 3. When evaluating one query set (50 topics), other two query sets (100 topics) are used to train a regression model. 15 top ranked documents for every training query are divided into positive and negative training instance groups based on their relevance judgments. The main evaluation measures are P@1, P@3, and P@5 since our method aims to achieve high accuracy at top ranks via re-ranking. The relevance judgement set consists of two distict aspects, relevance and opinionated
1We empirically tuned k1=1.2, b=0.1 for BM25 and µ=5000 for QL smoothing parameter.

Table 1: Performance of topical retrieval

Method

 P@1 P@3 P@5

BM25

N/A 0.5800 0.6334 0.6041

BM25+RF 0.15 0.6333 0.6422 0.6427

BM25+SRF 0.00 0.7200 0.7089 0.6907

QL

N/A 0.5933 0.6200 0.6160

QL+RF

0.02 0.6333 0.6711 0.6587

QL+SRF 0.01 0.7600 0.6956 0.6933

Table 2: Performance of opinion retrieval

Method

 P@1 P@3 P@5

BM25

N/A 0.4400 0.4556 0.4320

BM25+SRF 0.01 0.5800 0.5800 0.5573

QL

N/A 0.4800 0.4756 0.4640

QL+FULL N/A 0.5600 0.5222 0.5373

QL+PROX N/A 0.5800 0.5556 0.5547

QL+SRF

0.01 0.6333 0.5689 0.5520

relevance. Thus, we report the topic P@Ns and the opinion P@Ns of the system separately. For sentence boundary detection, we use a public sentence splitter software2. We use the sentiment word list in the General Inquirer3, which is a public opinion dictionary in the linguistics field. We only collect adjectives, adverbs, and verbs from the opinion and emotion categories; as a result, the lexicon is made up of 1,496 entries.
Table 1 shows the performance of topical retrieval for the two initial results, relevance flow based re-ranking (RF) and our sentiment-relevance flow based re-ranking (SRF). As mentioned from previous study, RF successfully re-ranks high position documents. It is notable that our SRF also improves the performance of traditional topical retrieval since it basically aims to capture the pattern of relevant sentences.
In aspect of opinion retrieval, as shown in Table 2, SRF shows significant and consistant improvement. We compare our method with two previous re-ranking approaches for opinion retrieval with QL setting (since it outperforms BM25). Opinion score measured by QL+FULL is dominated by a number of opinion words that appeared in a whole document, while QL+PROX only considers opinion words located within W sentences from the query terms. We can observe that proximity is a helpful feature for opinion retrieval. It is remarkable that our sentiment-relevance flow based re-ranking scheme achieves better improvement. Note that the maximum performance are acheived on low  values which implies that the trained regression model based on SRF features are very accurate and reliable.
4. REFERENCES
[1] I. Ounis, C. Macdonald, and I. Soboroff. Overview of the TREC-2008 blog track. In Proc. of TREC 2008.
[2] J. Seo and J. Jeon. High precision retrieval using relevance-flow graph. In Proc. of SIGIR 2009.
[3] M. Zhang and X. Ye. A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval. In Proc. of SIGIR 2008.
2http://l2r.cs.uiuc.edu/~ecogcomp/tools.php 3http://www.wjh.harvard.edu/~inquirer/

818

Query Term Ranking based on Dependency Parsing of Verbose Queries

Jae-Hyun Park and W. Bruce Croft
Center for Intelligent Information Retrieval Department of Computer Science
University of Massachusetts, Amherst, MA, 01003, USA
{jhpark,croft}@cs.umass.edu

ABSTRACT
Query term ranking approaches are used to select effective terms from a verbose query by ranking terms. Features used for query term ranking and selection in previous work do not consider grammatical relationships between terms. To address this issue, we use syntactic features extracted from dependency parsing results of verbose queries. We also modify the method for measuring the effectiveness of query terms for query term ranking.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query formulation
General Terms
Algorithm, Experimentation, Performance
Keywords
Dependency Parse, Query Reformulation, Query Term Ranking
1. INTRODUCTION
Most search engines have a tendency to show better retrieval results with keyword queries than with verbose queries. Verbose queries tend to contain more redundant terms and these terms have grammatical meaning for communication between humans to help identify the important concepts.. Search engines do not typically use syntactic information.. For example, given a verbose query, "Identify positive accomplishments of the Hubble telescope since it was launched ...", search engines cannot recognize that "Hubble telescope" is the key concept of the query whereas "accomplishments" should be considered as a complementary concept, while people can readily identify this by analyzing the grammatical structure of the query. Therefore, search engines potentially need a method for exploiting this structure.
In this work, we rank terms in a verbose query and reformulate a new query using selected highly ranked terms. Good selection methods should be able to leverage the grammatical roles of terms within a query. To do this, we use syntactic features extracted from dependency parsing trees of queries. In addition, we suggest a new method for measuring the effectiveness of terms for query term ranking.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. QUERY TERM RANKING
2.1 Features extracted from Dependency Parsing
We use syntactic features extracted from dependency parsing to capture the grammatical properties of terms for a query. Features used by previous work in query term ranking [1, 6] are inadequate to reflect these characteristics. The limitation of these features is that they are based on individual terms. Features such as tf, idf, part-of-speech (PoS) tag, etc. will not change even if the role of the term changes according to the syntactic structure of queries. Even features for sub-queries [5] are also unlikely to reflect grammatical characteristics because they are not affected by the structure of queries.
Therefore, we propose to overcome this limitation by using dependency parsing trees. A typed dependency parse labels dependencies with grammatical relations [3]. Figure 1 shows an example of a typed dependency parse tree. Dependency parsing tree fragments of terms can provide grammatical information about terms in queries [2].
It is infeasible to use all dependency parse tree fragments as syntactic features. We limit the number of arcs in syntactic features to two arcs. Even if we limit the number of arcs, some of collected tree fragments are too specific to

Sentence: Identify positive accomplishments of the Hubble telescope since it was launched in 1991.

Identify

dobj

amod accomplishments prep_of

positive

telescope nn

Hubble

Figure 1: An example of dependency parsing trees. Labels attached to arcs are types of dependencies.

Identify

dobj

*

dobj

Identify

*

accomplishments accomplishments

(a)

(b)

accomplishments (c)

Figure 2: Three types of syntactic features for the term "accomplishments". (a) An original syntactic feature (b) The word is generalized to a * (c) The type of the dependency is generalized to a *

829

have a reliable amount of training data and not all of them are useful. We generalize syntactic features which consist of arcs labeled with dependency types and nodes representing words which are dependent. Figure 2 shows an example of an original syntactic feature and its generalized features. In the figure, "*" means any word or any type of dependency.
2.2 Query Term Ranking
Our approach aims to rank terms in a query and to reformulate the query using the ranking. To build training data for a ranking model, Bendersky and Croft [1] manually annotate the concept from each query that had the most impact on effectiveness. For given terms = { 1, 2, ..., }, they used labeled instances ( , ), where is a binary label, as training data. However, queries can have more than one effective term or concept. In addition, it is difficult for annotators to judge the effectiveness of a term. Therefore, we estimate the effectiveness of terms, i.e., the labels for training data, by evaluating the search results of terms in training data. By using these estimated scores, we expect that a ranking model can take account of all terms in a query and consider how effective they are.
Lee et al. [6] point out the importance of underlying correlations between terms. Previous work has evaluated the effectiveness of groups of terms instead of individual terms to capture these relationships [5, 6]. The problem is that the number of unique groups will grow exponentially with the size of the term groups and it will cause a data sparseness problem. We used the following equation for ( ), the effectiveness of a term to reflect the effects of relationships between terms in training labels.

( )=

1



 (

(,

)-

( )),

(1)



where is all possible combinations of m terms except . N is the number of elements in and ( ) is the search performance of . Eq. (1) estimates the effectiveness of term
through aggregating the impacts of term on effectiveness when using it with other terms in . Thus, the scores of Eq. (1) reflects the correlations between and other terms.

3. EXPERIMENTS AND ANALYSIS
We evaluated our proposed method using two TREC collections: Robust 2004 (topic numbers are 301-450 and 601700) and Wt10g (topic numbers are 450-550). The average number of nouns, adjectives and verbs in queries of Robust2004 and Wt10g are 8.7 and 6.5 per a query, respectively. We used the language model framework with Dirichlet smoothing ( set to 1,500). Indexing and retrieval were conducted using the Indri toolkit.
To rank query terms, we used RankSVM [4]. We trained query term ranking models for each query using leave-oneout cross-validation in which one query was used for test data and the others were used for training data. We labeled training data based on Key concepts [1] and the effectiveness measured by Eq. 1 in which we chose nDCG as the performance measure. We used syntactic features in addition to tf, idf, and PoS tag features.
When we combined selected terms with original queries, we used two approaches. First, we assigned uniform weights to selected terms (binary). Alternatively, we used query term ranking scores as the weight for selected terms (weight).

Table 1: Mean Average Precision (MAP) of Ro-

bust04 and Wt10g collections, Key-Concept: using

key concept [1] as labels of training data, Auto: us-

ing effectiveness in retrieval as labels of training data

Robust04 Wt10g

<title>

25.17

18.55

<desc>

24.07

17.52

Key-Concept

binary weight

23.98 24.24

18.55 19.45

Auto

binary weight

25.40 26.21

17.91 19.15

Experimental results in Table 1 shows that selected terms by using query term ranking have better performance than description queries except for one result in which we used key concepts and uniform weighting. In this case, only the most important concepts in queries are labeled, whereas the effectiveness in retrieval is measured for all terms in queries. This difference makes the method using the effectiveness of terms (Auto) superior for the relatively longer queries in Robust2004, and the method using key concepts (Key Concept) better for the shorter queries in Wt10g.
4. CONCLUSIONS
In this paper, we propose a query term ranking method that uses syntactic features extracted from dependency parsing trees. By using syntactic features, we can take into account grammatical relationships between terms. We also modify the query term ranking method to measure the effectiveness of terms based on combinations of terms. Experimental results showed that the terms selected by the query term ranking method improved retrieval performance.
5. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS-0711348. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
6. REFERENCES
[1] M. Bendersky and W. B. Croft. Discovering key concepts in verbose queries. In Proc. ACM SIGIR, pages 491­498, 2008.
[2] A. Chanen. A comparison of human and computationally generated document features for automatic text classification. PhD thesis, The University of Sydney, 2009.
[3] M. De Marneffe, B. MacCartney, and C. Manning. Generating typed dependency parses from phrase structure parses. In Proc. LREC 2006, 2006.
[4] T. Joachims. Optimizing Search Engines Using Clickthrough Data. In Proc. ACM KDD, pages 133­142, 2002.
[5] G. Kumaran and V. Carvalho. Reducing long queries using query quality predictors. In Proc. ACM SIGIR, pages 564­571, 2009.
[6] C. Lee, R. Chen, S. Kao, and P. Cheng. A term dependency-based approach for query terms ranking. In Proc. CIKM, pages 1267­1276, 2009.

830

Probabilistic Latent Maximal Marginal Relevance

Shengbo Guo
ANU & NICTA Canberra, Australia
shengbo.guo@nicta.com.au

Scott Sanner
NICTA & ANU Canberra, Australia
scott.sanner@nicta.com.au

ABSTRACT
Diversity has been heavily motivated in the information retrieval literature as an objective criterion for result sets in search and recommender systems. Perhaps one of the most well-known and most used algorithms for result set diversification is that of Maximal Marginal Relevance (MMR). In this paper, we show that while MMR is somewhat adhoc and motivated from a purely pragmatic perspective, we can derive a more principled variant via probabilistic inference in a latent variable graphical model. This novel derivation presents a formal probabilistic latent view of MMR (PLMMR) that (a) removes the need to manually balance relevance and diversity parameters, (b) shows that specific definitions of relevance and diversity metrics appropriate to MMR emerge naturally, and (c) formally derives variants of latent semantic indexing (LSI) similarity metrics for use in PLMMR. Empirically, PLMMR outperforms MMR with standard term frequency based similarity and diversity metrics since PLMMR maximizes latent diversity in the results.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms
Algorithms
Keywords
diversity, graphical models, maximal marginal relevance
1. INTRODUCTION
Maximal marginal relevance (MMR) [2] is perhaps one of the most popular methods for balancing relevance and diversity in set-based information retrieval and has been cited over 530 times1 since its publication in 1998.
The basic idea of MMR is straightforward: suppose we have a set of items D and we want to recommend a small subset Sk  D (where |Sk| = k and k  |D|) relevant to a given query q. MMR proposes to build Sk in a greedy
1According to Google Scholar.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

manner by selecting sj given Sj-1 = {s1, . . . , sj-1} (where Sj = Sj-1  {sj }) according to the following criteria

sj

= arg max [(Sim1(sj, q)) - (1
sj D\Sj-1

-

) max
si Sj-1

Sim2(sj, si)]

(1)

where Sim1(·, ·) measures the relevance between an item and a query, Sim2(·, ·) measures the similarity between two items, and the manually tuned   [0, 1] trades off relevance and similarity. In the case of s1, the second term disappears.
While MMR is a popular algorithm, it was specified in a

rather ad-hoc manner and good performance typically relies

on careful tuning of the  parameter. Furthermore, MMR is

agnostic to the specific similarity metrics used, which indeed

allows for flexibility, but makes no indication as to the choice

of similarity metrics for Sim1 and Sim2 that are compatible with each other and also appropriate for good performance.

In the next section, we address these concerns by taking a

more principled approach to set-based information retrieval

via maximum a posteriori probabilistic inference in a latent

variable graphical model of marginal relevance (PLMMR).

As an elegant and novel contribution, we note that natural

relevance and diversity metrics emerge from this derivation

(with no analogous manually tuned  parameter) and that

these metrics also formally motivate variants of similarity

metrics used in latent semantic indexing (LSI) [3].

2. PROBABILISTIC LATENT MMR

Figure 1: Graphical model used in PLMMR.
We begin our discussion of PLMMR by introducing a graphical model of (marginal) relevance in Figure 1. Shaded nodes represent observed variables while unshaded nodes are latent; we do not distinguish between variables and their assignments. The observed variables are the vector of query terms q and the selected items s1  D and s2  D. For the latent variables, let T be a discrete topic set; variables t1  T and t2  T respectively represent topics for s1 and

833

s2 and t  T represents a topic for query q. r1  {0, 1} and
r2  {0, 1} are variables that indicate whether the respective
selected items s1 and s2 are relevant (1) or not (0). The conditional probability tables (CPTs) in this discrete
directed graphical model are defined as follows. P (t1|s1) and P (t2|s2) represent topic models of the items and P (t|q) represents a topic model of the query. There are a variety of ways to learn these topic CPTs based on the nature of the items and query; for an item set D consisting of text documents and a query that can be treated as a text document, a natural probabilistic model for P (ti|si) and P (t|q) can be derived from Latent Dirichlet Allocation (LDA) [1]. Finally, the CPTs for relevance ri have a very natural definition:

P (r1|t, t1) =

1 0

if t1 = t if t1 = t

P (r2|t, r1 = 0, t1, t2) =

1 0

if (t2 = t1)  (t2 = t) if (t2 = t1)  (t2 = t)

Simply, s1 is relevant if its topic t1 = t (the query topic).

s2 is relevant with the same condition and the addition that

if s1 was irrelevant (r1 = 0), then topic t2 for s2 should also

not match t1. Following the click-chain model [4], we assume

the user only examines s2 if s1 was irrelevant (r1 = 0).

Let us assume that like MMR we use a greedy item set se-

lection algorithm given S1 = {s1},

and we have already we want to select s2

selected s1 = s1. Now in order to maximize

its marginal relevance w.r.t. q given S1, formally defined as

MR(S1, s2, q) and derived as a query in the graphical model:

s2 = arg max MR(S1, s2, q) = arg max P (r2|s1, s2, q)

s2 D\S1

s2D\{s1 }

= arg max

P (r2|r1 = 0, t1, t2, t)P (t1|s1)

s2 D\{s1 } t1,t2,t

P (r1 = 0|t1, t)P (t2|s2)P (t|q)

= arg max
s2 D\{s1 }

P (t|q)P (t2 = t|s2) -
t
relevance

P (t|q)P (t1 = t|s1)P (t2 = t|s2)

(2)

t

diversity

The basic insight leading to this fascinating result is the
exploitation of the indicator structure of the relevance vari-
ables r1 and r2 to make convenient variable substitutions. We note that in this special case for MR(S1, s2, q), a very
natural mapping to the MMR algorithm in (1) when  = 0.5 has emerged automatically from the derivation that maximized MR. This derivation automatically balances relevance and diversity without an analogous  and it suggests very specific (and different) relevance and diversity metrics, both effectively variants of similarity metrics used in latent semantic indexing (LSI) [3]. To make this clear, we examine
tlqehuteerTryeleqavananndcdeTitm2emebtersic2reSwsipimtehP1cLtviMevcM etRotrogpeivilceemnpebrnoytbsPaTbLiMliitM=y RPve(wctthoe=rrsei|fwqoer) and T2i = P (t2 = i|s2) and using ·, · for the inner product:

SimP1 LMMR(q, s2) = P (t|q)P (t2 = t|s2) = T, T2 .
t

A similar analysis gives diversity metric SimP2 LMMR(s1, s2), yielding a variant LSI similarity metric reweighted by the query topic probability P (t|q). This points out the important correction to MMR that item set diversity should be

Table 1: Weighted subtopic loss (WSL) of three methods using all words and first 10 words. Standard error estimates are shown for PLMMR-LDA.

Method MMR-TF MMR-TFIDF PLMMR-LDA

WSL (first 10 words) 0.555 0.549
0.458 ± 0.0058

WSL (all words) 0.534 0.493
0.468 ± 0.0019

query-relevant! Given these definitions of SimP1 LMMR and SimP2 LMMR, we can now substitute these into the MMR algorithm defined in (1) to arrive at a definition of PLMMR.

3. EXPERIMENTAL COMPARISON
We report experiments on a subset of TREC 6-8 data focusing on diversity. We follow the same experimental setup as [6] who measure the weighted subtopic loss (WSL) of recommended item sets where in brief, WSL gives higher penalty for not covering popular subtopics. We do not compare directly to [6] as their method was supervised while MMR and PLMMR are inherently unsupervised.
Standard query and item similarity metrics used in MMR applied to text data include the cosine of the term frequency (TF) and TF inverse document frequency (TFIDF) vector space models [5]. We denote these variants of MMR as MMR-TF and MMR-TFIDF. PLMMR specifically suggests the use of LSI-based similarity metrics defined in the last section; thus, we use LDA to derive these models, referring to the resulting algorithm as PLMMR-LDA. LDA was trained with  = 2.0,  = 0.5, |T | = 15; we note the results were not highly sensitive to these parameter choices.
Average WSL scores are shown in Table 1 on the 17 queries examined by [6]. We use both full documents and also just the first 10 words of each document. For both MMR algorithms, the best performing  = 0.5 is shown. We note that due to the power of the latent topic model and derived similarity metrics, PLMMR-LDA is able to perform better than MMR with standard TF and TFIDF metrics and without a  parameter to be tuned. In addition, PLMMRLDA works very well with short documents since intrinsic document and query similarities are automatically derived from the latent PLMMR relevance and diversity metrics.

4. REFERENCES
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 3:993­1022, 2003.
[2] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR, 335­336. 1998.
[3] S. Deerwester, S. T. Dumaisand, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. JASIS, 41:391­407, 1990.
[4] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, Y.-M. Wang, and C. Faloutsos. Click chain model in web search. In WWW-09, Madrid, Spain, 2009. ACM.
[5] G. Salton and M. McGill. Introduction to modern information retrieval. McGraw-Hill, 1983.
[6] Y. Yue and T. Joachims. Predicting diverse subsets using structural SVMs. In ICML, 1224­1231, 2008.

834

A Framework for BM25F-based XML Retrieval

Kelly Y. Itakura
David R. Cheriton School of Computer Science University of Waterloo 200 University Ave. W. Waterloo, ON, Canada
yitakura@cs.uwaterloo.ca

Charles L.A. Clarke
David R. Cheriton School of Computer Science University of Waterloo 200 University Ave. W. Waterloo, ON, Canada
claclark@uwateloo.ca

ABSTRACT
We evaluate a framework for BM25F-based XML element retrieval. The framework gathers contextual information associated with each XML element into an associated field, which we call a characteristic field. The contents of the element and the contents of the characteristic field are then treated as distinct fields for BM25F weighting purposes. Evidence supporting this framework is drawn from both our own experiments and experiments reported in related work.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms
Theory, Experimentation
Keywords
XML retrieval, BM25, BM25F, Wikipedia, book search
1. INTRODUCTION
INEX [1], the annual Initiative for the Evaluation of XML retrieval, includes experiments on ad hoc focused XML element retrieval, where the task is to return a ranked list of document elements (e.g., paragraphs, sections, abstracts) in response to a previously unseen query. Elements are required to be non-overlapping, so that no returned element contains another, but otherwise any document element may be returned.
While various participating groups have reported attempts to exploit XML structure in order to improve performance on this task, none of these efforts have consistently outperformed the simple approach of applying Okapi BM25 [12] to score individual XML elements and then filtering the resulting ranked list to remove overlap. Under this approach, each element is scored as if it were an independent document. The context of the element -- such as information appearing in the elements that surround it -- is ignored. Runs using this basic approach ranked third in 2004, third in 2007, and first in 2008 [4, 5, 9].
Okapi BM25 is a well-established ranking formula, which has proven its value across a wide range of domains and ap-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

plications. For XML element retrieval, we use the following version:

s(E)



tQ

Wt

fE,t

fE,t(k + k(1 -

+ 1) b+b

elE avgdl

)

,

(1)

where Q is a set of query terms, Wt is the IDF weight of the term t, fE,t is the term frequencies in an element E, elE is an element length of E, and avgdl is the average document length in the collection. In applying BM25 to element retrieval, we continue to use document-level statistics for the computation of Wt.
Robertson et al. [2, 13] describe BM25F, an extension of BM25 that exploits structural information. Under BM25F, terms contained in a document's title, for example, may be given more weight than terms contained in the document's body.
In this paper, we explore a framework for adapting BM25F to XML element retrieval. Under this framework, we construct two fields for each element. One field contains the contents of the element itself; the other field, called the characteristic field, contains contextual information associated with the element. This contextual information will vary from domain to domain, but might include the title of the overall document, titles from containing elements, and document metadata.

2. BM25F

Using BM25F, an element's score is computed as follows:

BM 25F

(e)

=

tqe

K

xe,t + xe,t

Wt

,

where q is a query term, xe,t is a weighted normalized term frequency, K is a tunable parameter, and Wt is documentlevel IDF for a term t. To obtain xe,t, length normalization is first performed separately for each field f associated with an

element e, producing field-specific normalized term frequen-

cies. These normalized term frequencies xe,f,t are multiplied by field-specific weights Wf and summed to obtain xe,t

xe,f,t =

xe,f,t

1

+

Bf

(

le,f lf

- 1)

, xe,t =

f

Wf · xe,f,t ,

where xe,f,t is the term frequency of t in field f of element e, Bf is a tunable parameter, le,f is the length of f in e, and lf is the average field length of f . We report the results obtained by treating average document and field lengths as
a constant, but later experiments that treated them as pa-
rameters seem to give no advantage.

843

Table 1: Adhoc Focused Retrieval

Run

iP[0.01] rank

BM25F 0.6333 1

BM25 0.5940 12

For INEX 2005, Robertson et al. applied an earlier version of BM25F to XML element retrieval [8, 11], reporting 65% improvements over BM25 measured by nxCG(10) on INEX IEEE collection with a different task where overlap is allowed. In that work, an element's score is computed from multiple fields, which may include the body of the element, the document's title, the document's abstract, and ancestral section titles.
Trotman describes another effort to extend BM25F to XML element retrieval [15] on TREC Wall Street Journal collection, but showed that improvement obtained is 0.64% computed by mean average precision. BM25F has also been used for XML-encoded book retrieval, where the task was to return books not elements [7, 16] producing 9.09% improvement measured by NDCG@1.
3. THE CHARACTERISTIC FIELD
In order to simplify the application of BM25F to XML element retrieval, we propose a framework with only two fields for each element. The body field contains the element body, and the characteristic field contains any contextual or background information that characterizes the element. The precise contents of the characteristic field may vary from element to element. While this approach is similar to that of Robertson et al. [11] and Lu et al. [8] it avoids the complexity of multiple field types and allows a consistent approach to be applied across heterogenous elements.
4. EXPERIMENTS AND RESULTS
Ad Hoc Retrieval. We first report the results of runs on INEX 2009 ad hoc task. We trained on a 5.9GB INEX 2008 Wikipedia corpus [3] with 659,387 articles and 70 assessed topics and tested on a 50.7GB INEX 2009 Wikipedia corpus [14] with 2,666,190 articles and 68 assessed topics. Our training optimized the official metric of iP[0.01]. For these runs, we used a characteristic field formed from the titles of the article and the sections in which an element occurs.
Table 1 shows the official INEX results. The BM25F run that ranked first gives a 6.62% improvement over the BM25 run that ranked 12th.
Book Page Retrieval . We used INEX 2008 Book Track data [6] of 50239 books of size 37GB after pre-processing. Only 25 out of 70 topics had relevance judgements, thus we used 17 of them for training, and 8 for testing. The corpus comes with a file, machine readable cataloging (MARC) format [10], that contains information such as book category and library of congress classification (LCC) code.
The Book track task required to group the pages by the books and rank the books. Thus all of our runs did so and ranked the books by the highest scoring page returned for the book. Training maximized mean average precision.
Table 2 shows the results of our experiments. The runs with the plus signs indicate information used in the characteristic field. We see that using characteristic information gives up to 48.92% and 35.45% improvement over BM25 during training and testing respectively.

Table 2: Book Page Retrieval

Run

MAP (training) MAP (test)

BM25

0.0278

0.0110

BM25F+title

0.0412

0.0149

BM25F+title+cat

0.0413

0.0139

BM25F+title+cat+LCC

0.0414

0.0137

5. CONCLUSIONS
We propose a framework for applying BM25F to XML element retrieval through the addition of a single characteristic field. This characteristic field merges contextual information from multiple sources, which may include inherited titles and metadata. The proposal is inspired by previous work, but aims to avoid the complexity of multiple fields and heterogenous structure by merging contextual information into this single field.
The proposal is evaluated in the context of the INEX effort. While our results are preliminary, and the results of the INEX 2009 book track have not yet been fully judged, they suggest that the benefits of field weights may be obtainable even in this simplified framework.
Future work includes experimenting our version of BM25F on INEX heterogeneous track collection and taking advantage of more detailed structural information available in the new INEX 2009 Wikipedia collection.
6. REFERENCES
[1] Initiative for the Evaluation of XML retrieval. www.inex.otago.ac.nz.
[2] N. Craswell, H. Zaragoza, and S. Robertson. Microsoft Cambridge at TREC 14: Enterprise track. In Proceedings of the TREC 14, 2005.
[3] L. Denoyer and P. Gallinari. The Wikipedia XML corpus. SIGIR Forum, 40(1):64­69, 2006.
[4] N. Fuhr, J. Kamps, M. Lalmas, S. Malik, and A. Trotman. Overview of the INEX 2007 Ad Hoc Track. INEX 2007, 4862:1­23, 2007.
[5] J. Kamps, S. Geva, A. Trotman, A. Woodley, and M. Koolen. Overview of the INEX 2008 Ad Hoc Track. INEX 2008, 5631:1­28, 2009.
[6] G. Kazai, A. Doucet, and M. Landoni. Overview of the INEX 2008 Book Track. In INEX 2008, pages 106­123, 2009.
[7] G. Kazai and N. Milic-Frayling. Effects of social approval votes on search performance. In Proceedings of ITNG 2009, pages 1554­1559, 2009.
[8] W. Lu, S. Robertson, and A. MacFarlane. Field-weighted XML retrieval based on BM25. In INEX 2005, pages 161­171, 2006.
[9] S. Malik, M. Lalmas, and N. Fuhr. Overview of INEX 2004. LNCS, 3493:1­15, 2005.
[10] L. of Congress Network Development and M. S. Office. MARC standards. www.loc.gov/marc/.
[11] S. Robertson, W. Lu, and A. MacFarlane. XML-structured documents: Retrievable units and inheritance. In Proceedings of FQAS 2006, pages 121­132, 2006.
[12] S. Robertson, S. Walker, and M. Beaulieu. Okapi at TREC-7: Automatic ad hoc, filtering, vlc and interactive track. Proceedings of TREC-7, 1998.
[13] S. Robertson, H. Zaragoza, and M. Taylor. Simple BM25 extension to multiple weighted fields. In Proceedings of CIKM 2004, pages 42­49, 2004.
[14] R. Schenkel, F. Suchanek, and G. Kasneci. YAWN: A semantically annotated Wikipedia XML corpus. In 12. GI-Fachtagung fu¨r Datenbanksysteme in Business, Technologie und Web, pages 277­291, 2007.
[15] A. Trotman. Choosing document structure weights. Inf. Process. Manage., 41(2):243­264, 2005.
[16] H. Wu, G. Kazai, and M. Taylor. Book search experiments: Investigating IR methods for the indexing and retrieval of books. LNCS, 4956:234, 2008.

844

Learning to Select Rankers
Niranjan Balasubramanian and James Allan
Department of Computer Science University of Massachusetts Amherst 140 Governors Drive, Amherst, MA 01003, USA
niranjan@cs.umass.edu, allan@cs.umass.edu

ABSTRACT
Combining evidence from multiple retrieval models has been widely studied in the context of of distributed search, metasearch and rank fusion. Much of the prior work has focused on combining retrieval scores (or the rankings) assigned by different retrieval models or ranking algorithms. In this work, we focus on the problem of choosing between retrieval models using performance estimation. We propose modeling the differences in retrieval performance directly by using rank-time features ­ features that are available to the ranking algorithms ­ and the retrieval scores assigned by the ranking algorithms. Our experimental results show that when choosing between two rankers, our approach yields significant improvements over the best individual ranker.
Categories and Subject Descriptors: H.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation, Theory
Keywords: Combining Searches, Learning to Rank, Metasearch
1. INTRODUCTION
Combining evidence from multiple sources has been studied in various contexts [2, 1, 4, 6]. The basic premise for combining evidence from multiple retrieval models is that there is no single model that performs the best on all queries. Several rank fusion [7] and rank aggregation [4] approaches have been proposed to re-rank documents based on retrieval scores (or rankings) obtained from individual rankers. However, most of these approaches either learn a fixed (query independent) set of weights that are used to combine document scores or utilize a voting scheme for combining the rankings.
Instead of learning to combine document scores in a query dependent manner, we consider the problem of selecting a ranker for a given query. We propose a simple framework that directly predicts the differences in effectiveness between the results of different retrieval models. In particular, we consider the web-search scenario, where a large number of features are often combined using sophisticated learning to rank algorithms (rankers). For the sake of simplicity, we assume that we have access to two different rankers that operate on the same set of features. We formally define the ranker selection problem as follows:
Problem Definition. Given two rankers, Ra and Rb, we choose one ranker to be the baseline ranker (say Rb) ­ either arbitrarily, or based on the prior knowledge about the average performance of the
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

rankers. Then, for each query, the selection problem is to determine whether to use the baseline ranker Rb or the alternate ranker Ra.
This problem definition and the selection framework that we propose can be extended to the scenario where we have access to multiple alternate rankers.
Ranker Selection Framework. We propose a simple framework for directly predicting the difference between the performance of two rankers(Rb and Ra) in terms of average precision (AP). We use the retrieval scores and the features of the top ranked documents (referred as retrieval features henceforth) to train a regressor. In addition to being closely related to the performance of the rankers, these features are also easy to compute, compared to typical performance prediction measures such as Clarity [3].
As shown in Figure 1, for a given test query, we first rank documents using both rankers. Then, for each ranked list, we compute mean and variance of the scores and the standard deviation of the retrieval features to generate aggregate feature vectors. The difference between the two aggregate vectors is input to the regressor which predicts the difference in effectiveness (AP(Ra) - AP(Rb)). If the predicted difference is positive, then we select the alternate ranker, otherwise, we use the baseline ranker.

Ra DDocoucRmuemesnuetlntst
Rb DDocoucRmuemesnuetlntst

Doc 1 f1,f2,...,fn
...

Score 1 ...

Doc k f1,f2,...,fn

Score k

Doc 1 f1,f2,...,fn
...

Score 1 ...

Doc k f1,f2,...,fn

Score k

Aggregator Aggregator

Ra Features
a11,a12,...,a1n ...
al1,al2,...,aln
Ra Features
b11,b12,...,b1n ...
bl1,bl2,...,bln

Regressor
AP(Ra) >
AP(Rb) ?

Figure 1: Ranker Selection Process

2. EXPERIMENTS
To evaluate our ranker selection approach, we use the LETOR 3.0 dataset [8] built on top of the TREC Gov2 collection. We conduct 5-fold cross validation experiments on a set of 225 queries created for the TREC named page finding tasks (NP 2003 and NP 2004). We use the results from three ranker baselines: RankBoost [5], Regression, and FRank [9]. To create features for the selection framework, we use the published test runs1 for these rankers to obtain the document scores for top 10 ranking documents, and the list of 64 features that are available as part of LETOR. We use a non-linear Random Forest regression model for our experiments. We compare the rankers using mean-average precision (MAP).
1http://research.microsoft.com/enus/um/beijing/projects/letor/letor3baseline.aspx

855

In terms of MAP, RankBoost is the best individual ranker, followed by FRank and Regression. Table 1 shows the potential for the use of query dependent ranker selection for named page finding. For example, RankBoost outperforms Regression on 90 queries, but performs worse on nearly half as many. Furthermore, we see that an oracle selection method can provide nearly 30% improvement over Regression, and nearly 15% improvement over FRank and 12% improvement over RankBoost.

Table 1: Potential for Improvement using Ranker Selection. Rb ­ Baseline ranker (RankBoost), Ra ­ Alternate ranker. Worse and Better indicate the number of queries for which Ra is worse than Rb in terms of MAP and vice versa. RankBoost has a MAP of 0.6596

Ra

MAP(Ra) Worse Better Oracle Selection

Regression 0.5476

90

42

0.7096

FRank

0.6429

62

45

0.7316

We conduct two sets of selection experiments one with RankBoost as the baseline ranker, and the other with RankBoost as the alternate ranker. Even though the rankers train on differences in performance, the distribution of the positive and negative differences change for each setting, thereby leading to different behavior in terms of the achieved improvements.

Table 2: Ranker Selection effectiveness on a set of 225 name page finding queries on the Gov2 collection. Rb ­ Baseline ranker, Ra ­ Alternate ranker. M AP (Rs) indicates the MAP achieved with ranker selection. Underline indicates best MAP. * indicates significant improvements over Regression/FRank when using a paired t-test with p < 0.05

Rb Regression FRank
RankBoost RankBoost

Ra RankBoost RankBoost
Regression FRank

MAP(Rb) 0.5476 0.6429
0.6596 0.6596

MAP(Ra) 0.6596 0.6596
0.5476 0.6429

MAP(Rs) 0.6623 0.6591
0.6722 0.6607

Results of the two selection experiments are tabulated in Table 2. When using RankBoost as the alternate ranker, selection yields improvements over both Regression and FRank. This is in part because RankBoost performs better than both these algorithms for most queries. However, selection does not provide substantial improvements over RankBoost, the best individual ranker. On the other hand, when using RankBoost as the baseline ranker and Regression as the alternate ranker, we obtain substantial improvements using selection. Interestingly, even though FRank has a higher MAP compared to Regression, using FRank as the alternate ranker yields smaller improvements. This suggests that effectiveness of the selection also depends on the type of ranking algorithm used, in addition to the performance of the ranker itself.
The distribution of gains achieved for ranker selection between RankBoost and Regression is shown in Figures 2 (a) and (b). In both cases, we see that for a large fraction of the queries, choosing the alternate ranker results in gains, and very few cases result in losses. When using RankBoost as the baseline ranker, selection uses Regression for a small number of queries (28), and provides gains for subset (14), but the choice results in fewer losses (4). However, when using RankBoost as the alternate ranker, selection uses RankBoost for a large number of queries (198), out of which 83 queries result in gains and 29 result in losses. This suggests that while ranker selection yields substantial gains, it can also benefit from limiting losses due to poor selection. For example, thresholding on the predicted differences can reduce the number of queries for which the alternate ranker is queried.

Difference in AP

-0.6

-0.2 0.0 0.2 0.4 0.6

(a) Rb: RankBoost

(b) Rb: Regression

Figure 2: Distribution of Ranker Selection Gains: (a) When using RankBoost as the baseline and (b) When using RankBoost as the alternate ranker

3. CONCLUSIONS
In this paper, we proposed a simple learning approach for querydependent selection of rankers. Our selection framework utilizes rank-time features ­ features that are available to the ranking algorithms during ranking. For selecting between two rankers, our experimental results show that a simple regression model that directly predicts differences in effectiveness, can achieve substantial improvements over the best individual ranker. As part of future work, we plan to investigate selection between multiple rankers using more sophisticated features for performance prediction.

4. ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-0910884. Any opinions, findings and conclusions or recommendations expressed here are the authors' and do not necessarily reflect those of the sponsor.

5. REFERENCES
[1] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked retrieval systems. In SIGIR '94: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 173­181, 1994.
[2] W. B. Croft. Incorporating different search models into one document retrieval system. SIGIR Forum, 16(1):40­45, 1981.
[3] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In SIGIR '02: 25th Annual ACM SIGIR Conference Proceedings, pages 299­306, 2002.
[4] M. Farah and D. Vanderpooten. An outranking approach for rank aggregation in information retrieval. In SIGIR '07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 591­598, 2007.
[5] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. J. Mach. Learn. Res., 4, 2003.
[6] J. H. Lee. Combining multiple evidence from different properties of weighting schemes. In SIGIR '95: Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pages 180­188, 1995.
[7] D. Lillis, F. Toolan, R. Collier, and J. Dunnion. Probfuse: a probabilistic approach to data fusion. In SIGIR '06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 139­146, 2006.
[8] T. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In Proceedings of SIGIR 2007 Workshop on Learning to Rank for Information Retrieval, pages 3­10, 2007.
[9] M.-F. Tsai, T.-Y. Liu, T. Qin, H.-H. Chen, and W.-Y. Ma. Frank: a ranking method with fidelity loss. In SIGIR '07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 383­390, 2007.

Difference in AP

-1.0 -0.5

0.0

0.5

1.0

856

VisualSum: An Interactive Multi-Document Summarization System Using Visualization

Yi Zhang, Dingding Wang, Tao Li
School of Computing and Information Sciences Florida International University Miami, FL 33199
{yzhan004, dwang003, taoli}@cs.fiu.edu

ABSTRACT
Given a collection of documents, most of existing multidocument summarization methods automatically generate a static summary for all the users. However, different users may have different opinions on the documents, thus there is a necessity for improving users' interactions in the summarization process. In this paper, we propose an interactive document summarization system using information visualization techniques.
Categories and Subject Descriptors: H.3.3[Information Storage and Retrieval]: Information Search and Retrieval; H.5.2[User Interfaces]: Interaction styles.
General Terms: Algorithms, Experimentation, Performance
Keywords: Multi-Document Summarization, visualization
1. INTRODUCTION
With huge volume of text resources on the Internet, document summarization has been receiving a lot of attentions. Existing document summarization methods usually involve natural language processing and machine learning techniques. However, most of these methods exclude human from the summarization process, which is efficient in terms of reducing users' workload, but is not desired since the generated summaries are identical for all the users, contradicting to the subjective nature of summarization [6].
To address the issue that people with diverse interests may expect dynamic summaries based on their own preference, we develop VisualSum, an interactive visualized document summarization system, to help users select their preferred sentences to form the summaries.
The summarization process of VisualSum is performed in an iterative manner as illustrated in Figure 1. It starts with all the sentences in the documents, and stops when a satisfactory summary is obtained by a user. Each sentence selection iteration includes three steps as follows. Step (1): The system generates a 2-D view graph of current sentences, in which each node represents a sentence, and the location and color of the sentence are determined by the layout and clustering algorithms respectively. Step (2): The user selects a sentence based on the visualization results in Step (1). Step (3): The system removes the sentence clusters of the selected sentences from the current sentence set.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: The diagram of user interactive summarization

Experiments and a user study demonstrate the effectiveness of the VisualSum system.

2. METHODOLOGY
In this section, we introduce the components of VisualSum including sentence graph representation, layout and clustering algorithms, and user interaction function.

2.1 Sentence Graph Representation
Given a collection of documents, we first decompose them into sentences. An undirected graph G = (V, E) is then constructed to represent the relationships among the sentences, where V is the vertex set and E is the edge set. Each vertex in V is a sentence, and each edge in E is associated with the cosine similarity between two sentences (vertices). Two vertices are connected if their cosine similarity is greater than 0.

2.2 Linlog Layout Algorithm

Here, we use Linlog, a popular energy-based layout algo-

rithm[7], to display the sentence relationships. The energy

function in Linlog is



E(p) =

({u,v}pu - pv - dvdulnpu - pv)

{u,v}:u=v

Where {u,v} is the weight of the edge connecting vertices u and v, and du and dv are the degrees of u and v respectively. The optimal positions p of all the vertices are obtained by
minimizing E.

2.3 Clustering with Maximum Modularity

The node (sentence) positions displayed by the energybased layout algorithm are consistent with the clustering results obtained by maximizing graph modularity [2, 7]. Modularity can be defined as

 [ wc cC wC

d(c)2 - ( d(C)2 )]

where wc, wC are the sum of edge weights in cluster c and

857

Figure 2: An example visualization and summarization by VisualSum

cluster set C respectively, and d(c) and d(C) are the sum of node degrees for all the nodes in cluster c and cluster set C.
The clustering results can be easily obtained by a bottomup algorithm, in which each sentence is treated as a singleton cluster at the beginning and then successively merge pairs of clusters until the maximum modularity is reached.
2.4 User Interaction
Now we show how VisualSum assists users to interactively select sentences to create summaries. The visualization in VisualSum clearly illustrates the following information for users. (1) Each node is a sentence and the color of the node indicates the cluster it belongs to. (2) The radius of each node is determined by the sentence's degree. The larger the node, the more important the corresponding sentence. (3) Important sentences in the largest cluster are labeled by their sentence IDs and recommended to users as candidates. (4) Large nodes in the overlapping area of two clusters may be the transition sentences between the clusters. (5) The larger the distance between two clusters, the dissimilar the two topics.
Since the visualization process clearly shows the relationships among the sentences, users can easily select the important sentences they are interested in to form the summary. Figure 2 shows an example of the visualization and sentence selection procedure.
3. EXPERIMENTS
3.1 Automatic Summarization
First of all, we examine the summarization performance of VisualSum using DUC 2006 dataset. Since the DUC evaluation is not personalized, we select the largest sentence node in the largest cluster at each iteration, until the required length of summaries is reached. Table 1 shows the evaluation results using ROUGE toolkit [5] (intuitively, the higher the scores, the better the performance). We compare VisualSum with four widely used baseline summarizers. From Table 1, we observe that the summarization performance of VisualSum outperforms LeadBase and Random and is comparable with NMF and LSA. Note that the motivation of VisualSum is not to build an automatic summarizer, but to

Systems VisualSum LeadBase [1]
Random NMF [4] LSA [3]

R-1 0.332 0.320 0.317 0.324 0.331

R-2 0.055 0.052 0.049 0.055 0.050

R-L 0.308 0.297 0.294 0.300 0.305

R-W 0.113 0.110 0.108 0.113 0.112

R-SU 0.107 0.104 0.101 0.106 0.102

Table 1: Summarization performance comparison.

help users to create their desired summaries using visualization. Thus in this experiment, we just demonstrate the comparable performance of VisualSum for automatic document summarization.
3.2 User Study
To better evaluate the summarization results of VisualSum, we conduct a user survey. The subjects of the survey are fifteen students at different levels and from various majors at Florida International university. Each participant randomly selects a set of news documents, and uses VisualSum to form a summary. Then they are asked to assign a score of 1 (the least satisfaction) to 10 (the highest satisfaction), according to their satisfaction of the use of VisualSum. The average scores of VisualSum and the baseline summarizers are 8.07, 7.5 respectively, which demonstrate the effectiveness of VisualSum.
Acknowledgements: The work is partially supported by NSF grants IIS-0546280 and DMS-0915110.

4. REFERENCES
[1] http://www-nlpir.nist.gov/projects/duc/pubs.html.
[2] G. Agarwal and D. Kempe. Modularity-maximizing graph communities via mathematical programming. The European Physical Journal B, 66(3):409­418, November 2008.
[3] Y. Gong and X. Liu. Generic text summarization using relevance measure and latent semantic analysis. SIGIR, 2001.
[4] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. NIPS, 2001.
[5] C.-Y. Lin and E. Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. Association for Computational Linguistics, pages 71 ­ 78, 2003.
[6] S. Liu, M. X. Zhou, S. Pan, W. Qian, W. Cai, and X. Lian. Interactive, topic-based visual text summarization and analysislysis. CIKM, 2009.
[7] A. Noack. Modularity clustering is force-direced layout. Physical Review E, 79:026102, 2009.

858

Retrieval System Evaluation: Automatic Evaluation versus Incomplete Judgments

Claudia Hauff
University of Twente Enschede, The Netherlands
c.hauff@ewi.utwente.nl

Franciska de Jong
University of Twente Enschede, The Netherlands
f.m.g.dejong@ewi.utwente.nl

ABSTRACT
In information retrieval (IR), research aiming to reduce the cost of retrieval system evaluations has been conducted along two lines: (i) the evaluation of IR systems with reduced (i.e. incomplete) amounts of manual relevance assessments, and (ii) the fully automatic evaluation of IR systems, thus foregoing the need for manual assessments altogether. The proposed methods in both areas are commonly evaluated by comparing their performance estimates for a set of systems to a ground truth (provided for instance by evaluating the set of systems according to mean average precision). In contrast, in this poster we compare an automatic system evaluation approach directly to two evaluations based on incomplete manual relevance assessments. For the particular case of TREC's Million Query track, we show that the automatic evaluation leads to results which are highly correlated to those achieved by approaches relying on incomplete manual judgments.
Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval General Terms: Experimentation, Performance Keywords: Automatic System Evaluation
1. INTRODUCTION
In information retrieval (IR), research aiming to reduce the cost of retrieval system evaluations has been conducted along two lines: (i) the evaluation of IR systems with reduced amounts of manual relevance assessments, and (ii) the fully automatic evaluation of IR systems, thus foregoing the need for manual assessments altogether. The two most important approaches in the first category are the determination of good documents to assess (the MTC approach) [6] and the proposal of alternative pooling methods (the statAP approach) [4]. Both, MTC and statAP, are now accepted system evaluation metrics at TREC1. They stand in contrast to the depth pooling methodology which has until recently been employed at TREC; due to the ever increasing size of test collections and query sets though, pooling the top 100 documents of each retrieval run participating in a benchmark and assessing those documents manually for their relevance, has become infeasible. The earliest method for a fully automatic evaluation was proposed by Soboroff et al.
1http://trec.nist.gov/
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

(the RS approach) [7]. It relies on drawing random samples from the pool of top retrieved documents.
The quality of statAP, MTC and RS is usually evaluated by comparing the performances of a set of retrieval runs for which sufficient relevance judgments are available according to a standard effectiveness metric (mean average precision) with the estimated system performances. Generally missing though is a direct comparison between statAP /MTC and an automatic method such as RS.
In recent work [3], we found the commonly reported problem of automatic evaluation approaches (the severe misranking of the very best retrieval runs [5]) not to be inherent to automatic system evaluation methods. The extent of this problem is strongly related to the degree of human intervention in the best retrieval runs: the larger the amount of human intervention, the less able automatic approaches are to identify the best runs correctly.
In this poster, we turn to investigating how closely the automatic evaluation of retrieval runs approximates the evaluation with incomplete manual relevance assessments. We perform this analysis in a setting which favors automatic evaluation: TREC's Million Query tracks of 2007 (MQ2007) [2] and 2008 (MQ-2008) [1]. Due to the size of the query sets, creating retrieval runs with a great amount of human intervention is virtually impossible. We thus expect the RS approach to lead to similar estimates of system performances as statAP and MTC respectively. If this would indeed be the case, it would bring into question the need for manual assessments in this type of setting.
2. EXPERIMENTS
For our experiments, we relied on the twenty-nine retrieval runs submitted to MQ-2007 and the twenty-four2 runs submitted to MQ-2008. Both sets of retrieval runs as well as their retrieval effectiveness scores according to statAP and MTC are available from the TREC website. Specifically, for MQ-2007, TREC provides the statAP measures3, while for MQ-2008 both, MTC and statAP, are provided. Of the 10000 queries that were released for each year, 1153 (MQ2007) and 564 (MQ-2008) queries respectively have valid statAP measurements; 784 (MQ-2008) queries have valid MTC measurements. These are the queries we also rely on in the RS approach.
2In total, twenty-five runs exist, though one is not accessible from the TREC website and thus had to be ignored. 3The MTC measures are not accessible from the TREC website.

863

Pool Depth p Avg. Sampled Documents

MQ-2007 statAP

10

4.6

50

22.2

100

43.0

250

102.6

MQ-2008 statAP

10

6.0

50

28.8

100

55.7

250

132.4

MQ-2008 MTC

10

6.1

50

29.4

100

56.8

250

135.0

Kendall's Tau
0.803 0.783 0.754 0.719
0.768 0.812 0.833 0.841
0.722 0.759 0.773 0.780

Table 1: Kendall's Tau rank correlation coefficient between the automatic RS approach and statAP/MTC respectively. All correlations are significant (p < 0.01). Column 2 contains the average number of sampled documents from the pool.

For the automatic evaluation, we implemented the random sampling approach [7]: first, the top p retrieved documents of all retrieval runs for a particular query are pooled together such that a document that is retrieved by x runs, appears x times in the pool. Then, a number m of documents are drawn at random from the pool; those are now considered to be the pseudo relevant documents. This process is performed for each query and the subsequent evaluation of each system is performed with pseudo relevance judgments instead of relevance judgments. Due to the randomness of the sampling, we performed 20 trials per query and averaged the pseudo relevance based system performance. We fixed the number m of documents to sample 5% of the number of unique documents in the pool and evaluated pool depths of p = {10, 50, 100, 250}.
In Table 1 (column 3) we report the rank correlation coefficient Kendall's Tau ( ) between the performance scores estimated by the automatic RS approach and the performance scores estimated by statAP /MTC which exploit manual relevance assessments. In the ideal case,  = 1.0, that is, RS leads to the same rank estimate of system performances as statAP /MTC. It is apparent, that although the correlations are not perfect, the correlation coefficients are consistently high; in the worst instance the correlation reaches  = 0.72 for MQ-2007 statAP and a pool depth of p = 250; at best the correlation reaches  = 0.84 for MQ-2008 statAP and p = 250.
Figures 1 and 2 show scatter plots of MQ-2007 statAP scores versus RS scores and of MQ-2008 MTC scores versus RS scores respectively. It is evident that the best retrieval runs as identified by statAP /MTC are also identified correctly by the automatic RS approach.
3. DISCUSSION AND CONCLUSION
In this poster, we investigated the ability of an automatic system evaluation approach (RS [7]) to approximate the system performance estimates as derived by two evaluation methods that rely on manually derived incomplete relevance judgments: statAP and MTC. Experiments on TREC's Mil-

random sampling

0.18 0.16 0.14 0.12
0.1 0.08 0.06 0.04 0.02
0

=0.803

0.05

0.1

0.15

0.2

0.25

0.3

statAP

Figure 1: MQ-2007 statAP scores (x-axis) versus RS scores (y-axis) for a pool depth of p = 10.

random sampling

0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01
0 0

=0.780

0.02

0.04

0.06

0.08

0.1

MTC

Figure 2: MQ-2008 MTC scores (x-axis) versus RS scores (y-axis) for a pool depth of p = 250.

lion Query tracks showed that RS is highly correlated to statAP and MTC, an outcome which implies that retrieval runs, which are automatic in nature, can be evaluated by an automatic approach such as RS which requires no manual assessments at all.
One direction of future work will be the adaptation of RS to further improve the method's correlation with statAP and MTC by for instance taking advantage of the relationship between queries of a query set (as is possible for larger sets of queries) in contrast to the current approach where each query is viewed in isolation.
4. REFERENCES
[1] J. Allan, J. A. Aslam, V. Pavlu, E. Kanoulas, and B. Carterette. Million Query Track 2008 Overview. In TREC 2008, 2008.
[2] J. Allan, B. Carterette, B. Dachev, J. A. Aslam, V. Pavlu, and E. Kanoulas. Million Query Track 2007 Overview. In TREC 2007, 2007.
[3] C. Hauff, D. Hiemstra, L. Azzopardi, and F. de Jong. A Case for Automatic System Evaluation. In ECIR '10, pages 153­165, 2010.
[4] J. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In SIGIR '06, pages 541­548, 2006.
[5] J. A. Aslam and R. Savell. On the effectiveness of evaluating retrieval systems in the absence of relevance judgments. In SIGIR '03, pages 361­362, 2003.
[6] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In SIGIR '06, pages 268­275, 2006.
[7] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In SIGIR '01, pages 66­73, 2001.

864

Aspect Presence Verification Conditional on Other
Aspects
Dmitri Roussinov
University of Strathclyde 16 Richmond Street, Glasgow, UK G1 1XQ
+44 141 548 3706
dmitri.roussinov@cis.strath.ac.uk

ABSTRACT
I have shown that the presence of difficult query aspects that are revealed only implicitly (e.g. exploration, opposition, achievements, cooperation, risks) can be improved by taking advantage of the known presence of other, easier to verify query aspects. The approach proceeds by mining a large external corpus and results in substantial improvements in re-ranking the subset of the top retrieved documents.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models; ­ Retrieval Models
General Terms
Algorithms, Experimentation, Theory.
Keywords
Information retrieval, machine learning, external corpus.
1. INTRODUCTION
It has been noticed that a common reason for the search results to be of poor quality is missing one or more aspects of the user information need [1], where an aspect can be represented by a subset of query words. E.g., in the query antarctica exploration, the word antarctica is relatively rare and specific. Along with its same-stem variants, it explicitly occurs in virtually all documents about Antarctica. On the other side, the word exploration, capturing the other aspect of the query, is not only more frequent, but also represents a higher level concept (theme, topic, etc.), often revealing itself only implicitly by such statements like "... scientists began to collect data..." The ranking of the retrieved documents by currently state-of-the-art algorithms would be primarily determined by antarctica, mildly affected by the explicit occurrence of the word exploration, and not affected by the implicit presence of the exploration theme. Although techniques aimed at detecting and representing missing aspects have been suggested [2][3] they have not been yet methodologically studied or revealed convincing improvements. I suggest that it was because they were essentially limited to the bag-of-words representations and the query expansion models. On the contrary, this research builds on the prior works studying the prediction of occurrence of given words (concepts) in a given text context (e.g. a sentence) such as [5][7]. Specifically, I proceeded by the exploring the following innovations: 1) Going beyond the bag of words by looking for the indicators of implicit aspect presence among all the sequences of words
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

(up to a certain length) from the document (e.g. polar station, oil drilling proposed, expedition ships, etc.) rather than relying on a finite number of expansion terms. 2) Considering the problem of aspect verification conditional on the presence of other, often easier to establish aspects. E.g., in a document about Antarctica, station is a good indication of the exploration theme, while not necessary so in unrestricted context. 3) Modeling subsumption of indicators, e.g. if the word station is a part of train station then its connection to the exploration theme is weaker.
2. FRAMEWORK
Here, I considered the simplest, but a common situation with the two aspects in a query, each can be specified by one or more keywords: 1) Ap is already known to be present in the document d and 2) the other aspect Am, explicit representation of which is missing in the document, thus simply referred below as the "missing" aspect. While Am is typically a difficult aspect to verify, as I have demonstrated here, the task of estimating the presence of Am conditional on the presence of Ap happens to be easier. As the possible indicators of the aspect implicit presence, I considered all the sequences of words (up to length of 3) in a document. For each indicator i, the algorithm estimated P(Am|i,Ap) , the probability of the occurrence of Am within the proximity (e.g. same sentence) of i, conditional on the occurrence of Ap. In order to do that, a regression (M5P model tree [6]) was trained independently for each topic, using the normalized frequency counts obtained from Microsoft's Bing search engine as independent variables. For example, the high ratio #((station NEAR exploration) AND Antarctica)/#(station AND A ntarctica) would indicate the high probability of occurrence of exploration near the word station in the external corpus, conditional on the presence of Antarctica. The snippets returned by Bing for the query representing both aspects (e.g. antarctica NEAR exploration) served as training examples, thus no parameter tuning was necessary. The advantage of using M5P was learning automatically the reliability thresholds for the frequency counts. The subsumption step of the algorithm discarded all the subsequences (e.g. station) within the word sequences (e.g. train station) for which the reliable estimates of the conditional probabilities were obtained. This resulted in the average of 531 preserved indicators per document, with 90% of the strongest 100 indicators having 2 or more words, some of those presented in Table 2 below. To rank the documents, the sum of all the conditional probability estimates over all the remaining indicators was treated as the total number of "implied" occurrences of the missing aspect:

865

 tf m =

P ( Am | i , Ap ) , and the bm25 formula was

i d

applied.

3. EMPIRICAL EVALUATION

B1

B2

Entire Wikipedi

Web

a only

MAP

0.39

0.43

0.65**

.53*

Standard

0.13

0.11

0.16

.14

Deviation

% change -9% n/a

+51%

+23%

over B2

Table 1. The results of the evaluation. Statistically significant differences at the levels of 0.05 and .1 are marked with ** and * accordingly.

Since the approaches studied here involved a large number of time-consuming external queries, I built a small but "clean" data set that was sensitive enough to evaluate various configurations and the overall verification accuracy of the techniques suggested here based on the top ranked documents retrieved by bm25 ranking function (described as B1 below) using the HARD 2005 TREC topics, which are known to be difficult and often result in missing aspects [1]. I chose only those topics in which 1) It was possible to interpret the title as consisting of missing and present aspects. 2) The present aspect was occurring in at least 90% of the top 20 documents. 3) The missing aspect was not explicitly occurring in more than half of the top 20 documents. This left me with 18 topics, some examples of them listed in Table 2. Only up to 20 top ranked irrelevant and relevant documents were selected, and only those that did not have the missing aspect mentioned explicitly. I also removed (approx. 10% of) the documents assessed by TREC as irrelevant but still having both aspects present in order to reduce the sensitivity of the evaluation to the details supplied only in the narratives of the topics and thus not available to neither the baseline nor the suggested algorithms. The resulting data set was almost "balanced" with approximately 15 negative and 10 positive examples on average per each topic. I compared the performance against two strong baselines: B1 was obtained by applying bm25 ranking formula using the topic titles only, with

stemming and pseudo-relevance feedback using Lemur retrieval engine default settings. Since the approach here involves an external corpus, to make a fair comparison, I also involved B2 obtained and optimized the same way as in [4], which was essentially an application of a relevance model with an external corpus (here, using Bing's snippets).
4. CONCLUSIONS
As the results in Table 1 indicate, it was possible to significantly improve verifying the presence of the difficult implicit aspects by the techniques suggested here. Also, no specific topic has been harmed as a result. While the actual impact on a larger set of topics remains to be seen, I have nevertheless been able to handle a very common scenario with the implicit presence of a difficult aspect (e.g. exploration) and the explicit presence of an easier aspect (e.g. Antarctica). Although the dataset involved in experiments here was very small, it still represented the top ranked results from the state of the art techniques, and thus being able to improve them by re-ranking has great practical implications. The ablation studies (not reported here due to the space limitations) confirmed that each of the novelties 1-4 stated in the introduction was crucial. While sending queries to a search portal delayed the processing substantially, in order to achieve the real time speeds, future implementations can make use of the processed large corpus data such as Google's 1T or a faster access to a search engine index.
5. REFERENCES
[1] Buckley, C. Why current IR engines fail. SIGIR 2004. [2] Collins-Thompson, K., Callan, J. Query expansion using
random walk models. CIKM 2005. [3] Crabtree, D.W., Andreae, P. Gao, X. Exploiting
underrepresented query aspects for automatic query expansion. SIGKDD 2007. [4] Diaz, F. and Metzler, D. Improving the estimation of relevance models using large external corpora. SIGIR 2006. [5] Edmonds, P. Choosing the word most typical in context using a lexical co-occurrence network, ACL 1997. [6] Monz, C. Model Tree Learning for Query Term Weighting in Question Answering. Advances in Information Retrieval, Volume 4425/2007, pp. 589-596. [7] SzeWang F., Roussinov, D., Skillicorn, D.B. Detecting Word Substitutions in Text. IEEE TKDE, 2008.

Table 2. Top indicators of implicit aspect presence for some topics. The missing aspects are underlined. Bold font highlights the

indicators that are specific to the present aspect.

Topic

Strongest Indicators of Presence

Transportation train caught fire, people were killed, caused by sparks, the midst of, described the crash, in spite

Tunnel

of, millions of dollars, quickly as possible, billions of dollars, around the corner, was caused by,

Disasters

turned into a, the worst, rescue, emergency, explosion, coal, killed, fire

Black Bear

were forced to, bear attempted to, fire, the bear was, officials say, reduced, kill, defense,

Attacks

occurred in, officials said, carcass, attract, documented, bear was sighted, dangerous, killed

by, ripped, threatening

Iran Iraq

Iran will ship, minister to visit, delegation will visit, positive, visit to Iraq, oil exports, Iranian

Cooperation delegation, normalizing, relations, the joint, agreement with Iran, mutual, to bilateral, visit to Iran,

accepted an invitation, invitation to visit, normalization, war ended in, the withdrawal of, gesture

Journalist Risks journalist was shot, press freedom, murdered, circumstances, in prison, Iraq, Russia, body, killer,

gunned down, win, posed, detained

866

The Value of Visual Elements in Web Search

Marilyn Ostergren, Seung-yon Yu, Efthimis N. Efthimiadis
The Information School University of Washington, Box 352840
Seattle, WA 98195-2840 {ostergrn, syyu, efthimis}@uw.edu

ABSTRACT
We used eye-tracking equipment to observe 36 participants as they performed three search tasks using three graphicallyenhanced web search interfaces (Kartoo, SearchMe and Viewzi). In this poster we describe findings of the study focusing on how the presentation of SERP results influences how the user scans and attends to the results, and the user satisfaction with these search engines.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search process H.5.2 [Information interfaces and presentation]: User Interfaces. - Graphical user interfaces.
General Terms
Design, Human Factors.
Keywords
Search Engine Results Page Display (SERP), Eye-tracking study, Search Engine Evaluation, User Study
1. INTRODUCTION
The results of a web search are generally presented as a collection of web-page surrogates. Each surrogate conveys information that

can be used to support the decision `should I follow this link?' These surrogates, along with other information on the page may also support the decision to reformulate the search query ­ either because the results don't seem relevant, or because they trigger ideas that alter the searcher's target or conceptualization of the information need [4].
Our study investigates how searchers interact with graphical, nontextual search engine results page user interfaces (SERP UIs) to reveal the potential value of these alternative display strategies. We study whether the unique characteristics of these displays facilitate the work of scanning the page for the clues that support the decision to follow a link or reformulate a query.
2. REVIEW OF RESEARCH
Others have also used eye-tracking to gain an understanding of what happens during search. A major finding of this work is that, when results are presented as a ranked list, users direct most of their attention to results near the top of the list [2,5]. This is true even if the list is manipulated so that more relevant results appear lower on the list [5], and even if the eye-tracking data shows that the viewer looked at those more relevant results [2]. Cutrell & Guan [1] found that this bias toward earlier results can also be affected by the content of the surrogates ­ in this case, by the length of the text snippet.

Figure 1: Screenshots of ViewZi, SearchMe, and KartOO

Figure 2: Hot spots for ViewZi, SearchMe, and KartOO

Copyright is held by the author/owner(s).
SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Some other observations revealed by eye-tracking that are relevant to our work are that placement and proximity of image search results affects a viewers gaze path [6], and that an organized display with an explicit hierarchy reinforced with

867

headings and indentations facilitates a more efficient visual search [3].
3. METHODOLOGY
To gather evidence for how graphical qualities of a SERP support or hinder a user, we collected and analyzed two sources of data: 1. self-report (both audio recordings of verbalizations during search and responses to a questionnaire), and 2. observation (cursor and gaze behavior). We collected this data while users completed a series of search tasks using three graphically-enhanced search interfaces. The interfaces were Kartoo (kartoo.com), SearchMe (searchme.com) and Viewzi (viewzi.com). We chose these three because they display the results using graphical representation rather than the typical text-based ranked list. They also have enough similarities to each other to allow for meaningful comparisons.
3.1 Participants
We recruited 36 participants, all undergraduate students at a major research university. They were divided into two groups: 18 were trained to search the three SE, 18 were untrained.
3.2 Search Tasks
We chose search tasks which require different types of cognitive processing and navigation to elicit a range of search behaviors. Task 1 asked the user to find the schedule of events at a local performance theater. Task 2 asked to find two sites where a specific camera model could be purchased. Task 3 asked the user to find two credible sites describing the side-effects of aspirin.
3.3 Data Collection
Data was collected using questionnaires, the think-aloud process, eye-tracking, transaction logging, and participant observation. We used the Tobii eye-tracking system and ClearView 2.7.1 software to record and analyze the eye-tracking data. After each search, we asked the users to reflect upon how the interface affected their searching. After the entire set of searches, we asked them to compare the interfaces with each other.
4. DISCUSSION
Each of these three SERP interfaces uses an arrangement of page surrogates which differs from the usual top-to-bottom linear display. Viewzi (upper left in Figure ) uses a grid. SearchMe (central image in Figure ) uses a horizontal display in which only one surrogate (the central one) is clearly displayed at a time, while the previous and subsequent surrogates are smaller and displayed as if in a stack set at an angle to the central surrogate. Kartoo (right in Figure ) spreads the surrogates across the page organized by major topic area and resembles a map. The three displays also differ in the content of each surrogate. All three include a thumbnail image of the site though they vary substantially in size (Figure ). All three include some text ­ a partial URL and keywords in Kartoo, a slightly-abbreviated Google-derived snippet in Viewzi, and a snippet plus access to the full page (using a magnifying window) in SearchMe. We can easily see differences in scanning behavior across the three interfaces. This is evident in the heat-map images in Figure which show the combined gaze data from 18 participants. Our analysis indicates that the Kartoo display elicited the most scanning. Participants generally spent time looking at all 10 surrogates before clicking to go to a page. The SearchMe Display elicited careful analysis of individual surrogates, but inhibited scanning beyond the first few results. Though it is possible to

scan sequentially through the entire result set using the slider control that is located at the bottom of the screen, none of our participants took advantage of this feature choosing instead to analyze each carefully in sequence. The Viewzi display facilitates scanning in a way that is similar to the typical ranked list. The results are displayed from left to right and continue in rows from top to bottom. The most attention is given to the surrogate in the upper-left hand corner with correspondingly less attention to the surrogates to the right and bottom. However we did find that this strong preference for the surrogates near the beginning of the sequence is less pronounced than that reported by Guan & Cutrell with the typical ranked list display [2]. The question remains how much of this scanning behavior was influenced by the arrangement of the surrogates and how much was influenced by the content of the surrogates. The fact that users viewed more surrogates using the Kartoo display appears to be mostly because the surrogates provide fewer textual cues as to the content of the target website. The results of our questionnaires indicate that Kartoo was the least popular interface while SearchMe, which heavily favored the first site in the list and elicited very little browsing, was the most popular. In terms of ease of interaction on a scale of 1-9, where 1 is most difficult, 5 is average, and 9 is least difficult, 30% found Kartoo most difficult (1-2), 70% found SearchMe least difficult (7-9), and 42% found Viewzi average.
5. CONCLUDING REMARKS
The major insight from our initial analysis is that the visual, nonlinear qualities of these SERP displays strongly influence the user interaction, in particular, the number and sequence in which the surrogates are explored. Satisfaction with the SERP displays is correlated more closely with the textual content of the surrogates. Our more detailed analysis will look at what elements of the surrogate were examined (e.g. the URL, the text snippet, the screenshot) and compare these results with a similar data for a standard text-based search result display. Familiarity to SE and knowledge about the functionality of the SE affected user satisfaction. Training how to search using the visual search engines enhanced the user's search effectiveness, e.g., less number of query reformulations, more efficient search by using search features, and better user satisfaction. When asked if they intend to use the visual search engine again, more trained users answered positively than non-trained users. The most popular search engine that people want to use again was SearchMe (72%), followed by Viewzi (36%), and then Kartoo (33%).
6. REFERENCES
[1] E. Cutrell & Z. Guan. What are you looking for?: An eyetracking study of information usage in Web search. CHI '07. ACM, New York, NY, (2007), 407-416.
[2] Guan, Z., & Cutrell, E. An eye tracking study of the effect of target rank on Web search. In CHI 2007, ACM (2007).
[3] Hornof, A.J., & Halverson, T. Cognitive strategies and eye movements for searching hierarchical computer displays. In Proceedings of CHI 2003, ACM Press (2003), 249-256.
[4] Johnson, F C. User interactions with results summaries. In Proceedings of the ACM SIGIR 2007 workshop on web information seeking and interaction (2007), 131-134.
[5] Lorigo, L., et al. 2008. Eye tracking and online search: Lessons learned and challenges ahead. J. Am. Soc. Inf. Sci. Technol. 59, 7 (May. 2008), 1041-1052.
[6] Tseng, Y.C., Howes, A. The Adaptation of Visual Search Strategy to Expected Information Gain. CHI 2008, 1075-84.

868

Diversification of Search Results using Webgraphs
Praveen Chandar and Ben Carterette {pcr,carteret}@udel.edu
Department of Computer and Information Sciences University of Delaware
Newark, DE, USA 19716

ABSTRACT
A set of words is often insufficient to express a user's information need. In order to account for various information needs associated with a query, diversification seems to be a reasonable strategy. By diversifying the result set, we increase the probability of results being relevant to the user's information needs when the given query is ambiguous. A diverse result set must contain a set of documents that cover various subtopics for a given query. We propose a graph based method which exploits the link structure of the web to return a ranked list that provides complete coverage for a query. Our method not only provides diversity to the results set, but also avoids excessive redundancy. Moreover, the probability of relevance of a document is conditioned on the documents that appear before it in the result list. We show the effectiveness of our method by comparing it with a query-likelihood model as the baseline.
Categories and Subject Descriptors: H.3.3 [Informa-
tion Storage and Retrieval]
General Terms: Algorithms
Keywords: information retrieval, diversity, webgraphs.
1. INTRODUCTION
Users express information needs using a set of keywords. Current retrieval systems fail to capture the different information needs that could be expressed by users using the same set of keywords. Clearly this leads to multiple interpretations for a given query. For example, consider the query kcs. There are multiple interpretations for this query, one being the Kansas City Southern railroad; another, being Kanawha County Schools in West Virginia; one more interpretation is information on KCS Energy, Inc.
In order to maximize the user experience it appears reasonable to diversify the result set. Diversify means to examine the query with a broader perspective and account for the multiple information needs for the query. This diversification would provide complete coverage of subtopics for a given query to the user. Ranking with diversity requires moving away from the assumption that documents are independently relevant to the query. Each document must be ranked based not just on its similarity to the query but also based on the documents retrieved before it.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Our task is same as the "diversity" task of the TREC Web Track [4]; the goal of the system is to return a ranked list of documents that provides complete coverage for a given topic, while avoiding excessive redundancy in the result set. We have used the topics created for this task. These topics consists of a query, a description of an information need, and one or more subtopics or alternative interpretations of the query. These topics were developed from information extracted from the logs of a commercial Web search engine, thereby ensuring a good mix of user needs for a given query.
Most previous work, including the MMR approach of Carbonell & Goldstein [2] and the language modeling framework proposed by Zhai et al. [7], involve a greedy approach to finding subtopics. In this work, we propose a method using the link structure of the web to maximize the subtopics covered for a given query. Our method identifies authoritative documents in a set and assumes that these authoritative documents represent a subtopic. We evaluate our proposed method using the -nDGC measure proposed by Clarke [5] and intent aware precision (P-IA) proposed by Agarwal et al [1] and compare it to a query-likelihood baseline.
2. THE WEBGRAPH METHOD
The link structure has often provided a rich source of information about the content of the environment. Our method uses the information provided by the link structure to find several densely linked collections of hubs and authorities within a subset of the results. Each densely linked collection could potentially cover different subtopics for a given query.
In our approach, we re-rank an initial ranking of documents (query-likelihood results) to provide a diverse ranking of documents. The documents in this initial ranking consisting of hyperlinked pages are represented as a directed graph G = (V, E): nodes corresponds to pages and a directed edge (p, q)  E correspond to the presence of link from page p to q. We expand the subgraph to include all the in-links to the subgraph and out-links from the subgraph. The hubs and authorities scores are calculated for each document using the iterative procedure described by Kleinberg [6].
Kleinberg's procedure begins by representing the directed graph as an adjacency matrix. The principal and non-principal eigenvectors are calculated from this matrix multiplied by its transpose. Each value in an eigenvector represents a document score. The values in the principal eigenvector correspond to the Kleinberg's hub score for a document. The nonprincipal eigenvectors represent other densely-linked clusters in the graph; they have both positive and negative entries, but we consider only the positive entries.

869

No. of Eigenvectors
5 10 25 50 100 0 (baseline)

-nDCG10
0.100 0.154 0.143 0.169 0.142 0.124

P-IA10
0.038 0.050 0.051 0.057 0.047 0.061

Table 1: Diversity results for varied number of eigenvectors and 50 terms.

For each eigenvector we construct a language model using the documents corresponding to the k greatest values. Therefore, the m language models constructed from the documents correspond to the k greatest values in each of the first m eigenvectors. The intuition is that the link structure clusters the documents into subtopics, therefore these language models provide a hypothetical set of subtopic models. The language model corresponding to each subtopic is evaluated against the query and then we take the document with the greatest score. This produces a set of documents (possibly fewer than m) which are the highest scoring for the hypothesized subtopics that are then ranked in decreasing order of the original query-likelihood scores. We iterate in this way, taking the highest-scoring set of documents remaining, until we rank the top 200 documents in the original ranking. This method of iterating to obtain the final ranking is similar to the one described by Cartertte et al [3].

3. IMPLEMENTATION AND RESULTS
In our experiments, we used the ClueWeb09 dataset consisting of one billion web pages (5 TB compressed, 25 TB uncompressed), in ten languages, crawled in January and February 2009. We indexed the smaller set of "Category B" which consists of 50 million web pages in English. We used the webgraphs in the dataset which has about 428,136,613 unique URLs and 454,075,638 outlinks. This test collection was used for the diversity task at TREC'09. A total of 50 queries were evaluated and the subtopics for each query ranged from 3 to 8. We used the Lemur Toolkit and the Indri search engine in our experiment. The query-likelihood result set with Dirichlet smoothing (µ = 2000) was used as our baseline results for reranking.
Our method was evaluated using the two measures which reward novelty and diversity, namely -normalized discounted cumulative gain (-nDCG) and intent-aware precision (PIA). All our methods were evaluated at rank 10 with  = 0.5 in -nDCG. To see whether the setting of parameters such as m (the number of eigenvectors) and n (number of terms) may affect the performance, we compare the results for a range of values.
By comparing the results of the two parameters in Figure 1 we see that in general the performance increases and reaches a maximum at 50 eigenvectors and starts to decrease again. The number of terms in the model has less effect on the results. We report the diversity results by varying the number of eigenvectors along the Indri baseline model in Table 1. This table shows that our method did considerably well in diversifying the results set for all parameter values according to the -nDCG measure although for the P-IA measure the results were below the baseline.

alpha-nDCG Values

0.2 0.18 0.16 0.14 0.12 0.1 0.08 0.06
5

n = 5 n = 10 n = 25 n = 50 n = 100 baseline

10

25

50

100

Number of Eigenvectors

Figure 1: -nDCG averaged over 50 queries with increasing numbers of eigenvectors (subtopic models) and terms in each model.

4. CONCLUSIONS AND FUTURE WORK
In this work, we have proposed a novel method for diversifying search results. The webgraph method produces a diverse ranking from an initial set of documents for a given query by considering the underlying link structure of the retrieved documents. We believe more information can be harnessed from the hyperlink structure of retrieved documents; our work provides enough evidence for future work along these lines.
5. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of WSDM '09, pages 5­14, 2009.
[2] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR '98, pages 335­336, 1998.
[3] B. Carterette and P. Chandar. Probabilistic models of ranking novel documents for faceted topic retrieval. In Proceeding of CIKM '09, pages 1287­1296, 2009.
[4] C. L. Clarke, N. Craswell, and I. Soboroff. Overview of the trec 2009 web track. In Proceedings of TREC, 2009.
[5] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of SIGIR '08, pages 659­666, 2008.
[6] J. M. Kleinberg. Authoritative sources in a hyperlinked environment. J. ACM, 46(5):604­632, 1999.
[7] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proceedings of SIGIR '03, pages 10­17, 2003.

870

Capturing Page Freshness for Web Search
Na Dai and Brian D. Davison
Department of Computer Science & Engineering Lehigh University
Bethlehem, PA 18015 USA
{nad207,davison}@cse.lehigh.edu

ABSTRACT
Freshness has been increasingly realized by commercial search engines as an important criteria for measuring the quality of search results. However, most information retrieval methods focus on the relevance of page content to given queries without considering the recency issue. In this work, we mine page freshness from web user maintenance activities and incorporate this feature into web search. We first quantify how fresh the web is over time from two distinct perspectives--the page itself and its in-linked pages--and then exploit a temporal correlation between two types of freshness measures to quantify the confidence of page freshness. Results demonstrate page freshness can be better quantified when combining with temporal freshness correlation. Experiments on a realworld archival web corpus show that incorporating the combined page freshness into the searching process can improve ranking performance significantly on both relevance and freshness.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Performance
Keywords: temporal correlation, web freshness, web search
1 Introduction
Web search engines exploit a variety of evidence in ranking web pages to satisfy users' information needs as expressed by the submitted queries. These information needs may contain distinct implicit demands, such as relevance and diversity. Recency is another such need, and so is utilized as an important criteria in the measurement of search quality. However, most information retrieval methods only match queries based on lexical similarity. Link-based ranking algorithms such as PageRank [1] typically favor old pages since the authority scores are estimated based on a static web structure and old pages have more time to attract in-links.
To overcome this problem, we quantify page freshness from web activities over time. We observe that pages and links may have diverse update activity distributions from inception to deletion time points. We infer that pages having similar activity distributions with their in-links suggest that such page activities have stronger influence on their parents' activities.
Motivated by the above analysis, in this work we incorporate a temporal freshness correlation (TFC) component in quantifying page freshness, and show that by using TFC, we can achieve a good estimate of how up-to-date the page tends to be, which is helpful to improve search quality in terms of both result freshness and rel-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Link activity
1 creation of link l : q  p 2 update on link l : q  p (changed anchor) 3 update on link l : q  p (unchanged anch.) 4 removal of link l : q  p
Page activity
1 creation of page q 2 update on page q 3 removal of page q

Infl. on p's InF
   
Infl. on q's PF
  

Gain of p's InF
3 2 1.5 -0.5 Gain of q's PF 3 1.5 -0.5

Table 1: Activities on pages and links and their influence on web freshness. (The link l points from page q to page p. : positive influence on web freshness. : negative influence on web freshness. The number of  or 
indicates the magnitude.)

evance. We consider the effects of other aspects of freshness on retrieval quality elsewhere [4].

2 Page Freshness Estimation

We start by quantifying web freshness over time. We assign every
page two types of freshness: (1) page freshness (PF) inferred from
the activities on the page itself; and (2) in-link freshness (InF) in-
ferred from the activities of in-links. Table 1 lists the detailed web activities and their contributions1 to page and in-link freshness. To
simplify analysis, we break the time axis into discrete time points (t0, t1, . . . , ti, . . .) with a unit time interval t = ti - ti-1, where i > 0. It is reasonable to assume that any activities that occur in [ti-1, ti] can be considered as occurring at ti, especially when t is small. We assume that the influence of activity decays exponentially over time. Therefore, we estimate PF and InF at ti by aggregating the web activities with such a decay, written as:

X ti

X

P Fti (p) =

e(i-j)t

wk Ctj ,k (p)

tj =1

kP A

X ti

XX

InFti (p) =

e(i-j)t

wk Ctj ,k (l)

tj =1

l:qp kLA

where wk and wk are contributions associated with each type of page and link activities, and Ctj,k(p) is the number of the kth type of page activity on page p at tj , and Ctj,k(l) is the number of the kth type of page activity on link l at tj, and P A and LA are
the page and link activity sets. In this way, we estimate web page
freshness at multiple predefined time points from web activities.

1The sensitivity of activity weights with respect to freshness estimation is omitted due to space limitation.

871

We next quantify the temporal freshness correlation between
pages and their in-links. We exploit the method by Chien and Im-
morlica [3], in which the authors measure query semantic similarity
by using temporal correlation. Given a page p, its page and in-link freshness are denoted as (P Ftc (p), P Ftc+1 (p), . . . , P Ftr (p)) and (InFtc (p), InFtc+1 (p), . . . , InFtr (p)) covering p's life span. The temporal freshness correlation (TFC) between page p and its
in-links is given by:

T F C(p)

=

1 n

X tr " P Ft(p) - P F (p) "" InFt(p) - InF (p) "

t=tc

P F (p)

InF (p)

where P F (p) and InF (p) are the standard deviations of P F (p) and InF (p), respectively.
Once we calculate the temporal freshness correlation for every page (tr - tc  2t), we next combine it with page freshness score by ranks. Given a time point of interest ti, the combined page freshness rank of document d is written as:

Rankcombined(d) = (1 - )RankP Fti (d) + RankT F C (d)

where 

=

a-1 n-1+a-1

,

and

n is the total number

of time points, and

a is the number of time points on which p exists. As a increases,

T F C(d) is more stable, and therefore we emphasize its contribu-

tion in the combined page freshness estimation.

3 Experimental Results and Discussion

Our goal is to improve web search quality on both relevance and freshness. To test the effect of combined page freshness on web search, we use an archival corpus of the .ie domain provided by the Internet Archive [5], covering from Jan. 2000 to Dec. 2007, and extract page and link activities. To minimize the influence of transient pages, we remove pages with fewer than 5 archival snapshots. The remaining sub-collection (with 3.8M unique URLs and 908M temporal links) is used for ranking evaluation.
We choose April 2007 as our time point of interest. 90 queries are selected from popular queries in Google Trends2 for evaluation. For each query, we have an average of 84.6 URLs labeled by at least one worker of Amazon Mechanical Turk3. Editors give judgments on each document with respect to a given query for both relevance and freshness. Relevance is judged from "highly relevant" (4) to "not related" (0). Freshness is judged from "very fresh" (4) to "very stale" (0). The document with an average score above 2.5 is marked as relevant/fresh.
To evaluate the effectiveness of the combined page freshness, we compare with PageRank, running on a single web snapshot of April 2007. The global ranking lists generated by the combined page freshness and PageRank scores are linearly combined with Okapi BM2500 [6] (baseline) by ranks individually. The parameters are the same as Cai et al. [2]. Precision@k and NDCG@k are used as metrics for ranking evaluation on both relevance and freshness. All methods are compared based on their best rank combination of query-specific scores and global scores on metric Precision@10 of relevance. The decay parameter  is set to 1 in this work.
Table 2 lists the ranking performance comparison varying the time span involved in the combined page freshness computation. For relevance, except for NDCG@3, the correlation between ranking performance and the time span is not consistent. Unlike relevance, freshness performance consistently improves with the increase of time span used in the combined page freshness computation. This suggests temporal freshness correlation calculated from

2http://www.google.com/trends 3http://www.mturk.com

NDCG@3 NDCG@3

Method Okapi BM2500
PageRank 200601-200704 200401-200704 200201-200704 200001-200704
Method Okapi BM2500
PageRank 200601-200704 200401-200704 200201-200704 200001-200704

P@10 0.4695 0.4894 0.5021 0.4893 0.5002 0.4986
P@10 0.3138 0.3325 0.3288 0.3342 0.3361 0.3374

Relevance NDCG@3
0.2478 0.2589
0.2917 0.3027 0.3081 0.3115 Freshness NDCG@3 0.2137 0.1946
0.2315 0.2329 0.2416 0.2477

NDCG@5 0.2740 0.2840 0.3152 0.3201 0.3157 0.3211
NDCG@5 0.2379 0.2345 0.2490 0.2552 0.2565 0.2617

NDCG@10 0.3344 0.3457 0.3675 0.3657 0.3642 0.3647
NDCG@10 0.2805 0.2838 0.2979 0.2988 0.3027 0.3028

Table 2: Ranking performance comparison. A  means the performance improvement is statistically significant (p-value<0.1) over Okapi BM2500. Performance improvement with p-value<0.05 is marked as .

0.32

combined page freshness

0.315

page freshness

temporal correlation

0.31

0.305

0.3

0.295

0.29

0.285

0.28

0.275 200601-

200501- 200401- 200301- 200201-
time span
(a) relevance

200101-

200001-

0.25

combined page freshness

0.245

page freshness

temporal correlation

0.24

0.235

0.23

0.225

0.22

0.215 200601-

200501- 200401- 200301- 200201-
time span
(b) freshness

200101-

200001-

Figure 1: Ranking performance on metric NDCG@3 while varying the time span involved in page freshness calculation.

long-term web freshness measures can benefit more on accurate page freshness estimation. Figure 1 shows the performance on NDCG@3 with the variance of the time span for both relevance and freshness. We observe that (1) the ranking performance of page freshness first decreases, and then keeps nearly constant with the increase of time span, indicating the page activities within the past 1-2 years influence page freshness estimation the most; (2) the ranking performance of temporal freshness correlation shows unstable trends with variance of time span; and (3) the combined page freshness shows promising performance, and demonstrates its superiority over either page freshness or TFC.
Acknowledgments
This work was supported in part by a grant from the National Science Foundation under award IIS-0803605 and an equipment grant from Sun Microsystems. We also thank Anlei Dong for helpful comments on the ranking evaluation criteria issue.
4 References
[1] S. Brin and L. Page. The anatomy of a large-scale hypertextual Web search engine. In Proc. of 7th Int'l World Wide Web Conf., pages 107­117, Apr. 1998.
[2] D. Cai, X. He, J. Wen and W. Ma. Block-level link analysis. In Proc. 27th Annual Int'l ACM SIGIR Conf., pages 440­447, Jul, 2004.
[3] S. Chien and N. Immorlica. Semantic similarity between search engine queries using temporal correlation. In Proc. 14th Int'l World Wide Web Conf., pages 2­11, 2005.
[4] N. Dai and B. D. Davison. Freshness Matters: In Flowers, Food, and Web Authority. In Proc. of 33rd Annual Int'l ACM SIGIR Conf., Jul, 2010.
[5] The Internet Archive, 2010. http://www.archive.org/. [6] S. E. Robertson. Overview of the OKAPI projects. Journal of Documentation,
53:3­7, 1997.

872

S-PLSA+: Adaptive Sentiment Analysis with Application to Sales Performance Prediction

Yang Liu§,, Xiaohui Yu,§, Xiangji Huang, and Aijun An
§School of Computer Science and Technology, Shandong University Jinan, Shandong, China, 250101
York University, Toronto, ON, Canada, M3J 1P3
yliu@sdu.edu.cn,xhyu@yorku.ca,jhuang@yorku.ca,aan@cse.yorku.ca

ABSTRACT
Analyzing the large volume of online reviews would produce useful knowledge that could be of economic values to vendors and other interested parties. In particular, the sentiments expressed in the online reviews have been shown to be strongly correlated with the sales performance of products. In this paper, we present an adaptive sentiment analysis model called S-PLSA+, which aims to capture the hidden sentiment factors in the reviews with the capability to be incrementally updated as more data become available. We show how S-PLSA+can be applied to sales performance prediction using an ARSA model developed in previous literature. A case study is conducted in the movie domain, and results from preliminary experiments confirm the effectiveness of the proposed model.
Categories and Subject Descriptors
H.4.0 [Information Systems Applications]: General
General Terms
Algorithm, Experiment
Keywords
sentiment analysis, review mining, prediction
1. INTRODUCTION
Online reviews present a wealth of information on products and services, and if properly utilized, can provide vendors highly valuable intelligence to facilitate the improvement of their business. As such, a growing number of recent studies have focused on the economic values of reviews, exploring the relationship between the sales performance of products and their reviews [3, 2, 4]. Gruhl et al. [3] show that the volume of relevant postings can help predict the sales rank of books on Amazon, especially the spikes in sales ranks. Ghose et al. [2] also demonstrate that subjectivity of reviews can have an impact on sales performance.
Liu et al. [4] propose a probability model called Sentiment PLSA (S-PLSA for short) based on the assumption that sentiment consists of multiple hidden aspects. They develop a model called ARSA (which stands for Auto-Regressive Sentiment-Aware) to quantitatively measure the relationship between sentiment aspects and reviews. Our experience with running ARSA on several online review datasets reveals that the model is highly sensitive to the sentiment factors, which are constantly changing over time as new reviews become available. It is therefore essential to allow the SPLSA model to adapt to newly available review data.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

To this end, we take a Bayesian approach, and propose an adaptive version of the S-PLSA model that is equipped with the incremental learning capability for continuously updating the model using newly observed reviews. The proposed model is motivated by the principle of quasi-Bayesian (QB) estimation, which has found successful applications in various domains such as adaptive speech recognition and text retrieval [1]. We call the proposed model the S-PLSA+model, in which the parameters are estimated by maximizing an approximate posterior distribution. One salient feature of our modeling is the judicious use of hyperparameters, which can be recursively updated in order to obtain up-to-date posterior distribution and to estimate new model parameters. This modeling approach makes it possible to efficiently update the model parameters in an incremental manner without the need to re-train the model from scratch each time as new reviews become available.

2. S-PLSA

In the S-PLSA model [4], a review can be considered as being

generated under the influence of a number of hidden sentiment fac-

tors. The use of hidden factors provides the model the ability to

accommodate the intricate nature of sentiments, with each hidden

factor focusing on one specific aspect. What differentiates S-PLSA

from conventional PLSA is its use of a set of appraisal words [4]

as the basis for feature representation. The rationale is that those

appraisal words, such as "good" or "terrible", are more indicative

of the review's sentiments than other words.

For a given set of N reviews D = {d1, . . . , dN }, and the set

of M appraisal words W = {w1, . . . , wM }, the S-PLSA model

dictates that the joint probability of observed pair (di, wj) is gen-

erated by P (di, wj) = P (di)

K k=1

P (wj |zk)P (zk|di),

where

zk  Z = {z1, . . . , zK } corresponds to the latent sentiment fac-

tor, and where we assume that di and wj are independent condi-

tioned on the mixture of associated sentiment factor zk. The set of

parameters  of this model consist of {P (wj|zk), P (zk|di)}, the

maximum likelihood estimates of which can be obtained through

an expectation-maximization (EM) algorithm [4].

3. ADAPTIVE S-PLSA
The S-PLSA model can be trained in a batch manner on a collection of reviews, and then be applied to analyze others. In many cases, however, the reviews are continuously becoming available, with the sentiment factors constantly changing. We thus hope to adapt the model to the newly obtained reviews, in order to make it more suitable to the changing contexts. A naïve way to perform the adaptation is to re-train the model from scratch using all data available including the newly obtained data, which is clearly highly inefficient. Here, we propose a model called S-PLSA+, which performs incremental learning based on the principle of quasi-Bayesian

873

(QB) estimation. The basic idea is to perform updating and down-

dating at the same time by (i) incrementally accumulating statistics

on the training data, and (ii) fading out the out-of-date data. Let Dn be the set of reviews made available at epoch n (e.g.,
the reviews published on a certain day, but the time unit used can be set to be finer or coarser based on the need), and denote by n = {D1, . . . , Dn} the set of reviews obtained up to epoch n. QB S-PLSA estimates at epoch n are determined by maximizing the posterior probability using n:

(n) = arg max P (|n) = arg max P (Dn|)P (|n-1)





In order to allow closed-form recursive update of , we use the

closest tractable prior density g(|(n-1)) with sufficient statistics

to approximate the posterior density P (|n-1), where n-1 is

evolved from review sets n-1. This leads to (n)  arg max P (Dn|)g(|(n-1)). Note that at epoch n, only the new reviews

Dn and the current statistics (n-1) are used to update the S-PLSA+

parameters, and the set of reviews Dn are discarded after new parameter values (n) are obtained, which results in significant sav-

ings in computational resources.

The particular choice of the prior g(|) in our model is the

Dirichlet density, which can be expressed by


KM


N

g(|) =

 P (wj |zk)j,k-1

P (zk|di)k,i-1

k=1 j=1

i=1

where  = {j,k, k,i} are the hyperparameters of the Dirichlet
distribution. Assuming for the moment that (n-1) is known, we can show that (n) can be obtained through an EM algorithm [1].
A major benefit of S-PLSA+ lies in its ability to continuously update the hyperparameters. We can show that the new hyperparameters are given by

|Dn |

(jn,k) =

c(d(in), wj(n))P (n)(zk |d(in), wj(n)) + (jn,k-1)

(1)

i=1

M

k(n,i) =

c(d(in), wj(n))P (n)(zk |d(in), wj(n)) + k(n,i-1).

(2)

j=1

where the posterior P (n)(zk|di(n), wj(n)) is computed using Dn and the current parameters (n), and c(di(n), wj(n)) denotes the number of (d(in), wj(n)) pairs.
To summarize, S-PLSA+works as follows. In the startup phase, initial estimates of the hyperparameters (0) are obtained. Then, at each learning epoch n, (i) new estimates of the parameters (n)
are computed based on the newly available data Dn and hyperpa-
rameters obtained from epoch n - 1; and (ii) new estimates of the hyperparameters (n) are obtained using (1) and (2). This way, the
model is continuously updated when new reviews (Dn) become available, and at the same time fades out historical data n-1, with the information contained in n-1 already captured by (n-1).

4. APPLICATION TO SALES PREDICTION

The proposed S-PLSA+model can be employed in a variety of

tasks, e.g., sentiment clustering, sentiment classification, etc. As a

sample application, we plug it into the ARSA model proposed in

[4], which is used to predict sales performance based on reviews

and past sales data. The original ARSA model uses S-PLSA as the

component for capturing sentiment information. With S-PLSA+,

the ARSA model can be formulated as follows:

p

qR

yt =

iyt-1 +

i,j t-i,j + t,

i=1

i=1 j=1

where (i) yt denotes the sales figure at time t after proper preprocessing such as de-seasoning, (ii) p, q, and R are user-chosen

parameters, (iii) i and i,j are coefficents to be estimated using

training data, and (iv) t,j

=

1 |Rt |

dRt p(zj |d), where Rt is

the set of reviews available at time t and p(zj|d) is computed based

on S-PLSA+. It reflects the sentiment "mass" that can be attributed
to factor zj. The ARSA model can be trained using linear least
squares regression. Note that the notion of time (t) in the ARSA model is different from the epoch (n) in S-PLSA+. For example,
sales prediction can be made for each day using ARSA, whereas the model adaptation of S-PLSA+can happen every other day.

5. EXPERIMENTS

Experiments were conducted on an IMDB dataset to evaluate

the effectiveness of the proposed approach by comparing the prediction accuracy of ARSA using S-PLSA+and that of the original

ARSA. The dataset was obtained from the IMDB Website by col-

lecting 28,353 reviews for 20 drama films released in the US from

May 1, 2006 to September 1, 2006, along with their daily gross

box office revenues. Half of the movies are used for batch training.

For the original ARSA, the trained model is then used to make pre-

dictions in the testing data consisting of the other half the movies. For the proposal model, adaptation of the S-PLSA+component is

performed for each movie in the testing set, in four epochs on four

different days v (v = 2, 4, 6, 8) using the review data available up

to day v. The up-to-date model at day v is then used for subsequent

prediction tasks.

We use the mean absolute percentage error (MAPE) to measure

the prediction accuracy:

MAPE

=

1 T

Ti=1(|Predi-Truei|/Truei),

where T is the number of instances in the testing set, and Predi and

Truei are the predicted value and the true value respectively. The

results on the accuracy of the original ARSA and that of the ARSA

using S-PLSA+updated at Epochs 1-4 (v = 2, 4, 6, 8) respectively

are shown in the table below.

Original Epoch 1 Epoch 2 Epoch 3 Epoch 4

0.352

0.295

0.241

0.247

0.240

The accuracy improves as the model is getting updated in the first two epochs, which demonstrates the benefits of having an incremental model to absorb new information; especially in our case, S-PLSA+allows the models to be adapted to the individual movies. The accuracy stays stable from Epoch 2 through Epoch 4, indicating that no significant new information is available from Epoch 2 to Epoch 4.

6. CONCLUSIONS AND FUTURE WORK
In this paper, we have presented an adaptive S-PLSA model that is capable of incrementally updating its parameters and automatically downdating old information when new review data become available. This model has been used in conjunction with the ARSA model for predicting sales performance. Preliminary experimental results show that by allowing the model to be adaptive, we can capture new sentiment factors arising from newly available reviews, which can greatly improve the prediction accuracy. For future work, we plan to study the performance of S-PLSA+in other information retrieval and data mining tasks.

Acknowledgements
This work is supported by NSERC Discovery Grants, an Early Researcher Award of Ontario and an NSFC Grant (No. 60903108).
7. REFERENCES
[1] Jen-Tzung Chien and Meng-Sung Wu. Adaptive bayesian latent semantic analysis. IEEE TASLP, 16(1):198­207, 2008.
[2] Anindya Ghose and Panagiotis G. Ipeirotis. Designing novel review ranking systems: predicting the usefulness and impact of reviews. In ICEC, pages 303­310, 2007.
[3] Daniel Gruhl, R. Guha, Ravi Kumar, Jasmine Novak, and Andrew Tomkins. The predictive power of online chatter. In KDD '05, pages 78­87, 2005.
[4] Yang Liu, Xiangji Huang, Aijun An, and Xiaohui Yu. ARSA: a sentiment-aware model for predicting sales performance using blogs. In SIGIR, pages 607­614, 2007.

874

Supervised Query Modeling Using Wikipedia
Edgar Meij and Maarten de Rijke
ISLA, University of Amsterdam, The Netherlands
{edgar.meij, derijke}@uva.nl

ABSTRACT
We use Wikipedia articles to semantically inform the generation of query models. To this end, we apply supervised machine learning to automatically link queries to Wikipedia articles and sample terms from the linked articles to re-estimate the query model. On a recent large web corpus, we observe substantial gains in terms of both traditional metrics and diversity measures.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval
General Terms
Algorithms, Experimentation, Measurement
Keywords
Machine Learning, Query Modeling, Wikipedia
1. INTRODUCTION
In a web retrieval setting, there is a clear need for precision enhancing methods [5]. For example, the query "the secret garden" (a novel that has been adapted into movies and musicals) is a query that is easily led astray because of the generality of the individual query terms. While some methods address this issue at the document level, e.g., by using anchor texts or some function of the web graph, we are interested in improving the query; a prime example of such an approach is leveraging phrasal or proximity information [8]. Besides degrading the user experience, another significant downside of a lack of precision is its negative impact on the effectiveness of pseudo relevance feedback methods. An example of this phenomenon can be observed for a query such as "indexed annuity" where the richness of the financial domain plus the broad commercial use of the web introduces unrelated terms. To address these issues, we propose a semantically informed manner of representing queries that uses supervised machine learning on Wikipedia. We train an SVM that automatically links queries to Wikipedia articles which are subsequently used to update the query model.
Wikipedia and supervised machine learning have previously been used to select optimal terms to include in the query model [10]. We, however, are interested in selecting those Wikipedia articles which best describe the query and use those to sample terms from. This is similar to the unsupervised manner used, e.g., in the context of retrieving blogs [9]. Such approaches are completely unsupervised in that they only consider a fixed number of pseudo relevant Wikipedia articles. As we will see below, focusing this set using machine learning improves overall retrieval performance.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. QUERY MODELING

We adopt a language modeling for IR framework in which doc-

uments are ranked according to their likelihood of generating the

query:

log P (D|Q)



log P (D)

+

P
tQ

P

(t|Q

)

log

P

(t|D

).

In our experiments we assume a uniform document prior and apply

Bayesian smoothing using a Dirichlet prior (set to the average doc-

ument length) to obtain each document model D. For the query model we use a linear interpolation: P (t|Q) = QP (t|~Q) + (1 - Q)P (t|^Q), where P (t|~Q) indicates the empirical estimate on the initial query and P (t|^Q) an expanded part which we obtain

using the formula below. Note that when we set Q = 1 we obtain

a query-likelihood ranking which will serve as our baseline.

We take relevance model 1 for our estimations of P (t|^Q) [6]:

P (t|^Q)

=

1 |R|

P
DR

P

(t|D)P

(Q|D).

(1)

Here, R indicates a set of (pseudo) relevant documents which we obtain in three ways: (i) on the collection ("normal" pseudo relevance feedback), (ii) on Wikipedia (similar to so-called "external expansion" [4, 9]), and (iii) using automatically linked Wikipedia articles, which are introduced in the next section.

3. LINKING QUERIES TO WIKIPEDIA
To be able to derive query models based on Wikipedia, we first need to link queries to Wikipedia articles. To this end, we follow the approach in [7] which maps queries to DBpedia concepts, without performing any subsequent query modeling as we do in this paper. We take their best performing settings, i.e., SVM with a polynomial kernel using full queries. Instead of using a proprietary dataset, however, we take two ad hoc TREC test collections, i.e., TREC Terabyte 2004­2006 (.GOV2) and TREC Web 2009 (ClueWeb09, Category A).1 In order to classify Wikipedia articles as being relevant to a query, the approach uses manual query-to-article annotations to train an SVM model. For new queries, a retrieval run is performed on Wikipedia which is then classified using the trained model. The output of this step is a binary classification on each Wikipedia article, where the class indicates the relevance status as predicted by the SVM.
Our features include those pertaining to the query, the Wikipedia article, and their combination. See [7] for an extensive description of each feature. Since we are using ad hoc test collections, we do not have session information and omit the history-based features used there. In order to obtain training data, we have asked 4 annotators to manually identify all relevant Wikipedia articles for each query. The average number of Wikipedia articles the annotators identified per query is around 2 for both collections. The average number of articles identified as relevant per query by SVM is slightly different, with 1.6 for TREC Terabyte and 2.7 for TREC
1http://trec.nist.gov/.

875

QL
RM (C) RM (WP)
WP-SVM WP-SVM WP-SVM

Q MAP MRR Recall P10

1 0.2803 0.7121 7874 0.5081

0.5 0.2882 0.6126 7599 0.5068 0.5 0.2680 0.7331 7364 0.5203

0.8 0.2856 0.7108 7902 0.5 0.2769 0.6937 7731 0 0.2284 0.6307 6965

0.5324 0.5176 0.4392

Table 1: Results on the TREC Terabyte 2004­2006 collection.

QL
RM (C) RM (WP)
WP-SVM WP-SVM WP-SVM

Q MAP MRP MPC30 MNDCG
1 0.02583 0.07765 0.08333 0.04443
0.5 0.02523 0.07612 0.07823 0.04107 0.5 0.02320 0.07274 0.07847 0.04359
0.8 0.03371 0.08882 0.11304 0.06188 0.5 0.03635 0.08961 0.13437 0.07529 0 0.02917 0.07403 0.12577 0.06480

Table 2: Results on the TREC Web 2009 collection (using stat measures [2]).

QL
RM (C) RM (WP)
WP-SVM WP-SVM WP-SVM

Q eMAP -NDCG@10 IA-P@10

1 0.03614 0.04200

0.01700

0.5 0.03919 0.03200 0.5 0.03474 0.03900

0.01300 0.01600

0.8 0.04702 0.05700 0.5 0.06364 0.06100 0 0.09418 0.03300

0.03000 0.03500 0.01800

Table 3: Results on the TREC Web 2009 test collection (using expectedMAP (eMAP) and diversity measures [1­3]).
Web 2009. This seems to be due to the differences in queries; the TREC Web queries are shorter and, thus, more prone to ambiguity.
For the TREC Web 2009 query (#48) "wilson antenna," it predicts ROBERT WOODROW WILSON as the only relevant article, classifying articles such as MOUNT WILSON (CALIFORNIA) as not relevant. For the query "the music man" (#42) it identifies the company, song, 1962 film, and 2003 film which indicates the inherent ambiguity of many web queries. The same effect can be observed for the query "disneyland hotel" (#39) with articles TOKYO DISNEYLAND HOTEL, DISNEYLAND HOTEL (CALIFORNIA), and DISNEYLAND HOTEL (PARIS). There are also mistakes however, such as predicting the article FLAME OF RECCA (a Japanese manga series) for the query (#49) "flame designs."
4. RESULTS AND DISCUSSION
To determine whether the automatically identified articles are a useful resource to improve the query model, we compare our approach (WP-SVM) against a query-likelihood (QL) baseline and against Eq. 1 on pseudo relevant documents. In the latter case, we use either the collection (RM (C)) or the top-ranked Wikipedia articles (RM (WP)). For both we use the top 10 retrieved documents. In order to make results comparable, we include the 10 terms with the highest probability in P (t|^Q) for all approaches. We leave the influence of varying these numbers for future work.
To train the SVM model, we split each test collection in a training and test set. For TREC Terabyte 2004­2006, we have 150 topics which are split equally. For TREC Web 2009 we have 50 topics and use five-fold cross validation.

Tables 1 and 2 show the results on TREC Terabyte and Web 2009 respectively (best scores in boldface). For TREC Terabyte, we observe that WP-SVM obtains highest recall and P10. Although pseudo relevance feedback on the collection obtains highest MAP, MRR is relatively low. An example of a topic helped by WP-SVM is "train station security measures" (#711) caused by the suggested article SECURITY ON THE MASS RAPID TRANSIT.
As to TREC Web 2009, performing pseudo relevance feedback on the collection introduces very general terms and thus does not improve overall retrieval effectiveness. Using WP-SVM to estimate the query model, however, introduces focused terms which improves overall performance. These results indicate that supervised query modeling using Wikipedia is helpful for large, noisy collections.
When we evaluate WP-SVM on the TREC Web 2009 collection using the diversity track's measures, cf. Table 3, we arrive at the same picture. Using WP-SVM we obtain an -nDCG@10 score of 0.06100 which would have placed this run in the top-7 of participating systems in that particular track. This finding, in conjunction with the examples provided earlier, indicates that our query modeling approach caters for multiple interpretations of the query since prominent terms from each identified Wikipedia article are included in the query model.
5. CONCLUSIONS
We have presented a query modeling method based on Wikipedia that is aimed at obtaining high-precision representations of the original query. We find limited improvements on a relatively small web collection, only beating state-of-the-art query expansion methods according to some metrics. On a much larger web corpus, we achieve improvements on all metrics, whether precision or recall oriented. When using diversity measures, we observe major improvements, especially when relying exclusively on externally derived contributions to the query model.
Acknowledgements This research was supported by the European Union's ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme, CIP ICT-PSP under grant agreement nr 250430, by the DuOMAn project carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments under project nr STE-09-12, and by the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.066.512, 612.061.814, 612.061.815, 640.004.802.
6. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In WSDM '09, 2009.
[2] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation over thousands of queries. In SIGIR '08, 2008.
[3] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Büttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR '08, 2008.
[4] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In SIGIR '06, 2006.
[5] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In SIGIR '05, 2005.
[6] V. Lavrenko and B. W. Croft. Relevance models in information retrieval. In B. W. Croft and J. Lafferty, editors, Language Modeling for Information Retrieval. Kluwer, 2003.
[7] E. J. Meij, M. Bron, B. Huurnink, L. Hollink, and M. de Rijke. Learning semantic query suggestions. In ISWC '09, 2009.
[8] G. Mishne and M. de Rijke. Boosting web retrieval through query operations. In ECIR '05, 2005.
[9] W. Weerkamp, K. Balog, and M. de Rijke. A generative blog post retrieval model that uses query expansion based on external collections. In ACL-ICNLP 2009, 2009.
[10] Y. Xu, G. J. Jones, and B. Wang. Query dependent pseudo-relevance feedback based on wikipedia. In SIGIR '09, 2009.

876

A Two-Stage Model for Blog Feed Search

Wouter Weerkamp w.weerkamp@uva.nl

Krisztian Balog k.balog@uva.nl

Maarten de Rijke derijke@uva.nl

ISLA, University of Amsterdam, Science Park 107 1098 XG Amsterdam

ABSTRACT
We consider blog feed search: identifying relevant blogs for a given topic. An individual's search behavior often involves a combination of exploratory behavior triggered by salient features of the information objects being examined plus goal-directed in-depth information seeking behavior. We present a two-stage blog feed search model that directly builds on this insight. We first rank blog posts for a given topic, and use their parent blogs as selection of blogs that we rank using a blog-based model.
Categories and Subject Descriptors:
H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval
General Terms: Algorithms, Measurement, Performance,
Experimentation
Keywords: Blog feed search, two-stage model
1. INTRODUCTION
We focus on blogs: the unedited, unregulated voice of an individual [5], as published on a web page containing time-stamped entries. The blogosphere has shown a huge increase in volume in recent years, and is now a major source of information online. To allow for end users to follow a blog that regularly covers a given topic, we can provide them with a ranking of blogs that are likely to show a recurring interest in the topic. This task of identifying topically relevant blogs is referred to as blog feed search. Even though the unit of retrieval is blogs, the indexing unit should be blog posts, as this allows for easy incremental indexing, and the use of a single index for both blog feed search and blog post retrieval. Indeed, all current approaches to this task use a post index [1, 2, 4, 6].
We propose a two-stage model to blog feed search. The model exploits the following observation about human strategies for identifying complex information objects such as blogs (or people, for that matter). Prior to in-depth examination of complex information objects, humans display exploratory search behavior triggered by salient features of such objects [3]. This insight gives rise to the following two-stage model for blog feed search. In stage 1, we take individual utterances (i.e., posts) to play the role of "attention triggers" and select an initial sample of blogs based on the most interesting posts given the query, using a post-based approach. Here, we define "interesting" as topically relevant, but more elaborate techniques can also be applied (e.g., credibility, novelty, etc. [8]). Then, in stage 2, we only consider these most interesting blogs, which we then examine more in-depth by considering all their posts to deter-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

mine the likelihood of the topic being a central theme of the blog, using a blog-based approach.
We hypothesize that by pruning the list of posts taken into account in stage 1, and thus focusing on the most interesting utterances, we can achieve improvements over a blog-based model baseline on effectiveness and, as a result of blog selection, also on efficiency. Furthermore, we expect to see a considerable improvement on precision when applying pruning, and more so, when using a lean, title-only document representation in stage 1.

2. RELATED WORK
Previous work in blog feed search centered around two families of approaches: blog-based models and post-based models. Blogbased models use posts to construct a model for each blog. Ranking is done by matching the query against these blog models. Examples of blog-based models include the Blogger model [1] and the Long Document model [2]. The other type of approaches, post-based models, start from a ranking of blog posts and scores of individual posts are aggregated in order to infer a ranking of blogs [1, 2, 4]; these models boil down to estimating the relevance of blog posts and associating these posts to their parent blogs, using some weighted association value. Various ways of aggregating scores [4] and association values [2, 9] have previously been discussed. Both families of approaches have problems: post-based models ignore the recurring interest requirement; just a few relevant posts in a blog can be enough for it to be ranked high, even though other posts are non-relevant. On the other hand, blog-based models can become quite inefficient when applied to large numbers of frequently updated blogs, where each consists of several posts.

3. A TWO-STAGE MODEL
We are working in a language modeling setting and rank documents (blogs, posts) based on their likelihood of being generated from the query. We use Bayes' Theorem to rewrite this probability: P (D|Q) = P (D)P (Q|D)/P (Q). As P (Q) does not influence the ranking, we drop this term; we assume P (D), i.e., the a priori belief of a document D being relevant, to be uniform. This term can therefore be ignored. We are left with P (D|Q)  P (Q|D). In our specific case we rank blogs, and try to estimate the probability P (Q|blog).
In stage 1, we are looking for salient utterances on the topic in blog posts. Therefore, we rank posts for a given query using:

P (Q|post) =

P (t|post)n(t,Q),

(1)

tQ

where P (t|post) = (1-post)P (t|post)+P (t) (i.e., smoothing with the background collection) and n(t, Q) is the number of times

877

term t is present in the query. We use the top N most relevant utterances (posts) to identify the set of possibly interesting blogs:

B = {blog|

P (Q|post)P (post|blog) > 0}, (2)

postN

where P (post|blog) denotes the importance of a post given a blog, which is assumed to be uniform. Note that the summation part in Eq. 2 corresponds to a post-based model for ranking blogs, however, in our approach it is only used for identifying blogs that deserve to be ranked for the topic.
Having identified the set of possibly interesting blogs, we now estimate the probability of each blog  B having generated the query, i.e., displaying a recurring interest in the topic:

P (Q|blog)  P (t|blog)n(t,Q).

(3)

tQ

We represent blogs as a multinomial probability distributions over the vocabulary terms, and infer a blog model blog, such that the probability of a term given the blog model is P (t|blog). To construct such a representation, we first aggregate all terms from posts
of the blog to estimate an empirical model:

P (t|blog) = P (t|post) · P (post|blog).

(4)

posts

Then, this probability P (t|blog) is smoothed using the background collection probability P (t), to arrive at P (t|blog) (smoothing blogs is done analogously to smoothing applied to posts).

4. EXPERIMENTS AND RESULTS
We test whether our two-stage model is capable of effective blog feed search. We perform two series of experiments. First, we investigate the amount of pruning applied, i.e., the value of N in Eq. 2. We consider three settings: no pruning, topic-independent pruning (train on 2007 topics, test on 2008, and vice versa), and topic-dependent pruning (best empirically found values per topic). The "no pruning" condition corresponds to the blog-based model (Blogger model in [1]). Our second set of experiments concerns the representation of posts for stage 1 of our approach. We compare the results of the two-stage model to a blog-based model only, since blog-based models usually outperform post-based models [1].
The blog feed search task ran at TREC 2007 and 2008 [7] and uses the TRECBlog06 corpus. We use the English blog posts, and ignore blogs that only have 1 post. We have a total of 95 topics and relevance judgments, and we only use the title field of the topics. In our experiments, we optimize for MAP. Testing for significance is done using a two-tailed paired t-test; significant differences are indicated using and ( = 0.01), and and ( = 0.05).
Table 1 lists the results on the 2007 and 2008 topics for the blogbased model, and the various settings for stage 1 in our two-stage model. Results show that our model is at least as effective as the blog-based model, while being considerably more efficient: The blog-based model examines all 2.5M blog-post associations, while our two-stage model considers just 1% (23,700) in stage 2. Topic independent pruning results in a slight improvement in effectiveness over the blog-based model, while topic dependent pruning results in significant improvements. The use of a lean document representation, with an average document length of just 12 words, results in very good overall precision scores.

5. CONCLUSIONS
We have proposed a two-stage model for blog feed search. The model only tries to rank bloggers that stand out because of salient

2007 topics

Blog-based model

Two-stage model Representation Pruning

full content full content title-only title-only title-only

1,700 topic-dep. 7,000 topic-dep.

2008 topics Blog-based model

Two-stage model Representation Pruning

full content full content title-only title-only title-only

1,700 topic-dep. 7,000 topic-dep.

MAP 0.3260
0.3348 0.3611 0.3549 0.3577 0.3813
0.2521
0.2551 0.2747 0.2363 0.2368 0.2571

P@5 0.5422
0.5422 0.5689 0.6444 0.6622 0.6889
0.4880
0.4960 0.5080 0.4880 0.4840 0.5080

MRR 0.7193
0.7213 0.7243 0.8476 0.8587 0.8604
0.7447
0.7483 0.7504 0.7524 0.7524 0.7591

Table 1: Results of the various instances of the two-stage model compared to the blog-based model. Significance tested against blog-based model.

posts and then determines whether the topic is a central concern. Experiments show that the two-stage model can improve over a blog-based model. Topic dependent pruning of the post list in stage 1 helps, and we can combine this with a lean document representation to improve early precision even further. Future work is aimed at learning the optimal pruning level per topic.
Acknowledgements This research was supported by the European Union's ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme, CIP ICT-PSP under grant agreement nr 250430, by the DuOMAn project carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments under project nr STE-09-12, and by the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.066.512, 612.061.814, 612.061.815, 640.004.802.
REFERENCES
[1] K. Balog, M. de Rijke, and W. Weerkamp. Bloggers as experts. In SIGIR 2008, pages 753­754, 2008.
[2] J. Elsas, J. Arguello, J. Callan, and J. Carbonell. Retrieval and feedback models for blog feed search. In SIGIR 2008, 2008.
[3] C. Kuhlthau. Seeking Meaning: A Process Approach to Library and Information Services. Libraries Unlimited, 2003.
[4] C. Macdonald and I. Ounis. Key blog distillation: Ranking aggregates. In CIKM 2008, pages 1043­1052, 2008.
[5] G. Mishne. Applied Text Analytics for Blogs. PhD thesis, University of Amsterdam, 2007.
[6] J. Seo and W. B. Croft. Blog site search using resource selection. In CIKM 2008, 2008.
[7] TREC Blog track wiki. http://ir.dcs.gla.ac.uk/wiki/TRECBLOG, 2010.
[8] W. Weerkamp and M. de Rijke. Credibility Improves Topical Blog Post Retrieval. In ACL-08: HLT, pages 923­931, 2008.
[9] W. Weerkamp, K. Balog, and M. de Rijke. Finding key bloggers, one post at a time. In ECAI 2008, pages 318­322, 2008.

878

Machine Learned Ranking of Entity Facets

Roelof van Zwol
Yahoo! Research
roelof@yahoo-inc.com

Lluis Garcia Pueyo
Yahoo! Research
lluis@yahoo-inc.com

Mridul Muralidharan

Börkur Sigurbjörnsson

Yahoo! Research

Yahoo! Research

mridulm@yahoo-inc.com borkur@yahoo-inc.com

ABSTRACT
The research described in this paper forms the backbone of a service that enables the faceted search experience of the Yahoo! search engine. We introduce an approach for a machine learned ranking of entity facets based on user click feedback and features extracted from three different ranking sources. The objective of the learned model is to predict the click-through rate on an entity facet. In an empirical evaluation we compare the performance of gradient boosted decision trees (GBDT) against a linear combination of features on two different click feedback models using the raw click-through rate (CTR), and click over expected clicks (COEC). The results show a significant improvement in ranking performance, in terms of discounted cumulated gain, when ranking entity facets with GBDT trained on the COEC model. Most notably this is true when evaluated against the CTR test set.
Categories and Subject Descriptors
H.3.3 [Information Retrieval]: Information Search and Retrieval; H.3.5 [Information Retrieval]: On-line Information Services
General Terms
Experimentation, Measurement, Performance
Keywords
ranking entity facets, click feedback, GBDT
1. ABOUT RANKING ENTITY FACETS
The major Web search engines are gradually changing the search experience. Most notably this is visible through the introduction of semantic search assistants, the enrichment of the search results shown to the user and other components that try to predict the user intent. Key to enriching the search experience is the wide-scale availability of user-generated content and other knowledge bases such as Wikipedia, the Internet Movie Database (IMDB), GeoPlanetTM, or Freebase to name a few.
The research presented here is part of the faceted search experience of the Yahoo! Web and Image search engines [4].
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Screen captions of entity facets show in the search engine interface.
Figure 1 shows a fragment of the search engine interface depicting the facets bar for the celebrity "Daniel Day-Lewis" and the location "Geneva, Switzerland".
To support the entity facet ranking application of Figure 1, we propose a machine learned ranking of entity facets based on user click feedback. Given an entity of interest, we have collected a large pool of related candidate facets, e.g. related entities. These entity facet pairs have been extracted from various knowledge bases such as Wikipedia, GeoPlanetTMand other sources. Typically this provides us with a few hundred candidate facets for an entity. The task is then to rank the candidate facets related to each entity in our pool based on its relevance.
Facets are ranked using statistical features extracted from three different sources that contain entity information in the context of images: query terms entered by users in query logs, query session information from users in query logs, and tags provided by users annotating their photos in Flickr. Query term information captures entities that co-occur in a single user query, while query sessions provide information about entities that frequently co-occur in a user session. Flickr tags have a good coverage of travel and location related entities, as well as topics of a more general nature, but

879

tend to be less focussed on, for instance, celebrity entities. For every source, we extract different unary, symmetric and asymmetric features such as query frequency, conditional probability, KL divergence, etc. [4]. For the initial launch of the faceted search experience, we constructed a ranking function that is a linear combination of the conditional probabilities extracted from the three ranking sources .
The main contribution of this paper is a machined learned approach for ranking entity facets based on user click feedback. We propose to learn a ranking using the full set of features extracted from the ranking sources that will predict the click-through rate (CTR) on an entity facet [1]. For that purpose we introduce two click models: raw clickthrough rate on the facets, and the click over expected click (COEC), which is claimed to be more robust towards the position-bias on a click as users tend to click more on those results shown high in the ranking [3].
The click-feedback is used as the ground truth for our training, development and test sets. We have experimented with various learners, but for the experiment reported here we limit ourselves to the discussion of the performance using stochastic gradient boosted decision trees (GBDT) [2]. We used least squares regression as our loss function.
Next we collected the user click feedback on the facets over a period of three months, based on which we compute the click-through rate and click over expected click for each entity facet pair that was shown at least 25 times to a user. The latter constraint is to ensure that the CTR and COEC values are stable enough to be used as the labels for our training, development, and test sets. We join the feature set with the CTR and COEC sets, using the entity facet pair as the key. Next we split the collection into training, development and test sets. When splitting, we ensure that an entity can only occur in one of the three collections.
2. EVALUATION
The objective of the experiment is to measure the prediction accuracy based on the click-through rate of an entity facet pair. We first present the setup of the experiment, followed by a discussion of the results of the evaluation.
Ranking strategies. Central in the experiment are the three
ranking strategies: (1) Baseline. A linear combination of the conditional probabilities. (2) GBDTctr. GBDT trained on the CTR click model and (3) GBDTcoec. GBDT trained on the COEC click model.
Test sets. For the experiment we use two test sets both
containing the same 100 entities and their 10+ facets that have not been used for training or parameter tuning. For a fair comparison of the performance between queries we have normalized the CTR and COEC values for each of the facets of the 100 selected entities to be in the range of [0, 1].
Evaluation metrics. To evaluate the performance on the
test collection, we adopt Discounted Cumulative Gain (DCG) as our metric. DCG is an effectiveness measure that is used frequently for information retrieval tasks, and allows for the use of a graded relevance scale.
Results. The overall performance of the baseline and the
two GBDT models is reported in Table 1. For each of the two test sets, CTR and COEC, the mDCG and mnDCG is included. The performance of all strategies, independent of

Table 1: Overall performance.

CTR

COEC

Run

mDCG mnDCG mDCG mnDCG

Ideal

2.375 -

2.594 -

Baseline 1.728 0.709

1.812 0.677

GBDTctr 2.090

0.874

-

-

GBDTcoec 2.343

0.986

2.436 0.930

Figure 2: nDCG comparison of performance of baseline and GBDT on CTR vs COEC test sets.
the test set is good (mnDCG > 0.67). It can be clearly seen that on both the CTR and COEC test sets, the GBDT models outperform the baseline strategy. Using the normalized metric (mnDCG) we see that can better estimate the actual COEC using the GBDTcoec model than predicting the raw CTR with our GBDTctr model. This gives us a first indication that the COEC click model is more effective than the CTR click model when learning to rank entity facets.
Figure 2 plots the nDCG scores at different points in the ranking. This allows for a direct comparison of the different strategies across the two test sets. In addition to the strategies already discussed, we introduce a new variant where we evaluate the performance of the GBDTcoec strategy, e.g. the GBDT model that was trained to optimized the click prediction on the COEC click model, against the CTR test set. As can be seen this gives a near optimal performance over the first ten positions in the ranking, and a perfect prediction of the most important facet for each of the 100 entities in our test set.
3. REFERENCES
[1] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In WSDM '08: Proceedings of the international conference on Web search and web data mining, pages 87­94, New York, NY, USA, 2008. ACM.
[2] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189­1232, 2001.
[3] Y. Zhang and R. Jones. Comparing click logs and editorial labels for query rewriting. In Query Log Analysis: Social And Technological Challenges, 2007.
[4] R. van Zwol, B. Sigurbj¨ornsson,et al. Faceted Exploration of Image Search Results. In WWW2010. Raleigh, NC, USA. 2010.

880

User Comments for News Recommendation in Social Media

Jia Wang


Qing Li

Southwestern Univ. of Finance Southwestern Univ. of Finance

and Economics

and Economics

55 Guanghua Cun Road

55 Guanghua Cun Road

Chengdu, China

Chengdu, China

wangjia@2008.swufe.edu.cn liq_t@swufe.edu.cn

Yuanzhu Peter Chen
Memorial Univ. of Newfoundland
St. John's, A1B 3X5 NL, Canada
yzchen@mun.ca

ABSTRACT
Reading and Commenting online news is becoming a common user behavior in social media. Discussion in the form of comments following news postings can be effectively facilitated if the service provider can recommend articles based on not only the original news itself but also the thread of changing comments. This turns the traditional news recommendation to a "discussion moderator" that can intelligently assist online forums. In this work, we present a framework to recommend relevant information in the forum-based social media using user comments. When incorporating user comments, we consider structural and semantic information carried by them. Experiments indicate that our proposed solutions provide an effective recommendation service.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Filtering
General Terms
Algorithms, Experimentation
1. INTRODUCTION
Web is one of the most important vehicles for "social media", e.g. Internet forums, blogs, wikis, and twitters. One form of social media of particular interest here is self-publishing. In selfpublishing, a user can publish an article or post news to share with other users. Other users can read and comment on the posting and these comments can, in turn, be read and commented on. Digg (digg.com) and Yahoo!Buzz (buzz.yahoo.com) are commercial examples of self-publishing. A useful extension of this self-publishing application is to add a recommendation feature to the current discussion thread. That is, based on the original posting and various levels of comments, the system can provide a set of relevant articles, which are expected to be of interest of the active users of the thread.
Here, we explore the problem of news recommendation for dynamic discussion threads. A fundamental challenge in adaptive news recommendation is to account for topic divergence, i.e. the change of gist during the process of discussion. In a forum, the original news is typically followed by other readers' opinions, in
This research is supported by National Natural Science Foundation of China Grant No.60803106.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Multi-relation graph of comments
the form of comments. Concerns and intention of active users may change as the discussion continues. Therefore, news recommendation, if it were only based on the original posting, can not benefit the potentially changing interests of the users. Apparently, there is a need to consider topic evolution in adaptive news recommendation and this requires novel techniques that can help to capture topic evolution precisely to prevent wild topic shifting which returns completely irrelevant news to users. A related problem is content-based information filtering (or recommendation). Most information recommender systems select articles based solely on the contents of the original postings [1] [3] [4].
In this work, we propose a framework of adaptive news recommendation in social media. It has the following contributions. (1) It is the first attempt of incorporating reader comments for adaptive news recommendation. (2) We model the relationship among comments and that relative to the original posting in order to evaluate their overall impact on recommendations.
2. SYSTEM DESIGN
The proposed news recommender first constructs a topic profile for each news posting along with the comments from readers, and uses this profile to retrieve relevant news.
We first model the relationship among comments and that relative to the original posting in order to evaluate their overall impact. In our model, we treat the original posting and the comments each as a text node. This model both considers the content similarity between text nodes and the logic relationship among them. On one hand, the content similarity between two nodes can be measured by any commonly adopted metric, such as cosine similarity and Jaccard coefficient. This metric is taken over every node pair in the discussion thread. On the other hand, the logic relation between nodes takes two forms. First, a comment is always made in response to the original posting or an earlier comment. In graph theoretic terms, the hierarchy can be represented as a tree  = (,  ), where  is the set of all text nodes and  is the edge set. In particular, the original posting is the root and all the comments are ordinary nodes.

881

There is a directed edge    from node  to node , denoted (, ), if the corresponding comment  is made in response to comment (or original posting) . Second, a comment can quote from one or more earlier comments. From this perspective, the hierar-
chy can be modeled using a directed acyclic graph (DAG), denoted  = (, ). There is a directed edge    from node  to node , denoted (, ), if the corresponding comment  quotes from comment (or original posting) . As shown in Figure 1, for either graph  or , we can use a   ×   adjacency matrix, denoted  and , respectively, to record them. Inline with the adjacency matrices, we can also use a   ×   matrix defined on [0, 1] to record the content similarity between nodes and denote it by  . Thus, we can combine these three aspects linearly.
Intuitively, the important comments are those whose topics are
discussed by a large number of other important comments. There-
fore, we propose to apply the PageRank algorithm [2] to rank the
comments as 
 = /  + (1 - ) × (, ) × ,


where  is the damping factor as in PageRank and this value is recommended to be 0.85,  and  are node indices, and   denotes the number of text nodes in the thread. In addition, (, ) is the normalized weight of comment  referring to  defined as

(,  )

=



, ,

+



,



where , is an entry in the graph adjacency matrix and  is a constant to avoid division by zero.
Once the importance of comments on one news posting is quan-
tified by our model, this information along with the news itself are
fed into a synthesizer to construct a topic profile of this news dis-
cussion thread. The profile is a weight vector of terms to model the language used in the thread. Consider a news posting 0 and its comment sequence {1, 2,    , }. For each term , a compound weight  () is calculated. It is a linear combination of the contribution by the news posting itself, 1(), and that by the comments, 2(). The weight contributed by the news itself, 0, is:

1() = (, 0)/ma x (, 0)

The weight contribution from the comments {1, 2,    , } incorporates not only the language features of these documents but also their importance of leading a discussion in related topics. That is, the contribution of comment score is incorporated into weight calculation of the words in a text node.

 

2() =

(,



)/max 

(

,



)

×



/max 



=1

Such a treatment of compounded weight  () is essentially to recognize that readers' impact on selecting relevant news and the difference of their influence strength.
With the topic profile constructed as above, we can use it to select relevant news for recommendations. That is, the retriever returns an order list of news with decreasing relevance to the topic. Our model to differentiate the importance of each comment can be easily incorporated into any good retrieval model. In this work, our retrieval model is derived from [4].

3. EXPERIMENTAL EVALUATION
To gauge how well the proposed recommendation approach performs, we carry out a series of experiments on a synthetic data set

 @10  

Table 1: Overall performance The Proposed CF Okapi

0.94

0.789 0.827

0.932

0.8 0.833

LM
0.804 0.833

collected from Digg and Reuters news website. We randomly select 20 news articles with corresponding reader comments from Digg website. These news articles with different topics are treated as the original news postings, recommended news are selected from a corpus of articles collected from Reuters news website. This simulates the scenario of recommending relevant news from traditional media to social media readers for their further reading. We compared the proposed approach to three other retrieval approaches as the baseline: one is a simple content filter (CF) which treats news and comments as a single topic profile, the other two are well-known news recommendation methods [1], Okapi and LM.
To observe the impact of readers' concerns on original news posting in social media, we investigate the effect of the three forms of relationship among comments, i.e. content similarity, reply, and quotation. We carry out a series of experiments for this purpose. we find that replies are slightly more effective than quotations and both of these outperform pure content similarity. In other words, the importance of comments can be well evaluated by the logic organization of these comments. We also notice that the incorporation of content similarity decreases the system effectiveness. This may seem to contradict our intuition that the textual information should complement the logic-based models. By further investigating our results, we find that content similarity sometimes misleads the decision on the importance of the comments. Besides, the computation cost of calculating the content similarity matrix  is very high. Therefore, we only apply the structural information to determine the importance of each comment.
We have -tests using  @10 and MAP as performance measures, respectively, and the  values of these tests are all less than 0.05, which means that the results of experiments are statistically significant. We conduct a series of preliminary experiments to find the optimal performance obtained when the topic file word number is 60 and combination coefficient  is 0.7. As shown in Table 1, the overall performance of the proposed approach performed significantly better than the best baseline methods.

4. CONCLUSION
In this work, we present a framework for adaptive news recommendation that incorporates information from the entire discussion thread. This study can be extended in a few interesting ways. For example, we can use this technique to process personal Web blogs and email archives. The technique itself can also be extended by incorporating such information as reader scores on comments, chronological information of comments, and reputation of users. Indeed, its power is yet to be further improved and investigated.

5. REFERENCES
[1] T. Bogers and A. Bosch. Comparing and evaluating information retrieval algorithms for news recommendation. In Proc. of ACM Recommender systems, 2007.
[2] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107­117, 1998.
[3] J.-H. Chiang and Y.-C. Chen. An intelligent news recommender agent for filtering and categorizing large volumes of text corpus. International Journal of Intelligent Systems, 19(3):201­216, 2004.
[4] V. Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie, D. Jensen, and J. Allan. Language models for financial news recommendation. In Proc. of CIKM, 2000.

882

Incorporating Global Information into Named Entity Recognition Systems using Relational Context

Yuval Merhav

Filipe Mesquita

Denilson Barbosa

Illinois Institute of Technology University of Alberta, Canada University of Alberta, Canada

yuval@ir.iit.edu

mesquita@cs.ualberta.ca denilson@cs.ualberta.ca

Wai Gen Yee

Ophir Frieder

Illinois Institute of Technology

Georgetown University

yee@iit.edu

ophir@cs.georgetown.edu

ABSTRACT
The state-of-the-art in Named Entity Recognition relies on a combination of local features of the text and global knowledge to determine the types of the recognized entities. This is problematic in some cases, resulting in entities being classified as belonging to the wrong type. We show that using global information about the corpus improves the accuracy of type identification. We explore the notion of a global domain frequency that relates relationidentifying terms with pairs of entity types which are used in that relation. We use this to identify entities whose types are not compatible with the terms they co-occur in the text. Our results on a large corpus of social media content allows the identification of mistyped entities with 70% accuracy.
Categories and Subject Descriptors
I.2.7 [Natural Language Processing]: Text analysis
General Terms
Experimentation, Performance
1. INTRODUCTION
Named Entity Recognition (NER) is an important task for many Information Retrieval applications. One sub-task of particular importance is type identification: assigning meaningful types (e.g., Person, Organization, Location, etc.) to the extracted entities. The state-of-the-art NER systems rely on a mix of local information (statistics and results of lexical analysis) about small portions of the corpora and external knowledge (usually obtained through learning on training data) to perform type identification. For example, LBJ [3] analyzes the corpus in fixed-sized text windows ignoring document boundaries and relies on two sources of external information: high-precision lists of named entities (gazetteers) and clusters of commonly used words in different contexts. While the use of external information has been shown to improve accuracy over purely local methods, they are limited to the knowledge contained in a small collection of documents or tokens for every entity type assignment decision they make. Inevitably, these methods eventually assign incorrect types to the entities they extract. As an example, consider the snippet "[MISC Jewish] by birth, [ORG Alamo] married [PER Edith Opal] who was also [MISC Jewish]" which is tagged with LBJ. As one can see, Alamo is correctly identified
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

as an entity, but assigned type ORG (for Organization) instead of PER (for Person). As Alamo appears in LBJ's gazetteers as an organization, it is likely that this is the reason LBJ labeled Alamo incorrectly.
To overcome the limitation, we propose a scoring feature based on global information (extracted from the entire corpus) for improving the assignment accuracy of entity types. The feature is designed to be used as a supplement in different NER systems and other tasks. This work is motivated by our social network extractor system SONEX [2], that extracts latent social networks from social media text (in [2] we report results on the ICWSM'09 Spinn3r Blog dataset with 44 millions posts [1]). SONEX works by extracting named entities with LBJ, and individual sentences with LingPipe1. Then it identifies relations that associate pairs of entities (e.g., Alamo and Edith Opal) by clustering those sentences using a variety of features. In the example above, SONEX identifies that Alamo and Edith are married to each other, and thus assigns this term to the pair. In this work, we show how to automatically use the results of this relation extraction in SONEX to identify entities which are incorrectly typed (Alamo in the example).

2. DOMAIN FREQUENCY
It is natural to expect that the relations extracted in SONEX are strongly correlated with a given context. For instance, marriage is a relation between Persons, and thus, belongs to the domain PER-PER. We exploit this observation to identify mistyped entities. Starting from the social network extracted by SONEX, which we call the dataset in the sequel, we proceed as follows. For each relation-identifying term in the dataset (excluding stop words), we compute its relative frequency in every possible domain. We refer to the frequency of a term t in a given domain i as the term's Domain Frequency (DF) score, and refer to it as dfi (t).
We normalize the domain frequency of every term across all domains associated with the term. More precisely, let t be a term and let i1, . . . , in be all possible domains of pairs of entity types; let fi(t), . . . , fn(t) be the frequencies in which that term identifies a relation of a pair of entities in each of the domains. Then,

dfi (t) =

fi(t)

.

1jn fj (t)

The number of possible domains is the square of the number of types the NER system identifies. LBJ offers the following types: PER (Person), ORG (Organization), LOC (Location), and MISC

1http://alias-i.com/lingpipe

883

PER - PER ORG - ORG ORG - PER

0.8409 0.0681 0.0227

Table 1: Top-3 Domain Frequencies for "married" in the Spinn3r social network extracted with SONEX.

(miscellaneous)2. Table 1 shows the most significant relative DF scores for the term married across different domains. As expected, the DF score for the PER to PER domain is significantly larger than all other domains. If a term does not appear in a certain domain, its DF score for the domain is zero (e.g., "married" does not appear in the LOC to LOC domain, and hence, it does not appear in Table 1). The entire list of terms associated with their DF scores is available by request. This list can be used as an external knowledge source in different NER systems, in various dataset domains.
2.1 Detecting Incorrectly Typed Entities
Our premise is that a pair of entities (E1, E2) from a given domain d1 = T1 × T2 contains at least one mistyped entity if there is a relationship-identifying term t that connects them in the dataset, and the term's domain frequency for d1 is "significantly" lower than that of another different domain d2. Our hypothesis is that we can detect mistyped entities by comparing such domain frequencies. We validate it as follows.
Setup. Since the ICWSM'09 dataset does not include labels for
named entities, we identify all type errors through manual evaluation. The complete evaluation we performed is available by request. We compute the list of terms in the dataset and their associated DF scores. Then, we obtain two random entity sets for our evaluation. The first, RANDOM contains 70 entity pairs (thus 140 typed entities) randomly chosen from the dataset. The second, SUSPICIOUS, consists of 326 entity pairs for which the gap in domain frequencies (highest to lowest) is higher a threshold.
More precisely, SUSPICIOUS is obtained as follows. For each term t in the dataset, we compute the difference between its highest and the lowest DF scores. We keep those terms for which this difference is larger than 0.5 (empirically tested to produce high precision with reasonable recall), resulting in 482 unique terms (i.e., relations). From these, we randomly chose 30 to obtain the entity pairs in our tests. For each term, we gathered up to 15 entity pairs, resulting in a total of 326 unique entity pairs (some terms were associated with less than 15 pairs), that generated 375 entities for the SUSPICIOUS evaluation (for most of the entity pairs, only one entity out of the two in a domain is identified as a mistake).
Hypothesis. Our hypothesis is that pairs in the SUSPICIOUS
set are more likely to contain at least one mistyped entity. This is justified by the following observation: out of the 375 entities, 269 (or 72%) are of type ORG, 83 (22%) are of type LOC, and 23 (6%) are of type PER. This distribution is very different than the one that takes into account all entities in the dataset: 12% for ORG, 43% for LOC, and 45% for PER. The type ORG, which appears in only 12% of all entities, appears in 72% of the entities in the SUSPICIOUS set, implying a higher error rate for organizations compared to other entity types.
Table 2 validates this hypothesis. In the table, precision is the number of entities with correct types (we did not consider the cor-
2We do not consider the MISC type in our experiments. They are too generic, making it hard to evaluate whether the entities assigned to this type are misclassified by LBJ.

PER LOC ORG Weighted

RANDOM 0.79 0.70 0.43 0.62

SUSPICIOUS 0.43 0.44 0.25 0.30

Table 2: Correctness of the entity types in the 2 evaluation sets

rectness of the entity boundary) divided by the total number of entities in each set. Weighted is the weighted average of the precision for the three types. Observe that in the RANDOM set entities are correctly typed 62% of the times, whereas in the SUSPICIOUS set this happens only 30% of the time. This 30% reflects the entities we incorrectly identified as mis-typed. Also, observe that LBJ is much more accurate in correctly identifying persons and locations compared to organizations.
Method. The significant observation from our experiment is that
the difference in domain frequency scores may be an effective way of identifying mis-typed entities. It effectively yields a very simple and automatic procedure for detecting incorrect type assignments which has 70% precision. Given the much lower rate mistyping rate of just 38% for the RANDOM set, these results are promising.
3. CONCLUSION
We proposed the use of Domain Frequency scores to predict entities which are erroneously typed by NER systems. This measure can be readily incorporated into existing NER systems with ease. DF exploits terms between pairs of entities to estimate the likelihood of a term to appear between given entity types. DF relies on global (corpus-wide) information as is thus sensitive to the domain at hand. We showed experimentally that the difference in DF scores for a given term serves as a good indicator that the entities associated through that term are incorrectly typed, and that this simple rule was able to detect an entity with incorrect type in 70% of the cases, and that this rate is much higher than that of a random sample of the dataset.
We are investigating ways in which to use the DF scores to further improve not only entity type identification but also the extraction of the relations among the entities. We envision a mutual refinement scheme in which both tasks go hand-in-hand.
4. ACKNOWLEDGEMENTS
This work was supported in part by the Natural Sciences and Engineering Research Council of Canada and the Alberta Ingenuity Fund.
5. REFERENCES
[1] K. Burton, A. Java, and I. Soboroff. The icwsm 2009 spinn3r dataset. In ICWSM '09: Proceedings of the 3rd Int'l AAAI Conference on Weblogs and Social Media, 2009.
[2] F. Mesquita, Y. Merhav, and D. Barbosa. Extracting information networks from the blogosphere: State-of-the-art and challenges. In ICWSM '10: Proceedings of the 4th Int'l AAAI Conference on Weblogs and Social Media, 2010.
[3] L. Ratinov and D. Roth. Design challenges and misconceptions in named entity recognition. In CoNLL '09: Proceedings of the 13th Conference on Computational Natural Language Learning, pages 147­155, Morristown, NJ, USA, 2009. Association for Computational Linguistics.

884

Achieving High Accuracy Retrieval using Intra-Document Term Ranking

Hyun-Wook Woo hwwoo@nlp.korea.ac.kr

Jung-Tae Lee jtlee@nlp.korea.ac.kr

Seung-Wook Lee swlee@nlp.korea.ac.kr

Young-In Song yosong@microsoft.com

Hae-Chang Rim rim@nlp.korea.ac.kr

 Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea  Microsoft Research Asia, Beijing, China

ABSTRACT
Most traditional ranking models roughly score the relevance of a given document by observing simple term statistics, such as the occurrence of query terms within the document or within the collection. Intuitively, the relative importance of query terms with regard to other individual non-query terms in a document can also be exploited to promote the ranks of documents in which the query is dedicated as the main topic. In this paper, we introduce a simple technique named intra-document term ranking, which involves ranking all the terms in a document according to their relative importance within that particular document. We demonstrate that the information regarding the rank positions of given query terms within the intra-document term ranking can be useful for enhancing the precision of top-retrieved results by traditional ranking models. Experiments are conducted on three standard TREC test collections.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Retrieval models
General Terms
Algorithms, Experimentation
Keywords
inter-document term ranking, precision at top ranks
1. INTRODUCTION
With the rapid growth of Web document collection sizes in recent years, achieving high precision at the top of the retrieved result has become a major issue for search engine users [1]. In traditional IR ranking models [2, 3], the relevance of a document for a given query is scored primarily based on simple term statistics, such as the number of occurrence of query terms within the document or within the whole document collection. Basically, the more query terms occur in a document, the higher the chance that the document would be relevant to the query and thus would be
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

provided at the top part of the ranked list. Despite its simplicity, such a scoring method has been effective in many previous document ranking experiments.
However, traditional models virtually do not consider the relative importance of query terms compared to other individual non-query terms within a document in ranking. For example, consider the query word "q" and the following two equal-length documents at top ranks in which "q" occurs twice: "q q b c d e f " and "q q b b b b c". Traditional ranking models would assume that the weight of "q" is roughly the same for both documents. However, this is indeed not true if we examine the relative importance of "q" within individual documents. In the first document, "q" occurs relatively more than any of the other non-query terms, which implies that the document may be mainly about "q". In contrast, in the second document, "q" occurs relatively less than the word "b", which implies that "q" may be a subtopic of that document. Intuitively, given two documents that both contain the same number of query terms, we would like to rank the document in which the query is dedicated as a main topic above the one where the query is regarded as a minor topic.
The relative importance of query terms with regard to other terms in a document can be observed by ranking all the terms in the document with some weighting scheme, such as the tf-idf method. We refer to this technique as intra-document term ranking. In this paper, we present two heuristic document ranking methods based on the rank positions of given query terms in intra-document term ranking lists of individual documents. Our hypothesis is two-fold:
· First, when query terms are ranked in relatively higher term rank positions than other terms in a given document, it implies that the query is dedicated as the central topic of the document. Thus, there is a higher chance that the document is relevant to the query.
· Second, individual query terms having similar term rank positions to each other in a document reflect that they are treated equally. Thus, there is a chance that the query terms have close relationships to each other within the document, which implies higher relevance.
The succeeding section will elaborate on how we rank documents based on the two hypotheses. We validate our method on three standard TREC test collections.

885

2. PROPOSED METHOD
To analyze the relative importance of individual terms that occurred in a given document, we rank the terms by their standard tf-idf weights. We use the term frequency normalized with the document length as the tf component, and the logarithm of the inverse document frequency as the idf component. We normalize the ranks in range 0 to 1 so that the term with highest tf-idf weight will be at rank 0 and the term with the lowest weight at rank 1.
Given this ranked list of terms within a document, we derive two document ranking heuristics based on the rank positions of query terms within the list. The first heuristic, which corresponds to our first hypothesis, ranks documents according to the average rank position of query terms. Here we basically assume that the higher the rank of the query term within a document, the more likely that the document is relevant. Given a query Q and the term ranking result of a document D, the average rank position of query terms is calculated as follows:

R1(Q, D) =

qiQ{1 - rank(qi, D)} |Q|

(1)

where qi represents ith query term, |Q| is the size of the query in words, and rank(qi, D) is a function that returns the normalized term rank of qi in D.
The second heuristic, which corresponds to our second hypothesis, assumes that the more cohesive the rank positions of query terms (i.e. have similar rank positions to each other) in a document, the higher the probability that the document is relevant. There are a number of ways to measure the cohesiveness of the term ranks. In this paper, we calculate such measure by the maximum rank difference of all pairs of query terms as follows:

R2(Q, D) = 1 - { max {dif f (qi, qj , D)}} (2) qi ,qj QD,qi=qj

where dif f (qi, qj , D) is a function that returns the absolute value of the difference in rank positions of qi and qj . For example, if three query terms were ranked at 0, 0.1, and 0.4, the maximum difference would be 0.3.

3. EXPERIMENTS
For test collections, we use the following standard TREC collections: AP88-90 (Associated Press news 1988-90), WT2g (2 gigabyte Web data), and Blogs06 (Web data used in TREC Blog Track 2006-08). For queries, we use the title field of the TREC topics 50-150 for AP88-90, 401-450 for WT2g, and 851-950 and 1001-1050 for Blogs06.
To investigate whether the two new heuristics can infer the true relevance for documents at top ranks of the retrieved list by traditional ranking models, we retrieved the top 20 documents from the collections using query-likelihood language models [2] with dirichlet prior1 (LM) and computed the correlation between the output value of either of the heuristics and the relevance. We observe that both have positive correlations with document relevance (0.21 for R1 and 0.31 for R2). This result is consistent with our two hypotheses.
We validate the effectiveness of the two ranking heuristics in a re-ranking scheme. First, given a query, we generate a
1We tuned µ to be optimal for each collection (1000, 3000, and 5000 for AP88-90, WT2g, and Blogs06, respectively).

Table 1: Retrieval performance.

Corpus Method MRR

P@1

P@5

LM

0.4598 0.3100 0.2840

+R1

0.4433 0.3000 0.2740 (-3.59%) (-3.23%) (-3.52%)

AP88-90 +R2

0.4671 0.3200 0.2900 (1.59%) (3.23%) (2.11%)

+R1+R2

0.4743 (3.15%)

0.3200 (3.23%)

0.3020 (6.34%)

LM

0.6342 0.6000 0.4880

+R1

0.6874 0.5800 0.4640 (8.39%) (-3.33%) (-4.92%)

WT2g +R2

0.7151 0.6200 0.4640 (12.76%) (3.33%) (-4.92%)

+R1+R2

0.7097 (11.90%)

0.6200 (3.33%)

0.5000 (2.64%)

LM

0.6634 0.5333 0.5747

+R1

0.7090 0.6000 0.5987 (6.87%) (12.51%) (4.18%)

Blogs06 +R2

0.7404 0.6467 0.6067 (11.61%) (21.26%) (5.57%)

+R1+R2

0.7384 (11.31%)

0.6400 (20.01%)

0.6453 (12.28%)

ranked list consisting of top n documents from a test collection using LM, which represents a state-of-the-art baseline. Then, we create a new ranked list of the n documents using each heuristic function. We finally re-rank the documents in the initial list in the ascending order of the mean rank of documents among individual ranked lists. If a tie occurs, a document that has a higher rank in the initial ranked list is promoted. We compare the top results of the re-ranked result with the baseline result using Mean Reciprocal Rank (MRR) and Precision at k ranks (P@1 and P@5). The Indri implementation2 is used for indexing and retrieval; stemming and stopword removal are performed.
The retrieval performances of the baseline and the proposed method are presented in Table 1. We observe that when the baseline ranking is aggregated with either one of the heuristic rankings, the improvement is not consistent across different data collections. However, when the baseline ranking is merged with all two heuristic rankings, the improvement over the baseline is consistent across all collections for all evaluation measures. This demonstrates that the analysis of rank positions of query terms in interdocument term ranking is effective in improving the precision at top ranks. For future work, we plan to explore other features that capture various aspects of inter-document term ranking. We also plan to investigate on designing a unified ranking model that combines the proposed heuristics with the features of the traditional retrieval models.
4. REFERENCES
[1] I. Matveeva, C. Burges, T. Burkard, A. Laucius, and L. Wong. High accuracy retrieval with multiple nested ranker. In SIGIR '06, 2006.
[2] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR '98, 1998.
[3] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613­620, 1975.
2http://www.lemurproject.org/indri/

886

Author Interest Topic Model
Noriaki Kawamae
NTT Comware 1-6 Nakase Mihama-ku Chiba-shi, Chiba 261-0023 Japan
kawamae@gmail.com

ABSTRACT
This paper presents a hierarchical topic model that simultaneously captures topics and author's interests. Our proposed model, the Author Interest Topic model (AIT), introduces a latent variable with a probability distribution over topics into each document. Experiments on a research paper corpus show that AIT is very useful as a generative model.
Categories and Subject Descriptors
H.3.1 [Content Analysis and Indexing]:
General Terms
Algorithms, experimentation
Keywords
Topic Modeling, Latent Variable Modeling
1. INTRODUCTION
Attention is being focused on how to model users' interests in several fields. A model of interest allows us to infer which topics each user prefers and to measure the similarity between them in terms of their interests. For example, the Author-Topic(AT) [3] groups all papers associated with a given author by using a single topic distribution associated with this author. Author-Persona-Topic(APT) [2] introduces a persona, which is also a latent variable, under a single given author. Thus, these models allow each author's documents to be divided into one or more clusters, each with its own separate topic distribution specific to that persona
This paper presents the Author Interest Topic(AIT) model; it is a generalization of known author interest models such as AT and APT. AIT allows a number of possible latent variables to be associated with author's interest, while previous models limit this number. Therefore, AIT can describe a wider variety of authors' interests than other models, which reduces the perplexity. Moreover, AIT can infer the overall interest in the training data and so can assign probabilities to previously unseen documents.
2. AUTHOR INTEREST TOPIC MODEL
This section details our model. Table 1 shows the notations used in this paper. Figure 1 shows graphical models to
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Figure 1: Graphical models: In this figure, shaded and unshaded variables indicate observed and latent variables, respectively. An arrow indicates a conditional dependency between variables and the plates indicate a repeated sampling with the iteration number shown. This figure shows that each author produces words from a set of topics that are preferred by the author in (a), persona associated with the author in (b), each document class in (c). In learning a document written by multiple authors, AIT makes copies of the document and associates one copy with each author.
describe the generative process. For modeling each author's interest, our proposal, AIT, incorporates document class cd; it provides an indicator variable that describes which mixture of topics each document d takes, into d. Accordingly, AIT represents documents of similar topics as the same document class in the same way that topic models represent cooccurrence words as the same topic variable. Therefore, the difference between AIT and AT, APT is that rather than representing author's interest as a mixture of topic variables a(AT) or Pa (APT) in each document layer, AIT represents each author's interest as a mixture of document classes a in each author layer. Although both a(AT) and Pa (APT) are associated with only authors, the document class can be shared among authors. This class allows AIT to represent documents having similar topics as the same document class by merging parameters; this reduces the number of possible parameters without losing generality. Accordingly, as the size of training data is increased, relatively fewer parameters are needed. On the contrary, the parameters of the other models track the order of authors and so experience linear growth with the size of the training data. Moreover we decide the number of latent variables following CRP [1]. Consequently, AIT increases the number of possible latent variables for explaining all authors' interests.
AIT employs Gibbs sampling to perform inference approximation. In the Gibbs sampling procedure, we need to cal-

887

Table 1: Notations used in this paper

SYMBOL DESCRIPTION

A

number of authors

J

number of document classes

T

number of topics

D

number of documents

V

number of unique words

Ad

authors associated with document d

Da

number of documents written by author a

Nd

number of word tokens in document d

ai

author associated with ith token in document d

pd

persona associated with document d

cd

document class associated with document d

zdi

topic associated with the ith token in document

d

wdi

ith token in document d

a

multinomial distribution of document classes

specific to author a (a|  Dirichlet() )

j

multinomial distribution of topics specific to in-

terest j (j |  Dirichlet() )

t

multinomial distribution of words specific to

topic t (t|  Dirichlet() )

culate the conditional distributions. The predictive distribution of adding interest class cd in documents written by author a to topic cd = j is given by

8 (Pt

njt\d +t)

Q t

(njt +t )

>>< n , aj\d

Q t

(njt\d +t)

(Pt

njt +t )

P (j|c\d, a, z, , )  if j is an existing class class

> > :

j

(Pt njt\d +t)

Q t

(njt\d +t)

, Q t

(njt +t

)

(Pt njt+t)

otherwise

(1)

where naj\d represents the number of documents assigned to j in all documents written by author a, except d, and njt\di represents the total number of tokens assigned to topic t

in the documents associated with document class j, except

token di.

The predictive distribution of adding word wdi in docu-

ment d written by a to topic zd = t is given by

8
> > <

n , ntw\di+w

jt\di

PV v

(ntv\di +v )

P (t|j, z\di, w, , )  if t is an existing class

> > :

 , ntw\di+w

t

PV v

(ntv\di +v

)

otherwise

(2)

where ntw\di represents the total number of tokens assigned to word w in topic t, except token di, and njt\di represents the total number of tokens assigned to topic t in

all tokens assigned to j, except token di.

3. EXPERIMENTS
We focus here on the extraction of interests from given documents, and demonstrate AIT's performance as a generative model. The dataset used in our experiments consisted of research papers in the proceedings of ACM CIKM, SIGIR, KDD, and WWW gathered over the last 8 years (2001-2008). We removed stop words, numbers, and the words that appeared less than five times in the corpus. Accordingly, we obtained a total set of 3078 documents and 20286 unique words from 2204 authors. Additionally, we applied both AT and APT to this dataset for training and comparison.
In our evaluation, the smoothing parameters ,  and

Table 2: Perplexity of AT, APT and AIT: This difference between AIT and APT is significant according to one-tailed t-test with the number of samples G = 100. For fair comparison, the number of topic variables T was fixed at 200, the number of document classes J was fixed at 40(AIT). Results that differ significantly by t-test p < 0.01, p < 0.05 from APT are marked with '**', '*' respectively. The value of Avg means the average computing time for each iteration in gibbs sampling.

Iteration AT APT AIT

2000 1529 1454 1321

4000 1488 1304 1217

6000 1343 1180 1103

8000 1339 1059 988

10000 1333 1027 964

Avg 3.2s 10.4s 11.7s

 were set at 0.1, 10(APT),1(AIT) and 1, respectively. We ran single Gibbs sampling chains for 10000 iterations on machines with Dual Core 2.66 GHz Xeon processors.
To measure the ability of a model to act as a generative model, we computed test-set perplexity under estimated parameters and compared the resulting values.
Perplexity, which is widely used in the language modeling community to assess the predictive power of a model, is algebraically equivalent to the inverse of the geometric mean per-word likelihood (lower numbers are better). Table 2 shows the results of the perplexity comparison. This table shows that AIT yielded significantly lower perplexity on the test set than AT or APT, which shows that AIT is better as a topic model. This is due to the ability of AIT to allow the document class to be shared across authors and to group documents under the various topic distributions rather than grouping documents by a given author or persona under a few topic distributions. This implies that clustered documents contain less noise than otherwise. If the number of document classes is overly restricted, the difference between the observed data and the data generated by the model under test increases, which raises the perplexity.
4. CONCLUSION
Our proposed model, AIT, supports the expression of topics in text documents and can identify the interests of authors in these documents. Future work includes extending AIT by taking other metadata such as time, references and link structure into account, for tracking the dynamics of interests and topics.
5. REFERENCES
[1] D. J. D. Aldous. Exchangeability and related topics, volume 1117 of Lecture Notes in Math. Springer, Berlin, 1985.
[2] D. Mimno and A. McCallum. Expertise modeling for matching papers with reviewers. In KDD, pages 500­509, 2007.
[3] M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. L. Griffiths. Probabilistic author-topic models for information discovery. In KDD, pages 306­315, 2004.

888

On the Relationship Between Effectiveness and Accessibility

Leif Azzopardi
Department of Computing Science University of Glasgow United Kingdom
leif@dcs.gla.ac.uk
ABSTRACT
Typically the evaluation of Information Retrieval (IR) systems is focused upon two main system attributes: efficiency and effectiveness. However, it has been argued that it is also important to consider accessibility, i.e. the extent to which the IR system makes information easily accessible. But, it is unclear how accessibility relates to typical IR evaluation, and specifically whether there is a trade-off between accessibility and effectiveness. In this poster, we empirically explore the relationship between effectiveness and accessibility to determine whether the two objectives i.e. maximizing effectiveness and maximizing accessibility, are compatible, or not. To this aim, we empirically examine this relationship using two popular IR models and explore the trade-off between access and performance as these models are tuned.
Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval - Retrieval Models
General Terms: Theory, Experimentation
Keywords: Information Retrieval, Accessibility, Findability, Retrievability, Evaluation
1. INTRODUCTION
Historically, there have been two main ways to evaluate an Information Retrieval (IR) system: efficiency and effectiveness [7]. A complementary and so-called higher order evaluation has been recently proposed based on accessibility [1]. Instead of assessing how well the system performs in terms of speed or performance, access-based measures provide an indication of how easily documents within the collection can be retrieved using a particular retrieval system [1]. Evaluations based on accessibility have been performed in a number of different contexts (see [2, 6, 3, 4]), but there has been little work examining the relationship between accessbased measures and effectiveness measures.
2. MEASURING ACCESSIBILITY
The accessibility of information in a collection given a system has been considered from two points of view, the system side i.e. retrievability [2] and the user side findability [6]. Retrievability measures provide an indication of how easily a
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Richard Bache
Department of Computing Science University of Glasgow United Kingdom
bache@dcs.gla.ac.uk

document could be retrieved using a given IR system, while findability measures provide an indication of how easily a document can be found by a user with the IR system. Here we consider the accessibility measures based on retrievability (see [2] for more details and [3, 4] for examples of its usage in practice.)
Retrievability: The retrievability r(d) of a document d with respect to an IR system, is the ease with which it can be retrieved given all possible queries Q1. Formally,

X

r(d)  f (kdq, c)

(1)

qQ

where q is a query in Q, kdq is the rank at which d is retrieved given q, and f (kdq, c) is a function which denotes how accessible the document d is for the query q given the rank cutoff c. A simple measure of retrievability employs an access function f (kdq, c), such that if d is retrieved in the top c documents given q, then f (kdq, c) = 1 else f (kdq, c) = 0. This is referred to as a cumulative-based retrievability measure [2]; and it provides an intuitive value for each document, i.e. it is the number of times that the document is retrieved in the top c documents. To provide a single measure of access given the retrievability scores for all documents, the Gini measure [5] was proposed in [2]. Intuitively, if all documents were equally accessible according to r(d), then the Gini would be zero (equality), while if all but one document had r(d) = 0 then the Gini would be one (total inequality). Usually, most documents have some level of retrievability and the Gini measures is somewhere between zero and one. Essentially, the Gini coefficient provides an indication of the level of inequality between documents given how easily they can be retrieved with a particular retrieval system.
It is important to note that retrievability can be estimated without recourse to relevance judgments, making it an attractive alternative for automatic evaluation. That is, if there is a positive correlation between accessibility and effectiveness based measures.
Relating Retrievability and Effectiveness: Given the definition of retrievability then a purely random IR system would provide equal access to all documents (i.e. Gini=0); however, this would also result in very poor effectiveness. While, if a (hypothetical) IR system retrieved the set of known relevant documents, and only these documents, regardless of the query, then there would be a very high inequality among documents and Gini would be close to one.

1Since we cannot know the set of all the possible queries, it is usually approximated using a large (order of 105) set of automatically generated queries[2, 3, 4].

889

Neither extreme is desirable, but to what extent do we need to trade-off retrievability for effectiveness. In this poster, we examine the relationship between the Gini measure (i.e. the summarized retrievability measure) and precision, by examining the change in each measure as the parameter values of different retrieval models are varied.
Experimental Setup Two TREC collections were used: Associated Press (AP) 1988-1989 and Wall Street Journal (WSJ) 1987-1992, both with TREC query sets 1, 2 and 3. Two popular IR models were selected: Multinomial Language Modelling with Bayes Smoothing and Okapi BM25. For Language modelling, the smoothing parameter  was varied from 10-4 to 105 in multiplicative steps of 10. For BM25, the b parameter, which adjusts length normalisation, was varied from 0.1 to 1.0 in steps of 0.1. Effectiveness was measured using both precision at 10 (P@10) and mean average precision (MAP). To estimate retrievability, the same methodology employed in [2] was applied, where we used 100,000 two-word queries derived from the most frequent collocations found in each corpus to estimate the retrievability values. Retrievability was measured using the cumulative measure (described above) where c = 10 and c = 100. The degree of equality was measured using the Gini coefficient denoted as Gini@10 and Gini@100, respectively.
Results In Figure 1, plots of the different measures are shown for each model (top: Language Model, bottom: BM25) and each collection (left: AP, right: WSJ) across the parameter values. From these plots, the first point of interest is that Gini varies considerably across the parameter ranges for both models, where minimizing the Gini coefficient translates into providing more access to documents in the collection (this is around  = 10 - 100 for the Language Model and b = 0.6 - 0.8 for BM25. Of note, is that the suggested/default value for b is usually 0.75, which is well within this range.). While this does not directly correspond to when performance is maximized, the difference in performance is quite small; in the range of (0.01-0.03 for both P@10 and MAP). While, these differences in performance were significantly difference (p < 0.05 using Student's T-test) for all but BM25 on WSJ, there does appear to be a positive correlation between the two measures, and this opens up the possibility of using access based measures to tune retrieval systems. These findings suggest that a systematic relationship appears to exist between the gini measurements (representing Accessibility) and the precision measurements (representing Effectiveness).
3. DISCUSSION AND FURTHER WORK
This preliminary analysis of the relationship between accessibility measures (specifically retrievability measures) and effectiveness measures shows that the two goals of maximizing access and maximizing performance are quite compatible. In fact, reasonably good retrieval performance is still obtained by selecting parameters that maximize access (i.e. when there is the least inequality between documents according to Gini given the r(d) values). This motivates the hypothesis that retrieval models/systems can be effectively tuned using access based measures. If this holds, then it suggests that when relevance information is not available a sensible approach to configuring a system is to ensure that users can access all documents as easily as possible. However, further research is needed to test this hypothesize more

Precision/Gini

Precision/Gini

conclusively and to explore this interesting and complex relationship in detail. In future work, we shall examine different models, and the influence of the different parameters on access and performance.
Acknowledgments: This work is supported by Matrixware. We would like to thank the Information Retrieval Facility for the use of their computing resources, and Tamara Polajnar for her helpful feedback and comments on this work.

0.8

1

0.6

0.8

Precision/Gini

0.6 0.4
0.4

0.2

0.2

0

10-4

10-2

100

102

104

LM Parameter  - AP

0

10-4

10-2

100

102

104

LM Parameter  - WSJ

0.8

P@10

MAP

0.7

Gini@10

0.5

Gini@100

0.6

Precision/Gini

0.4

0.5

0.4 0.3
0.3

0.2

0.2 0.4 0.6 0.8

1

BM25 Parameter b - AP

0.2

0.2 0.4 0.6 0.8

1

BM25 Parameter b - WSJ

Figure 1: Plots of Precision and Gini measures across parameters for the LM and BM25 models.

4. REFERENCES
[1] L. Azzopardi and V. Vinay. Accessibility in IR. In Proceedings of 30th ECIR 2008, 482­489, 2008.
[2] L. Azzopardi and V. Vinay. Retrievability: An evaluation measure for higher order information access tasks. In Proceedings of the 17th ACM CIKM 2008, 561­570, 2008.
[3] S. Bashir and A. Rauber. Improving retrievability of patents with cluster-based pseudo-relevance feedback documents selection. In Proceedings of the 18th ACM CIKM 2009, 1863­1866, 2009.
[4] S. Bashir and A. Rauber. Improving retrievability of patents in prior-art search. In Proceedings of 32nd ECIR 2010, 457­470, 2010.
[5] J. Gastwirth. The estimation of the lorenz curve and gini index. The Review of Economics and Statistics, 54:306­316, 1972.
[6] H. Ma, R. Chandrasekar, C. Quirk, and A. Gupta. Improving search engines using human computation games. In Proceedings of the 18th ACM CIKM 2009, 275­284, 2009.
[7] C. J. van Rijsbergen. Information Retrieval. Butterworths, London, 1979.

890

Visual Concept-based Selection of Query Expansions for Spoken Content Retrieval
Stevan Rudinac, Martha Larson, Alan Hanjalic
Multimedia Information Retrieval Lab, Delft University of Technology, Delft, The Netherlands
{s.rudinac, m.a.larson, a.hanjalic}@tudelft.nl

ABSTRACT
In this paper we present a novel approach to semantic-themebased video retrieval that considers entire videos as retrieval units and exploits automatically detected visual concepts to improve the results of retrieval based on spoken content. We deploy a query prediction method that makes use of a coherence indicator calculated on top returned documents and taking into account the information about visual concepts presence in videos to make a choice between query expansion methods. The main contribution of our approach is in its ability to exploit noisy shot-level concept detection to improve semantic-theme-based video retrieval. Strikingly, improvement is possible using an extremely limited set of concepts. In the experiments performed on TRECVID 2007 and 2008 datasets our approach shows an interesting performance improvement compared to the best performing baseline.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Performance, Experimentation
Keywords: Semantic-theme-based video retrieval, video-level retrieval, query expansion, concept-based video indexing, query performance prediction
1. INTRODUCTION
The semantic theme (or subject matter) of a video is encoded in both its speech track and visual channel. This paper presents a novel multimodal retrieval approach aiming at videos covering the semantic theme expressed by the query. The approach improves the results of a speech-based retrieval system by exploiting concepts (e.g. human, car, house, female, children, indoor) detected in the visual channel. In contrast with most previous works on video retrieval, which focus on shot-level retrieval, e.g., [2, 5], our approach is designed to retrieve entire videos. These larger retrieval units are more appropriate than shots in cases where the searcher is looking for informational material or for entertainment, typical for the semantic-theme-based retrieval scenario (e.g., find a video about archaeology or psychology). Recent work on retrieval beyond the shot level includes [1]. Moving from the shot to the video level requires the combination of multiple shot-level concepts into an effective video-level representation. The novel contribution of our work is the successful use of such a video-level representation to combine the output of automatic speech recognition and visual-concept detection, both known to be noisy, and achieve an overall improvement in retrieval of videos on the basis of semantic theme. The key insight motivating our approach is that the presence, frequency and co-occurrence of
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

visual concepts are potentially powerful indicators of the similarity of videos with respect to their semantic themes. In this initial realization, we focus on exploiting the effects of concept presence and frequency.
In order to combine speech-based retrieval with visual concepts, we select a coherence-based query prediction framework [4]. This technique automatically chooses between results lists produced by a range of different query expansions by making use of a coherence-based indicator calculated over top documents of the results list returned by each expansion. Our specific innovation is use of concept-based representations of videos for calculation of the query prediction indicator. In the following, we first introduce our concept-based video representations and our query prediction method. Then we report on results of experiments that confirm the viability of our approach for effectively exploiting visual concepts to refine spoken content retrieval of videos at the semantic theme level.
2. APPROACH
The input for creating video-level representations is a vector in which each component represents a single concept. In order to weight each concept, we make use of term frequency (TF) and inverse document frequency (IDF) analogously to their use in text retrieval. However, concept detectors do not output binary concept occurrences, but rather shot-level lists of confidence scores. Each score represents the probability that a particular concept occurred in each shot. Our concept weights are created by accumulating the confidence scores for each concept over all shots in the video and then normalizing with the total shot count.
Following the previous feature generation step, we select in the next (feature selection) step the concepts that are potentially the most helpful in discriminating between videos with respect to their semantic themes. Because the output of concept detectors is notoriously noisy (in TRECVID 2009 the best performance failed to exceed 0.25 in the terms of MAP [5]), feature selection is a key aspect of our approach since it introduces a noise control effect. We developed our feature selection method by performing exploratory experimentation. Our experiments make use of TRECVID 2007 dataset and 46 semantic theme labels introduced by the VideoCLEF 2009 (www.multimediaeval.org) benchmark. The labels are manually assigned by professional archivists from the Netherlands Institute for Sound and Vision. We use a set of publicly available concept detection scores, generated for a set of 374 concepts selected from the LSCOM (www.lscom.org) ontology. In order to determine the most representative concepts, we trained classifiers that can identify videos related to each semantic theme and ranked the concepts according to their usefulness to the classifier. A simple voting approach was then applied to merge the lists into a single list that ranked the concept from most to least valuable for semantic discrimination. Our investigation revealed that it

891

is the most frequently occurring concepts in the videos that best

support discrimination between videos in terms of semantic class.

We use this result to select features in the coherence-based query

prediction step. The fact that the most frequent concepts are most

discriminative suggests that it is not so much the occurrence of a

particular visual concept in a video that distinguishes that video's

semantic class, but rather its relative frequency.

Our approach compares results of multiple query expansions

and returns, as the final output, the results list with the highest

coherence score over the Top N videos. The coherence score is

calculated as:

 ( ) Co(TopN ) =

 v , v i j{1,...,N}

ij

1 N ( N -1)

(1)

2

Here  is a function defined on the pair of videos (vi,vj) that is

equal to 1 if their similarity is higher than the similarity of particu-

larly close video pairs from the collection (i.e., closer than TP% of

pair-wise similarities, where TP is the threshold defined below).

The sum is taken over all video pairs in the set of Top N videos.

As a similarity measure between vectors of concept frequencies

we used the cosine similarity. Use of alternative similar-

ity/distance measures such as Kullback-Leibler divergence and

Euclidean distance yielded similar results.

3. EXPERIMENTAL SETUP
We test our approach on TRECVID 2007 and TRECVID 2008 datasets, using the 46 VideoCLEF 2009 semantic labels as queries. We index the Dutch speech recognition transcripts and the English machine translation and carry out retrieval using the Lemur toolkit. The initial results lists are produced using the original query and three query expansions: 1) Conventional PRF, where the number of feedback documents and terms used for expansion are selected for the optimal performance, 2) WordNet (http://wordnet.princeton.edu/) expansion and 3) Google Sets (http://labs.google.com/sets) expansion, where each query is expanded with a set of up to 15 related items (words or multi-word phrases). For each query, the results list yielding the highest coherence indicator is selected. In the (rare) cases of the same indicator values the priority is given to the baseline or the expansions following the ordering as above. We use the concept detection output provided by [3], as mentioned above. In the experiments on both collections we swept the parameter space for the following parameters: number of most frequent (discriminative) semantic concepts (NC), number of documents from the top of results list (N) and the threshold TP used to calculate the coherence score. We reported the results for the optimal parameter setting.

4. EXPERIMENTAL RESULTS
The quality of the initial results list for the original query and three expansion methods is reported in terms of Mean Average Precision (MAP) in Table 1.

Table 1. MAPs of the Baseline and Expansion Methods

Baseline PRF WordNet Google Sets

TRECVID 2007 0.326 0.332 0.260

0.120

TRECVID 2008 0.245 0.265 0.268

0.142

Table 2 contains the MAPs of our concept-based selection of query expansion for TRECVID 2007 and TRECVID 2008 datasets. Statistical significance w.r.t. Wilcoxon Signed Rank Test, p < 0.02 is indicated with `^'.

Table 2. MAPs After Query Expansion Selection (QES)

QES Best Baseline

TRECVID 2007 0.355^ 0.332

TRECVID 2008 0.296 0.268

The results confirm the viability of exploiting visual concepts for refining the output of spoken-content-based video retrieval at the level of a semantic theme. Recall that our feature selection approach was optimized using TRECVID 2007 as a development set. The fact that TRECVID 2008 yielded similar performance demonstrates the ability of our feature selection method to generalize to new data. The optimal parameter settings for TRECVID 2007 and TRECVID 2008 are not the same for both datasets, but are in the similar range: N=5-10, TP=80-90%, NC = 5-10.

5. CONCLUSIONS AND OUTLOOK
We have proposed a multimodal approach to semantic-themebased retrieval of entire videos that exploits frequencies of (semantic) concepts detected in a video to enhance the initial retrieval result obtained at the spoken-content level. We have demonstrated that our approach can be effectively used to decide whether the query should be expanded and which of several query expansions to use. Further, we are making use of only a fraction (5-10) of the set of available concepts (374). This result suggests that concept detectors that focus on a very small number of concepts have large potential to be useful for improving the results of semantic-theme-based video retrieval. In our future work we will further study the characteristics of concept detector output that contribute to effective performance of our approach, investigating, for example, whether the relatively larger performance improvement achieved on the TRECVID 2008 set (cf. Table 2) can be attributed to better performing concept detectors. We will also work to take into account concept co-occurrences and to combine proposed concept-based and text-based indicators to further improve query prediction.

6. ACKNOWLEDGMENTS
The research leading to these results has received funding from the European Commission's 7th Framework Programme (FP7) under grant agreement n° 216444 (NoE PetaMedia).

7. REFERENCES
[1] Aly, R., Doherty, A., Hiemstra, D., and Smeaton, A. 2010. Beyond shot retrieval: searching for broadcast news items using language models of concepts. In ECIR, Milton Keynes, UK, 2010.
[2] Hsu, W. H., Kennedy, L. S., and Chang, S. 2006. Video search reranking via information bottleneck principle. In ACM MM, Santa Barbara, CA, USA, 2006.
[3] Jiang, Y-G., Yanagawa, A., Chang, S-F., and Ngo, C-W. 2008. CU-VIREO374: Fusing Columbia374 and VIREO374 for Large Scale Semantic Concept Detection. Columbia University ADVENT Technical Report #223-2008-1.
[4] Rudinac, S., Larson, M., and Hanjalic, A. 2010. Exploiting result consistency to select query expansions for spoken content retrieval. In ECIR, Milton Keynes, UK, 2010.
[5] Snoek, C. G. M., van de Sande, K. E. A., de Rooij, O., et al. 2009. The MediaMill TRECVID 2009 semantic video search engine. In TRECVID Workshop, Gaithersburg, USA, 2009.

892

Mining Adjacent Markets from a Large-scale Ads Video Collection for Image Advertising

Guwen Feng
Nanjing University Nanjing, Jiangsu, P.R.China
linvondepp@gmail.com

Xin-Jing Wang, Lei Zhang, Wei-Ying Ma
Microsoft Research Asia Beijing, P.R.China
{xjwang,leizhang,wyma}@microsoft.com

ABSTRACT
The research on image advertising is still in its infancy. Most previous approaches suggest ads by directly matching an ad to a query image, which lacks the power to identify ads from adjacent market. In this paper, we tackle the problem by mining knowledge on adjacent markets from ads videos with a novel Multi-Modal Dirichlet Process Mixture Sets model, which is a unified model of (video frames) clustering and (ads) ranking. Our approach is not only capable of discovering relevant ads (e.g. car ads for a query car image), but also suggesting ads from adjacent markets (e.g. tyre ads). Experimental results show that our proposed approach is fairly effective.
Categories and Subject Descriptors
I.5.4 [Pattern Recognition]: Applications ­ computer vision. G.3 [Probability and Statistics]: Nonparametric statistics.
General Terms
Algorithms, Performance.
Keywords
Image advertising, adjacent marketing, video retrieval.
1. INTRODUCTION
Though image has become an important media in the Web, how to monetize web images is a seldom touched problem. Few research works have been published in the literature. Among them, most of the works suggest directly mapping an ad to an image [2]. They suffer from the vocabulary impedance problem so that if a term does not appear in both an image and an ad' features, no connections will be built between them. The approach of Wang et al. [4] improves this by leveraging the ODP ontology to bridge the vocabulary gap, but it is still limited by the available texts.
Adjacent marketing means to develop additional items which compliment a customer's needs in some manner, e.g. suggest insurance when one buys a car. The insurance package thus makes up of qualified adjacent markets of cars. A key challenge is to discover the potential adjacent market (e.g. insurance) of a certain
 This work was performed in Microsoft Research Asia.
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

Retrieved And Expanded Frames

... ...

Query

Ranking Results

Advertise

Benz

Insurance

Tires

Figure 1. Key idea: the story frames (in red blocks) and ads frames (cyan blocks) of video ads suggest adjacent markets.

product/object (e.g. car).
In this paper we propose a solution of adjacent marketing for image advertising based on a large-scale ads video collection. It is motivated by the fact that generally a video ad contains two types of frames ­ story frames and ads frames, as shown in Figure 1. Story frames in general provide the main concepts (e.g. cars) that are related to the corresponding ads (e.g. insurance, tires). They imply certain human knowledge on the adjacent markets, e.g. showing tire ads on car images. A novel Multi-Modal Dirichlet Process Mixture Sets (MoM-DPM Sets) model is proposed as the key technique behind.
2. SYSTEM OVERVIEW
Figure 2 shows the system overview. In the offline stage, we extract video keyframes and perform auto-tagging on them. The online stage contains three steps: 1) multimodal search on keyframes of ads videos given a query image (or query images). Both visual and textual similarities are taken into consideration. The texts come from three resources: image auto-tagging, OCR and surrounding texts; 2) search result expansion. Since the search step tends to find video frames of similar content, to incorporate the adjacent information between objects, we expand the retrieved frames with the rest frames in corresponding videos. For example, given the pizza image in Figure 2, the retrieved frames are generally about food, but by expanding them with the rest frames from the same ads videos, we are able to retrieve those soft drink and restaurant video frames which suggest the adjacent markets; and 3) ads clustering and ranking. The disadvantage brought by frame expansion is that the search results become noisy and with scattered topics. We propose the MoM-DPM Sets model to automatically learn the key topics from the expanded search results, and rank an ads database with the learnt topics. The top-

893

Query Image
Pizza 1. Search

Advertise

Annotate Using Arista
Visutal Features Detection Using ColorDescriptor

Large-Scale Video Frames DB

2. Cosine Measurement

Expanded Video Frames 3. Expand

Suggested Ads

Pizza

Pepsi Restaurant

6. Sorting Approach

Ads DB

MoM-DPM Sets

Clusters

5. Ranking

4. Clustering

...

...

Figure 2. System Overview.

ranked ads will be output. Therefore we find ads of Pepsi Cola and restaurants for the pizza image.

3. THE MoM-DPM SETS MODAL
The MoM-DPM Sets model addresses four challenges: 1) discover the latent topics shared among the frames, 2) automatically determine the number of topics, 3) leverage both the visual and textual features to ensure a better performance of topic discovery, and 4) unify the approaches of topic mining and ads ranking.

Let be the th latent topic and , be the visual and textual features of a query image respectively. Let , , denote the
concentration parameter and base distributions of visual and textual features respectively. Let , be model parameters and
, be the visual and textual features of the observed video
frames labeled by the topic respectively. The general form of is MoM-DPM Sets is given in Eq(1).

| , ,, ,

,

|

,



|

|,

(1)

if

for some ;



|

|

|

|

if

for all .

where and are the observed video frames corresponding to topic . is the normalization factor. is the number of observed video frames and , is the number of observed video frames (except the th) which belong to topic . We use Gibbs sampling to solve the model, which generally converges in 30 iterations.
MoM-DPM Sets has two key features which make it different from previous multimodal DP Mixture processes [3]. Firstly, rather than to learn an optimal parameter set of , , it intends to figure out the membership of each video frame given the observed video frames , . In our approach, , , and
are known ( and are learnt from the clustering step), while
the model parameters are going to be integrated out analytically. Such a set-based reasoning strategy [1] is more powerful in discovering analogical objects, e.g. given a frame set of Pepsi-cola and Coca-cola, this model is able to discover soda because they share the same concept of soft drinks. Secondly, since the model does not rely on certain parameter set, the clustering (topic mining) step and ranking step shares the same model formulation. The ranking process is as Eq.(2).

Average Precision

0.8

0.7

0.6

0.5

0.4

0.3

0.2

1

3

5

Our Approach

7

10

15

20

top N

Argo [4] DM [2]

Figure 3. Average precision performance @ top N.

| .. , ,

max



|

,

(2)



|

,

where ..

, , ... , defines the latent topic space.

4. EXPERIMENTS

We crawled about 32k videos from Youtube.com initiated by 30 popular concepts for advertising. In total 327,889 key frames were extracted, which make up of the ads videos collection for frame search. We randomly selected 450 ads as a separate ads DB for the ranking purpose. 100 Flickr images were used as queries.

Figure 3 illustrates the average precision at top 20 ads of our approach compared with those of the baselines Argo [4] and direct match [2]. It can be seen that our approach consistently outperforms the baselines. The gap between the blue curve and the green one indicates that our approach is able to identify the relevant ads from potential adjacent market, which have little overlap with the query image in both visual and textual features. And the gap between the red curve and the green one indicates that Argo [4] also tackles the adjacent marketing problem to a certain extent but it is not effective enough.
There are big gaps between our methods and the baselines in top 3 results, while the gap narrows down from top 5 to top 20. This may due to the limited size of our ads DB. Considering that generally a publisher such as Google shows less than five targeted ads, our method suggests a promising research direction for adjacent marketing.
5. CONCLUSION
Web image is an uncovered gold mine. Our method is the first work to tackle the adjacent marketing problem for image advertising. It leverages the human intelligence embedded in video ads to build the connections among ads objects based on a novel Multi-Modal Dirichlet Process Mixture Sets model.

6. REFERENCES
[1] Z. Ghahramani, and K. A. Heller. Bayesian Sets. Neural Information Processing Systems (NIPS). 2005.
[2] T. Mei, X.-S. Hua, and S.-P. Li. Contextual In-Image Advertising. 2008.
[3] A. Velivelli, and T.S. Huang. Automatic Video Annotation Using Multimodal Dirichlet Process Mixture Model. ICNSC 2008.
[4] X.-J. Wang, M. Yu, et al. Argo: Intelligent Advertising by Mining a User's Interest from His Photo Collections, in conjunction with SIGKDD (ADKDD), Paris, 2009.

894

A Co-learning Framework for Learning User Search Intents from Rule-Generated Training Data

Jun Yan1 Zeyu Zheng1
1Microsoft Research Asia Sigma Center, No.49, Zhichun Road Beijing, 100190, China
{junyan, v-zeyu, zhengc}@microsoft.com

Li Jiang2 Yan Li2 Shuicheng Yan3 Zheng Chen1

2Microsoft Corporation

3Department of Electrical and

One Microsoft Way

Computer Engineering

Redmond, WA 98004

National University of Singapore

{lij, roli}@microsoft.com

117576, Singapore
eleyans@nus.edu.sg

ABSTRACT
Learning to understand user search intents from their online behaviors is crucial for both Web search and online advertising. However, it is a challenging task to collect and label a sufficient amount of high quality training data for various user intents such as "compare products", "plan a travel", etc. Motivated by this bottleneck, we start with some user common sense, i.e. a set of rules, to generate training data for learning to predict user intents. The rule-generated training data are however hard to be used since these data are generally imperfect due to the serious data bias and possible data noises. In this paper, we introduce a Co-learning Framework (CLF) to tackle the problem of learning from biased and noisy rule-generated training data. CLF firstly generates multiple sets of possibly biased and noisy training data using different rules, and then trains the individual user search intent classifiers over different training datasets independently. The intermediate classifiers are then used to categorize the training data themselves as well as the unlabeled data. The confidently classified data by one classifier are added to other training datasets and the incorrectly classified ones are instead filtered out from the training datasets. The algorithmic performance of this iterative learning procedure is theoretically guaranteed.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process.

Though various popular machine learning techniques could be applied to learn the underlying search intents of the users, it is generally laborious or even impossible to collect sufficient and label high quality training data for such learning task [1]. Despite of the laborious human labeling efforts, many intuitive insights, which could be formulated as rules, can help generate small scale possibly biased and noisy training data. For example, to identify whether the users have intents to compare different products, several assumptions may help make the judgment. Generally, we may assume that if a user submits a query with explicit intent expression, such as "Canon 5d compare with Nikon D300", he/she may want to compare products. Though the rules satisfy the human common sense, there are two major limitations if we directly use them to infer ground truth. First, the coverage of each rule is often small and thus the training data may be seriously biased and insufficient. Second, the training data are usually not clean since no matter which rule we use, there may exist exceptions. In this paper, we propose a co-learning framework (CLF) for learning user intent from the rule-generated training data, which are possibly biased and noisy. The problem is,

Without laborious human labeling work, is it possible to train

user search intent classifier using the rule-generated training data,

which are generally noisy and biased? Given sets of rule-

generated training datasets , 1,2, ... , how to train the

classifier :

on top of these biased and noisy training data

sets with good performance?

General Terms
Algorithms, Experimentation
Keywords
User intent, search engine, classification.
1. INTRODUCTION
The classical relevance based search strategies may fail in satisfying the end users due to the lack of consideration on the real search intents of users. For example, when different users search with the same query "Canon 5D" under different contexts, they may have distinct intents such as to buy Canon 5D, to repair Canon 5D, etc. The search results about Canon 5D repairing obviously cannot satisfy the users who want to buy a Canon 5D camera. Learning to understand the true user intents behind the users' search queries is becoming a crucial problem for both Web search and behavior targeted online advertising.
Copyright is held by the author/owner(s). SIGIR'10, July 19-23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

2. THE CO-LEARNING FRAMEWORK

Suppose we have sets of rule-generated training data ,

1,2, ... , which are possibly noisy and biased, and a set of

unlabeled user behavioral data . Each data sample in the

training datasets is represented by a triple , ,

1,

1,2, ... | |, where stands for the feature vector of the data

sample in the training data , is its class label and | | is the

total number of training data in . On the other hand, each

unlabeled data sample, i.e. the user search session that could not

be covered by our rules, is represented as , ,

0,

1,2, ... | |. Suppose for any

, all the features constituting

the feature space are represented as a set

| 1,2, ... .

Suppose among all the features F, some have direct correlation to

the rules, that is they are used to generate the training dataset .

These features are denoted by

, which constitute a subset

of F. Let =

be the subset of features having no direct

correlation to the rules used for generating training dataset .

Given a classifier :

, where

is any subset of F, we

use to represent an untrained classifier and use to represent

the classifier trained by the training data . Suppose

|

895

means to train the classifier by training dataset using the

features

, we have

trained classifier , let

| , 1,2, ... . For the | stands for classifying

using features F. We assume for each output result of trained

classifier , it can output a confidence score. Let

|

,

where is the class label of assigned by and the is the corresponding confidence score.

After generating a set of training data , 1,2, ... , based on rules, we first train the classifier by , 1,2, ... ,
independently. Then we can get a set of K classifiers,

| , 1,2, ... .

Note that the reason why we use to train classifier on top of

instead of using the full set of features F is that is generated

from some rules correlated to , which may overfit the classifier

if we do not exclude them. After each classifier is trained

by , we use to classify the training dataset itself. A basic assumption of CLF is that the confidently classified instances by

classifier , 1,2, ... , have high probability to be correctly

classified. Based on this assumption, for any

, if the

confidence score of the classification is larger than a threshold, i.e. > and the class label assigned by the classifier is different

from the class label assigned by the rule, i.e.

, then

is considered as noise in the training data . Note that here

is the label of assigned by classifier, is its observed

class label in training data, and is the true class label, which is

not observed. We exclude it from and put it into the unlabeled dataset . Thus we update the training data by

,

.

Then we use the classifier ,

1,2, ... , to classify the

unlabeled data independently. Based on the same assumption

that the confidently classified instances by classifier have high probability to be correctly classified, for any data belonging to ,

if the confidence score of the classification is larger than a

threshold, i.e. > , where

|

, we

include into the training dataset. In other words,

,

, 1,2 ... , .

Through this way, we can gradually reduce the bias of the rulegenerated training data.

On the other hand, some unlabeled data are added into the training

datasets. Suppose the ,

1| is the probability of a

data sample to be involved in the training data at the iteration n

conditioned on this data sample is represented as a feature vector

and

1 is the probability of any data sample in D is

considered as a training data sample. It can be proved that after n

iterations using CLF, for each training dataset, we have

,

1|

1.

The remaining questions are when to stop the iteration and how to

train the classifier after iteration stops. In this work, we define the

iteration stopping criteria as "if |{ |

,

}| < n

or the number of iterations reaches N, then we stop the iteration".

After the iterations stop, we obtain K updated training datasets

with both noise and bias reduced. Finally, we merge all these

training datasets into one. Thus we can train the final classifier as

.

3. EXPERIMENTS
In this short paper, we utilize the real user search behavioral dataset, which comes from the search click-through log of a commonly used commercial search engine. It contains 3,420 user search sessions, in each of which, the user queries and clicked Web pages are all logged. Six labelers are asked to label the user intents according to the user behaviors as ground truth for results validation. We name this dataset as the "Real User Behavioral Data". The n-gram features are used for classification in the Bag of Words (BOW) model. One of the most classical evaluation metrics for classification problems, F1, which is a tradeoff between Precision (Pre) and recall (Rec) is used as the evaluation metric. For comparison purpose, we utilize several baselines to show the effectiveness of the proposed CLF. Firstly, since we can use different rules to initialize several sets of training data, directly utilizing one training dataset or the combination of all rule-generated training datasets to train the same classification model can give us a set of classifiers. Among them, we take the classifier with the best performance as the first baseline, referred to as "Baseline" in the remaining parts of this section. The second baseline is the DL-CoTrain algorithm, which is a variant of cotraining algorithm. It also starts from the rule-generated training data for classification and thus has the same experiments configuration as CLF. The classification method selected in CLF is the classical Support Vector Machine (SVM). In Table 3, we show the experimental results of CLF after 25 rounds of iterations compared with the baseline algorithms. From the results we can see that, in terms of F1, the CLF can improve the classification performance as high as 47% compared with the baseline.
Table 3. Results of CLF after 25 iterations

Baseline

DL-CoTrain

Pre

0.78

0.78

Rec

0.24

0.12

F1

0.36

0.21

CLF 0.81 0.39 0.53

4. CONCLUSION
One bottleneck of user search intent learning for Web search and online advertising is the laborious training data collection. In this paper, we proposed a co-learning framework (CLF), which aims to classify the users' search intents without laborious human labeling efforts. We firstly utilize a set of rules coming from the common sense of humans to automatically generate some initial training datasets. Since the rule-generated training data are generally noisy and biased, we propose to iteratively reduce the bias of the training data and control the noises in the training data. Experimental results on both real user search click data and public dataset show the good performance of the co-learning framework.

5. REFERENCE
[1] Russell, D.M., Tang, D., Kellar, M. and Jeffries, R. 2009. Task behaviors during web search: the difficulty of assigning labels. Proceedings of the 42nd Hawaii International Conference on System Sciences (Hawaii, United States, January 05 - 08, 2009). HICSS '09. IEEE Press, 1-5. DOI= 10.1109/HICSS.2009.417.

896

Learning the Click-Through Rate for Rare/New Ads from Similar Ads

Kushal Dave
Language Technologies Research Centre International Institute of Information Technology
Hyderabad, India
kushal.dave@research.iiit.ac.in

Vasudeva Varma
Language Technologies Research Centre International Institute of Information Technology
Hyderabad, India
vv@iiit.ac.in

ABSTRACT
Ads on the search engine (SE) are generally ranked based on their Click-through rates (CTR). Hence, accurately predicting the CTR of an ad is of paramount importance for maximizing the SE's revenue. We present a model that inherits the click information of rare/new ads from other semantically related ads. The semantic features are derived from the query ad click-through graphs and advertisers account information. We show that the model learned using these features give a very good prediction for the CTR values.
Categories and Subject Descriptors
I.2.6 [Computing Methodologies]: Artificial Intelligence-- Learning; I.6.5 [Computing Methodologies]: Simulation and Modeling--model development; H.3.3 [Information Systems]: Information Storage and Retrieval
General Terms
Algorithms, Economics, Experimentation.
Keywords
Sponsored Search, Click-Through Rate Prediction, Ranking
1. INTRODUCTION
Sponsored search can be seen as an interaction between three parties - SE, User and the Advertiser. The user issues a query to a SE related to the topic on which he/she seeks information. Advertisers and SEs try to exploit the immediate interest of user in the topic by displaying ads relevant to the query topic. Advertisers bid on certain keywords known as bid terms and their ads may get displayed based on the match between bid term and the user query. SEs try to rank the ads in a way that maximizes its revenue.
Search engines typically rank ads based on the expected revenue ( ad(Rev)). Expected revenue from an ad is a function of both bid and relevance: ad(Rev) = BidRelevancead. The relevace of an ad is measured using its CTR. The CTR of an ad for a query is the no. of clicks normalized by no. of impressions for that query. CTR of an ad is a function of both ad and the query, i.e. an ad can have a different CTR for different queries. The CTR value for an ad-query pair
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

is calculated form past click logs. For new/rare ads, we do not have any/sufficient past click data. Hence CTR for such ads need to be predicted so that they can be ranked along with other frequent ads. Richardson et. al. [5] predict the CTR based on ad text, ad quality etc. Fain et. al. [4] predict the CTR based on term clusters. We propose similarity features derived from click logs and advertisers hierarchy to accurately predict the CTR for new ads.

2. DATASET

The dataset used in our experiments comprised 12 days search log from Yahoo! search engine's US market. After removing redundant fields, each record in the dataset contained following fields: 1.Query 2.Term Id 3. Creative ID 4.Adgroup ID 5.Campaign ID 6.Account ID 8.CTR. Fields 2-6 point to a unique ad. Creative id points to the ad text. An ad text comprises bid term, title, abstract & display URL. The CTR values are normalized by removing the position & presentation bias. After some preprocessing, we got 1,447,543 unique query-ad pairs from the click through logs. It contained 1,97,080 unique queries and 9,43,431 unique ads. We randomly divide this dataset into 65-25-10 ratio for training, testing and validation respectively. We use Gradient boosted decision trees (GBDT) as a regression model [3]. Using validation set, the number of trees and no. of nodes parameters of GBDT were set to 600 and 150 respectively.

3. FEATURES

Features from Query-ad click graph: These features

are based on the semantic relations of the queries and ads

with other similar queries and ads. Regelson [4] have shown

that similar ads (bid terms in their case) follow similar CTR

distribution. The idea here is to learn the CTR values of

query-ad pair from semantically similar queries and ads.

We derive the semantic similarity from the query ad click-

through graph. The click graph is built from 12 days query

log (same period from which we generated our dataset).

Queries are represented as vectors and these query vectors

are compared to find similarity amongst the queries. A

query q is represented as a vector of transition probability

from q to all the ads in the graph. Edges are weighted us-

ing click frequency-inverse query frequency (CF-IQF) model:

cf iqf (qi, aj ) = cij  iqf (aj ).

The transition probability from a query to an ad, P (aj|qi) =

cf iqf (qi, aj)/cf iqf (qi). Each query is represented as q =

(P (a1|qi), P (a2|qi), · · · , P (an|qi)). The similarity between

two queries qi and qj is the cosine similarity between the

two query vectors. Sim(qi, qj) = Cosine

qi  qj qi · qj

. This

897

Table 1: Improvement for various features (p-value  0.01)

Feature
Baseline Sim-Q Sim-A Sim-QA Term Creative Adgroup

RMSE (1e-3)
7.20 5.86 6.31 5.68 6.24 6.51 5.87

KL Diver-gence (1e-1)
1.72 1.42 1.53 1.38 1.45 1.50 1.35

% Improvement
18.61% 12.36% 21.11% 13.34%
9.6% 18.48%

Feature
Campaign Account AdH
SimQA+Camp QADL
SimQA+Camp +QADL

RMSE (1e-3) 5.67 5.94 6.20 5.28 6.50
5.14

KL Diver-gence (1e-1)
1.32 1.39 1.46 1.24 1.56
1.21

% Improvement
21.25% 17.50% 13.9% 26.67% 9.72%
28.61%

similarity is used to predict the CTR for new query-ad pair by retrieving top k queries similar to q' and calculating the weighted average of the CTR values for all the ads over query q' as in [1]. Using query similarity,the CTR is estimated as:
X CT R(qk)  Sim(qi, qk)
QCT R(ai) = k X Sim(qi, qk)
k
The similarity between ads is also calculated in a similar fashion, with each ad being represented by the transition probability from ad to query P (qj|ai) and similarity between two ads is reffered as Sim(ai, ak). Using ad similarity, The CTR of is estimated as follows:
X CT R(ak)  Sim(ai, ak)
ACT R(ai) = k X Sim(ai, ak)
k
Along with QCTR/ACTR We also consider the number of similar queries/ads retrieved (Nq/Na). The Query and ad similarity featuers are called Sim-Q & Sim-A.
Figure 1: A typical Ad hierarchy
Features from Ad Hierarchy: Advertisements on an ad engine are typically maintained in some kind of a hierarchy. One such hierarchy is shown in Fig. 1. There are numerous reasons for maintaining ads in a hierarchy: (1)Advertiser's business may span various business units (BU). Ads from the same advertiser but from different BUs are maintained in different accounts. (2)For each BU, the advertisers can have ads on a range of products. Advertisements from the same account on similar products fall under the same Campaign. (3)Adgroups do further granular classification of ads. (4) Finally, an ad comprises a bid term and ad text (creative). Combination of these two makes an ad. We aggregate ads at each level viz. Term, Creative, Adgroup, Campaign and Account, compute the average within each group and use them as features in our model. In addition, number of featuers in each group are also taken as

features. We call these features as AdH features. Detailed explanation of all the features is available in [2].
Features from Query-ad lexical match: In an attempt to capture how relevant an ad is to the query, we compute the lexical overlap between the query and these ad units. We compute various text matching features such as cosine similarity, word overlap, character overlap, and string edit distance for each combination of unigrams and bi-grams. We refer to this category of features as QADL. For all the set of features we also consider log of each feature as a feature. In all we have 50 features.
As shown in Table 1, Sim-Q & Sim-A give good improvements and when combined (Sim-QA) give an improvement of 21.11%. In the AdH category, Campaign (Camp) gave the best result and when Sim-QA was clubbed with Camp the improvement over baseline reached 26.67%. Finally, lexical feature did not yeild much improvement alone, but (SimQA+Camp+QADL) give the best performance with a good 28.61% improvement over the baseline. All these improvements are staistically significant at 99% significance level.
When all the features were ranked according to the feature importance [3]. Features like Campaign, ACTR, log(ACTR), No. of ads in campaign were amongst the top few.
4. CONCLUSIONS
We have proposed an approach to predict the CTR for new ads based on the similarity with other ads/queries. The similarity of ads is derived from sources like query ad clickthrough graph and advertisement hierarchies maintained by the ad engine. The model gives good prediction on the CTR values of new ads. Analysis of the feature's contribution shows that the features derived from the ad hierarchy and from the click-through graphs contribute the most to the model followed by some of the word overlap features.
5. ACKNOWLEDGMENTS
We are grateful to Yahoo! labs Bangalore for granting access to the ad click-through logs.
6. REFERENCES
[1] T. Anastasakos, D. Hillard, S. Kshetramade, and H. Raghavan. A collaborative filtering approach to ad recommendation using the query-ad click graph. In CIKM '09, pages 1927­1930, 2009.
[2] K. Dave and V. Varma. Predicting the click-through rate for rare/new ads. Technical report IIIT/TR/2010/15, IIIT-H, 2010.
[3] J. H. Friedman. Stochastic gradient boosting. Comput. Stat. Data Anal., 38(4):367­378, 2002.
[4] M. Regelson and D. C. Fain. Predicting click-through rate using keyword clusters. In Electronic Commerce (EC). ACM, 2006.
[5] M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: estimating the click-through rate for new ads. In WWW '07, pages 521­530, 2007.

898

Graphical Models for Text: A New Paradigm for Text Representation and Processing

Charu C. Aggarwal
IBM T. J. Watson Research Center Hawthorne, New York, USA
charu@us.ibm.com

Peixiang Zhao
University of Illinois at Urbana-Champaign Urbana, Illinois, USA
pzhao4@uiuc.edu

ABSTRACT
Almost all text applications use the well known vector-space model for text representation and analysis. While the vector-space model has proven itself to be an effective and efficient representation for mining purposes, it does not preserve information about the ordering of the words in the representation. In this paper, we will introduce the concept of distance graph representations of text data. Such representations preserve distance and ordering information between the words, and provide a much richer representation of the underlying text. This approach enables knowledge discovery from text which is not possible with the use of a pure vector-space representation, because it loses much less information about the ordering of the underlying words. Furthermore, this representation does not require the development of new mining and management techniques. This is because the technique can also be converted into a structural version of the vector-space representation, which allows the use of all existing tools for text. In addition, existing techniques for graph and XML data can be directly leveraged with this new representation. Thus, a much wider spectrum of algorithms is available for processing this representation.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models
General Terms: Algorithms
1. INTRODUCTION
The most common representation for text is the vector-space representation. The vector-space representation treats each document as an unordered "bag-of-words". While the vector-space representation is very efficient because of its simplicity, it loses information about the structural ordering of the words in the document. For many applications, such an approach can lose key analytical insights. This is especially the case for applications in which the structure of the document plays a key role in the underlying semantics. The efficiency of the vector-space representation has been a key reason that it has remained the technique of choice for a variety of text processing applications. On the other hand, the vectorspace representation is very lossy because it contains absolutely no information about the ordering of the words in the document. One of the goals of this paper is to design a representation which retains at least some of the ordering information among the words in the document without losing its flexibility and efficiency for data processing.
While the processing-efficiency constraint has remained a strait-
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

jacket on the development of richer representations of text, this constraint has become easier to overcome in recent years. This is because of advances in the computational power of hardware, and the increased sophistication of algorithms in other fields such as graph mining, which can be leveraged with representations such as those discussed in this paper. In this paper, we will design graphical models for representing and processing text data. In particular, we will define the concept of distance graphs, which represents the document in terms of the distances between the distinct words. We will show that such a representation can retain much richer information about the underlying data. It also allows for the use of many current text and graph mining algorithms without the need for new algorithmic efforts for this new representation. In fact, we will see that the only additional work required is a change in the underlying representation, and all existing text applications can be directly used with a vector-space representation of the structured data. In some cases, it also enables distance-based applications which are not possible with the vector-space representations.
2. DISTANCE GRAPHS
While the vector-space representation maintains no information about the ordering of the words, the string representation is at the other end of spectrum in maintaining complete ordering information. Distance graphs are a natural intermediate representation which preserve a high level of information about the ordering and distance between the words in the document. At the same time, the structural representation of distance graphs make it an effective representation for easy processing. Distance graphs can be defined to be of a variety of orders depending upon the level of distance information which is retained. Specifically, distance graphs of order k retain information about word pairs which are at a distance of at most k in the underlying document. We define a distance graph as follows:
DEFINITION 1. A distance graph of order k for a document D in corpus C is defined as graph G(C, D, k) = (N (C), A(D, k)), where N (C) is the set of nodes defined specific to the corpus C, and A(D, k) is the set of edges in the document. The sets N (C) and A(D, k) are defined as follows:
(a) The set N (C) contains one node for each distinct word in the entire document corpus C. Therefore, we will use the term "node i" and "word i" interchangeably to represent the index of the corresponding word in the corpus. Note that the corpus C may contain a large number of documents, and the index of the corresponding word (node) remains unchanged over the representation of the different documents in C. Therefore, the set of nodes is denoted by N (C), and is a function of the corpus C. (b) The set A(D, k) contains a directed edge from node i to node j

899

MARY HAD A LITTLE LAMB, LITTLE LAMB, LITTLE LAMB, MARY HAD A LITTLE LAMB, ITS FLEECE WAS WHITE AS SNOW

REMOVE STOPWORDS
CREATE

MARY LITTLE LAMB, LITTLE LAMB, LITTLE LAMB, MARY LITTLE LAMB, FLEECE WHITE SNOW

Order 0:

DISTANCE GRAPH

2

4

4

1

1

1

MARY

LITTLE

LAMB

FLEECE

WHITE

SNOW

Order 1:

2 2
MARY
Order 2:
2 2
MARY

4

4

4 1

LITTLE

LAMB

2

1

6

4

6 1

LITTLE

LAMB

3

2

1 1
FLEECE
1 1
FLEECE 1

1 1
WHITE
1 1
1 WHITE

1 SNOW
1 SNOW

Figure 1: Illustration of Distance Graph Representation

if the word i precedes word j by at most k positions. For example, for successive words, the value of k is 1. The frequency of the edge is the number of times that word i precedes word j by at most k positions in the document.
We note that the set A(D, k) always contains an edge from each node to itself. The frequency of the edge is the number of times that the word precedes itself in the document at a distance of at most k. Since any word precedes itself at distance 0 by default, the frequency of the edge is at least equal to the frequency of the corresponding word in the document.
Most text collections contain many frequently occurring words (known as stop-words), which are typically filtered out before text processing. Therefore, it is assumed that these words are removed from the text before the distance graph construction. In other words, stop-words are not counted while computing the distances for the graph, and are also not included in the node set N (C). This greatly reduces the number of edges in the distance graph representation. This also translates to better efficiency during processing.
We note that the order-0 representation contains only self loops with corresponding word frequencies. Therefore, this representation is quite similar to the vector-space representation. Representations of higher orders provide structural insights of different levels of complexity. An example of the distance graph representation for a well-known nursery rhyme "Mary had a little lamb" is illustrated in Figure 1. In this figure, we have illustrated the distance graphs of orders 0, 1 and 2 for the text fragment. The distance graph is constructed only with respect to the remaining words in the document, after the stop-words have already been pruned. The distances are then computed with respect to the pruned representation. Note that the distance graphs or order 0 contain only self loops. The frequencies of these self-loops in the order-0 representation corresponds to the frequency of the word, since this is also the number of times that a word occurs within a distance of 0 of itself. The number of edges in the representation will increase for distance graphs of successively higher orders. Another observation is that the frequency of the self loops in distance graphs of order 2 increases over the order-0 and order-1 representations. This is because of repetitive words like "little" and "lamb" which occur within alternate positions of one another. Such repetitions do not change the self-loop frequencies of order-0 and order-1 distance graphs, but do affect the order-2 distance graphs. We note that distance graphs of higher orders may sometimes be richer, though this is not necessarily true for orders higher than 5 or 10. For example, a distance graph with order greater than the number of distinct words in the document would be a complete clique. Clearly, this does not necessarily encode useful information. On the other hand, distance graphs of order-0 do not encode a lot of useful information either.
From a database perspective, such distance graphs can also be

represented in XML with attribute labels on the nodes corresponding to word-identifiers, and labels on the edges corresponding to the frequencies of the corresponding edges. Such a representation has the advantage that numerous data management and mining techniques for semi-structured data have already been developed. These can directly be used for such applications. Distance graphs provide a much richer representation for storage and retrieval purposes, because they partially store the structural behavior of the underlying text data.
An important characteristic of distance graphs is that they are relatively sparse, and contain a small number of edges for low values of the order k. As we will see in the experimental results presented in [1], it suffices to use low values of k for effective processing in most mining applications.
PROPERTY 1. Let f (D) denote the number of words in document D (counting repetitions), of which n(D) are distinct. Distance graphs of order k contain at least n(D)·(k+1)-k·(k-1)/2 edges, and at most f (D) · (k + 1) edges.
The modest size of the distance graph is extremely important from the perspective of storage and processing. In fact, the above observation suggests that for small values of k, the total storage requirement is not much higher than that required for the vectorspace representation. This is a modest price to pay for the semantic richness captured by the distance graph representation.
One advantage of the distance-graph representation is that it can be used directly in conjunction with either existing text applications or with structural and graph mining techniques, as follows: (a) Use with existing text applications: Most of the currently existing text applications use the vector-space model for text representation and processing. It turns out that the distance graph can also be converted to a vector-space representation. The main property which can be leveraged to this effect is that the distance-graph is sparse and the number of edges in it is relatively small compared to the total number of possibilities. For each edge in the distancegraph, we can create a unique "token" or "pseudo-word". The frequency of this token is equal to the frequency of the corresponding edge. Thus, the new vector-space representation contains tokens only corresponding to such pseudo-words (including self-loops). All existing text applications can be used directly in conjunction with this "edge-augmented" vector-space representation. (b) Use with structural mining and management algorithms: The database literature has seen an explosion of management and mining techniques for graph and XML mining. Since our distancebased representation can be naturally expressed as a graph or XML document, such techniques can also be used in conjunction with the distance graph representation. The advantages of such approaches are that they are specifically tailored to graph data, and can therefore determine novel insights in the underlying word-distances.
Both of the above methods have different advantages, and work well in different cases. The former provides ease in interoperability with existing text algorithms whereas the latter representation provides ease in interoperability with recently developed structural mining methods. In [1], we have presented details of methods and corresponding experimental results for problems such as clustering, classification, similarity search and plagiarism detection. We illustrate advantages both in terms of accuracy and the enablement of distance-based applications.
3. REFERENCES
[1] C. Aggarwal, P. Zhao. Graphical Models for Text: A New Paradigm for Text Representation and Processing, IBM Research Report, 2010.

900

A Survival Modeling Approach to Biomedical Search Result Diversification

Xiaoshi Yin1,2, Jimmy Xiangji Huang 2, Xiaofeng Zhou 2, Zhoujun Li 1
1School of Computer Science and Technology, Beihang University, Beijing, China. 2School of Information Technology, York University, Toronto, Canada.
xiaoshiyin@cse.buaa.edu.cn; jhuang@yorku.ca; lizj@buaa.edu.cn

ABSTRACT
In this paper, we propose a probabilistic survival model derived from the survival analysis theory for measuring aspect novelty. The retrieved documents' query-relevance and novelty are combined at the aspect level for re-ranking. Experiments conducted on the TREC 2006 and 2007 Genomics collections demonstrate the effectiveness of the proposed approach in promoting ranking diversity for biomedical information retrieval. Categories and Subject Descriptors: H.3.3 [Informa-
tion Storage & Retrieval]: Information Search & Retrieval General Terms: Performance, Experimentation
Keywords: Survival Modeling, Diversity, Biomedical IR
1. INTRODUCTION
In the biomedical domain, the desired information of a question (query) asked by biologists usually is a list of a certain type of entities covering different aspects that are related to the question [6], such as genes, proteins, diseases, mutations, etc. Hence it is important for a biomedical information retrieval (IR) system to provide comprehensive and diverse answers to fulfill biologists' information needs. In the TREC 2006 and 2007 Genomics tracks, the "aspect retrieval" was investigated. Its purpose was to study how a biomedical IR system can support a user gather information about the different aspects of a topic. Aspects of a retrieved passage could be a list of named entities or MeSH terms, representing answers that cover different portions of a full answer to the query. Aspect Mean Average Precision (Aspect MAP) was defined in the Genomics tracks to capture similarities and differences among retrieved passages. It is a measurement for diversity of the IR ranked list [5].
The Genomics aspect retrieval was firstly proposed in the TREC 2006 Genomics track and further investigated in the 2007 Genomics track. However, to the best of our knowledge, there is not too much previous work conducted on the Genomics aspect retrieval for promoting diversity in the ranked list. University of Wisconsin re-ranked the retrieved passages using a clustering-based approach named GRASSHOPPER to promote ranking diversity [4]. Unfortunately, for the Genomics aspect retrieval, this re-ranking method hurt their system's performance and decreased the Aspect MAP of the original results [4]. Later in the TREC 2007 Genomics track, most teams tried to obtain the aspect
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

level performance through their passage level results, instead of working on the aspect level retrieval directly [3, 6].
In this paper, we first propose a survival modeling approach to measuring the novel information provided by an aspect with respect to its occurrences. Then, the relevance and novelty of a retrieved document are combined at the aspect level. Evaluation results show that the proposed approach is effective in biomedical search result diversification.
2. SURVIVAL MODELING AND ANALYSIS FOR MEASURING NOVELTY
Survival analysis is a statistical methodology used for modeling and evaluating survival data, also called time-to-event data, where one is interested in the occurrence of events [2]. Survival time refers to a variable which measures the time from a particular starting time to a particular endpoint of interest. Events are usually referred as birth, death and failure that happen to an individual in the context of study. For example, in clinical trial, one may interested in the number of days that patient can survive in the study of the effectiveness of a new treatment for a disease. Formally, the survival function is defined as:
S(t) = P r(surviving longer than time t) (1)
= P r(T > t)
where t is a specific time, T is a random variable denoting the time of death, and "Pr" stands for probability. That is, the survival function gives the probability that the time of death is later than a specified time t. The survival function must be non-increasing: S(u)  S(t) if u > t. Usually one assumes S(0) = 1, that is, at the start of the study, the probability of surviving past time zero is one. The survival function is also assumed to approach zero as t increases without bound [2].
In the context of information retrieval, aspects covered by a document can be considered as treatments, a document can be considered as a patient in the clinical trial case. The number of times that an aspect has been observed can be considered as the survival time. The new information that can be provided by an aspect corresponds to the effectiveness of a treatment. One can expect that, in a ranked list, as the number of times that an aspect has been observed increases, the new information that this aspect can provide to a document decreases. For example, in a ranked document list, when aspect "stroke treatment" is observed in the jth document at the first time, the information provided by this aspect should be counted as completely new. We presume that "stroke treatment" in the jth document covers the

901

topic of "medications taken by mouth for long-term stroke

Aspect mean average precision (MAP), since our objective

treatment". Then, when aspect "stroke treatment" is ob-

is to promote diversity in the IR ranked list.

served again in the kth (k > j) document of the ranked list,

Evaluation results of using the proposed approach for doc-

it may provide new information about "injection for short-

ument re-ranking are shown in Table 1. The values in the

term stroke treatment", but it is also possible that it only

parentheses are the relative rates of improvement over the

provides redundant information about "medications taken

original results. As we can see, our approach achieves promis-

by mouth for long-term stroke treatment". As we can see,

ing and consistent performance improvements over all base-

this situation satisfies the properties of the survival function

line runs. Performance improvements can be observed on

described above.

both levels of evaluation measures. It is worth mention-

We assume that the occurrences of an aspect follow Pois-

ing that our approach can further improve the best result

son distribution. Then, the survival model derived from

(NLMinter) reported in the TREC 2007 Genomics track by

Equation 1 can be formally written as:

x
Saj (x) = P r(X > x) = 1 - e-
i=0

i i!

(2)

where  is the rate parameter of Poisson distribution. The

achieving 18.9% improvement on Aspect MAP and 11% improvement on Passage2 MAP.

on 2007's topics

MAP

Aspect Passage2

on 2006's topics MAP Aspect Passage2

value of Saj (x) states the probability of obtaining new information from aspect aj(j = 1, 2, ..., n; where n is the number
of observed aspects) after it has been observed x times. Note

NLMinter 0.2631 SvvModel 0.3117
(+18.5%) MuMshFd 0.2068

0.1148 0.1270 (+10.6%) 0.0895

Okapi06a 0.2176 SvvModel 0.2379
(+9.3%) Okapi06b 0.3147

0.0450 0.0472 (+4.9%) 0.0968

that in this paper, the aspects covered by a retrieved document are presented by concepts detected from Wikipedia [9].

SvvModel 0.2432 (+17.6%)
Okapi07 0.1428

0.0926 SvvModel 0.3236

(+3.5%)

(+2.8%)

0.0641 Okapi06c 0.2596

0.1009 (+4.2%)
0.0601

SvvModel 0.1660

0.0669 SvvModel 0.2697

0.0624

3. COMBINING NOVELTY AND RELEVANCE

(+16.2%) (+4.4%)

(+3.9%) (+3.8%)

For retrieved documents, the document rankings should depend on which documents the user has already seen. Suppose that we have ranked top i - 1 documents, and now we need to decide which document should be ranked at the ith position in the ranking list. The document that can deliver the most new and relevant aspects should be considered as the ith document in the ranking list. Assume that aspect novelty and aspect query-relevance are independent of each other. Then given previous ranked i - 1 documents, we rank the ith document using the following scoring function:

score(di; d1, ..., di-1) = P (N ew and Rel|di)

=

P (N ew and Rel|aj )P (aj )

aj Adi



P (N ew|aj )P (aj |Rel)

(3)

aj Adi

where aj is an aspect detected from document di, which follows Poisson distribution with an estimated rate parameter. P (N ew and Rel|aj) denotes the probability that aj is query-relevant and can provide new information as well.
P (N ew|aj) in Equation 3 states the probability of obtaining new information from aspect aj, which can be calculated using the survival models proposed in Section 2. P (N ew|aj) = 1 when i = 1. Since we do not usually have relevance information, P (aj|Rel) is unavailable. One possible solution, as introduced in [7], is to consider that
the best bet is to relate the probability of aspect aj to the conditional probability of observing aj given the query: P (aj|Rel)  P (aj|Q). P (aj|Q) can be calculated by the two-stage model presented in [9].

4. EXPERIMENTAL RESULTS
We conduct a series of experiments to evaluate the effectiveness of the proposed model on the TREC 2006 and 2007 Genomics collections. For the 2007's topics, three baseline runs are used, which are NLMinter [3], MuMshFd [8] and an Okapi run [1]. NLMinter and MuMshFd were two of the most competitive IR runs submitted to the TREC 2007 Genomics track. For 2006's topics, we test our approach on three Okapi runs. In this paper, we mainly focus on the

Table 1: Re-ranking Performance on Genomics topics
5. CONCLUSIONS
In this paper, we propose a survival modeling approach to promoting ranking diversity for biomedical information retrieval. The probabilistic survival model derived from the survival analysis theory measures the probability that novel information can be provided by an aspect with respect to its occurrences. Experimental results show that the proposed survival model can successfully capture the novel information delivered by aspects and achieve significant improvements on ranking diversity. We also show that combining the novelty and the relevance of a retrieved document at the aspect level is an effective way of promoting diversity of the ranked list, while keeping the relevance of retrieved documents. The proposed approach not only achieves promising performance improvements on the diversity based evaluation measure, but also on the relevance based evaluation measure.
Acknowledgements
This work is supported by NSERC Discovery Grant and an Early Researher Award of Ontario.
6. REFERENCES
[1] M. Beaulieu, M. Gatford, X. Huang, S. Robertson, S. Walker, and P. William. Okapi at TREC-5. In Proc. of TREC-5, 1997.
[2] D. Cox and D. Oakes. Analysis of Survival Data. Chapman Hall. [3] D. Demner-Fushman and et. al. Combining resources to find
answers to biomedical questions. In Proc. of TREC-16, 2007. [4] A. B. Goldberg and et. al. Ranking biomedical passages for
relevance and diversity: University of Wisconsin, Madison at TREC Genomics 2006. In Proc. of TREC-15, 2006. [5] W. Hersh, A. Cohen, P. Roberts, and H. Rekapalli. TREC 2006 Genomics track overview. In Proc. of TREC-15, 2006. [6] W. Hersh, A. Cohen, L. Ruslen, and P. Roberts. TREC 2007 Genomics track overview. In Proc. of TREC-16, 2007. [7] V. Lavrenko and W. B. Croft. Relevance based language models. In Proceeding of the 24th ACM SIGIR. [8] N. Stokes and et. al. Entity-based relevance feedback for genomic list answer retrieval. In Proc. of TREC-16, 2007. [9] X. Yin, X. Huang, and Z. Li. Promoting ranking diversity for biomedical information retrieval using wikipedia. In Proc. of the 32nd ECIR, 2010.

902

Low Cost Evaluation in Information Retrieval
Ben Carterette
University of Delaware, carteret@cis.udel.edu
Evangelos Kanoulas
University of Sheffield, e.kanoulas@sheffield.ac.uk
Emine Yilmaz
Microsoft Research Cambridge, eminey@microsoft.com

Abstract:
Search corpora are growing larger and larger: over the last 10 years, the IR research community has moved from the several hundred thousand documents on the TREC disks to the tens of millions of U.S. government web pages of GOV2 to the one billion general-interest web pages in the new ClueWeb09 collection. But traditional means of acquiring relevance judgments and evaluating ­ e.g. pooling documents to calculate average precision ­ do not seem to scale well to these new large collections. They require substantially more cost in human assessments for the same reliability in evaluation; if the additional cost goes over the assessing budget, errors in evaluation are inevitable.
Some alternatives to pooling that support low-cost and reliable evaluation have recently been proposed. A number of them have already been used in TREC and other evaluation forums (TREC Million Query, Legal, Chemical, Web, Relevance Feedback Tracks, CLEF Patent IR, INEX). Evaluation via implicit user feedback (e.g. clicks) and crowdsourcing have also recently gained attention in the community. Thus it is important that the methodologies, the analysis they support, and their strengths and weaknesses are well-understood by the IR community. Furthermore, these approaches can support small research groups attempting to start investigating new tasks on new corpora with relatively low cost. Even groups that do not participate in TREC, CLEF, or other evaluation conferences can benefit from understanding how these methods work, how to use them, and what they mean as they build test collections for tasks they are interested in.
The goal of this tutorial is to provide attendees with a comprehensive overview of techniques to perform low cost (in terms of judgment effort) evaluation. A number of topics will be covered, including alternatives to pooling, evaluation measures robust to incomplete judgments, evaluating with no relevance judgments, statistical inference of evaluation metrics, inference of relevance judgments, query selection, techniques to test the reliability of the evaluation and reusability of the constructed collections.
The tutorial should be of interest to a wide range of attendees. Those new to the field will come away with a solid understanding of how low cost evaluation methods can be applied to construct inexpensive test collections and evaluate new IR technology, while those with intermediate knowledge will gain deeper insights and further understand the risks and gains of low cost evaluation. Attendees should have a basic
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

knowledge of the traditional evaluation framework (Cranfield) and metrics (such as average precision and nDCG), along with some basic knowledge on probability theory and statistics. More advanced concepts will be explained during the tutorial.
ACM Categories & Descriptors:
H.3.4 Information Storage and Retrieval; Performance evaluation (efficiency and effectiveness)
General Terms:
Experimentation, Measurement
Keywords:
information retrieval, evaluation, test collections
Bios
Ben Carterette is an Assistant Professor of Computer and Information Sciences at the University of Delaware, where he teaches Information Retrieval, Databases, and Artificial Intelligence. He has published extensively on constructing and using test collections for low cost. He has co-organized two SIGIR workshops on test collections that go beyond binary independent relevance judgments, and co-coordinated the TREC Million Query track from 2007­2009 as well as the TREC Session track in 2010. He is also co-organizing a SIGIR workshop on constructing the next generation of information retrieval test collections.
Evangelos Kanoulas is a postdoctoral researcher (Marie Curie fellow) in the Department of Information Studies at the University of Sheffield. He received his Ph.D. from Northeastern University. His main research interest is evaluation methods for information retrieval. He has published papers in SIGIR and CIKM. Further, Evangelos was actively involved in coordinating the Million Query Track in TREC 2007 ­ 2009 and he is one of the co-coordinators of the TREC 2010 Session Track.
Emine Yilmaz is a postdoctoral researcher in the Information Retrieval and Analysis Group at Microsoft Research Cambridge. She obtained her Ph.D. from Northeastern University. Most of her current work involves evaluation of retrieval systems, the effect of evaluation metrics on learning to rank problems and modeling user behavior. Her main interests are information retrieval and applications of information theory, statistics and machine learning. She has previously published research papers at major information retrieval venues, including SIGIR, CIKM and served as one of the organizers of the ICTIR Conference in 2009. She is also one of the organizers of a SIGIR workshop on crowdsourcing.

903

904

Learning to Rank for Information Retrieval
Tie-Yan Liu,
Microsoft Research Asia, tyliu@microsoft.com

Abstract: This tutorial is concerned with a comprehensive introduction to the research area of learning to rank for information retrieval. In the first part of the tutorial, we will introduce three major approaches to learning to rank, i.e., the pointwise, pairwise, and listwise approaches, analyze the relationship between the loss functions used in these approaches and the widely-used IR evaluation measures, evaluate the performance of these approaches on the LETOR benchmark datasets, and demonstrate how to use these approaches to solve real ranking applications. In the second part of the tutorial, we will discuss some advanced topics regarding learning to rank, such as relational ranking, diverse ranking, semi-supervised ranking, transfer ranking, query-dependent ranking, and training data preprocessing. In the third part, we will briefly mention the recent advances in statistical learning theory for ranking, which explain the generalization ability and statistical consistency of different ranking methods. In the last part, we will conclude the tutorial and show several future research directions.
ACM Categories & Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models; I.2.6 [Learning]: Parameter learning.

General Terms: Algorithms, Experimentation, Theory.
Keywords: Learning to Rank, Information Retrieval, Learning Theory, Ranking Models.
Bio: Tie-Yan Liu is a lead researcher at Microsoft Research Asia. He leads a team working on learning to rank for information retrieval, and large-scale graph learning. So far, he has more than 70 quality papers published in referred conferences and journals and over 50 filed US / international patents or pending applications. He is the co-author of the Best Student Paper for SIGIR 2008, and the Most Cited Paper for the Journal of Visual Communication and Image Representation (2004~2006). He is a Program Committee Co-Chair of RIAO (2010), an Area Chair of SIGIR (2008~2010) and AIRS (2009~2010), a Co-Chair of SIGIR workshop on learning to rank for IR (2007~2009), a Co-Chair of ICML workshop on learning to rank (2010), and a Program Committee member of many other international conferences such as WWW, ICML, KDD, WSDM, and ACL. He is on the Editorial Board of the Information Retrieval Journal (IRJ), and is a guest editor of the special issue on learning to rank of IRJ. He has given tutorials on learning to rank at several conferences including SIGIR 2008, WWW 2008, and WWW 2009.

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.
904

Estimating the Query Difficulty for Information Retrieval
David Carmel, Elad Yom-Tov
IBM Research lab in Haifa, {carmel,yomtov}@il.ibm.com

Abstract:
Many information retrieval (IR) systems suffer from a radical variance in performance when responding to users' queries. Even for systems that succeed very well on average, the quality of results returned for some of the queries is poor. Thus, it is desirable that IR systems will be able to identify "difficult" queries in order to handle them properly. Understanding why some queries are inherently more difficult than others is essential for IR, and a good answer to this important question will help search engines to reduce the variance in performance, hence better servicing their customer needs.
The high variability in query performance has driven a new research direction in the IR field on estimating the expected quality of the search results, i.e. the query difficulty, when no relevance feedback is given. Estimating the query difficulty is a significant challenge due to the numerous factors that impact retrieval performance. Many prediction methods have been proposed recently. However, as many researchers observed, the prediction quality of state-of-the-art predictors is still too low to be widely used by IR applications. The low prediction quality is due to the complexity of the task, which involves factors such as query ambiguity, missing content, and vocabulary mismatch.
The goal of this tutorial is to expose participants to the current research on query performance prediction (also known as query difficulty estimation). Participants will become familiar with states-of-the-art performance prediction methods, and with common evaluation methodologies for prediction quality. We will discuss the reasons that cause search engines to fail for some of the queries, and provide an overview of several approaches for estimating query difficulty. We then describe common methodologies for evaluating the prediction quality of those estimators, and some experiments conducted recently with their prediction quality, as measured over several TREC benchmarks. We will cover a few potential applications that can utilize query difficulty estimators by handling each query individually and selectively based on its estimated difficulty. Finally we will summarize with a discussion on open issues and challenges in the field.
ACM Categories & Descriptors:
H.3.3 [Information Search
and Retrieval]: Retrieval models

General Terms:
Algorithms, Measurement
Keywords:
Retrieval robustness, Query difficulty estimation, Performance prediction
Bio/Bios:
David Carmel is a Research Staff Member at the Information Retrieval group at IBM Haifa Research Laboratory. David earned his PhD in Computer Science from the Technion, Israel Institute of Technology in 1997. David's research is focused on search in the enterprise, query performance prediction, social search, and text mining. For several years David taught the Introduction to IR course at the CS department at Haifa University.
At IBM, David is a key contributor to IBM enterprise search offerings. David is a co-founder of the Juru search engine which provides integrated search capabilities to several IBM products, and was used as a search platform for several studies in the TREC conferences. David has published more than 60 papers in Information retrieval and Web journals and conferences, and serves in the Program Committee of many conferences (SIGIR, WWW, WSDM, CIKM, ECIR) and workshops.
Elad Yom-Tov is a Research Staff Member at the Analytics Department at the IBM Haifa Research Laboratory. The main focus of his work is research into methods for large-scale machine learning, with a recent focus on social analytics. Prior to his current position he worked at Rafael Inc., where he applied machine learning to image processing. Elad is a graduate of Tel-Aviv University (B.Sc.) and the Technion, Haifa (M.Sc. and Ph.D). He is the author (with David Stork) of the Computer Manual to accompany Pattern classification, a book and a Matlab toolbox on pattern classification.
Elad's work in Information Retrieval includes query difficulty estimation, social tagging, and novelty detection.
Both David and Elad published many papers on query performance prediction, and organized a workshop on this subject in SIGIR 2005. Their paper on learning to estimate query difficulty won the Best Paper Award at SIGIR 2005.

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.
911

Learning Hidden Variable Models for Blog Retrieval
Mengqiu Wang
Computer Science Department Stanford University
Stanford, CA 94305, USA
mengqiu@cs.stanford.edu

ABSTRACT
We describe probabilistic models that leverage individual blog post evidence to improve blog seed retrieval performances. Our model offers a intuitive and principled method to combine multiple posts in scoring a whole blog site by treating individual posts as hidden variables. When applied to the seed retrieval task, our model yields state-of-the-art results on the TREC 2007 Blog Distillation Task dataset.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Retrieval Models

General Terms
Design, Algorithms, Experimentation, Performance

Keywords
Learning to Rank, Passage Retrieval, Blog Retrieval

1. INTRODUCTION
In blog seed retrieval tasks, we are interested in finding blogs with relevant and recurring interests for given topics. Rather than ranking individual blog posts, whole sites are ranked (i.e. all posts within a blog). We propose two discriminatively trained probabilistic models that model individual posts as hidden variables.

2. PROBABILISTIC PASSAGE MODELS

We make a modeling assumption that given a set of topranked passages of a document, the document is relevant if any one of the passages is relevant.
The first independent model (IND) assumes that the relevance of a specific top-ranked passage si is independent of the relevance of any other passage in s. We use the logistic function to model the relevance of a passage. Our second model (RBM) takes a step further and exploit the correlations among individual passages in a Restricted Boltzmann Machine framework.

P (z = 0|s) = e-f(s)
1+e-f (s)

P (z^|s))

=

1 Z

exp(Pi<j

f

(si

,

sj

,

zi,

zj

)

+

P|s|
i=1

g(si

,

zi

))

Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.

where f (s) is a feature vector of the passage s, and  is
the corresponding weight vector. Z is the partition function computed by summing over all possible relevance assignments. f (si, sj, zi, zj) are passage correlation features (cosine-sim, URL overlapping) and g(si, zi) are passage relevance feature (e.g., rank, score).

3. BLOG SEED RETRIEVAL
We evaluated our models on TREC 2007 Blog Distillation Track dataset. We would first obtain top 5 ranked passages for each document using Indri's LM-based retrieval system, and then apply our model to re-rank each document. Training and testing is done by performing 5-fold cross-validation. We compare our models with four strong baselines. The first two are the Indri language model passage and document retrieval systems (Indri-psg, Indri-doc). The third one is the CMU system, which gives the best performance in TREC 2007 and 2008 evaluations [1]. The last one is the ReDDE federated search algorithm used in [2]. Our IND model showed significant improvements over the Indri passage and document retrieval baselines (58.5% and 9.4% relative improvements). The RBM model gained a small improvement over the IND model, and significantly outperformed the baseline CMU and ReDDE models.

Baseline

This work

Indri-psg 0.2267

IND

0.3596

Indri-doc 0.3284

RBM

0.3702

CMU 0.3385 RBM+cosine sim 0.3779

ReDDE 0.3150 RBM+url 0.3685

4. CONCLUSIONS
In this paper, we introduced two probabilistic models that model individual blog posts as hidden variables for blog seed retrieval tasks. Our models produced state-of-the-art results on TREC 2007 Blog Distillation dataset.
5. REFERENCES
[1] J. Elsas, J. Arguello, J. Callan, and J. Carbonell. Retrieval and feedback models for blog distillation. In Proceedings of TREC, 2007.
[2] J. Elsas, J. Arguello, J. Callan, and J. Carbonell. Retrieval and feedback models for blog feed search. In Proceedings of SIGIR, 2008.

922

Investigation on Smoothing and Aggregation Methods in Blog Retrieval

Mostafa Keikha
Faculty of Informatics, University of Lugano Lugano, Switzerland
mostafa.keikha@usi.ch

ABSTRACT
Recently, user generated data is growing rapidly and becoming one of the most important source of information in the web. Blogosphere (the collection of blogs on the web) is one of the main source of information in this category. In my work for my PhD, I mainly focussed on the blog distillation task which is: given a user query find the blogs that are most related to the query topic [3].
There are some properties of blogs that make blog analysis different from usual text analysis. One of these properties is related to the time stamp assigned to each post; it is possible that the topics of a blog change over the time and this can affect blog relevance to the query. Also each post in a blog can have viewer generated comments that can change the relevance of the blog to the query if these are considered as part of the content of the blog. Another property is related to the meaning of the links between blogs which are different than links between websites. Finally, blog distillation is different from traditional ad-hoc search since the retrieval unit is a blog (a collection of posts), instead of a single document. With this view, blog distillation is similar to the task of resource selection in federated search [1].
Researchers have applied different methods from similar problems to blog distillation like ad-hoc search methods, expert search algorithms or methods from resource selection in distributed information retrieval.
Based on our preliminary experiments, I decided to divide the blog distillation problem into two sub-problems. First of all, I want to use mentioned properties of blogs to retrieve the most relevant posts for a given query. This part is very similar to the ad hoc retrieval. After that, I want to aggregate relevance of posts in each blog and calculate relevance of the blog. This part requires the development of a crossmodal aggregation model that combines the different blog relevance clues found in the blogosphere.
We use structure based smoothing methods for improving posts retrieval. The idea behind these smoothing methods is to change the score of a document based on the score of its similar or related documents. We model the blogosphere as a single graph that represents relations between posts and terms [2]. The idea is that in accordance with the Clustering Hypothesis, related documents should have similar scores for the same query. To model the relatedness between posts, we
Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland ACM 978-1-60558-896-4/10/07.

define a new measure which takes into account both content similarity and temporal distance.
In more recent work, in the aggregation part of the problem, we model each post as evidence about relevance of a blog to the query, and use aggregation methods like Ordered Weighted Averaging operators to combine the evidence. The ordered weighted averaging operator, commonly called OWA operator, was introduced by Yager [4]. OWA provides a parametrized class of mean type aggregation operators, that can generate OR operator (M ax), AN D operator (M in) and any other aggregation operator between them.
For the next steps, I'm thinking about capturing the temporal properties of the blogs. Bloggers can change their interests over the time or write about different topics periodically. Capturing these changes and using them in the retrieval is one the future woks that I'm interested in. Also, studying the relations between blogs and news and their effect on each other is an interesting problem.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms
Keywords
Blog search, Temporal analysis, User generated data.
1. REFERENCES
[1] J. L. Elsas, J. Arguello, J. Callan, and J. G. Carbonell. Retrieval and feedback models for blog feed search. In SIGIR, pages 347­354, 2008.
[2] M. Keikha, M. J. Carman, and F. Crestani. Blog distillation using random walks. In SIGIR, pages 638­639, 2009.
[3] I. Ounis, M. De Rijke, C. Macdonald, G. Mishne, and I. Soboroff. Overview of the TREC-2006 blog track. In Proceedings of TREC, pages 15­27, 2006.
[4] R. Yager. On ordered weighted averaging aggregation operators in multicriteria decision making. IEEE Trans. Syst. Man Cybern., 18(1):183­190, 1988.

923

A Joint Probabilistic Classification Model for Resource Selection


Dzung Hong , Luo Si
Department of Computer Science Purdue University
250 N. University Street West Lafayette, IN 47907, USA
{dthong, lsi}@cs.purdue.edu

Paul Bracke, Michael Witt
Purdue University Libraries Purdue University
504 West State Street West Lafayette, IN 47907, USA
{pbracke,mwitt}@purdue.edu

Tim Juchcinski
Department of Computer Science Purdue University
250 N. University Street West Lafayette, IN 47907, USA
tjuchcin@purdue.edu

ABSTRACT
Resource selection is an important task in Federated Search to select a small number of most relevant information sources. Current resource selection algorithms such as GlOSS, CORI, ReDDE, Geometric Average and the recent classificationbased method focus on the evidence of individual information sources to determine the relevance of available sources. Current algorithms do not model the important relationship information among individual sources. For example, an information source tends to be relevant to a user query if it is similar to another source with high probability of being relevant. This paper proposes a joint probabilistic classification model for resource selection. The model estimates the probability of relevance of information sources in a joint manner by considering both the evidence of individual sources and their relationship. An extensive set of experiments have been conducted on several datasets to demonstrate the advantage of the proposed model.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms
Algorithms, Design, Performance
Keywords
Federated Search, Resource Selection, Joint Classification
1. INTRODUCTION
Federated text search provides a unified search interface for multiple search engines of distributed text information sources. There are three major research problems in federated search as resource representation, resource selection
Vietnam Education Foundation Fellow
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

and results merging. This paper focuses on resource selection, which selects a small number of most relevant information sources to search for any particular user query.
Resource selection for federated search has been a popular research topic in the last two decades. Many methods treat each information source as a single big document and rank available sources either by using statistics from the sample documents (CORI [6]), or by building a language model for each source (Xu and Croft [27], Si and Callan [22]). Other methods such as GlOSS [12], Geometric Average [17], ReDDE [20], CRCS [18] and SUSHI [24] look further inside an information source by estimating the relevance of each document and calculate the source's score as an aggregate function of the documents that the source contains. More recent methods such as the classification-based method [1] and the work in [2] treat resource selection as a classification problem and build probabilistic models by combining multiple types of evidence of individual sources.
Existing resource selection methods judge an information source by its own characteristics, but miss an important piece of evidence, which is the relationship between available sources. In practice, we notice that relationship can be meaningful and indicative. An information source that is "similar" to another highly relevant source has a better chance of being relevant. The evidence of source relationship can be very valuable for real world federated search solutions. In particular, the resource representation (e.g., sample documents) of each information source is often limited and prevents resource selection algorithms from identifying relevant sources, while the relationship between sources can help to alleviate the problem by providing more evidence from similar sources. Our study of a real world federated search application with digital libraries also suggests that it is difficult to obtain thorough resource representation from many sources (i.e., digital libraries). For example, some sources may only provide the abstracts of its documents instead of the full texts.
This paper proposes a novel probabilistic discriminative model for resource selection that explicitly models the relationship between information sources. In particular, the new research combines both the evidence of individual sources and the relationship evidence between sources into a single probabilistic model for estimating the joint probability of relevance of a set of sources. Different similarity metrics have been studied to explore the relationship between information sources. An extensive set of experiments have been conducted on two TREC testbeds for federated search

98

research and one real world application for searching digital libraries. The experiment results demonstrate the effectiveness and robustness of the proposed resource selection algorithm with the joint classification model.
The rest of this paper is organized as follows: the next section discusses the related research work. Section 3 presents the classification model for resource selection. Section 4 proposes the joint classification model. Section 5 discusses experimental methodology. Section 6 presents experimental results and related discussions. Section 7 concludes and points out some future research work.
2. RELATED WORK
There has been considerable research on all of the three subtasks of federated search as resource representation, resource selection and results merging. Since this paper focuses on the resource selection task, we mainly survey most related prior research work in resource selection and briefly talk about resource representation and results merging.
The first step in federated search is to obtain representative resource descriptions from available information sources. The START protocol [11] provides accurate information in collaborative federated search environments, but it does not work for uncooperative environments. On other side, the query-based sampling technique [4] has been widely used in federated search to obtain sample documents from each source by issuing randomly generated queries. In particular, the query-based sampling approach is used in this work to acquire sample documents from available sources. After that, all sample documents are merged together as a centralized sample database.
Resource selection selects a small set of most relevant sources for each user query [3][8][13]. Most early resource selection algorithms treat each individual source as a single big document which they extract summary statistics from. Those big document methods such as KL [27], CORI [6], and CVV [28] utilize different types of summary statistics of sources and finally rank available sources by matching the statistics with the user's query. These methods ignore the boundaries of individual documents within individual sources, which limits their performance of identifying sources with a large number of relevant documents.
Some recent resource selection algorithms such as ReDDE [20], DTF [9][10], CRCS [18] and SUSHI [24] step away from treating each source as a single big document. Those algorithms often analyze individual sample documents within resource representation for ranking sources. For example, the ReDDE selection algorithm estimates the distribution of relevant documents by treating top-ranked sample documents as a representative subset of relevant documents in available sources. Related algorithms such as UUM [22], RUM [23] and CRCS [18] have been proposed, which use different methods to weight top-ranked documents and estimate the probability of relevance.
More recent resource selection algorithms such as the classification-based resource selection in federated search [1] or vertical search [2] treat resource selection as a classification problem. A classification model can be learned from a set of training queries and is used to predict the relevance of a source for test queries. It has been shown [1] that the classification approach can outperform state-of-the-art resource selection algorithms like ReDDE.
Existing resource selection methods utilize evidence within

individual sources to judge their relevance but ignore the evidence of the relationship between available sources. However, the relationship evidence is a valuable piece of information, which promises to improve the accuracy of resource selection.
Two other related research work in [25][7] learn from the results of past queries for resource selection. However, these two methods do not model the relationship between information sources and do not use formal models based on classification.
The last step of federated search is results merging, which merges returned documents from selected sources into a single list. The most effective method is to download and recalculate scores for all returned documents within a centralized retrieval model, but this is often inefficient. More efficient methods such as the CORI merging formula, the SSL [21] and the SAFE merging algorithms [19] try to approximate the results of centralized retrieval in different ways.
3. CLASSIFICATION MODEL
3.1 Classification Approach
Many resource selection algorithms are unsupervised and provide one source of evidence. To combine different evidence in a unified framework, one needs a training dataset, usually in the form of binary judgments on sources. Specifically, given a set of sources C and a set of training queries Q, the objective is to find a mapping F of the form
F : Q × C  {+1, -1}
where +1 indicates the relevance between the query and the source, and -1 indicates irrelevance.
Arguello et al.[1] have proposed a method to construct those judgments. Each query q  Q will be issued to a fulldataset index for searching. A source Ci  C is considered to be relevant with q if more than  documents from Ci are present in top T of the full-dataset result. Otherwise, it is marked as irrelevant.
While this method can produce a rank list that mimics the rank list produced by a full-dataset retrieval, it is difficult to apply in a real world environment because of the absence of a full-dataset. We propose an alternative method that could be more feasible. A query q is now issued to each remote source Ci and we only count their returned documents that are relevant. Top T documents from each source will be inspected, then a source is marked as relevant if it has more than  relevant documents presenting in that list. In our work, we set T = 100. For dataset with a large average number of relevant documents per query (over 100), we set  = 3; otherwise  is equal to 1.
3.2 Sources of Evidence
This section presents different types of evidence of individual sources for building our classification model.
3.2.1 Big Document
Big Document (BIGDOC) approach treats each information source as a big document that contains all of its sample documents. A query is then issued to an index which contains a set of big documents, each representing one source. Sources are then ranked by how their merged sample documents match the query. The disadvantage of this method is that it does not take into account the variation of sources'

99

sizes. Assuming that the sampling process is uniform, for a very big source, the sampling process only covers a small fraction of its documents. Therefore, it may present fewer relevant documents in the centralized sample database than a much smaller one, although the absolute number of relevant documents in the big source is higher. Without considering the sources' sizes, it would be misleading to conclude that the small source is the better choice. Nevertheless, when combined with other features, BIGDOC could have a good contribution, especially in the case that many sources contain roughly the same number of documents. While CORI (discussed in the next part) also treats each source as one document, BIGDOC approach is more flexible since it can be used with different retrieval algorithms. In our experiments, the algorithm is Indri [14]. For each pair of a query and a source, one BIGDOC feature is built from the sample documents.

3.2.2 CORI
The CORI resource selection algorithm [6] uses Bayesian Inference Network model to rank sources. The belief P (q|Ci) that a source Ci satisfies query q is the combination of multiple P (rj|Ci), the belief corresponding to each term rj of query q. CORI applies a variant of tf.idf formula to determine each P (rj|Ci) and combine them together to calculate the final belief score of each source. CORI was proven to have robust performance for resource selection. In our experiments, one CORI feature is used for each pair of a query and a source.

3.2.3 Geometric Average

In this method, a query is first issued to a centralized sample database, which was mentioned in section 2. Then, each source Ci is scored according to the geometric average query likelihood of its top K sample documents [17],

  K

1 K

GAV Gq(Ci) =

P (q|dij )

j=1

where dij is the j-th sample document in the rank list of source Ci. If Ci presents less than K documents in the rank list, the product above is padded with the minimum query likelihood score.

3.2.4 Modified ReDDE & ReDDE.top
Recall that ReDDE score [20] is calculated according to :

ReDDEq (Ci )

=

Niest Nisamp



×

I (d

dRsNamp



Ci) × Pq(rel|d)

where RsNamp is the top N documents returned from searching the centralized sample database. Niest is the estimated size of source Ci, Nisamp is the sample size of Ci, and I(.) is the indicator function. The number of top returned documents, N , is equal to  × Naelslt, where Naelslt is the estimated total number of documents of all sources and  is a constant,
which is usually in the range 0.002-0.005.
ReDDE uses a step function to estimate Pq(rel|d), the probability that the document d is relevant to query q. For
all top N documents, that probability is equal to a constant.
In our experiment, we use a modified version of ReDDE,
which replaces Pq(rel|d) by P (q|d), the retrieval score of document d with respect to query q. The Indri retrieval

algorithm [14] is used for searching the centralized sample database. The modified ReDDE feature has been shown empirically better than the original ReDDE feature. There is one modified ReDDE feature for each pair of a query and a source.
ReDDE.top [1] is another variant of ReDDE. Unlike ReDDE, ReDDE.top set a specific number to N . In our experiment, we add another two ReDDE.top features with N = 100 and N = 1000 respectively.

4. JOINT CLASSIFICATION MODEL

4.1 Probabilistic Discriminative Model
We propose a novel joint probabilistic model for the resource selection task. First of all, a logistic model is built to combine all the features of individual sources. We refer to this model as the independent model (Ind).
Let v = {v1, ..., vn} be the relevance vector. vi = 1 indicates that the i-th source is relevant, otherwise vi = 0. The relevance probability of a source ci given its feature vector f(ci) is calculated as:

P (vi

=

1|ci)

=

1

exp(f(ci) · ) + exp(f(ci) · )

where  denotes the combination weight vector. For simplicity, the vector f(ci) contains the bias feature (which is 1 for every pair of a query and a source) and the weight vector 
contains the bias element 0. The conditional probability of v given n sources is:

P (v|c)

=

1 Z

n exp

 log P

(vi

=

1|ci)vi P (vi

=

0|ci)1-vi 

i

where Z is the normalizing constant. Our joint classification model (Jnt) expands the above
formula with a new term to model the relationship between sources. The conditional probability of v given n sources is now:

P (v|c)

=

1 Z

n exp

 log P (vi

=

1|ci)vi P (vi

=

0|ci)1-vi 

+

 |v|

i


 sim(ci, cj )vivj

i,j(i<j)

which can be rewritten as:

P (v|c)

=

1 Z

n exp

(1

-

vi

)(f(ci)

·

)

-

log(1

+

exp(f(ci)

·

))

+

 |v|

i


 sim(ci, cj )vivj

i,j(i<j)

where sim(ci, cj) denotes the similarity between two sources ci and cj, and Z is another normalizing constant.
The parameter  controls the influence of similarity. If || is high, the model tends to promote only similar (or dissimilar) sources. When  = 0, we get back to the independent model.
In the learning step, we learn the feature weight vector  from the independent model by using logistic regression. This vector is then used in the joint model. Learning , however, is generally intractable. One can see that the space of vector v is 2n, and so inferencing and estimation become

100

impossible when n is large. We resolve this issue by first ranking the sources using the independent model, and then apply the joint classification model only to the top K = 10 sources. This is equivalent to reranking the top K sources.
From the set of training queries, we use maximum loglikelihood estimation to learn the parameter . Because there is no closed-form solution for the maximum of this loglikelihood function, gradient search method is used instead.
In the prediction step, for a test query, the score of each source ci is assigned by its probability of being relevant:
 R(ci) = P (vi = 1|c) = P (v1, v2, ..., vi = 1, ..., vn|c)
v\vi
where v\vi denotes the set of variables in v with variable vi omitted. In practice, the summation is taken over K - 1 variables and so is feasible when K is small. After that, the top K sources will be reranked according to the new score.
4.2 Similarity Metrics

4.2.1 Similarity Metric based-on Evaluation
Given a set of training queries, the similarity between two sources can be measured by looking at the set of queries for wich they are both relevant. The bigger that set is, the more related they are. Specifically, we apply a cross-product formula to measure this metric:
 SM E(ci, cj) = rel(ci, q)rel(cj, q)
qQ
where Q is the set of training queries, rel(ci, q) is equal to 1 if source ci is relevant to query q based on the classification approach described above, otherwise it is 0. This method is called Similarity Metric based-on Evaluation (SME).

4.2.2 Similarity Metric based-on Query-specific
Evaluation
One issue with the SME is that it is independent of the query. A source may be highly related with another source with respect to a query but unrelated with that source with respect to another query. Therefore, it is better to incorporate the similarity between queries into this formula. By extending the above SME, we derive another metric called Similarity Metric based on Query-specific Evaluation (SMQE).



SM QEq(ci, cj ) =

sim(q, q)rel(ci, q)rel(cj , q)

q Q

where sim(q, q) denotes the correlation (or similarity) between the test query q and a training query q. There are many studies that explore the topicality or classification of queries, however, in this paper, we choose one simple approach. A query in consideration is issued to the centralized sample database, and the number of documents from each source that appear in top M documents of the result is recorded. In our work, M is equal to 100. The correlation between two queries is derived by a cosine-like formula:

sim(q, q)

=




i

numdoc(q,

ci)numdoc(q



,

ci

)

i numdoc(q, ci)2 i numdoc(q, ci)2

where numdoc(q, ci) is the number of documents of source ci that appear in the top M documents returned from query q.

Both the SME and SMQE metrics can be modified in many ways. First of all, the term rel(i, q) can be represented either by a binary number or the absolute number of relevant documents. Or we can set different thresholds to the searching on the centralized sample database. Another choice is to normalize the relevance vector. However, in our experiments, those changes do not have much effect on the results. In fact, SMQE provides the best result, proving that it better reflects the relationship between sources.
4.2.3 Similarity-Metric based-on Kullback-Leibler
divergence
This method tries to reveal the similarity between sources by looking at their own vocabularies. Specifically, a language model [16] is built for each sample source. Then we calculate the Kullback-Leibler divergence between those two language models. Recall that the Kullback-Leibler divergence is actually the distance between two probabilistic models, which is the inverse of their similarity. However, because our model can adapt this change by inferring a negative similarity coefficient , we keep the KL-value as it is. This metric is referred to as SMKL.
5. EXPERIMENTAL METHODOLOGY
We evaluate our proposed algorithms on 3 datasets. The first two datasets are well-known TREC testbeds, the last one comes from a real world application.
· TREC123-100col-bysource (TREC123): 100 collections (information sources) were created from TREC CDs 1,2 and 3 [3]. They are organized by publication source and publication date. The size of each source varies from 7,000 to 39,700 documents (see Table 1 for more statistics). This testbed comes with 100 queries (TREC topics 51-150) with judgments.
· TREC4-100col-bysource (TREC4): 100 collections were created according to the publication source of documents in TREC4 [26]. One actual publication source is distributed across a number of information sources depending on its total number of documents. Each information source has roughly 5,675 documents. This testbed comes with 50 queries (TREC topics 201250) with judgments. More statistics about both TREC123 and TREC4 are presented in Table 1.
· Digital Library (DIGLIB): This real world dataset contains 80 sources (i.e., digital libraries) that are accessible from Purdue University Libraries1. This testbed presents a heterogeneous sources of information. Each document from those sources composes of many fields. Three fields that convey rich information are the abstract, subject heading (or document's category) and full text. Not all sources provide all those three fields. A document from one source may not be provided with its full text, while a document from another source may not have the subject heading. Table 2 shows that only 65% of 80 sources provide abstracts, 65% provide subject headings, and only 30% provide full texts. In our current work, we temporally merge all those available information into one document. Future research may
1We make the dataset available as feature file at http://www.cs.purdue.edu/homes/dthong/

101

Table 1: Summary Statistics of TREC123 and TREC4

Testbed
TREC123 TREC4

Size (GB)
3.2 2.0

Number of Documents (x1000) Min Avg Max 0.7 10.8 39.7 5.6 5.6 5.6

Size(MB)
Min Avg Max 28 32 42 4 20 138

Rk

=

k ki=1
i=1

Ei Bi

Let Ei denote the number of relevant documents of the i-th source according to the ranking E, Bi denote the same thing with respect to ranking B. We also report the results at P @{1, 3, 5, 10} as in source level accuracy.

Table 2: Statistical Information about DIGLIB: Number of Sources Corresponding to their Available Information Fields
Abstract Subject Full Text Number of Sources 52(65%) 52(65%) 24(30%)

· Document Level, High Precision: To make it independent from result merging algorithm, we use fulldataset retrieval as our merging method. The chosen retrieval algorithm is Inquery in Lemur Toolkit [5]. For each query, top 5 sources from the joint classification rank list are selected for this step. Documents not from selected sources are filtered out from the full-dataset rank list. The remaining list is checked by their precision at P @{5, 10, 15, 20, 30} accordingly. We also report a full-dataset precision which includes all sources.

consider to treat each source differently according the type of information that is available.
We also build a set of 100 queries, some of them are extracted from the library log, which are real queries. For each pair of a query and a source, we manually assign a binary value to indicate their relevance.

On TREC123 and TREC4, all tests at different levels are presented. On DIGLIB dataset, we only report the results at source level because the document judgments are difficult to make as many sources do not provide their full text information. In most of the experiments, SMQE is used as our default similarity metric. However, in section 6.4, we also discuss the experimental results with different other metrics.

A note on resource specific retrieval algorithm: DIGLIB is a real world application of digital libraries, each of its sources implements a different retrieval algorithm, which is not known. We can only access those sources through a unified interface. For TREC123 and TREC4, we assign one retrieval algorithm to each source in a round-robin manner. The set of assigned algorithms is Inquery, Language Model and Vector Space (tf.idf). These algorithms influence the query-based sampling process, as well as the classification process. A less effective retrieval algorithm like Vector Space model may reduce a source's chance of being marked as relevant.
For each testbed, we repeat every experiment 5 times. In each trial, we randomly select 50% of the queries as training set, and test on the other 50%. All the results shown in the next section are averaged over 5 trials.
Each source is sampled with 300 documents. We also compare the main results with 100 sample documents. The experiments are measured on several levels:
· Source Level (Resource Selection), Accuracy: This level measures the precision of the resource selection algorithms. Top sources (i.e., top 10) are judged by their precisions at different levels. The judgments come from the classification method, as described in section 3.1. We report the results at P @{1, 3, 5, 10} accordingly.
· Source Level, Recall Metric (R-metric): This metric is widely used in comparing resource selection algorithms [3]. Let E denote the ranking produced by a resource selection algorithm, B denote the based line ranking, in this case, the Relevance-Based Ranking. At level k, the R-metric is defined as:

6. EXPERIMENTAL RESULTS
In all of our experiments, we use paired t-test on queries to check significance. A  denotes a significance on p < 0.1 level;  corresponds to p < 0.05 level and  corresponds to p < 0.01 level.
6.1 TREC123 & TREC4
First of all, we compare the joint classification model with the independent model on the two TREC testbeds. Table 3 represents the source level results in accuracy on TREC123 and TREC4. The second column of each dataset is the joint classification model. Numbers in parentheses show the relative improvement of the joint classification model (denoted as "Jnt") over the independent model (denoted as "Ind").
Table 4 shows the R-metric comparison between the independent model and joint classification model. Table 5 shows the high precision at document level. We also report the full centralized retrieval, which includes all sources. This is denoted as the "Full" column in the table.
It can be seen that joint classification model always leads to better results than independent model, as it shows in all three tables. Both models have the same source level accuracy and R-metric values at top 10 because of the fact that we rerank the top 10 sources. The results are more statistically significant on TREC123 than on TREC4. This can be explained as in TREC123, we have trained on 50 queries; whereas in TREC4, we use only 25 queries out of 50 for training.
6.2 Digital Library
The result at source level of Digital Library is reported in Table 6. In this real world dataset, the joint classification model significantly outperforms the independent model. This accounts to the fact that many sources only provide

102

Table 3: Source Level Results in Accuracy on TREC123 & TREC4 with 300 Sample Documents

Table 5: Document Level Results in High Precision on TREC123 & TREC4 with 300 Sample Documents

Src Rank
@1 @3 @5 @10

TREC123

Ind 0.512 0.456 0.451 0.439

Jnt 0.524(2.3%) 0.499(9.4%) 0.484(7.3%)
0.439(0%)

TREC4

Ind 0.480 0.451 0.430 0.414

Jnt 0.536(11.7%) 0.475(5.3%) 0.446(3.7%)
0.414(0%)

Docs Rank
@5 @10 @15 @20 @30

Full 0.446 0.444 0.435 0.430 0.414

TREC123

Ind 0.392 0.355 0.332 0.309 0.280

Jnt 0.410(4.6%) 0.360(1.4%) 0.347(4.5%) 0.326(5.5%) 0.300(7.1%)

Table 4: Source Level Results in R-metric on TREC123 & TREC4 with 300 Sample Documents

Src Rank
@1 @3 @5 @10

TREC123

Ind 0.262 0.309 0.354 0.426

Jnt 0.319(21.8%) 0.364(17.8%) 0.400(13.0%)
0.426(0%)

TREC4

Ind 0.287 0.324 0.343 0.414

Jnt 0.309(7.7%) 0.340(4.9%) 0.355(3.5%)
0.414(0%)

Docs Rank
@5 @10 @15 @20 @30

Full 0.549 0.459 0.422 0.384 0.354

TREC4

Ind 0.282 0.238 0.209 0.186 0.167

Jnt 0.290(2.8%) 0.254(6.7%) 0.224(7.2%) 0.200(7.5%) 0.170(1.8%)

partial information about themselves. This also shows that the joint classification model can alleviate the problem of missing information.
6.3 Tests with Different Sample Sizes
We conduct experiments on three datasets with only 100 documents sampled from each source. This test is to show the robustness of the model, as well as the effect of sampling size on the results. The results of source level (both in accuracy and R-metric) and document level are reported for TREC123 and TREC4 (Table 7, Table 8 and Table 9 respectively), while only source level is reported for DIGLIB (Table 10).
The sample size clearly affects TREC123. Its performance of the independent model drops significantly. However, this also leaves room for joint classification model to show its effectiveness: the accuracy on source level is statistically more significant. On document level, the improvement is a bit weaker. This can be explained as the initial choice of top 10 sources from the independent model is less precise, so is the joint classification model, which uses the initial ranking list directly.
Most results on TREC4 from Table 7 to Table 9 indicate the advantage of the joint classification model against independent model with a small number of sample documents, although the difference is smaller than TREC123 due to the limited amount of training information.
The results on DIGLIB (Table 10) are also consistent. The performances of both resource selection algorithms drop with 100 sample documents. However, the results of the joint classification method are still significantly better than those of the independent method.
6.4 Test with Different Similarity Metrics
We conduct tests on three testbeds with different similarity metrics discussed in Section 4.2. Figure 1 shows the

Table 6: Source Level Results in Accuracy on DIGLIB with 300 Sample Documents

Src Rank
@1 @3 @5 @10

DIGLIB

Ind 0.552 0.460 0.419 0.356

Jnt 0.640(15.9%) 0.536(16.5%) 0.487(16.2%)
0.356(0%)

Table 7: Source Level Results in Accuracy on TREC123 & TREC4 with 100 Sample Documents

Src Rank
@1 @3 @5 @10

TREC123

Ind 0.320 0.299 0.318 0.319

Jnt 0.380(18.8%) 0.373(24.7%) 0.357(12.3%)
0.319(0%)

TREC4

Ind
0.496 0.405 0.379 0.367

Jnt 0.480(-3.2%) 0.411(1.5%) 0.403(6.3%)
0.367(0%)

Table 8: Source Level Results in R-metric on TREC123 & TREC4 with 100 Sample Documents

Src Rank
@1 @3 @5 @10

TREC123

Ind 0.183 0.214 0.244 0.311

Jnt 0.233(27.3%) 0.262(22.4%) 0.279(14.3%)
0.311(0%)

TREC4

Ind 0.278 0.264 0.293 0.341

Jnt 0.317(14%) 0.293(11%) 0.311(6.1%)
0.341(0%)

103

Table 9: Document Level Results in High Precision on TREC123 & TREC4 with 100 Sample Documents

TREC123

0.3

0.42

SMQE

SME

0.4

Independent

0.28

SMKL

TREC4
SMQE SME Independent SMKL

Docs Rank
@5 @10 @15 @20 @30

TREC123

Ind 0.328 0.302 0.288 0.277 0.253

Jnt 0.329(0.3%) 0.316(4.6%) 0.306(6.2%) 0.296(6.9%) 0.268(5.9%)

TREC4

Ind 0.283 0.243 0.223 0.195 0.165

Jnt 0.301(6.4%) 0.254(4.5%) 0.227(1.8%) 0.204(4.6%) 0.166(0.6%)

Table 10: Source Level Results in Accuracy of DIGLIB with 100 Sample Documents

Precision Precision

0.38 0.36 0.34 0.32
0.3 0.28 0.26
0

10

20

Document Rank

0.26

0.24

0.22

0.2

0.18

0.16

30

0

10

20

30

Document Rank

Src Rank
@1 @3 @5 @10

DIGLIB

Ind 0.436 0.383 0.375 0.318

Jnt 0.620(42.2%) 0.531(38.6%) 0.474(26.4%)
0.318(0%)

results of TREC123 and TREC4 at document level. From this figure, we notice that the SMQE method outperforms all other metrics, due to the fact the it considers the similarity between queries. The SME produces a quite close-to-best result, but the SMKL tends not to be a good choice for the joint classification model. On TREC4, SMKL is comparable with independent model, but it is worse than SMQE and SME.
Figure 2 shows the results of DIGLIB at source level. In this case, both SMQE and SME are comparable, except for the precision at top 1. Again SMKL is not a good choice.

Figure 1: Document Level High Precision on TREC123 & TREC4 with Different Similarity Metrics

Precision

DIGLIB

0.65

SMQE

SME

Independent

0.6

SMKL

0.55

0.5

0.45

0.4

0.35

0

2

4

6

8

10

Source Rank

7. CONCLUSION & FUTURE WORK
This paper proposes a novel joint probabilistic classification model for the resource selection task in federated text search. Existing resource selection algorithms only utilize evidence of individual information sources to select relevant sources, but they do not model the valuable relationship information between the sources. The proposed algorithm estimates the probability of relevance of information sources in a joint manner by combining both the evidence of individual sources and the relationship between the sources. The importance of different types of evidence is determined in a discriminative manner for maximizing the accuracy of resource selection with some training queries. Different types of similarity metrics have been explored to model source similarity based on the performance of available sources on training queries and the Kullback-Leibler divergence on the contents of the sources. A set of experiments were conducted with two TREC datasets and one real world application with digital libraries. The empirical results in different configurations have demonstrated the effectiveness of the proposed joint classification model.
There are several directions to extend the research work in the paper. First, one advantage of the proposed joint probabilistic model is to integrate different types of evidence of

Figure 2: Source Level Accuracy on DIGLIB with
Different Similarity Metrics
individual sources and their relationship. We plan to explore more features for improving the performance of resource selection. For example, we can combine multiple types of similarity evidence in a single framework (with different  weights), which may better model sources' relationship for more accurate resource selection. Second, the joint model in this paper utilizes a reranking approach in resource selection with a small set of information sources (e.g., top 10) to avoid large computational complexity. It is possible to break this limit by utilizing some other approximate inference algorithms (e.g., the pseudo likelihood approach [15]) or making further assumptions on the sources' relationship. For example, one strategy is to first divide available sources into groups of closely related sources. Inference can be conducted by building a small model in each group and assuming independence of sources between different groups.
8. ACKNOWLEDGMENTS
This research was partially supported by the Vietnam Education Foundation (VEF) and the NSF grant IIS-0749462.

104

The opinions, findings, and conclusions stated herein are those of the authors and do not necessarily reflect those of the sponsors.
9. REFERENCES
[1] J. Arguello, J. Callan, and F. Diaz. Classification-based resource selection. Proceeding of
the 18th ACM Conference on Information and Knowledge Management, 2009. [2] J. Arguello, F. D´iaz, J. Callan, and J. Crespo. Sources of evidence for vertical selection. In Proceedings of the
32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2009. [3] J. Callan. Distributed information retrieval. Advances in Information Retrieval, pages 127­150, 2000. [4] J. Callan and M. Connell. Query-based sampling of text databases. ACM Transactions on Information Systems, 19(2):97­130, 2001. [5] J. Callan, W. B. Croft, and S. M. Harding. The inquery retrieval system. In Proceedings of the Third
International Conference on Database and Expert Systems Applications, 1992. [6] J. Callan, Z. Lu, and W. B. Croft. Searching distributed collection with inference networks. In
Proceedings of the 18th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval, 1995. [7] S. Cetintas, L. Si, and H. Yuan. Learning from past queries for resource selection. In Proceeding of the 18th
ACM Conference on Information and Knowledge Management. ACM, 2009. [8] N. Craswell, P. Bailey, and D. Hawking. Server selection on the world wide web. In Proceedings of the 5th ACM Conference on Digital Libraries. ACM, 2000. [9] N. Fuhr. A decision-theoretic approach to database selection in networked ir. ACM Transactions on Information Systems (TOIS), 17(3):229­249, 1999. [10] N. Fuhr. Resource discovery in distributed digital libraries. In In Digital Libraries '99: Advanced Methods and Technologies, Digital Collections, 1999. [11] L. Gravano, K. Chang, C-C., H. Garc´ia-Molina, and A. Paepcke. Starts: Stanford proposal for internet meta-searching. In Proceedings of the ACM-SIGMOD
International Conference on Management of Data (SIGMOD). ACM, 1997. [12] L. Gravano, H. Garc´ia-Molina, and A. Tomasic. Gloss: Text-source discovery over the internet. ACM Transactions on Database Systems, 24(2):229­264, 1999. [13] W. Meng, C. Yu, and K. Liu. Building efficient and effective metasearch engines. ACM Computing Surveys (CSUR), 34(1):48­89, 2002. [14] D. Metzler and W. Croft. Combining the language model and inference network approaches to retrieval. Information Processing and Management, 40(5):735­750, 2004. [15] S. Parise and M. Welling. Learning in markov random fields: An empirical study. In Joint Statistical Meeting (JSM2005), volume 4, 2005. [16] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of

the 25th Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval. ACM, 1998. [17] J. Seo and W. B. Croft. Blog site search using resource selection. In CIKM '08: Proceeding of the
17th ACM Conference on Information and Knowledge Management, pages 1053­1062, New York, NY, USA, 2008. ACM. [18] M. Shokouhi. Central-rank-based collection selection in uncooperative distributed information retrieval. In
Proceedings of the 29th European Conference on Information Retrieval, 2007. [19] M. Shokouhi and J. Zobel. Robust result merging using sample-based score estimates. ACM Transactions on Information Systems, 27(3):1­29, 2009. [20] L. Si and J. Callan. Relevant document distribution estimation method for resource selection. In
Proceedings of the 26th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval. ACM, 2003. [21] L. Si and J. Callan. A semi-supervised learning method to merge search engine results. ACM Transactions on Information Systems, 21(4):457­491, 2003. [22] L. Si and J. Callan. Unified utility maximization framework for resource selection. In Proceedings of
13th ACM International Conference on Information and Knowledge Management (CIKM), 2004. [23] L. Si and J. Callan. Modeling search engine effectiveness for federated search. In Proceedings of the
28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2005. [24] P. Thomas and M. Shokouhi. Sushi: scoring scaled samples for server selection. In SIGIR '09: Proceedings
of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2009. [25] E. Voorhees, N. K. Gupta, and B. Johnson-Laird. Learning collection fusion strategies. In Proceedings of
the 18th Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval. ACM, 1995. [26] J. Xu and J. Callan. Effective retrieval with distributed collections. In Proceedings of the 21st
Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1998. [27] J. Xu and W. B. Croft. Cluster-based language models for distributed retrieval. In Proceedings of the 22nd
Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1999. [28] B. Yuwono and D. L. Lee. Server ranking for distributed text retrieval systems on the internet. In
Proceedings of the 5th Annual International
Conference on Database Systems for Advanced Applications, 1997.

105


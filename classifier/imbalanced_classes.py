# -*- coding: utf-8 -*-
"""Imbalanced_Classes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GTN2Q9XyFLIyex-nmuU7PB8k4tUmSW-q
"""
import random
import sys
import numpy as np # linear algebra
np.set_printoptions(threshold=sys.maxsize)
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import re
import csv
import time
from scipy import stats
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, f1_score
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from timeit import default_timer as timer

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier

import os
# print(os.listdir("../input"))
plt.style.use('ggplot')

df = pd.DataFrame()

raw = open(r"C:\\Users\\z3696\\Documents\\Document-Classification\\classifier\\NIST_FULL\\2010-neg.txt", encoding="ISO-8859-1")
lines = raw.readlines()
raw.close()

# remove /n at the end of each line
for index, line in enumerate(lines):
    lines[index] = line.strip()
    print(lines[index])

neg_df = pd.DataFrame(columns=['sentence'])
i = 0
first_col = ""
for line in lines:
    first_col = re.sub(r' \(.*', "", line)
    neg_df.loc[i] = [first_col]
    i = i+1

neg_df.head()
neg_df['label'] = 0
print(neg_df.shape)

raw1 = open(r"C:\\Users\\z3696\\Documents\\Document-Classification\\classifier\\NIST_FULL\\2010-pos.txt", encoding="ISO-8859-1")
lines1 = raw1.readlines()
raw1.close()

# remove /n at the end of each line
for index, line in enumerate(lines1):
    lines1[index] = line.strip()
    print(lines1[index])

pos_df = pd.DataFrame(columns=['sentence'])
i = 0
first_col = ""
for line in lines1:
    first_col = re.sub(r' \(.*', "", line)
    pos_df.loc[i] = [first_col]
    i = i+1

pos_df.head()
pos_df['label'] = 1

df = df.append(pos_df)
df = df.append(neg_df)
print(df)
print(pos_df.shape)
print(neg_df.shape)

import pandas as pd
import numpy as np
import nltk
import re
import string
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support as score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
import string
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
nltk.download('wordnet')
nltk.download('stopwords')
stopwords.words('english')

#Removing punctuations from entire dataset
punc_set = string.punctuation
punc_set

#Function for removing punctions
def remove_punc(text):
    clean = "".join([x.lower() for x in text if x not in punc_set])
    return clean

#Applying the 'remove_punc' function to entire dataset
df['no_punc'] = df['sentence'].apply(lambda z:remove_punc(z))

#Function for Tokenizing entire data for representing every word as datapoint
def tokenize(text):
    tokens = re.split("\W+",text)
    return tokens

#Applying the 'tokenize' function to entire dataset
df['tokenized_Data'] = df['no_punc'].apply(lambda z:tokenize(z))

#Importing stopwords from NLTK Library to remove stopwords now that we have tokenized it
stopwords = nltk.corpus.stopwords.words('english')

#Function for removing stopwords from single row
def remove_stopwords(tokenized_words):
    Ligit_text=[word for word in tokenized_words if word not in stopwords]
    return Ligit_text

#Applying the function 'remove_stopwords' from the entire dataset
df["no_stop"] = df["tokenized_Data"].apply(lambda z:remove_stopwords(z))

#Importing 'WordNetLemmatizer' as lemmatizing function to find lemma's of words
wnl = nltk.wordnet.WordNetLemmatizer()

#Function for lemmatizing the tokenzied text
def lemmatizing(tokenized_text):
    lemma = [wnl.lemmatize(word) for word in tokenized_text]
    return lemma

#Applying the 'lemmatizing' function to entire dataset
df['lemmatized'] = df['no_stop'].apply(lambda z:lemmatizing(z))

# #Importing the 'SnowballStemmer' and declaring variable 'sno' to save the stemmer in.
# #This Stemmer gives slightly better results as compared to 'PorterStemmer'
# sno = nltk.SnowballStemmer('english')

# #Function for applying stemming to find stem roots of all words
# def stemming(tokenized_text):
#     text= [sno.stem(word) for word in tokenized_text]
#     return text

# #Applying the 'stemming' function to entire dataset
# data['ss_stemmed'] = data['lemmatized'].apply(lambda z:stemming(z))


# ps = nltk.PorterStemmer()

# def stemming(tokenized_text):
#     text= [ps.stem(word) for word in tokenized_text]
#     return text

# data['ps_stemmed'] = data['lemmatized'].apply(lambda z:stemming(z))

#This step is done here because, the 'lemmatized' column is a list of tokenized words and when we apply vectorization
#techniques such as count vectorizer or TFIDF, they require string input. Hence convert all tokenzied words to string
df['lemmatized'] = [" ".join(review) for review in df['lemmatized'].values]

df.head()

time.sleep(500)

pos_num = 29708

pos_95 = round(pos_num * 0.95)
pos_85 = round(pos_num * 0.85)
pos_75 = round(pos_num * 0.75)
pos_65 = round(pos_num * 0.65)
pos_55 = round(pos_num * 0.55)
pos_45 = round(pos_num * 0.45)
pos_35 = round(pos_num * 0.35)
pos_25 = round(pos_num * 0.25)
pos_15 = round(pos_num * 0.15)
pos_05 = round(pos_num * 0.05)

neg_num = 48521

neg_95 = round(neg_num * 0.95)
neg_85 = round(neg_num * 0.85)
neg_75 = round(neg_num * 0.75)
neg_65 = round(neg_num * 0.65)
neg_55 = round(neg_num * 0.55)
neg_45 = round(neg_num * 0.45)
neg_35 = round(neg_num * 0.35)
neg_25 = round(neg_num * 0.25)
neg_15 = round(neg_num * 0.15)
neg_05 = round(neg_num * 0.05)

# Splitting Datasets Manually
pos_pt1=df.iloc[0:pos_num-pos_95 :]
neg_pt1=df.iloc[neg_num:neg_num+neg_95, :]
pos_pt2=df.iloc[0:pos_num-pos_85, :]
neg_pt2=df.iloc[neg_num:neg_num+neg_85, :]
pos_pt3=df.iloc[0:pos_num-pos_75, :]
neg_pt3=df.iloc[neg_num:neg_num+neg_75, :]
pos_pt4=df.iloc[0:pos_num-pos_65, :]
neg_pt4=df.iloc[neg_num:neg_num+neg_65, :]
neg_pt5=df.iloc[0:pos_num-pos_55, :]
pos_pt5=df.iloc[neg_num:neg_num+neg_55, :]
pos_pt6=df.iloc[0:pos_num-pos_45, :]
neg_pt6=df.iloc[neg_num:neg_num+neg_45, :]
pos_pt7=df.iloc[0:pos_num-pos_35, :]
neg_pt7=df.iloc[neg_num:neg_num+neg_35, :]
pos_pt8=df.iloc[0:pos_num-pos_25, :]
neg_pt8=df.iloc[neg_num:neg_num+neg_25, :]
pos_pt9=df.iloc[0:pos_num-pos_15, :]
neg_pt9=df.iloc[neg_num:neg_num+neg_15, :]
neg_pt10=df.iloc[0:pos_num-pos_05, :]
pos_pt10=df.iloc[neg_num:neg_num+neg_05, :]
'''
# Splitting Datasets Manually
pos_pt1=df.iloc[0:pos_num-xy_num10, :]
neg_pt1=df.iloc[neg_num:neg_num+xy_num1, :]
pos_pt2=df.iloc[0:pos_num-xy_num9, :]
neg_pt2=df.iloc[neg_num:neg_num+xy_num2, :]
pos_pt3=df.iloc[0:pos_num-xy_num8, :]
neg_pt3=df.iloc[neg_num:neg_num+xy_num3, :]
pos_pt4=df.iloc[0:pos_num-xy_num7, :]
neg_pt4=df.iloc[neg_num:neg_num+xy_num4, :]
neg_pt5=df.iloc[0:pos_num-xy_num6, :]
pos_pt5=df.iloc[neg_num:neg_num+xy_num5, :]
pos_pt6=df.iloc[0:pos_num-xy_num5, :]
neg_pt6=df.iloc[neg_num:neg_num+xy_num6, :]
pos_pt7=df.iloc[0:pos_num-xy_num4, :]
neg_pt7=df.iloc[neg_num:neg_num+xy_num7, :]
pos_pt8=df.iloc[0:pos_num-xy_num3, :]
neg_pt8=df.iloc[neg_num:neg_num+xy_num8, :]
pos_pt9=df.iloc[0:pos_num-xy_num2, :]
neg_pt9=df.iloc[neg_num:neg_num+xy_num9, :]
neg_pt10=df.iloc[0:pos_num-xy_num1, :]
pos_pt10=df.iloc[neg_num:neg_num+xy_num10, :]
'''
df1 = pd.concat([pos_pt1, neg_pt1])
print("DF1:", round(df1.shape[0]))
df2 = pd.concat([pos_pt2, neg_pt2])
print("DF2:", round(df2.shape[0]))
df3 = pd.concat([pos_pt3, neg_pt3])
print("DF3:", round(df3.shape[0]))
df4 = pd.concat([pos_pt4, neg_pt4])
print("DF4:", round(df4.shape[0]))
df5 = pd.concat([pos_pt5, neg_pt5])
print("DF5:", round(df5.shape[0]))
df6 = pd.concat([pos_pt6, neg_pt6])
print("DF6:", round(df6.shape[0]))
df7 = pd.concat([pos_pt7, neg_pt7])
print("DF7:", round(df7.shape[0]))
df8 = pd.concat([pos_pt8, neg_pt8])
print("DF8:", round(df8.shape[0]))
df9 = pd.concat([pos_pt9, neg_pt9])
print("DF9:", round(df9.shape[0]))
df10 = pd.concat([pos_pt10, neg_pt10])
print("DF10:", round(df10.shape[0]))
#Splitting data into smaller dataframes for the purpose of Training and Testing

df1_l = "Split 1"
df2_l = "Split 2"
df3_l = "Split 3"
df4_l = "Split 4"
df5_l = "Split 5"
df6_1 = "Split 6"
df7_l = "Split 7"
df8_l = "Split 8"
df9_l = "Split 9"
df10_l = "Split 10"

ts01 = "TS 0.1"
ts02 = "TS 0.2"
ts03 = "TS 0.3"
ts04 = "TS 0.4"
ts05 = "TS 0.5"

df1_split = round(df1.shape[0] * .5)
print(df1_split)
df2_split = round(df2.shape[0] * .5)
print(df2_split)
df3_split = round(df3.shape[0] * .5)
print(df3_split)
df4_split = round(df4.shape[0] * .5)
print(df4_split)
df5_split = round(df5.shape[0] * .5)
print(df5_split)
df6_split = round(df6.shape[0] * .5)
print(df6_split)
df7_split = round(df7.shape[0] * .5)
print(df7_split)
df8_split = round(df8.shape[0] * .5)
print(df8_split)
df9_split = round(df9.shape[0] * .5)
print(df9_split)
df10_split = round(df10.shape[0] * .5)
print(df10_split)

'''
xy_ran1=random.randint(1, df.shape[0])
xy_ran2=random.randint(1, df.shape[0])
xy_ran3=random.randint(1, df.shape[0])
xy_ran4=random.randint(1, df.shape[0])
xy_ran5=random.randint(1, df.shape[0])
xy_ran6=random.randint(1, df.shape[0])
xy_ran7=random.randint(1, df.shape[0])
xy_ran8=random.randint(1, df.shape[0])
xy_ran9=random.randint(1, df.shape[0])
xy_ran10=random.randint(1, df.shape[0])
'''

df1_sen = round(df1.shape[0])
df2_sen = round(df2.shape[0])
df3_sen = round(df3.shape[0])
df4_sen = round(df4.shape[0])
df5_sen = round(df5.shape[0])
df6_sen = round(df6.shape[0])
df7_sen = round(df7.shape[0])
df8_sen = round(df8.shape[0])
df9_sen = round(df9.shape[0])
df10_sen = round(df10.shape[0])

x1 = df1.iloc[0:df1_split,5]
x2 = df1.iloc[df1_sen-df1_split:df1_sen,5]
x3 = df2.iloc[0:df2_split,5]
x4 = df2.iloc[df2_sen-df2_split:df2_sen,5]
x5 = df3.iloc[0:df3_split,5]
x6 = df3.iloc[df3_sen-df3_split:df3_sen,5]
x7 = df4.iloc[0:df4_split,5]
x8 = df4.iloc[df4_sen-df4_split:df4_sen,5]
x9 = df5.iloc[0:df5_split,5]
x10 = df5.iloc[df5_sen-df5_split:df5_sen,5]
x11 = df6.iloc[0:df6_split,5]
x12 = df6.iloc[df6_sen-df6_split:df6_sen,5]
x13 = df7.iloc[0:df7_split,5]
x14 = df7.iloc[df7_sen-df7_split:df7_sen,5]
x15 = df8.iloc[0:df8_split,5]
x16 = df8.iloc[df8_sen-df8_split:df8_sen,5]
x17 = df9.iloc[0:df9_split,5]
x18 = df9.iloc[df9_sen-df9_split:df9_sen,5]
x19 = df10.iloc[0:df10_split,5]
x20 = df10.iloc[df10_sen-df10_split:df10_sen,5]

y1 = df1.iloc[0:df1_split,1]
y2 = df1.iloc[df1_sen-df1_split:df1_sen,1]
y3 = df2.iloc[0:df2_split,1]
y4 = df2.iloc[df2_sen-df2_split:df2_sen,1]
y5 = df3.iloc[0:df3_split,1]
y6 = df3.iloc[df3_sen-df3_split:df3_sen,1]
y7 = df4.iloc[0:df4_split,1]
y8 = df4.iloc[df4_sen-df4_split:df4_sen,1]
y9 = df1.iloc[0:df1_split,1]
y10 = df1.iloc[df1_sen-df1_split:df1_sen,1]
y11 = df6.iloc[0:df6_split,1]
y12 = df6.iloc[df6_sen-df6_split:df6_sen,1]
y13 = df7.iloc[0:df7_split,1]
y14 = df7.iloc[df7_sen-df7_split:df7_sen,1]
y11 = df8.iloc[0:df8_split,1]
y16 = df8.iloc[df8_sen-df8_split:df8_sen,1]
y17 = df9.iloc[0:df9_split,1]
y18 = df9.iloc[df9_sen-df9_split:df9_sen,1]
y19 = df10.iloc[0:df10_split,1]
y20 = df10.iloc[df10_sen-df10_split:df10_sen,1]



#x_seg = df.iloc[0:5,5]
#y_seg = df.iloc[0:5,1]
print("X1:", x1.shape)
print("X2:", x2.shape)
print("X3:", x3.shape)
print("X4:", x4.shape)
print("X5:", x5.shape)
print("X6:", x6.shape)
print("X7:", x7.shape)
print("X8:", x8.shape)
print("X9:", x9.shape)
print("X10:", x10.shape)
print("Y1:", y1.shape)
print("Y2:", y2.shape)
print("Y3:", y3.shape)
print("Y4:", y4.shape)
print("Y5:", y5.shape)
print("Y6:", y6.shape)
print("Y7:", y7.shape)
print("Y8:", y8.shape)
print("Y9:", y9.shape)
print("Y10:", y10.shape)

'''
x = df['lemmatized'].values
y = df['label'].values
print(x.shape)
print(y.shape)
'''
'''
train_set = [x1,x3,x5,x7,x9]
test_set = [x2,x4,x6,x8,x10]

train_label = [y1,y3,y5,y7,y9]
test_set = [y2,y4,y6,y8,y10]
'''
#Declaring and applying TFIDF functions to train and test data

tfidf_vect = TfidfVectorizer(ngram_range=(1,2))
tfidf_train = tfidf_vect.fit_transform(x1.values)
tfidf_test=tfidf_vect.transform(x2.values)
print(tfidf_train.shape)
print(tfidf_test.shape)
#tfidf_train.toarray()

#from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
'''
# Values for testing set - PTest
Accuracy_LRN = []
Precision_LRN = []
Recall_LRN = []

Accuracy_DCT = []
Recall_DCT = []
Precision_DCT = []

Accuracy_NBB = []
Recall_NBB = []
Precision_NBB = []

Accuracy_XGB = []
Recall_XGB = []
Precision_XGB = []

# Values for testing set - Sampling
Accuracy_LRN_s = []
Precision_LRN_s = []
Recall_LRN_s = []

Accuracy_DCT_s = []
Recall_DCT_s = []
Precision_DCT_s = []

Accuracy_NBB_s = []
Recall_NBB_s = []
Precision_NBB_s = []

Accuracy_XGB_s = []
Recall_XGB_s = []
Precision_XGB_s = []
'''
'''
# Values for training set - PTest
Accuracy_LRN_tr = []
Precision_LRN_tr = []
Recall_LRN_tr = []

Accuracy_DCT_tr = []
Recall_DCT_tr = []
Precision_DCT_tr = []

Accuracy_NBB_tr = []
Recall_NBB_tr = []
precision_NBB_tr = []

Accuracy_RDD_tr = []
Recall_RDD_tr = []
precision_RDD_tr = []
'''
tfidf_vect = TfidfVectorizer()
# Combine dataframes
#x_lemm1 = pd.concat([x1, x2])
#x_lemm2 = pd.concat([x3, x4])
#x_lemm3 = pd.concat([x5, x6])
#x_lemm4 = pd.concat([x7, x8])
#x_lemm5 = pd.concat([x9, x10])

#y_label1 = pd.concat([y1, y2])
#y_label2 = pd.concat([y3, y4])
#y_label3 = pd.concat([y5, y6])
#y_label4 = pd.concat([y7, y8])
#y_label5 = pd.concat([y9, y10])
#== DF1 ==
x_tfidf = tfidf_vect.fit_transform(df["lemmatized"])

probs_lr_scol = []
f1_lr_scol = []
rocauc_lr_scol = []
recall_lr_scol = []
precision_lr_scol = []
accuracy_lr_scol = []

probs_dt_scol = []
f1_dt_scol = []
rocauc_dt_scol = []
recall_dt_scol = []
precision_dt_scol = []
accuracy_dt_scol = []

probs_nb_scol = []
f1_nb_scol = []
rocauc_nb_scol = []
recall_nb_scol = []
precision_nb_scol = []
accuracy_nb_scol = []

probs_xg_scol = []
f1_xg_scol = []
rocauc_xg_scol = []
recall_xg_scol = []
precision_xg_scol = []
accuracy_xg_scol = []

probs_rf_scol = []
f1_rf_scol = []
rocauc_rf_scol = []
recall_rf_scol = []
precision_rf_scol = []
accuracy_rf_scol = []

# , 0.394, 0.454, 0.496, 0.303, 0.370, 0.381, 0.325, 0.359, 0.410
train_values= np.array([0.236])
#train_sizes = round(train_list, 3)
for i in train_values:
    # train_sizes = train_sizes + i
    #random_portion = round(np.random.uniform(0.2, 0.5), 3)
    #print(random_portion)
    x_train, x_test, y_train, y_test = train_test_split(x_tfidf,df["label"], train_size=i, test_size=0.5)

    start1 = time.time()
    log = LogisticRegression(penalty='l2',random_state=0, solver='lbfgs', multi_class='auto', max_iter=500)
    model_lr = log.fit(x_train,y_train)
    probs_lr = model_lr.predict_proba(x_test)[:, 1]
    probs_lr_scol.append(probs_lr)
    ly_prediction = log.predict(x_test)
    fly = f1_score(ly_prediction,y_test)
    f1_lr_scol.append(fly)
    rocauc_lr = roc_auc_score(y_test, ly_prediction)
    rocauc_lr_scol.append(rocauc_lr)
    recalls_lr = recall_score(y_test, ly_prediction)
    recall_lr_scol.append(recalls_lr)
    precisions_lr = precision_score(y_test, ly_prediction)
    precision_lr_scol.append(precisions_lr)
    accuracys_lr = accuracy_score(y_test, ly_prediction)
    accuracy_lr_scol.append(accuracys_lr)
    print("===Logistic Regression with TfidfVectorizer Imbalanced - 2010", df1_l, ts05, i)
    lr_end = time.time()
    print('Logistic F1-score',fly*100)
    print('Logistic ROCAUC score:',rocauc_lr*100)
    print('Logistic Recall score:', recalls_lr*100)
    print('Logistic Precision Score:', precisions_lr*100)
    print('Logistic Confusion Matrix', confusion_matrix(y_test,ly_prediction), "\n")
    print('Logistic Classification', classification_report(y_test,ly_prediction), "\n")
    print('Logistic Accuracy Score', accuracys_lr*100)
    print("Execution Time for Logistic Regression Imbalanced: ", lr_end - start1, "seconds")

    start2 = time.time()
    from sklearn.tree import DecisionTreeClassifier
    DCT = DecisionTreeClassifier()
    model_dt = DCT.fit(x_train, y_train)
    probs_dt = model_dt.predict_proba(x_test)[:, 1]
    probs_dt_scol.append(probs_dt)
    dct_pred = DCT.predict(x_test)
    fdct = f1_score(dct_pred,y_test)
    f1_dt_scol.append(fdct)
    rocauc_dt = roc_auc_score(y_test, dct_pred)
    rocauc_dt_scol.append(rocauc_dt)
    recalls_dt = recall_score(y_test, dct_pred)
    recall_dt_scol.append(recalls_dt)
    precisions_dt = precision_score(y_test, dct_pred)
    precision_dt_scol.append(precisions_dt)
    accuracys_dt = accuracy_score(y_test, dct_pred)
    accuracy_dt_scol.append(accuracys_dt)
    print("===DecisionTreeClassifier with TfidfVectorizer Imbalanced - 2010", df1_l, ts05, i)
    dt_end = time.time()
    print('DCT F1-score',fdct*100)
    print('DCT ROCAUC score:',rocauc_dt*100)
    print('DCT Recall score:', recalls_dt*100)
    print('DCT Precision Score:', precisions_dt*100)
    print('DCT Confusion Matrix', confusion_matrix(y_test, dct_pred), "\n")
    print('DCT Classification', classification_report(y_test, dct_pred), "\n")
    print('DCT Accuracy Score', accuracys_dt*100)
    print("Execution Time for Decision Tree Imbalanced: ", dt_end - start2, "seconds")


    from sklearn.naive_bayes import MultinomialNB
    start3 = time.time()
    Naive = MultinomialNB()
    model_nb = Naive.fit(x_train,y_train)
    probs_nb = model_nb.predict_proba(x_test)[:, 1]
    probs_nb_scol.append(probs_nb)
    # predict the labels on validation dataset
    ny_pred = Naive.predict(x_test)
    fnb = f1_score(ny_pred,y_test)
    f1_nb_scol.append(fnb)
    rocauc_nb = roc_auc_score(y_test, ny_pred)
    rocauc_nb_scol.append(rocauc_nb)
    recalls_nb = recall_score(y_test, ny_pred)
    recall_nb_scol.append(recalls_nb)
    precisions_nb = precision_score(y_test, ny_pred)
    precision_nb_scol.append(precisions_nb)
    accuracys_nb = accuracy_score(y_test, ny_pred)
    accuracy_nb_scol.append(accuracys_nb)
    nb_end = time.time()
    # Use accuracy_score function to get the accuracy
    print("===Naive Bayes with TfidfVectorizer Imabalanced - 2010", df1_l, ts05, i)
    print('Naive F1-score',fnb*100)
    print('Naive ROCAUC score:',rocauc_nb*100)
    print('Naive Recall score:', recalls_nb*100)
    print('Naive Precision Score:', precisions_nb*100)
    print('Naive Confusion Matrix', confusion_matrix(y_test, ny_pred), "\n")
    print('Naive Classification', classification_report(y_test, ny_pred), "\n")
    print('Naive Accuracy Score', accuracys_nb*100)
    print("Execution Time for Naive Bayes Imbalanced: ", nb_end - start3, "seconds")

# XGBoost Classifier

    start4 = time.time()
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
    xgb_model = XGBClassifier().fit(x_train, y_train)
    probs_xg = xgb_model.predict_proba(x_test)[:, 1]
    probs_xg_scol.append(probs_xg)
    # predict
    xgb_y_predict = xgb_model.predict(x_test)
    fxg = f1_score(xgb_y_predict,y_test)
    f1_xg_scol.append(fxg)
    rocauc_xg = roc_auc_score(xgb_y_predict, y_test)
    rocauc_xg_scol.append(rocauc_xg)
    recall_xg = recall_score(xgb_y_predict, y_test)
    recall_xg_scol.append(recall_xg)
    precisions_xg = precision_score(xgb_y_predict, y_test)
    precision_xg_scol.append(precisions_xg)
    accuracys_xg = accuracy_score(xgb_y_predict, y_test)
    accuracy_xg_scol.append(accuracys_xg)
    xg_end = time.time()
    print("===XGB with TfidfVectorizer Imbalanced - 2010", df1_l, ts05, i)
    print('XGB F1-Score', fxg*100)
    print('XGB ROCAUC Score:', rocauc_xg*100)
    print('XGB Recall score:', recall_xg*100)
    print('XGB Precision Score:', precisions_xg *100)
    print('XGB Confusion Matrix', confusion_matrix(xgb_y_predict, y_test), "\n")
    print('XGB Classification', classification_report(xgb_y_predict, y_test), "\n")
    print('XGB Accuracy Score', accuracys_nb*100)
    print("Execution Time for XGBoost Classifier Imbalanced: ", xg_end - start4, "seconds")

# Random Forest Classifier
    from sklearn.ensemble import RandomForestClassifier
    start5 = time.time()
    rfc_model = RandomForestClassifier(n_estimators=1000, random_state=0).fit(x_train,y_train)
    probs_rf = rfc_model.predict_proba(x_test)[:, 1]
    probs_rf_scol.append(probs_rf)
    rfc_pred = rfc_model.predict(x_test)
    frfc = f1_score(rfc_pred,y_test)
    f1_rf_scol.append(frfc)
    rocauc_rf = roc_auc_score(y_test, rfc_pred)
    rocauc_rf_scol.append(rocauc_rf)
    recalls_rf = recall_score(rfc_pred, y_test)
    recall_rf_scol.append(recalls_rf)
    precisions_rf = precision_score(rfc_pred, y_test)
    precision_rf_scol.append(precisions_rf)
    accuracys_rf = accuracy_score(rfc_pred, y_test)
    accuracy_rf_scol.append(accuracys_rf)
    rf_end = time.time()
    print("====RandomForest with Tfidf Imbalanced 2010", df1_l, ts05, i)
    print('RFC F1 score', frfc*100)
    print('RFC ROCAUC Score:', rocauc_rf*100)
    print('RFC Recall score:', recalls_rf*100)
    print('RFC Precision Score:', precisions_rf*100)
    print('RFC Confusion Matrix', confusion_matrix(y_test,rfc_pred), "\n")
    print('RFC Classification', classification_report(y_test,rfc_pred), "\n")
    print('RFC Accuracy Score', accuracys_rf*100)
    print("Execution Time for Random Forest Classifier Imbalanced: ", rf_end - start5, "seconds")

    print("Array of Prob Scores LR-Imb:", probs_lr_scol)
    print("Array of F1 Scores LR-Imb:", f1_lr_scol)
    print("Array of ROCAUC Scores LR-Imb:", rocauc_lr_scol)
    print("Array of Recall Scores LR-Imb:", recall_lr_scol)
    print("Array of Precision Scores LR-Imb:", precision_lr_scol)
    print("Array of Accuracy Scores LR-Imb:", accuracy_lr_scol)

    print("Array of Prob Scores DT-Imb:", probs_dt_scol)
    print("Array of F1 Scores DT-Imb:", f1_dt_scol)
    print("Array of ROCAUC Scores DT-Imb:", rocauc_dt_scol)
    print("Array of Recall Scores DT-Imb:", recall_dt_scol)
    print("Array of Precision Scores DT-Imb:", precision_dt_scol)
    print("Array of Accuracy Scores DT-Imb:", accuracy_dt_scol)

    print("Array of Prob Scores NB-Imb:", probs_nb_scol)
    print("Array of F1 Scores NB-Imb:", f1_nb_scol)
    print("Array of ROCAUC Scores NB-Imb:", rocauc_nb_scol)
    print("Array of Recall Scores NB-Imb:", recall_nb_scol)
    print("Array of Precision Scores NB-Imb:", precision_nb_scol)
    print("Array of Accuracy Scores NB-Imb:", accuracy_nb_scol)

    print("Array of Prob Scores XG-Imb:", probs_xg_scol)
    print("Array of F1 Scores XG-Imb:", f1_xg_scol)
    print("Array of ROCAUC Scores XG-Imb:", rocauc_xg_scol)
    print("Array of Recall Scores XG-Imb:", recall_xg_scol)
    print("Array of Precision Scores XG-Imb:", precision_xg_scol)
    print("Array of Accuracy Scores XG-Imb:", accuracy_xg_scol)

    print("Array of Prob Scores RF-Imb:", probs_rf_scol)
    print("Array of F1 Scores RF-Imb:", f1_rf_scol)
    print("Array of ROCAUC Scores RF-Imb:", rocauc_rf_scol)
    print("Array of Recall Scores RF-Imb:", recall_rf_scol)
    print("Array of Precision Scores RF-Imb:", precision_rf_scol)
    print("Array of Accuracy Scores RF-Imb:", accuracy_rf_scol)

from sklearn.metrics import auc, precision_recall_curve

y_test_int = y_test.replace({'Positive': 1, 'Negative': 0})

baseline_model = sum(y_test_int == 1) / len(y_test_int)
'''
precision_lr, recall_lr, _ = precision_recall_curve(y_test_int, probs_lr)
auc_lr = auc(recall_lr, precision_lr)

precision_dt, recall_dt, _ = precision_recall_curve(y_test_int, probs_dt)
auc_dt = auc(recall_dt, precision_dt)

precision_nb, recall_nb, _ = precision_recall_curve(y_test_int, probs_nb)
auc_nb = auc(recall_nb, precision_nb)

precision_xg, recall_xg, _ = precision_recall_curve(y_test_int, probs_xg)
auc_xg = auc(recall_xg, precision_xg)
'''
'''
precision_cs, recall_cs, _ = precision_recall_curve(y_test_int, probs_cs)
auc_cs = auc(recall_cs, precision_cs)
'''

'''
precision_rf, recall_rf, _ = precision_recall_curve(y_test_int, probs_rf)
auc_rf  = auc(recall_rf, precision_rf)
'''

df['label'].value_counts()

import seaborn as sns
g = sns.countplot(df1['label'])
g.set_xticklabels(['Negative','Positive'])
plt.show()

# class count
label_count_neg, label_count_pos = df['label'].value_counts()

# Separate class
label_neg = df1[df1['label'] == 0]
label_pos = df1[df1['label'] == 1]# print the shape of the class
print('Label Negative:', label_neg.shape)
print('Label Positive:', label_pos.shape)

label_neg_under = label_neg.sample(label_count_pos)

test_under = pd.concat([label_neg_under, label_pos], axis=0)

print("total class of pos and neg :",test_under['label'].value_counts())# plot the count after under-sampling
test_under['label'].value_counts().plot(kind='bar', title='label (target)')

label_pos_over = label_pos.sample(label_count_neg, replace=True)

test_over = pd.concat([label_pos_over, label_neg], axis=0)

print("total class of pos and neg:",test_under['label'].value_counts())# plot the count after under-sampeling
test_over['label'].value_counts().plot(kind='bar', title='label (target)')

import imblearn

# import library
from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
x = x_tfidf
y = df['label']
print(x.shape)
print(y.shape)


rus = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable
x_rus, y_rus = rus.fit_resample(x, y)

print('Original dataset shape:', Counter(y))
print('Resample dataset shape', Counter(y_rus))

# import library
from collections import Counter
from imblearn.over_sampling import RandomOverSampler
'''
x =x_tfidf
y = df1["label"]
'''
ros = RandomUnderSampler(random_state=42)
#ros = RandomOverSampler(random_state=42)
#Random over-sampling with imblearn
# fit predictor and target variable
x_rus, y_rus = rus.fit_resample(x, y)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_rus))
#Random over-sampling with imblearn
ros = RandomOverSampler(random_state=42)

# fit predictor and target variable
x_ros, y_ros = ros.fit_resample(x, y)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_ros))
# import library

# Under-sampling: Tomek links
from imblearn.under_sampling import TomekLinks
from collections import Counter


tl = RandomOverSampler(sampling_strategy='majority')

# fit predictor and target variable
x_tl, y_tl = ros.fit_resample(x, y)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_ros))

# import library
from imblearn.over_sampling import SMOTE

smote = SMOTE()

# fit predictor and target variable
x_smote, y_smote = smote.fit_resample(x, y)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_ros))


from imblearn.under_sampling import NearMiss

nm = NearMiss()

x_nm, y_nm = nm.fit_resample(x, y)

print('Original dataset shape:', Counter(y))
print('Resample dataset shape:', Counter(y_nm))
# 0.370, 0.381, 0.325, 0.359, 0.410

probs_lr_scol1 = []
f1_lr_scol1 = []
rocauc_lr_scol1 = []
recall_lr_scol1 = []
precision_lr_scol1 = []
accuracy_lr_scol1 = []

probs_dt_scol1 = []
f1_dt_scol1 = []
rocauc_dt_scol1 = []
recall_dt_scol1 = []
precision_dt_scol1 = []
accuracy_dt_scol1 = []

probs_nb_scol1 = []
f1_nb_scol1 = []
rocauc_nb_scol1 = []
recall_nb_scol1 = []
precision_nb_scol1 = []
accuracy_nb_scol1 = []

probs_xg_scol1 = []
f1_xg_scol1 = []
rocauc_xg_scol1 = []
recall_xg_scol1 = []
precision_xg_scol1 = []
accuracy_xg_scol1 = []

probs_rf_scol1 = []
f1_rf_scol1 = []
rocauc_rf_scol1 = []
recall_rf_scol1 = []
precision_rf_scol1 = []
accuracy_rf_scol1 = []

train_values = np.array([0.236, 0.394, 0.454])
#train_sizes = round(train_list, 3)
#====Sampling Technique=====
for i in train_values:
   # train_sizes = train_values + i
    x1_train, x1_test, y1_train, y1_test = train_test_split(x_tl,y_tl, train_size=i, test_size=0.5)

    start6 = time.time()
    log = LogisticRegression(penalty='l2',random_state=0, solver='lbfgs', multi_class='auto', max_iter=500)
    model_lr1 = log.fit(x1_train,y1_train)
    probs_lr1 = model_lr1.predict_proba(x1_test)[:, 1]
    probs_lr_scol1.append(probs_lr1)
    ly_prediction = log.predict(x1_test)
    fly1 = f1_score(ly_prediction,y1_test)
    f1_lr_scol1.append(fly1)
    rocauc_lrs = roc_auc_score(ly_prediction, y1_test)
    rocauc_lr_scol1.append(rocauc_lrs)
    recalls_lrs = recall_score(y1_test, ly_prediction)
    recall_lr_scol1.append(recalls_lrs)
    precisions_lrs = precision_score(y1_test, ly_prediction)
    precision_lr_scol1.append(precisions_lrs)
    accuracys_lrs = accuracy_score(y1_test, ly_prediction)
    accuracy_lr_scol1.append(accuracys_lrs)
    lr1_end = time.time()
    print("===Logistic Regression with TfidfVectorizer Tomelinks - 2010", df1_l, ts05, i,"-")
    print('Logistic F1-score',fly1*100)
    print('Logistic ROCAUC score:',rocauc_lrs*100)
    print('Logistic Recall score:', recalls_lrs*100)
    print('Logistic Precision Score:', precisions_lrs*100)
    print('Logistic Confusion Matrix', confusion_matrix(y1_test,ly_prediction), "\n")
    print('Logistic Classification', classification_report(y1_test,ly_prediction), "\n")
    print('Logistic Accuracy Score', accuracys_lrs*100)
    print("Execution Time for Logistic Regression Tomelinks: ", lr1_end - start6, "seconds")

    from sklearn.tree import DecisionTreeClassifier
    start7 = time.time()
    DCT = DecisionTreeClassifier()
    model_dt1 = DCT.fit(x1_train, y1_train)
    probs_dt1 = model_dt1.predict_proba(x1_test)[:, 1]
    probs_dt_scol1.append(probs_dt1)
    dct_pred = DCT.predict(x1_test)
    fdct1 = f1_score(dct_pred,y1_test)
    f1_dt_scol1.append(fdct1)
    rocauc_dts = roc_auc_score(y1_test, dct_pred)
    rocauc_dt_scol1.append(rocauc_dts)
    recalls_dts = recall_score(y1_test, dct_pred)
    recall_dt_scol1.append(recalls_dts)
    precisions_dts = precision_score(y1_test, dct_pred)
    precision_dt_scol1.append(precisions_dts)
    accuracys_dts = accuracy_score(y1_test, dct_pred)
    accuracy_dt_scol1.append(accuracys_dts)
    print("===DecisionTreeClassifier with TfidfVectorizer Tomelinks - 2010", df1_l, ts05, i)
    dt1_end = time.time()
    print('DCT F1-score',fdct1*100)
    print('DCT ROCAUC score:', rocauc_dts*100)
    print('DCT Recall score:', recalls_dts*100)
    print('DCT Precision Score:', precisions_dts*100)
    print('DCT Confusion Matrix', confusion_matrix(y1_test, dct_pred), "\n")
    print('DCT Classification', classification_report(y1_test, dct_pred), "\n")
    print('DCT Accuracy Score', accuracys_dts*100)
    print("Execution Time for Decision Tree Tomelinks: ", dt1_end - start7, "seconds")

    from sklearn.naive_bayes import MultinomialNB
    start8 = time.time()
    Naive = MultinomialNB()
    model_nb1 = Naive.fit(x1_train,y1_train)
    probs_nb1 = model_nb1.predict_proba(x1_test)[:, 1]
    probs_nb_scol1.append(probs_nb1)
    # predict the labels on validation dataset
    ny_pred = Naive.predict(x1_test)
    fnb1 = f1_score(ny_pred,y1_test)
    f1_nb_scol1.append(fnb1)
    rocauc_nbs = roc_auc_score(y1_test, ny_pred)
    rocauc_nb_scol1.append(rocauc_nbs)
    recalls_nbs = recall_score(y1_test, ny_pred)
    recall_nb_scol1.append(recalls_nbs)
    precisions_nbs = precision_score(y1_test, ny_pred)
    precision_nb_scol1.append(precisions_nbs)
    accuracys_nbs = accuracy_score(y1_test, ny_pred)
    accuracy_nb_scol1.append(accuracys_nbs)
    # Use accuracy_score function to get the accuracy
    print("===Naive Bayes with TfidfVectorizer Tomelinks - 2010", df1_l, ts05, i)
    nb1_end = time.time()
    print('Naive F1-score',fnb1*100)
    print('Naive ROCAUC score:',roc_auc_score(y1_test, ny_pred)*100)
    print('Naive Recall score:', recalls_nbs*100)
    print('Naive Precision Score:', precisions_nbs*100)
    print('Naive Confusion Matrix', confusion_matrix(y1_test, ny_pred), "\n")
    print('Naive Classification', classification_report(y1_test, ny_pred), "\n")
    print('Naive Accuracy Score', accuracys_nbs*100)
    print("Execution Time for Naive Bayes Tomelinks: ", nb1_end - start8, "seconds")

    # XGBoost Classifier
    from xgboost import XGBClassifier
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
    start9 = time.time()
    XGB = XGBClassifier()
    xgb_model1 = XGB.fit(x1_train,y1_train)
    probs_xg1 = xgb_model1.predict_proba(x1_test)[:, 1]
    probs_xg_scol1.append(probs_xg1)
    # predict
    xgb_y_predict = xgb_model1.predict(x1_test)
    fxg1 = f1_score(xgb_y_predict,y1_test)
    f1_xg_scol1.append(fxg1)
    rocauc_xgs = roc_auc_score(xgb_y_predict, y1_test)
    rocauc_xg_scol1.append(rocauc_xgs)
    recalls_xgs = recall_score(y1_test, xgb_y_predict)
    recall_xg_scol1.append(recalls_xgs)
    precisions_xgs = precision_score(y1_test, xgb_y_predict)
    precision_xg_scol1.append(precisions_xgs)
    accuracys_xgs = accuracy_score(y1_test, xgb_y_predict)
    accuracy_xg_scol1.append(accuracys_xgs)
    print("===XGB with TfidfVectorizer Tomelinks- 2010", df1_l, ts05, i)
    xg1_end = time.time()
    print('XGB F1-Score', fxg1*100)
    print('XGB ROCAUC Score:', rocauc_xgs*100)
    print('XGB Recall score:', recalls_xgs*100)
    print('XGB Precision Score:', precisions_xgs*100)
    print('XGB Confusion Matrix', confusion_matrix(xgb_y_predict, y1_test), "\n")
    print('XGB Classification', classification_report(xgb_y_predict, y1_test), "\n")
    print('XGB Accuracy Score', accuracys_xgs*100)
    print("Execution Time for XGBoost Tomelinks: ", xg1_end - start9, "seconds")

    '''
    # Support Vector Machine Classifier
    from sklearn.svm import SVC
    csvm_model1 = SVC(C=1.0, kernel='linear', degree=3, gamma='auto').fit(x1_train,y1_train)
    probs_cs1 = csvm_model1.predict_proba(x1_test)[:, 1]
    # predict the labels on validation dataset
    csvy_pred = csvm_model1.predict(x1_test)
    fcsvm = f1_score(csvy_pred,y1_test)
    print("===C-SVM with TfidfVectorizer Tomelinks - 2010===")
    print('C-SVM F1-score',fcsvm*100)
    # Use accuracy_score function to get the accuracy
    print('C-SVM ROCAUC score:',roc_auc_score(y1_test, csvy_pred)*100)
    print('C-SVM Recall score:', recall_score(y1_test, csvy_pred)*100)
    print('C-SVM Precision Score:', precision_score(y1_test, csvy_pred)*100)
    print('C-SVM Confusion Matrix', confusion_matrix(y1_test, csvy_pred))
    print('C-SVM Classification', classification_report(y1_test, csvy_pred))
    print('C-SVM Accuracy Score', accuracy_score(y1_test, csvy_pred)*100)
    '''
    # Random Forest Classifier
    from sklearn.ensemble import RandomForestClassifier
    start10 = time.time()
    rfc_model1 = RandomForestClassifier(n_estimators=1000, random_state=0).fit(x1_train,y1_train)
    probs_rf1 = rfc_model1.predict_proba(x1_test)[:, 1]
    probs_rf_scol1.append(probs_rf1)
    rfc_pred = rfc_model1.predict(x1_test)
    frfc1 = f1_score(rfc_pred,y1_test)
    f1_rf_scol1.append(frfc1)
    rocauc_rfs = roc_auc_score(y1_test, rfc_pred)
    rocauc_rf_scol1.append(rocauc_rfs)
    recalls_rfs = recall_score(y1_test, rfc_pred)
    recall_rf_scol1.append(recalls_rfs)
    precisions_rfs = precision_score(y1_test, rfc_pred)
    precision_rf_scol1.append(precisions_rfs)
    accuracys_rfs = accuracy_score(y1_test, rfc_pred)
    accuracy_rf_scol1.append(accuracys_rfs)
    rf1_end = time.time()
    print("====RandomForest with Tfidf Tomelinks-  2010", df1_l, ts05, i)
    print('RFC F1 score', frfc*100)
    print('RFC ROCAUC Score:', rocauc_rfs*100)
    print('RFC Recall score:', recalls_rfs*100)
    print('RFC Precision Score:', precisions_rfs*100)
    print('RFC Confusion Matrix', confusion_matrix(y1_test,rfc_pred))
    print('RFC Classification', classification_report(y1_test,rfc_pred))
    print('RFC Accuracy Score', accuracys_rfs*100)
    print("Execution Time for Random Forest Tomelinks: ", rf1_end - start10, "seconds")

    print("Array of Prob Scores LR-Sam:", probs_lr_scol1)
    print("Array of F1 Scores LR-Sam:", f1_lr_scol1)
    print("Array of ROCAUC Scores LR-Sam:", rocauc_lr_scol1)
    print("Array of Recall Scores LR-Sam:", recall_lr_scol1)
    print("Array of Precision Scores LR-Sam:", precision_lr_scol1)
    print("Array of Accuracy Scores LR-Sam:", accuracy_lr_scol1)

    print("Array of Prob Scores DT-Sam:", probs_dt_scol1)
    print("Array of F1 Scores DT-Sam:", f1_dt_scol1)
    print("Array of ROCAUC Scores DT-Sam:", rocauc_dt_scol1)
    print("Array of Recall Scores DT-Sam:", recall_dt_scol1)
    print("Array of Precision Scores DT-Sam:", precision_dt_scol1)
    print("Array of Accuracy Scores DT-Sam:", accuracy_dt_scol1)

    print("Array of Prob Scores NB-Sam:", probs_nb_scol1)
    print("Array of F1 Scores NB-Sam:", f1_nb_scol1)
    print("Array of ROCAUC Scores NB-Sam:", rocauc_nb_scol1)
    print("Array of Recall Scores NB-Sam:", recall_nb_scol1)
    print("Array of Precision Scores NB-Sam:", precision_nb_scol1)
    print("Array of Accuracy Scores NB-Sam:", accuracy_nb_scol1)

    print("Array of Prob Scores XG-Sam:", probs_xg_scol1)
    print("Array of F1 Scores XG-Sam:", f1_xg_scol1)
    print("Array of ROCAUC Scores XG-Sam:", rocauc_xg_scol1)
    print("Array of Recall Scores XG-Sam:", recall_xg_scol1)
    print("Array of Precision Scores XG-Sam:", precision_xg_scol1)
    print("Array of Accuracy Scores XG-Sam:", accuracy_xg_scol1)

    print("Array of Prob Scores RF-Sam:", probs_rf_scol1)
    print("Array of F1 Scores RF-Sam:", f1_rf_scol1)
    print("Array of ROCAUC Scores RF-Sam:", rocauc_rf_scol1)
    print("Array of Recall Scores RF-Sam:", recall_rf_scol1)
    print("Array of Precision Scores RF-Sam:", precision_rf_scol1)
    print("Array of Accuracy Scores RF-Sam:", accuracy_rf_scol1)

'''
# Update the collection over the test set
Accuracy_LRN_s.append(accuracys_lr)
Recall_LRN_s.append(recalls_lr)
Precision_LRN_s.append(precisions_lr)

Accuracy_DCT_s.append(accuracys_dt)
Recall_DCT_s.append(recalls_dt)
Precision_DCT_s.append(precisions_dt)

Accuracy_NBB_s.append(accuracys_nb)
Recall_NBB_s.append(recalls_nb)
Precision_NBB_s.append(precisions_nb)

Accuracy_XGB_s.append(accuracys_xg)
Recall_XGB_s.append(recalls_xg)
Precision_XGB_s.append(precisions_xg)
'''
y_test_int1 = y1_test.replace({'Positive': 1, 'Negative': 0})

baseline_model1 = sum(y_test_int1 == 1) / len(y_test_int1)
'''
precision_lr1, recall_lr1, _ = precision_recall_curve(y_test_int1, probs_lr1)
auc_lr1 = auc(recall_lr1, precision_lr1)

precision_dt1, recall_dt1, _ = precision_recall_curve(y_test_int1, probs_dt1)
auc_dt1 = auc(recall_dt1, precision_dt1)

precision_nb1, recall_nb1, _ = precision_recall_curve(y_test_int1, probs_nb1)
auc_nb1 = auc(recall_nb1, precision_nb1)

precision_xg1, recall_xg1, _ = precision_recall_curve(y_test_int1, probs_xg1)
auc_xg1 = auc(recall_xg1, precision_xg1)
'''
'''
precision_cs1, recall_cs1, _ = precision_recall_curve(y_test_int1, probs_cs1)
auc_cs1 = auc(recall_cs1, precision_cs1)
'''
'''
precision_rf1, recall_rf1, _ = precision_recall_curve(y_test_int1, probs_rf1)
auc_rf1 = auc(recall_rf1, precision_rf1)
'''
'''
plt.plot(recall_lr1, precision_lr1, label=f'AUC Log. Reg. Tomelinks) = {auc_lr:.2f}')
plt.plot(recall_dt1, precision_dt1, label=f'AUC Dec. Tree Tomelinks) = {auc_dt:.2f}')
plt.plot(recall_nb1, precision_nb1, label=f'AUC Nai Bay. Tomelinks) = {auc_rf:.2f}')
plt.plot(recall_xg1, precision_xg1, label=f'AUC XGB Tomelinks) = {auc_xg:.2f}')
'''
'''
print(60*'-')
print('The results below over test set')

print("Mean Accuracy call for Logistic Regression: {0}".format(np.mean(Accuracy_LRN)))
print("Mean Precision call for Logistic Regression: {0}".format(np.mean(Precision_LRN)))
print("Mean Recall call for Logistic Regression: {0}".format(np.mean(Recall_LRN)))

print("Mean Accuracy call for Decision Tree: {0}".format(np.mean(Accuracy_DCT)))
print("Mean Precision call for Decision Tree: {0}".format(np.mean(Recall_DCT)))
print("Mean Recall call for Decision Tree: {0}".format(np.mean(Precision_DCT)))

print("Mean Accuracy call for Naive Bayes: {0}".format(np.mean(Accuracy_NBB)))
print("Mean Precision call for Naive Bayes: {0}".format(np.mean(Precision_NBB)))
print("Mean Recall call for Naive Bayes: {0}".format(np.mean(Recall_NBB)))

print("Mean Accuracy call for XGBoost Classifier: {0}".format(np.mean(Accuracy_XGB)))
print("Mean Precision call for XGBoost Classifier: {0}".format(np.mean(Precision_XGB)))
print("Mean Recall call for XGBoost Classifier: {0}".format(np.mean(Recall_XGB)))

print(20*'-')
print('The results below for sampling techniques')
print("T - Mean Accuracy call for Logistic Regression: {0}".format(np.mean(Accuracy_LRN_s)))
print("T- Mean Precision call for Logistic Regression: {0}".format(np.mean(Precision_LRN_s)))
print("T- Mean Recall call for Logistic Regression: {0}".format(np.mean(Recall_LRN_s)))

print("T- Mean Accuracy call for Decision Tree: {0}".format(np.mean(Accuracy_DCT_s)))
print("T- Mean Precision call for Decision Tree: {0}".format(np.mean(Recall_DCT_s)))
print("T- Mean Recall call for Decision Tree: {0}".format(np.mean(Precision_DCT_s)))

print("T- Mean Accuracy call for Naive Bayes: {0}".format(np.mean(Accuracy_NBB_s)))
print("T- Mean Precision call for Naive Bayes: {0}".format(np.mean(Precision_NBB_s)))
print("T- Mean Recall call for Naive Bayes: {0}".format(np.mean(Recall_NBB_s)))

print("T- Mean Accuracy call for XGBoost: {0}".format(np.mean(Accuracy_XGB_s)))
print("T- Mean Precision call for XGBoost: {0}".format(np.mean(Precision_XGB_s)))
print("T- Mean Recall call for XGBoost: {0}".format(np.mean(Recall_XGB_s)))
'''
## Paired-t test
# Naive Bayes Classifiers
'''
ttest_pre_1 = stats.ttest_rel(precision_nb,precision_nb1)
ttest_rec_1 = stats.ttest_rel(recall_nb,recall_nb1)
#ttest_pre_1 = stats.ttest_rel(Precision_NBB,Precision_LRN)
#ttest_re_1 = stats.ttest_rel(Recall_NBB,Recall_LRN)
#ttest_run_1 = stats.ttest_rel(Runtime_NNN,Runtime_LLL)
print('Pair T-Test between Precision between Naive Bayes Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_1))
print('Pair T-Test between Recall between Naive Bayes Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_1))
#print('Pair T-Test between Precision and Recall of Naive Bayes - 2010 Tomelinks {0}'.format(ttest_sam_1))
#print('Precision - Pair-t test between Neural and Logistic {0}'.format(ttest_pre_1))
#print('Recall - Pair-t test between Neural and Logistic {0}'.format(ttest_re_1))
#print('Runtime - Pair-t test between Neural and Logistic {0}'.format(ttest_run_1))
print(60*'-')

# Decision Tree Classifier
ttest_pre_2 = stats.ttest_rel(precision_dt,precision_dt1)
ttest_rec_2 = stats.ttest_rel(recall_dt1,recall_dt1)
#ttest_pre_2 = stats.ttest_rel(Precision_NBB,Precision_DCT)
#ttest_re_2 = stats.ttest_rel(Recall_NBB,Recall_DCT)
#ttest_run_2 = stats.ttest_rel(Runtime_NBB,Runtime_DCT)
print('Pair T-Test of Precision between Decision Tree Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_2))
print('Pair T-Test of Recall between Decision Tree Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_2))
#print('Precision - Pair-t test between Neural and Naive {0}'.format(ttest_pre_2))
#print('Recall - Pair-t test between Neural and Naive {0}'.format(ttest_re_2))
#print('Runtime - Pair-t test between Neural and Naive {0}'.format(ttest_run_2))
print(60*'-')

# XGB Boost Classifier
ttest_pre_3 = stats.ttest_rel(precision_xg,precision_xg1)
ttest_rec_3 = stats.ttest_rel(recall_xg,recall_xg1)
#ttest_pre_3 = stats.ttest_rel(Precision_XGB,Precision_NBB)
#ttest_re_3 = stats.ttest_rel(Recall_XGB,Recall_NBB)
#ttest_run_3 = stats.ttest_rel(Runtime_LLL,Runtime_NBB)
print('Pair T-Test of Precision between XGBoost Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_3))
print('Pair T-Test of Recall between XGBoost Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_3))
#print('Precision - Pair-t test between Logistic and Naive {0}'.format(ttest_pre_3))
#print('Recall - Pair-t test between Logistic and Naive {0}'.format(ttest_re_3))
#print('Runtime - Pair-t test between Logistic and Naive {0}'.format(ttest_run_3))

# Random Forest Classifier
ttest_pre_4 = stats.ttest_rel(precision_rf,precision_rf1)
ttest_rec_4 = stats.ttest_rel(recall_rf,recall_rf1)
#ttest_pre_4 = stats.ttest_rel(Precision_XGB,Precision_NBB)
#ttest_re_4 = stats.ttest_rel(Recall_XGB,Recall_NBB)
#ttest_run_4 = stats.ttest_rel(Runtime_LLL,Runtime_NBB)
print('Pair T-Test of Precision between Random Forest Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_4))
print('Pair T-Test of Recall between Random Forest Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_4))
#print('Precision - Pair-t test between Logistic and Naive {0}'.format(ttest_pre_4))
#print('Recall - Pair-t test between Logistic and Naive {0}'.format(ttest_re_4))
#print('Runtime - Pair-t test between Logistic and Naive {0}'.format(ttest_run_4))

ttest_pre_5 = stats.ttest_rel(precision_lr,precision_lr1)
ttest_rec_5 = stats.ttest_rel(recall_lr,recall_lr1)
print('Pair T-Test of Precision between Logistic Regression Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_5))
print('Pair T-Test of Precision between Logistic Regression Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_5))

print(20*'-')
print("Execution Time for Logistic Regression Imbalanced - 2010", df1_l, ts05 ,":", lr_end - start1, "seconds")
print("Execution Time for Decision Tree Imbalanced - 2010", df1_l, ts05 ,":", dt_end - start2, "seconds")
print("Execution Time for Naive Bayes Imbalanced - 2010 ", df1_l, ts05 ,":", nb_end - start3, "seconds")
print("Execution Time for XGB Imbalanced - 2010 ", df1_l, ts05 ,":", xg_end - start4, "seconds")
print("Execution Time for Random Forest Imbalanced - 2010 ", df1_l, ts05 ,":", rf_end - start5, "seconds")
print("Execution Time for Logistic Regression Tomelinks - 2010 ", df1_l, ts05 ,":", lr1_end - start6, "seconds")
print("Execution Time for Decision Tree Tomelinks - 2010 ", df1_l, ts05 ,":", dt1_end - start7, "seconds")
print("Execution Time for Naive Bayes Tomelinks - 2010 ", df1_l, ts05 ,":", nb1_end - start8, "seconds")
print("Execution Time for XGB Tomelinks - 2010 ", df1_l, ts05 ,":", xg1_end - start9, "seconds")
print("Execution Time for Random Forest Tomelinks - 2010 ", df1_l, ts05 ,":", rf1_end - start10, "seconds")
print("\n")


y_test_int1 = y_test.replace({'Positive': 1, 'Negative': 0})
baseline_model = sum(y_test_int == 1) / len(y_test_int)
precision_lr1, recall_lr1, _ = precision_recall_curve(y_test_int1, probs_lr1)
auc_lr1 = auc(recall_lr1, precision_lr1)
'''
'''
from sklearn.metrics import auc, precision_recall_curve

y_test_int2 = y2_test.replace({'Positive': 1, 'Negative': 0})

baseline_model2 = sum(y_test_int2 == 1) / len(y_test_int2)

precision_lr2, recall_lr2, _ = precision_recall_curve(y_test_int2, probs_lr1)
auc_lr2 = auc(recall_lr2, precision_lr2)

precision_dt2, recall_dt2, _ = precision_recall_curve(y_test_int1, probs_dt1)
auc_dt2 = auc(recall_dt2, precision_dt2)

precision_nb2, recall_nb2, _ = precision_recall_curve(y_test_int1, probs_nb1)
auc_nb2 = auc(recall_nb2, precision_nb2)

precision_xg2, recall_xg2, _ = precision_recall_curve(y_test_int1, probs_xg1)
auc_xg2 = auc(recall_xg2, precision_xg2)

precision_cs, recall_cs, _ = precision_recall_curve(y_test_int, probs_cs)
auc_cs = auc(recall_cs, precision_cs)


precision_rf2, recall_rf2, _ = precision_recall_curve(y_test_int1, probs_rf1)
auc_rf2  = auc(recall_rf2, precision_rf2)
'''

'''
y_test_int = y_test.replace({'Positive': 1, 'Negative': 0})
print("Print:", type(y_test_int))
y_test_int1 = y1_test.replace({'Positive': 1, 'Negative': 0})
print("Print:", type(y_test_int1))
ytest_sp1 = pd.concat([y_test_int,y_test_int1])
print("Type of Baseline Test Split 1:", ytest_sp1)
precision_imb1 =  (precisions_lr + precisions_dt + precisions_nb + precisions_xg + precisions_rf) / 5
print("Avergage of Precision Imbalanced Split 1:", precision_imb1)
precision_sam1 =  (precisions_lrs + precisions_dts + precisions_nbs + precisions_xgs + precisions_rfs) / 5
print("Average of Precision Sampling Scores Split 1:", precision_sam1)
precision_sp1 = (precision_imb1 + precision_sam1) / 2
print("Average of Precision Imbalanced and Sampling Split 1:", precision_sp1)
recall_imb1 = (recalls_lr + recalls_dt + recalls_nb + recalls_xg + recalls_rf) / 5
print("Average of Recall Imbalanced Split 1:", recall_imb1)
recall_sam1 = (recalls_lrs + recalls_lrs + recalls_nbs + recalls_xgs + recalls_rfs) / 5
print("Average of Recall Sampling Split 1:", recall_sam1)
recall_sp1 = (recall_imb1 + recall_sam1) / 2
print("Average of Recall Imbalanced and Sampling Split 1:", recall_sp1)
probs_imb1 = (probs_lr + probs_dt + probs_nb + probs_xg + probs_rf) / 5
print("Probability Array of Imbalanced Split 1:", probs_imb1)
probs_sam1 = (probs_lr1 + probs_dt1 + probs_nb1 + probs_xg1 + probs_rf1) / 5
print("Probability Array of Sampling Split 1:", probs_sam1)
#print(probs_imb1.shape())
#print(probs_sam1.shape())
#probs_sp1 = ((np.array(probs_imb1)) + (np.array(probs_sam1))) / 2

#y_test_avg1 = ytest_sp1.replace({'Positive': 1, 'Negative': 0})

precision_imb1, recall_imb1, _ = precision_recall_curve(y_test_int, probs_imb1)
auc_imb1 = auc(recall_imb1, precision_imb1)

precision_sam1, recall_sam1, _ = precision_recall_curve(y_test_int1, probs_sam1)
auc_sam1 = auc(recall_sam1, precision_sam1)
'''

precision_imb1 = (precision_lr_scol[0] + precision_dt_scol[0]+ precision_rf_scol[0] + precision_nb_scol[0] + precision_xg_scol[0] ) / 5
precision_sam1 = (precision_lr_scol1[0] + precision_dt_scol1[0]+ precision_rf_scol1[0] + precision_nb_scol1[0] + precision_xg_scol1[0] ) / 5
precision_imb2 = (precision_lr_scol[1] + precision_dt_scol[1]+ precision_rf_scol[1] + precision_nb_scol[1] + precision_xg_scol[1] ) / 5
precision_sam2 = (precision_lr_scol1[1] + precision_dt_scol1[1]+ precision_rf_scol1[1] + precision_nb_scol1[1] + precision_xg_scol1[1] ) / 5
precision_imb3 = (precision_lr_scol[2] + precision_dt_scol[2]+ precision_rf_scol[2] + precision_nb_scol[2] + precision_xg_scol[2] ) / 5
precision_sam3 = (precision_lr_scol1[2] + precision_dt_scol1[2]+ precision_rf_scol1[2] + precision_nb_scol1[2] + precision_xg_scol1[2] ) / 5

recall_imb1 = (recall_lr_scol[0] + recall_dt_scol[0]+ recall_rf_scol[0] + recall_nb_scol[0] + recall_xg_scol[0] ) / 5
recall_sam1 = (recall_lr_scol1[0] + recall_dt_scol1[0]+ recall_rf_scol1[0] + recall_nb_scol1[0] + recall_xg_scol1[0] ) / 5
recall_imb2 = (recall_lr_scol[1] + recall_dt_scol[1]+ recall_rf_scol[1] + recall_nb_scol[1] + recall_xg_scol[1] ) / 5
recall_sam2 = (recall_lr_scol1[1] + recall_dt_scol1[1]+ recall_rf_scol1[1] + recall_nb_scol1[1] + recall_xg_scol1[1] ) / 5
recall_imb3 = (recall_lr_scol[2] + recall_dt_scol[2]+ recall_rf_scol[2] + recall_nb_scol[2] + recall_xg_scol[2] ) / 5
recall_sam3 = (recall_lr_scol1[2] + recall_dt_scol1[2]+ recall_rf_scol1[2] + recall_nb_scol1[2] + recall_xg_scol1[2] ) / 5

precision_sp1 = (precision_imb1 + precision_sam1) / 2
precision_sp2 = (precision_imb2 + precision_sam2) / 2
precision_sp3 = (precision_imb3 + precision_sam3) / 2

recall_sp1 = (recall_imb1 + recall_sam1) / 2
recall_sp2 = (recall_imb2 + recall_sam2) / 2
recall_sp3 = (recall_imb3 + recall_sam3) / 2

probs_imb1 = (probs_lr_scol[0] + probs_dt_scol[0]+probs_rf_scol[0] + probs_nb_scol[0] + probs_xg_scol[0] ) / 5
probs_sam1 = (probs_lr_scol1[0] + probs_dt_scol1[0]+ probs_rf_scol1[0] + probs_nb_scol1[0] + probs_xg_scol1[0] ) / 5
probs_imb2 = (probs_lr_scol[1] + probs_dt_scol[1]+ probs_rf_scol[1] + probs_nb_scol[1] + probs_xg_scol[1] ) / 5
probs_sam2 = (probs_lr_scol1[1] + probs_dt_scol1[1]+ probs_rf_scol1[1] + probs_nb_scol1[1] + probs_xg_scol1[1] ) / 5
probs_imb3 = (probs_lr_scol[2] + probs_dt_scol[2]+ probs_rf_scol[2] + probs_nb_scol[2] + probs_xg_scol[2] ) / 5
probs_sam3 = (probs_lr_scol1[2] + probs_dt_scol1[2]+ probs_rf_scol1[2] + probs_nb_scol1[2] + probs_xg_scol1[2] ) / 5


precision_imb1, recall_imb1, _ = precision_recall_curve(y_test_int, probs_imb1)
auc_imb1 = auc(recall_imb1, precision_imb1)

precision_sam1, recall_sam1, _ = precision_recall_curve(y_test_int1, probs_sam1)
auc_sam1 = auc(recall_sam1, precision_sam1)

precision_imb2, recall_imb2, _ = precision_recall_curve(y_test_int, probs_imb2)
auc_imb2 = auc(recall_imb2, precision_imb2)

precision_sam2, recall_sam2, _ = precision_recall_curve(y_test_int1, probs_sam2)
auc_sam2 = auc(recall_sam2, precision_sam2)

precision_imb3, recall_imb3, _ = precision_recall_curve(y_test_int, probs_imb3)
auc_imb3 = auc(recall_imb3, precision_imb3)

precision_sam3, recall_sam3, _ = precision_recall_curve(y_test_int1, probs_sam3)
auc_sam3 = auc(recall_sam3, precision_sam3)

plt.plot([0, 1], [baseline_model1], linestyle='--', label='Baseline model')
plt.plot(recall_imb1, precision_imb1, label=f'AUC .236 (Imb) = {auc_imb1:.2f}')
plt.plot(recall_sam1, precision_sam1, label=f'AUC .236 (Sam) = {auc_sam1:.2f}')
plt.plot(recall_imb2, precision_imb2, label=f'AUC .394 (Imb) = {auc_imb2:.2f}')
plt.plot(recall_sam2, precision_sam2, label=f'AUC .394 (Sam) = {auc_sam2:.2f}')
plt.plot(recall_imb3, precision_imb3, label=f'AUC .454 (Imb) = {auc_imb3:.2f}')
plt.plot(recall_sam3, precision_sam3, label=f'AUC .454 (Sam) = {auc_sam3:.2f}')

plt.title('Precision-Recall Curves 2010 - ', size=20)
plt.xlabel('Recall', size=14)
plt.ylabel('Precision', size=14)
plt.legend()
plt.show();
'''
#==DF 1 ==

#== DF 2 ==
x_tfidf2 = tfidf_vect.fit_transform(df2["lemmatized"])

x2_train, x2_test, y2_train, y2_test = train_test_split(x_tfidf2,df2["label"],test_size=0.5)
'''
'''
# Convert [1,0,0] to 0. [0,1,0] to 1. [0,0,1] to 2
ytrain_ = np.zeros(y_train.shape[0])
for i in range(y_train.shape[0]):
    index = np.argmax(y_train[i])
    if index == 0:
        ytrain_[i] = int(0)
    elif index == 1:
        ytrain_[i] = int(1)
    else:
        ytrain_[i] = int(2)

    ytest_ = np.zeros(y_test.shape[0])
    for i in range(y_test.shape[0]):
        index = np.argmax(y_test[i])
        if index == 0:
            ytest_[i] = int(0)
        elif index == 1:
            ytest_[i] = int(1)
        else:
            ytest_[i] = int(2)
'''
'''
start11 = time.time()
log = LogisticRegression(penalty='l2',random_state=0, solver='lbfgs', multi_class='auto', max_iter=500)
model_lr2 = log.fit(x2_train,y2_train)
probs_lr2 = model_lr2.predict_proba(x2_test)[:, 1]
ly_prediction = log.predict(x2_test)
fly = f1_score(ly_prediction,y2_test)
recalls_lr2 = recall_score(y2_test, ly_prediction)
precisions_lr2 = precision_score(y2_test, ly_prediction)
accuracys_lr2 = accuracy_score(y2_test, ly_prediction)
print("===Logistic Regression with TfidfVectorizer Imbalanced - 2010", df2_l, ts05)
lr2_end = time.time()
print('Logistic F1-score',fly*100)
print('Logistic ROCAUC score:',roc_auc_score(y2_test, ly_prediction)*100)
print('Logistic Recall score:', recalls_lr2*100)
print('Logistic Precision Score:', precisions_lr2*100)
print('Logistic Confusion Matrix', confusion_matrix(y2_test,ly_prediction), "\n")
print('Logistic Classification', classification_report(y2_test,ly_prediction), "\n")
print('Logistic Accuracy Score', accuracys_lr2*100)
print("Execution Time for Logistic Regression Imbalanced: ", lr2_end - start11, "seconds")

start12 = time.time()
from sklearn.tree import DecisionTreeClassifier
DCT = DecisionTreeClassifier()
model_dt2 = DCT.fit(x2_train, y2_train)
probs_dt2 = model_dt2.predict_proba(x2_test)[:, 1]
dct_pred = DCT.predict(x2_test)
fdct = f1_score(dct_pred,y2_test)
recalls_dt2 = recall_score(y2_test, dct_pred)
precisions_dt2 = precision_score(y2_test, dct_pred)
accuracys_dt2 = accuracy_score(y2_test, dct_pred)
print("===DecisionTreeClassifier with TfidfVectorizer Imbalanced - 2010", df1_l, ts05)
dt2_end = time.time()
print('DCT F1-score',fdct*100)
print('DCT ROCAUC score:',roc_auc_score(y2_test, dct_pred)*100)
print('DCT Recall score:', recalls_dt2*100)
print('DCT Precision Score:', precisions_dt2*100)
print('DCT Confusion Matrix', confusion_matrix(y2_test, dct_pred), "\n")
print('DCT Classification', classification_report(y2_test, dct_pred), "\n")
print('DCT Accuracy Score', accuracys_dt2*100)
print("Execution Time for Decision Tree Imbalanced: ", dt2_end - start12, "seconds")


from sklearn.naive_bayes import MultinomialNB
start13 = time.time()
Naive = MultinomialNB()
model_nb2 = Naive.fit(x2_train,y2_train)
probs_nb2 = model_nb2.predict_proba(x2_test)[:, 1]
# predict the labels on validation dataset
ny_pred = Naive.predict(x2_test)
fna = f1_score(ny_pred,y2_test)
recalls_nb2 = recall_score(y2_test, ny_pred)
precisions_nb2 = precision_score(y2_test, ny_pred)
accuracys_nb2 = accuracy_score(y2_test, ny_pred)
nb2_end = time.time()
# Use accuracy_score function to get the accuracy
print("===Naive Bayes with TfidfVectorizer Imabalanced - 2010", df2_l, ts05)
print('Naive F1-score',fna*100)
print('Naive ROCAUC score:',roc_auc_score(y2_test, ny_pred)*100)
print('Naive Recall score:', recalls_nb2*100)
print('Naive Precision Score:', precisions_nb2*100)
print('Naive Confusion Matrix', confusion_matrix(y2_test, ny_pred), "\n")
print('Naive Classification', classification_report(y2_test, ny_pred), "\n")
print('Naive Accuracy Score', accuracys_nb2*100)
print("Execution Time for Naive Bayes Imbalanced: ", nb2_end - start13, "seconds")

# XGBoost Classifier

from xgboost import XGBClassifier
start14 = time.time()
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
xgb_model2 = XGBClassifier().fit(x2_train, y2_train)
probs_xg2 = xgb_model2.predict_proba(x2_test)[:, 1]
# predict
xgb_y_predict = xgb_model2.predict(x2_test)
fxg = f1_score(xgb_y_predict,y2_test)
recalls_xg2 = recall_score(xgb_y_predict, y2_test)
precisions_xg2 = precision_score(xgb_y_predict, y2_test)
accuracys_xg2 = accuracy_score(xgb_y_predict, y2_test)
xg2_end = time.time()
print("===XGB with TfidfVectorizer Imbalanced - 2010", df2_l, ts05)
print('XGB F1-Score', fxg*100)
print('XGB ROCAUC Score:', roc_auc_score(xgb_y_predict, y2_test)*100)
print('XGB Recall score:', recalls_xg2*100)
print('XGB Precision Score:', precisions_xg2 *100)
print('XGB Confusion Matrix', confusion_matrix(xgb_y_predict, y2_test), "\n")
print('XGB Classification', classification_report(xgb_y_predict, y2_test), "\n")
print('XGB Accuracy Score', accuracys_nb2*100)
print("Execution Time for XGBoost Classifier Imbalanced: ", xg2_end - start4, "seconds")
'''
'''
# Support Vector Machine Classifier
from sklearn.svm import SVC
csvm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='auto').fit(x_train,y_train)
probs_cs = csvm_model.predict_proba(x_test)[:, 1]
# predict the labels on validation dataset
csvy_pred = csvm_model.predict(x_test)
fcsvm = f1_score(csvy_pred,y_test)
print("===C-SVM with TfidfVectorizer Imbalanced - 2010===")
print('C-SVM F1-score',fcsvm*100)
# Use accuracy_score function to get the accuracy
print('C-SVM ROCAUC score:',roc_auc_score(y_test, csvy_pred)*100, "\n")
print('C-SVM Recall score:', recall_score(y_test, csvy_pred)*100, "\n")
print('C-SVM Precision Score:', precision_score(y_test, csvy_pred)*100, "\n")
print('C-SVM Confusion Matrix', confusion_matrix(y_test, csvy_pred), "\n")
print('C-SVM Classification', classification_report(y_test, csvy_pred), "\n")
print('C-SVM Accuracy Score', accuracy_score(y_test, csvy_pred)*100, "\n")
'''
'''
# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
start15 = time.time()
rfc_model2 = RandomForestClassifier(n_estimators=1000, random_state=0).fit(x2_train,y2_train)
probs_rf2 = rfc_model2.predict_proba(x2_test)[:, 1]
rfc_pred = rfc_model2.predict(x2_test)
frfc = f1_score(rfc_pred,y2_test)
recalls_rf2 = recall_score(rfc_pred, y2_test)
precisions_rf2 = precision_score(rfc_pred, y2_test)
accuracys_rf2 = accuracy_score(rfc_pred, y2_test)
rf2_end = time.time()
print("====RandomForest with Tfidf Imbalanced 2010", df2_l, ts05)
print('RFC F1 score', frfc*100)
print('RFC ROCAUC Score:', roc_auc_score(y2_test, rfc_pred)*100)
print('RFC Recall score:', recalls_rf*100)
print('RFC Precision Score:', precisions_rf*100)
print('RFC Confusion Matrix', confusion_matrix(y2_test,rfc_pred), "\n")
print('RFC Classification', classification_report(y2_test,rfc_pred), "\n")
print('RFC Accuracy Score', accuracys_rf*100)
print("Execution Time for Random Forest Classifier Imbalanced: ", rf_end - start5, "seconds")

from sklearn.metrics import auc, precision_recall_curve

y2_test_int = y2_test.replace({'Positive': 1, 'Negative': 0})

baseline_model2 = sum(y2_test_int == 1) / len(y2_test_int)

precision_lr2, recall_lr2, _ = precision_recall_curve(y2_test_int, probs_lr2)
auc_lr2 = auc(recall_lr2, precision_lr2)

precision_dt2, recall_dt2, _ = precision_recall_curve(y2_test_int, probs_dt2)
auc_dt2 = auc(recall_dt2, precision_dt2)

precision_nb2, recall_nb2, _ = precision_recall_curve(y2_test_int, probs_nb2)
auc_nb2 = auc(recall_nb2, precision_nb2)

precision_xg2, recall_xg2, _ = precision_recall_curve(y2_test_int, probs_xg2)
auc_xg2 = auc(recall_xg2, precision_xg2)

precision_cs, recall_cs, _ = precision_recall_curve(y_test_int, probs_cs)
auc_cs = auc(recall_cs, precision_cs)


precision_rf2, recall_rf2, _ = precision_recall_curve(y2_test_int, probs_rf2)
auc_rf2  = auc(recall_rf2, precision_rf2)

'''
'''
# Update the collection over the test set
Accuracy_LRN.append(accuracys_lr)
Recall_LRN.append(recalls_lr)
Precision_LRN.append(precisions_lr)

Accuracy_DCT.append(accuracys_dt)
Recall_DCT.append(recalls_dt)
Precision_DCT.append(precisions_dt)

Accuracy_NBB.append(accuracys_nb)
Recall_NBB.append(recalls_nb)
Precision_NBB.append(precisions_nb)

Accuracy_XGB.append(accuracys_xg)
Recall_XGB.append(recalls_xg)
Precision_XGB.append(precisions_xg)
'''
'''
plt.figure(figsize=(12, 7))
plt.plot([0, 1], [baseline_model, baseline_model], linestyle='--', label='Baseline model')
plt.plot(recall_lr, precision_lr, label=f'AUC Log. Reg. Imb.) = {auc_lr:.2f}')
plt.plot(recall_dt, precision_dt, label=f'AUC Dec. Tree Imb.) = {auc_dt:.2f}')
plt.plot(recall_nb, precision_nb, label=f'AUC Nai Bay. Imb.) = {auc_rf:.2f}')
plt.plot(recall_xg, precision_xg, label=f'AUC XGB Imb.) = {auc_xg:.2f}')

plt.plot(recall_lr1, precision_lr1, label=f'AUC Log. Reg. ROS) = {auc_lr:.2f}')
plt.plot(recall_dt1, precision_dt1, label=f'AUC Dec. Tree ROS) = {auc_dt:.2f}')
plt.plot(recall_nb1, precision_nb1, label=f'AUC Nai Bay. ROS) = {auc_rf:.2f}')
plt.plot(recall_xg1, precision_xg1, label=f'AUC XGB ROS) = {auc_xg:.2f}')
plt.plot(recall_lr2, precision_lr2, label=f'AUC Log. Reg. ROS) = {auc_lr2:.2f}')
plt.plot(recall_dt2, precision_dt2, label=f'AUC Dec. Tree ROS) = {auc_dt2:.2f}')
plt.plot(recall_nb2, precision_nb2, label=f'AUC Nai Bay. ROS) = {auc_rf2:.2f}')
plt.plot(recall_xg2, precision_xg2, label=f'AUC XGB ROS) = {auc_xg2:.2f}')

plt.title('Precision-Recall Curves 2010: Imbalanced', size=20)
plt.xlabel('Recall', size=14)
plt.ylabel('Precision', size=14)
plt.legend()
plt.show();
'''
'''
df2['label'].value_counts()

import seaborn as sns
h = sns.countplot(df2['label'])
h.set_xticklabels(['Negative','Positive'])
plt.show()

# class count
label_count_neg1, label_count_pos1 = df2['label'].value_counts()

# Separate class
label_neg1 = df2[df2['label'] == 0]
label_pos1 = df2[df2['label'] == 1]# print the shape of the class
print('Label Negative:', label_neg1.shape)
print('Label Positive:', label_pos1.shape)

label_neg_under1 = label_neg1.sample(label_count_pos1)

test_under1 = pd.concat([label_neg_under1, label_pos1], axis=0)

print("total class of pos and neg :",test_under1['label'].value_counts())# plot the count after under-sampling
test_under1['label'].value_counts().plot(kind='bar', title='label (target)')

label_pos_over1 = label_pos1.sample(label_count_neg1, replace=True)

test_over1 = pd.concat([label_pos_over1, label_neg1], axis=0)

print("total class of pos and neg:",test_under1['label'].value_counts())# plot the count after under-sampeling
test_over1['label'].value_counts().plot(kind='bar', title='label (target)')

import imblearn

# import library
from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
x1 = x_tfidf2
y1 = df2['label']
print(x1.shape)
print(y1.shape)


rus1 = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable
x_rus1, y_rus1 = rus1.fit_resample(x1, y1)

print('Original dataset shape:', Counter(y1))
print('Resample dataset shape', Counter(y_rus1))

# import library
from collections import Counter
from imblearn.over_sampling import RandomOverSampler
'''
'''
x =x_tfidf
y = df1["label"]
'''
'''
#ros1 = RandomUnderSampler(random_state=42)
#ros = RandomOverSampler(random_state=42)
#Random over-sampling with imblearn
# fit predictor and target variable
x_rus1, y_rus1 = rus1.fit_resample(x1, y1)

print('Original dataset shape', Counter(y1))
print('Resample dataset shape', Counter(y_rus1))
#Random over-sampling with imblearn
ros1 = RandomOverSampler(random_state=42)

# fit predictor and target variable
x_ros1, y_ros1 = ros1.fit_resample(x1, y1)
'''
'''
print('Original dataset shape', Counter(y1))
print('Resample dataset shape', Counter(y_ros1))
# import library
'''
'''
# Under-sampling: Tomek links
from imblearn.under_sampling import TomekLinks
from collections import Counter


tl1 = RandomOverSampler(sampling_strategy='majority')

# fit predictor and target variable
x_tl1, y_tl1 = ros.fit_resample(x1, y1)

print('Original dataset shape', Counter(y1))
print('Resample dataset shape', Counter(y_ros1))

# import library
from imblearn.over_sampling import SMOTE

smote1 = SMOTE()

# fit predictor and target variable
x_smote1, y_smote1 = smote1.fit_resample(x1, y1)

print('Original dataset shape', Counter(y1))
print('Resample dataset shape', Counter(y_ros1))


from imblearn.under_sampling import NearMiss

nm1 = NearMiss()

x_nm1, y_nm1 = nm1.fit_resample(x1, y1)

print('Original dataset shape:', Counter(y1))
print('Resample dataset shape:', Counter(y_nm1))

#====Sampling Technique=====
x3_train, x3_test, y3_train, y3_test = train_test_split(x_tl1,y_tl1,test_size=0.5)

start16 = time.time()
log = LogisticRegression(penalty='l2',random_state=0, solver='lbfgs', multi_class='auto', max_iter=500)
model_lr3 = log.fit(x3_train,y3_train)
probs_lr3 = model_lr3.predict_proba(x3_test)[:, 1]
ly_prediction = log.predict(x3_test)
fly = f1_score(ly_prediction,y3_test)
recalls_lr3 = recall_score(y3_test, ly_prediction)
precisions_lr3 = precision_score(y3_test, ly_prediction)
accuracys_lr3 = accuracy_score(y3_test, ly_prediction)
lr3_end = time.time()
print("===Logistic Regression with TfidfVectorizer Tomelinks - 2010", df2_l, ts05)
print('Logistic F1-score',fly*100)
print('Logistic ROCAUC score:',roc_auc_score(y3_test, ly_prediction)*100)
print('Logistic Recall score:', recalls_lr3*100)
print('Logistic Precision Score:', precisions_lr3*100)
print('Logistic Confusion Matrix', confusion_matrix(y3_test,ly_prediction), "\n")
print('Logistic Classification', classification_report(y3_test,ly_prediction), "\n")
print('Logistic Accuracy Score', accuracys_lr3*100)
print("Execution Time for Logistic Regression Tomelinks: ", lr3_end - start6, "seconds")

from sklearn.tree import DecisionTreeClassifier
start17 = time.time()
DCT = DecisionTreeClassifier()
model_dt3 = DCT.fit(x3_train, y3_train)
probs_dt3 = model_dt3.predict_proba(x3_test)[:, 1]
dct_pred = DCT.predict(x3_test)
fdct = f1_score(dct_pred,y3_test)
recalls_dt3 = recall_score(y3_test, dct_pred)
precisions_dt3 = precision_score(y3_test, dct_pred)
accuracys_dt3 = accuracy_score(y3_test, dct_pred)
print("===DecisionTreeClassifier with TfidfVectorizer Tomelinks - 2010", df2_l, ts05)
dt3_end = time.time()
print('DCT F1-score',fdct*100)
print('DCT ROCAUC score:',roc_auc_score(y3_test, dct_pred)*100)
print('DCT Recall score:', recalls_dt3*100)
print('DCT Precision Score:', precisions_dt3*100)
print('DCT Confusion Matrix', confusion_matrix(y3_test, dct_pred), "\n")
print('DCT Classification', classification_report(y3_test, dct_pred), "\n")
print('DCT Accuracy Score', accuracys_dt3*100)
print("Execution Time for Decision Tree Tomelinks: ", dt3_end - start17, "seconds")

from sklearn.naive_bayes import MultinomialNB
start18 = time.time()
Naive = MultinomialNB()
model_nb3 = Naive.fit(x3_train,y3_train)
probs_nb3 = model_nb3.predict_proba(x3_test)[:, 1]
# predict the labels on validation dataset
ny_pred = Naive.predict(x3_test)
fna = f1_score(ny_pred,y3_test)
recalls_nb3 = recall_score(y3_test, ny_pred)
precisions_nb3 = precision_score(y3_test, ny_pred)
accuracys_nb3 = accuracy_score(y3_test, ny_pred)
# Use accuracy_score function to get the accuracy
print("===Naive Bayes with TfidfVectorizer Tomelinks - 2010", df2_l, ts05)
nb3_end = time.time()
print('Naive F1-score',fna*100)
print('Naive ROCAUC score:',roc_auc_score(y3_test, ny_pred)*100)
print('Naive Recall score:', recalls_nb3*100)
print('Naive Precision Score:', precisions_nb3*100)
print('Naive Confusion Matrix', confusion_matrix(y3_test, ny_pred), "\n")
print('Naive Classification', classification_report(y3_test, ny_pred), "\n")
print('Naive Accuracy Score', accuracys_nb3*100)
print("Execution Time for Naive Bayes Tomelinks: ", nb3_end - start8, "seconds")

# XGBoost Classifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
start19 = time.time()
XGB = XGBClassifier()
xgb_model3 = XGB.fit(x3_train,y3_train)
probs_xg3 = xgb_model3.predict_proba(x3_test)[:, 1]
# predict
xgb_y_predict = xgb_model3.predict(x3_test)
fxg = f1_score(xgb_y_predict,y3_test)
recalls_xg3 = recall_score(y3_test, xgb_y_predict)
precisions_xg3 = precision_score(y3_test, xgb_y_predict)
accuracys_xg3 = accuracy_score(y3_test, xgb_y_predict)
print("===XGB with TfidfVectorizer Tomelinks- 2010", df2_l, ts05)
xg3_end = time.time()
print('XGB F1-Score', fxg*100)
print('XGB ROCAUC Score:', roc_auc_score(xgb_y_predict, y3_test)*100)
print('XGB Recall score:', recalls_xg3*100)
print('XGB Precision Score:', precisions_xg3*100)
print('XGB Confusion Matrix', confusion_matrix(xgb_y_predict, y3_test), "\n")
print('XGB Classification', classification_report(xgb_y_predict, y3_test), "\n")
print('XGB Accuracy Score', accuracys_xg3*100)
print("Execution Time for XGBoost Tomelinks: ", xg3_end - start19, "seconds")


# Support Vector Machine Classifier
from sklearn.svm import SVC
csvm_model1 = SVC(C=1.0, kernel='linear', degree=3, gamma='auto').fit(x1_train,y1_train)
probs_cs1 = csvm_model1.predict_proba(x1_test)[:, 1]
# predict the labels on validation dataset
csvy_pred = csvm_model1.predict(x1_test)
fcsvm = f1_score(csvy_pred,y1_test)
print("===C-SVM with TfidfVectorizer Tomelinks - 2010===")
print('C-SVM F1-score',fcsvm*100)
# Use accuracy_score function to get the accuracy
print('C-SVM ROCAUC score:',roc_auc_score(y1_test, csvy_pred)*100)
print('C-SVM Recall score:', recall_score(y1_test, csvy_pred)*100)
print('C-SVM Precision Score:', precision_score(y1_test, csvy_pred)*100)
print('C-SVM Confusion Matrix', confusion_matrix(y1_test, csvy_pred))
print('C-SVM Classification', classification_report(y1_test, csvy_pred))
print('C-SVM Accuracy Score', accuracy_score(y1_test, csvy_pred)*100)


# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
start20 = time.time()
rfc_model3 = RandomForestClassifier(n_estimators=1000, random_state=0).fit(x3_train,y3_train)
probs_rf3 = rfc_model3.predict_proba(x3_test)[:, 1]
rfc_pred = rfc_model3.predict(x3_test)
frfc = f1_score(rfc_pred,y3_test)
recalls_rf3 = recall_score(y3_test, rfc_pred)
precisions_rf3 = precision_score(y3_test, rfc_pred)
accuracys_rf3 = accuracy_score(y3_test, rfc_pred)
rf3_end = time.time()
print("====RandomForest with Tfidf Tomelinks-  2010", df2_l, ts05)
print('RFC F1 score', frfc*100)
print('RFC ROCAUC Score:', roc_auc_score(y3_test, rfc_pred)*100)
print('RFC Recall score:', recalls_rf3*100)
print('RFC Precision Score:', precisions_rf3*100)
print('RFC Confusion Matrix', confusion_matrix(y3_test,rfc_pred))
print('RFC Classification', classification_report(y3_test,rfc_pred))
print('RFC Accuracy Score', accuracys_rf3*100)
print("Execution Time for Random Forest Tomelinks: ", rf3_end - start10, "seconds")

# Update the collection over the test set
Accuracy_LRN_s.append(accuracys_lr)
Recall_LRN_s.append(recalls_lr)
Precision_LRN_s.append(precisions_lr)

Accuracy_DCT_s.append(accuracys_dt)
Recall_DCT_s.append(recalls_dt)
Precision_DCT_s.append(precisions_dt)

Accuracy_NBB_s.append(accuracys_nb)
Recall_NBB_s.append(recalls_nb)
Precision_NBB_s.append(precisions_nb)

Accuracy_XGB_s.append(accuracys_xg)
Recall_XGB_s.append(recalls_xg)
Precision_XGB_s.append(precisions_xg)

y_test_int3 = y3_test.replace({'Positive': 1, 'Negative': 0})

baseline_model3 = sum(y_test_int3 == 1) / len(y_test_int3)

precision_lr3, recall_lr3, _ = precision_recall_curve(y_test_int3, probs_lr3)
auc_lr3 = auc(recall_lr3, precision_lr3)

precision_dt3, recall_dt3, _ = precision_recall_curve(y_test_int3, probs_dt3)
auc_dt3 = auc(recall_dt3, precision_dt3)

precision_nb3, recall_nb3, _ = precision_recall_curve(y_test_int3, probs_nb3)
auc_nb3 = auc(recall_nb3, precision_nb3)

precision_xg3, recall_xg3, _ = precision_recall_curve(y_test_int3, probs_xg3)
auc_xg3 = auc(recall_xg3, precision_xg3)

precision_cs1, recall_cs1, _ = precision_recall_curve(y_test_int1, probs_cs1)
auc_cs1 = auc(recall_cs1, precision_cs1)

precision_rf3, recall_rf3, _ = precision_recall_curve(y_test_int3, probs_rf3)
auc_rf3 = auc(recall_rf3, precision_rf3)




plt.plot(recall_lr1, precision_lr1, label=f'AUC Log. Reg. Tomelinks) = {auc_lr:.2f}')
plt.plot(recall_dt1, precision_dt1, label=f'AUC Dec. Tree Tomelinks) = {auc_dt:.2f}')
plt.plot(recall_nb1, precision_nb1, label=f'AUC Nai Bay. Tomelinks) = {auc_rf:.2f}')
plt.plot(recall_xg1, precision_xg1, label=f'AUC XGB Tomelinks) = {auc_xg:.2f}')
'''
'''
print(60*'-')
print('The results below over test set')

print("Mean Accuracy call for Logistic Regression: {0}".format(np.mean(Accuracy_LRN)))
print("Mean Precision call for Logistic Regression: {0}".format(np.mean(Precision_LRN)))
print("Mean Recall call for Logistic Regression: {0}".format(np.mean(Recall_LRN)))

print("Mean Accuracy call for Decision Tree: {0}".format(np.mean(Accuracy_DCT)))
print("Mean Precision call for Decision Tree: {0}".format(np.mean(Recall_DCT)))
print("Mean Recall call for Decision Tree: {0}".format(np.mean(Precision_DCT)))

print("Mean Accuracy call for Naive Bayes: {0}".format(np.mean(Accuracy_NBB)))
print("Mean Precision call for Naive Bayes: {0}".format(np.mean(Precision_NBB)))
print("Mean Recall call for Naive Bayes: {0}".format(np.mean(Recall_NBB)))

print("Mean Accuracy call for XGBoost Classifier: {0}".format(np.mean(Accuracy_XGB)))
print("Mean Precision call for XGBoost Classifier: {0}".format(np.mean(Precision_XGB)))
print("Mean Recall call for XGBoost Classifier: {0}".format(np.mean(Recall_XGB)))

print(20*'-')
print('The results below for sampling techniques')
print("T - Mean Accuracy call for Logistic Regression: {0}".format(np.mean(Accuracy_LRN_s)))
print("T- Mean Precision call for Logistic Regression: {0}".format(np.mean(Precision_LRN_s)))
print("T- Mean Recall call for Logistic Regression: {0}".format(np.mean(Recall_LRN_s)))

print("T- Mean Accuracy call for Decision Tree: {0}".format(np.mean(Accuracy_DCT_s)))
print("T- Mean Precision call for Decision Tree: {0}".format(np.mean(Recall_DCT_s)))
print("T- Mean Recall call for Decision Tree: {0}".format(np.mean(Precision_DCT_s)))

print("T- Mean Accuracy call for Naive Bayes: {0}".format(np.mean(Accuracy_NBB_s)))
print("T- Mean Precision call for Naive Bayes: {0}".format(np.mean(Precision_NBB_s)))
print("T- Mean Recall call for Naive Bayes: {0}".format(np.mean(Recall_NBB_s)))

print("T- Mean Accuracy call for XGBoost: {0}".format(np.mean(Accuracy_XGB_s)))
print("T- Mean Precision call for XGBoost: {0}".format(np.mean(Precision_XGB_s)))
print("T- Mean Recall call for XGBoost: {0}".format(np.mean(Recall_XGB_s)))
'''
## Paired-t test
# Naive Bayes Classifiers
'''
ttest_pre_1 = stats.ttest_rel(precision_nb,precision_nb1)
ttest_rec_1 = stats.ttest_rel(recall_nb,recall_nb1)
#ttest_pre_1 = stats.ttest_rel(Precision_NBB,Precision_LRN)
#ttest_re_1 = stats.ttest_rel(Recall_NBB,Recall_LRN)
#ttest_run_1 = stats.ttest_rel(Runtime_NNN,Runtime_LLL)
print('Pair T-Test between Precision between Naive Bayes Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_1))
print('Pair T-Test between Recall between Naive Bayes Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_1))
#print('Pair T-Test between Precision and Recall of Naive Bayes - 2010 Tomelinks {0}'.format(ttest_sam_1))
#print('Precision - Pair-t test between Neural and Logistic {0}'.format(ttest_pre_1))
#print('Recall - Pair-t test between Neural and Logistic {0}'.format(ttest_re_1))
#print('Runtime - Pair-t test between Neural and Logistic {0}'.format(ttest_run_1))
print(60*'-')

# Decision Tree Classifier
ttest_pre_2 = stats.ttest_rel(precision_dt,precision_dt1)
ttest_rec_2 = stats.ttest_rel(recall_dt1,recall_dt1)
#ttest_pre_2 = stats.ttest_rel(Precision_NBB,Precision_DCT)
#ttest_re_2 = stats.ttest_rel(Recall_NBB,Recall_DCT)
#ttest_run_2 = stats.ttest_rel(Runtime_NBB,Runtime_DCT)
print('Pair T-Test of Precision between Decision Tree Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_2))
print('Pair T-Test of Recall between Decision Tree Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_2))
#print('Precision - Pair-t test between Neural and Naive {0}'.format(ttest_pre_2))
#print('Recall - Pair-t test between Neural and Naive {0}'.format(ttest_re_2))
#print('Runtime - Pair-t test between Neural and Naive {0}'.format(ttest_run_2))
print(60*'-')

# XGB Boost Classifier
ttest_pre_3 = stats.ttest_rel(precision_xg,precision_xg1)
ttest_rec_3 = stats.ttest_rel(recall_xg,recall_xg1)
#ttest_pre_3 = stats.ttest_rel(Precision_XGB,Precision_NBB)
#ttest_re_3 = stats.ttest_rel(Recall_XGB,Recall_NBB)
#ttest_run_3 = stats.ttest_rel(Runtime_LLL,Runtime_NBB)
print('Pair T-Test of Precision between XGBoost Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_3))
print('Pair T-Test of Recall between XGBoost Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_3))
#print('Precision - Pair-t test between Logistic and Naive {0}'.format(ttest_pre_3))
#print('Recall - Pair-t test between Logistic and Naive {0}'.format(ttest_re_3))
#print('Runtime - Pair-t test between Logistic and Naive {0}'.format(ttest_run_3))

# Random Forest Classifier
ttest_pre_4 = stats.ttest_rel(precision_rf,precision_rf1)
ttest_rec_4 = stats.ttest_rel(recall_rf,recall_rf1)
#ttest_pre_4 = stats.ttest_rel(Precision_XGB,Precision_NBB)
#ttest_re_4 = stats.ttest_rel(Recall_XGB,Recall_NBB)
#ttest_run_4 = stats.ttest_rel(Runtime_LLL,Runtime_NBB)
print('Pair T-Test of Precision between Random Forest Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_4))
print('Pair T-Test of Recall between Random Forest Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_4))
#print('Precision - Pair-t test between Logistic and Naive {0}'.format(ttest_pre_4))
#print('Recall - Pair-t test between Logistic and Naive {0}'.format(ttest_re_4))
#print('Runtime - Pair-t test between Logistic and Naive {0}'.format(ttest_run_4))

ttest_pre_5 = stats.ttest_rel(precision_lr,precision_lr1)
ttest_rec_5 = stats.ttest_rel(recall_lr,recall_lr1)
print('Pair T-Test of Precision between Logistic Regression Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_5))
print('Pair T-Test of Precision between Logistic Regression Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_5))

print(20*'-')
print("Execution Time for Logistic Regression Imbalanced - 2010", df2_l, ts05 ,":", lr2_end - start1, "seconds")
print("Execution Time for Decision Tree Imbalanced - 2010", df2_l, ts05 ,":", dt2_end - start2, "seconds")
print("Execution Time for Naive Bayes Imbalanced - 2010 ", df2_l, ts05 ,":", nb2_end - start3, "seconds")
print("Execution Time for XGB Imbalanced - 2010 ", df2_l, ts05 ,":", xg2_end - start4, "seconds")
print("Execution Time for Random Forest Imbalanced - 2010 ", df2_l, ts05 ,":", rf2_end - start5, "seconds")
print("Execution Time for Logistic Regression Tomelinks - 2010 ", df2_l, ts05 ,":", lr3_end - start6, "seconds")
print("Execution Time for Decision Tree Tomelinks - 2010 ", df2_l, ts05 ,":", dt3_end - start7, "seconds")
print("Execution Time for Naive Bayes Tomelinks - 2010 ", df2_l, ts05 ,":", nb3_end - start8, "seconds")
print("Execution Time for XGB Tomelinks - 2010 ", df2_l, ts05 ,":", xg3_end - start9, "seconds")
print("Execution Time for Random Forest Tomelinks - 2010 ", df2_l, ts05 ,":", rf3_end - start10, "seconds")
print("\n")

y_test_int2 = y2_test.replace({'Positive': 1, 'Negative': 0})
print("Print:", type(y_test_int2))
y_test_int3 = y3_test.replace({'Positive': 1, 'Negative': 0})
print("Print:", type(y_test_int3))
#ytest_sp1 = pd.concat([y_test_int,y_test_int1])
#print(ytest_sp1)
precision_imb2 =  (precisions_lr2 + precisions_dt2 + precisions_nb2 + precisions_xg2 + precisions_rf2) / 5
print("Average of Precision Imbalanced Split 2:", precision_imb2)
precision_sam2 =  (precisions_lr3 + precisions_dt3 + precisions_nb3 + precisions_xg3 + precisions_rf3) / 5
print("Average of Precision Sampling Split 2:", precision_sam2)
precision_sp2 = (precision_imb2 + precision_sam2) / 2
print("Average of Precision and Sampling Split 2:", precision_sp2)
recall_imb2 = (recalls_lr2 + recalls_dt2 + recalls_nb2 + recalls_xg2 + recalls_rf2) / 5
print("Average of Recall Imbalanced Split 2:", recall_imb2)
recall_sam2 = (recalls_lr3 + recalls_lr3 + recalls_nb3 + recalls_xg3 + recalls_rf3) / 5
print("Average of Recall Sampling Split 2:", recall_sam2)
recall_sp2 = (recall_imb2 + recall_sam2) / 2
print("Average of Recall Imbalanced and Sampling Split 2:", recall_sp2)
probs_imb2 = (probs_lr2 + probs_dt2 + probs_nb2 + probs_xg2 + probs_rf2) / 5
print("Probability Array of Imbalanced Split 2:", probs_imb2)
probs_sam2 = (probs_lr3 + probs_lr3 + probs_nb3 + probs_xg3 + probs_rf3) / 5
print("Probability Array of Sampling Split 2:",probs_sam2)
#print(probs_imb1.shape())
#print(probs_sam1.shape())
#probs_sp1 = ((np.array(probs_imb1)) + (np.array(probs_sam1))) / 2

#y_test_avg1 = ytest_sp1.replace({'Positive': 1, 'Negative': 0})

precision_imb2, recall_imb2, _ = precision_recall_curve(y_test_int2, probs_imb2)
auc_imb2 = auc(recall_imb2, precision_imb2)

precision_sam2, recall_sam2, _ = precision_recall_curve(y_test_int3, probs_sam2)
auc_sam2 = auc(recall_sam2, precision_sam2)
# ==DF2==

# ==DF3==
x_tfidf3 = tfidf_vect.fit_transform(df3["lemmatized"])

x4_train, x4_test, y4_train, y4_test = train_test_split(x_tfidf3,df3["label"],test_size=0.5)

# Convert [1,0,0] to 0. [0,1,0] to 1. [0,0,1] to 2
ytrain_ = np.zeros(y_train.shape[0])
for i in range(y_train.shape[0]):
    index = np.argmax(y_train[i])
    if index == 0:
        ytrain_[i] = int(0)
    elif index == 1:
        ytrain_[i] = int(1)
    else:
        ytrain_[i] = int(2)

    ytest_ = np.zeros(y_test.shape[0])
    for i in range(y_test.shape[0]):
        index = np.argmax(y_test[i])
        if index == 0:
            ytest_[i] = int(0)
        elif index == 1:
            ytest_[i] = int(1)
        else:
            ytest_[i] = int(2)

start21 = time.time()
log = LogisticRegression(penalty='l2',random_state=0, solver='lbfgs', multi_class='auto', max_iter=500)
model_lr4 = log.fit(x4_train,y4_train)
probs_lr4 = model_lr4.predict_proba(x4_test)[:, 1]
ly_prediction = log.predict(x4_test)
fly = f1_score(ly_prediction,y4_test)
recalls_lr4 = recall_score(y4_test, ly_prediction)
precisions_lr4 = precision_score(y4_test, ly_prediction)
accuracys_lr4 = accuracy_score(y4_test, ly_prediction)
print("===Logistic Regression with TfidfVectorizer Imbalanced - 2010", df3_l, ts05)
lr4_end = time.time()
print('Logistic F1-score',fly*100)
print('Logistic ROCAUC score:',roc_auc_score(y4_test, ly_prediction)*100)
print('Logistic Recall score:', recalls_lr4*100)
print('Logistic Precision Score:', precisions_lr4*100)
print('Logistic Confusion Matrix', confusion_matrix(y4_test,ly_prediction), "\n")
print('Logistic Classification', classification_report(y4_test,ly_prediction), "\n")
print('Logistic Accuracy Score', accuracys_lr4*100)
print("Execution Time for Logistic Regression Imbalanced: ", lr4_end - start11, "seconds")

start22 = time.time()
from sklearn.tree import DecisionTreeClassifier
DCT = DecisionTreeClassifier()
model_dt4 = DCT.fit(x4_train, y4_train)
probs_dt4 = model_dt4.predict_proba(x4_test)[:, 1]
dct_pred = DCT.predict(x4_test)
fdct = f1_score(dct_pred,y4_test)
recalls_dt4 = recall_score(y4_test, dct_pred)
precisions_dt4 = precision_score(y4_test, dct_pred)
accuracys_dt4 = accuracy_score(y4_test, dct_pred)
print("===DecisionTreeClassifier with TfidfVectorizer Imbalanced - 2010", df3_l, ts05)
dt4_end = time.time()
print('DCT F1-score',fdct*100)
print('DCT ROCAUC score:',roc_auc_score(y4_test, dct_pred)*100)
print('DCT Recall score:', recalls_dt4*100)
print('DCT Precision Score:', precisions_dt4*100)
print('DCT Confusion Matrix', confusion_matrix(y4_test, dct_pred), "\n")
print('DCT Classification', classification_report(y4_test, dct_pred), "\n")
print('DCT Accuracy Score', accuracys_dt4*100)
print("Execution Time for Decision Tree Imbalanced: ", dt4_end - start12, "seconds")


from sklearn.naive_bayes import MultinomialNB
start13 = time.time()
Naive = MultinomialNB()
model_nb4 = Naive.fit(x4_train,y4_train)
probs_nb4 = model_nb4.predict_proba(x4_test)[:, 1]
# predict the labels on validation dataset
ny_pred = Naive.predict(x4_test)
fna = f1_score(ny_pred,y4_test)
recalls_nb4 = recall_score(y4_test, ny_pred)
precisions_nb4 = precision_score(y4_test, ny_pred)
accuracys_nb4 = accuracy_score(y4_test, ny_pred)
nb4_end = time.time()
# Use accuracy_score function to get the accuracy
print("===Naive Bayes with TfidfVectorizer Imabalanced - 2010", df3_l, ts05)
print('Naive F1-score',fna*100)
print('Naive ROCAUC score:',roc_auc_score(y4_test, ny_pred)*100)
print('Naive Recall score:', recalls_nb4*100)
print('Naive Precision Score:', precisions_nb4*100)
print('Naive Confusion Matrix', confusion_matrix(y4_test, ny_pred), "\n")
print('Naive Classification', classification_report(y4_test, ny_pred), "\n")
print('Naive Accuracy Score', accuracys_nb4*100)
print("Execution Time for Naive Bayes Imbalanced: ", nb4_end - start13, "seconds")

# XGBoost Classifier

from xgboost import XGBClassifier
start14 = time.time()
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
xgb_model4 = XGBClassifier().fit(x4_train, y4_train)
probs_xg4 = xgb_model4.predict_proba(x4_test)[:, 1]
# predict
xgb_y_predict = xgb_model4.predict(x4_test)
fxg = f1_score(xgb_y_predict,y4_test)
recalls_xg4 = recall_score(xgb_y_predict, y4_test)
precisions_xg4 = precision_score(xgb_y_predict, y4_test)
accuracys_xg4 = accuracy_score(xgb_y_predict, y4_test)
xg4_end = time.time()
print("===XGB with TfidfVectorizer Imbalanced - 2010", df3_l, ts05)
print('XGB F1-Score', fxg*100)
print('XGB ROCAUC Score:', roc_auc_score(xgb_y_predict, y4_test)*100)
print('XGB Recall score:', recalls_xg4*100)
print('XGB Precision Score:', precisions_xg4 *100)
print('XGB Confusion Matrix', confusion_matrix(xgb_y_predict, y4_test), "\n")
print('XGB Classification', classification_report(xgb_y_predict, y4_test), "\n")
print('XGB Accuracy Score', accuracys_nb4*100)
print("Execution Time for XGBoost Classifier Imbalanced: ", xg4_end - start4, "seconds")

# Support Vector Machine Classifier
from sklearn.svm import SVC
csvm_model = SVC(C=1.0, kernel='linear', degree=3, gamma='auto').fit(x_train,y_train)
probs_cs = csvm_model.predict_proba(x_test)[:, 1]
# predict the labels on validation dataset
csvy_pred = csvm_model.predict(x_test)
fcsvm = f1_score(csvy_pred,y_test)
print("===C-SVM with TfidfVectorizer Imbalanced - 2010===")
print('C-SVM F1-score',fcsvm*100)
# Use accuracy_score function to get the accuracy
print('C-SVM ROCAUC score:',roc_auc_score(y_test, csvy_pred)*100, "\n")
print('C-SVM Recall score:', recall_score(y_test, csvy_pred)*100, "\n")
print('C-SVM Precision Score:', precision_score(y_test, csvy_pred)*100, "\n")
print('C-SVM Confusion Matrix', confusion_matrix(y_test, csvy_pred), "\n")
print('C-SVM Classification', classification_report(y_test, csvy_pred), "\n")
print('C-SVM Accuracy Score', accuracy_score(y_test, csvy_pred)*100, "\n")


# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
start15 = time.time()
rfc_model4 = RandomForestClassifier(n_estimators=1000, random_state=0).fit(x4_train,y4_train)
probs_rf4 = rfc_model4.predict_proba(x4_test)[:, 1]
rfc_pred = rfc_model4.predict(x4_test)
frfc = f1_score(rfc_pred,y4_test)
recalls_rf4 = recall_score(rfc_pred, y4_test)
precisions_rf4 = precision_score(rfc_pred, y4_test)
accuracys_rf4 = accuracy_score(rfc_pred, y4_test)
rf4_end = time.time()
print("====RandomForest with Tfidf Imbalanced 2010", df3_l, ts05)
print('RFC F1 score', frfc*100)
print('RFC ROCAUC Score:', roc_auc_score(y4_test, rfc_pred)*100)
print('RFC Recall score:', recalls_rf4*100)
print('RFC Precision Score:', precisions_rf4*100)
print('RFC Confusion Matrix', confusion_matrix(y4_test,rfc_pred), "\n")
print('RFC Classification', classification_report(y4_test,rfc_pred), "\n")
print('RFC Accuracy Score', accuracys_rf4*100)
print("Execution Time for Random Forest Classifier Imbalanced: ", rf4_end - start5, "seconds")

from sklearn.metrics import auc, precision_recall_curve

y_test_int4 = y4_test.replace({'Positive': 1, 'Negative': 0})

baseline_model4 = sum(y_test_int4 == 1) / len(y_test_int4)

precision_lr4, recall_lr4, _ = precision_recall_curve(y_test_int4, probs_lr4)
auc_lr4 = auc(recall_lr4, precision_lr4)

precision_dt4, recall_dt4, _ = precision_recall_curve(y_test_int4, probs_dt4)
auc_dt4 = auc(recall_dt4, precision_dt4)

precision_nb4, recall_nb4, _ = precision_recall_curve(y_test_int4, probs_nb4)
auc_nb4 = auc(recall_nb4, precision_nb4)

precision_xg4, recall_xg4, _ = precision_recall_curve(y_test_int4, probs_xg4)
auc_xg4 = auc(recall_xg4, precision_xg4)

precision_cs, recall_cs, _ = precision_recall_curve(y_test_int, probs_cs)
auc_cs = auc(recall_cs, precision_cs)


precision_rf4, recall_rf4, _ = precision_recall_curve(y_test_int4, probs_rf4)
auc_rf4  = auc(recall_rf4, precision_rf4)


# Update the collection over the test set
Accuracy_LRN.append(accuracys_lr)
Recall_LRN.append(recalls_lr)
Precision_LRN.append(precisions_lr)

Accuracy_DCT.append(accuracys_dt)
Recall_DCT.append(recalls_dt)
Precision_DCT.append(precisions_dt)

Accuracy_NBB.append(accuracys_nb)
Recall_NBB.append(recalls_nb)
Precision_NBB.append(precisions_nb)

Accuracy_XGB.append(accuracys_xg)
Recall_XGB.append(recalls_xg)
Precision_XGB.append(precisions_xg)
'''
'''
plt.figure(figsize=(12, 7))
plt.plot([0, 1], [baseline_model, baseline_model], linestyle='--', label='Baseline model')
plt.plot(recall_lr, precision_lr, label=f'AUC Log. Reg. Imb.) = {auc_lr:.2f}')
plt.plot(recall_dt, precision_dt, label=f'AUC Dec. Tree Imb.) = {auc_dt:.2f}')
plt.plot(recall_nb, precision_nb, label=f'AUC Nai Bay. Imb.) = {auc_rf:.2f}')
plt.plot(recall_xg, precision_xg, label=f'AUC XGB Imb.) = {auc_xg:.2f}')

plt.plot(recall_lr1, precision_lr1, label=f'AUC Log. Reg. ROS) = {auc_lr:.2f}')
plt.plot(recall_dt1, precision_dt1, label=f'AUC Dec. Tree ROS) = {auc_dt:.2f}')
plt.plot(recall_nb1, precision_nb1, label=f'AUC Nai Bay. ROS) = {auc_rf:.2f}')
plt.plot(recall_xg1, precision_xg1, label=f'AUC XGB ROS) = {auc_xg:.2f}')
plt.plot(recall_lr2, precision_lr2, label=f'AUC Log. Reg. ROS) = {auc_lr2:.2f}')
plt.plot(recall_dt2, precision_dt2, label=f'AUC Dec. Tree ROS) = {auc_dt2:.2f}')
plt.plot(recall_nb2, precision_nb2, label=f'AUC Nai Bay. ROS) = {auc_rf2:.2f}')
plt.plot(recall_xg2, precision_xg2, label=f'AUC XGB ROS) = {auc_xg2:.2f}')

plt.title('Precision-Recall Curves 2010: Imbalanced', size=20)
plt.xlabel('Recall', size=14)
plt.ylabel('Precision', size=14)
plt.legend()
plt.show();

df3['label'].value_counts()

import seaborn as sns
i = sns.countplot(df3['label'])
i.set_xticklabels(['Negative','Positive'])
plt.show()

# class count
label_count_neg2, label_count_pos2 = df3['label'].value_counts()

# Separate class
label_neg2 = df3[df3['label'] == 0]
label_pos2 = df3[df3['label'] == 1]# print the shape of the class
print('Label Negative:', label_neg2.shape)
print('Label Positive:', label_pos2.shape)

label_neg_under2 = label_neg2.sample(label_count_pos2)

test_under2 = pd.concat([label_neg_under2, label_pos2], axis=0)

print("total class of pos and neg :",test_under2['label'].value_counts())# plot the count after under-sampling
test_under2['label'].value_counts().plot(kind='bar', title='label (target)')

label_pos_over2 = label_pos2.sample(label_count_neg2, replace=True)

test_over2 = pd.concat([label_pos_over2, label_neg2], axis=0)

print("total class of pos and neg:",test_under2['label'].value_counts())# plot the count after under-sampeling
test_over2['label'].value_counts().plot(kind='bar', title='label (target)')

import imblearn

# import library
from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
x2 = x_tfidf3
y2 = df3['label']
print(x2.shape)
print(y2.shape)


rus2 = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable
x_rus2, y_rus2 = rus2.fit_resample(x2, y2)

print('Original dataset shape:', Counter(y2))
print('Resample dataset shape', Counter(y_rus2))

# import library
from collections import Counter
from imblearn.over_sampling import RandomOverSampler

x =x_tfidf
y = df1["label"]

rus2 = RandomUnderSampler(random_state=42)
#ros = RandomOverSampler(random_state=42)
#Random over-sampling with imblearn
# fit predictor and target variable
x_rus2, y_rus2 = rus2.fit_resample(x2, y2)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_rus))
#Random over-sampling with imblearn
ros2 = RandomOverSampler(random_state=42)

# fit predictor and target variable
x_ros2, y_ros2 = ros2.fit_resample(x2, y2)

print('Original dataset shape', Counter(y2))
print('Resample dataset shape', Counter(y_ros2))
# import library

# Under-sampling: Tomek links
from imblearn.under_sampling import TomekLinks
from collections import Counter


tl2 = RandomOverSampler(sampling_strategy='majority')

# fit predictor and target variable
x_tl2, y_tl2 = ros2.fit_resample(x2, y2)

print('Original dataset shape', Counter(y2))
print('Resample dataset shape', Counter(y_ros2))

# import library
from imblearn.over_sampling import SMOTE

smote2 = SMOTE()

# fit predictor and target variable
x_smote2, y_smote2 = smote2.fit_resample(x2, y2)

print('Original dataset shape', Counter(y2))
print('Resample dataset shape', Counter(y_ros2))


from imblearn.under_sampling import NearMiss

nm2 = NearMiss()

x_nm2, y_nm2 = nm2.fit_resample(x2, y2)

print('Original dataset shape:', Counter(y2))
print('Resample dataset shape:', Counter(y_nm2))

#====Sampling Technique=====
x5_train, x5_test, y5_train, y5_test = train_test_split(x_tl2,y_tl2,test_size=0.5)

start26 = time.time()
log = LogisticRegression(penalty='l2',random_state=0, solver='lbfgs', multi_class='auto', max_iter=500)
model_lr5 = log.fit(x5_train,y5_train)
probs_lr5 = model_lr5.predict_proba(x5_test)[:, 1]
ly_prediction = log.predict(x5_test)
fly = f1_score(ly_prediction,y5_test)
recalls_lr5 = recall_score(y5_test, ly_prediction)
precisions_lr5 = precision_score(y5_test, ly_prediction)
accuracys_lr5 = accuracy_score(y5_test, ly_prediction)
lr5_end = time.time()
print("===Logistic Regression with TfidfVectorizer Tomelinks - 2010", df3_l, ts05)
print('Logistic F1-score',fly*100)
print('Logistic ROCAUC score:',roc_auc_score(y5_test, ly_prediction)*100)
print('Logistic Recall score:', recalls_lr5*100)
print('Logistic Precision Score:', precisions_lr5*100)
print('Logistic Confusion Matrix', confusion_matrix(y5_test,ly_prediction), "\n")
print('Logistic Classification', classification_report(y5_test,ly_prediction), "\n")
print('Logistic Accuracy Score', accuracys_lr5*100)
print("Execution Time for Logistic Regression Tomelinks: ", lr5_end - start26, "seconds")

from sklearn.tree import DecisionTreeClassifier
start27 = time.time()
DCT = DecisionTreeClassifier()
model_dt5 = DCT.fit(x5_train, y5_train)
probs_dt5 = model_dt5.predict_proba(x5_test)[:, 1]
dct_pred = DCT.predict(x5_test)
fdct = f1_score(dct_pred,y5_test)
recalls_dt5 = recall_score(y5_test, dct_pred)
precisions_dt5 = precision_score(y5_test, dct_pred)
accuracys_dt5 = accuracy_score(y5_test, dct_pred)
print("===DecisionTreeClassifier with TfidfVectorizer Tomelinks - 2010", df3_l, ts05)
dt5_end = time.time()
print('DCT F1-score',fdct*100)
print('DCT ROCAUC score:',roc_auc_score(y5_test, dct_pred)*100)
print('DCT Recall score:', recalls_dt5*100)
print('DCT Precision Score:', precisions_dt5*100)
print('DCT Confusion Matrix', confusion_matrix(y5_test, dct_pred), "\n")
print('DCT Classification', classification_report(y5_test, dct_pred), "\n")
print('DCT Accuracy Score', accuracys_dt5*100)
print("Execution Time for Decision Tree Tomelinks: ", dt5_end - start17, "seconds")

from sklearn.naive_bayes import MultinomialNB
start28 = time.time()
Naive = MultinomialNB()
model_nb5 = Naive.fit(x5_train,y5_train)
probs_nb5 = model_nb5.predict_proba(x5_test)[:, 1]
# predict the labels on validation dataset
ny_pred = Naive.predict(x5_test)
fna = f1_score(ny_pred,y5_test)
recalls_nb5 = recall_score(y5_test, ny_pred)
precisions_nb5 = precision_score(y5_test, ny_pred)
accuracys_nb5 = accuracy_score(y5_test, ny_pred)
# Use accuracy_score function to get the accuracy
print("===Naive Bayes with TfidfVectorizer Tomelinks - 2010", df1_l, ts05)
nb5_end = time.time()
print('Naive F1-score',fna*100)
print('Naive ROCAUC score:',roc_auc_score(y5_test, ny_pred)*100)
print('Naive Recall score:', recalls_nb5*100)
print('Naive Precision Score:', precisions_nb5*100)
print('Naive Confusion Matrix', confusion_matrix(y5_test, ny_pred), "\n")
print('Naive Classification', classification_report(y5_test, ny_pred), "\n")
print('Naive Accuracy Score', accuracys_nb5*100)
print("Execution Time for Naive Bayes Tomelinks: ", nb5_end - start28, "seconds")

# XGBoost Classifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
start29 = time.time()
XGB = XGBClassifier()
xgb_model5 = XGB.fit(x5_train,y5_train)
probs_xg5 = xgb_model5.predict_proba(x5_test)[:, 1]
# predict
xgb_y_predict = xgb_model5.predict(x5_test)
fxg = f1_score(xgb_y_predict,y5_test)
recalls_xg5 = recall_score(y5_test, xgb_y_predict)
precisions_xg5 = precision_score(y5_test, xgb_y_predict)
accuracys_xg5 = accuracy_score(y5_test, xgb_y_predict)
print("===XGB with TfidfVectorizer Tomelinks- 2010", df3_l, ts05)
xg5_end = time.time()
print('XGB F1-Score', fxg*100)
print('XGB ROCAUC Score:', roc_auc_score(xgb_y_predict, y5_test)*100)
print('XGB Recall score:', recalls_xg5*100)
print('XGB Precision Score:', precisions_xg5*100)
print('XGB Confusion Matrix', confusion_matrix(xgb_y_predict, y5_test), "\n")
print('XGB Classification', classification_report(xgb_y_predict, y5_test), "\n")
print('XGB Accuracy Score', accuracys_xg5*100)
print("Execution Time for XGBoost Tomelinks: ", xg5_end - start29, "seconds")


# Support Vector Machine Classifier
from sklearn.svm import SVC
csvm_model1 = SVC(C=1.0, kernel='linear', degree=3, gamma='auto').fit(x1_train,y1_train)
probs_cs1 = csvm_model1.predict_proba(x1_test)[:, 1]
# predict the labels on validation dataset
csvy_pred = csvm_model1.predict(x1_test)
fcsvm = f1_score(csvy_pred,y1_test)
print("===C-SVM with TfidfVectorizer Tomelinks - 2010===")
print('C-SVM F1-score',fcsvm*100)
# Use accuracy_score function to get the accuracy
print('C-SVM ROCAUC score:',roc_auc_score(y1_test, csvy_pred)*100)
print('C-SVM Recall score:', recall_score(y1_test, csvy_pred)*100)
print('C-SVM Precision Score:', precision_score(y1_test, csvy_pred)*100)
print('C-SVM Confusion Matrix', confusion_matrix(y1_test, csvy_pred))
print('C-SVM Classification', classification_report(y1_test, csvy_pred))
print('C-SVM Accuracy Score', accuracy_score(y1_test, csvy_pred)*100)


# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
start30 = time.time()
rfc_model5 = RandomForestClassifier(n_estimators=1000, random_state=0).fit(x5_train,y5_train)
probs_rf5 = rfc_model5.predict_proba(x5_test)[:, 1]
rfc_pred = rfc_model5.predict(x5_test)
frfc = f1_score(rfc_pred,y5_test)
recalls_rf5 = recall_score(y5_test, rfc_pred)
precisions_rf5 = precision_score(y5_test, rfc_pred)
accuracys_rf5 = accuracy_score(y5_test, rfc_pred)
rf5_end = time.time()
print("====RandomForest with Tfidf Tomelinks-  2010", df3_l, ts05)
print('RFC F1 score', frfc*100)
print('RFC ROCAUC Score:', roc_auc_score(y5_test, rfc_pred)*100)
print('RFC Recall score:', recalls_rf5*100)
print('RFC Precision Score:', precisions_rf5*100)
print('RFC Confusion Matrix', confusion_matrix(y5_test,rfc_pred))
print('RFC Classification', classification_report(y5_test,rfc_pred))
print('RFC Accuracy Score', accuracys_rf5*100)
print("Execution Time for Random Forest Tomelinks: ", rf5_end - start10, "seconds")

# Update the collection over the test set
Accuracy_LRN_s.append(accuracys_lr)
Recall_LRN_s.append(recalls_lr)
Precision_LRN_s.append(precisions_lr)

Accuracy_DCT_s.append(accuracys_dt)
Recall_DCT_s.append(recalls_dt)
Precision_DCT_s.append(precisions_dt)

Accuracy_NBB_s.append(accuracys_nb)
Recall_NBB_s.append(recalls_nb)
Precision_NBB_s.append(precisions_nb)

Accuracy_XGB_s.append(accuracys_xg)
Recall_XGB_s.append(recalls_xg)
Precision_XGB_s.append(precisions_xg)

y_test_int5 = y5_test.replace({'Positive': 1, 'Negative': 0})

baseline_model5 = sum(y_test_int5 == 1) / len(y_test_int5)

precision_lr5, recall_lr5, _ = precision_recall_curve(y_test_int5, probs_lr5)
auc_lr5 = auc(recall_lr5, precision_lr5)

precision_dt5, recall_dt5, _ = precision_recall_curve(y_test_int5, probs_dt5)
auc_dt5 = auc(recall_dt5, precision_dt5)

precision_nb5, recall_nb5, _ = precision_recall_curve(y_test_int5, probs_nb5)
auc_nb5 = auc(recall_nb5, precision_nb5)

precision_xg5, recall_xg5, _ = precision_recall_curve(y_test_int5, probs_xg5)
auc_xg5 = auc(recall_xg5, precision_xg5)

precision_cs1, recall_cs1, _ = precision_recall_curve(y_test_int1, probs_cs1)
auc_cs1 = auc(recall_cs1, precision_cs1)

precision_rf5, recall_rf5, _ = precision_recall_curve(y_test_int5, probs_rf5)
auc_rf5 = auc(recall_rf5, precision_rf5)

y_test_int4 = y4_test.replace({'Positive': 1, 'Negative': 0})
print("Print:", type(y_test_int4))
y_test_int5 = y5_test.replace({'Positive': 1, 'Negative': 0})
print("Print:", type(y_test_int5))
#ytest_sp1 = pd.concat([y_test_int,y_test_int1])
#print(ytest_sp1)
precision_imb3 =  (precisions_lr4 + precisions_dt4 + precisions_nb4 + precisions_xg4 + precisions_rf4) / 5
print("Average of Precision Imbalanced Split 3:", precision_imb3)
precision_sam3 =  (precisions_lr5 + precisions_dt5 + precisions_nb5 + precisions_xg5 + precisions_rf5) / 5
print("Average of Precision Sampling Split 3:", precision_sam3)
precision_sp3 = (precision_imb3 + precision_sam2) / 2
print("Average of Precision Imbalanced and Sampling Split 3:", precision_sp3)
recall_imb3 = (recalls_lr4 + recalls_dt4 + recalls_nb4 + recalls_xg4 + recalls_rf4) / 5
print("Average of Recall Imbalanced Split 3:", recall_imb2)
recall_sam3 = (recalls_lr5 + recalls_lr5 + recalls_nb5 + recalls_xg5 + recalls_rf5) / 5
print("Average of Recall Sampling Split 3:", recall_sam2)
recall_sp3 = (recall_imb3 + recall_sam3) / 2
print("Average of Recall Imbalanced and Sampling Split 3:", recall_sp2)
probs_imb3 = (probs_lr4 + probs_dt4 + probs_nb4 + probs_xg4 + probs_rf4) / 5
print("Probability Array of Imbalanced Split 3:", probs_imb3)
probs_sam3 = (probs_lr5 + probs_lr5 + probs_nb5 + probs_xg5 + probs_rf5) / 5
print("Probability Array of Sampling Split 3:",probs_sam3)
#print(probs_imb1.shape())
#print(probs_sam1.shape())
#probs_sp1 = ((np.array(probs_imb1)) + (np.array(probs_sam1))) / 2

#y_test_avg1 = ytest_sp1.replace({'Positive': 1, 'Negative': 0})

precision_imb3, recall_imb3, _ = precision_recall_curve(y_test_int4, probs_imb3)
auc_imb3 = auc(recall_imb3, precision_imb3)

precision_sam3, recall_sam3, _ = precision_recall_curve(y_test_int5, probs_sam3)
auc_sam3 = auc(recall_sam3, precision_sam3)


plt.plot(recall_lr1, precision_lr1, label=f'AUC Log. Reg. Tomelinks) = {auc_lr:.2f}')
plt.plot(recall_dt1, precision_dt1, label=f'AUC Dec. Tree Tomelinks) = {auc_dt:.2f}')
plt.plot(recall_nb1, precision_nb1, label=f'AUC Nai Bay. Tomelinks) = {auc_rf:.2f}')
plt.plot(recall_xg1, precision_xg1, label=f'AUC XGB Tomelinks) = {auc_xg:.2f}')

print(60*'-')
print('The results below over test set')

print("Mean Accuracy call for Logistic Regression: {0}".format(np.mean(Accuracy_LRN)))
print("Mean Precision call for Logistic Regression: {0}".format(np.mean(Precision_LRN)))
print("Mean Recall call for Logistic Regression: {0}".format(np.mean(Recall_LRN)))

print("Mean Accuracy call for Decision Tree: {0}".format(np.mean(Accuracy_DCT)))
print("Mean Precision call for Decision Tree: {0}".format(np.mean(Recall_DCT)))
print("Mean Recall call for Decision Tree: {0}".format(np.mean(Precision_DCT)))

print("Mean Accuracy call for Naive Bayes: {0}".format(np.mean(Accuracy_NBB)))
print("Mean Precision call for Naive Bayes: {0}".format(np.mean(Precision_NBB)))
print("Mean Recall call for Naive Bayes: {0}".format(np.mean(Recall_NBB)))

print("Mean Accuracy call for XGBoost Classifier: {0}".format(np.mean(Accuracy_XGB)))
print("Mean Precision call for XGBoost Classifier: {0}".format(np.mean(Precision_XGB)))
print("Mean Recall call for XGBoost Classifier: {0}".format(np.mean(Recall_XGB)))

print(20*'-')
print('The results below for sampling techniques')
print("T - Mean Accuracy call for Logistic Regression: {0}".format(np.mean(Accuracy_LRN_s)))
print("T- Mean Precision call for Logistic Regression: {0}".format(np.mean(Precision_LRN_s)))
print("T- Mean Recall call for Logistic Regression: {0}".format(np.mean(Recall_LRN_s)))

print("T- Mean Accuracy call for Decision Tree: {0}".format(np.mean(Accuracy_DCT_s)))
print("T- Mean Precision call for Decision Tree: {0}".format(np.mean(Recall_DCT_s)))
print("T- Mean Recall call for Decision Tree: {0}".format(np.mean(Precision_DCT_s)))

print("T- Mean Accuracy call for Naive Bayes: {0}".format(np.mean(Accuracy_NBB_s)))
print("T- Mean Precision call for Naive Bayes: {0}".format(np.mean(Precision_NBB_s)))
print("T- Mean Recall call for Naive Bayes: {0}".format(np.mean(Recall_NBB_s)))

print("T- Mean Accuracy call for XGBoost: {0}".format(np.mean(Accuracy_XGB_s)))
print("T- Mean Precision call for XGBoost: {0}".format(np.mean(Precision_XGB_s)))
print("T- Mean Recall call for XGBoost: {0}".format(np.mean(Recall_XGB_s)))
'''
## Paired-t test
# Naive Bayes Classifiers
'''
ttest_pre_1 = stats.ttest_rel(precision_nb,precision_nb1)
ttest_rec_1 = stats.ttest_rel(recall_nb,recall_nb1)
#ttest_pre_1 = stats.ttest_rel(Precision_NBB,Precision_LRN)
#ttest_re_1 = stats.ttest_rel(Recall_NBB,Recall_LRN)
#ttest_run_1 = stats.ttest_rel(Runtime_NNN,Runtime_LLL)
print('Pair T-Test between Precision between Naive Bayes Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_1))
print('Pair T-Test between Recall between Naive Bayes Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_1))
#print('Pair T-Test between Precision and Recall of Naive Bayes - 2010 Tomelinks {0}'.format(ttest_sam_1))
#print('Precision - Pair-t test between Neural and Logistic {0}'.format(ttest_pre_1))
#print('Recall - Pair-t test between Neural and Logistic {0}'.format(ttest_re_1))
#print('Runtime - Pair-t test between Neural and Logistic {0}'.format(ttest_run_1))
print(60*'-')

# Decision Tree Classifier
ttest_pre_2 = stats.ttest_rel(precision_dt,precision_dt1)
ttest_rec_2 = stats.ttest_rel(recall_dt1,recall_dt1)
#ttest_pre_2 = stats.ttest_rel(Precision_NBB,Precision_DCT)
#ttest_re_2 = stats.ttest_rel(Recall_NBB,Recall_DCT)
#ttest_run_2 = stats.ttest_rel(Runtime_NBB,Runtime_DCT)
print('Pair T-Test of Precision between Decision Tree Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_2))
print('Pair T-Test of Recall between Decision Tree Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_2))
#print('Precision - Pair-t test between Neural and Naive {0}'.format(ttest_pre_2))
#print('Recall - Pair-t test between Neural and Naive {0}'.format(ttest_re_2))
#print('Runtime - Pair-t test between Neural and Naive {0}'.format(ttest_run_2))
print(60*'-')

# XGB Boost Classifier
ttest_pre_3 = stats.ttest_rel(precision_xg,precision_xg1)
ttest_rec_3 = stats.ttest_rel(recall_xg,recall_xg1)
#ttest_pre_3 = stats.ttest_rel(Precision_XGB,Precision_NBB)
#ttest_re_3 = stats.ttest_rel(Recall_XGB,Recall_NBB)
#ttest_run_3 = stats.ttest_rel(Runtime_LLL,Runtime_NBB)
print('Pair T-Test of Precision between XGBoost Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_3))
print('Pair T-Test of Recall between XGBoost Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_3))
#print('Precision - Pair-t test between Logistic and Naive {0}'.format(ttest_pre_3))
#print('Recall - Pair-t test between Logistic and Naive {0}'.format(ttest_re_3))
#print('Runtime - Pair-t test between Logistic and Naive {0}'.format(ttest_run_3))

# Random Forest Classifier
ttest_pre_4 = stats.ttest_rel(precision_rf,precision_rf1)
ttest_rec_4 = stats.ttest_rel(recall_rf,recall_rf1)
#ttest_pre_4 = stats.ttest_rel(Precision_XGB,Precision_NBB)
#ttest_re_4 = stats.ttest_rel(Recall_XGB,Recall_NBB)
#ttest_run_4 = stats.ttest_rel(Runtime_LLL,Runtime_NBB)
print('Pair T-Test of Precision between Random Forest Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_4))
print('Pair T-Test of Recall between Random Forest Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_4))
#print('Precision - Pair-t test between Logistic and Naive {0}'.format(ttest_pre_4))
#print('Recall - Pair-t test between Logistic and Naive {0}'.format(ttest_re_4))
#print('Runtime - Pair-t test between Logistic and Naive {0}'.format(ttest_run_4))

ttest_pre_5 = stats.ttest_rel(precision_lr,precision_lr1)
ttest_rec_5 = stats.ttest_rel(recall_lr,recall_lr1)
print('Pair T-Test of Precision between Logistic Regression Imbalanced and Tomelinks - 2010 {0}'.format(ttest_pre_5))
print('Pair T-Test of Precision between Logistic Regression Imbalanced and Tomelinks - 2010 {0}'.format(ttest_rec_5))

print(20*'-')
print("Execution Time for Logistic Regression Imbalanced - 2010", df3_l, ts05 ,":", lr2_end - start1, "seconds")
print("Execution Time for Decision Tree Imbalanced - 2010", df3_l, ts05 ,":", dt2_end - start2, "seconds")
print("Execution Time for Naive Bayes Imbalanced - 2010 ", df3_l, ts05 ,":", nb2_end - start3, "seconds")
print("Execution Time for XGB Imbalanced - 2010 ", df3_l, ts05 ,":", xg2_end - start4, "seconds")
print("Execution Time for Random Forest Imbalanced - 2010 ", df3_l, ts05 ,":", rf2_end - start5, "seconds")
print("Execution Time for Logistic Regression Tomelinks - 2010 ", df3_l, ts05 ,":", lr5_end - start6, "seconds")
print("Execution Time for Decision Tree Tomelinks - 2010 ", df3_l, ts05 ,":", dt5_end - start7, "seconds")
print("Execution Time for Naive Bayes Tomelinks - 2010 ", df3_l, ts05 ,":", nb5_end - start8, "seconds")
print("Execution Time for XGB Tomelinks - 2010 ", df3_l, ts05 ,":", xg5_end - start9, "seconds")
print("Execution Time for Random Forest Tomelinks - 2010 ", df3_l, ts05 ,":", rf5_end - start10, "seconds")
print("\n")

ytest_sp3 = (y4_test + y5_test) / 2
precision_sp3 = (precision_lr3 + precision_dt3 + precision_nb3 + precision_xg3 + precision_rf3 + precision_lr4 + precision_dt4 + precision_nb4 + precision_xg4 + precision_rf4) / 8
recall_sp3 = (recall_lr3 + recall_dt3 + recall_nb3 + recall_xg3 + recall_rf3 + recall_lr4 + recall_lr4 + recall_nb3 + recall_xg3 + recall_rf3) / 8
probs_sp3 = (probs_lr3 + probs_dt3 + probs_nb3 + probs_xg3 + probs_rf3 + probs_lr4 + probs_lr4 + probs_nb4 + probs_xg4 + probs_rf4) / 8

y_test_avg3 = ytest_sp3.replace({'Positive': 1, 'Negative': 0})

precision_sp3, recall_sp3, _ = precision_recall_curve(y_test_avg3, probs_sp3)
auc_sp3 = auc(recall_sp3, precision_sp3)

plt.figure(figsize=(12, 7))
#plt.plot([0, 1], [baseline_model1, baseline_model2, baseline_model3, baseline_model4, baseline_model5], linestyle='--', label='Baseline model')
plt.plot(recall_imb1, precision_imb1, label=f'AUC SP1 (Imb) = {auc_imb1:.2f}')
plt.plot(recall_sam1, precision_sam1, label=f'AUC SP1 (Sam) = {auc_sam1:.2f}')
plt.plot(recall_imb2, precision_imb2, label=f'AUC SP2 (Imb) = {auc_imb2:.2f}')
plt.plot(recall_sam2, precision_sam2, label=f'AUC SP2 (Sam) = {auc_sam2:.2f}')
plt.plot(recall_imb3, precision_imb3, label=f'AUC SP3 (Imb) = {auc_imb3:.2f}')
plt.plot(recall_sam3, precision_sam3, label=f'AUC SP3 (Sam) = {auc_sam3:.2f}')

plt.plot(recall_xg, precision_xg, label=f'AUC XGB Imb. = {auc_xg:.2f}')
#plt.plot(recall_cs, precision_cs, label=f'AUC CSVM Imb. = {auc_cs:.2f}')
plt.plot(recall_rf, precision_rf, label=f'AUC RFC Imb. = {auc_rf:.2f}')
plt.plot(recall_lr1, precision_lr1, label=f'AUC Log. Reg. Tomelinks = {auc_lr1:.2f}')
plt.plot(recall_dt1, precision_dt1, label=f'AUC Dec. Tree Tomelinks = {auc_dt1:.2f}')
plt.plot(recall_nb1, precision_nb1, label=f'AUC Nai Bay. Tomelinks = {auc_nb1:.2f}')
plt.plot(recall_xg1, precision_xg1, label=f'AUC XGB Tomelinks = {auc_xg1:.2f}')
#plt.plot(recall_cs1, precision_cs1, label=f'AUC CSVM Tomelinks = {auc_cs1:.2f}')
plt.plot(recall_rf1, precision_rf1, label=f'AUC RFC Tomelinks = {auc_rf1:.2f}')

plt.title('Precision-Recall Curves 2010 - ', size=20)
plt.xlabel('Recall', size=14)
plt.ylabel('Precision', size=14)
plt.legend()
plt.show();


plt.figure(figsize=(12, 7))
plt.plot([0, 1], [baseline_model, baseline_model], linestyle='--', label='Baseline model')
plt.plot(recall_lr, precision_lr, label=f'AUC Log. Reg. Imb. = {auc_lr:.2f}')
plt.plot(recall_dt, precision_dt, label=f'AUC Dec. Tree Imb. = {auc_dt:.2f}')
plt.plot(recall_nb, precision_nb, label=f'AUC Nai Bay. Imb. = {auc_nb:.2f}')
plt.plot(recall_xg, precision_xg, label=f'AUC XGB Imb. = {auc_xg:.2f}')
#plt.plot(recall_cs, precision_cs, label=f'AUC CSVM Imb. = {auc_cs:.2f}')
plt.plot(recall_rf, precision_rf, label=f'AUC RFC Imb. = {auc_rf:.2f}')
plt.plot(recall_lr1, precision_lr1, label=f'AUC Log. Reg. Tomelinks = {auc_lr1:.2f}')
plt.plot(recall_dt1, precision_dt1, label=f'AUC Dec. Tree Tomelinks = {auc_dt1:.2f}')
plt.plot(recall_nb1, precision_nb1, label=f'AUC Nai Bay. Tomelinks = {auc_nb1:.2f}')
plt.plot(recall_xg1, precision_xg1, label=f'AUC XGB Tomelinks = {auc_xg1:.2f}')
#plt.plot(recall_cs1, precision_cs1, label=f'AUC CSVM Tomelinks = {auc_cs1:.2f}')
plt.plot(recall_rf1, precision_rf1, label=f'AUC RFC Tomelinks = {auc_rf1:.2f}')
plt.title('Precision-Recall Curves 2010 - Imbalanced/Tomelinks', size=20)
plt.xlabel('Recall', size=14)
plt.ylabel('Precision', size=14)
plt.legend()
plt.show();
'''
'''
'''
'''
fig2, ax = plt.subplots(1, figsize=(10,7))
samp = [ttest_imb_1, ttest_sam_1, ttest_imb_2, ttest_sam_2, ttest_imb_3, ttest_sam_3, ttest_imb_4, ttest_sam_4]
labels1 = ['NB-Imb', 'NB-TL', 'DT-Imb', 'DT-TL', 'LR-Imb', 'LR-TL', 'RF-Imb', 'RF-TL']
#ax = fig2.add_axes([0,0,1,1])
bp1= ax.boxplot(samp, vert=True, patch_artist=True, labels=labels1)
ax.set_title('T-Test of Precision and Recall Scores for Classifier and Tomelinks 2010')
ax.yaxis.grid(True)
ax.set_xlabel('Samples by Label, Classifier and/or Technique')
ax.set_ylabel('Statistic')
fig2.show()
plt.show();
'''
'''
ticks = ['Naive_Bayes', 'Decision_Tree', 'XGBoost', 'Random_Forest', 'Logistic_Regression']

def set_box_color(bp, color):
    plt.setp(bp['boxes'], color=color)
    plt.setp(bp['whiskers'], color=color)
    plt.setp(bp['caps'], color=color)
    plt.setp(bp['medians'], color=color)

data_imb = [ttest_pre_1, ttest_pre_2, ttest_pre_3, ttest_pre_4, ttest_pre_5]
data_sam = [ttest_rec_1, ttest_rec_2, ttest_rec_3, ttest_rec_4, ttest_rec_5]

bpl = plt.boxplot(data_imb, sym='', widths=0.6, labels=ticks)
bpr = plt.boxplot(data_sam, sym='', widths=0.6, labels=ticks)
set_box_color(bpl, '#D7191C') # colors are from http://colorbrewer2.org/
set_box_color(bpr, '#2C7BB6')

# draw temporary red and blue lines and use them to create a legend
plt.plot([], c='#D7191C', data=data_imb, label='Precision')
plt.plot([], c='#2C7BB6', data=data_sam, label='Recall')
plt.legend()

plt.xticks(range(0, len(ticks) * 2, 2), ticks)
plt.xlim(-2, len(ticks)*2)
plt.ylim(0, 8)
plt.tight_layout()
plt.savefig('boxplot.png')
# show plot

plt.plot(recall_lr2, precision_lr2, label=f'AUC Log. Reg. RUS) = {auc_lr2:.2f}')
plt.plot(recall_dt2, precision_dt2, label=f'AUC Dec. Tree RUS) = {auc_dt2:.2f}')
plt.plot(recall_nb2, precision_nb2, label=f'AUC Nai Bay. RUS) = {auc_rf2:.2f}')
plt.plot(recall_xg2, precision_xg2, label=f'AUC XGB RUS) = {auc_xg2:.2f}')

'''
#====RandomOverSampler====
'''
'''
'''

# import library
from imblearn.under_sampling import TomekLinks
from collections import Counter


tl = RandomOverSampler(sampling_strategy='majority')

# fit predictor and target variable
x_tl, y_tl = ros.fit_resample(x, y)


print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_ros))

# import library
from imblearn.under_sampling import TomekLinks
from collections import Counter

tl = RandomOverSampler(sampling_strategy='majority')

# fit predictor and target variable
x_tl, y_tl = ros.fit_resample(x, y)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_ros))

# import library
from imblearn.over_sampling import SMOTE

smote = SMOTE()

# fit predictor and target variable
x_smote, y_smote = smote.fit_resample(x, y)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_ros))

from imblearn.under_sampling import SMOTE

nm = SMOTE()

x_nm, y_nm = nm.fit_resample(x, y)

print('Original dataset shape:', Counter(y))
print('Resample dataset shape:', Counter(y_nm))

x1_train, x1_test, y1_train, y1_test = train_test_split(x_smote,y_smote,test_size=0.5)

xgb_predict = xgb_model.predict(x1_test)
fxg = f1_score(xgb_predict,y1_test)
print("===XGB Balanced - 2010 SMOTE===", )
print('XGB F1-Score', fxg*100)
print('XGB ROCAUC Score:', roc_auc_score(xgb_predict, y1_test), "\n")
print('XGB Confusion Matrix', confusion_matrix(xgb_predict, y1_test), "\n")
print('XGB Classification', classification_report(xgb_predict, y1_test), "\n")
print('XGB Accuracy Score', accuracy_score(xgb_predict, y1_test)*100)

from sklearn.naive_bayes import MultinomialNB
Naive = MultinomialNB()
Naive.fit(x1_train,y1_train)
# predict the labels on validation dataset
nb_pred = Naive.predict(x1_test)
fna = f1_score(nb_pred,y1_test)
# Use accuracy_score function to get the accuracy
print("===Naive Bayes Balanced - 2010 SMOTE===")
print('Naive F1-score',fna*100)
print('Naive ROCAUC score:',roc_auc_score(y1_test, nb_pred)*100, "\n")
print('Naive Confusion Matrix', confusion_matrix(y1_test, nb_pred), "\n")
print('Naive Classification', classification_report(y1_test, nb_pred), "\n")
print('Naive Accuracy Score', accuracy_score(y1_test, nb_pred)*100)

from sklearn.tree import DecisionTreeClassifier
Tree = DecisionTreeClassifier()
Tree.fit(x1_train, y1_train)
tree_pred = Tree.predict(x1_test)
ftree = f1_score(tree_pred,y1_test)
print("===Decision Tree Balanced - 2010 SMOTE ===")
print('DCT F1-score', ftree*100)
print('DCT ROCAUC score:',roc_auc_score(y1_test, tree_pred)*100, "\n")
print('DCT Confusion Matrix', confusion_matrix(y1_test, tree_pred), "\n")
print('DCT Classification', classification_report(y1_test, tree_pred), "\n")
print('DCT Accuracy Score', accuracy_score(y1_test, tree_pred)*100)

lR = LogisticRegression(penalty='l2',random_state=0, solver='lbfgs', multi_class='auto', max_iter=500)
lR.fit(x1_train,y1_train)
lr_y_prediction = lR.predict(x1_test)
fly = f1_score(lr_y_prediction,y1_test)
print("===Logistic Regression - 2010 SMOTE ===")
print('Logistic F1-score',fly*100)
print('Logistic ROCAUC score:',roc_auc_score(y1_test, lr_y_prediction))
print('Logistic Confusion Matrix', confusion_matrix(y1_test,lr_y_prediction), "\n")
print('Logistic Classification', classification_report(y1_test,lr_y_prediction), "\n")
print('Logistic Accuracy Score', accuracy_score(y1_test,lr_y_prediction)*100)

#LinearSVC Classifier
from sklearn.svm import LinearSVC
lsvc_model = LinearSVC()
lsvc_model.fit(x1_train,y1_train)
lsvcm_pred = lsvc_model.predict(x1_test)
lsvccm = f1_score(lsvc_pred,y1_test)
print("=== LSVC - 2010 SMOTE ===")
print('LSVC F1 score', lsvccm*100)
print('LSVC ROCAUC Score:', roc_auc_score(y1_test, lsvcm_pred)*100, "\n")
print('LSVC Confusion Matrix', confusion_matrix(y1_test, lsvcm_pred), "\n")
print('LSVC Classification', classification_report(y1_test, lsvcm_pred), "\n")
print('LSVC Accuracy Score', accuracy_score(y1_test, lsvcm_pred)*100)
'''
'''

# load library
from sklearn.ensemble import RandomForestClassifier

rfc1 = RandomForestClassifier()

# fit the predictor and target
rfc1.fit(x1_train, y1_train)

# predict
rfc_predict = rfc1.predict(x1_test)# check performance
print("=== RFC Balanced - 2010 SMOTE  ===")
print('RFC ROCAUC score:',roc_auc_score(y1_test, rfc_predict)*100)
print('RFC Accuracy score:',accuracy_score(y1_test, rfc_predict)*100)
print('RFC F1 score:',f1_score(y1_test, rfc_predict)*100)
print('RFC Confusion Matrix', confusion_matrix(y1_test, rfc_predict), "\n")
print('RFC Classification', classification_report(y1_test, rfc_predict), "\n")

# load library
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score
from sklearn.metrics import f1_score

# we can add class_weight='balanced' to add panalize mistake
csvc_model = SVC(class_weight='balanced', probability=True)

csvc_model.fit(x1_train, y1_train)

csvc_predict = csvc_model.predict(x1_test)# check performance
print("====CSVM Balanced - 2010 SMOTE ====")
print('CSVM ROCAUC score:',roc_auc_score(y1_test, csvc_predict)*100)
print('CSVM Accuracy score:',accuracy_score(y1_test, csvc_predict)*100)
print('CSVM F1 score:',f1_score(y1_test, csvc_predict)*100)
print('CSVM Confusion Matrix', confusion_matrix(y1_test, csvc_predict), "\n")
print('CSVM Classification', classification_report(y1_test, csvc_predict), "\n")
'''
'''
#Multiple models
models = [KNeighborsClassifier(),
          LogisticRegression(solver='lbfgs', multi_class='ovr'),
          DecisionTreeClassifier(),
          SVC(gamma='scale'),
          RandomForestClassifier(n_estimators=100),
          ExtraTreesClassifier(n_estimators=100)]

tvec = TfidfVectorizer(stop_words='english',
                       #sublinear_tf=True,
                       max_df=0.5,
                       max_features=1000)

tvec.fit(data_train['data'])
X_train = tvec.transform(data_train['data'])
X_test = tvec.transform(data_test['data'])

res = []

for model in models:
    print(model)
    print()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    score = accuracy_score(y_test, y_pred)
    print(score)
    print()
    cm = docm(y_test, y_pred, data_train.target_names)
    print(cm)
    res.append([model, score])
    print()
    print('-'*60)
    print()

from sklearn.metrics import precision_recall_curve
from sklearn.metrics import plot_precision_recall_curve
from sklearn.metrics import (precision_recall_curve, PrecisionRecallDisplay)

import matplotlib.pyplot as plt
#predictions = clf.predict(X_test)
precision, recall, _ = precision_recall_curve(y_test, y_pred)

disp = PrecisionRecallDisplay(precision=precision, recall=recall)
disp.plot()
plt.show()

plot_precision_recall_curve(xgb_model, x1_test, y1_test, ax = plt.gca(),name = "XGB - SMOTE")
plot_precision_recall_curve(Naive, x1_test, y1_test, ax = plt.gca(),name = "Naive Bayes - SMOTE")
plot_precision_recall_curve(Tree, x1_test, y1_test, ax = plt.gca(),name = "Decision Tree - SMOTE")
plot_precision_recall_curve(lR, x1_test, y1_test, ax = plt.gca(),name = "Logistic Regression - SMOTE")
plt.title('Precision-Recall curve for SMOTE')
'''